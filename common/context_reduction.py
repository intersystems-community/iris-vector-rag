"""
Context Reduction Strategies

This module implements various strategies for reducing document context
size to fit within LLM token limits. These strategies are useful when
dealing with large documents or multiple documents that exceed the
context window of an LLM.
"""

import re
import heapq
import numpy as np
from typing import List, Dict, Any, Callable, Optional
from .utils import Document # Changed to relative import

def count_tokens(text: str) -> int:
    """
    Approximate token count based on word splitting.
    For more accurate counts, a real tokenizer should be used.
    
    Args:
        text: Text to count tokens for
        
    Returns:
        Estimated token count
    """
    # Simple approximation - in production this should use the actual
    # LLM's tokenizer for more accurate counts
    words = text.split()
    # Adjust for the fact that tokenizers typically split some words
    # Rough heuristic: 1 word â‰ˆ 1.3 tokens
    return int(len(words) * 1.3)

def simple_truncation(documents: List[Document], max_tokens: int) -> str:
    """
    Simple truncation strategy that keeps documents in order of score
    and truncates to fit within token limits.
    
    Args:
        documents: List of Document objects
        max_tokens: Maximum number of tokens to include
        
    Returns:
        String of concatenated document content that fits within token limit
    """
    if not documents:
        return ""
    
    # Sort documents by score (highest first)
    sorted_docs = sorted(documents, key=lambda d: d.score if hasattr(d, 'score') and d.score is not None else 0, reverse=True)
    
    included_docs = []
    current_token_count = 0
    
    for doc in sorted_docs:
        doc_tokens = count_tokens(doc.content)
        
        if current_token_count + doc_tokens <= max_tokens:
            # Full document fits
            included_docs.append(doc.content)
            current_token_count += doc_tokens
        else:
            # Only part of document fits
            remaining_tokens = max_tokens - current_token_count
            if remaining_tokens > 100:  # Only include if meaningful chunk remains
                words = doc.content.split()
                estimated_words = int(remaining_tokens / 1.3)
                truncated_content = " ".join(words[:estimated_words]) + "..."
                included_docs.append(truncated_content)
            break
            
    return "\n\n".join(included_docs)

def generate_summary(text: str, query: str = None) -> str:
    """
    Generate a summary of the provided text, optionally focusing on the query.
    In a real implementation, this would call an LLM.
    
    Args:
        text: Text to summarize
        query: Optional query to focus the summary on
        
    Returns:
        Summarized text
    """
    # This is a placeholder. In production, this would call an LLM API.
    # For example:
    # 
    # from langchain.llms import OpenAI
    # 
    # llm = OpenAI(model_name="gpt-3.5-turbo")
    # if query:
    #     prompt = (
    #         f"Summarize the following text, focusing on information relevant to: {query}\n\n"
    #         f"Text: {text}\n\nSummary:"
    #     )
    # else:
    #     prompt = f"Summarize the following text:\n\n{text}\n\nSummary:"
    # 
    # return llm.invoke(prompt)
    
    # For this placeholder, just return a much shorter version
    return "This is a summarized version of the document."

def recursive_summarization(documents: List[Document], query: str, max_tokens: int) -> str:
    """
    Recursively summarize documents until they fit within token limits.
    
    Args:
        documents: List of Document objects
        query: Query to focus summaries on
        max_tokens: Maximum number of tokens to include
        
    Returns:
        Summarized context that fits within token limit
    """
    if not documents:
        return ""
    
    # First, generate summaries for each document
    document_summaries = []
    for doc in documents:
        summary = generate_summary(doc.content, query)
        document_summaries.append({
            "id": doc.id,
            "content": summary,
            "original_score": doc.score if hasattr(doc, 'score') and doc.score is not None else 0
        })
    
    # Combine summaries
    combined_summaries = "\n\n".join([doc["content"] for doc in document_summaries])
    
    # Check if fits within token limit
    if count_tokens(combined_summaries) <= max_tokens:
        return combined_summaries
    
    # If still too large, either:
    # 1. Summarize the summaries again (recursive)
    # 2. Or truncate the least important ones
    
    # Option 1: Recursive summarization (limited recursion depth)
    meta_summary = generate_summary(combined_summaries, query)
    if count_tokens(meta_summary) <= max_tokens:
        return meta_summary
    
    # Option 2: Truncate least important summaries
    sorted_summaries = sorted(document_summaries, key=lambda d: d["original_score"], reverse=True)
    
    final_summaries = []
    current_token_count = 0
    
    for summary in sorted_summaries:
        summary_tokens = count_tokens(summary["content"])
        
        if current_token_count + summary_tokens <= max_tokens:
            final_summaries.append(summary["content"])
            current_token_count += summary_tokens
        else:
            break
            
    return "\n\n".join(final_summaries)

def embeddings_reranking(documents: List[Document], query: str, embedding_model: Any, max_tokens: int) -> str:
    """
    Rerank document chunks based on embedding similarity to query and 
    select most relevant chunks that fit within token limits.
    
    Args:
        documents: List of Document objects
        query: Query to compare chunks against
        embedding_model: Model to generate embeddings
        max_tokens: Maximum number of tokens to include
        
    Returns:
        Reranked and truncated context
    """
    if not documents:
        return ""
    
    # Break documents into smaller chunks
    chunks = []
    for doc in documents:
        # Simple chunking by paragraphs
        paragraphs = re.split(r'\n\s*\n', doc.content)
        for i, para in enumerate(paragraphs):
            if para.strip():  # Skip empty paragraphs
                chunks.append({
                    "doc_id": doc.id,
                    "chunk_id": f"{doc.id}_p{i}",
                    "content": para.strip(),
                    "original_score": doc.score if hasattr(doc, 'score') and doc.score is not None else 0
                })
    
    # Generate embeddings for query and chunks
    query_embedding = embedding_model.encode(query)
    
    # Process chunks in batches to prevent memory issues with large documents
    batch_size = 32
    all_chunk_embeddings = []
    
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        batch_texts = [chunk["content"] for chunk in batch]
        batch_embeddings = embedding_model.encode(batch_texts)
        all_chunk_embeddings.extend(batch_embeddings)
    
    # Calculate similarity scores
    for i, chunk in enumerate(chunks):
        # Cosine similarity between query and chunk embeddings
        similarity = np.dot(query_embedding, all_chunk_embeddings[i]) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(all_chunk_embeddings[i])
        )
        # Fix for NumPy deprecation warning - ensure we're converting a scalar value
        chunk["similarity_score"] = float(similarity.item() if hasattr(similarity, 'item') else similarity)
        
        # Combined score (blend of similarity and original document score)
        chunk["combined_score"] = 0.7 * chunk["similarity_score"] + 0.3 * chunk["original_score"]
    
    # Sort chunks by combined score
    sorted_chunks = sorted(chunks, key=lambda x: x["combined_score"], reverse=True)
    
    # Select chunks until max_tokens is reached
    selected_chunks = []
    current_token_count = 0
    
    for chunk in sorted_chunks:
        chunk_tokens = count_tokens(chunk["content"])
        
        if current_token_count + chunk_tokens <= max_tokens:
            selected_chunks.append(chunk)
            current_token_count += chunk_tokens
        else:
            # If we can't add any more full chunks, and we've selected some already, stop
            if selected_chunks:
                break
            
            # If we haven't selected any chunks yet, take the most important one and truncate it
            words = chunk["content"].split()
            estimated_words = int(max_tokens / 1.3)
            truncated_content = " ".join(words[:estimated_words]) + "..."
            
            selected_chunks.append({
                **chunk,
                "content": truncated_content
            })
            break
    
    # Sort selected chunks back by document ID and chunk position for readability
    # (this is optional - keeping similarity order might be better in some cases)
    selected_chunks.sort(key=lambda x: (x["doc_id"], x["chunk_id"]))
    
    return "\n\n".join([chunk["content"] for chunk in selected_chunks])

def process_document(document: Document, query: str) -> str:
    """
    Process a single document to extract relevant information for the query.
    In a real implementation, this would use an LLM.
    
    Args:
        document: Document to process
        query: Query to focus on
        
    Returns:
        Processed/extracted information
    """
    # This is a placeholder. In production, this would call an LLM API
    # with a prompt designed to extract only the relevant information from
    # the document based on the query.
    return "This is a processed document summary."

def map_reduce_approach(documents: List[Document], query: str) -> str:
    """
    Process each document separately to extract relevant information,
    then combine the results.
    
    Args:
        documents: List of Document objects
        query: Query to focus information extraction on
        
    Returns:
        Combined extracted information
    """
    if not documents:
        return ""
    
    # MAP phase: Process each document separately
    document_extracts = []
    for doc in documents:
        # Extract relevant information from each document
        extracted_info = process_document(doc, query)
        document_extracts.append(extracted_info)
    
    # REDUCE phase: Combine the extracted information
    # In a production system, this could be another LLM call to synthesize
    # all the extracted pieces into a coherent context
    combined_context = "\n\n".join(document_extracts)
    
    return combined_context

# --- Main Dispatcher Function ---
def reduce_context(
    query: str,
    documents: List[Dict[str, Any]], # Expects list of dicts with "content" key
    max_tokens: int,
    strategy: str = "simple_truncation",
    embedding_model: Optional[Any] = None # Needed for some strategies like embeddings_reranking
) -> str:
    """
    Reduces context using the specified strategy.

    Args:
        query: The user query.
        documents: List of document dictionaries (must have a "content" key).
        max_tokens: Maximum desired token count for the reduced context.
        strategy: The reduction strategy to use.
        embedding_model: Optional embedding model instance (needed for some strategies).

    Returns:
        A string representing the reduced context.
    """
    # Convert list of dicts to list of Document objects for strategy functions
    # The test passes docs_for_reduction = [{"content": doc.content} for doc in documents]
    # So, the input 'documents' here is already List[Dict[str, Any]] with 'content'
    # The strategy functions expect List[Document]
    
    # Create Document objects from the input dictionaries
    # Assuming input `documents` is List[Dict[str, Any]] where each dict has at least 'content'
    # and optionally 'id' and 'score' if needed by strategies.
    # For simplicity, let's assume strategies can adapt or we adapt here.
    # The current test provides `docs_for_reduction = [{"content": doc.content} ...]`
    # The strategies like simple_truncation expect Document objects.
    
    # Let's make strategy functions more flexible or adapt here.
    # For now, let's assume the test will pass List[Document] or we adapt the test.
    # The test actually calls basic_pipeline._retrieve_documents which returns List[Document].
    # Then it converts to docs_for_reduction = [{"content": doc.content} for doc in documents]
    # This means our reduce_context should expect List[Dict[str, Any]] as per its type hint.
    # And the individual strategy functions need to be called with List[Document].

    # Convert input List[Dict[str, Any]] to List[Document] for strategy functions
    doc_objects = []
    for i, doc_dict in enumerate(documents):
        doc_objects.append(Document(
            id=doc_dict.get("id", f"doc_{i}"), # Use index if id not present
            content=doc_dict.get("content", ""),
            score=doc_dict.get("score") 
        ))

    if strategy == "simple_truncation":
        return simple_truncation(doc_objects, max_tokens)
    elif strategy == "recursive_summarization":
        # This strategy requires an LLM for generate_summary, which is currently a placeholder.
        # For testing, it will use the placeholder summary.
        return recursive_summarization(doc_objects, query, max_tokens)
    elif strategy == "embeddings_reranking":
        if embedding_model is None:
            raise ValueError("Embedding model is required for 'embeddings_reranking' strategy.")
        return embeddings_reranking(doc_objects, query, embedding_model, max_tokens)
    elif strategy == "map_reduce":
        # This strategy also uses a placeholder LLM call in process_document.
        return map_reduce_approach(doc_objects, query)
    elif strategy == "semantic_clustering":
        # "semantic_clustering" is not implemented, default to simple_truncation for now.
        # In a real scenario, this would either be implemented or raise NotImplementedError.
        print(f"Warning: Strategy '{strategy}' not implemented, defaulting to 'simple_truncation'.")
        return simple_truncation(doc_objects, max_tokens)
    else:
        raise ValueError(f"Unknown context reduction strategy: {strategy}")
