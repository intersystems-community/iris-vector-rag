# Implementation Plan: GraphRAG Storage Performance Optimization

**Branch**: `057-graphrag-performance-fix` | **Date**: 2025-11-12 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/057-graphrag-performance-fix/spec.md`

## Summary

The GraphRAG storage pipeline is experiencing 80-87% performance degradation, processing tickets at 60 seconds each instead of the expected 10-15 seconds. LLM entity extraction is fast (5-6 seconds), but IRIS storage operations consume 50-120 seconds per ticket due to redundant embedding model loads, serial entity storage without batching, and IRIS connection/transaction overhead. This feature optimizes the storage layer to achieve 240-360 tickets/hour throughput (5-8x improvement), reducing complete dataset processing time from 96 hours to 11-17 hours.

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**:
- iris-vector-rag framework (existing)
- SentenceTransformer (`all-MiniLM-L6-v2`) for embeddings
- InterSystems IRIS Community Edition
- DSPy for entity extraction (no changes needed)

**Storage**: InterSystems IRIS database (vector + graph storage)
**Testing**: pytest with contract tests, integration tests, performance benchmarks
**Target Platform**: Linux server (dpgenai1.iscinternal.com production environment)
**Project Type**: Single project (iris-vector-rag framework enhancement)

**Performance Goals**:
- Individual ticket processing: â‰¤15 seconds (currently 60s)
- Throughput: â‰¥240 tickets/hour (currently 42/hour)
- IRIS storage operations: â‰¤10 seconds (currently 50-120s)
- Complete dataset (10,150 tickets): â‰¤17 hours (currently 96 hours)

**Constraints**:
- Zero data loss or corruption (100% data integrity required)
- No changes to entity extraction logic (already meeting performance targets)
- Backward compatibility with existing storage format
- Memory usage must remain stable (no leaks)
- Must support graceful shutdown and resume capabilities

**Scale/Scope**:
- Production dataset: 10,150 tickets
- Entity volume: 8-12 entities per ticket (82,000-122,000 total)
- Relationship volume: 4-6 relationships per ticket (41,000-61,000 total)
- Performance testing required for 100-ticket batches and 1,000-ticket sustained load

## Constitution Check
*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

**I. Framework-First Architecture**:
- âœ“ Component enhances existing iris-vector-rag framework services
- âœ“ No application-specific logic (pure framework optimization)
- âœ“ CLI interface exposed via existing Make targets and service commands

**II. Pipeline Validation & Requirements**:
- âœ“ Automated requirement validation exists in framework
- âœ“ Setup procedures remain idempotent (no changes to validation)

**III. Test-Driven Development**:
- âœ“ Contract tests required before implementation
- âœ“ Performance tests for 100-ticket and 1,000-ticket scenarios mandatory
- âœ“ Data integrity validation tests required

**IV. Performance & Enterprise Scale**:
- âœ“ This feature IMPLEMENTS batch entity storage optimization
- âœ“ IRIS vector operations will be optimized via batching and connection pooling
- âœ“ Addresses enterprise scale requirement (10K+ documents)

**V. Production Readiness**:
- âœ“ Structured logging exists, will add throughput monitoring
- âœ“ Health checks exist in framework
- âœ“ Docker deployment already supported

**VI. Explicit Error Handling**:
- âœ“ No silent failures (framework already enforces this)
- âœ“ Clear exception messages for storage failures
- âœ“ Actionable error context for performance degradation

**VII. Standardized Database Interfaces**:
- âœ“ Will use existing iris_rag database utilities
- âœ“ Batch operations will extend proven patterns
- âœ“ Optimizations will be contributed back to shared utilities

**Initial Constitution Check: PASS** âœ…

## Project Structure

### Documentation (this feature)
```
specs/057-graphrag-performance-fix/
â”œâ”€â”€ plan.md              # This file (/plan command output)
â”œâ”€â”€ research.md          # Phase 0 output (/plan command)
â”œâ”€â”€ data-model.md        # Phase 1 output (/plan command)
â”œâ”€â”€ quickstart.md        # Phase 1 output (/plan command)
â”œâ”€â”€ contracts/           # Phase 1 output (/plan command)
â”‚   â”œâ”€â”€ performance_contracts.yaml
â”‚   â”œâ”€â”€ data_integrity_contracts.yaml
â”‚   â””â”€â”€ monitoring_contracts.yaml
â””â”€â”€ tasks.md             # Phase 2 output (/tasks command - NOT created by /plan)
```

### Source Code (repository root)
```
iris_rag/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ unified_embedding_service.py    # âœ… EXISTS (needs integration)
â”‚   â”œâ”€â”€ entity_storage.py               # âš ï¸ NEEDS BATCH OPTIMIZATION
â”‚   â””â”€â”€ batch_entity_processor.py       # ğŸ†• NEW (batch processing logic)
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ manager.py                       # âš ï¸ NEEDS UNIFIED SERVICE INTEGRATION
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ pattern_extractor.py            # âš ï¸ NEEDS UNIFIED SERVICE INTEGRATION
â”œâ”€â”€ relationships/
â”‚   â””â”€â”€ relationship_processor.py       # âš ï¸ NEEDS UNIFIED SERVICE INTEGRATION
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ iris_connection_manager.py      # âš ï¸ NEEDS CONNECTION POOLING
â”‚   â””â”€â”€ performance_monitor.py          # ğŸ†• NEW (throughput tracking)
â””â”€â”€ validation/
    â””â”€â”€ data_integrity_validator.py     # ğŸ†• NEW (post-optimization validation)

tests/
â”œâ”€â”€ contract/
â”‚   â”œâ”€â”€ test_performance_contract.py    # ğŸ†• NEW (FR-001 to FR-004)
â”‚   â”œâ”€â”€ test_data_integrity_contract.py # ğŸ†• NEW (FR-005 to FR-007)
â”‚   â””â”€â”€ test_monitoring_contract.py     # ğŸ†• NEW (FR-008 to FR-011)
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_batch_entity_storage.py    # ğŸ†• NEW
â”‚   â”œâ”€â”€ test_unified_embedding_integration.py # ğŸ†• NEW
â”‚   â””â”€â”€ test_connection_pooling.py      # ğŸ†• NEW
â””â”€â”€ performance/
    â”œâ”€â”€ test_100_ticket_throughput.py   # ğŸ†• NEW
    â””â”€â”€ test_1000_ticket_sustained.py   # ğŸ†• NEW
```

**Structure Decision**: Single project enhancement to iris-vector-rag framework. New components follow existing framework patterns in `iris_rag/services/` and `iris_rag/common/`. Tests organized by contract/integration/performance tiers per constitutional requirements.

## Phase 0: Outline & Research

**Research Tasks** (all NEEDS CLARIFICATION items from Technical Context have been resolved via performance issue report):

1. **Embedding Batch Processing Best Practices**
   - Research: SentenceTransformer batch encoding patterns
   - Research: Optimal batch sizes for `all-MiniLM-L6-v2` model
   - Research: Memory usage patterns for batch operations

2. **IRIS Batch Operations**
   - Research: IRIS SQL batching patterns (executemany, prepared statements)
   - Research: Transaction boundaries for batch writes
   - Research: Connection pooling with InterSystems IRIS

3. **Performance Monitoring Patterns**
   - Research: Real-time throughput monitoring (non-blocking)
   - Research: Performance metric collection without overhead
   - Research: Alert thresholds for performance degradation

4. **Data Integrity Validation**
   - Research: Entity count validation strategies
   - Research: Relationship integrity checking patterns
   - Research: Content hash verification methods

**Output**: research.md consolidating findings with decisions, rationales, and alternatives considered.

## Phase 1: Design & Contracts

*Prerequisites: research.md complete*

### 1. Extract Entities â†’ data-model.md

**Key Entities** (from spec.md):

**Ticket**:
- Fields: ticket_id (string), content_text (string), processing_timestamp (datetime), status (enum: pending/processing/completed/failed)
- Relationships: HasMany Entity (8-12 per ticket)
- State transitions: pending â†’ processing â†’ completed/failed

**Entity**:
- Fields: entity_id (string), entity_text (string), entity_type (string), extraction_source_ticket_id (string), embedding_vector (float[])
- Relationships: BelongsTo Ticket, HasMany Relationship (4-6 relationships)
- Validation: entity_text must be non-empty, embedding_vector dimension matches model

**Relationship**:
- Fields: relationship_id (string), relationship_type (string), source_entity_id (string), target_entity_id (string), confidence_score (float 0-1)
- Relationships: References two Entity records
- Validation: confidence_score between 0.0 and 1.0, entity IDs must exist

**ProcessingMetrics**:
- Fields: metric_id (string), timestamp (datetime), ticket_id (string), extraction_time_ms (int), storage_time_ms (int), total_time_ms (int), success (boolean)
- Purpose: Real-time performance tracking
- Validation: All time fields â‰¥0, total_time = extraction_time + storage_time

### 2. Generate API Contracts â†’ /contracts/

**Performance Contracts** (from FR-001 to FR-004):
```yaml
# performance_contracts.yaml
contracts:
  - id: PC-001
    requirement: FR-001
    description: Individual ticket processing â‰¤15 seconds
    test: Process single ticket with 8-12 entities
    assertion: total_time_ms <= 15000

  - id: PC-002
    requirement: FR-002
    description: Throughput â‰¥240 tickets/hour
    test: Process 100 continuous tickets
    assertion: tickets_per_hour >= 240

  - id: PC-003
    requirement: FR-003
    description: Storage operations â‰¤10 seconds
    test: Measure storage time post-extraction
    assertion: storage_time_ms <= 10000

  - id: PC-004
    requirement: FR-004
    description: Complete dataset â‰¤17 hours
    test: Extrapolate from 1000-ticket run
    assertion: estimated_total_hours <= 17
```

**Data Integrity Contracts** (from FR-005 to FR-007):
```yaml
# data_integrity_contracts.yaml
contracts:
  - id: DIC-001
    requirement: FR-005
    description: 100% data integrity maintained
    test: Process 50 tickets, verify no entity loss
    assertion: extracted_entity_count == stored_entity_count

  - id: DIC-002
    requirement: FR-006
    description: Stored entities match extracted entities
    test: Compare entity content extracted vs stored
    assertion: entity_content_hash_extracted == entity_content_hash_stored

  - id: DIC-003
    requirement: FR-007
    description: All relationships preserved
    test: Verify relationship integrity after storage
    assertion: relationship_count_extracted == relationship_count_stored AND all_foreign_keys_valid
```

**Monitoring Contracts** (from FR-008 to FR-011):
```yaml
# monitoring_contracts.yaml
contracts:
  - id: MC-001
    requirement: FR-008
    description: Track processing time with millisecond precision
    test: Verify ProcessingMetrics records have ms precision
    assertion: metrics.timestamp_precision == 'millisecond'

  - id: MC-002
    requirement: FR-009
    description: Track throughput in real-time
    test: Query throughput metric during processing
    assertion: throughput_metric_exists AND throughput_updated_realtime

  - id: MC-003
    requirement: FR-010
    description: Alert when processing >20 seconds per ticket
    test: Simulate slow ticket, verify alert triggered
    assertion: alert_triggered_when(total_time_ms > 20000)

  - id: MC-004
    requirement: FR-011
    description: Log timing breakdowns
    test: Verify logs contain extraction_time and storage_time
    assertion: 'extraction_time' in log_entry AND 'storage_time' in log_entry
```

### 3. Generate Contract Tests

**test_performance_contract.py**:
- `test_pc001_single_ticket_15_seconds()` - MUST fail initially
- `test_pc002_throughput_240_per_hour()` - MUST fail initially
- `test_pc003_storage_10_seconds()` - MUST fail initially
- `test_pc004_dataset_17_hours()` - MUST fail initially

**test_data_integrity_contract.py**:
- `test_dic001_no_entity_loss()` - MUST pass (no data corruption)
- `test_dic002_entity_content_match()` - MUST pass (exact content)
- `test_dic003_relationship_integrity()` - MUST pass (all relationships preserved)

**test_monitoring_contract.py**:
- `test_mc001_millisecond_precision()` - MUST fail initially (not implemented)
- `test_mc002_realtime_throughput()` - MUST fail initially (not implemented)
- `test_mc003_slow_ticket_alert()` - MUST fail initially (not implemented)
- `test_mc004_timing_breakdowns()` - MUST fail initially (not implemented)

### 4. Extract Test Scenarios from User Stories

**Primary User Story**: Ticket ingestion system operator needs system to process tickets within expected timeframes (10-15 seconds each).

**Integration Test Scenarios**:

1. **Scenario: Process single ticket with typical entity count**
   - Given: Fresh IRIS database with no entities
   - When: Process one ticket with 10 entities and 5 relationships
   - Then: Total time â‰¤15 seconds AND all entities stored AND all relationships valid

2. **Scenario: Process batch of 100 tickets continuously**
   - Given: Fresh IRIS database
   - When: Process 100 tickets with 8-12 entities each
   - Then: Throughput â‰¥240/hour AND memory usage stable AND all data validated

3. **Scenario: Complete dataset processing within time limit**
   - Given: 10,150 tickets ready for processing
   - When: Process all tickets with optimized pipeline
   - Then: Total time â‰¤17 hours AND 100% data integrity validated

4. **Scenario: Extraction fast, storage optimized**
   - Given: Single ticket ready for processing
   - When: Extraction completes in 5-6 seconds
   - Then: Storage completes in â‰¤10 seconds (not 50-120 seconds)

### 5. Update Agent Context (CLAUDE.md)

Run `.specify/scripts/bash/update-agent-context.sh claude` to add:
- New technologies: Connection pooling patterns, batch entity processing
- Recent changes: GraphRAG storage optimization, performance monitoring
- Testing patterns: Performance contract tests with throughput validation

**Output**:
- data-model.md with Ticket, Entity, Relationship, ProcessingMetrics entities
- contracts/performance_contracts.yaml, data_integrity_contracts.yaml, monitoring_contracts.yaml
- Failing contract tests in tests/contract/
- quickstart.md with performance validation workflow
- CLAUDE.md updated with new context

## Phase 1 Constitution Re-Check

**Post-Design Constitution Review**:

**I. Framework-First Architecture**: âœ“ All components remain framework-level, no application logic
**II. Pipeline Validation & Requirements**: âœ“ Existing validation untouched
**III. Test-Driven Development**: âœ“ Contract tests written, will fail initially (TDD principle)
**IV. Performance & Enterprise Scale**: âœ“ Addresses 10K+ document performance (core goal)
**V. Production Readiness**: âœ“ Monitoring and logging enhanced
**VI. Explicit Error Handling**: âœ“ All storage errors will surface with context
**VII. Standardized Database Interfaces**: âœ“ Uses existing utilities, contributes batching patterns back

**Post-Design Constitution Check: PASS** âœ…

## Phase 2: Task Planning Approach
*This section describes what the /tasks command will do - DO NOT execute during /plan*

**Task Generation Strategy**:
1. Load `.specify/templates/tasks-template.md` as base structure
2. Generate tasks from Phase 1 contracts and data model:
   - Each performance contract â†’ performance contract test task [P]
   - Each data integrity contract â†’ data integrity contract test task [P]
   - Each monitoring contract â†’ monitoring contract test task [P]
   - Data model entities â†’ no new models (existing IRIS tables)
   - Unified embedding service â†’ integration task
   - Batch entity processor â†’ implementation task
   - Connection pooling â†’ implementation task
   - Performance monitoring â†’ implementation task
3. Order by TDD principles: Tests first, then implementation
4. Mark [P] for parallel-executable tasks (independent test files)

**Ordering Strategy**:
1. **Phase 0**: Research complete (already done via performance issue report)
2. **Phase 1**: Contract test creation (all parallel [P])
   - Write performance contract tests
   - Write data integrity contract tests
   - Write monitoring contract tests
3. **Phase 2**: Core optimization implementation
   - Task 1: Integrate UnifiedEmbeddingService into all services
   - Task 2: Implement batch_entity_processor with batching logic
   - Task 3: Add connection pooling to iris_connection_manager
   - Task 4: Implement performance_monitor for real-time tracking
4. **Phase 3**: Integration tests (parallel where possible)
   - Test unified embedding integration
   - Test batch entity storage
   - Test connection pooling
   - Test performance monitoring
5. **Phase 4**: Performance validation
   - Run 100-ticket throughput test
   - Run 1,000-ticket sustained load test
   - Validate all contract tests pass

**Estimated Output**: 18-22 numbered, dependency-ordered tasks in tasks.md

**Task Priorities**:
- P0: Contract test creation (gates implementation)
- P0: UnifiedEmbeddingService integration (12-30 sec savings per ticket)
- P0: Batch entity storage (30-64 sec savings per ticket)
- P1: Connection pooling (3-7 sec savings per ticket)
- P1: Performance monitoring (observability for validation)

**IMPORTANT**: This phase is executed by the /tasks command, NOT by /plan

## Phase 3+: Future Implementation
*These phases are beyond the scope of the /plan command*

**Phase 3**: Task execution (/tasks command creates tasks.md)
**Phase 4**: Implementation following constitutional TDD principles
**Phase 5**: Validation against all acceptance criteria from spec.md

**Validation Criteria**:
- All 14 functional requirements (FR-001 to FR-014) verified
- Performance targets achieved: 10-15s/ticket, 240-360/hour, 11-17 hours total
- Data integrity: 100% entity preservation, 100% relationship preservation
- Monitoring: Real-time throughput tracking, alert on >20s tickets
- Operational: Graceful shutdown, resume from last completed

## Complexity Tracking
*Fill ONLY if Constitution Check has violations that must be justified*

No constitutional violations. Feature aligns with all constitutional principles:
- Framework-first: Pure framework enhancement
- Performance focus: Directly addresses Principle IV (Enterprise Scale)
- TDD approach: Contract tests before implementation
- Production readiness: Enhances existing monitoring and logging

## Progress Tracking
*This checklist is updated during execution flow*

**Phase Status**:
- [x] Phase 0: Research complete (/plan command) âœ…
- [x] Phase 1: Design complete (/plan command) âœ…
- [x] Phase 2: Task planning complete (/plan command - describe approach only) âœ…
- [ ] Phase 3: Tasks generated (/tasks command)
- [ ] Phase 4: Implementation complete
- [ ] Phase 5: Validation passed

**Gate Status**:
- [x] Initial Constitution Check: PASS âœ…
- [x] Post-Design Constitution Check: PASS âœ…
- [x] All NEEDS CLARIFICATION resolved (via performance issue report)
- [x] Complexity deviations documented (none - all principles aligned)

**Artifacts Generated**:
- [x] research.md (Phase 0)
- [x] data-model.md (Phase 1)
- [x] contracts/performance_contracts.yaml (Phase 1)
- [x] contracts/data_integrity_contracts.yaml (Phase 1)
- [x] contracts/monitoring_contracts.yaml (Phase 1)
- [x] quickstart.md (Phase 1)
- [x] CLAUDE.md updated (Phase 1)

---
*Based on Constitution v1.8.0 - See `/.specify/memory/constitution.md`*
