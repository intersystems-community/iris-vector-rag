# RAG Templates - Standardized Build and Test System
# Provides consistent, config-driven testing and data management

.PHONY: help test test-unit test-integration test-pipelines test-schema
.PHONY: data-status data-scale data-populate data-clear
.PHONY: evaluate evaluate-ragas benchmark-performance
.PHONY: schema-validate schema-migrate schema-sync

# Configuration - Using uv for consistency
UV := uv
PYTHON := uv run python
PYTEST := uv run pytest
TEST_CONFIG := tests/conftest_standardized.py
SCHEMA_CONFIG := config/database_schema.yaml

help: ## Show this help message
	@echo "ðŸ”§ RAG Templates - Standardized Make Targets"
	@echo "=" * 50
	@awk 'BEGIN {FS = ":.*##"} /^[a-zA-Z_-]+:.*##/ {printf "  %-20s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

# ================================================
# TESTING TARGETS - Standardized and Consistent
# ================================================

test: ## Run all tests with standardized configuration
	$(PYTEST) -v --tb=short -c $(TEST_CONFIG)

test-unit: ## Run unit tests only
	$(PYTEST) -v -m "unit and not slow" --tb=short

test-integration: ## Run integration tests
	$(PYTEST) -v -m "integration" --tb=short

test-pipelines: ## Test all RAG pipelines
	$(PYTEST) -v -m "pipeline" --tb=short tests/test_pipelines/

test-schema: ## Test database schema consistency
	$(PYTEST) -v -m "schema" --tb=short tests/test_schema/

test-fast: ## Run fast tests only (excludes slow/integration)
	$(PYTEST) -v -m "not slow and not integration" --tb=short

test-coverage: ## Run tests with coverage report
	$(PYTEST) --cov=iris_rag --cov=common --cov-report=html --cov-report=term

# ================================================
# DATA MANAGEMENT - Config-driven
# ================================================

data-status: ## Check current data status across all tables
	$(PYTHON) scripts/check_data_status.py

data-scale: ## Scale up data to full dataset
	@echo "ðŸš€ Scaling up all pipeline data..."
	$(PYTHON) scripts/populate_document_chunks.py
	$(PYTHON) scripts/populate_graphrag_entities.py  
	$(PYTHON) scripts/populate_colbert_token_embeddings.py

data-populate: ## Populate missing data for all pipelines
	@echo "ðŸ“Š Populating missing pipeline data..."
	$(PYTHON) -c "from common.database_schema_manager import get_schema_manager; print('Using schema:', get_schema_manager().get_schema_name())"
	$(MAKE) data-scale

data-clear: ## Clear all RAG data (keep schema)
	$(PYTHON) -c "\
	from common.database_schema_manager import get_schema_manager; \
	from common.iris_connector import get_iris_connection; \
	schema = get_schema_manager(); \
	conn = get_iris_connection(); \
	cursor = conn.cursor(); \
	[cursor.execute(f'DELETE FROM {schema.get_table_name(table_key)}') or print(f'Cleared {schema.get_table_name(table_key)}') for table_key in schema.get_all_tables()]; \
	conn.commit(); \
	"

# ================================================
# EVALUATION - Standardized RAGAS and Benchmarks
# ================================================

evaluate: ## Run comprehensive evaluation on all pipelines
	$(PYTHON) scripts/utilities/run_unified_evaluation.py \
		--pipelines BasicRAG,HyDE,CRAG,GraphRAG,ColBERT,NodeRAG,HybridIFind \
		--iterations 10 --parallel --output-dir eval_results

evaluate-ragas: ## Run RAGAS evaluation specifically
	$(PYTHON) scripts/utilities/run_unified_evaluation.py \
		--pipelines BasicRAG,CRAG,GraphRAG \
		--iterations 5 --output-dir ragas_results

evaluate-fast: ## Quick evaluation for development
	$(PYTHON) scripts/utilities/run_unified_evaluation.py \
		--dev --iterations 3

benchmark-performance: ## Run performance benchmarks
	$(PYTHON) scripts/utilities/run_performance_benchmarks.py \
		--all-pipelines --output-dir benchmarks

# ================================================
# SCHEMA MANAGEMENT - Config-driven
# ================================================

schema-validate: ## Validate database schema against config
	$(PYTHON) -c "\
	from common.database_schema_manager import DatabaseSchemaManager; \
	manager = DatabaseSchemaManager(); \
	print('âœ… Schema configuration valid'); \
	print('Schema:', manager.get_schema_name()); \
	print('Tables:', list(manager.get_all_tables().keys())); \
	"

schema-info: ## Show current schema configuration
	$(PYTHON) -c "\
	from common.database_schema_manager import get_schema_manager; \
	schema = get_schema_manager(); \
	[print(f'{table_key}: {schema.get_table_info(table_key)[\"fully_qualified_name\"]}') or \
	 [print(f'  {col_key} â†’ {col_name}') for col_key, col_name in schema.get_table_info(table_key)['columns'].items()] \
	 for table_key in schema.get_all_tables()]; \
	"

schema-migrate: ## Run schema migrations (if needed)
	@echo "ðŸ”„ Running schema migrations..."
	$(PYTHON) scripts/schema_migration.py

# ================================================
# DEVELOPMENT HELPERS
# ================================================

lint: ## Run code linting
	ruff check iris_rag/ common/ scripts/ tests/
	black --check iris_rag/ common/ scripts/ tests/

format: ## Format code
	black iris_rag/ common/ scripts/ tests/
	isort iris_rag/ common/ scripts/ tests/

clean: ## Clean temporary files
	find . -type f -name "*.pyc" -delete
	find . -type d -name "__pycache__" -delete
	rm -rf .pytest_cache/ .coverage htmlcov/

# ================================================
# INTEGRATION WORKFLOWS
# ================================================

ci-test: ## Full CI test suite
	$(MAKE) schema-validate
	$(MAKE) test-fast
	$(MAKE) test-integration

dev-setup: ## Development environment setup
	$(MAKE) schema-validate
	$(MAKE) data-status
	@echo "ðŸŽ¯ Ready for development!"

full-pipeline: ## Complete pipeline from scratch
	@echo "ðŸš€ Running full RAG Templates pipeline..."
	$(MAKE) data-clear
	$(MAKE) data-populate
	$(MAKE) test-pipelines
	$(MAKE) evaluate-fast
	@echo "âœ… Full pipeline completed!"

# ================================================
# DOCUMENTATION
# ================================================

docs: ## Generate documentation
	@echo "ðŸ“š Documentation targets:"
	@echo "  - Schema config: $(SCHEMA_CONFIG)"
	@echo "  - Test config: $(TEST_CONFIG)"
	@echo "  - Available pipelines: BasicRAG, HyDE, CRAG, GraphRAG, ColBERT, NodeRAG, HybridIFind"