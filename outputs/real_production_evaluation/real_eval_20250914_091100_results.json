{
  "run_id": "real_eval_20250914_091100",
  "documents_processed": 5,
  "questions_evaluated": 20,
  "execution_time_seconds": 244.0235719680786,
  "execution_time_minutes": 4.06705953280131,
  "pipeline_results": {
    "BasicRAGPipeline": {
      "metrics": {
        "faithfulness": 0.005,
        "answer_relevancy": 0.0,
        "context_precision": 0.795,
        "context_recall": 0.75,
        "answer_similarity": 0.79,
        "answer_correctness": 0.826,
        "overall_score": 0.5276666666666666
      },
      "total_questions": 20
    },
    "CRAGPipeline": {
      "metrics": {
        "faithfulness": 0.0,
        "answer_relevancy": 0.0,
        "context_precision": 0.795,
        "context_recall": 0.75,
        "answer_similarity": 0.79,
        "answer_correctness": 0.826,
        "overall_score": 0.5268333333333334
      },
      "total_questions": 20
    },
    "GraphRAGPipeline": {
      "metrics": {
        "faithfulness": 0.0,
        "answer_relevancy": 0.0,
        "context_precision": 0.795,
        "context_recall": 0.75,
        "answer_similarity": 0.79,
        "answer_correctness": 0.826,
        "overall_score": 0.5268333333333334
      },
      "total_questions": 20
    },
    "BasicRAGRerankingPipeline": {
      "metrics": {
        "faithfulness": 0.0,
        "answer_relevancy": 0.0,
        "context_precision": 0.795,
        "context_recall": 0.75,
        "answer_similarity": 0.79,
        "answer_correctness": 0.826,
        "overall_score": 0.5268333333333334
      },
      "total_questions": 20
    }
  },
  "infrastructure": {
    "vector_database": "InterSystems IRIS",
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    "llm_model": "gpt-4o-mini",
    "evaluation_framework": "RAGAS with real LLM judges"
  }
}