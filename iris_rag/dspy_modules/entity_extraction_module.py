"""
DSPy-powered Entity Extraction for TrakCare Support Tickets.

This module provides optimized entity and relationship extraction using DSPy
with TrakCare-specific entity types and domain knowledge.
"""
import dspy
import logging
import json
from typing import List, Dict, Optional, Any

logger = logging.getLogger(__name__)


class EntityExtractionSignature(dspy.Signature):
    """
    Extract structured entities and relationships from TrakCare support tickets.

    Focuses on high-quality extraction with 4+ entities per ticket including:
    - Products (TrakCare, IRIS, Cache, HealthShare)
    - Users (role names, user types, access levels)
    - Modules (Appointment, Lab, Patient, Pharmacy, etc.)
    - Errors (error codes, error messages, exceptions)
    - Actions (login, access, configure, activate, etc.)
    """

    ticket_text = dspy.InputField(
        desc="TrakCare support ticket text (summary + description + resolution)"
    )
    entity_types = dspy.InputField(
        desc="Comma-separated list of entity types to extract: PRODUCT, USER, MODULE, ERROR, ACTION, ORGANIZATION, VERSION"
    )

    entities = dspy.OutputField(
        desc="""List of extracted entities as JSON array. Each entity MUST have:
- text: The exact entity text from ticket
- type: One of PRODUCT, USER, MODULE, ERROR, ACTION, ORGANIZATION, VERSION
- confidence: 0.0-1.0 confidence score

Example: [{"text": "TrakCare", "type": "PRODUCT", "confidence": 0.95}, {"text": "appointment module", "type": "MODULE", "confidence": 0.90}]

CRITICAL: Extract AT LEAST 3-5 entities per ticket. Look for products, modules, error messages, user roles, and actions."""
    )

    relationships = dspy.OutputField(
        desc="""List of relationships between entities as JSON array. Each relationship MUST have:
- source: Entity text (from entities list)
- target: Entity text (from entities list)
- type: Relationship type (uses, has_error, affects, configures, accesses, belongs_to)
- confidence: 0.0-1.0 confidence score

Example: [{"source": "user", "target": "TrakCare", "type": "accesses", "confidence": 0.90}]

CRITICAL: Extract AT LEAST 2-3 relationships per ticket showing how entities interact."""
    )


class TrakCareEntityExtractionModule(dspy.Module):
    """
    DSPy module for extracting entities and relationships from TrakCare tickets.

    Uses ChainOfThought reasoning to maximize entity extraction quality and
    ensure we get 4+ entities per ticket with proper relationships.
    """

    # TrakCare-specific entity types for domain-specific extraction
    TRAKCARE_ENTITY_TYPES = [
        "PRODUCT",        # TrakCare, IRIS, Cache, HealthShare, Ensemble
        "USER",           # user, admin, clinician, receptionist, nurse
        "MODULE",         # appointment, lab, patient, pharmacy, orders
        "ERROR",          # error code, exception, failure message
        "ACTION",         # login, access, configure, create, update, delete
        "ORGANIZATION",   # hospital name, department, facility
        "VERSION",        # software version numbers
    ]

    def __init__(self):
        super().__init__()
        self.extract = dspy.ChainOfThought(EntityExtractionSignature)
        logger.info("Initialized TrakCare Entity Extraction Module with DSPy Chain of Thought")

    def forward(self, ticket_text: str, entity_types: Optional[List[str]] = None) -> dspy.Prediction:
        """
        Extract entities and relationships from ticket text.

        Args:
            ticket_text: Support ticket content
            entity_types: Optional list of entity types to extract. Defaults to all TrakCare types.

        Returns:
            dspy.Prediction with 'entities' and 'relationships' fields
        """
        # Use provided entity types or default to TrakCare types
        if entity_types is None:
            entity_types = self.TRAKCARE_ENTITY_TYPES

        entity_types_str = ", ".join(entity_types)

        try:
            # Perform DSPy chain of thought extraction
            prediction = self.extract(
                ticket_text=ticket_text,
                entity_types=entity_types_str
            )

            # Parse JSON from DSPy output
            entities = self._parse_entities(prediction.entities)
            relationships = self._parse_relationships(prediction.relationships)

            # Validate extraction quality
            if len(entities) < 2:
                logger.warning(
                    f"Low entity count ({len(entities)}) - DSPy should extract 4+ entities. "
                    f"Consider retraining or adjusting prompt."
                )

            # Create validated prediction
            validated_prediction = dspy.Prediction(
                entities=json.dumps(entities),
                relationships=json.dumps(relationships),
                entity_count=len(entities),
                relationship_count=len(relationships)
            )

            logger.info(
                f"Extracted {len(entities)} entities and {len(relationships)} relationships via DSPy"
            )

            return validated_prediction

        except Exception as e:
            logger.error(f"DSPy entity extraction failed: {e}")
            # Return empty extraction on failure
            return dspy.Prediction(
                entities="[]",
                relationships="[]",
                entity_count=0,
                relationship_count=0,
                error=str(e)
            )

    def _parse_entities(self, entities_str: str) -> List[Dict[str, Any]]:
        """Parse entities from DSPy JSON output with validation."""
        try:
            # Try to parse as JSON
            entities = json.loads(entities_str)

            # Validate structure
            validated_entities = []
            for entity in entities:
                if not isinstance(entity, dict):
                    continue

                # Ensure required fields
                if "text" not in entity or "type" not in entity:
                    continue

                # Ensure confidence field
                if "confidence" not in entity:
                    entity["confidence"] = 0.8  # Default confidence

                # Validate entity type
                if entity["type"] not in self.TRAKCARE_ENTITY_TYPES:
                    logger.debug(f"Unknown entity type: {entity['type']}, keeping anyway")

                validated_entities.append(entity)

            return validated_entities

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse entities JSON: {e}. Raw output: {entities_str[:200]}")
            # Try to extract entities using regex as fallback
            return self._fallback_entity_extraction(entities_str)

    def _parse_relationships(self, relationships_str: str) -> List[Dict[str, Any]]:
        """Parse relationships from DSPy JSON output with validation."""
        try:
            # Try to parse as JSON
            relationships = json.loads(relationships_str)

            # Validate structure
            validated_relationships = []
            for rel in relationships:
                if not isinstance(rel, dict):
                    continue

                # Ensure required fields
                if "source" not in rel or "target" not in rel or "type" not in rel:
                    continue

                # Ensure confidence field
                if "confidence" not in rel:
                    rel["confidence"] = 0.7  # Default confidence for relationships

                validated_relationships.append(rel)

            return validated_relationships

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse relationships JSON: {e}. Raw output: {relationships_str[:200]}")
            return []  # No fallback for relationships - require proper JSON

    def _fallback_entity_extraction(self, text: str) -> List[Dict[str, Any]]:
        """
        Fallback entity extraction using regex patterns when DSPy JSON parsing fails.
        This should rarely be needed if DSPy is properly configured.
        """
        import re
        entities = []

        # Extract TrakCare product names
        products = re.findall(r'\b(TrakCare|IRIS|Cache|HealthShare|Ensemble)\b', text, re.IGNORECASE)
        for product in set(products):
            entities.append({
                "text": product,
                "type": "PRODUCT",
                "confidence": 0.9
            })

        # Extract common modules
        modules = re.findall(
            r'\b(appointment|lab|patient|pharmacy|orders|admission|discharge|clinical)\b\s*module',
            text,
            re.IGNORECASE
        )
        for module in set(modules):
            entities.append({
                "text": module,
                "type": "MODULE",
                "confidence": 0.8
            })

        # Extract error patterns
        errors = re.findall(r'error\s*[:\-]\s*([^.]+)', text, re.IGNORECASE)
        for error in errors[:3]:  # Limit to first 3 errors
            entities.append({
                "text": error.strip(),
                "type": "ERROR",
                "confidence": 0.7
            })

        logger.info(f"Fallback extraction produced {len(entities)} entities")
        return entities


def configure_dspy(llm_config: dict):
    """
    Configure DSPy to use any LLM provider (Ollama, OpenAI-compatible, etc.).

    Respects configuration flags like supports_response_format and use_json_mode
    to ensure compatibility with various LLM endpoints.

    Args:
        llm_config: LLM configuration dict containing model, api_base, api_type, etc.
    """
    try:
        import dspy

        model = llm_config.get("model", "qwen2.5:7b")
        api_base = llm_config.get("api_base", "http://localhost:11434")
        api_type = llm_config.get("api_type", "ollama")
        max_tokens = llm_config.get("max_tokens", 2000)
        temperature = llm_config.get("temperature", 0.1)

        # Check if endpoint supports response_format (for JSON mode)
        supports_response_format = llm_config.get("supports_response_format", True)
        use_json_mode = llm_config.get("use_json_mode", True)

        # Configure based on API type
        if api_type == "openai" or model.startswith("openai/"):
            # OpenAI-compatible endpoint (like GPT-OSS)
            lm = dspy.LM(
                model=model,
                api_base=api_base,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            # Disable JSON mode if endpoint doesn't support response_format
            if not supports_response_format or not use_json_mode:
                logger.warning(
                    f"Model {model} does not support response_format parameter - "
                    "DSPy may fall back to text parsing"
                )

            logger.info(f"✅ DSPy configured with OpenAI-compatible model: {model}")

        else:
            # Ollama or other provider
            try:
                # Modern DSPy 2.5+ API with ollama/ prefix
                lm = dspy.LM(
                    model=f"ollama/{model}",
                    api_base=api_base,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                logger.info(f"✅ DSPy configured with Ollama model: {model}")
            except Exception as e:
                logger.warning(f"dspy.LM failed: {e}, trying fallback...")
                # Fallback: try direct Ollama integration
                from dspy import OLlama
                lm = OLlama(
                    model=model,
                    base_url=api_base,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                logger.info(f"✅ DSPy configured with Ollama model: {model} (fallback)")

        dspy.configure(lm=lm)

    except Exception as e:
        logger.error(f"Failed to configure DSPy: {e}")
        raise


def configure_dspy_for_ollama(model_name: str = "qwen2.5:7b", base_url: str = "http://localhost:11434"):
    """
    Configure DSPy to use Ollama for LLM inference (legacy function).

    Deprecated: Use configure_dspy() with llm_config dict instead.

    Args:
        model_name: Ollama model name (default: qwen2.5:7b - fast and accurate)
        base_url: Ollama API base URL
    """
    # Call the new generic function
    llm_config = {
        "model": model_name,
        "api_base": base_url,
        "api_type": "ollama",
        "max_tokens": 2000,
        "temperature": 0.1
    }
    configure_dspy(llm_config)
