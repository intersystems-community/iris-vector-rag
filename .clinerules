# Cline Project Rules

This file defines key rules and standards for the RAG templates project.

## General

1. **ALWAYS USE UV**: All Python commands (pytest, pip, python, etc.) MUST be prefixed with `uv run`. Examples:
   - `uv run pytest tests/` (NOT `python -m pytest tests/`)
   - `uv run python script.py` (NOT `python script.py`)
   - This is MANDATORY for all Python execution in this project.

2. **IRIS Edition Requirements**: For large-scale RAG testing, IRIS Enterprise edition is required due to data size constraints:
   - **Community Edition**: 10GB data limit (insufficient for 92K+ documents)
   - **Enterprise Edition**: No data limits (required for large-scale testing)
   - **Default for Testing**: Use Enterprise/ML edition by default
   - **Environment Variable**: Set `IRIS_DOCKER_IMAGE` to override edition selection

3. **ALWAYS KEEP CODE FILES UNDER 500 LINES**

## TDD Workflow

1. **Test-First Development**: Always write failing tests first, then implement the code to make them pass.

2. **Red-Green-Refactor**: Follow the TDD cycle:
   - Red: Write a failing test
   - Green: Implement the minimum code to make the test pass
   - Refactor: Clean up the code while keeping tests passing

3. **Test Isolation**: Each test should be independent and not rely on the state created by other tests.

4. **Incremental Implementation**: Fix one failing test at a time, commit working code frequently.

5. **Bottom-Up Testing**: Start with testing smaller components, then integrate them into larger systems.

## Testing Rules

1. **Always Use pytest**: Tests must be proper pytest tests, not shell scripts. Pure Python is preferred for testing automation.

2. **Real End-to-End Tests**: Tests must verify that all RAG techniques actually work with real data. No skipping tests.

3. **Real Data Required**: Tests must use real PMC documents, not synthetic data. At least 1000 documents should be used.
   - Implementation: A custom pytest fixture in `tests/conftest_1000docs.py` ensures 1000+ documents
   - Usage: `make test-1000` to run tests with 1000+ documents
   - Direct test: `tests/test_all_with_1000_docs.py` tests all RAG techniques with 1000+ docs

4. **Complete Pipeline Testing**: Tests should exercise the full pipeline, from data ingestion to answer generation.

5. **Assert Actual Results**: Tests must make assertions on actual result properties (not just logging).

6. **Log Pytest Output to File**: All pytest commands MUST use `tee` to display output in terminal AND log to file in the `test_output` directory. The filename should be descriptive of the test being run. Examples:
   - `uv run pytest tests/test_ingestion.py | tee test_output/test_ingestion.log`
   - `uv run pytest tests/test_fallback_behavior_validation.py -v | tee test_output/test_fallback_behavior_validation.log`
   - This allows real-time viewing of test output while preserving logs for analysis

## Code Organization Rules

1. **Pythonic Approach**: Prefer Python functionality over shell scripts for automation.

2. **Reuse Fixtures**: Leverage existing pytest fixtures in conftest.py whenever possible.

3. **Clear Module Structure**: Every module should have a clear docstring and logical organization.

4. **Parameter Naming**: Be consistent with parameter names across different RAG techniques:
   - `iris_connector` (not `connection`)
   - `embedding_func` (not `embed_func`)
   - `llm_func` (consistent across all)

5. **Standard Return Format**: All pipelines should return a dictionary with at least:
   - `"query"`: The original query
   - `"answer"`: The generated answer
   - `"retrieved_documents"`: The documents/nodes used

## SQL Rules

1. **Use TOP instead of LIMIT**: IRIS SQL uses TOP keyword instead of LIMIT. Always use "SELECT TOP n" syntax rather than "SELECT ... LIMIT n".
## Vector Store Architecture Rules

1. **Always Use Vector Store Interface**: All vector operations (search, insertion, retrieval) MUST use the `IVectorStore` interface and its `IRISVectorStore` implementation instead of direct SQL queries. This ensures consistent abstraction and proper encapsulation of vector operations.

2. **No Direct Vector SQL in Tests or Application Code**: Never write direct SQL queries with `VECTOR_COSINE`, `TO_VECTOR`, or other vector functions. Always delegate to the vector store interface methods like `similarity_search()`, `add_documents()`, etc.

3. **Vector Store Usage Pattern**: Initialize and use vector store through the proper interface:
   ```python
   from iris_rag.storage.vector_store.iris_impl import IRISVectorStore
   from iris_rag.config.manager import ConfigurationManager
   
   config_manager = ConfigurationManager()
   vector_store = IRISVectorStore(config_manager=config_manager)
   
   # Use interface methods instead of direct SQL
   results = vector_store.similarity_search(query_embedding, k=10)
   ```

4. **Benchmarking and Performance Tests**: Even performance and benchmarking tests must use the vector store interface to ensure they test the actual production code paths and maintain architectural consistency.


## Vector Insertion Rules

1. **Always Use insert_vector Utility**: All vector insertions into IRIS database tables MUST use the `common.db_vector_utils.insert_vector()` utility function for consistency and proper handling of vector formatting, dimension truncation/padding, and TO_VECTOR() syntax.

2. **No Direct Vector SQL**: Never write direct INSERT statements with vector data. Always delegate to the `insert_vector()` utility function to ensure consistent vector handling across the entire codebase.

3. **Consistent Vector Format**: The `insert_vector()` utility handles all vector formatting requirements including proper TO_VECTOR() syntax, dimension validation, and error handling.

## Database Schema Management Rules

1. **Always Use SchemaManager**: For database schema initialization and table creation, MUST use the `iris_rag.storage.schema_manager.SchemaManager` class instead of manual SQL scripts or direct table creation.

2. **Schema Manager Usage Pattern**: Initialize schema manager with connection_manager and config_manager, then use `ensure_table_schema(table_name)` method:
   ```python
   from iris_rag.storage.schema_manager import SchemaManager
   from common.iris_connection_manager import get_iris_connection
   from iris_rag.config.manager import ConfigurationManager
   
   # Initialize components
   connection_manager = type('ConnectionManager', (), {
       'get_connection': lambda: get_iris_connection()
   })()
   config_manager = ConfigurationManager()
   
   # Create and use schema manager
   schema_manager = SchemaManager(connection_manager, config_manager)
   schema_manager.ensure_table_schema('SourceDocuments')
   ```

3. **Automatic Migration Support**: The SchemaManager automatically handles vector dimension changes, model updates, and schema versioning without manual intervention.

4. **No Manual Table Creation**: Never create tables manually with CREATE TABLE statements. Always delegate to SchemaManager for consistent schema management across all RAG techniques.

## Performance & Benchmarking Rules

1. **Scale Testing**: Tests should be adaptable to run with 1000 documents by default, but scalable to 92,000+ documents.

2. **Performance Metrics**: Record execution times and resource usage for large-scale tests.

3. **Comparative Benchmarking**: All RAG techniques must be benchmarked against each other as defined in [IMPLEMENTATION_PLAN.md section 4](IMPLEMENTATION_PLAN.md#4-performance-benchmarks--comparative-analysis).

4. **Benchmarking Test Coverage**: Follow TDD workflow for benchmark implementation according to section 4.1 of IMPLEMENTATION_PLAN.md.

5. **Metric Requirements**:
   - All techniques must implement the metrics defined in section 4.3 of IMPLEMENTATION_PLAN.md
   - Results must compare techniques across retrieval quality, answer quality, and performance

6. **Real Data Requirement**: All benchmarks must be run against real PMC data stored in IRIS, never against mock data. At least 1000 documents must be used for meaningful comparisons.

7. **Published Benchmark Comparison**: Benchmark results must be compared against published benchmarks for similar techniques to validate implementation quality.

8. **Standardized Benchmark Process**: Follow the process outlined in [BENCHMARK_EXECUTION_PLAN.md](BENCHMARK_EXECUTION_PLAN.md) for all benchmark runs.

9. **Visualization Requirements**: All benchmark results must include:
   - Radar charts comparing all techniques across metrics
   - Per-metric bar charts
   - Comparison charts against published benchmarks

10. **Benchmarking Documentation**: Results from benchmark runs must be preserved and documented, including:
    - Raw results (JSON format)
    - Analysis report (Markdown format)
    - Visualizations (PNG format)
    - Comparison with previous benchmark runs

## Documentation Rules

1. **Technique Documentation**: Each RAG technique should have corresponding implementation documentation (e.g., COLBERT_IMPLEMENTATION.md).

2. **Test Documentation**: Test strategy should be documented, explaining how to run tests at various scales.
