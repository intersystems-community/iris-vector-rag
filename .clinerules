# Cline Project Rules

This file defines key rules and standards for the RAG templates project.

## TDD Workflow

1. **Test-First Development**: Always write failing tests first, then implement the code to make them pass.

2. **Red-Green-Refactor**: Follow the TDD cycle:
   - Red: Write a failing test
   - Green: Implement the minimum code to make the test pass
   - Refactor: Clean up the code while keeping tests passing

3. **Test Isolation**: Each test should be independent and not rely on the state created by other tests.

4. **Incremental Implementation**: Fix one failing test at a time, commit working code frequently.

5. **Bottom-Up Testing**: Start with testing smaller components, then integrate them into larger systems.

## Testing Rules

1. **Always Use pytest**: Tests must be proper pytest tests, not shell scripts. Pure Python is preferred for testing automation.

2. **Real End-to-End Tests**: Tests must verify that all RAG techniques actually work with real data. No skipping tests.

3. **Real Data Required**: Tests must use real PMC documents, not synthetic data. At least 1000 documents should be used.
   - Implementation: A custom pytest fixture in `tests/conftest_1000docs.py` ensures 1000+ documents
   - Usage: `make test-1000` to run tests with 1000+ documents
   - Direct test: `tests/test_all_with_1000_docs.py` tests all RAG techniques with 1000+ docs

4. **Complete Pipeline Testing**: Tests should exercise the full pipeline, from data ingestion to answer generation.

5. **Assert Actual Results**: Tests must make assertions on actual result properties (not just logging).

## Code Organization Rules

1. **Pythonic Approach**: Prefer Python functionality over shell scripts for automation.

2. **Reuse Fixtures**: Leverage existing pytest fixtures in conftest.py whenever possible.

3. **Clear Module Structure**: Every module should have a clear docstring and logical organization.

4. **Parameter Naming**: Be consistent with parameter names across different RAG techniques:
   - `iris_connector` (not `connection`)
   - `embedding_func` (not `embed_func`)
   - `llm_func` (consistent across all)

5. **Standard Return Format**: All pipelines should return a dictionary with at least:
   - `"query"`: The original query
   - `"answer"`: The generated answer
   - `"retrieved_documents"`: The documents/nodes used

## SQL Rules

1. **Use TOP instead of LIMIT**: IRIS SQL uses TOP keyword instead of LIMIT. Always use "SELECT TOP n" syntax rather than "SELECT ... LIMIT n".

## Vector Insertion Rules

1. **Always Use insert_vector Utility**: All vector insertions into IRIS database tables MUST use the `common.db_vector_utils.insert_vector()` utility function for consistency and proper handling of vector formatting, dimension truncation/padding, and TO_VECTOR() syntax.

2. **No Direct Vector SQL**: Never write direct INSERT statements with vector data. Always delegate to the `insert_vector()` utility function to ensure consistent vector handling across the entire codebase.

3. **Consistent Vector Format**: The `insert_vector()` utility handles all vector formatting requirements including proper TO_VECTOR() syntax, dimension validation, and error handling.

## Performance & Benchmarking Rules

1. **Scale Testing**: Tests should be adaptable to run with 1000 documents by default, but scalable to 92,000+ documents.

2. **Performance Metrics**: Record execution times and resource usage for large-scale tests.

3. **Comparative Benchmarking**: All RAG techniques must be benchmarked against each other as defined in [IMPLEMENTATION_PLAN.md section 4](IMPLEMENTATION_PLAN.md#4-performance-benchmarks--comparative-analysis).

4. **Benchmarking Test Coverage**: Follow TDD workflow for benchmark implementation according to section 4.1 of IMPLEMENTATION_PLAN.md.

5. **Metric Requirements**:
   - All techniques must implement the metrics defined in section 4.3 of IMPLEMENTATION_PLAN.md
   - Results must compare techniques across retrieval quality, answer quality, and performance

6. **Real Data Requirement**: All benchmarks must be run against real PMC data stored in IRIS, never against mock data. At least 1000 documents must be used for meaningful comparisons.

7. **Published Benchmark Comparison**: Benchmark results must be compared against published benchmarks for similar techniques to validate implementation quality.

8. **Standardized Benchmark Process**: Follow the process outlined in [BENCHMARK_EXECUTION_PLAN.md](BENCHMARK_EXECUTION_PLAN.md) for all benchmark runs.

9. **Visualization Requirements**: All benchmark results must include:
   - Radar charts comparing all techniques across metrics
   - Per-metric bar charts
   - Comparison charts against published benchmarks

10. **Benchmarking Documentation**: Results from benchmark runs must be preserved and documented, including:
    - Raw results (JSON format)
    - Analysis report (Markdown format)
    - Visualizations (PNG format)
    - Comparison with previous benchmark runs

## Documentation Rules

1. **Technique Documentation**: Each RAG technique should have corresponding implementation documentation (e.g., COLBERT_IMPLEMENTATION.md).

2. **Test Documentation**: Test strategy should be documented, explaining how to run tests at various scales.
