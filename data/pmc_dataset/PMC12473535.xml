<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473535</article-id><article-id pub-id-type="pmcid-ver">PMC12473535.1</article-id><article-id pub-id-type="pmcaid">12473535</article-id><article-id pub-id-type="pmcaiid">12473535</article-id><article-id pub-id-type="pmid">41013067</article-id><article-id pub-id-type="doi">10.3390/s25185830</article-id><article-id pub-id-type="publisher-id">sensors-25-05830</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Dual-Segmentation Framework for the Automatic Detection and Size Estimation of Shrimp</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-8138-2978</contrib-id><name name-style="western"><surname>Waqar</surname><given-names initials="MM">Malik Muhammad</given-names></name><xref rid="af1-sensors-25-05830" ref-type="aff">1</xref><xref rid="af2-sensors-25-05830" ref-type="aff">2</xref><xref rid="c1-sensors-25-05830" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-5149-2367</contrib-id><name name-style="western"><surname>Ali</surname><given-names initials="H">Hassan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05830" ref-type="aff">1</xref><xref rid="af2-sensors-25-05830" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2125-6908</contrib-id><name name-style="western"><surname>Zhou</surname><given-names initials="H">Heng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05830" ref-type="aff">1</xref><xref rid="af2-sensors-25-05830" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0443-1049</contrib-id><name name-style="western"><surname>Mohamed</surname><given-names initials="HG">Heba G.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af3-sensors-25-05830" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5647-629X</contrib-id><name name-style="western"><surname>Kim</surname><given-names initials="SC">Sang Cheol</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af2-sensors-25-05830" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9102-4929</contrib-id><name name-style="western"><surname>Strzelecki</surname><given-names initials="M">Michal</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af4-sensors-25-05830" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Zamora Izquierdo</surname><given-names initials="MA">Miguel A.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05830"><label>1</label>Division of Electronics and Information Engineering, Jeonbuk National University, Jeonju 54896, Republic of Korea; <email>202350496@jbnu.ac.kr</email> (H.A.); <email>hengz@jbnu.ac.kr</email> (H.Z.)</aff><aff id="af2-sensors-25-05830"><label>2</label>Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju 54896, Republic of Korea; <email>sckim7777@jbnu.ac.kr</email></aff><aff id="af3-sensors-25-05830"><label>3</label>Department of Electrical Engineering, College of Engineering, Princess Nourah Bint Abdulrahman University, P.O. Box 84428, Riyadh 11671, Saudi Arabia; <email>hegmohamed@pnu.edu.sa</email></aff><aff id="af4-sensors-25-05830"><label>4</label>Institute of Electronics, Lodz University of Technology, 93-590 Lodz, Poland; <email>michal.strzelecki@p.lodz.pl</email></aff><author-notes><corresp id="c1-sensors-25-05830"><label>*</label>Correspondence: <email>malikwaqarhaider@jbnu.ac.kr</email>; Tel.: +82-010-4636-0514</corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5830</elocation-id><history><date date-type="received"><day>18</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>12</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>15</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>18</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05830.pdf"/><abstract><p>In shrimp farming, determining the physical traits of shrimp is vital for assessing their health and growth. One of the critical traits is their size, as it serves as a key indicator of growth rates, biomass, and effective feed management. However, the accurate measurement of shrimp size is challenged by factors such as their naturally curved body posture, frequent overlapping among individuals, and their tendency to blend with the background, all of which hinder precise size estimation. Traditional methods for measuring the size of shrimp involve manual sampling, which is labor-intensive and time consuming. In contrast, image processing and classical computer vision techniques provide some reasonable results but often suffer from inaccuracies, making them unsuitable for large-scale monitoring. To address this problem, this paper proposes a dual-segmentation deep learning-based framework for accurately estimating shrimp size. It integrates instance segmentation using the RTMDet-m model with an enhanced semantic segmentation model to effectively predict the centerline of the shrimp&#8217;s body, enabling precise size measurements. The first stage employs the RTMDet-m model for the instance segmentation of shrimp, achieving an average precision (<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) of 96% with fewer parameters and the highest frames per second (FPS) count among state-of-the-art models. The second stage utilizes our custom segmentation model for centerline predictive module, attaining the highest FPS and F1-score of 88.3%. The proposed framework achieves the lowest mean absolute error of 1.02 cm and a root mean square error of 1.27 cm in shrimp size estimation compared to the baseline methods discussed in comparative study sections. Our proposed dual-segmentation framework outperforms both traditional and deep learning based methods used for measuring shrimp size.</p></abstract><kwd-group><kwd>computer vision</kwd><kwd>deep learning</kwd><kwd>instance segmentation</kwd><kwd>semantic segmentation</kwd><kwd>shrimp centerline extraction</kwd><kwd>aquaculture</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05830"><title>1. Introduction</title><p>Shrimp farming plays an important role in global seafood production, and farmed shrimp represents 63% of total shrimp production in 2022 [<xref rid="B1-sensors-25-05830" ref-type="bibr">1</xref>]. The marketplace is expanding due to the increase in the number of shrimp-based products. In order to increase shrimp production, careful feeding strategies, environment management, and monitoring their health [<xref rid="B2-sensors-25-05830" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05830" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05830" ref-type="bibr">4</xref>] are required. To gain insights into these factors, it is important to measure the physical traits of the shrimp. In shrimp farming, accurately estimating shrimp size is vital for assessing overall health, monitoring growth patterns, grading, and estimating biomass [<xref rid="B4-sensors-25-05830" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05830" ref-type="bibr">5</xref>]. Measuring the size of shrimp is crucial for feed optimization, making harvesting decisions, and ensuring sustainable farm management. A significant amount of research has focused on size estimation in aquaculture; however, most studies have focused on fish rather than shrimp. Additionally, the proposed methods are influenced by the fish&#8217;s pose and orientation, and they require a specialized control environment for accurate size measurement. The following paragraph provides a detailed discussion of existing methods employed for size estimation in both fish and shrimp along with their limitations.</p><p>The automated size measurement methods are crucial for managing aquaculture species, as they offer a more efficient and accurate alternative to traditional manual methods [<xref rid="B6-sensors-25-05830" ref-type="bibr">6</xref>]. The recent advancements in computer vision and image processing have greatly improved automation in aquaculture by tackling various challenges related to counting, detection, segmentation, and size measurement [<xref rid="B7-sensors-25-05830" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05830" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05830" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05830" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05830" ref-type="bibr">11</xref>]. In earlier research, Hsieh et al. [<xref rid="B12-sensors-25-05830" ref-type="bibr">12</xref>] proposed an image-based method for estimating the snout-to-fork length (SNFL) of tuna on fishing vessels using handheld cameras and Hough transform techniques. This method reduced manual handling but required operator training for proper camera positioning and was sensitive to the angle of image capture. Miranda et al. [<xref rid="B13-sensors-25-05830" ref-type="bibr">13</xref>] developed a polynomial curve-fitting method (PCFM) to measure the size of rainbow trout. Fish swim through a channel, and a camera captures images to create a third-order polynomial curve that predicts size. However, this method requires the fish to be in a tube, which is not ideal for aquaculture settings. It also necessitates manual selection of the polynomial degree, which may not generalize well due to variations in fish orientation and posture. These methods are limited in their ability to handle curved body postures and varying shapes, whereas deep learning approaches can robustly capture complex body shapes and features, thereby providing more accurate and reliable measurements.</p><p>Tseng et al. [<xref rid="B14-sensors-25-05830" ref-type="bibr">14</xref>] developed a patch-wise CNN classifier for identifying fish body parts to estimate size by projecting these regions onto the original image using a pixel-to-distance ratio. The method performed well in controlled settings but struggles with multiple fish and real-time processing due to its computationally intensive sliding-window approach. In a recent study, Wang et al. [<xref rid="B3-sensors-25-05830" ref-type="bibr">3</xref>] present a method to automatically measure shrimp size. It uses a U-shaped fully convolutional network along with second-order anisotropic Gaussian kernels. This method relies on silhouette segmentation results of shrimp; however, it is not effective in scenarios with dense shrimp due to the weak segmentation model. Zhao et al. [<xref rid="B15-sensors-25-05830" ref-type="bibr">15</xref>] proposed a heat-to-tail matching algorithm for identifying fish heads and tails, which helps with differentiating them and predicting their size. However, distinguishing contours can be difficult in dense clusters, and the assumption that each fish has both head and tail information may not always be valid. Zhou et al. [<xref rid="B16-sensors-25-05830" ref-type="bibr">16</xref>] present a binocular stereo vision method to measure fish size by extracting a center line. They use the GrabCut algorithm for segmentation, but its inaccuracies and the deburring process can affect measurement precision. Rodrigo et al. [<xref rid="B17-sensors-25-05830" ref-type="bibr">17</xref>] recently proposed a Voronoi diagram-based method (VDBM), which is an automated method for 3D centerline extraction in coronary arteries using the Voronoi diagram. This method is effective in extracting the centerline. However, it requires post-processing to eliminate extraneous branches, which increases overhead and reduces throughput, impacting real-time usability.</p><p>To address the limitations of previous studies, this work proposes a dual-segmentation deep learning framework for accurate shrimp size measurement. Unlike traditional approaches, the proposed framework does not require manual handling, specialized settings, or predefined assumptions. It is capable of delivering reliable predictions even in the presence of multiple shrimp instances or overlapping conditions. Furthermore, the method requires only minimal post-processing, which is achieved through the skeletonization algorithm proposed by Zhang et al. [<xref rid="B18-sensors-25-05830" ref-type="bibr">18</xref>]. Our framework consists two segmentation models along with a skeletonization operator to achieve impressive results. The first segmentation model employs the RTMDet-m model for instance segmentation, allowing us to isolate individual shrimp. The second model is our custom segmentation model designed to perform precise centerline extraction based on the outputs from the instance segmentation model. Our proposed framework outperforms earlier methods for measuring shrimp size in terms of error metric and processing time. The main contributions of this study are summarized as follows:<list list-type="order"><list-item><p>This study introduces a novel framework that integrates instance and semantic segmentation to efficiently extract the centerline of individual shrimp without requiring any post-processing step. Additionally, our proposed framework is robust to variations in the pose or orientation of the shrimp.</p></list-item><list-item><p>An enhancement to the baseline semantic segmentation model was proposed to improve the accuracy of shrimp centerline prediction. Our proposed model outperforms other baseline approaches significantly and prevents the creation of burrs along the centerline.</p></list-item><list-item><p>Furthermore, our study presents two comprehensive datasets designed to advance research in shrimp instance segmentation and size estimation. The dataset comprises high-quality images collected from three diverse environments with varying numbers of shrimp.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05830"><title>2. Materials and Methods</title><p>This section provides a comprehensive overview of the experimental setup, dataset, proposed framework, and its components to estimate shrimp size. <xref rid="sec2dot1-sensors-25-05830" ref-type="sec">Section 2.1</xref> outlines the experimental setup used to conduct the experiments. <xref rid="sec2dot2-sensors-25-05830" ref-type="sec">Section 2.2</xref> describes the two custom datasets utilized in this study. <xref rid="sec2dot3-sensors-25-05830" ref-type="sec">Section 2.3</xref> presents an overall overview of the proposed framework for estimating shrimp size. <xref rid="sec2dot4-sensors-25-05830" ref-type="sec">Section 2.4</xref> details our first model within the framework, specifically the instance segmentation model. <xref rid="sec2dot5-sensors-25-05830" ref-type="sec">Section 2.5</xref> provides information about our custom segmentation model in the centerline predictive module and the skeletonization operator used to derive a one-pixel-wide centerline representation.</p><sec id="sec2dot1-sensors-25-05830"><title>2.1. Experimental Setup</title><p>The experimental setup for estimating shrimp size consists of shrimp, a rectangular container, a camera, and a laptop. The shrimp used to create the dataset, which ranged from 4 cm to 11 cm in length, were collected from a farm and placed in a container. The container has internal dimensions of 50 cm (length) &#215; 35 cm (width) &#215; 25 cm (depth) and was filled with water up to a depth of 3&#8211;5 cm. This preserves the natural posture and mobility of the shrimp while keeping them within the imaging setup. The video data were recorded under controlled laboratory conditions using an Intel RealSense Depth Camera D435 (Intel Corporation, Santa Clara, CA, USA), which operated in RGB mode at 30 frames per second (FPS) with a resolution of 848 &#215; 480 pixels. The camera was mounted centrally on the container lid at a fixed vertical distance of 25 cm above the water surface to ensure consistent coverage. To capture variations in visual conditions, three experimental environments were prepared by varying the shrimp density. Subsequently, the recorded video streams were used to extract the image frames from them uniformly. The details on the construction of the dataset from these recordings are provided in the following section.</p></sec><sec id="sec2dot2-sensors-25-05830"><title>2.2. Dataset</title><p>This study employs two specialized shrimp datasets, which were developed to enable the experimentation and validation of our framework for estimating shrimp size in real time. The first dataset is an instance segmentation dataset that comprises 1000 high-quality images collected from 3 different environmental video streams, each with varying shrimp densities. Environments 1, 2, and 3 differed in shrimp counts, with the numbers ranging between 10 and 18. The first environment contained 10 shrimps, the second environment contained 15, and the third environment contained 18. Due to the live movement of shrimps, the dataset comprises diverse images, including nonoccluded and occluded cases. Each image was precisely annotated, and instance segmentation masks were generated for each individual shrimp. The visualization of the instance segmentation dataset from three distinct environments is provided in <xref rid="sensors-25-05830-f001" ref-type="fig">Figure 1</xref>.</p><p>The second dataset, known as the mask-to-centerline dataset, was constructed to aid in semantic segmentation for the extraction of the shrimp centerline. To speed up the labeling of the shrimp mask and create the mask-to-centerline dataset, a better strategy was adopted. An instance segmentation model was first trained on a small portion of the dataset with reasonable accuracy. The trained model was then utilized to obtain mask predictions, which were carefully filtered to remove any low-quality masks. This process guarantees high-quality data for training our custom segmentation model for the centerline prediction of shrimp. These data contain approximately 2400 shrimp mask instances, and each mask was manually labeled with a corresponding centerline. <xref rid="sensors-25-05830-f002" ref-type="fig">Figure 2</xref> provides a visualization of mask samples and their ground truth centerline labels from our second dataset.</p><p>The total images in both datasets were divided into three parts: 80% for training, 10% for validation, and 10% for testing. The open-source Computer Vision Annotation Tool (CVAT, version 2.30.0) was used to label the images. Both of our datasets are instrumental in developing models that can accurately segment and estimate the size of shrimp. The following section provides a detailed description of the proposed framework and the models used to develop the shrimp size estimation system.</p></sec><sec id="sec2dot3-sensors-25-05830"><title>2.3. Proposed Framework Overview</title><p>Our proposed dual-segmentation framework for estimating shrimp size consists of two sequential modules: the instance segmentation module and the centerline predictive module. The instance segmentation module utilizes an instance segmentation model to predict the mask of individual shrimp. In contrast, the centerline prediction module uses our custom segmentation model to predict the centerline of each shrimp. Each of these models is trained independently and operates sequentially during inference. During inference, the original RGB image is passed to the instance segmentation model to isolate individual shrimp precisely. The masks obtained from the instance segmentation are concatenated with the RGB image to form a four-dimensional feature map. This feature map is then reduced to a three-dimensional feature map using a 1 &#215; 1 convolution filter. The resultant feature would serve as input to our second custom segmentation model that provides precise centerline predictions for each shrimp. The extracted centerline from the second module is passed to a morphological operator to obtain a single-pixel line. Our framework is optimized to ensure fast and accurate processing. An overview of the proposed framework is illustrated in <xref rid="sensors-25-05830-f003" ref-type="fig">Figure 3</xref>.</p><p>The working principal of the instance segmentation model and our proposed segmentation model is discussed in the following <xref rid="sec2dot4-sensors-25-05830" ref-type="sec">Section 2.4</xref> and <xref rid="sec2dot5-sensors-25-05830" ref-type="sec">Section 2.5</xref>, respectively.</p></sec><sec id="sec2dot4-sensors-25-05830"><title>2.4. Instance Segmentation of Shrimp</title><p>The first stage of our dual-segmentation framework, which measures the size of the shrimp, is the instance segmentation stage. This stage is essential for detecting and isolating individual shrimp from images, as it directly affects the precision of the subsequent stage for center line prediction. In this study, the RTMDet [<xref rid="B19-sensors-25-05830" ref-type="bibr">19</xref>] model was employed specifically for shrimp instance segmentation. This model is integral to our framework due to its high accuracy and real-time performance capabilities. RTMDet uses a fully convolutional approach, which is different from two-stage instance segmentation models like Mask R-CNN that rely on regions of interest (ROIs) to predict the mask precisely. Its advanced mask prediction module allows instance segmentation to be performed directly from the feature maps, which improves FPS.</p><sec><title>Model Selection for Instance Segmentation of Shrimp</title><p>The real-time segmentation of shrimp poses a significant challenge that demands both high-precision modeling and robust generalization capabilities. The primary difficulties stem from the physical characteristics of shrimps themselves. Shrimps are naturally semi-transparent or translucent, which causes them to blend easily into backgrounds, particularly in aquatic environments or on light-colored surfaces. Additionally, shrimps can assume various postures&#8212;curved, twisted, or overlapping with other shrimps&#8212;which further complicates the task of distinguishing individual instances. These visual ambiguities can lead to either under-segmentation, where parts of a shrimp are missed, or over-segmentation, where multiple shrimps are merged into one another. To address these challenges, it was crucial to find a model that could accurately detect and segment each shrimp under different visual conditions. A comparative analysis of several advanced instance segmentation algorithms was performed, including SOLOv2 [<xref rid="B20-sensors-25-05830" ref-type="bibr">20</xref>], YOLACT [<xref rid="B21-sensors-25-05830" ref-type="bibr">21</xref>], CondInst [<xref rid="B22-sensors-25-05830" ref-type="bibr">22</xref>], SparseInst [<xref rid="B23-sensors-25-05830" ref-type="bibr">23</xref>], YOLOv8-m, and RTMDet-m. Among these options, RTMDet consistently showed superior segmentation quality and computational efficiency performance. During our experiments, RTMDet achieved a competitive FPS while still maintaining a high average precision (AP). In addition to accuracy, the need for real-time performance significantly influenced our choice of model. The RTMDet model used in our framework is based on the RTMDet-ins-m variant, which is specifically designed for instance segmentation tasks. The architecture of the selected RTMDet-ins-M model comprises several essential components, which are detailed as follows.</p><p>Given an input image <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, RTMDet predicts a set of instance masks <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the binary mask corresponding to the <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> shrimp instance. The model follows a fully convolutional pipeline comprising three major components: a strong backbone CSPNeXt [<xref rid="B24-sensors-25-05830" ref-type="bibr">24</xref>], a BiFusion neck for multi-scale feature aggregation, and a decoupled detection head that independently predicts classification, bounding boxes, and segmentation masks. The input image <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is first passed through a deep convolutional neural network (e.g., CSPNeXt in RTMDet) to extract multi-scale feature maps:<disp-formula id="FD1-sensors-25-05830"><label>(1)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the backbone network, and <italic toggle="yes">l</italic> indexes the feature level in the feature pyramid. The feature maps <italic toggle="yes">F</italic> are passed through the detection head to produce three outputs. First, the classification branch outputs the class logits <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">C</italic> is the number of classes. Second, the regression branch produces the bounding box coordinates <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>reg</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Third, for instance segmentation, a mask branch generates mask prediction features <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>mask</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">D</italic> is the dimensionality of the dynamic mask features. The segmentation head uses dynamic convolutional layers to create pixel-level masks directly from feature maps, enabling dense predictions without requiring ROI alignment. The mask predictions improve through aligned convolution and point-wise segmentation branches that enhance boundary localization. The final generated mask can be defined as below:<disp-formula id="FD2-sensors-25-05830"><label>(2)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>dyn</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>mask</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>Instance</mml:mi><mml:mi>-</mml:mi><mml:mi>Specific</mml:mi></mml:mrow><mml:mspace width="4.pt"/><mml:mi>Parameters</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>dyn</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the dynamic convolution operator conditioned on per-instance features to produce the binary mask <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The loss function of the RTMDet model is composed of three main terms: classification loss (<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>cls</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), bounding box regression loss (<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>reg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and mask loss (<inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>mask</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), which were typically implemented using Dice loss or Binary Cross-Entropy. The total loss <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>total</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in RTMDet combines three key components: classification loss, regression loss, and mask loss. The classification loss uses the focal loss to handle class imbalance and is defined as <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mi>FL</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>cls</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>cls</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>FL</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the focal loss between the predicted and ground truth class labels. The regression loss, which measures the quality of the predicted bounding boxes, uses the Generalized IoU (GIoU) and is computed as <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>reg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mi>GIoU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>box</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>box</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>box</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the ground truth box coordinates. The mask loss applies a pixel-wise binary cross-entropy (BCE) between each predicted instance mask and the ground truth mask, which is defined as <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>mask</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mi>BCE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. These components are combined into a single loss function:<disp-formula id="FD3-sensors-25-05830"><label>(3)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mfenced separators="" open="[" close="]"><mml:mi>FL</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>cls</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>cls</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>GIoU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>box</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>box</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>BCE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> are weighting factors that balance the contributions of the box regression and mask prediction losses.</p></sec></sec><sec id="sec2dot5-sensors-25-05830"><title>2.5. Shrimp Centerline Prediction via Semantic Segmentation</title><p>The second stage of the framework addresses the challenge of predicting shrimp centerlines from the instance masks produced in the first stage. The classical image processing methods, such as morphological skeletonization or medial axis transformation, cannot be directly applied to binary masks to obtain their centerlines. These methods tend to create burrs, particularly around the head and tail sides of the shrimp, making them unsuitable for our application. Furthermore, these methods require substantial post-processing to extract a single clean centerline from these skeletons. Post-processing includes pruning unnecessary branches, resolving junctions, and navigating the skeleton graph using custom heuristics or shortest path algorithms. These steps are computationally intensive, difficult to generalize across different shrimp shapes, and ultimately hinder the feasibility of real-time processing.</p><p>To address these challenges, this study proposed a custom segmentation model, illustrated in <xref rid="sensors-25-05830-f004" ref-type="fig">Figure 4</xref>, to precisely predict the centerline. The segmentation model takes a three-channel image, which consists of a fused RGB image and a binary instance mask, as input and generates a centerline for the shrimp&#8217;s body. This method is used to specifically capture the precise structure of the shrimp&#8217;s centerline.</p><sec id="sec2dot5dot1-sensors-25-05830"><title>2.5.1. Proposed Model for Shrimp Centerline Predictive Module</title><p>To address the challenge of accurately extracting the shrimp centerline, this study proposed an enhanced segmentation model built upon the baseline DeepLabV3 model. The proposed segmentation model integrates the outputs of UNet and the DeepLabv3 model to enhance the accuracy of centerline construction. This integration enables better preservation of fine-grained details, which are essential for the prediction of the centerline structure. This novel modification to the DeepLabv3 model achieves more precise centerline predictions. Our proposed segmentation model addresses the following key challenges in centerline segmentation: (1) capturing global contextual cues to resolve spatial ambiguity, and (2) preserving fine spatial details necessary for defining narrow structures. The original DeepLabv3 model utilizes Atrous Spatial Pyramid Pooling (ASPP) to capture multi-scale context, but it does not explicitly incorporate mechanisms for recovering high-resolution details. To address this limitation, the strength of a UNet-like structure is combined with DeepLabv3&#8217;s multi-scale contextual learning. This integration leads to a network with enhanced semantic precision and spatial resolution.</p><p>To train our proposed segmentation model, an efficient training strategy was adopted, which enhances the model&#8217;s generalizability and contextual awareness in centerline prediction. At first, a multi-branch design was employed to merge the original RGB image with the extracted binary mask using a lightweight 1 &#215; 1 convolution operation. This fusion effectively integrates prior information regarding the object&#8217;s location, specifically the shrimp, directly into the input of our proposed segmentation model. The workflow of our proposed centerline predictive module is discussed as follows:</p><p>The RGB image <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">I</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is concatenated with a binary mask <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> along the channel dimension to produce a four-channel input tensor:<disp-formula id="FD4-sensors-25-05830"><label>(4)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>concat</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This fused input contains both color information and spatial object cues. To reduce the number of channels and retain compatibility with existing architectures, a <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution is applied:<disp-formula id="FD5-sensors-25-05830"><label>(5)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>concat</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The resulting <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>fused</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is fed simultaneously into two parallel modules: a U-Net encoder that extracts hierarchical feature representations, and an Atrous Spatial Pyramid Pooling (ASPP) module that captures multi-scale contextual information using parallel atrous convolutions with different dilation rates. The deepest feature map from the encoder is concatenated with a downsampled version of the fused image to guide the ASPP with additional spatial cues. The ASPP output is then upsampled and fused with skip connections from the U-Net encoder. Each fusion stage is followed by convolutional refinement layers. Finally, the decoder produces a centerline mask through a final convolution and sigmoid activation:<disp-formula id="FD6-sensors-25-05830"><label>(6)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>final</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>dec</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>dec</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is a high-resolution, semantically rich feature representation of the image after all encoder&#8211;decoder processing. <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the final upsampling operation, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>final</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the final convolutional layer after which raw logits are converted into probabilities using the <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> function. The final predicted binary centerline mask is denoted by <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. This multi-stage process effectively combines hierarchical encoder features, raw RGB cues, and multi-scale context to produce a highly detailed and structurally accurate centerline segmentation output. The architecture of our improved segmentation model for the shrimp centerline prediction is contained in the centerline predictive module in <xref rid="sensors-25-05830-f003" ref-type="fig">Figure 3</xref>. During training, the ground truth centerline is represented as a three-pixel-wide binary mask to improve gradient flow and learning stability. The model is optimized using a composite loss function that combines Dice loss and Binary Cross-Entropy (BCE) loss.<disp-formula id="FD7-sensors-25-05830"><label>(7)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>Dice</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>BCE</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05830"><label>(8)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>Dice</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> addresses class imbalance and promotes overlap between prediction and ground truth, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>BCE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> penalizes pixel-wise misclassification. <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> presents the ground-truth label at pixel <italic toggle="yes">i</italic> and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the predicted probability at pixel <italic toggle="yes">i</italic>. The <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> is there for numerical stability to prevent the zero division, and the scalar <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is used to balance the contribution of the two loss terms. In semantic segmentation tasks, especially those involving thin structures like shrimp centerlines, a significant challenge arises due to class imbalance. In these cases, the majority of pixels in the training images belong to the background class, while the foreground (the centerline) occupies only a small portion of the image. This imbalance can result in models being biased toward predicting the background and neglecting the essential foreground structures. The Dice loss function is particularly effective in addressing this issue, as it emphasizes the overlap between the predicted regions and the ground truth. This helps to mitigate the adverse effects of class imbalance and ensures better segmentation of the foreground class.</p></sec><sec id="sec2dot5dot2-sensors-25-05830"><title>2.5.2. Centerline Extraction for Size Estimation</title><p>After centerline extraction, a skeletonization algorithm was applied to transform the binary map into a one-pixel-wide centerline. This algorithm is based on the method proposed by Zhang and Suen [<xref rid="B18-sensors-25-05830" ref-type="bibr">18</xref>] and follows an iterative thinning process. The algorithm removes pixels from the edges of binary shapes while keeping the object&#8217;s connections and structure intact. It does this by eliminating boundary pixels in stages, making sure the object stays connected. In each stage, the surrounding <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> area of a pixel is represented as an 8-bit value.<disp-formula id="FD9-sensors-25-05830"><label>(9)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>7</mml:mn></mml:munderover><mml:msup><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msup><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the binary value of the <italic toggle="yes">i</italic>-th neighbor of pixel <italic toggle="yes">p</italic>. The process continues until no more pixels can be removed, resulting in a clean, one-pixel-wide centerline. The final shrimp&#8217;s size is calculated by measuring the arc length of this centerline in pixels. Then, we convert the pixel size to millimeters using a conversion factor obtained from our image setup. The performance of our shrimp size estimation method was evaluated using the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) [<xref rid="B15-sensors-25-05830" ref-type="bibr">15</xref>] as follows.<disp-formula id="FD10-sensors-25-05830"><label>(10)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>MAE</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05830"><label>(11)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>RMSE</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the ground-truth length of the <italic toggle="yes">i</italic>-th shrimp in pixels and <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted length, and N is the total number of shrimps in the view. MAE provides the average magnitude of error, whereas the RMSE gives higher weight to large errors and reflects the overall deviation of predictions from the ground truth. The frames per second (FPS) metric was utilized to evaluate the inference performance of the model, measuring the processing speed of each algorithm, i.e., how many shrimp images can be processed per second.</p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-25-05830"><title>3. Results</title><p>This section evaluates various instance and semantic segmentation models to identify the best combination for measuring shrimp size. The most effective models from each category were selected based on quantitative performance metrics and integrated into the final pipeline. The proposed approach was compared with existing approaches in the literature for centerline extraction and size estimation. Performance comparison was performed using the key metrics such as Average Precision (AP), Mean Intersection over Union (mIoU), F1-score, number of trainable parameters, size estimation error, and FPS to demonstrate the accuracy and reliability of our approach.</p><sec id="sec3dot1-sensors-25-05830"><title>3.1. Instance Segmentation Module</title><p>The instance segmentation module is designed to accurately detect and segment individual shrimp in the input images. Several instance segmentation architectures were evaluated, and the most effective one was selected based on key performance metrics, including Average Precision (AP), number of parameters, and FPS. The model that performs best across these metrics is used to extract accurate shrimp masks. The output masks are then fed to the second module, the centerline predictive module, which estimates the centerline of a shrimp.</p><sec id="sec3dot1dot1-sensors-25-05830"><title>3.1.1. Implementation Details</title><p>To develop an accurate and efficient instance segmentation system for shrimp segmentation, the shrimp dataset developed in this study was employed. This dataset includes detailed annotations for each shrimp instance in the form of both masks and bounding boxes. Implementation and evaluation were carried out on six state-of-the-art, real-time instance segmentation models: SOLOv2 [<xref rid="B20-sensors-25-05830" ref-type="bibr">20</xref>], YOLACT [<xref rid="B21-sensors-25-05830" ref-type="bibr">21</xref>], CondInst [<xref rid="B22-sensors-25-05830" ref-type="bibr">22</xref>], SparseInst [<xref rid="B23-sensors-25-05830" ref-type="bibr">23</xref>], YOLOv8 [<xref rid="B25-sensors-25-05830" ref-type="bibr">25</xref>], and RTMDet [<xref rid="B19-sensors-25-05830" ref-type="bibr">19</xref>]. All models are anchor-free, except for YOLACT, which uses anchor-based mechanisms. Anchor-free models are typically faster and more efficient [<xref rid="B26-sensors-25-05830" ref-type="bibr">26</xref>], making them ideal for real-time applications due to their simpler design and lower computational overhead. To ensure consistency across experiments, ResNet-50 was employed as the backbone for all models except YOLOv8 and RTMDet, which utilize CSPDarknet [<xref rid="B27-sensors-25-05830" ref-type="bibr">27</xref>] and CSPNeXt [<xref rid="B24-sensors-25-05830" ref-type="bibr">24</xref>] backbones. These two models are architecturally designed around CSP-based backbones, and using their native configurations ensures optimal compatibility and performance. Each algorithm was trained for 70 epochs using a Stochastic Gradient Descent (SGD) optimizer. The learning rate was set to 0.01 with a momentum of 0.9 and a weight decay of 0.0001. A multi-step learning rate scheduler was employed to improve training stability. The experiments were conducted on NVIDIA TITAN RTX GPUs with 24 GB VRAM.</p><p>The model performance was evaluated using COCO-style metrics including Average Precision (AP) at IoU thresholds of 0.50 <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, 0.75 <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>75</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and averaged AP from 0.50 to 0.95 <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mo>&#8211;</mml:mo><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The number of parameters and frames per second (FPS) metrics for all models are also provided. These factors help us compare the model&#8217;s size and its ability to work in real time, which are important considerations for future deployment on embedded systems.</p></sec><sec id="sec3dot1dot2-sensors-25-05830"><title>3.1.2. Experimental Results</title><p>This section presents the evaluation of instance segmentation models on the shrimp dataset. The assessment highlights the trade-offs between accuracy, model size, and real-time performance. <xref rid="sensors-25-05830-t001" ref-type="table">Table 1</xref> presents the quantitative evaluation of models on the test dataset across three distinct environments. RTMDet-m achieves the highest performance across all evaluation metrics. It achieves the highest accuracy with an <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of 0.960, <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>75</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of 0.795, and <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mrow><mml:mo>&#8211;</mml:mo></mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.631. Additionally, it is the fastest among the models, running at 58 FPS. This strong performance and low parameter count of 34.2 million make it a great choice for real-time applications that need both accuracy and speed. YOLOv8-m also delivers strong results, attaining an <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mrow><mml:mo>&#8211;</mml:mo></mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.630 and the highest speed among all models except RTMDet-m, at 55 FPS. CondInst achieves a competitive <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of 0.934, but its performance drops at higher IoU thresholds with an <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>75</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of 0.663 and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mrow><mml:mo>&#8211;</mml:mo></mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.574. It has a moderate number of parameters and a decent FPS. SOLOv2 performs reasonably well with an <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mrow><mml:mo>&#8211;</mml:mo></mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 0.593 and a solid FPS of 44, although it has the highest number of parameters 46.2 M, potentially limiting its efficiency on edge devices. SparseInst and YOLACT show comparatively lower performance, particularly at stricter IoU thresholds. SparseInst, despite having a decent <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of 0.836, suffers from a low <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>75</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of 0.428. YOLACT, although efficient, yields the lowest accuracy overall, indicating limitations in mask quality and fine-grained localization.</p><p>In summary, the RTMDet-m model stands out as the most effective option for both segmentation quality and real-time performance, which is closely followed by the YOLOv8-m. These findings show that the RTMDet-m method is instrumental for real-time instance segmentation of shrimps. The model demonstrates strong performance in terms of segmentation accuracy, particularly in handling challenges such as varying shrimp poses and the transparency of shrimp bodies that often blend into the background. Based on these evaluation metrics, RTMDet-m was selected as the primary model for the instance segmentation of shrimp. <xref rid="sensors-25-05830-f005" ref-type="fig">Figure 5</xref> further illustrates the qualitative results of the RTMDet-m model, showcasing its impressive precision in segmenting individual shrimps under diverse conditions within our dataset.</p></sec></sec><sec id="sec3dot2-sensors-25-05830"><title>3.2. Centerline Predictive Module</title><p>In the first stage of our shrimp centerline prediction, an instance segmentation model was used to detect shrimp instances and generate their individual masks. These masks are then sent to the second module, i.e., the centerline prediction module, which estimates the centerline structure using our custom segmentation algorithm. The proposed custom segmentation model shown in <xref rid="sensors-25-05830-f004" ref-type="fig">Figure 4</xref> was evaluated against state-of-the-art semantic segmentation algorithms. The comparative analysis based on metrics precision, recall, Intersection over Union (IoU), F1-score, the number of trainable parameters, model size, and frames per second (FPS) demonstrates the overall superior performance of our proposed segmentation model.</p><sec id="sec3dot2dot1-sensors-25-05830"><title>3.2.1. Implementation Details</title><p>A specialized dataset was constructed to predict the centerline of each individual shrimp by mapping shrimp masks to their corresponding centerlines. Each mask was manually annotated to represent a centerline, enabling the semantic segmentation algorithms to learn a pixel-wise mapping from the fused shrimp mask and RGB image to its centerline representation. Four advanced semantic segmentation models: U-Net [<xref rid="B28-sensors-25-05830" ref-type="bibr">28</xref>], U-Net++ [<xref rid="B29-sensors-25-05830" ref-type="bibr">29</xref>], LinkNet [<xref rid="B30-sensors-25-05830" ref-type="bibr">30</xref>], and DeepLabv3 [<xref rid="B31-sensors-25-05830" ref-type="bibr">31</xref>] were implemented and evaluated in comparison with the proposed model. To ensure lightweight and real-time performance, all models utilized MobileNetV2 [<xref rid="B32-sensors-25-05830" ref-type="bibr">32</xref>] as the encoder backbone. To train the models, a Stochastic Gradient Descent (SGD) optimizer was used due to its effectiveness in navigating complex loss landscapes. The models were trained for 40 epochs, starting with an initial learning rate of 0.02. To stabilize and improve training performance, a multi-step learning rate scheduler was employed, which gradually decreases the learning rate as training progresses. All experiments and training were performed using NVIDIA TITAN RTX GPUs each with 24 GB of VRAM. The performance of each model was evaluated using several key metrics: Intersection over Union (IoU), F1-score, precision, recall, the number of trainable parameters, and FPS during inference. These metrics are useful to evaluate the accuracy and the model&#8217;s ability to perform in real time.</p></sec><sec id="sec3dot2dot2-sensors-25-05830"><title>3.2.2. Experimental Results</title><p>To evaluate the performance of semantic segmentation models for the centerline prediction, a comprehensive comparison of the segmentation models was conducted. Those models include UNet, UNet++, LinkNet, DeepLabv3 and our proposed segmentation model. To make a fair comparison, all models have the same backbone (MobileNetv2). The models were evaluated based on standard segmentation metrics, which include precision, recall, F1-score, mean Intersection over Union (mIoU), number of parameters and FPS. These metrics are essential for evaluating the accuracy and efficiency of the models. The results of all models are summarized in <xref rid="sensors-25-05830-t002" ref-type="table">Table 2</xref>. Among all the models, our proposed segmentation model achieved the best overall performance across all evaluation metrics, recording the highest F1-score (0.883) and mIoU (0.791). This indicates its superior capability in accurately predicting the centerline regions. It also had the highest recall (0.900), which is a crucial strength that ensures accurate centerline detection, as missing parts of a centerline can be more problematic than slight over-segmentation. UNet++ followed closely, achieving an F1-score of 0.868 and mIoU of 0.784. Its enhanced performance over the traditional UNet can be attributed to its nested skip connections. However, this improvement comes with increased model complexity and reduced FPS. LinkNet demonstrated a strong balance between accuracy and efficiency. Although its F1-score (0.844) and mIoU (0.731) were slightly lower than those of UNet++, it required significantly fewer parameters (2.1 M) and operated faster, making it suitable for lightweight deployment scenarios. The baseline UNet performed reasonably well, achieving an F1-score of 0.843 and mIoU of 0.729, outperforming LinkNet in precision but not in overall segmentation quality. It maintained a high FPS but had a larger model size compared to LinkNet and DeepLabv3.</p><p>Our proposed model, despite having a slightly lower FPS, shows superior segmentation performance in centerline prediction compared to the baseline DeepLabv3. Specifically, it achieves higher precision (0.900 vs. 0.882), F1-score (0.883 vs. 0.860), and mean Intersection over Union (IoU). Our proposed model identifies key features while maintaining consistent performance and efficiency. It offers the best balance of speed and accuracy, providing cutting-edge performance while using fewer parameters.</p><p><xref rid="sensors-25-05830-f006" ref-type="fig">Figure 6</xref> shows centerline predictions for multiple shrimp instances from different models. Each shrimp&#8217;s centerline is accurately segmented, demonstrating the model&#8217;s ability to capture structural details and extract precise centerlines, even in complex scenes with overlapping shrimp.</p></sec></sec><sec id="sec3dot3-sensors-25-05830"><title>3.3. Extraction of Centerline Skeleton for Precise Shrimp Size Estimation</title><p>The final stage of our pipeline involves the precise estimation of shrimp size based on the one-pixel-wide centerline. To begin with, the output centerline from the semantic segmentation module, which is typically a few pixels wide, is post-processed using the Zhang&#8211;Suen thinning algorithm. This algorithm systematically removes redundant edge pixels while preserving the topological structure of the centerline. A one-pixel-wide path represents the medial axis of the shrimp body. This refined line serves as a robust and scale-invariant factor for the precise measurement.</p><p>This pixel length is then converted to real-world units (millimeters) using a fixed pixel-to-length ratio obtained through calibration using a reference object of known size. Our proposed framework provides the precise size estimation of individual shrimp across various environments. <xref rid="sensors-25-05830-f007" ref-type="fig">Figure 7</xref> shows the results of our predictions with centerlines marked on each shrimp along with the estimated size in millimeters.</p><p>Quantitative results (<xref rid="sensors-25-05830-t003" ref-type="table">Table 3</xref>) show that the proposed framework substantially outperforms the other approaches, achieving the lowest MAE (10.23 mm) and RMSE (12.79 mm), indicating highly precise size estimation. In contrast, PCFM and VDBM exhibited significantly higher errors, while U-Net with SAG kernels provided moderate improvements but still lagged behind the proposed framework.</p><p>Furthermore, the proposed framework maintains 5.14 FPS, demonstrating a favorable trade-off between accuracy and computational efficiency. The qualitative results shown in <xref rid="sensors-25-05830-f007" ref-type="fig">Figure 7</xref> further confirm the robustness of the approach, illustrating more accurate and consistent shrimp centerline predictions. Collectively, these results validate the effectiveness of the proposed framework in predicting shrimp centerlines and accurately estimating shrimp size.</p></sec><sec id="sec3dot4-sensors-25-05830"><title>3.4. Comparative Study of Methods for Measuring Shrimp Size</title><p>In this section, the comparison of our dual-segmentation framework for estimating shrimp size with several existing methods in the literature is presented. The proposed framework combines RTMDet-based instance segmentation with our custom segmentation model to facilitate precise shrimp centerline prediction. This is followed by a skeletonization step that transforms the centerline into a one-pixel representation to calculate the size in millimeters. Through extensive experiments in our study, it is found that the integration of the RTMDet model and our custom centerline segmentation model provides excellent results. This combination offers both high accuracy and FPS. This makes our framework well suited for deployment in practical aquaculture settings.</p><p>To assess the effectiveness of our proposed framework, our study provides both quantitative and qualitative comparisons with three notable approaches from the literature for centerline extraction. These approaches include methods such as polynomial curve fitting, the Voronoi diagram-based method, and the deep learning-based silhouette segmentation method. The detailed comparison of those methods is discussed in the following paragraph.</p><p>The method proposed by Kim et al. [<xref rid="B13-sensors-25-05830" ref-type="bibr">13</xref>] uses polynomial curve fitting to estimate the centerline of rainbow trout. This approach allows for a precise geometric interpretation of the centerline. However, it heavily depends on preprocessing steps, especially the horizontal alignment of the object&#8217;s outline. This alignment is accomplished by estimating the principal direction using image moments. However, it is essential to note that this technique is sensitive to posture and orientation variations commonly encountered in natural aquatic environments. Additionally, the method requires manual or heuristic selection of the polynomial degree, which can be quite challenging, especially in cases where shrimp postures overlap or are non-linearly distorted. These complexities make the polynomial fitting approach less generalizable and difficult to automate across various datasets.</p><p>Zhao et al. [<xref rid="B33-sensors-25-05830" ref-type="bibr">33</xref>] proposed a technique based on Voronoi diagrams to extract the medial axis of objects. This method was adapted for estimating the centerlines of shrimp silhouettes. While it effectively captures the overall structure of the shape, it tends to introduce spurious branches, commonly referred to as &#8220;burrs,&#8221; particularly in the head and tail regions. These artifacts distort size measurements and necessitate additional pruning steps. To address this issue, a graph-based algorithm was implemented to retain the longest path as the centerline. However, the additional computational burden of this post-processing significantly reduces throughput, which hinders real-time applicability and increases system complexity.</p><p>Wang et al. [<xref rid="B3-sensors-25-05830" ref-type="bibr">3</xref>] present a deep learning-based approach that combines U-shaped fully convolutional networks (U-Net) with SAG kernels to estimate the centerline from Artemia silhouettes. While this method is effective at modeling complex morphological features, it is limited by the challenges associated with silhouette-based segmentation. In multi-object scenarios, overlapping individuals are merged into a single foreground region, preventing the separation of instances. Additionally, the approach struggles with different orientations of shrimp, often leading to incomplete or noisy segmentation masks. This results in inaccurate or disconnected centerlines, making the instance-level identity hard, which is essential for individual size measurement.</p><p>Among the evaluated methods, the polynomial curve-fitting method (PCFM) provides the highest FPS due to its simplicity and the absence of learning-based components. Its speed advantage comes from being a conventional, model-free approach. However, thePCFM faces difficulty in accurately estimating centerlines when shrimp shapes become complex or irregular. As shown in <xref rid="sensors-25-05830-f008" ref-type="fig">Figure 8</xref> (samples 1 and 5), the fitted curves deviate from the true centerline, resulting in misleading outputs. Additionally, the PCFM requires parameter tuning, which is impractical for shrimp because of their inconsistent morphology. The Voronoi diagram-based method produces reasonably accurate centerlines, particularly in the mid-body regions. However, toward the tail, the method often selects the longest path without considering anatomical accuracy due to its inherent algorithmic structure. Additionally, this method tends to generate many extraneous branches, or &#8220;burrs,&#8221; which must be removed using a graph-based approach to isolate the longest valid path. While this preprocessing step is somewhat effective, it introduces significant computational overhead, resulting in a notably lower FPS.</p><p>The UNet+SAG performs well, mainly when supported by high-quality instance masks. It provides robust centerline predictions, though its FPS is moderate. In contrast to the above-mentioned approaches, our proposed framework strikes the best balance between accuracy and efficiency. It consistently outperforms all baseline methods in terms of Mean Absolute Error and Root Mean Squared Error (MSE) while providing a reasonable FPS.</p><p>To evaluate shrimp size estimation methods, both pixel-based measurements and real-world dimensions were used. Our dataset includes ground-truth lengths in pixel units and millimeters, allowing us to assess the model&#8217;s accuracy in both domains for practical aquaculture applications. The primary metrics used in our comparative study are Mean Absolute Error (MAE), which measures the average deviation between predicted and actual lengths, and Root Mean Squared Error (RMSE), which highlights larger errors. These metrics are reported in both pixels and millimeters in <xref rid="sensors-25-05830-t003" ref-type="table">Table 3</xref> for quantitative comparison.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05830"><title>4. Discussion</title><p>This work presents a dual-segmentation framework for centerline prediction in shrimp and achieves the highest segmentation accuracy and processing speed in terms of FPS. The system is designed to support real-time processing, which is crucial for measuring shrimp in aquaculture environments. The following discussion will provide insights into quantitative results from each stage of our framework along with a comparative evaluation against state-of-the-art methods.</p><p>During the evaluation phase, first-instance segmentation models were compared that provide instance segmentation results for shrimp. Among the models tested, RTMDet-m demonstrated the most consistent and superior performance. It achieved an average precision (AP<sub>50</sub>) of 0.960, outperforming other models such as SOLOv2 (AP<sub>50</sub> = 0.899), YOLACT (AP<sub>50</sub> = 0.902), CondInst (AP<sub>50</sub> = 0.934), SparseInst (AP<sub>50</sub> = 0.836) and YOLOv8-m (AP<sub>50</sub> = 0.931). In addition to this high precision, RTMDet-m attained an AP<sub>75</sub> of 0.795 and a COCO-style mean average precision (AP<sub>50&#8211;95</sub>) of 0.631, indicating its high performance in instance segmentation of shrimp. The model&#8217;s ability to accurately segment instances at higher IoU thresholds shows that the model can create accurate masks around the complex shapes of shrimp, which is crucial for precise measurements. RTMDet-m achieved this level of accuracy while running at 58 FPS, outperforming other baseline models.</p><p>The second part of the evaluation provides a comparison between our proposed centerline segmentation model and other baseline methods. This task is particularly challenging due to the nature of shrimp bodies and their varying postures. To address this challenging task, our study proposed a custom segmentation model followed by a morphological skeletonization (MS) process for shrimp centerline prediction. The combination of our proposed segmentation model and MS proved to be the most effective among other baseline methods. It achieved the highest F1-score of 0.883 and a mean Intersection-over-Union (mIoU) of 0.791. These results indicate both high pixel-wise accuracy and strong spatial alignment with the ground-truth centerline. The method achieved a highest FPS of 150, which is essential for real-time applications. While other models like UNet++, LinkNet, and DeepLabv3 had competitive F1-scores of 0.868, 0.844, and 0.860, respectively, they showed lower FPS and produced slightly lower segmentation quality. The superior performance of our proposed segmentation model for centerline prediction can be attributed to the Atrous Spatial Pyramid Pooling (ASPP) and a UNet-type feature fusion mechanism, which effectively captures multi-scale context.</p><p>To validate the effectiveness of our framework, the accuracy of shrimp centerline prediction and its size are compared with existing methods reported in the above paragraphs. Our analysis revealed that our framework achieved the lowest mean absolute error (MAE) of 1.02 centimeters (cms) compared to other baseline methods. This marks a significant upper hand over competing methods, such as PCFM, which recorded an MAE of 4.46 cms, and VDBM with an MAE of 5.07 cms. The UNet+SAG deep learning based approach also reported a close but still high MAE of 3.02 cms. Additionally, our framework provides a higher FPS of 5.14 compared to other methods for centerline prediction.</p><p>In summary, our dual-segmentation framework provides accurate results for instance segmentation and centerline prediction, outperforming state-of-the-art methods in terms of accuracy metrics and operational speed. The integration of RTMDet-m and our improved segmentation model shows the effectiveness of our proposed framework for shrimp size estimation by achieving the lowest MAE and RMSE.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05830"><title>5. Conclusions</title><p>This study presents a robust dual-segmentation framework for the size measurement of shrimp using center-line predictions, providing an accurate and non-intrusive length estimation method. The system is designed to precisely predict the individual shrimp size regardless of their shape or posture. In the first stage, the RTMDet-m model was employed for high-speed instance segmentation, achieving a high average precision (AP<sub>50</sub>) of 96%. This performance surpasses that of other instance segmentation models in both accuracy and FPS. In the second stage, the centerline of individual shrimp was extracted using our proposed custom segmentation model. The proposed centerline segmentation model in this study demonstrated exceptional performance with an F1-score of 88.3%. Our proposed framework outperforms other conventional and deep learning based methods, providing the lowest mean absolute error of 1.02 cm. This framework represents a significant advancement in aquaculture management, allowing for continuous and stress-free monitoring of shrimp growth. Additionally, it can be adapted for other aquatic species, such as fish, enabling early disease detection and overall monitoring of stock health, all without disrupting the natural behavior.</p></sec></body><back><ack><title>Acknowledgments</title><p>Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2025R140), Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.M.W. and M.S.; methodology, M.M.W. and H.A.; software, M.M.W. and H.Z.; validation, H.A., H.Z. and H.G.M.; data curation, H.A. and S.C.K.; writing&#8212;original draft preparation, M.M.W.; writing review and editing, M.M.W. and H.G.M.; visualization, H.Z. and H.G.M.; supervision, S.C.K. and M.S.; project administration, S.C.K.; resources, S.C.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available upon request from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05830"><label>1.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Mandal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>P.</given-names></name></person-group><article-title>Global Scenario of Shrimp Industry: Present Status and Future Prospects</article-title><source>Shrimp Culture Technology: Farming, Health Management and Quality Assurance</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2025</year><fpage>1</fpage><lpage>23</lpage></element-citation></ref><ref id="B2-sensors-25-05830"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ran</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>Q.</given-names></name></person-group><article-title>Shrimp phenotypic data extraction and growth abnormality identification method based on instance segmentation</article-title><source>Comput. Electron. Agric.</source><year>2025</year><volume>229</volume><fpage>109701</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109701</pub-id></element-citation></ref><ref id="B3-sensors-25-05830"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Van Stappen</surname><given-names>G.</given-names></name><name name-style="western"><surname>De Baets</surname><given-names>B.</given-names></name></person-group><article-title>Automated Artemia length measurement using U-shaped fully convolutional networks and second-order anisotropic Gaussian kernels</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>168</volume><fpage>105102</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2019.105102</pub-id></element-citation></ref><ref id="B4-sensors-25-05830"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Puig-Pons</surname><given-names>V.</given-names></name><name name-style="western"><surname>Mu&#241;oz-Benavent</surname><given-names>P.</given-names></name><name name-style="western"><surname>Espinosa</surname><given-names>V.</given-names></name><name name-style="western"><surname>Andreu-Garc&#237;a</surname><given-names>G.</given-names></name><name name-style="western"><surname>Valiente-Gonz&#225;lez</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Estruch</surname><given-names>V.D.</given-names></name><name name-style="western"><surname>Ord&#243;&#241;ez</surname><given-names>P.</given-names></name><name name-style="western"><surname>P&#233;rez-Arjona</surname><given-names>I.</given-names></name><name name-style="western"><surname>Atienza</surname><given-names>V.</given-names></name><name name-style="western"><surname>M&#232;lich</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Automatic Bluefin Tuna (Thunnus thynnus) biomass estimation during transfers using acoustic and computer vision techniques</article-title><source>Aquac. Eng.</source><year>2019</year><volume>85</volume><fpage>22</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.aquaeng.2019.01.005</pub-id></element-citation></ref><ref id="B5-sensors-25-05830"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saleh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hasan</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Raadsma</surname><given-names>H.W.</given-names></name><name name-style="western"><surname>Khatkar</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Jerry</surname><given-names>D.R.</given-names></name><name name-style="western"><surname>Azghadi</surname><given-names>M.R.</given-names></name></person-group><article-title>Prawn morphometrics and weight estimation from images using deep learning for landmark localization</article-title><source>Aquac. Eng.</source><year>2024</year><volume>106</volume><fpage>102391</fpage><pub-id pub-id-type="doi">10.1016/j.aquaeng.2024.102391</pub-id></element-citation></ref><ref id="B6-sensors-25-05830"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name></person-group><article-title>An automatic method of fish length estimation using underwater stereo system based on LabVIEW</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>173</volume><fpage>105419</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2020.105419</pub-id></element-citation></ref><ref id="B7-sensors-25-05830"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name></person-group><article-title>The measurement of fish size by machine vision-a review</article-title><source>Proceedings of the International Conference on Computer and Computing Technologies in Agriculture</source><conf-loc>Dongying, China</conf-loc><conf-date>19&#8211;21 October 2016</conf-date><fpage>15</fpage><lpage>32</lpage></element-citation></ref><ref id="B8-sensors-25-05830"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hoon</surname><given-names>K.S.</given-names></name><name name-style="western"><surname>Cheol</surname><given-names>K.S.</given-names></name><name name-style="western"><surname>Won</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>Won</surname><given-names>K.S.</given-names></name><name name-style="western"><surname>Hyongsuk</surname><given-names>K.</given-names></name></person-group><article-title>Instance segmentation of shrimp based on contrastive learning</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>6979</elocation-id><pub-id pub-id-type="doi">10.3390/app13126979</pub-id></element-citation></ref><ref id="B9-sensors-25-05830"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>S.W.</given-names></name></person-group><article-title>Size estimation for shrimp using deep learning method</article-title><source>Smart Media J.</source><year>2023</year><volume>12</volume><fpage>112</fpage><pub-id pub-id-type="doi">10.30693/SMJ.2023.12.3.112</pub-id></element-citation></ref><ref id="B10-sensors-25-05830"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ram&#237;rez-Coronel</surname><given-names>F.J.</given-names></name><name name-style="western"><surname>Rodr&#237;guez-El&#237;as</surname><given-names>O.M.</given-names></name><name name-style="western"><surname>Esquer-Miranda</surname><given-names>E.</given-names></name><name name-style="western"><surname>P&#233;rez-Patricio</surname><given-names>M.</given-names></name><name name-style="western"><surname>P&#233;rez-B&#225;ez</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Hinojosa-Palafox</surname><given-names>E.A.</given-names></name></person-group><article-title>Non-Invasive Fish Biometrics for Enhancing Precision and Understanding of Aquaculture Farming through Statistical Morphology Analysis and Machine Learning</article-title><source>Animals</source><year>2024</year><volume>14</volume><elocation-id>1850</elocation-id><pub-id pub-id-type="doi">10.3390/ani14131850</pub-id><pub-id pub-id-type="pmid">38997962</pub-id><pub-id pub-id-type="pmcid">PMC11240837</pub-id></element-citation></ref><ref id="B11-sensors-25-05830"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Quan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name></person-group><article-title>Machine vision-based estimation of body size and weight of pearl gentian grouper</article-title><source>Aquac. Int.</source><year>2024</year><volume>32</volume><fpage>5325</fpage><lpage>5351</lpage><pub-id pub-id-type="doi">10.1007/s10499-024-01428-0</pub-id></element-citation></ref><ref id="B12-sensors-25-05830"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hsieh</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>H.Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.H.</given-names></name><name name-style="western"><surname>Liou</surname><given-names>J.H.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>T.T.</given-names></name></person-group><article-title>A simple and effective digital imaging approach for tuna fish length measurement compatible with fishing operations</article-title><source>Comput. Electron. Agric.</source><year>2011</year><volume>75</volume><fpage>44</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2010.09.009</pub-id></element-citation></ref><ref id="B13-sensors-25-05830"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name></person-group><article-title>A prototype to measure rainbow trout&#8217;s length using image processing</article-title><source>J. Korean Soc. Fish. Aquat. Sci.</source><year>2020</year><volume>53</volume><fpage>123</fpage><lpage>130</lpage></element-citation></ref><ref id="B14-sensors-25-05830"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tseng</surname><given-names>C.H.</given-names></name><name name-style="western"><surname>Hsieh</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Kuo</surname><given-names>Y.F.</given-names></name></person-group><article-title>Automatic measurement of the body length of harvested fish using convolutional neural networks</article-title><source>Biosyst. Eng.</source><year>2020</year><volume>189</volume><fpage>36</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2019.11.002</pub-id></element-citation></ref><ref id="B15-sensors-25-05830"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Y.p.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Z.Y.</given-names></name><name name-style="western"><surname>Du</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bi</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Y.</given-names></name></person-group><article-title>A novel centerline extraction method for overlapping fish body length measurement in aquaculture images</article-title><source>Aquac. Eng.</source><year>2022</year><volume>99</volume><fpage>102302</fpage><pub-id pub-id-type="doi">10.1016/j.aquaeng.2022.102302</pub-id></element-citation></ref><ref id="B16-sensors-25-05830"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>M.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name></person-group><article-title>In-water fish body-length measurement system based on stereo vision</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6325</elocation-id><pub-id pub-id-type="doi">10.3390/s23146325</pub-id><pub-id pub-id-type="pmid">37514620</pub-id><pub-id pub-id-type="pmcid">PMC10384091</pub-id></element-citation></ref><ref id="B17-sensors-25-05830"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dalvit Carvalho da Silva</surname><given-names>R.</given-names></name><name name-style="western"><surname>Soltanzadeh</surname><given-names>R.</given-names></name><name name-style="western"><surname>Figley</surname><given-names>C.R.</given-names></name></person-group><article-title>Automated Coronary Artery Tracking with a Voronoi-Based 3D Centerline Extraction Algorithm</article-title><source>J. Imaging</source><year>2023</year><volume>9</volume><elocation-id>268</elocation-id><pub-id pub-id-type="doi">10.3390/jimaging9120268</pub-id><pub-id pub-id-type="pmid">38132686</pub-id><pub-id pub-id-type="pmcid">PMC10743762</pub-id></element-citation></ref><ref id="B18-sensors-25-05830"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Suen</surname><given-names>C.Y.</given-names></name></person-group><article-title>A fast parallel algorithm for thinning digital patterns</article-title><source>Commun. ACM</source><year>1984</year><volume>27</volume><fpage>236</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1145/357994.358023</pub-id></element-citation></ref><ref id="B19-sensors-25-05830"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lyu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name></person-group><article-title>Rtmdet: An empirical study of designing real-time object detectors</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2212.07784</pub-id><pub-id pub-id-type="arxiv">2212.07784</pub-id></element-citation></ref><ref id="B20-sensors-25-05830"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name></person-group><article-title>Solov2: Dynamic and fast instance segmentation</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>17721</fpage><lpage>17732</lpage></element-citation></ref><ref id="B21-sensors-25-05830"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bolya</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y.J.</given-names></name></person-group><article-title>Yolact: Real-time instance segmentation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>9157</fpage><lpage>9166</lpage></element-citation></ref><ref id="B22-sensors-25-05830"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name></person-group><article-title>Conditional convolutions for instance segmentation</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><fpage>282</fpage><lpage>298</lpage></element-citation></ref><ref id="B23-sensors-25-05830"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name></person-group><article-title>Sparse instance activation for real-time instance segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LO, USA</conf-loc><conf-date>21&#8211;24 June 2022</conf-date><fpage>4433</fpage><lpage>4442</lpage></element-citation></ref><ref id="B24-sensors-25-05830"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Karmouni</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name></person-group><article-title>CSPNeXt: A new efficient token hybrid backbone</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>132</volume><fpage>107886</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.107886</pub-id></element-citation></ref><ref id="B25-sensors-25-05830"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reis</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kupec</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Daoudi</surname><given-names>A.</given-names></name></person-group><article-title>Real-Time Flying Object Detection with YOLOv8</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2305.09972</pub-id></element-citation></ref><ref id="B26-sensors-25-05830"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>T.</given-names></name></person-group><article-title>Fcos: Fully convolutional one-stage object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>9627</fpage><lpage>9636</lpage></element-citation></ref><ref id="B27-sensors-25-05830"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.Y.M.</given-names></name></person-group><article-title>Yolov4: Optimal speed and accuracy of object detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id><pub-id pub-id-type="arxiv">2004.10934</pub-id></element-citation></ref><ref id="B28-sensors-25-05830"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><source>Proceedings of the Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015: 18th International Conference</source><conf-loc>Munich, Germany</conf-loc><conf-date>5&#8211;9 October 2015</conf-date><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="B29-sensors-25-05830"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Rahman Siddiquee</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Tajbakhsh</surname><given-names>N.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>J.</given-names></name></person-group><article-title>Unet++: A nested u-net architecture for medical image segmentation</article-title><source>Proceedings of the Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018</source><conf-loc>Granada, Spain</conf-loc><conf-date>20 September 2018</conf-date><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/978-3-030-00889-5_1</pub-id><pub-id pub-id-type="pmcid">PMC7329239</pub-id><pub-id pub-id-type="pmid">32613207</pub-id></element-citation></ref><ref id="B30-sensors-25-05830"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chaurasia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Culurciello</surname><given-names>E.</given-names></name></person-group><article-title>Linknet: Exploiting encoder representations for efficient semantic segmentation</article-title><source>Proceedings of the 2017 IEEE Visual Communications and Image Processing (VCIP)</source><conf-loc>St. Petersburg, FL, USA</conf-loc><conf-date>10&#8211;13 December 2017</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B31-sensors-25-05830"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schroff</surname><given-names>F.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date></element-citation></ref><ref id="B32-sensors-25-05830"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sandler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Howard</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhmoginov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name></person-group><article-title>Mobilenetv2: Inverted residuals and linear bottlenecks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>4510</fpage><lpage>4520</lpage></element-citation></ref><ref id="B33-sensors-25-05830"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>A Voronoi-diagram-based method for centerline extraction in 3D industrial line-laser reconstruction using a graph-centrality-based pruning algorithm</article-title><source>Opt. Lasers Eng.</source><year>2018</year><volume>107</volume><fpage>129</fpage><lpage>139</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05830-f001" orientation="portrait"><label>Figure 1</label><caption><p>Extracted images for instance segmentation dataset collected from three diverse environments, varying in number of shrimps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g001.jpg"/></fig><fig position="float" id="sensors-25-05830-f002" orientation="portrait"><label>Figure 2</label><caption><p>Visualization of centerline dataset for semantic segmentation with shrimp masks in the top row and corresponding centerline labels at the bottom.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g002.jpg"/></fig><fig position="float" id="sensors-25-05830-f003" orientation="portrait"><label>Figure 3</label><caption><p>System overview of our proposed dual-segmentation framework for segmentation and size estimation of shrimp.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g003.jpg"/></fig><fig position="float" id="sensors-25-05830-f004" orientation="portrait"><label>Figure 4</label><caption><p>Our proposed segmentation model for the prediction of shrimp centerline with enhanced decoder compared to DeepLabv3 model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g004.jpg"/></fig><fig position="float" id="sensors-25-05830-f005" orientation="portrait"><label>Figure 5</label><caption><p>Visualization of Instance segmentation results on test data with three different environments using RTMDet-m model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g005.jpg"/></fig><fig position="float" id="sensors-25-05830-f006" orientation="portrait"><label>Figure 6</label><caption><p>Visualization of shrimp instances, their ground truths and predicted centerlines generated using our proposed segmentation model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g006.jpg"/></fig><fig position="float" id="sensors-25-05830-f007" orientation="portrait"><label>Figure 7</label><caption><p>Real-time inference results of our proposed framework, displaying shrimp instances with predicted centerlines and estimated size in millimeters (mm).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g007.jpg"/></fig><fig position="float" id="sensors-25-05830-f008" orientation="portrait"><label>Figure 8</label><caption><p>Qualitative results of convential and deep learning based methods for finding the centerline of the shrimp. The orange dashed boxes indicates regions where centerline predictions mismatch from the ground-truth centerlines.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05830-g008.jpg"/></fig><table-wrap position="float" id="sensors-25-05830-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05830-t001_Table 1</object-id><label>Table 1</label><caption><p>Performance comparison of RTMDet-m model with other real-time instance segmentation models in terms of accuracy, number of parameters, and FPS.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold">AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold">AP</mml:mi><mml:mn>75</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold">AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mrow><mml:mo>&#8211;</mml:mo></mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SOLOv2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.899</td><td align="center" valign="middle" rowspan="1" colspan="1">0.724</td><td align="center" valign="middle" rowspan="1" colspan="1">0.593</td><td align="center" valign="middle" rowspan="1" colspan="1">46.2</td><td align="center" valign="middle" rowspan="1" colspan="1">44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLACT</td><td align="center" valign="middle" rowspan="1" colspan="1">0.783</td><td align="center" valign="middle" rowspan="1" colspan="1">0.461</td><td align="center" valign="middle" rowspan="1" colspan="1">0.437</td><td align="center" valign="middle" rowspan="1" colspan="1">34.7</td><td align="center" valign="middle" rowspan="1" colspan="1">43</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CondInst</td><td align="center" valign="middle" rowspan="1" colspan="1">0.934</td><td align="center" valign="middle" rowspan="1" colspan="1">0.663</td><td align="center" valign="middle" rowspan="1" colspan="1">0.574</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5</td><td align="center" valign="middle" rowspan="1" colspan="1">42</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SparseInst</td><td align="center" valign="middle" rowspan="1" colspan="1">0.836</td><td align="center" valign="middle" rowspan="1" colspan="1">0.428</td><td align="center" valign="middle" rowspan="1" colspan="1">0.432</td><td align="center" valign="middle" rowspan="1" colspan="1">42.6</td><td align="center" valign="middle" rowspan="1" colspan="1">43</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8-m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.931</td><td align="center" valign="middle" rowspan="1" colspan="1">0.735</td><td align="center" valign="middle" rowspan="1" colspan="1">0.630</td><td align="center" valign="middle" rowspan="1" colspan="1">27.2</td><td align="center" valign="middle" rowspan="1" colspan="1">55</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RTMDet-m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.960</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.795</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.631</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>34.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>58</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>Bold values indicate the best result for each metric among the compared models.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05830-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05830-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative comparison of semantic segmentation models with our proposed model for shrimp centerline prediction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Param (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.839</td><td align="center" valign="middle" rowspan="1" colspan="1">0.847</td><td align="center" valign="middle" rowspan="1" colspan="1">0.843</td><td align="center" valign="middle" rowspan="1" colspan="1">0.729</td><td align="center" valign="middle" rowspan="1" colspan="1">4.4</td><td align="center" valign="middle" rowspan="1" colspan="1">164</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet++</td><td align="center" valign="middle" rowspan="1" colspan="1">0.864</td><td align="center" valign="middle" rowspan="1" colspan="1">0.893</td><td align="center" valign="middle" rowspan="1" colspan="1">0.868</td><td align="center" valign="middle" rowspan="1" colspan="1">0.784</td><td align="center" valign="middle" rowspan="1" colspan="1">4.6</td><td align="center" valign="middle" rowspan="1" colspan="1">127</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LinkNet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.826</td><td align="center" valign="middle" rowspan="1" colspan="1">0.863</td><td align="center" valign="middle" rowspan="1" colspan="1">0.844</td><td align="center" valign="middle" rowspan="1" colspan="1">0.731</td><td align="center" valign="middle" rowspan="1" colspan="1">2.1</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DeepLabV3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.842</td><td align="center" valign="middle" rowspan="1" colspan="1">0.882</td><td align="center" valign="middle" rowspan="1" colspan="1">0.860</td><td align="center" valign="middle" rowspan="1" colspan="1">0.754</td><td align="center" valign="middle" rowspan="1" colspan="1">2.1</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>169</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Our Proposed Model</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.866</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.900</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.883</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.791</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>150</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>Bold values indicate the best result for each metric among the compared models.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05830-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05830-t003_Table 3</object-id><label>Table 3</label><caption><p>Quantitative comparison of centerline extraction methods with our proposed framework using MAE and RMSE for size estimation in both pixels and real size along with FPS for evaluating real-time performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE (px)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (px)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">PCFM</td><td align="center" valign="middle" rowspan="1" colspan="1">84.76</td><td align="center" valign="middle" rowspan="1" colspan="1">44.61</td><td align="center" valign="middle" rowspan="1" colspan="1">105.95</td><td align="center" valign="middle" rowspan="1" colspan="1">55.76</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VDBM</td><td align="center" valign="middle" rowspan="1" colspan="1">96.27</td><td align="center" valign="middle" rowspan="1" colspan="1">50.67</td><td align="center" valign="middle" rowspan="1" colspan="1">120.34</td><td align="center" valign="middle" rowspan="1" colspan="1">63.34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet+SAG</td><td align="center" valign="middle" rowspan="1" colspan="1">57.40</td><td align="center" valign="middle" rowspan="1" colspan="1">30.21</td><td align="center" valign="middle" rowspan="1" colspan="1">71.75</td><td align="center" valign="middle" rowspan="1" colspan="1">37.21</td><td align="center" valign="middle" rowspan="1" colspan="1">3.2</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Our Framework</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>19.44</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>10.23</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>24.30</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>12.79</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.14</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>Bold values indicate the best result for each metric among the compared methods.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>