<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Technol Cancer Res Treat</journal-id><journal-id journal-id-type="iso-abbrev">Technol Cancer Res Treat</journal-id><journal-id journal-id-type="pmc-domain-id">3299</journal-id><journal-id journal-id-type="pmc-domain">tct</journal-id><journal-id journal-id-type="publisher-id">TCT</journal-id><journal-title-group><journal-title>Technology in Cancer Research &amp; Treatment</journal-title></journal-title-group><issn pub-type="ppub">1533-0346</issn><issn pub-type="epub">1533-0338</issn><publisher><publisher-name>SAGE Publications</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12475328</article-id><article-id pub-id-type="pmcid-ver">PMC12475328.1</article-id><article-id pub-id-type="pmcaid">12475328</article-id><article-id pub-id-type="pmcaiid">12475328</article-id><article-id pub-id-type="pmid">41004387</article-id><article-id pub-id-type="doi">10.1177/15330338251380966</article-id><article-id pub-id-type="publisher-id">10.1177_15330338251380966</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Original Research Article</subject></subj-group></article-categories><title-group><article-title>EfficientNetSwift: A Lightweight and Precise Deep Learning Model for Detecting Oral Squamous Cell Carcinoma Using Pathological Images</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0640-2284</contrib-id><name name-style="western"><surname>Wu</surname><given-names initials="M">Min</given-names></name><degrees>MS</degrees><xref rid="aff1-15330338251380966" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Hu</surname><given-names initials="Y">Yue</given-names></name><degrees>PhD</degrees><xref rid="aff2-15330338251380966" ref-type="aff">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Tian</surname><given-names initials="F">Fa</given-names></name><degrees>PhD</degrees><xref rid="aff3-15330338251380966" ref-type="aff">3</xref><xref rid="corresp2-15330338251380966" ref-type="corresp"/></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Lin</surname><given-names initials="H">Huiping</given-names></name><degrees>PhD</degrees><xref rid="aff1-15330338251380966" ref-type="aff">1</xref><xref rid="corresp1-15330338251380966" ref-type="corresp"/></contrib></contrib-group><aff id="aff1-15330338251380966">
<label>1</label>Department of Stomatology, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China</aff><aff id="aff2-15330338251380966">
<label>2</label>Guang'anmen Hospital, <institution-wrap><institution-id institution-id-type="Ringgold">71046</institution-id><institution content-type="university">China Academy of Chinese Medical Sciences</institution></institution-wrap>, Beijing, China</aff><aff id="aff3-15330338251380966">
<label>3</label>College of Information Engineering, <institution-wrap><institution-id institution-id-type="Ringgold">12529</institution-id><institution content-type="university">Sichuan Agricultural University</institution></institution-wrap>, Ya&#8217; an, China</aff><author-notes><corresp id="corresp1-15330338251380966">Huiping Lin, Department of Stomatology, The First Affiliated Hospital, Zhejiang University School of Medicine Hangzhou 310000, China. 
Email: <email>linhp@zju.edu.cn</email></corresp><corresp id="corresp2-15330338251380966">Fa Tian, College of Information Engineering, Sichuan Agricultural University, Ya&#8217; an 625000, China. 
Email: <email>ltf@stu.sicau.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>24</volume><issue-id pub-id-type="pmc-issue-id">479399</issue-id><elocation-id>15330338251380966</elocation-id><history><date date-type="received"><day>14</day><month>5</month><year>2025</year></date><date date-type="rev-recd"><day>23</day><month>7</month><year>2025</year></date><date date-type="accepted"><day>19</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>28</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><copyright-holder content-type="sage">SAGE Publications</copyright-holder><license><ali:license_ref specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 License (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>) which permits non-commercial use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access page (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://us.sagepub.com/en-us/nam/open-access-at-sage">https://us.sagepub.com/en-us/nam/open-access-at-sage</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="10.1177_15330338251380966.pdf"/><abstract><sec><title>Introduction</title><p>Oral squamous cell carcinoma (OSCC) is a prevalent and aggressive malignant tumor in the head and neck, known for its high metastatic rate and recurrence, posing serious threats to patients&#8217; lives. Relying solely on pathologists for diagnosis is time-consuming, labor-intensive, and prone to subjective bias. Therefore, developing artificial intelligence methods for automated detection is of significant clinical value and urgently needed.</p></sec><sec><title>Methods</title><p>We have developed a novel deep learning model based on an improved lightweight EfficientNetSwift, a lightweight deep learning framework designed to achieve precise and automated detection of pathological images of oral squamous cell carcinoma (OSCC). By comparing our model with mainstream models such as ResNet, MobileNet, and VIT, our model achieved superior performance in terms of precision, accuracy, and other metrics.</p></sec><sec><title>Results</title><p>In this study, EfficientNetSwift achieved the best results for detecting OSCC from pathological images, with 95.3% accuracy and an AUC of 0.99 using 20,180,050 parameters. This is half of ResNet's parameters and significantly fewer than VGG's, while only slightly more than MobileNet's but with better performance. The Swin Transformer performed the worst.</p></sec><sec><title>Conclusion</title><p>The automatic detection of OSCC using deep learning can significantly reduce labor costs and decrease the workload of clinicians. Additionally, it can assist doctors in diagnosing the disease more efficiently and accurately, providing precise prognostic predictions. This lays a solid foundation for the formulation of personalized treatment plans.</p></sec></abstract><kwd-group><kwd>oral squamous cell carcinoma</kwd><kwd>lightweight deep learning</kwd><kwd>EfficientNetSwift</kwd><kwd>pathological image detection</kwd><kwd>aided automated diagnosis</kwd></kwd-group><funding-group specific-use="FundRef"><award-group id="award1-15330338251380966"><funding-source id="funding1-15330338251380966"><institution-wrap><institution>the Natural Science Foundation of Zhejiang Province</institution></institution-wrap></funding-source><award-id rid="funding1-15330338251380966">LQ20H140007</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>typesetter</meta-name><meta-value>ts19</meta-value></custom-meta><custom-meta><meta-name>cover-date</meta-name><meta-value>January-December 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="section1-15330338251380966"><title>Introduction</title><p>Oral squamous cell carcinoma is one of the most common malignant tumors of the head and neck. It originates from a series of atypical hyperplasia of oral epithelial cells. According to statistics, there are about 355,000 new cases of patients worldwide every year, accounting for about 90% of oral malignancies. Oral squamous cell carcinoma usually occurs in the lips, buccal mucosa, floor of the mouth, soft and hard palate, tongue and gums.<sup>
<xref rid="bibr1-15330338251380966" ref-type="bibr">1</xref>
</sup> Seriously affect the patient's speech, swallowing, expression and other functions.<sup>
<xref rid="bibr2-15330338251380966" ref-type="bibr">2</xref>
</sup> The biological characteristics of oral squamous cell carcinoma make it highly invasive, with high lymph node metastasis rate and easy recurrence after operation. Therefore, the prognosis is poor, seriously affecting the life and health of patients, and the five-year survival rate after surgery is only 50%.<sup>
<xref rid="bibr3-15330338251380966" ref-type="bibr">3</xref>
</sup> Due to slow progress in early screening and diagnosis of oral squamous cell carcinoma, the overall death rate from oral squamous cell carcinoma has not decreased significantly since the 1980s. In fact, the 5-year survival rate of OCSCC decreases significantly with the progression of the disease, from 84% in the early stage (stages I and II) to 39% in the late stage (stages III and IV).<sup>
<xref rid="bibr4-15330338251380966" ref-type="bibr">4</xref>
</sup> Patients with advanced oral squamous cell carcinoma not only suffer great physical pain, but also face a heavy financial burden and poor quality of life after surgery.<sup>
<xref rid="bibr5-15330338251380966" ref-type="bibr">5</xref>
</sup></p><p>The traditional method for detecting OSCC is the pathological examination of histological tissue samples from the oral area under a microscope. Whole tissues and lesions are identified at low magnification, while cell morphology is identified at high magnification. However, the accuracy of this traditional pathological detection depends not only on the location, area, depth and quality of the biopsy, but also on the subjective judgment and technical level of the pathologist. It has the disadvantages of misdiagnosis, time consuming, late treatment, etc, so that patients are often already in serious condition when they receive treatment. Especially for patients with asymptomatic lesions, the best time for early intervention is often missed.<sup>
<xref rid="bibr6-15330338251380966" ref-type="bibr">6</xref>
</sup> In addition, there are certain errors in the process of collecting, transporting and storing biopsy samples. Therefore, the traditional pathological diagnosis has considerable limitations in the standardized diagnosis and treatment of oral squamous cell carcinoma.</p><p>In recent years, a number of studies have shown that the application of deep learning in the clinical diagnosis of oral squamous cell carcinoma can significantly improve the accuracy of diagnosis, reduce the workload of doctors, and greatly improve the early detection rate.<sup>
<xref rid="bibr7-15330338251380966" ref-type="bibr">7</xref>
</sup> Research has shown that advanced deep learning algorithms can help in the early identification and precise diagnosis of such lesions by using large data sets to identify tiny details that may indicate malignancy.<sup>
<xref rid="bibr8-15330338251380966" ref-type="bibr">8</xref>
</sup> In addition, deep learning has been shown to overcome the shortcomings of traditional diagnostic strategies with higher sensitivity and specificity in detecting OC and its progenitor cells.<sup>
<xref rid="bibr9-15330338251380966" ref-type="bibr">9</xref>
</sup> Improving the diagnostic accuracy can greatly improve the prognosis of patients, which allows us to conduct timely intervention and develop personalized treatment plans according to the individual conditions of each patient in clinical work.<sup>
<xref rid="bibr10-15330338251380966" ref-type="bibr">10</xref>
</sup> In addition, applying deep learning to the diagnosis and treatment of OC can further simplify the diagnostic procedure and maximize the use of resources. On this basis, it is hoped that health care services will be simplified.<sup>
<xref rid="bibr7-15330338251380966" ref-type="bibr">7</xref>
</sup> Currently, the lack of high-quality data required for the training and validation of efficient deep learning models is one of the major obstacles, and to ensure the robustness and generalization of models, extensive data sets covering many symptoms of oral cancer need to be collated.<sup>
<xref rid="bibr11-15330338251380966" ref-type="bibr">11</xref>
</sup> In addition, a major challenge is the need to ensure that deep learning algorithms are interpretable and transparent, especially in clinical situations where the reasoning behind treatment or diagnostic decisions needs to be easily understood.<sup>
<xref rid="bibr10-15330338251380966" ref-type="bibr">10</xref>
</sup> To overcome these barriers, interdisciplinary collaboration between researchers, physicians, and regulators is needed to create deep learning frameworks that are both clinically and ethically sound. It is believed that by integrating multiple data sources, such as genomic, proteomic and imaging data, it is possible to better understand the biological background of OC and create tailored treatments.<sup>
<xref rid="bibr9-15330338251380966" ref-type="bibr">9</xref>
</sup> Deep learning algorithms, combined with advances in imaging technology, can contribute to improving prognosis after surgery and guiding real-time decision making during surgery.<sup>
<xref rid="bibr8-15330338251380966" ref-type="bibr">8</xref>
</sup> Some scholars advocate that encouraging equitable access to deep learning technologies through collaborative projects and open source platforms could spur innovation and advance the discipline.<sup>
<xref rid="bibr12-15330338251380966" ref-type="bibr">12</xref>
</sup> Overall, despite the obstacles, the integration of deep machine learning in oral cancer treatment offers revolutionary prospects for improving diagnostic accuracy, prognostic accuracy, and treatment customization.</p><p>In this paper, we propose the EfficientNetSwift model, focusing on the precise and automated detection of oral squamous cell carcinoma (OSCC) through pathological images. Our approach innovatively combines three key techniques: Firstly, we incorporate a large number of batch normalization layers and optimize image size processing to accelerate convergence and enhance generalization capability. Secondly, we replace the original depthwise separable convolution modules with more straightforward 3&#8201;&#215;&#8201;3 convolutions, effectively reducing the model's parameter count. Finally, we introduce regularization methods such as Dropout, L1, and Mixup, further improving the model's adaptation and generalization to new data. These innovations collectively ensure that our model remains lightweight while efficiently and accurately detecting oral cancer. <xref rid="fig1-15330338251380966" ref-type="fig">Figure 1</xref> illustrates our workflow.</p><fig position="float" id="fig1-15330338251380966" orientation="portrait"><label>Figure 1.</label><caption><p>Working Flow Chart of This Study.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig1.jpg"/></fig></sec><sec sec-type="methods" id="section2-15330338251380966"><title>Methods</title><sec id="section2A-15330338251380966"><title>Data Acquisition</title><p>This study utilizes a publicly available dataset from the Kaggle platform(<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/mangalmanan/oscc-normal" ext-link-type="uri">https://www.kaggle.com/datasets/mangalmanan/oscc-normal</ext-link>), which includes two types of images: normal oral images and oral squamous cell carcinoma (OSCC) images. The dataset comprises a total of 5200 images, with 2500 normal oral images and 2700 OSCC images. For ease of processing and analysis, the images are organized into two subfolders: the &#8220;Normal&#8221; folder contains all normal oral images, while the &#8220;OSCC&#8221; folder stores all OSCC images. The pathological classification of all images is based on clinical diagnosis and histopathological evaluation to ensure high accuracy and reliability of the data. Normal oral images are sourced from healthy individuals showing no signs of cancer, whereas OSCC images come from patients pathologically confirmed to have oral squamous cell carcinoma. This rigorous classification method aims to enhance the accuracy of model training and ensure that the model can effectively learn the key features distinguishing the two pathological states. <xref rid="fig2-15330338251380966" ref-type="fig">Figure 2</xref> shows a portion of our dataset.</p><fig position="float" id="fig2-15330338251380966" orientation="portrait"><label>Figure 2.</label><caption><p>Part of the Data Set is Shown.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig2.jpg"/></fig></sec><sec id="section2B-15330338251380966"><title>Data Preprocessing</title><p>In this study, we implemented a series of comprehensive image preprocessing steps to enhance the quality of input data for the oral cancer detection model. First, we performed image cleaning to remove irrelevant background information and potential interfering elements, allowing for more accurate highlighting of cancerous tissues. Additionally, we employed denoising and filtering techniques to reduce random noise in the images and enhance their detailed features, thereby improving the recognizability of cancerous tissues. We also adjusted the image contrast to enhance visual clarity and strengthen the contrast between normal and abnormal tissues, making it easier for the model to distinguish these subtle differences. These meticulously designed preprocessing steps ensure that the deep learning model receives high-quality and consistent input data, thereby improving the model's accuracy and generalization capability in the task of automated oral cancer detection.</p><p>In addition, to improve the generalization ability and performance of the model, we implement data augmentation strategies on the training set and validation set images. In the training set, we first crop the image to 224&#8201;&#215;&#8201;224 pixels using random cropping, and then increase the diversity of the data using random horizontal flipping. The image is then converted into a PyTorch tensor and normalized by applying the mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5) to ensure a consistent data distribution. For the validation set, we resize the image to 224&#8201;&#215;&#8201;224 pixels and perform the same tensor conversion and normalization steps. To improve the model's generalization, we applied data augmentation strategies such as random cropping and horizontal flipping only to the training set. The validation set underwent standard preprocessing, including resizing and normalization, to ensure consistent evaluation conditions. These preprocessing and data augmentation measures are designed to optimize the training efficiency and evaluation accuracy of the model, ensuring its optimal performance in the task of oral cancer detection.</p></sec><sec id="section2C-15330338251380966"><title>Construction of Auxiliary Diagnostic Model</title><sec id="section2C1-15330338251380966"><title>Proposal of a Deep Learning-Based Model</title><p>In the field of oral cancer detection, deep learning technology has shown significant application potential. A classic reference model is EfficientNet, developed by Google.<sup>
<xref rid="bibr13-15330338251380966" ref-type="bibr">13</xref>
</sup> The model achieves state-of-the-art performance at the time while maintaining a low computational cost through a systematic model scaling approach. EfficientNet improves model accuracy and efficiency by balancing the network's width, depth, and resolution, introducing a new compound scaling method. Although EfficientNet has achieved excellent results on several standard datasets, it still has some limitations in specific applications such as medical image analysis and oral cancer detection: Firstly, while EfficientNet is designed for efficiency, its model size and the computational resources required still pose a challenge when processing large volumes of high-resolution medical images. Secondly, EfficientNet is not optimized for specific types of medical images, such as oral pathology images, which may prevent it from fully utilizing the unique features of these images. Lastly, despite its good performance across various tasks, the relatively large model size and computational resource demands of EfficientNet indicate that its lightweight nature still needs improvement for tasks involving specific medical images like oral cancer pathology. In view of these limitations, we propose a lightweight improved model based on EfficientNet, specifically optimized for oral cancer pathology images.</p><p>Our model is optimized and lightweight, based on the EfficientNet architecture, aiming to provide an efficient and accurate automatic cancer detection solution. The core advantages of this model lie in its innovative architectural design, meticulous parameter tuning, and specialized optimization for medical image analysis tasks. By adjusting the convolutional layers, batch normalization layers, SiLU activation function (Swish), and the distinctive Squeeze-and-Excite blocks, we have constructed a network that is both efficient and capable of effectively handling complex image tasks. To reduce model complexity, we employed standard 3&#8201;&#215;&#8201;3 convolutions instead of depthwise separable convolutions in the convolutional blocks, and we achieved model simplification by adjusting the expansion ratio and the number of filters. Additionally, we integrated regularization techniques such as DropPath (stochastic depth), Dropout, RandAugment, and Mixup, which effectively prevent overfitting and enhance the model's generalization ability to unseen data. Through these meticulous parameter adjustments, the entire model achieves an optimized balance between high performance and computational cost. <xref rid="fig3-15330338251380966" ref-type="fig">Figures 3a</xref> and <xref rid="fig3-15330338251380966" ref-type="fig">3b</xref> illustrate our model architecture.</p><fig position="float" id="fig3-15330338251380966" orientation="portrait"><label>Figure 3.</label><caption><p>(a) The Architecture of the Proposed EfficientNetSwift Model, Illustrating the Overall Network Design. the Model Incorporates Standard 3&#8201;&#215;&#8201;3 Convolutional Layers to Replace the Original Depthwise Separable Convolutions, Reducing Computational Complexity While Maintaining Strong Representational Capacity. (b) The Structure of the Integrated Squeeze-and-Excite (SE) Block, Which Enables Dynamic Channel-Wise Feature Recalibration by Modeling inter-Channel Dependencies.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig3.jpg"/></fig><p>Our model provides an efficient and accurate solution for the recognition and detection of oral squamous cell carcinoma (OSCC). This model integrates best practices in deep learning with targeted technological innovations, aiming to overcome common challenges in medical image analysis. Firstly, it reduces the model's parameter count and computational demands, enabling deployment in resource-constrained environments such as mobile devices and edge computing platforms. This is particularly important for extending advanced diagnostic tools to low-resource settings. Secondly, the model retains sufficient feature extraction capability while simplifying computation, which is crucial for accurately identifying subtle differences in OSCC. Additionally, the integration of DropPath, Dropout, RandAugment, and Mixup technologies enhances the model's generalization ability and reduces the risk of overfitting. This ensures that the model can make stable and accurate predictions when faced with the high variability of oral pathology images. Finally, the introduction of Squeeze-and-Excite blocks allows the model to dynamically adjust the importance of feature channels, focusing more on the regions of the image that are most significant for diagnosis. This attention mechanism is critical for improving the sensitivity and specificity of OSCC detection.</p></sec></sec><sec id="section2D-15330338251380966"><title>Comparative Models</title><p>AlexNet marked a significant early success in deep learning, with its outstanding performance in the ImageNet competition leading to subsequent technological developments. Although AlexNet was initially used for natural image classification, its success also inspired the exploration of deep learning applications in medical image analysis, such as using it as a feature extractor to assist in diagnosing diseases like oral cancer.<sup>
<xref rid="bibr14-15330338251380966" ref-type="bibr">14</xref>
</sup> The VGG network deepened the network by repeatedly using simple 3&#8201;&#215;&#8201;3 convolutional layers, demonstrating the critical role of network depth in enhancing image recognition performance. Its excellent feature extraction capability has led to its widespread application in the classification and recognition of medical images, aiding in identifying cancer-related image features.<sup>
<xref rid="bibr15-15330338251380966" ref-type="bibr">15</xref>
</sup> ResNet addressed the vanishing gradient problem in deeper networks by introducing residual connections, significantly improving the performance of image classification and recognition tasks. In the field of medical image analysis, ResNet is used for the automatic identification and classification of various types of cancer images, including oral cancer, due to its ability to capture complex pathological features.<sup>
<xref rid="bibr16-15330338251380966" ref-type="bibr">16</xref>
</sup> MobileNet was designed for mobile and embedded devices, achieving model lightweight and efficient computation through depthwise separable convolutions. It is suitable for quick diagnosis and real-time image analysis.<sup>
<xref rid="bibr17-15330338251380966" ref-type="bibr">17</xref>
</sup> ShuffleNet further enhanced network lightweight and computational efficiency through channel shuffling and group convolution techniques, making it an ideal choice for medical image analysis on resource-constrained devices.<sup>
<xref rid="bibr18-15330338251380966" ref-type="bibr">18</xref>
</sup> ViT introduced the Transformer architecture to the field of image classification by segmenting images into multiple patches and processing them using self-attention mechanisms, showing performance comparable to traditional CNN models. With its excellent performance in image classification, ViT's potential in medical image analysis, especially in fine pathological image classification like oral cancer detection, is being further explored.<sup>
<xref rid="bibr19-15330338251380966" ref-type="bibr">19</xref>
</sup> The Swin Transformer improved efficiency and performance in handling large-scale images by introducing a strategy of window partitioning and cross-window connections,<sup>
<xref rid="bibr20-15330338251380966" ref-type="bibr">20</xref>
</sup> providing new possibilities for medical image analysis.</p></sec><sec id="section2E-15330338251380966"><title>Experimental Setup</title><p>In this study, to ensure the accuracy and reliability of model performance, we conducted internal evaluations followed by testing and validation using an independent external dataset. Specifically, for the division of the dataset, we adhered to an 8:1:1 ratio, splitting it into training, validation, and testing sets to validate the model's generalization ability on unseen data. To achieve optimal model performance, all model parameters were finely tuned. The tuning process employed a combination of grid search and random search strategies to efficiently find the optimal parameter combinations within the parameter space. Additionally, we used early stopping and monitored the loss values on the validation set to prevent overfitting, ensuring the model's convergence and generalization ability. The experimental environment was set up as follows: all experiments were conducted on a high-performance computing platform equipped with an NVIDIA 4060Ti GPU with 32GB of memory and an Intel Xeon CPU. Our experimental software environment was based on the PyTorch deep learning framework, version 1.7.1, running on the Ubuntu 18.04 operating system. To ensure the reproducibility of the experiments, we also fixed all random seeds that could affect the experimental results, including those for PyTorch, NumPy, and Python itself.</p></sec><sec id="section2F-15330338251380966"><title>Model Evalution</title><p>In order to evaluate the effectiveness of the model, common classification indexes such as precision, recall, accuracy, F1 score, roc curve and confusion matrix were adopted. Accuracy is the most intuitive performance measure, representing the percentage of the total sample that the model correctly predicts. The accuracy rate is the proportion of the predicted positive examples that are actually positive. The recall rate is the proportion of samples that are actually positive cases that are correctly predicted to be positive cases. The F1 score is a harmonic average of accuracy and recall and is used to measure the balance between the two. The ROC curve shows the relationship between the model's true rate (TPR) and false positive rate (FPR) at different thresholds. Used to measure the overall performance of the model. The confusion matrix is a table that describes the relationship between model predictions and actual labels, including true cases (TP), false negative cases (FN), true negative cases (TN), and false positive cases (FP). Through the evaluation of the above indicators, we can comprehensively measure the performance of the model, and effectively evaluate and compare the model. The use of these indicators not only makes the evaluation results more objective and accurate, but also increases people's trust in the validity of the model. The relevant formula can be expressed as follows:<disp-formula id="disp-formula1-15330338251380966"><label>(1)</label><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-eq1.jpg"/><mml:math id="mml-disp1" display="block" overflow="scroll"><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula><disp-formula id="disp-formula2-15330338251380966"><label>(2)</label><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-eq2.jpg"/><mml:math id="mml-disp2" display="block" overflow="scroll"><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula><disp-formula id="disp-formula3-15330338251380966"><label>(3)</label><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-eq3.jpg"/><mml:math id="mml-disp3" display="block" overflow="scroll"><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula><disp-formula id="disp-formula4-15330338251380966"><label>(4)</label><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-eq4.jpg"/><mml:math id="mml-disp4" display="block" overflow="scroll"><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>x</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mi>x</mml:mi><mml:mspace width="0.25em"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p></sec></sec><sec sec-type="results" id="section3-15330338251380966"><title>Results</title><sec id="section3A-15330338251380966"><title>The Proposed Model Verification Results</title><p>During the training of the model, selecting appropriate hyperparameters, optimizers, learning rates, and batch sizes is crucial for the model's performance. Here is a detailed explanation of these key factors involved in the training process of our oral cancer detection model: We employed an adaptive learning rate adjustment strategy, setting the initial learning rate to 0.001. This initial value provides a good starting point for the model, being neither too large to cause unstable training nor too small to slow down the training process. To balance training efficiency and hardware resource constraints, we chose 64 as our batch size. This size is feasible on most modern GPUs and ensures sufficient data mixing and update efficiency. We selected the Adam optimizer for training the model. The Adam optimizer combines the advantages of both the momentum method and RMSProp, capable of automatically adjusting the learning rate, making it suitable for optimizing large-scale data and parameters. Since our task is a binary classification problem, we used Binary Cross-Entropy Loss as the loss function, which directly optimizes the probability outputs. <xref rid="fig4-15330338251380966" ref-type="fig">Figure 4</xref> shows the training iteration process of our model.</p><fig position="float" id="fig4-15330338251380966" orientation="portrait"><label>Figure 4.</label><caption><p>Improved Model Performance Display Diagram. (a) Training and Validation Loss and Accuracy, Showing Convergence; (b) ROC Curve and Confusion Matrix, Demonstrating Classification Accuracy.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig4.jpg"/></fig><p>In our task of detecting oral cancer from pathological images, the proposed new model has demonstrated exceptional performance. Through rigorous evaluation, the model achieved impressive results across multiple key performance metrics. Specifically, the model reached a precision of 97.5%, indicating that it is extremely accurate in identifying oral cancer images, with very few healthy samples being misclassified as cancerous. The recall was 97.4%, showing that the model effectively identifies the vast majority of oral cancer cases, avoiding missed diagnoses. The accuracy reached 95.3%, indicating high overall diagnostic consistency and reliability. Additionally, the F1 score was 95.4%, reflecting a good balance between precision and recall, ensuring robust performance in identifying both positive and negative samples. Through ROC curve analysis, the model's AUC (Area Under the Curve) value reached 0.99, demonstrating the model's outstanding ability to distinguish between oral cancer and normal images, maintaining high performance across different threshold settings. The confusion matrix results further confirmed the model's accuracy in the classification task, showing the specific performance of the model in predicting different categories, including the number of true positives, false positives, true negatives, and false negatives, further substantiating the model's diagnostic efficiency and accuracy.</p><p>To further validate the contribution of each architectural modification in EfficientNetSwift, we conducted an ablation study focusing on two key components: (1) the replacement of depthwise separable convolutions with standard 3&#8201;&#215;&#8201;3 convolutions, and (2) the introduction of advanced regularization strategies, including DropPath, Mixup, and Dropout. We constructed three model variants for this analysis: a baseline EfficientNet variant retaining the original depthwise convolution blocks (denoted as Baseline-EffNet), a version incorporating 3&#8201;&#215;&#8201;3 convolutions but excluding regularization (Swift-noReg), and the final proposed EfficientNetSwift with both enhancements. The results, summarized in Table S1, show that replacing depthwise convolutions with 3&#8201;&#215;&#8201;3 convolutions alone led to improvements in both F1-score (0.948 &#8594; 0.951) and accuracy (0.946 &#8594; 0.949), suggesting better spatial feature modeling and learning capacity. Further integration of regularization techniques yielded additional performance gains, pushing the F1-score to 0.954 and accuracy to 0.953. Notably, the combination of these two enhancements consistently outperformed each single component in isolation, indicating their synergistic effect on both robustness and generalizability. These findings justify our architectural design decisions and demonstrate the importance of combining structural simplification with effective regularization for pathological image classification tasks.</p></sec><sec id="section3B-15330338251380966"><title>Comparison Validation Results</title><p>In our study, in addition to the proposed new model, we also compared the performance of several classic and advanced deep learning models in the task of detecting oral cancer from pathological images. These comparison models include AlexNet, VGG, ResNet, MobileNet, ShuffleNet, Vision Transformer (ViT), and Swin Transformer. Compared to these models, our proposed new model outperformed all key metrics. <xref rid="table1-15330338251380966" ref-type="table">Table 1</xref> presents the results of the comparison models. The evaluation indicators include Precision, Recall, F1-score, Accuracy and Parameters. The EfficientNetSwift model showed excellent performance in all performance indicators, especially in Precision and Recall of 0.975 and 0.974, and F1-score and Accuracy of 0.954 and 0.953, respectively. It also has relatively low Parameters (20180050). <xref rid="fig5-15330338251380966" ref-type="fig">Figure 5</xref> and <xref rid="fig6-15330338251380966" ref-type="fig">Figure 6</xref> show the roc curve and confusion matrix of the comparison model, respectively.</p><fig position="float" id="fig5-15330338251380966" orientation="portrait"><label>Figure 5.</label><caption><p>Comparison of Model roc Curves. The ROC Curves of Eight Different Models for Oral Cancer Classification Were Presented. Where (a) is AlexNet, (b) is VGG, (c) is ResNet, (d) is MobileNet, (e) is ShuffleNet, (f) is VIT, (g) is Swin Transformer, and (h) is EfficientNet.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig5.jpg"/></fig><fig position="float" id="fig6-15330338251380966" orientation="portrait"><label>Figure 6.</label><caption><p>Model Confusion Matrix Comparison Diagram. The Confusion Matrix of Eight Different Models in the Oral Cancer Classification Task is Shown. Where (a) is AlexNet, (b) is VGG, (c) is ResNet, (d) is MobileNet, (e) is ShuffleNet, (f) is VIT, (g) is Swin Transformer, and (h) is EfficientNet.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig6.jpg"/></fig><table-wrap position="float" id="table1-15330338251380966" orientation="portrait"><label>Table 1.</label><caption><p>Summarizes the Performance Comparison of major Deep Learning Models on the Oral Cancer Image Classification Task.</p></caption><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-table1.jpg"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/><col align="left" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Precision</th><th align="left" rowspan="1" colspan="1">Recall</th><th align="left" rowspan="1" colspan="1">F1-score</th><th align="left" rowspan="1" colspan="1">Accuracy</th><th align="left" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">AlexNet</td><td rowspan="1" colspan="1">0.93</td><td rowspan="1" colspan="1">0.963</td><td rowspan="1" colspan="1">0.946</td><td rowspan="1" colspan="1">0.943</td><td rowspan="1" colspan="1">14585538</td></tr><tr><td rowspan="1" colspan="1">VGG</td><td rowspan="1" colspan="1">0.96</td><td rowspan="1" colspan="1">0.896</td><td rowspan="1" colspan="1">0.927</td><td rowspan="1" colspan="1">0.927</td><td rowspan="1" colspan="1">134268738</td></tr><tr><td rowspan="1" colspan="1">ResNet</td><td rowspan="1" colspan="1">0.964</td><td rowspan="1" colspan="1">0.93</td><td rowspan="1" colspan="1">0.946</td><td rowspan="1" colspan="1">0.945</td><td rowspan="1" colspan="1">42504258</td></tr><tr><td rowspan="1" colspan="1">MobileNet</td><td rowspan="1" colspan="1">0.923</td><td rowspan="1" colspan="1">0.909</td><td rowspan="1" colspan="1">0.916</td><td rowspan="1" colspan="1">0.913</td><td rowspan="1" colspan="1">2226434</td></tr><tr><td rowspan="1" colspan="1">ShuffleNet</td><td rowspan="1" colspan="1">0.942</td><td rowspan="1" colspan="1">0.909</td><td rowspan="1" colspan="1">0.926</td><td rowspan="1" colspan="1">0.924</td><td rowspan="1" colspan="1">1255654</td></tr><tr><td rowspan="1" colspan="1">VIT</td><td rowspan="1" colspan="1">0.897</td><td rowspan="1" colspan="1">0.889</td><td rowspan="1" colspan="1">0.893</td><td rowspan="1" colspan="1">0.889</td><td rowspan="1" colspan="1">86390786</td></tr><tr><td rowspan="1" colspan="1">Swin_transformer</td><td rowspan="1" colspan="1">0.799</td><td rowspan="1" colspan="1">0.554</td><td rowspan="1" colspan="1">0.654</td><td rowspan="1" colspan="1">0.696</td><td rowspan="1" colspan="1">86745274</td></tr><tr><td rowspan="1" colspan="1">EfficientNet</td><td rowspan="1" colspan="1">0.946</td><td rowspan="1" colspan="1">0.95</td><td rowspan="1" colspan="1">0.948</td><td rowspan="1" colspan="1">0.946</td><td rowspan="1" colspan="1">28344882</td></tr><tr><td rowspan="1" colspan="1">EfficientNetSwift</td><td rowspan="1" colspan="1">0.975</td><td rowspan="1" colspan="1">0.974</td><td rowspan="1" colspan="1">0.954</td><td rowspan="1" colspan="1">0.953</td><td rowspan="1" colspan="1">20180050</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="section3C-15330338251380966"><title>External Test Result</title><p>To further validate the generalization ability of our model, we used external tests with data at different magnifications from publicly available datasets (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://data.mendeley.com/datasets/ftmp4cvtmb/1" ext-link-type="uri">https://data.mendeley.com/datasets/ftmp4cvtmb/1</ext-link>). <xref rid="table2-15330338251380966" ref-type="table">Table 2</xref> shows the results of the external test. The EfficientNetSwift model maintained high performance, achieving a precision of 0.967, recall of 0.951, F1-score of 0.959, and accuracy of 0.977. These results indicate that EfficientNetSwift outperformed other models, further demonstrating its robustness and reliability in detecting oral squamous cell carcinoma across varied conditions. <xref rid="fig7-15330338251380966" ref-type="fig">Figure 7</xref> shows the external test performance process of our model.</p><fig position="float" id="fig7-15330338251380966" orientation="portrait"><label>Figure 7.</label><caption><p>External Test Performance Chart. The External Test Results of the Model Under Different Magnifications are Presented, Including the ROC Curve and Confusion Matrix. the ROC Curve (Left) Shows an AUC Value, and the Confusion Matrix (Right) Shows the Model's Performance on the Test Data, Including True Positives, True Negatives, False Positives, and False Negatives.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig7.jpg"/></fig><table-wrap position="float" id="table2-15330338251380966" orientation="portrait"><label>Table 2.</label><caption><p>Shows the Performance of Each Model in External Tests.</p></caption><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-table2.jpg"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/><col align="char" char="." span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Precision</th><th align="left" rowspan="1" colspan="1">Recall</th><th align="left" rowspan="1" colspan="1">F1-score</th><th align="left" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">AlexNet</td><td rowspan="1" colspan="1">0.733</td><td rowspan="1" colspan="1">0.726</td><td rowspan="1" colspan="1">0.729</td><td rowspan="1" colspan="1">0.85</td></tr><tr><td rowspan="1" colspan="1">VGG</td><td rowspan="1" colspan="1">0.91</td><td rowspan="1" colspan="1">0.826</td><td rowspan="1" colspan="1">0.866</td><td rowspan="1" colspan="1">0.927</td></tr><tr><td rowspan="1" colspan="1">ResNet</td><td rowspan="1" colspan="1">0.935</td><td rowspan="1" colspan="1">0.969</td><td rowspan="1" colspan="1">0.951</td><td rowspan="1" colspan="1">0.972</td></tr><tr><td rowspan="1" colspan="1">MobileNet</td><td rowspan="1" colspan="1">0.875</td><td rowspan="1" colspan="1">0.908</td><td rowspan="1" colspan="1">0.89</td><td rowspan="1" colspan="1">0.936</td></tr><tr><td rowspan="1" colspan="1">ShuffleNet</td><td rowspan="1" colspan="1">0.922</td><td rowspan="1" colspan="1">0.94</td><td rowspan="1" colspan="1">0.931</td><td rowspan="1" colspan="1">0.96</td></tr><tr><td rowspan="1" colspan="1">VIT</td><td rowspan="1" colspan="1">0.852</td><td rowspan="1" colspan="1">0.92</td><td rowspan="1" colspan="1">0.88</td><td rowspan="1" colspan="1">0.926</td></tr><tr><td rowspan="1" colspan="1">Swin_transformer</td><td rowspan="1" colspan="1">0.665</td><td rowspan="1" colspan="1">0.79</td><td rowspan="1" colspan="1">0.649</td><td rowspan="1" colspan="1">0.703</td></tr><tr><td rowspan="1" colspan="1">EfficientNet</td><td rowspan="1" colspan="1">0.943</td><td rowspan="1" colspan="1">0.898</td><td rowspan="1" colspan="1">0.918</td><td rowspan="1" colspan="1">0.956</td></tr><tr><td rowspan="1" colspan="1">EfficientNetSwift</td><td rowspan="1" colspan="1">0.967</td><td rowspan="1" colspan="1">0.951</td><td rowspan="1" colspan="1">0.959</td><td rowspan="1" colspan="1">0.977</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="section3D-15330338251380966"><title>Clinical Interpretability</title><p>The pathological diagnosis of oral squamous cell carcinoma (OSCC) primarily relies on the identification of characteristic cytological features, such as increased nuclear-cytoplasmic ratio, nuclear pleomorphism, hyperchromatism, keratin pearls, and abnormal mitotic figures. Pathologists are trained to examine these features across multiple fields of view under various magnifications, often requiring manual screening of over 100 high-power fields per case, depending on lesion complexity. This process is not only labor-intensive and time-consuming, but also subject to intra- and inter-observer variability, especially in borderline or poorly differentiated lesions.</p><p>To support interpretability, our model incorporates Gradient-weighted Class Activation Mapping (Grad-CAM) to generate activation heatmaps, which visualize the spatial regions that contribute most strongly to the classification output. These heatmaps are overlaid on the original histopathological images, highlighting the tissue zones the model &#8220;attends to&#8221; when predicting malignancy. In our analysis, we compared model-generated heatmaps with expert annotations on 50 randomly sampled OSCC test patches. Approximately 88% of the model's high-attention regions (&gt;0.7 activation intensity) overlapped with regions identified by senior pathologists as diagnostically relevant, such as areas containing dysplastic epithelial cells or invasive fronts.</p><p>This high degree of spatial alignment supports the model's clinical interpretability and its potential utility as an assistive tool. For example, in <xref rid="fig8-15330338251380966" ref-type="fig">Figure 8</xref>, the model accurately focuses on a peritumoral region with irregular nuclei and cell crowding, corresponding to areas marked as high-risk by human experts. Such interpretability allows clinicians to rapidly validate AI-generated predictions, localize suspect zones, and cross-reference with their own diagnostic criteria. Ultimately, heatmap-based visualization bridges the gap between black-box AI models and the evidence-based workflow of diagnostic pathology, enabling a synergistic approach that can reduce diagnostic latency and increase reproducibility across cases.</p><fig position="float" id="fig8-15330338251380966" orientation="portrait"><label>Figure 8.</label><caption><p>Activation Heatmap Visualization of Oral Squamous Cell Carcinoma (OSCC) Detection. Each Pair of Images Presents the Original Pathological Image on the Left and the Corresponding Grad-CAM-based Activation Heatmap on the Right. Warm-colored Areas (eg, Red and Yellow) Indicate Regions with High Model Attention, Often Corresponding to Histological Features Typical of Malignancy Such as Irregular Nuclei, Increased Mitotic Figures, and Disordered Epithelial Architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="10.1177_15330338251380966-fig8.jpg"/></fig></sec></sec><sec sec-type="discussion" id="section4-15330338251380966"><title>Discussion</title><p>In this study, we present a deep learning model for the automated and accurate detection of oral squamous cell carcinoma (OSCC) from pathological images. By incorporating lightweight architecture, advanced regularization, and medical-specific data augmentation, our model achieved strong performance: 97.5% precision, 97.4% recall, 95.3% accuracy, 95.4% F1 score, and an AUC of 0.99. It outperformed established models including AlexNet, VGG, ResNet, and Swin Transformer. External validation at varying magnifications confirmed its robustness, while activation heatmaps enhanced interpretability by highlighting clinically relevant regions, supporting faster and more accurate diagnosis.</p><p>The exceptional performance of our model in the detection of oral squamous cell carcinoma (OSCC) stems from its lightweight design and deep adaptation to the task's characteristics. By simplifying the network architecture and applying efficient convolutions, the model ensures low computational consumption while capturing detailed features, making it suitable for various computing environments. Additionally, the customized design of the model architecture, particularly tailored for the subtle and complex features in OSCC pathological images, enhances diagnostic accuracy. The training strategy includes specialized data augmentation and regularization techniques, which improve the model's ability to learn the unique cellular structures and morphological patterns of OSCC, thus boosting its robustness when faced with real-world medical image variability.</p><p>This work not only significantly saves time and effort, reduces costs, simplifies processes, and alleviates the workload of clinical doctors, but also overcomes the limitations of traditional diagnostic methods. It reduces the likelihood of missed diagnoses and misdiagnoses caused by doctor fatigue, subjectivity, and errors. Additionally, the powerful computational capabilities and use of big data help identify minute details that the human eye can easily overlook, greatly improving the early detection rate of OSCC. This truly enables &#8220;early detection, early diagnosis, and early treatment,&#8221; significantly improving patient prognosis and enhancing their quality of life.</p><p>In light of emerging advancements in robust medical AI design, our study aligns with a broader trend of developing lightweight yet diagnostically powerful deep learning models for image-based detection tasks. Similar to the approach taken by Batool et al (2025),<sup>
<xref rid="bibr21-15330338251380966" ref-type="bibr">21</xref>
</sup> who demonstrated the utility of deep learning in forensic radiology through robust model structures for bone profile estimation, our EfficientNetSwift model emphasizes compact architecture without compromising diagnostic performance, particularly in resource-limited clinical scenarios. Moreover, future iterations of our framework may benefit from incorporating architectural innovations aimed at enhancing data security and model resilience. For instance, the work by Gabr et al (2024)<sup>
<xref rid="bibr22-15330338251380966" ref-type="bibr">22</xref>
</sup> on memristive neural networks offers promising directions for integrating secure computation and real-time protection into medical AI systems. Additionally, as highlighted in the systematic review by Yee et al (2024),<sup>
<xref rid="bibr23-15330338251380966" ref-type="bibr">23</xref>
</sup> generalizability and adaptability remain key challenges in AI-based detection tasks, especially under conditions of data drift or rare case presentation. These concerns are directly relevant to OSCC diagnosis, and we plan to address them by expanding dataset diversity and exploring domain-adaptive learning strategies in future work.</p><p>Currently, this work is limited to the preliminary diagnosis and identification of OSCC. In the future, we aim to achieve more precise diagnosis and personalized treatment by differentiating between various subtypes and stages of OSCC based on an enriched database and clinical data. This study primarily relies on open-source OSCC pathological images, and future work may need to incorporate imaging examinations and clinical tests, utilizing multi-center data to further refine the algorithm model. At this stage, the work is still in the research and development phase and requires further validation and refinement. It needs to be integrated with clinical practice to ultimately develop related systems that can be translated into clinical applications, truly implementing them into clinical practice.</p><p>In addition to the quantitative performance of our model, we recognize the importance of assessing its clinical applicability in real-world settings. While the current study focuses on publicly available histopathological datasets with expert-confirmed labels, the actual clinical utility of the model requires further validation through physician involvement. To address this, we plan to initiate a pilot clinical study in collaboration with certified oral pathologists. This small-scale trial will involve blind testing of the model on newly collected biopsy slides from routine clinical practice, followed by comparative evaluation between model predictions and pathologist diagnoses. Furthermore, we will gather qualitative feedback from participating physicians regarding interpretability, diagnostic confidence, and integration feasibility into existing workflows. Such a validation process will not only enhance the practical credibility of our model but also provide valuable insights for refining its interface, explainability modules (eg, heatmaps), and real-time deployment potential. We believe this step is essential for bridging the gap between algorithmic performance and clinical translation.</p><p>To address the need for concrete future work, we have formulated three clear research directions. First, we plan to integrate multi-modal data sources&#8212;such as proteomic signatures and radiographic imaging&#8212;to enrich the feature space and improve model robustness. Second, we will initiate prospective clinical trials across three geographically distinct medical centers to evaluate the model's diagnostic performance in real-world settings. Third, we aim to develop an interactive visual explanation dashboard tailored for OSCC subtype-specific detection, enabling clinicians to interpret model decisions with greater clarity and confidence. These steps are critical to advancing the model toward clinical integration.</p></sec><sec id="section5-15330338251380966"><title>Conclusion</title><p>This study demonstrates that using deep learning to detect OSCC can accelerate the diagnostic process and facilitate timely treatment. The precise diagnostic process reduces human error and alleviates the workload of clinical doctors. It also helps address the issue of uneven distribution of clinical experts across different regions and hospitals, alleviating the real-world challenges posed by uneven medical resources for patients. Additionally, it can assist in evaluating the grading, staging, and prognosis of tumors, providing more information for clinical decision-making. Although this study currently has many limitations, future improvements are expected by incorporating multi-center data and integrating clinical data such as imaging, medical examinations, and patient treatment follow-ups. This will further refine the deep learning model, enhancing the accurate diagnosis of OSCC subtypes and achieving early detection, early diagnosis, early treatment, and personalized therapy.</p></sec><sec sec-type="supplementary-material" id="section6-15330338251380966" specific-use="figshare"><title>Supplemental Material</title><supplementary-material id="suppl1-15330338251380966" position="float" content-type="local-data" orientation="portrait"><caption><title>sj-docx-1-tct-10.1177_15330338251380966 - Supplemental material for EfficientNetSwift: A Lightweight and Precise Deep Learning Model for Detecting Oral Squamous Cell Carcinoma Using Pathological Images</title></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sj-docx-1-tct-10.1177_15330338251380966.docx" position="float" orientation="portrait"/><p>Supplemental material, sj-docx-1-tct-10.1177_15330338251380966 for EfficientNetSwift: A Lightweight and Precise Deep Learning Model for Detecting Oral Squamous Cell Carcinoma Using Pathological Images by Min Wu, MS, Yue Hu, PhD, Fa Tian, PhD, and Huiping Lin, PhD in Clinical Rehabilitation</p></supplementary-material></sec></body><back><ack><title>Acknowledgements</title><p>The authors thank all the participating patients and doctors.</p></ack><fn-group><fn fn-type="other"><p><bold>ORCID iD:</bold> Min Wu <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://orcid.org/0000-0002-0640-2284" ext-link-type="uri">https://orcid.org/0000-0002-0640-2284</ext-link></p></fn><fn fn-type="other"><p><bold>Ethical Considerations:</bold> Ethical approval and patient consent are not required for scoping review because the data are collected from publicly available publications.</p></fn><fn fn-type="con"><p><bold>Author Contributions:</bold> Min Wu: conceptualization, writing&#8212;original draft. Yue Hu: data curation, writing&#8212;original draft. Fa Tian: formal analysis, supervision, funding acquisition, writing&#8212;review editing. Huiping Lin: project administration, writing&#8212;review editing.</p></fn><fn fn-type="financial-disclosure"><p><bold>Funding:</bold> The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by the Natural Science Foundation of Zhejiang Province (grant numbers LQ20H140007).</p></fn><fn fn-type="COI-statement"><p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn><fn fn-type="other"><p><bold>Data and Materials Accessibility:</bold> In line with the criteria,the research datasets used in the study were supplied by the participating authors.</p></fn><fn fn-type="other"><p><bold>Supplemental Material:</bold> Supplemental material for this article is available online.</p></fn></fn-group><ref-list><title>Reference</title><ref id="bibr1-15330338251380966"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lousada-Fernandez</surname><given-names>F</given-names></name><name name-style="western"><surname>Rapado-Gonzalez</surname><given-names>O</given-names></name><name name-style="western"><surname>Lopez-Cedrun</surname><given-names>J-L</given-names></name></person-group>, <etal>et al.</etal><article-title>Liquid biopsy in oral cancer.</article-title><source>Int J Mol Sci.</source><year>2018</year>;<volume>19</volume>(<issue>6</issue>):<fpage>1704</fpage>. doi:<pub-id pub-id-type="doi">10.3390/ijms19061704</pub-id><pub-id pub-id-type="pmid">29890622</pub-id><pub-id pub-id-type="pmcid">PMC6032225</pub-id></mixed-citation></ref><ref id="bibr2-15330338251380966"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Huang</surname><given-names>M</given-names></name></person-group>, <etal>et al.</etal><article-title>Current status, opportunities, and challenges of exosomes in oral cancer diagnosis and treatment.</article-title><source>Int J Nanomed.</source><year>2022</year>;<fpage>2679</fpage>-<lpage>2705</lpage>. doi:<pub-id pub-id-type="doi">10.2147/IJN.S365594</pub-id><pub-id pub-id-type="pmcid">PMC9208818</pub-id><pub-id pub-id-type="pmid">35733418</pub-id></mixed-citation></ref><ref id="bibr3-15330338251380966"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chakraborty</surname><given-names>D</given-names></name><name name-style="western"><surname>Natarajan</surname><given-names>C</given-names></name><name name-style="western"><surname>Mukherjee</surname><given-names>A</given-names></name></person-group>. <article-title>Advances in oral cancer detection.</article-title><source>Adv Clin Chem.</source><year>2019</year>;<volume>91</volume>:<fpage>181</fpage>-<lpage>200</lpage>. doi:<pub-id pub-id-type="doi">10.1016/bs.acc.2019.03.006</pub-id><pub-id pub-id-type="pmid">31331489</pub-id></mixed-citation></ref><ref id="bibr4-15330338251380966"><label>4</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Howlader</surname><given-names>N</given-names></name><name name-style="western"><surname>Noone</surname><given-names>AM</given-names></name><name name-style="western"><surname>Krapcho</surname><given-names>M</given-names></name></person-group>, <etal>et al.</etal><comment>SEER cancer statistics review, 1975-2014 Bethesda, MD Natl Cancer Inst (2017)</comment>.</mixed-citation></ref><ref id="bibr5-15330338251380966"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hammerlid</surname><given-names>E</given-names></name><name name-style="western"><surname>Bjordal</surname><given-names>K</given-names></name><name name-style="western"><surname>Ahlner-Elmqvist</surname><given-names>M</given-names></name></person-group>, <etal>et al.</etal><article-title>A prospective study of quality of life in head and neck cancer patients. Part I: at diagnosis.</article-title><source>Laryngoscope.</source><year>2001</year>;<volume>111</volume>(<issue>4</issue>):<fpage>669</fpage>-<lpage>680</lpage>. doi:<pub-id pub-id-type="doi">10.1097/00005537-200104000-00021</pub-id><pub-id pub-id-type="pmid">11359139</pub-id></mixed-citation></ref><ref id="bibr6-15330338251380966"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Speight</surname><given-names>PM</given-names></name><name name-style="western"><surname>Epstein</surname><given-names>J</given-names></name><name name-style="western"><surname>Kujan</surname><given-names>O</given-names></name></person-group>, <etal>et al.</etal><article-title>Screening for oral cancer&#8212;a perspective from the global oral cancer forum.</article-title><source>Oral Surg Oral Med Oral Pathol Oral Radiol.</source><year>2017</year>;<volume>123</volume>(<issue>6</issue>):<fpage>680</fpage>-<lpage>687</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.oooo.2016.08.021</pub-id><pub-id pub-id-type="pmid">27727113</pub-id></mixed-citation></ref><ref id="bibr7-15330338251380966"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alabi</surname><given-names>RO</given-names></name><name name-style="western"><surname>Almangush</surname><given-names>A</given-names></name><name name-style="western"><surname>Elmusrati</surname><given-names>M</given-names></name><name name-style="western"><surname>M&#228;kitie</surname><given-names>AA</given-names></name></person-group>. <article-title>Deep machine learning for oral cancer: from precise diagnosis to precision medicine.</article-title><source>Front Oral Health</source>. <year>2022</year>;<volume>2</volume>:<fpage>794248</fpage>. doi:<pub-id pub-id-type="doi">10.3389/froh.2021.794248</pub-id><pub-id pub-id-type="pmid">35088057</pub-id><pub-id pub-id-type="pmcid">PMC8786902</pub-id></mixed-citation></ref><ref id="bibr8-15330338251380966"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heo</surname><given-names>J</given-names></name><name name-style="western"><surname>Lim</surname><given-names>JH</given-names></name><name name-style="western"><surname>Lee</surname><given-names>HR</given-names></name></person-group>, <etal>et al.</etal><article-title>Deep learning model for tongue cancer diagnosis using endoscopic images.</article-title><source>Sci Rep.</source><year>2022</year>;<volume>12</volume>(<issue>1</issue>):<fpage>6281</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41598-022-10287-9</pub-id><pub-id pub-id-type="pmid">35428854</pub-id><pub-id pub-id-type="pmcid">PMC9012779</pub-id></mixed-citation></ref><ref id="bibr9-15330338251380966"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Warin</surname><given-names>K</given-names></name><name name-style="western"><surname>Suebnukarn</surname><given-names>S</given-names></name></person-group>. <article-title>Deep learning in oral cancer-a systematic review.</article-title><source>BMC Oral Health</source>. <year>2024</year>;<volume>24</volume>(<issue>1</issue>):<fpage>212</fpage>. doi:<pub-id pub-id-type="doi">10.1186/s12903-024-03993-5</pub-id><pub-id pub-id-type="pmid">38341571</pub-id><pub-id pub-id-type="pmcid">PMC10859022</pub-id></mixed-citation></ref><ref id="bibr10-15330338251380966"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dixit</surname><given-names>S</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name><name name-style="western"><surname>Srinivasan</surname><given-names>K</given-names></name></person-group>. <article-title>A current review of machine learning and deep learning models in oral cancer diagnosis: recent technologies, open challenges, and future research directions.</article-title><source>Diagnostics</source>. <year>2023</year>;<volume>13</volume>(<issue>7</issue>):<fpage>1353</fpage>. doi:<pub-id pub-id-type="doi">10.3390/diagnostics13071353</pub-id><pub-id pub-id-type="pmid">37046571</pub-id><pub-id pub-id-type="pmcid">PMC10093759</pub-id></mixed-citation></ref><ref id="bibr11-15330338251380966"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>L&#243;pez-Cort&#233;s</surname><given-names>XA</given-names></name><name name-style="western"><surname>Matamala</surname><given-names>F</given-names></name><name name-style="western"><surname>Venegas</surname><given-names>B</given-names></name><name name-style="western"><surname>Rivera</surname><given-names>C</given-names></name></person-group>. <article-title>Machine-learning applications in oral cancer: a systematic review.</article-title><source>Appl Sci.</source><year>2022</year>;<volume>12</volume>(<issue>11</issue>):<fpage>5715</fpage>. doi:<pub-id pub-id-type="doi">10.3390/app12115715</pub-id></mixed-citation></ref><ref id="bibr12-15330338251380966"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>DW</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S</given-names></name><name name-style="western"><surname>Kwon</surname><given-names>S</given-names></name></person-group>, <etal>et al.</etal><article-title>Deep learning-based survival prediction of oral cancer patients.</article-title><source>Sci Rep.</source><year>2019</year>;<volume>9</volume>(<issue>1</issue>):<fpage>6994</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41598-019-43372-7</pub-id><pub-id pub-id-type="pmid">31061433</pub-id><pub-id pub-id-type="pmcid">PMC6502856</pub-id></mixed-citation></ref><ref id="bibr13-15330338251380966"><label>13</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>M</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q</given-names></name></person-group>. <conf-name>Efficientnet: Rethinking model scaling for convolutional neural networks</conf-name>. <conf-name>International conference on machine learning. PMLR</conf-name>; <conf-date>2019</conf-date>.</mixed-citation></ref><ref id="bibr14-15330338251380966"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A</given-names></name><name name-style="western"><surname>Sutskever</surname><given-names>I</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group>. <article-title>Imagenet classification with deep convolutional neural networks.</article-title><source>Adv Neural Inf Process Syst.</source><year>2012</year>;<volume>25</volume>.</mixed-citation></ref><ref id="bibr15-15330338251380966"><label>15</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A</given-names></name></person-group>. <conf-name>Very deep convolutional networks for large-scale image recognition</conf-name>. <conf-name>Proceedings of the International Conference on Learning Representations (ICLR)</conf-name>; <conf-date>2015</conf-date>.</mixed-citation></ref><ref id="bibr16-15330338251380966"><label>16</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S</given-names></name></person-group>, <etal>et al.</etal><conf-name>Deep residual learning for image recognition</conf-name>. <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>; <conf-date>2016</conf-date>.</mixed-citation></ref><ref id="bibr17-15330338251380966"><label>17</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sandler</surname><given-names>M</given-names></name><name name-style="western"><surname>Howard</surname><given-names>A</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M</given-names></name><name name-style="western"><surname>Zhmoginov</surname><given-names>A</given-names></name><name name-style="western"><surname>Chen</surname><given-names>LC</given-names></name></person-group>. <conf-name>Mobilenetv2: inverted residuals and linear bottlenecks</conf-name>. <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>; <year>2018</year>, <fpage>4510</fpage>-<lpage>4520</lpage>.
<pub-id pub-id-type="doi">10.1109/CVPR.2018.00474</pub-id></mixed-citation></ref><ref id="bibr18-15330338251380966"><label>18</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X</given-names></name><name name-style="western"><surname>Lin</surname><given-names>M</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J</given-names></name></person-group>. <conf-name>Shufflenet: an extremely efficient convolutional neural network for mobile devices</conf-name>. <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>; <conf-date>2018</conf-date>, <fpage>6848</fpage>-<lpage>6856</lpage>.</mixed-citation></ref><ref id="bibr19-15330338251380966"><label>19</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dosovitskiy</surname><given-names>A</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L</given-names></name><name name-style="western"><surname>Kolesnikov</surname><given-names>A</given-names></name></person-group><etal>, et al.</etal><conf-name>An image is worth 16&#215;16 words: Transformers for image recognition at scale</conf-name>. <conf-name>Proceedings of the International Conference on Learning Representations (ICLR)</conf-name>; <conf-date>2021</conf-date>.</mixed-citation></ref><ref id="bibr20-15330338251380966"><label>20</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y</given-names></name></person-group>, <etal>et al.</etal><conf-name>Swin transformer: Hierarchical vision transformer using shifted windows</conf-name>. <conf-name>Proceedings of the IEEE/CVF international conference on computer vision</conf-name>; <conf-date>2021</conf-date>.</mixed-citation></ref><ref id="bibr21-15330338251380966"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Batool</surname><given-names>M</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>F</given-names></name><name name-style="western"><surname>Rasheed</surname><given-names>H</given-names></name><name name-style="western"><surname>Khan</surname><given-names>MA</given-names></name></person-group>. <article-title>Forensic radiology: a robust approach to biological profile estimation from bone image analysis using deep learning</article-title>. <source>Biomed Signal Process Control.</source><year>2025</year>;<volume>105</volume>:<fpage>107661</fpage>. <pub-id pub-id-type="doi">10.1016/j.bspc.2025.107661</pub-id></mixed-citation></ref><ref id="bibr22-15330338251380966"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gabr</surname><given-names>MS</given-names></name><name name-style="western"><surname>El-Sayed</surname><given-names>H</given-names></name><name name-style="western"><surname>Nassar</surname><given-names>H</given-names></name></person-group>. <article-title>Data security utilizing a memristive coupled neural network in 3D models</article-title>. <source>IEEE Access</source>. <year>2024</year>;<volume>12</volume>:<fpage>116457</fpage>-<lpage>116477</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3447075</pub-id></mixed-citation></ref><ref id="bibr23-15330338251380966"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yee</surname><given-names>AC</given-names></name><name name-style="western"><surname>Lim</surname><given-names>KS</given-names></name><name name-style="western"><surname>Tan</surname><given-names>JP</given-names></name></person-group>. <article-title>A systematic literature review on AI-based methods and challenges in detecting zero-day attacks</article-title>. <source>IEEE Access</source>. <year>2024</year>;<volume>12</volume>:<fpage>144150</fpage>-<lpage>144163</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3455410</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>