<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473790</article-id><article-id pub-id-type="pmcid-ver">PMC12473790.1</article-id><article-id pub-id-type="pmcaid">12473790</article-id><article-id pub-id-type="pmcaiid">12473790</article-id><article-id pub-id-type="pmid">41012956</article-id><article-id pub-id-type="doi">10.3390/s25185718</article-id><article-id pub-id-type="publisher-id">sensors-25-05718</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Attention-Based Transfer Enhancement Network for Cross-Corpus EEG Emotion Recognition</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="Z">Zongni</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05718" ref-type="aff">1</xref><xref rid="af2-sensors-25-05718" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wong</surname><given-names initials="KY">Kin-Yeung</given-names></name><xref rid="af1-sensors-25-05718" ref-type="aff">1</xref><xref rid="c1-sensors-25-05718" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8022-7744</contrib-id><name name-style="western"><surname>Lam</surname><given-names initials="CT">Chan-Tong</given-names></name><xref rid="af1-sensors-25-05718" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Sato</surname><given-names initials="W">Wataru</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05718"><label>1</label>Faculty of Applied Sciences, Macao Polytechnic University, Macao 999078, China; <email>p2109651@mpu.edu.mo</email> (Z.L.); <email>ctlam@mpu.edu.mo</email> (C.-T.L.)</aff><aff id="af2-sensors-25-05718"><label>2</label>Guangxi Key Laboratory of Machine Vision and Intelligent Control, Wuzhou University, Wuzhou 543000, China</aff><author-notes><corresp id="c1-sensors-25-05718"><label>*</label>Correspondence: <email>anguskywong@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>13</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5718</elocation-id><history><date date-type="received"><day>15</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>02</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05718.pdf"/><abstract><p>A critical challenge in EEG-based emotion recognition is the poor generalization of models across different datasets due to significant domain shifts. Traditional methods struggle because they either overfit to source-domain characteristics or fail to bridge large discrepancies between datasets. To address this, we propose the Cross-corpus Attention-based Transfer Enhancement network (CATE), a novel two-stage framework. The core novelty of CATE lies in its dual-view self-supervised pre-training strategy, which learns robust, domain-invariant representations by approaching the problem from two complementary perspectives. Unlike single-view models that capture an incomplete picture, our framework synergistically combines: (1) Noise-Enhanced Representation Modeling (NERM), which builds resilience to domain-specific artifacts and noise, and (2) Wavelet Transform Representation Modeling (WTRM), which captures the essential, multi-scale spectral patterns fundamental to emotion. This dual approach moves beyond the brittle assumptions of traditional domain adaptation, which often fails when domains are too dissimilar. In the second stage, a supervised fine-tuning process adapts these powerful features for classification using attention-based mechanisms. Extensive experiments on six transfer tasks across the SEED, SEED-IV, and SEED-V datasets demonstrate that CATE establishes a new state-of-the-art, achieving accuracies from 68.01% to 81.65% and outperforming prior methods by up to 15.65 percentage points. By effectively learning transferable features from these distinct, synergistic views, CATE provides a robust framework that significantly advances the practical applicability of cross-corpus EEG emotion recognition.</p></abstract><kwd-group><kwd>cross-corpus</kwd><kwd>emotion recognition</kwd><kwd>domain adaptation</kwd><kwd>self-supervised learning</kwd></kwd-group><funding-group><award-group><funding-source>Guangxi Colleges and Universities Key Laboratory of Intelligent Software</funding-source><award-id>AD22080026</award-id></award-group><funding-statement>This research was funded by Guangxi Colleges and Universities Key Laboratory of Intelligent Software, and in part by the Guangxi Science and Technology Program under Grant No.AD22080026.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05718"><title>1. Introduction</title><p>Emotion recognition has become a cornerstone of affective computing and human&#8211;computer interaction, with profound implications for mental health monitoring, adaptive user interfaces, and clinical diagnosis [<xref rid="B1-sensors-25-05718" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05718" ref-type="bibr">2</xref>]. Among various physiological signals, electroencephalography (EEG) offers a direct window into the brain&#8217;s emotional processing. Its high temporal resolution and objective nature make it an ideal modality for capturing genuine emotional states, as EEG signals reflect neural activity that is difficult to consciously manipulate [<xref rid="B3-sensors-25-05718" ref-type="bibr">3</xref>]. Advances in deep learning have enabled models to achieve impressive accuracies, often exceeding 95%, on single, standardized datasets like DEAP and SJTU Emotion EEG Dataset (SEED) [<xref rid="B4-sensors-25-05718" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05718" ref-type="bibr">5</xref>].</p><p>However, this success in controlled settings masks a critical challenge that severely impedes real-world application: the poor generalization of models across different datasets, a problem known as the cross-corpus generalization challenge [<xref rid="B6-sensors-25-05718" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05718" ref-type="bibr">7</xref>]. When a model trained on one dataset is tested on another, its performance often plummets by 20&#8211;30% [<xref rid="B8-sensors-25-05718" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05718" ref-type="bibr">9</xref>]. This performance degradation arises from significant &#8220;domain shifts&#8221; caused by a combination of factors: inter-subject physiological variability (e.g., brain anatomy, skull thickness) and inter-dataset discrepancies in experimental protocols, emotion elicitation stimuli, recording equipment, and ambient noise [<xref rid="B3-sensors-25-05718" ref-type="bibr">3</xref>,<xref rid="B10-sensors-25-05718" ref-type="bibr">10</xref>]. This gap between laboratory performance and practical applicability remains the primary hurdle for deploying robust EEG-based emotion recognition systems.</p><p>To bridge this domain gap, researchers have explored a variety of computational strategies. Early attempts relied on conventional supervised learning, which failed to generalize due to overfitting to source-specific features. Subsequently, domain adaptation techniques [<xref rid="B11-sensors-25-05718" ref-type="bibr">11</xref>], such as Domain Adversarial Neural Networks, were introduced to align feature distributions between source and target domains [<xref rid="B5-sensors-25-05718" ref-type="bibr">5</xref>]. However, these methods often presuppose a closer proximity between domain distributions than typically exists in diverse EEG datasets, limiting their effectiveness. More recent efforts have incorporated advanced deep learning architectures. Attention mechanisms, particularly multi-head self-attention from Transformers, have shown promise in adaptively focusing on the most informative EEG channels and temporal segments [<xref rid="B4-sensors-25-05718" ref-type="bibr">4</xref>,<xref rid="B9-sensors-25-05718" ref-type="bibr">9</xref>]. Concurrently, self-supervised and contrastive learning paradigms have emerged as a powerful means to learn subject-invariant representations from unlabeled data, reducing the costly dependency on manual annotation [<xref rid="B3-sensors-25-05718" ref-type="bibr">3</xref>,<xref rid="B12-sensors-25-05718" ref-type="bibr">12</xref>]. These methods learn robust features by leveraging the inherent structure of EEG signals, for instance, by predicting temporal context or contrasting different augmentations of the same sample.</p><p>Despite these advancements, existing methods still exhibit significant limitations when faced with the complexities of cross-corpus EEG data. Each approach, while addressing a piece of the puzzle, fails to provide a comprehensive solution. The primary shortcomings of current state-of-the-art approaches are systematically summarized in <xref rid="sensors-25-05718-t001" ref-type="table">Table 1</xref>.</p><p>Traditional domain adaptation methods like DANN assume that source and target distributions can be aligned through adversarial training. However, this assumption breaks down in cross-corpus EEG scenarios due to: (1) hardware variations&#8212;different EEG systems use varying sampling rates, electrode materials, and amplifier characteristics, creating systematic differences in signal properties; (2) protocol differences&#8212;SEED uses movie clips while other datasets may use images or music, resulting in fundamentally different neural response patterns; (3) subject heterogeneity&#8212;age, cultural background, and individual brain anatomy create irreducible inter-subject variability. When these differences accumulate, forcing alignment between such disparate distributions can cause negative transfer, where the model learns spurious correlations that harm performance. To systematically address these multifaceted challenges identified in <xref rid="sensors-25-05718-t001" ref-type="table">Table 1</xref>, our proposed CATE framework introduces targeted solutions: (1) against overfitting in supervised learning, CATE employs two-stage training with self-supervised pre-training to learn generalizable features rather than memorizing source-specific patterns; (2) against domain adaptation&#8217;s distribution assumptions, CATE&#8217;s dual-view strategy learns inherently robust representations without forced alignment, where NERM models domain variations as noise while WTRM extracts domain-invariant multi-scale patterns; (3) against single-view limitations, CATE&#8217;s dual-view architecture simultaneously captures complementary information with NERM providing frequency-domain robustness and WTRM capturing temporal-frequency dynamics through wavelet decomposition; and (4) against augmentation dependency, CATE introduces EEG-specific strategies with NERM&#8217;s adaptive noise injection and WTRM&#8217;s wavelet-based augmentation that preserve emotional content while ensuring robust feature learning. This comprehensive design enables CATE to effectively overcome the limitations of existing approaches.</p><p>Several recent architectures have attempted to address these limitations through more sophisticated self-supervised and contrastive learning approaches. Generic contrastive methods adapted for EEG, including SimCLR [<xref rid="B13-sensors-25-05718" ref-type="bibr">13</xref>] and Mixup variants [<xref rid="B14-sensors-25-05718" ref-type="bibr">14</xref>], employ augmentation strategies unsuited for EEG&#8217;s non-stationary nature and operate on single-view representations, missing complementary multi-domain information. GMSS [<xref rid="B15-sensors-25-05718" ref-type="bibr">15</xref>] applies SimCLR-based contrastive learning with meiosis augmentation but focuses solely on temporal features, neglecting crucial spatial channel interactions. MV-SSTMA [<xref rid="B16-sensors-25-05718" ref-type="bibr">16</xref>] employs masked autoencoding across multiple domains but shows significant performance degradation in self-supervised settings. JCFA [<xref rid="B17-sensors-25-05718" ref-type="bibr">17</xref>] uses dual-view contrastive learning from time-frequency domains and incorporates graph convolution during fine-tuning to model channel relationships; however, it suffers from prohibitively long training times due to the computational complexity of graph operations and lacks explicit mechanisms for handling domain-specific noise while assuming similar cross-domain distributions&#8212;an assumption that often fails in practice.</p><p>These limitations motivate our CATE framework, which introduces a fundamentally different approach to domain adaptation. Unlike existing methods that attempt to force alignment between disparate domains or rely on computationally expensive graph structures, CATE addresses domain shift through two synergistic mechanisms: (1) Noise-Enhanced Representation Modeling (NERM) actively learns to filter out domain-specific artifacts by training on adaptively noise-perturbed data, forcing the model to focus on robust, emotion-relevant patterns rather than superficial recording characteristics; and (2) Wavelet Transform Representation Modeling (WTRM) captures domain-invariant differential entropy patterns across frequency bands that persist across different recording conditions and protocols. This dual-view architecture, combined with efficient attention-based feature fusion instead of graph convolution, enables CATE to achieve superior cross-corpus performance with significantly reduced computational overhead compared to existing approaches. To address these multifaceted challenges, this paper introduces a novel Cross-corpus Attention-based Transfer Enhancement network (CATE). Our approach is built on a comprehensive two-stage framework that synergistically combines self-supervised pre-training with supervised fine-tuning. The core of CATE is a dual-view pre-training strategy that learns complementary representations from two perspectives: (1) NERM, which builds robustness to domain-specific noise and artifacts, and (2) Wavelet Transform Representation Modeling (WTRM), which captures essential frequency-domain differential entropy characteristics of EEG signals. During fine-tuning, attention mechanisms fuse these complementary features to perform robust emotion classification. A multi-component loss function, integrating alignment, contrastive, and style diversity objectives, ensures that the learned features are consistent, discriminative, and generalizable.</p><p>This work makes several key contributions to cross-corpus EEG-based emotion recognition:<list list-type="bullet"><list-item><p>We propose a novel dual-view self-supervised pre-training strategy that comprehensively captures complementary signal characteristics&#8212;noise robustness and multi-scale frequency patterns&#8212;via the NERM and WTRM views, respectively.</p></list-item><list-item><p>We design a multi-component optimization objective that effectively balances feature consistency, discriminability, and diversity, providing a solid foundation for cross-corpus generalization.</p></list-item><list-item><p>We develop an attention-based transfer enhancement mechanism that dynamically fuses multi-scale features while maintaining computational efficiency.</p></list-item><list-item><p>Through extensive experiments on the SEED series datasets, we demonstrate that CATE achieves state-of-the-art performance on six cross-corpus tasks, with accuracy improvements of 5.42% to 15.65% over existing methods, thereby setting a new benchmark for robust, practical EEG-based emotion recognition.</p></list-item><list-item><p>We pioneer the comprehensive evaluation of cross-corpus emotion recognition across multiple SEED datasets (SEED, SEED-IV, and SEED-V), particularly introducing SEED-V with its challenging five-class emotion taxonomy into cross-corpus research for the first time. Through extensive experimentation comparing multiple state-of-the-art models on these diverse emotion classification tasks (3-class, 4-class, and 5-class), we establish new benchmarks for multi-class cross-corpus EEG emotion recognition and demonstrate the scalability of our approach to increasingly complex emotion taxonomies.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05718"><title>2. The Proposed CATE Architecture</title><sec id="sec2dot1-sensors-25-05718"><title>2.1. Overall Architecture</title><p>The overall architecture of CATE is shown in <xref rid="sensors-25-05718-f001" ref-type="fig">Figure 1</xref>. It consists of two stages: (1) a dual-path attention-driven self-supervised pre-training stage based on frequency band perturbation view and wavelet modulation view, and (2) a supervised fine-tuning stage.</p><p>In the pre-training stage, the model learns robust representations from two complementary views through a shared feature extractor consisting of linear layers with LayerNorm and GELU activation. This simplified architecture focuses on learning domain-invariant features through contrastive learning across the NERM and WTRM views.</p><p>In the fine-tuning stage, the outputs of the pre-trained dual-path attention encoders are fused through attention-weighted feature integration and then connected to a task-specific classifier, enabling the model to effectively focus on key emotion-related patterns in EEG signals and achieve more robust cross-corpus emotion recognition.</p><p>The pre-training architecture creates two distinct augmented views of EEG data through specialized enhancement modules, followed by direct feature extraction for robust representation learning. The framework generates two complementary views from the original EEG differential entropy features to capture diverse signal characteristics.</p></sec><sec id="sec2dot2-sensors-25-05718"><title>2.2. The Dual-Path Self-Supervised Pre-Training Stage</title><p>(1) the shared feature extractor ensures both views learn in the same feature space; (2) the alignment loss ensures cross-view consistency by minimizing the distance between representations of the same sample across views; (3) the style diversity loss prevents representation collapse and ensures the two views learn complementary rather than redundant features.</p><sec id="sec2dot2dot1-sensors-25-05718"><title>2.2.1. Noise-Enhanced Representation Modeling (NERM) View</title><p>Given the input EEG differential entropy features <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">B</italic> denotes the batch size, <italic toggle="yes">C</italic> represents the number of channels, and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the five frequency bands (delta, theta, alpha, beta, gamma), the NERM view introduces a simple yet effective adaptive noise mechanism. This process enhances the robustness of the learned representations against domain-specific variations.</p><p>The core of this view is to add Gaussian noise whose magnitude is dynamically scaled based on the input features. The procedure is as follows:</p><p>First, an input scale is computed based on the mean absolute value of the features:<disp-formula id="FD1-sensors-25-05718"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>input</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>mean</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>|</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Next, this input scale is used to determine the noise scale, which is capped to prevent excessive perturbation:<disp-formula id="FD2-sensors-25-05718"><label>(2)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>noise</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi>s</mml:mi><mml:mi>input</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> is a small constant (e.g., <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) to ensure numerical stability.</p><p>Finally, the noise-enhanced features <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>NERM</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are generated by adding scaled Gaussian noise to the original features:<disp-formula id="FD3-sensors-25-05718"><label>(3)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>NERM</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi>s</mml:mi><mml:mi>noise</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents standard Gaussian noise.</p><p>This adaptive noise injection strategy forces the model to learn features that are less sensitive to minor variations and artifacts present in the input. By dynamically adjusting the noise level relative to the input&#8217;s magnitude, it ensures that the perturbation is meaningful without corrupting the underlying signal structure. This process effectively regularizes the model, discouraging it from overfitting to spurious, domain-specific characteristics and thereby improving its generalization capability for cross-corpus emotion recognition tasks.</p></sec><sec id="sec2dot2dot2-sensors-25-05718"><title>2.2.2. Wavelet Transform Representation Modeling (WTRM) View</title><p>The WTRM view employs wavelet decomposition to capture multi-scale spectral energy distribution patterns across frequency bands:</p><p>For each sample <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>62</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> channels and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> frequency bands, the wavelet decomposition is applied to the spectral energy profile of each channel:<disp-formula id="FD4-sensors-25-05718"><label>(4)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">c</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">c</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>DWT</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>&#968;</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the differential entropy features across five frequency bands for channel <italic toggle="yes">c</italic>, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">c</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> captures the overall spectral energy trend, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">c</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> captures the inter-band energy variations, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi>&#968;</mml:mi></mml:mrow></mml:math></inline-formula> is the wavelet basis function (Haar or db1 due to signal length constraint), and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the decomposition level (limited by <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>Enhanced coefficients are generated through adaptive masking and perturbation:<disp-formula id="FD5-sensors-25-05718"><label>(5)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">c</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8857;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05718"><label>(6)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">c</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8857;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.5</mml:mn><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi>&#963;</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are random binary masks with probabilities <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the mask probability, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are enhancement factors, and &#8857; denotes element-wise multiplication.</p><p>The WTRM view is reconstructed via an inverse wavelet transform:<disp-formula id="FD7-sensors-25-05718"><label>(7)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>IDWT</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#968;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This decomposition captures the spectral energy distribution patterns that are characteristic of different emotional states, where the approximation coefficients represent the overall brain activation level and the detail coefficients capture the coordination between adjacent frequency bands, providing complementary information to the NERM view for robust emotion recognition.</p></sec><sec id="sec2dot2dot3-sensors-25-05718"><title>2.2.3. Shared Feature Extraction</title><p>Both augmented views are processed through a shared feature extractor that directly maps the input to the latent representation space:<disp-formula id="FD8-sensors-25-05718"><label>(8)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>GELU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are learnable weight matrices, and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are bias vectors.</p><p>The features from both views are directly obtained through the shared extractor:<disp-formula id="FD9-sensors-25-05718"><label>(9)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05718"><label>(10)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represent the extracted features from NERM and WTRM views, respectively.</p></sec><sec id="sec2dot2dot4-sensors-25-05718"><title>2.2.4. Multi-Component Loss Function</title><p>The self-supervised pre-training optimizes three complementary loss components operating on the directly extracted features:<list list-type="simple"><list-item><label>(1)</label><p>Alignment Loss: This component ensures cross-view consistency by aligning representations from the same sample across different views in the feature space:<disp-formula id="FD11-sensors-25-05718"><label>(11)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">&#8741;</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo stretchy="true">&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="true">&#8741;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represent L2-normalized features from NERM and WTRM views, respectively. This loss function promotes semantic consistency between complementary views while preserving view-specific characteristics.</p></list-item><list-item><label>(2)</label><p>Style Diversity Loss: This component prevents representation collapse by enforcing orthogonality between different samples within the same view:<disp-formula id="FD12-sensors-25-05718"><label>(12)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="true">&#8741;</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo stretchy="true">&#8741;</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="true">&#8741;</mml:mo><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo stretchy="true">&#8741;</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05718"><label>(13)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:math></inline-formula> is the identity matrix, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the Frobenius norm. By constraining the Gram matrices to approximate the identity matrix, this loss effectively maintains feature diversity and prevents dimensional collapse during training.</p></list-item><list-item><label>(3)</label><p>Total Pre-training Loss: The complete pre-training objective combines the alignment and diversity components with appropriate weighting:<disp-formula id="FD14-sensors-25-05718"><label>(14)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are balancing coefficients that control the relative importance of alignment and diversity objectives. The combination of these two loss components forms the complete optimization objective for our self-supervised pre-training stage.</p></list-item></list></p><p>Through this multi-component loss framework, NERM and WTRM views form a synergistic interaction during pre-training: NERM focuses on learning representations robust to domain-specific noise and artifacts, while WTRM captures multi-scale spectral energy distribution patterns across frequency bands through wavelet decomposition. The alignment loss ensures both views produce consistent high-level semantic understanding of the same emotional state, while the style loss maintains feature diversity, preventing the model from simply learning identical representations. This dual-view approach enables the model to capture both noise-robust features and emotion-relevant spectral characteristics simultaneously.</p></sec></sec><sec id="sec2dot3-sensors-25-05718"><title>2.3. Supervised Fine-Tuning Stage</title><p>During the supervised fine-tuning stage, the pre-trained Shared Feature Extractor serves as the backbone. Its learned weights are used to initialize two parallel encoders: the NERM View Encoder and the WTRM View Encoder. Each encoder is then augmented with its own multi-head attention mechanism, allowing the model to adapt the powerful, pre-trained representations for the specific downstream classification task.</p><p>The model input consists of standard Differential Entropy (DE) features, extracted from EEG signals that have been downsampled to 200 Hz and band-pass filtered (1&#8211;75 Hz). To adapt these sample-based DE features for our Transformer-based architecture, we treat each vector as a sequence of length one (<inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). An input batch of shape <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>input</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is reshaped to <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>input</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and processed through the initialized encoders. As illustrated in <xref rid="sensors-25-05718-f002" ref-type="fig">Figure 2</xref>, the refined features from the parallel NERM and WTRM encoders are then fused to generate a joint representation for classification. This strategy allows the model to effectively leverage the powerful pre-trained architecture with non-sequential input data.</p><p>This strategy allows the model to effectively leverage the powerful pre-trained architecture, even with input data that is not inherently sequential. The proposed dual-view encoder architecture is illustrated in <xref rid="sensors-25-05718-f002" ref-type="fig">Figure 2</xref>. This architecture employs two parallel encoders, NERM and WTRM, to process EEG feature representations from different domains, ultimately generating a joint representation through feature fusion for emotion classification.</p><sec id="sec2dot3dot1-sensors-25-05718"><title>2.3.1. Multi-Head Attention Mechanism</title><p>The core multi-head attention mechanism operates on the pre-trained features through learned query, key, and value projections:<disp-formula id="FD15-sensors-25-05718"><label>(15)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">H</mml:mi><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi mathvariant="bold">K</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">H</mml:mi><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">H</mml:mi><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>model</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (with <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in our case) represents the input feature sequence, and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>model</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>model</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are learnable projection matrices.</p><p>The scaled dot-product attention is computed as:<disp-formula id="FD16-sensors-25-05718"><label>(16)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>model</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">h</italic> denotes the number of attention heads.</p><p>Multi-head attention concatenates outputs from multiple attention heads:<disp-formula id="FD17-sensors-25-05718"><label>(17)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>MultiHead</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>head</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>head</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>O</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>head</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Q</mml:mi><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>O</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>model</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>model</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the output projection matrix.</p></sec><sec id="sec2dot3dot2-sensors-25-05718"><title>2.3.2. NERM View Encoder</title><p>The NERM view adaptive encoder processes noise-enhanced representations through dual attention mechanisms to capture frequency-aware patterns:<disp-formula id="FD18-sensors-25-05718"><label>(18)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi>MultiHead</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD19-sensors-25-05718"><label>(19)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The final NERM representation is obtained through global average pooling:<disp-formula id="FD20-sensors-25-05718"><label>(20)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>GlobalAvgPool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3dot3-sensors-25-05718"><title>2.3.3. WTRM View Encoder</title><p>The WTRM view encoder employs dual attention mechanisms to capture spectral energy distribution patterns from wavelet-enhanced representations.<disp-formula id="FD21-sensors-25-05718"><label>(21)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi>MultiHead</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is a scaling factor for residual connections to ensure training stability.</p><p>The combined representation is normalized:<disp-formula id="FD22-sensors-25-05718"><label>(22)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The representation is further refined through a feed-forward network with scaled residual connection:<disp-formula id="FD23-sensors-25-05718"><label>(23)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mi>FFN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where FFN consists of:<disp-formula id="FD24-sensors-25-05718"><label>(24)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>FFN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Dropout</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mi>GELU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Dropout</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
with <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>After layer normalization:<disp-formula id="FD25-sensors-25-05718"><label>(25)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The final WTRM representation for fine-tuning is obtained through adaptive pooling:<disp-formula id="FD26-sensors-25-05718"><label>(26)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>GlobalAvgPool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3dot4-sensors-25-05718"><title>2.3.4. Cross-Domain Feature Fusion</title><p>The enhanced features from the NERM and WTRM views, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, are combined to form a final representation. While complex fusion strategies like attention-weighting are possible, we opted for simple averaging:<disp-formula id="FD27-sensors-25-05718"><label>(27)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This choice was based on a combination of empirical evidence, stability, and simplicity. As detailed in <xref rid="app2-sensors-25-05718" ref-type="app">Appendix B</xref>, we conducted experiments on the SEED-<inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo>&#8594;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> SEED-<inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> task to evaluate different fixed weighting ratios (see <xref rid="sensors-25-05718-t0A5" ref-type="table">Table A5</xref>).</p></sec><sec id="sec2dot3dot5-sensors-25-05718"><title>2.3.5. Feature Fusion and Classification</title><p>Classification is performed by a multi-layer perceptron (MLP) head that takes the fused features <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>fused</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as input. The MLP projects the features into a hidden representation, applies a GELU activation and dropout for regularization, and finally produces the output logits for classification.<disp-formula id="FD28-sensors-25-05718"><label>(28)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>GELU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD29-sensors-25-05718"><label>(29)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Dropout</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD30-sensors-25-05718"><label>(30)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">logits</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>hidden</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>model</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>output</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>hidden</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are learnable weight matrices.</p></sec><sec id="sec2dot3dot6-sensors-25-05718"><title>2.3.6. Fine-Tuning Objective</title><p>The fine-tuning stage adapts the model to the downstream classification task by optimizing a supervised objective. The loss function, therefore, consists of a standard cross-entropy classification loss:<disp-formula id="FD31-sensors-25-05718"><label>(31)</label><mml:math id="mm81" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>fine</mml:mi><mml:mo>-</mml:mo><mml:mi>tune</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>cls</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The classification loss is defined as the cross-entropy between the predicted logits and the one-hot encoded true labels:<disp-formula id="FD32-sensors-25-05718"><label>(32)</label><mml:math id="mm82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>CrossEntropy</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">logits</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>true</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3dot7-sensors-25-05718"><title>2.3.7. Training Strategy</title><p>The fine-tuning process employs several standard optimization strategies to ensure stable and efficient training:<list list-type="simple"><list-item><label>(1)</label><p>Mixed Precision Training: We employ automatic mixed precision (AMP) with gradient scaling to accelerate the fine-tuning process while maintaining numerical stability.</p></list-item><list-item><label>(2)</label><p>Gradient Clipping: To prevent the issue of exploding gradients, the norms of the gradients are clipped to a maximum threshold of 1.0, calculated as:<disp-formula id="FD33-sensors-25-05718"><label>(33)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>clipped</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi mathvariant="bold">g</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mi mathvariant="bold">g</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><label>(3)</label><p>Learning Rate Scheduling: A <monospace>ReduceLROnPlateau</monospace> scheduler is utilized with a factor of 0.5 and a patience of 5 epochs. This scheduler monitors the total loss and adaptively adjusts the learning rate when improvements stagnate.</p></list-item></list></p><p>This fine-tuning strategy allows the model to effectively adapt the learned representations for the emotion classification task. The combination of a direct classification objective with robust optimization techniques ensures that the model is efficiently tailored to the downstream task.</p></sec></sec></sec><sec id="sec3-sensors-25-05718"><title>3. Experimental Setting</title><sec id="sec3dot1-sensors-25-05718"><title>3.1. Datasets</title><p>The SEED series of datasets is an emotional electroencephalogram (EEG) database established by the BCMI Laboratory at Shanghai Jiao Tong University. This series includes three main datasets, each targeting classification for a different number of emotional categories.</p><p>The SEED dataset (original version) focuses on three-category emotion recognition (negative, neutral, positive). It involved 15 subjects participating in 3 experimental sessions, each watching 15 emotion-inducing film clips. EEG signals were recorded using a 62-channel ESI Neuroscan system, and various EEG features such as differential entropy (DE) and differential asymmetry (DASM) were extracted.</p><p>The SEED-IV dataset expands to four-category classification (neutral, sad, fear, and happy emotions). It also involved 15 subjects participating in 3 experimental sessions, but each session included 24 film clips.</p><p>The SEED-V dataset further expands to five-category classification (disgust, fear, sad, neutral, happy). Each subject participated in 3 experimental sessions, watching 15 film clips per session, with both EEG and eye-tracking data recorded.</p></sec><sec id="sec3dot2-sensors-25-05718"><title>3.2. Pre-Processing</title><p>After collecting the raw EEG data, signal preprocessing and feature extraction performed. To increase the signal-to-noise ratio (SNR), the raw EEG signals are first downsampled to a 200 <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mi>Hz</mml:mi></mml:mrow></mml:math></inline-formula> sampling rate, followed by a band-pass filter from 1 <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mi>Hz</mml:mi></mml:mrow></mml:math></inline-formula> to 75 <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mi>Hz</mml:mi></mml:mrow></mml:math></inline-formula>. Subsequently, feature extraction is carried out. Differential Entropy (DE), as referenced in the literature [<xref rid="B18-sensors-25-05718" ref-type="bibr">18</xref>], has the ability to distinguish patterns in different frequency bands. Therefore, we chose DE features as the input data for our model.</p><p>For one subject in one session in these three databases, a set of data is given in the form of channels (62) &#215; trials (15 for SEED, 24 for SEED-IV, 15 for SEED-V) &#215; frequency bands (5). We merge the channels and frequency bands, changing the form to trials &#215; 310 (<inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>62</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>Finally, all data are organized into <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3394</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>310</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (SEED dataset,3394 samples per subject per session), or <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>851</mml:mn><mml:mo>/</mml:mo><mml:mn>832</mml:mn><mml:mo>/</mml:mo><mml:mn>822</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>310</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (SEED-IV dataset,851/832/822 samples per subject per session, respectively), or <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>681</mml:mn><mml:mo>/</mml:mo><mml:mn>541</mml:mn><mml:mo>/</mml:mo><mml:mn>601</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>310</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (SEED-V dataset,681/541/ 601 samples per subject per session, respectively). The corresponding label vectors are generated in the form of <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3394</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, or <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>851</mml:mn><mml:mo>/</mml:mo><mml:mn>832</mml:mn><mml:mo>/</mml:mo><mml:mn>822</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, or <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>681</mml:mn><mml:mo>/</mml:mo><mml:mn>541</mml:mn><mml:mo>/</mml:mo><mml:mn>601</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot3-sensors-25-05718"><title>3.3. Evaluation Metrics</title><p>Accuracy is used as a measure to evaluate the EEG classification, which is calculated as follows:<disp-formula id="FD34-sensors-25-05718"><label>(34)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>acc</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>A</mml:mi><mml:mi>Num</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">A</italic> refers to the number of samples correctly classified by the algorithm, and Num refers to the total number of samples to be classified.</p></sec></sec><sec id="sec4-sensors-25-05718"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05718"><title>4.1. Experimental Setup</title><sec id="sec4dot1dot1-sensors-25-05718"><title>4.1.1. Datasets and Baselines</title><p>We evaluated our proposed model, CATE, on three publicly available EEG emotion recognition datasets: SEED [<xref rid="B19-sensors-25-05718" ref-type="bibr">19</xref>], SEED-IV [<xref rid="B20-sensors-25-05718" ref-type="bibr">20</xref>], and SEED-V [<xref rid="B21-sensors-25-05718" ref-type="bibr">21</xref>]. We conducted a comprehensive comparison against five state-of-the-art (SOTA) methods: DANN [<xref rid="B22-sensors-25-05718" ref-type="bibr">22</xref>], GECNN [<xref rid="B23-sensors-25-05718" ref-type="bibr">23</xref>], E2STN [<xref rid="B24-sensors-25-05718" ref-type="bibr">24</xref>], JCFA [<xref rid="B17-sensors-25-05718" ref-type="bibr">17</xref>], and Mixup [<xref rid="B14-sensors-25-05718" ref-type="bibr">14</xref>]. For fair comparison, all baselines were implemented using their officially reported hyperparameters.</p></sec><sec id="sec4dot1dot2-sensors-25-05718"><title>4.1.2. Evaluation Protocol</title><p>We followed a strict subject-independent, cross-corpus protocol with supervised target-domain fine-tuning applied consistently across all methods. Importantly, all baseline models and our proposed CATE method were evaluated under the identical protocol, where target domain labels were used during fine-tuning. The experiments were conducted across six transfer tasks between the datasets. We use accuracy (ACC) and its standard deviation across all subjects in the test set as the primary evaluation metric. The specific trial splits for fine-tuning and testing in each task were as follows:<list list-type="bullet"><list-item><p>SEED &#8596; SEED-<inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>: For SEED, 9 trials were used for fine-tuning and 6 for testing. For SEED-IV (3-class), 12 trials were used for fine-tuning and 6 for testing.</p></list-item><list-item><p>SEED &#8596; SEED-<inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>: For SEED, 9 trials for fine-tuning, 6 for testing. For SEED-V (3-class subset), 6 trials for fine-tuning, 3 for testing.</p></list-item><list-item><p>SEED-<inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>&#8596; SEED-<inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>: For SEED-IV (4-class), 16 trials for fine-tuning, 8 for testing. For SEED-V (4-class subset), 8 trials for fine-tuning, 4 for testing.</p></list-item></list></p></sec><sec id="sec4dot1dot3-sensors-25-05718"><title>4.1.3. Implementation Details</title><p>Models were implemented in Python 3.8 using PyTorch 1.9.0 and trained on an NVIDIA Tesla A10 GPU. The self-supervised pre-training was conducted for 200 epochs with a batch size of 256. The subsequent fine-tuning was performed for 20 epochs with a batch size of 128.</p></sec></sec><sec id="sec4dot2-sensors-25-05718"><title>4.2. Cross-Corpus Performance Analysis</title><p><xref rid="sensors-25-05718-t002" ref-type="table">Table 2</xref> presents the performance comparison across all six cross-corpus tasks. Our proposed CATE model consistently and significantly outperforms all baseline methods, establishing a new state-of-the-art. Notably, CATE demonstrates not only higher mean accuracy but also lower standard deviation in most cases, indicating superior performance and stability.</p><p>For instance, in the SEED &#8594; SEED-<inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and SEED-<inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>&#8594; SEED tasks, CATE achieves accuracies of 80.72% and 80.70%, surpassing the second-best method, JCFA, by a large margin of 15.20% and 14.37%, respectively. This substantial improvement highlights the effectiveness of our dual-view pre-training strategy. The Noise-Enhanced Representation Modeling (NERM) view learns to discard domain-specific noise stemming from different acquisition hardware, while the Wavelet Transform Representation Modeling (WTRM) view captures domain-invariant, multi-scale temporal-frequency patterns crucial for emotion recognition. The synergy between these views enables the model to learn robust and generalizable EEG representations.</p><p>The performance gains are also evident in the more challenging tasks involving SEED-V, which features a more complex emotion taxonomy. In the SEED-<inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>&#8594; SEED-<inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> task, CATE achieves a 13.41% improvement over JCFA. This scenario, involving transfer between two distinct 4-class paradigms, underscores CATE&#8217;s ability to handle complex domain shifts and partially overlapping emotion categories.</p><p>Further analysis of the confusion matrices (Figure <xref rid="sensors-25-05718-f003" ref-type="fig">Figure 3</xref>, <xref rid="sensors-25-05718-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-05718-f005" ref-type="fig">Figure 5</xref>) reveals CATE&#8217;s discriminative power. For example, in the SEED &#8594; SEED-<inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> task (Figure a), CATE significantly reduces the confusion between &#8216;positive&#8217; and &#8216;neutral&#8217; emotions compared to JCFA (Figure b). Similarly, in the challenging SEED-<inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>&#8594; SEED-<inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> experiment (<xref rid="sensors-25-05718-f005" ref-type="fig">Figure 5</xref>c), CATE improves the recognition of &#8216;fear&#8217; and reduces its confusion with &#8216;sad&#8217;, suggesting its capability to distinguish between nuanced emotional states. This confirms that our model learns more discriminative features, rather than just overfitting to the source domain&#8217;s characteristics.</p></sec><sec id="sec4dot3-sensors-25-05718"><title>4.3. Ablation Study</title><p>To validate the contributions of the core components of CATE, we conducted an extensive ablation study across all six tasks. We designed three variants: (1) w/o NERM, which removes the noise-enhanced view; (2) w/o WTRM, which removes the wavelet-based view; and (3) w/o pretrain, which trains the full model from scratch without self-supervised pre-training.</p><p>The results are summarized in <xref rid="sensors-25-05718-t003" ref-type="table">Table 3</xref>. The full CATE model achieves the best performance across all settings, demonstrating the efficacy of its integrated design.</p><list list-type="bullet"><list-item><p>Removing the NERM component (&#8216;w/o NERM&#8217;) causes the most significant performance degradation across all tasks (e.g., a 7.06% drop in the SEED &#8594; SEED-<inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> task). This confirms that modeling and mitigating domain-specific noise is critical for cross-corpus generalization.</p></list-item><list-item><p>Removing the WTRM component (&#8216;w/o WTRM&#8217;) also leads to a notable performance decline, highlighting the importance of capturing multi-scale temporal-frequency features, which serve as robust, emotion-related biomarkers.</p></list-item><list-item><p>Training the model from scratch (&#8216;w/o pretrain&#8217;) results in inferior performance compared to the full model. This validates that the dual-view self-supervised pre-training provides a powerful initialization, enabling the model to learn more transferable representations before fine-tuning on limited labeled data.</p></list-item></list><table-wrap position="anchor" id="sensors-25-05718-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation study results: average accuracy and standard deviation (%) of different model configurations across all cross-dataset tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED &#8594; SEED-<inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED &#8594; SEED-<inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED-<inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED-<inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">w/o NERM</td><td align="center" valign="middle" rowspan="1" colspan="1">73.66 &#177; 18.84</td><td align="center" valign="middle" rowspan="1" colspan="1">74.50 &#177; 17.83</td><td align="center" valign="middle" rowspan="1" colspan="1">52.36 &#177; 23.52</td><td align="center" valign="middle" rowspan="1" colspan="1">79.85 &#177; 12.60</td><td align="center" valign="middle" rowspan="1" colspan="1">66.53 &#177; 20.76</td><td align="center" valign="middle" rowspan="1" colspan="1">70.51 &#177; 16.47</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">w/o WTRM</td><td align="center" valign="middle" rowspan="1" colspan="1">73.82 &#177; 19.13</td><td align="center" valign="middle" rowspan="1" colspan="1">78.99 &#177; 18.32</td><td align="center" valign="middle" rowspan="1" colspan="1">50.37 &#177; 20.48</td><td align="center" valign="middle" rowspan="1" colspan="1">74.61 &#177; 14.23</td><td align="center" valign="middle" rowspan="1" colspan="1">64.44 &#177; 15.53</td><td align="center" valign="middle" rowspan="1" colspan="1">66.62 &#177; 14.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/o pretrain</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.52 &#177; 18.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.25 &#177; 16.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.36 &#177; 19.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.05 &#177; 10.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.40 &#177; 19.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.86 &#177; 15.94</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Full (CATE)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>80.72 &#177; 14.27</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>80.70 &#177; 10.09</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>68.01 &#177; 19.33</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>81.65 &#177; 09.35</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>69.48 &#177; 16.77</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.17 &#177; 14.01</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>The results demonstrate the contribution of each key component in the CATE model. NERM: Noise-Enhanced Representation Modeling; WTRM: Wavelet Transform Representation Modeling.</p></fn></table-wrap-foot></table-wrap><p>In summary, the ablation study empirically proves that the superior performance of CATE stems from the powerful synergy between its noise-robust modeling (NERM), multi-scale feature extraction (WTRM), and the effective feature initialization provided by the self-supervised pre-training paradigm.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05718"><title>5. Discussion</title><p>The empirical results robustly demonstrate that CATE sets a new state-of-the-art in cross-corpus EEG emotion recognition. Our discussion focuses on interpreting these performance gains by linking them to our model&#8217;s core architectural innovations, contextualizing its advantages over existing methods, and outlining the broader implications of this work.</p><sec id="sec5dot1-sensors-25-05718"><title>5.1. Interpretation of CATE&#8217;s Performance</title><p>The consistent and substantial outperformance of CATE across all six transfer tasks validates our central hypothesis: a dual-view, self-supervised pre-training framework can effectively learn domain-invariant representations from heterogeneous EEG data. The most significant accuracy improvements (over 14 percentage points against the next-best method in SEED &#8596; SEED-IV<sup>3</sup> tasks) are not merely incremental gains but signify a fundamental advantage in generalization capability.</p><p>This advantage stems directly from the synergistic interplay of our two complementary views. The Noise-Enhanced Representation Modeling (NERM) view forces the model to become robust against superficial, domain-specific signal characteristics&#8212;akin to learning to recognize a voice despite different background noises or microphone qualities. This is crucial for cross-corpus tasks where recording equipment and environments inevitably vary. Concurrently, the Wavelet Transform Representation Modeling (WTRM) view captures the intrinsic, multi-scale temporal-frequency dynamics that are foundational to emotional expression in EEG signals. The ablation study (<xref rid="sensors-25-05718-t003" ref-type="table">Table 3</xref>) provides clear evidence for this synergy: removing either view results in a significant performance drop, with the removal of NERM being particularly detrimental, confirming that tackling domain noise is a primary challenge.</p><p>Furthermore, the multi-head attention mechanism in the fine-tuning stage proves essential for adapting these robustly pre-trained features to specific classification tasks. As seen in the confusion matrices (e.g., <xref rid="sensors-25-05718-f005" ref-type="fig">Figure 5</xref>), CATE excels at distinguishing between nuanced and often-confused emotional states like &#8216;sad&#8217; and &#8216;fear&#8217;. This suggests that the attention mechanism effectively learns to focus on the subtle yet discriminative neural signatures captured by the dual-view encoders, a capability that simpler feature extractors may lack.</p></sec><sec id="sec5dot2-sensors-25-05718"><title>5.2. Visualization Analysis of Feature Representations</title><p>To gain deeper insights into how each component of CATE contributes to effective emotion representation learning, we conducted t-SNE visualization analysis of the learned features across different model configurations. We randomly selected 200 samples from the SEED dataset and employed t-SNE to project the high-dimensional feature representations into a two-dimensional space. <xref rid="sensors-25-05718-f006" ref-type="fig">Figure 6</xref> presents the t-SNE projections of feature representations from four model variants, providing intuitive evidence of each component&#8217;s contribution to discriminative feature learning.</p><p>From the perspective of an ablation study, the visualization results reveal several critical insights:</p><p>First, regarding the relative advantages of the full CATE model: Among the four configurations, the complete model demonstrates the relatively best feature organization structure. While overlaps between the three emotion categories still exist (which is common in EEG-based emotion recognition), compared to other configurations, the complete model exhibits more regular feature point distribution with relatively clearer inter-class boundaries. This indicates that the synergistic effect of the dual-view architecture and pre-training strategy indeed contributes to learning more discriminative feature representations.</p><p>Second, regarding the contribution of the NERM view encoder: When the NERM component is removed, feature distribution significantly deteriorates, manifesting as increased class overlap, particularly between neutral and negative emotions. This distribution degradation validates our quantitative findings that NERM view&#8217;s noise-enhanced modeling is crucial for extracting robust features that can distinguish subtle emotional differences in the presence of domain-specific noise. The absence of this component leads to the model&#8217;s difficulty in discriminating fine-grained emotional variations.</p><p>Third, regarding the effectiveness of the WTRM view encoder: Removing the WTRM component also compromises feature discriminability, though to a slightly lesser extent than removing NERM. The clusters show increased dispersion and partial overlap, particularly in the boundary regions between emotions. This suggests that wavelet-based multi-scale temporal-frequency features provide complementary information essential for capturing the full spectrum of emotion-related neural dynamics.</p><p>Most significantly, regarding the importance of the pre-training strategy: Compared to the full model, the model without pre-training exhibits the poorest feature organization, with all three emotion categories heavily intermingled. Despite achieving reasonable classification accuracy through supervised fine-tuning alone, the lack of self-supervised pre-training results in features that fail to capture the underlying structure of emotional states. This powerfully demonstrates that our dual-view pre-training strategy learns fundamental representations that supervised learning alone cannot achieve.</p><p>Core findings from the ablation study: The comprehensive ablation analysis reveals distinct contributions from each component across different transfer scenarios:</p><p>(1) Component-wise Analysis: Removing either the NERM or WTRM view consistently degrades performance, but their relative importance varies across tasks. NERM shows more significant impact in challenging cross-corpus scenarios (e.g., 7.06% drop in SEED &#8594; SEED-<inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>), while WTRM&#8217;s contribution is more pronounced in tasks involving complex emotion taxonomies (e.g., notable degradation in SEED-V related transfers).</p><p>(2) Pre-training vs. Architecture: The comparison between component removal and pre-training ablation reveals an interesting pattern. While removing individual components causes consistent performance drops, the impact of removing pre-training varies significantly across tasks. In some cases (e.g., SEED &#8594; SEED-<inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>), the architectural components (NERM/WTRM) contribute more than pre-training, while in others (e.g., SEED &#8594; SEED-<inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>), pre-training shows greater importance. This suggests that the value of self-supervised pre-training is task-dependent and complements rather than replaces good architectural design.</p><p>(3) Synergistic Integration: The full model consistently outperforms all ablated versions, confirming that the optimal performance emerges from the synergistic combination of dual-view architecture and self-supervised pre-training, rather than from any single component dominance.</p><p>The t-SNE analysis provides compelling visual evidence supporting our architectural design choices. The synergistic combination of NERM and WTRM views, enhanced through self-supervised pre-training, creates a feature space where emotions are naturally organized along meaningful dimensions. This inherent structure in the learned representations explains CATE&#8217;s superior cross-corpus generalization capability&#8212;by learning features that capture the essential characteristics of emotional states rather than dataset-specific patterns, the model can effectively transfer knowledge across different experimental paradigms and recording conditions.</p></sec><sec id="sec5dot3-sensors-25-05718"><title>5.3. Comparison with Previous Cross-Corpus Methods</title><sec id="sec5dot3dot1-sensors-25-05718"><title>5.3.1. Comparison with Traditional Domain Adaptation Methods</title><p>DANN (2016), as a classical domain adversarial method, attempts to align feature distributions between source and target domains through adversarial training. However, our experimental results show that DANN performs poorly across all tasks (e.g., only 45.20% on SEED &#8594; SEED-<inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>), which is 35.52 percentage points lower than CATE. This substantial gap exposes DANN&#8217;s fundamental limitation: its assumption that source and target distributions can be directly matched is brittle for highly disparate EEG domains. In contrast, CATE does not force explicit alignment but rather learns core representations that are inherently robust to domain shifts, proving to be a more effective strategy.</p><p>GECNN (2021) introduced graph-embedded convolution to model spatial relationships between EEG channels. While achieving 57.25% accuracy on SEED &#8594; SEED-<inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, an improvement over DANN, it still falls 23.47 percentage points below CATE.GECNN&#8217;s primary limitation is its reliance on fixed spatial patterns that may not generalize across different recording setups and protocols. Without explicit mechanisms for handling domain-specific artifacts or capturing multi-scale spectral dynamics, GECNN struggles with the inherent variability in cross-corpus scenarios. Our dual-view architecture addresses these gaps through complementary noise-resilient (NERM) and wavelet-based spectral (WTRM) representations, achieving superior domain adaptation.</p></sec><sec id="sec5dot3dot2-sensors-25-05718"><title>5.3.2. Comparison with Recent Self-Supervised Methods</title><p>E2STN (2025) employs an emotion style transfer network, achieving 61.24% on SEED &#8594; SEED-<inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Despite attempting to mitigate domain differences through style transfer, it still lags behind CATE by 19.48 percentage points. E2STN&#8217;s main issue is that its style transfer mechanism assumes emotional &#8220;style&#8221; can be simply separated from content, but emotional expression in EEG signals is often tightly coupled with multiple aspects of the signal.</p><p>JCFA (2024) represents the closest competitor to CATE, employing dual-view contrastive learning with graph convolution. However, even this state-of-the-art method only achieves 65.52% on SEED &#8594; SEED-<inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, 15.20 percentage points lower than CATE. More importantly, JCFA&#8217;s training time is prohibitively long due to the computational complexity of graph operations. Another critical limitation of JCFA is its lack of explicit noise handling mechanisms, while our NERM view is specifically designed to filter domain-specific artifacts.</p></sec></sec><sec id="sec5dot4-sensors-25-05718"><title>5.4. Limitations and Future Work</title><p>Despite its strong performance, this study has limitations that open avenues for future research. First, our validation was conducted on the SEED family of datasets. While these are standard benchmarks, testing CATE&#8217;s generalization capabilities on a wider array of datasets with even greater diversity (e.g., DEAP, AMIGOS) is a necessary next step. Second, the current CATE model operates in an offline setting. For many real-world applications, such as real-time adaptive interfaces or clinical monitoring, online adaptation capabilities are required.</p><p>Future work will, therefore, focus on three key directions: (1) Model Lightweighting: We will explore model compression and knowledge distillation techniques to create a more efficient version of CATE suitable for deployment on edge devices. (2) Broader-Scale Validation: We will extend our evaluation to more diverse and larger-scale EEG datasets to further probe the limits of CATE&#8217;s generalization. (3) Online and Multimodal Extension: We plan to develop an online adaptation mechanism for CATE and investigate the fusion of EEG data with other modalities (e.g., eye-tracking, ECG) to build even more robust and comprehensive emotion recognition systems.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05718"><title>6. Conclusions</title><p>In this paper, we introduced CATE, a novel two-stage deep learning framework designed to overcome the critical challenge of cross-corpus generalization in EEG-based emotion recognition. By synergistically combining a dual-view self-supervised pre-training strategy with an attention-based fine-tuning mechanism, CATE effectively learns robust, transferable representations from complex and noisy EEG signals. The core innovation lies in its complementary Noise-Enhanced (NERM) and Wavelet Transform (WTRM) modeling views, which equip the model to handle domain-specific noise while capturing essential temporal-frequency patterns of emotional states.</p><p>Extensive experiments across six challenging transfer tasks using the SEED, SEED-IV, and SEED-V datasets demonstrate the definitive superiority of our approach. CATE consistently set a new state-of-the-art, outperforming existing methods by substantial margins of up to 15.65 percentage points. These results, supported by a thorough ablation study, validate the architectural choices and confirm the efficacy of each component within the CATE framework. By significantly bridging the gap between single-dataset performance and real-world applicability, this work represents a meaningful advancement towards deploying practical and reliable EEG-based affective computing systems. CATE provides a robust and promising paradigm for future research in cross-domain physiological signal analysis. </p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Z.L. and K.-Y.W.; methodology, Z.L. and K.-Y.W.; software, Z.L.; validation, Z.L., K.-Y.W. and C.-T.L.; formal analysis, Z.L.; investigation, Z.L. and C.-T.L.; resources, K.-Y.W.; data curation, Z.L.; writing&#8212;original draft preparation, Z.L.; writing&#8212;review and editing, Z.L., K.-Y.W. and C.-T.L.; visualization, Z.L.; supervision, K.-Y.W.; project administration, K.-Y.W.; funding acquisition, K.-Y.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable for studies not involving humans or animals.</p></notes><notes><title>Informed Consent Statement</title><p>This study does not involve humans.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The database used in this study is publicly available at the following websites, SEED Dataset: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bcmi.sjtu.edu.cn/home/seed/seed.html">https://bcmi.sjtu.edu.cn/home/seed/seed.html</uri> (accessed on 1 March 2021); SEED-IV Dataset: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bcmi.sjtu.edu.cn/home/seed/seed-iv.html">https://bcmi.sjtu.edu.cn/home/seed/seed-iv.html</uri> (accessed on 1 March 2021); SEED-V Dataset: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bcmi.sjtu.edu.cn/home/seed/seed-v.html">https://bcmi.sjtu.edu.cn/home/seed/seed-v.html</uri> (accessed on 1 March 2022).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CATE</td><td align="left" valign="middle" rowspan="1" colspan="1">Cross-corpus Attention-based Transfer Enhancement network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EEG</td><td align="left" valign="middle" rowspan="1" colspan="1">Electroencephalography</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NERM</td><td align="left" valign="middle" rowspan="1" colspan="1">Noise-Enhanced Representation Modeling</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WTRM</td><td align="left" valign="middle" rowspan="1" colspan="1">Wavelet Transform Representation Modeling</td></tr></tbody></array></p></glossary><app-group><app id="app1-sensors-25-05718"><title>Appendix A. Implementation Details and Hyperparameter Settings</title><sec id="secAdot1-sensors-25-05718"><title>Appendix A.1. Training Configuration</title><table-wrap position="anchor" id="sensors-25-05718-t0A1" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t0A1_Table A1</object-id><label>Table A1</label><caption><p>Hyperparameter settings for the pre-training stage.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="left" valign="middle" rowspan="1" colspan="1">AdamW</td><td align="left" valign="middle" rowspan="1" colspan="1">Adaptive moment estimation with weight decay</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base learning rate</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Initial learning rate for pre-training</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Weight decay</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">L2 regularization coefficient</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Training epochs</td><td align="left" valign="middle" rowspan="1" colspan="1">200</td><td align="left" valign="middle" rowspan="1" colspan="1">Total number of pre-training epochs</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="left" valign="middle" rowspan="1" colspan="1">256</td><td align="left" valign="middle" rowspan="1" colspan="1">Number of samples per batch</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Gradient clipping threshold</td><td align="left" valign="middle" rowspan="1" colspan="1">1.0</td><td align="left" valign="middle" rowspan="1" colspan="1">Maximum gradient norm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Scheduler decay factor</td><td align="left" valign="middle" rowspan="1" colspan="1">0.5</td><td align="left" valign="middle" rowspan="1" colspan="1">Learning rate reduction ratio</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scheduler patience</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Epochs to wait for improvement</td></tr></tbody></table></table-wrap><table-wrap position="anchor" id="sensors-25-05718-t0A2" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t0A2_Table A2</object-id><label>Table A2</label><caption><p>Hyperparameter settings for the fine-tuning stage.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="left" valign="middle" rowspan="1" colspan="1">AdamW</td><td align="left" valign="middle" rowspan="1" colspan="1">Adaptive moment estimation with weight decay</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base learning rate</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Initial learning rate for fine-tuning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Weight decay</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">L2 regularization coefficient</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Training epochs</td><td align="left" valign="middle" rowspan="1" colspan="1">20</td><td align="left" valign="middle" rowspan="1" colspan="1">Total number of fine-tuning epochs</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Batch size</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of samples per batch</td></tr></tbody></table></table-wrap></sec><sec id="secAdot2-sensors-25-05718"><title>Appendix A.2. Loss Function Weights</title><table-wrap position="anchor" id="sensors-25-05718-t0A3" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t0A3_Table A3</object-id><label>Table A3</label><caption><p>Multi-component loss function weight configuration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Loss Component</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pre-Training</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fine-Tuning</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (Alignment loss)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.5</td><td align="left" valign="middle" rowspan="1" colspan="1">0.0</td><td align="left" valign="middle" rowspan="1" colspan="1">Cross-view consistency weight</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (Style diversity loss)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.5</td><td align="left" valign="middle" rowspan="1" colspan="1">0.0</td><td align="left" valign="middle" rowspan="1" colspan="1">Feature diversity weight</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (Classification loss)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cross-entropy classification weight</td></tr></tbody></table></table-wrap></sec><sec id="secAdot3-sensors-25-05718"><title>Appendix A.3. Model Architecture Parameters</title><table-wrap position="anchor" id="sensors-25-05718-t0A4" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t0A4_Table A4</object-id><label>Table A4</label><caption><p>CATE model architecture configuration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Input dimension <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">310</td><td align="left" valign="middle" rowspan="1" colspan="1">62 channels &#215; 5 frequency bands</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Model dimension <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">200</td><td align="left" valign="middle" rowspan="1" colspan="1">Feature representation dimension</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Hidden dimension <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">256</td><td align="left" valign="middle" rowspan="1" colspan="1">MLP hidden layer size</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Output dimension <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">3/4/5 *</td><td align="left" valign="middle" rowspan="1" colspan="1">Number of emotion classes</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Number of attention heads <italic toggle="yes">h</italic></td><td align="left" valign="middle" rowspan="1" colspan="1">8</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-head attention heads</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Number of Transformer layers</td><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">Encoder layers</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FFN expansion factor</td><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="left" valign="middle" rowspan="1" colspan="1">Feed-forward network expansion ratio</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Residual scaling factor <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scaling coefficient for training stability</td></tr></tbody></table><table-wrap-foot><fn><p>* Adjusted according to different dataset tasks.</p></fn></table-wrap-foot></table-wrap></sec></app><app id="app2-sensors-25-05718"><title>Appendix B. Feature Fusion Ablation Study</title><p>To determine the optimal method for fusing features from the NERM and WTRM views, we conducted an ablation study on the SEED-<inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo>&#8594;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> SEED-<inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> transfer task, experimenting with various fixed weighting ratios. The results, presented in <xref rid="sensors-25-05718-t0A5" ref-type="table">Table A5</xref>, show that performance peaks around a 0.6:0.4 to 0.5:0.5 ratio. Specifically, simple averaging (a 0.5:0.5 ratio) achieved performance (e.g., Accuracy: 69.48%, F1-Score: 65.31%) that was statistically indistinguishable from the best-performing, slightly unbalanced ratio (0.6:0.4). Given the negligible performance difference, we selected simple averaging as it is a parameter-free, more stable, and less complex approach. This choice avoids introducing additional hyperparameters that might require separate tuning for each cross-corpus task, thereby enhancing the model&#8217;s generalizability and robustness.</p><table-wrap position="anchor" id="sensors-25-05718-t0A5" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t0A5_Table A5</object-id><label>Table A5</label><caption><p>Performance comparison of different feature fusion weighting ratios for the NERM and WTRM views on the SEED-<inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo>&#8594;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> SEED-<inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> task. The best performance for each metric is highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ratio NERM:WTRM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUROC</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUPRC</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.9 : 0.1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6813 &#177; 0.1960</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6643 &#177; 0.2240</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6333 &#177; 0.2209</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8636 &#177; 0.1535</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7960 &#177; 0.1894</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.8 : 0.2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6830 &#177; 0.1761</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6807 &#177; 0.2103</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6337 &#177; 0.1993</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8648 &#177; 0.1511</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7952 &#177; 0.1881</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.7 : 0.3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6937 &#177; 0.1651</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6821 &#177; 0.2115</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6458 &#177; 0.1889</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8684 &#177; 0.1502</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8012 &#177; 0.1879</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.6 : 0.4</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6948 &#177; 0.1617</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6827 &#177; 0.2095</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6496 &#177; 0.1859</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8708 &#177; 0.1465</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8044 &#177; 0.1849</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.5 : 0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6948 &#177; 0.1677</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6946 &#177; 0.1942</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6531 &#177; 0.1910</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8699 &#177; 0.1454</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8019 &#177; 0.1839</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.4 : 0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6913 &#177; 0.1720</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6933 &#177; 0.1942</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6508 &#177; 0.1956</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8682 &#177; 0.1423</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7972 &#177; 0.1814</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.3 : 0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6868 &#177; 0.1668</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6879 &#177; 0.1908</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6456 &#177; 0.1922</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8656 &#177; 0.1409</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7910 &#177; 0.1803</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.2 : 0.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6792 &#177; 0.1658</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6827 &#177; 0.1910</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6357 &#177; 0.1924</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8629 &#177; 0.1397</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7856 &#177; 0.1799</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1 : 0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6708 &#177; 0.1607</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6752 &#177; 0.1891</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6282 &#177; 0.1869</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8611 &#177; 0.1359</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7782 &#177; 0.1785</td></tr></tbody></table></table-wrap></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-05718"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Torres</surname><given-names>E.P.</given-names></name><name name-style="western"><surname>Torres</surname><given-names>E.A.</given-names></name><name name-style="western"><surname>Hern&#225;ndez-&#193;lvarez</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yoo</surname><given-names>S.G.</given-names></name></person-group><article-title>EEG-based BCI emotion recognition: A survey</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>5083</elocation-id><pub-id pub-id-type="doi">10.3390/s20185083</pub-id><pub-id pub-id-type="pmid">32906731</pub-id><pub-id pub-id-type="pmcid">PMC7570756</pub-id></element-citation></ref><ref id="B2-sensors-25-05718"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pei</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name></person-group><article-title>Affective computing: Recent advances, challenges, and future trends</article-title><source>Intell. Comput.</source><year>2024</year><volume>1</volume><fpage>76</fpage><pub-id pub-id-type="doi">10.34133/icomputing.0076</pub-id></element-citation></ref><ref id="B3-sensors-25-05718"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tiwari</surname><given-names>P.</given-names></name><name name-style="western"><surname>Song</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.</given-names></name></person-group><article-title>EEG based emotion recognition: A tutorial and review</article-title><source>ACM Comput. Surv.</source><year>2022</year><volume>55</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1145/3524499</pub-id></element-citation></ref><ref id="B4-sensors-25-05718"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>J.</given-names></name></person-group><article-title>Deep learning-based EEG emotion recognition: Current trends and future perspectives</article-title><source>Front. Psychol.</source><year>2023</year><volume>14</volume><elocation-id>1126994</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2023.1126994</pub-id><pub-id pub-id-type="pmid">36923142</pub-id><pub-id pub-id-type="pmcid">PMC10009917</pub-id></element-citation></ref><ref id="B5-sensors-25-05718"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>A comprehensive review of deep learning in EEG-based emotion recognition: Classifications, trends, and practical implications</article-title><source>PeerJ Comput. Sci.</source><year>2024</year><volume>10</volume><fpage>e2065</fpage><pub-id pub-id-type="doi">10.7717/peerj-cs.2065</pub-id><pub-id pub-id-type="pmid">38855206</pub-id><pub-id pub-id-type="pmcid">PMC11157589</pub-id></element-citation></ref><ref id="B6-sensors-25-05718"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roshdy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Karar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kork</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Beyrouthy</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nait-Ali</surname><given-names>A.</given-names></name></person-group><article-title>Advancements in EEG emotion recognition: Leveraging multi-modal database integration</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>2487</elocation-id><pub-id pub-id-type="doi">10.3390/app14062487</pub-id></element-citation></ref><ref id="B7-sensors-25-05718"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Camilleri</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Camilleri</surname><given-names>K.P.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cichocki</surname><given-names>A.</given-names></name></person-group><article-title>EEG-based affective brain&#8211;computer interfaces: Recent advancements and future challenges</article-title><source>J. Neural Eng.</source><year>2025</year><volume>22</volume><fpage>031004</fpage><pub-id pub-id-type="doi">10.1088/1741-2552/ade290</pub-id><pub-id pub-id-type="pmid">40490007</pub-id></element-citation></ref><ref id="B8-sensors-25-05718"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>Ten challenges for EEG-based affective computing</article-title><source>Brain Sci. Adv.</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1177/2096595819896200</pub-id></element-citation></ref><ref id="B9-sensors-25-05718"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spezialetti</surname><given-names>M.</given-names></name><name name-style="western"><surname>Placidi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Rossi</surname><given-names>S.</given-names></name></person-group><article-title>Emotion recognition for human-robot interaction: Recent advances and future perspectives</article-title><source>Front. Robot. AI</source><year>2020</year><volume>7</volume><elocation-id>532279</elocation-id><pub-id pub-id-type="doi">10.3389/frobt.2020.532279</pub-id><pub-id pub-id-type="pmid">33501307</pub-id><pub-id pub-id-type="pmcid">PMC7806093</pub-id></element-citation></ref><ref id="B10-sensors-25-05718"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pal</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mukhopadhyay</surname><given-names>S.</given-names></name><name name-style="western"><surname>Suryadevara</surname><given-names>N.</given-names></name></person-group><article-title>Development and progress in sensors and technologies for human emotion recognition</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>5554</elocation-id><pub-id pub-id-type="doi">10.3390/s21165554</pub-id><pub-id pub-id-type="pmid">34451002</pub-id><pub-id pub-id-type="pmcid">PMC8402266</pub-id></element-citation></ref><ref id="B11-sensors-25-05718"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hang</surname><given-names>W.</given-names></name></person-group><article-title>Adaptive Deep Feature Representation Learning for Cross-Subject EEG Decoding</article-title><source>BMC Bioinform.</source><year>2024</year><volume>25</volume><elocation-id>393</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-024-06024-w</pub-id><pub-id pub-id-type="pmid">39741250</pub-id><pub-id pub-id-type="pmcid">PMC11686875</pub-id></element-citation></ref><ref id="B12-sensors-25-05718"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rafiei</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Gauthier</surname><given-names>L.V.</given-names></name><name name-style="western"><surname>Adeli</surname><given-names>H.</given-names></name><name name-style="western"><surname>Takabi</surname><given-names>D.</given-names></name></person-group><article-title>Self-Supervised Learning for Electroencephalography</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2024</year><volume>35</volume><fpage>1457</fpage><lpage>1471</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2022.3190448</pub-id><pub-id pub-id-type="pmid">35867362</pub-id></element-citation></ref><ref id="B13-sensors-25-05718"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kornblith</surname><given-names>S.</given-names></name><name name-style="western"><surname>Norouzi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>A simple framework for contrastive learning of visual representations</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>PMLR, Virtual</conf-loc><conf-date>13&#8211;18 July 2020</conf-date><fpage>1597</fpage><lpage>1607</lpage></element-citation></ref><ref id="B14-sensors-25-05718"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cisse</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dauphin</surname><given-names>Y.N.</given-names></name><name name-style="western"><surname>Lopez-Paz</surname><given-names>D.</given-names></name></person-group><article-title>mixup: Beyond empirical risk minimization</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1710.09412</pub-id></element-citation></ref><ref id="B15-sensors-25-05718"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name></person-group><article-title>Self-supervised group meiosis contrastive learning for EEG-based emotion recognition</article-title><source>Appl. Intell.</source><year>2023</year><volume>53</volume><fpage>27207</fpage><lpage>27225</lpage><pub-id pub-id-type="doi">10.1007/s10489-023-04971-0</pub-id></element-citation></ref><ref id="B16-sensors-25-05718"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>W.L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>B.L.</given-names></name></person-group><article-title>A Multi-View Spectral-Spatial-Temporal Masked Autoencoder for Decoding Emotions with Self-Supervised Learning</article-title><source>Proceedings of the 30th ACM International Conference on Multimedia (MM &#8217;22)</source><conf-loc>Lisboa, Portugal</conf-loc><conf-date>10&#8211;14 October 2022</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2022</year><fpage>3273</fpage><lpage>3281</lpage><pub-id pub-id-type="doi">10.1145/3503161.3548243</pub-id></element-citation></ref><ref id="B17-sensors-25-05718"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name></person-group><article-title>Joint Contrastive Learning with Feature Alignment for Cross-Corpus EEG-based Emotion Recognition</article-title><source>Proceedings of the 1st International Workshop on Brain-Computer Interfaces (BCI) for Multimedia Understanding, BCIMM &#8217;24</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>28 October&#8211;1 November 2024</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2024</year></element-citation></ref><ref id="B18-sensors-25-05718"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Duan</surname><given-names>R.N.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>B.L.</given-names></name></person-group><article-title>Differential entropy feature for EEG-based emotion classification</article-title><source>Proceedings of the 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>6&#8211;8 November 2013</conf-date><fpage>81</fpage><lpage>84</lpage></element-citation></ref><ref id="B19-sensors-25-05718"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>W.L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>B.L.</given-names></name></person-group><article-title>Investigating Critical Frequency Bands and Channels for EEG-based Emotion Recognition with Deep Neural Networks</article-title><source>IEEE Trans. Auton. Ment. Dev.</source><year>2015</year><volume>7</volume><fpage>162</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2015.2431497</pub-id></element-citation></ref><ref id="B20-sensors-25-05718"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>W.L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>B.L.</given-names></name><name name-style="western"><surname>Cichocki</surname><given-names>A.</given-names></name></person-group><article-title>EmotionMeter: A Multimodal Framework for Recognizing Human Emotions</article-title><source>IEEE Trans. Cybern.</source><year>2019</year><volume>49</volume><fpage>1110</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2018.2797176</pub-id><pub-id pub-id-type="pmid">29994384</pub-id></element-citation></ref><ref id="B21-sensors-25-05718"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>W.L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>B.L.</given-names></name></person-group><article-title>Comparing Recognition Performance and Robustness of Multimodal Deep Learning Models for Multimodal Emotion Recognition</article-title><source>IEEE Trans. Cogn. Dev. Syst.</source><year>2021</year><volume>14</volume><fpage>715</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1109/TCDS.2021.3071170</pub-id></element-citation></ref><ref id="B22-sensors-25-05718"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ganin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ustinova</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ajakan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Germain</surname><given-names>P.</given-names></name><name name-style="western"><surname>Larochelle</surname><given-names>H.</given-names></name><name name-style="western"><surname>Laviolette</surname><given-names>F.</given-names></name><name name-style="western"><surname>Marchand</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lempitsky</surname><given-names>V.</given-names></name></person-group><article-title>Domain-adversarial training of neural networks</article-title><source>J. Mach. Learn. Res.</source><year>2016</year><volume>17</volume><fpage>1</fpage><lpage>35</lpage></element-citation></ref><ref id="B23-sensors-25-05718"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Graph-embedded convolutional neural network for image-based EEG emotion recognition</article-title><source>IEEE Trans. Emerg. Top. Comput.</source><year>2021</year><volume>10</volume><fpage>1399</fpage><lpage>1413</lpage><pub-id pub-id-type="doi">10.1109/TETC.2021.3087174</pub-id></element-citation></ref><ref id="B24-sensors-25-05718"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><article-title>Enhancing Cross-Dataset EEG Emotion Recognition: A Novel Approach with Emotional EEG Style Transfer Network</article-title><source>IEEE Trans. Affect. Comput.</source><year>2025</year><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2025.3555439</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05718-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall architecture of the proposed CATE model for cross-corpus EEG-based emotion recognition. (<bold>a</bold>) Dual-Path Self-Supervised Pre-training Stage: The input EEG data undergoes dual-view data augmentation through Noise-Enhanced Representation Modeling (NERM) and Wavelet Transform Representation Modeling (WTRM). Both views are processed through a shared feature extractor with linear layers, LayerNorm, and GELU activation. The extracted NERM and WTRM features are then optimized using alignment loss (<inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), and style loss (<inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) to learn robust domain-invariant representations. (<bold>b</bold>) Supervised fine-tuning stage: The pre-trained model is adapted for emotion classification. Input data is normalized and processed through parallel NERM and WTRM view encoders, each containing attention mechanisms and feed-forward networks (FFN). The dual-view features are fused, pooled globally, and fed to a classifier for final emotion prediction. The inset shows the detailed internal structure of the encoders with multi-head attention layers.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05718-g001.jpg"/></fig><fig position="float" id="sensors-25-05718-f002" orientation="portrait"><label>Figure 2</label><caption><p>NERM-WTRM dual-view feature fusion architecture. Two complementary encoders process EEG representations from different perspectives: (1) The NERM encoder builds robustness against domain-specific artifacts through noise-enhanced processing and <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msub><mml:mi>MultiHead</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> attention; (2) The WTRM encoder extracts emotion-relevant spectral energy distributions through wavelet decomposition and <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:msub><mml:mi>MultiHead</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> attention. The complementary features <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are fused to form the robust joint representation <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for emotion classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05718-g002.jpg"/></fig><fig position="float" id="sensors-25-05718-f003" orientation="portrait"><label>Figure 3</label><caption><p>Confusion matrices for CATE vs. JCFA on the SEED &#8596; SEED-IV<sup>3</sup> tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05718-g003a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05718-g003b.jpg"/></fig><fig position="float" id="sensors-25-05718-f004" orientation="portrait"><label>Figure 4</label><caption><p>Confusion matrices for CATE vs. JCFA on the SEED &#8596; SEED-V<sup>3</sup> tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05718-g004.jpg"/></fig><fig position="float" id="sensors-25-05718-f005" orientation="portrait"><label>Figure 5</label><caption><p>Confusion matrices for CATE vs. JCFA on the SEED-<inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>&#8596; SEED-<inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05718-g005.jpg"/></fig><fig position="float" id="sensors-25-05718-f006" orientation="portrait"><label>Figure 6</label><caption><p>t-SNE visualization of learned feature representations from different CATE model configurations on the SEED dataset. Each point represents a sample projected into 2D space, colored by emotion category (red: positive, green: neutral, blue: negative).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05718-g006.jpg"/></fig><table-wrap position="float" id="sensors-25-05718-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t001_Table 1</object-id><label>Table 1</label><caption><p>Limitations in Existing Cross-Corpus EEG Emotion Recognition Methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Approach Category</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Key Idea</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Limitations in Cross-Corpus Context</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Conventional Supervised Learning</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Train a model on a large, labeled source dataset.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fails to generalize to new target domains due to overfitting; performance drops significantly.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Traditional Domain Adaptation</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Align the feature distributions of source and target domains, often adversarially.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Assumes distributions are close, which is often not the case for EEG; susceptible to negative transfer if domains are too dissimilar.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Attention-Based Models</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adaptively weight EEG channels, time steps, or frequency bands.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Often operate on a single view of the data, failing to capture the interplay between spatial, temporal, and spectral patterns.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Self-Supervised &amp; Contrastive Learning</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learn representations from unlabeled data by solving pretext tasks or contrasting augmented views.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning is highly dependent on the choice of data augmentation; may not sufficiently account for domain-specific noise and style variations.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05718-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05718-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of average accuracy and standard deviation (%) of different methods across the six cross-corpus emotion recognition tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED &#8594; SEED-<inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED &#8594; SEED-<inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED-<inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SEED-<inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="normal">V</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8594; SEED-<inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi>IV</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">DANN(2016) *</td><td align="center" valign="middle" rowspan="1" colspan="1">45.20 &#177; 24.31</td><td align="center" valign="middle" rowspan="1" colspan="1">62.89 &#177; 08.67</td><td align="center" valign="middle" rowspan="1" colspan="1">46.53 &#177; 23.52</td><td align="center" valign="middle" rowspan="1" colspan="1">62.27 &#177; 12.60</td><td align="center" valign="middle" rowspan="1" colspan="1">36.91 &#177; 14.89</td><td align="center" valign="middle" rowspan="1" colspan="1">33.50 &#177; 17.35</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GECNN(2021)</td><td align="center" valign="middle" rowspan="1" colspan="1">57.25 &#177; 07.53</td><td align="center" valign="middle" rowspan="1" colspan="1">58.02 &#177; 07.03</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIXUP(2018) *</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.23 &#177; 15.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.24 &#177; 16.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.11 &#177; 20.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.21 &#177; 14.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.25 &#177; 20.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.78 &#177; 18.36</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">E2STN(2025)</td><td align="center" valign="middle" rowspan="1" colspan="1">61.24 &#177; 15.14</td><td align="center" valign="middle" rowspan="1" colspan="1">60.51 &#177; 05.41</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">JCFA(2024) *</td><td align="center" valign="middle" rowspan="1" colspan="1">65.52 &#177; 15.92</td><td align="center" valign="middle" rowspan="1" colspan="1">66.33 &#177; 11.15</td><td align="center" valign="middle" rowspan="1" colspan="1">54.07 &#177; 23.26</td><td align="center" valign="middle" rowspan="1" colspan="1">79.73 &#177; 12.32</td><td align="center" valign="middle" rowspan="1" colspan="1">64.06 &#177; 23.22</td><td align="center" valign="middle" rowspan="1" colspan="1">61.76 &#177; 15.23</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CATE (ours)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>80.72 &#177; 14.27</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>80.70 &#177; 10.09</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>68.01 &#177; 19.33</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>81.65 &#177; 09.35</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>69.48 &#177; 16.77</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.17 &#177; 14.01</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>* Results obtained by our own implementation. &#8220;-&#8221; indicates results were not available in the original papers.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>