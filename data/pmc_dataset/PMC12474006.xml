<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12474006</article-id><article-id pub-id-type="pmcid-ver">PMC12474006.1</article-id><article-id pub-id-type="pmcaid">12474006</article-id><article-id pub-id-type="pmcaiid">12474006</article-id><article-id pub-id-type="pmid">41013143</article-id><article-id pub-id-type="doi">10.3390/s25185905</article-id><article-id pub-id-type="publisher-id">sensors-25-05905</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>AGSK-Net: Adaptive Geometry-Aware Stereo-KANformer Network for Global and Local Unsupervised Stereo Matching</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-0792-4976</contrib-id><name name-style="western"><surname>Feng</surname><given-names initials="Q">Qianglong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="X">Xiaofeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-05905" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Lu</surname><given-names initials="Z">Zhenglin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="H">Haiyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Qi</surname><given-names initials="T">Tingfeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="T">Tianyi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Smolka</surname><given-names initials="B">Bogdan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05905">School of Mathematical and Physical Sciences, Chongqing University of Science and Technology, Chongqing 401331, China; <email>2023211020@cqust.edu.cn</email> (Q.F.); <email>2023211001@cqust.edu.cn</email> (Z.L.); <email>2023211019@cqust.edu.cn</email> (H.W.); <email>2024211017@cqust.edu.cn</email> (T.Q.); <email>2024211019@cqust.edu.cn</email> (T.Z.)</aff><author-notes><corresp id="c1-sensors-25-05905"><label>*</label>Correspondence: <email>xfwang828@126.com</email></corresp></author-notes><pub-date pub-type="epub"><day>21</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5905</elocation-id><history><date date-type="received"><day>14</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>10</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>15</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>21</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05905.pdf"/><abstract><p>The performance of unsupervised stereo matching in complex regions such as weak textures and occlusions is constrained by the inherently local receptive fields of convolutional neural networks (CNNs), the absence of geometric priors, and the limited expressiveness of MLP in conventional ViTs. To address these problems, we propose an Adaptive Geometry-aware Stereo-KANformer Network (AGSK-Net) for unsupervised stereo matching. Firstly, to resolve the conflict between the isotropic nature of traditional ViT and the epipolar geometry priors in stereo matching, we propose Adaptive Geometry-aware Multi-head Self-Attention (AG-MSA), which embeds epipolar priors via an adaptive hybrid structure of geometric modulation and penalty, enabling geometry-aware global context modeling. Secondly, we design Spatial Group-Rational KAN (SGR-KAN), which integrates the nonlinear capability of rational functions with the spatial awareness of deep convolutions, replacing the MLP with flexible, learnable rational functions to enhance the nonlinear expression ability of complex regions. Finally, we propose a Dynamic Candidate Gated Fusion (DCGF) module that employs dynamic dual-candidate states and spatially aware pre-enhancement to adaptively fuse global and local features across scales. Experiments demonstrate that AGSK-Net achieves state-of-the-art accuracy and generalizability on Scene Flow, KITTI 2012/2015, and Middlebury 2021.</p></abstract><kwd-group><kwd>unsupervised stereo matching</kwd><kwd>epipolar geometry prior</kwd><kwd>adaptive fusion</kwd><kwd>KAN</kwd><kwd>swin transformer</kwd></kwd-group><funding-group><award-group><funding-source>Natural Science Foundation of Chongqing</funding-source><award-id>CSTB2022NSCQ-MSX0398</award-id></award-group><funding-statement>This research was funded by the Natural Science Foundation of Chongqing (CSTB2022NSCQ-MSX0398).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05905"><title>1. Introduction</title><p>Stereo matching is one of the core tasks in computer vision and plays a critical role in 3D scene reconstruction. This 3D perception capability makes it a fundamental component in various application domains, such as autonomous driving [<xref rid="B1-sensors-25-05905" ref-type="bibr">1</xref>], 3D reconstruction [<xref rid="B2-sensors-25-05905" ref-type="bibr">2</xref>], and robotic navigation [<xref rid="B3-sensors-25-05905" ref-type="bibr">3</xref>].</p><p>In recent years, deep learning has made significant progress in the field of computer vision. Currently, deep learning-based stereo matching can be broadly categorized into supervised learning [<xref rid="B4-sensors-25-05905" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05905" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05905" ref-type="bibr">6</xref>] and unsupervised learning [<xref rid="B7-sensors-25-05905" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05905" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05905" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05905" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05905" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>]. Supervised learning approaches heavily rely on annotated disparity values, and obtaining large-scale, high-quality depth labels is costly. Moreover, the generalization ability in new scenes is limited. Unsupervised stereo matching eliminates the reliance on ground-truth disparity labels, enabling training on large unlabeled datasets and improving cross-domain generalization and the feasibility of engineering deployment. Moreover, recent studies [<xref rid="B13-sensors-25-05905" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05905" ref-type="bibr">14</xref>] have indicated that the unsupervised paradigm holds significant potential for development in handling mismatched annotation modalities, improving generalization, and enhancing robustness in real-world applications, making it a key direction for future research.</p><p>However, existing unsupervised approaches [<xref rid="B7-sensors-25-05905" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05905" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05905" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05905" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05905" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>] struggle in complex regions requiring global context, such as weak textures and occlusions, mainly because they rely on convolutions with limited receptive fields constrained by their local nature.</p><p>To address this, Vision Transformers (ViTs) [<xref rid="B15-sensors-25-05905" ref-type="bibr">15</xref>] have been introduced for their exceptional ability to model long-range dependencies [<xref rid="B16-sensors-25-05905" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05905" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05905" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05905" ref-type="bibr">19</xref>]. However, their direct application to stereo matching reveals three critical, unaddressed limitations. Firstly, the isotropic nature of the standard self-attention mechanism treats displacements in all directions equally, whereas the search space for stereo image matching is typically restricted by horizontal epipolar geometric priors. This may lead to the ingestion of redundant contextual information, interfering with the critical horizontal matching procedure. Existing geometry-aware approaches [<xref rid="B20-sensors-25-05905" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05905" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05905" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05905" ref-type="bibr">23</xref>] mainly apply epipolar constraints during cross-view matching, leaving the issue of geometric consistency in feature extraction largely unaddressed. Secondly, the Multi-layer Perceptron (MLP) [<xref rid="B24-sensors-25-05905" ref-type="bibr">24</xref>], widely used in Transformers for nonlinear modeling, relies on fixed, nonlearnable activation functions and simple linear transformations. This design limits its expressive capacity and makes it difficult to capture the nonlinear relationships in complex regions of stereo matching. In contrast, the Kolmogorov&#8211;Arnold Network (KAN) [<xref rid="B25-sensors-25-05905" ref-type="bibr">25</xref>] enhances nonlinear expressiveness through flexible and learnable univariate functions, showing strong potential for dense tasks. However, the combination of KAN with ViTs has not yet been applied to dense tasks. Existing studies have primarily focused on sparse target scenarios, such as categories [<xref rid="B26-sensors-25-05905" ref-type="bibr">26</xref>] and objects [<xref rid="B27-sensors-25-05905" ref-type="bibr">27</xref>]. Finally, effective stereo matching requires not only global context for geometry-aware reasoning but also fine-grained local details for accurate complex detail recognition. Therefore, fusing global and local features becomes a critical step in stereo matching. Traditional fusion approaches [<xref rid="B16-sensors-25-05905" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05905" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05905" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05905" ref-type="bibr">19</xref>] typically rely on simple stacking, weighting, or attention guidance. However, these methods often introduce redundant information or show attention bias toward a specific modality, which can lead to the loss of critical information and compromise overall model performance.</p><p>To systematically address the above challenges, we build upon the Swin Transformer [<xref rid="B18-sensors-25-05905" ref-type="bibr">18</xref>] and develop the Adaptive Geometry-aware Stereo-KANformer Network (AGSK-Net). The core idea is to enhance global reasoning while preserving fine-grained local details under the guidance of epipolar geometry. Specifically, we introduce three key innovations in the Stereo-KANformer backbone. First, the Adaptive Geometry-aware Multi-head Self-Attention (AG-MSA) embeds epipolar priors through Adaptive Geometry-enhanced Relative Positioning (AGRP), which imposes anisotropic attention biases. AGRP forms a soft epipolar band that tolerates slight vertical perturbations while suppressing nonepipolar responses, thereby reshaping spatial prior integration and improving geometric consistency during feature extraction. Second, the Spatial Group-Rational KAN (SGR-KAN), inspired by the KAN paradigm [<xref rid="B25-sensors-25-05905" ref-type="bibr">25</xref>,<xref rid="B28-sensors-25-05905" ref-type="bibr">28</xref>], replaces conventional MLPs with flexible group-rational functions directly applied to 2D feature maps. This design preserves spatial topology and enables localized channel&#8211;spatial nonlinear modeling, significantly boosting expressive power in complex regions. Third, the Dynamic Candidate Gated Fusion (DCGF) module integrates CNN-derived local details with geometry-enhanced global features from the Stereo-KANformer. It introduces a dual-candidate gating mechanism combined with coordinate attention, which dynamically selects and balances complementary feature streams. As a result, DCGF mitigates redundancy, preserves critical details, and achieves fine-grained global&#8211;local fusion. Together, these components form an end-to-end unsupervised stereo matching framework that improves accuracy and robustness, particularly in challenging regions with weak textures, occlusions, or other ill-posed conditions. Our contributions are threefold:<list list-type="bullet"><list-item><p>We propose a novel Adaptive Geometry-aware Multi-head Self-Attention (AG-MSA) for unsupervised stereo matching. Our approach introduces an AGRP attention bias modulation framework, which encodes epipolar geometry priors through a hybrid design of geometric functions for adaptive modulation and penalty, replacing conventional isotropic or simple additive biases and thereby enhancing the geometric consistency and global inference capability.</p></list-item><list-item><p>We design a Spatial Group-Rational KAN (SGR-KAN) for unsupervised stereo matching. By integrating the powerful nonlinear expressive capability of rational functions with the spatial perception ability of deep convolution, flexible and learnable Group-Rational KAN are directly applied on 2D feature maps to replace MLP. This enables channel-wise and spatial grouped modeling, thereby explicitly preserving spatial structure and enhancing the nonlinear expressive capability in complex regions.</p></list-item><list-item><p>We propose a Dynamic Candidate Gated Fusion (DCGF) module for global and local unsupervised stereo matching. The module constructs a novel dynamic dual-candidate state mechanism and coordinate attention mechanism enhanced with spatial information and adaptively arbitrates between different fusion strategies based on feature content, ensuring a more effective and complementary fuse of information from the CNN and Stereo-KANformer backbones.</p></list-item></list></p><p>Extensive experiments on standard benchmarks, including Scene Flow, KITTI, and Middlebury, demonstrate that AGSK-Net achieves state-of-the-art performance, particularly in challenging ill-posed regions, validating the effectiveness of our integrated design.</p></sec><sec id="sec2-sensors-25-05905"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05905"><title>2.1. CNN-Based Unsupervised Stereo Matching</title><p>Convolutional Neural Networks (CNNs) have long been the cornerstone of stereo matching. Early unsupervised approaches [<xref rid="B7-sensors-25-05905" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05905" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05905" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05905" ref-type="bibr">10</xref>] establish a foundational paradigm based on view synthesis and left&#8211;right disparity consistency. In this setting, a network is trained to reconstruct a target view from a source view using the predicted disparity map. However, the accuracy of the resulting disparity maps remains limited. SMAR-Net [<xref rid="B11-sensors-25-05905" ref-type="bibr">11</xref>] employs stacked stereo image pairs to predict disparity, but this strategy faces challenges in large disparity variations and complex scenes. As one of the most popular unsupervised models, PASMNet [<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>] adopts cascaded cost aggregation and disparity attention mechanisms, effectively addressing the issue of fixed disparity ranges. However, despite significant progress, existing unsupervised stereo matching approaches [<xref rid="B7-sensors-25-05905" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05905" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05905" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05905" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05905" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>] still struggle in complex regions such as occlusions and weak textures. This highlights the need for models that can move beyond purely local operations and incorporate global contextual reasoning.</p></sec><sec id="sec2dot2-sensors-25-05905"><title>2.2. Transformer-BasedStereo Matching</title><p>To overcome the locality limitations of CNNs, Vision Transformers (ViTs) [<xref rid="B15-sensors-25-05905" ref-type="bibr">15</xref>] and their variants have been introduced to the stereo matching field. STTR [<xref rid="B16-sensors-25-05905" ref-type="bibr">16</xref>] is a pioneering work that frames stereo matching as a sequence-to-sequence problem, leveraging the global self-attention mechanism of Transformers to directly find correspondences. Following this, architectures based on the more efficient Swin Transformer [<xref rid="B18-sensors-25-05905" ref-type="bibr">18</xref>], such as those proposed by Jia et al. [<xref rid="B17-sensors-25-05905" ref-type="bibr">17</xref>] and STFC-Net [<xref rid="B19-sensors-25-05905" ref-type="bibr">19</xref>], demonstrate strong performance by combining hierarchical feature representation with shifted-window attention. These models confirm the immense potential of Transformers in capturing global context and achieving impressive cross-domain generalization. However, these works typically adopt the standard, geometry-agnostic self-attention mechanism, failing to explicitly incorporate the fundamental epipolar geometric prior of stereo vision into their core design, which may limit the ability in modeling geometric consistency.</p></sec><sec id="sec2dot3-sensors-25-05905"><title>2.3. Epipolar Geometry in Stereo and Multi-View Methods</title><p>Recent works explore epipolar geometry guidance in both stereo and multi-view settings. H-Net [<xref rid="B20-sensors-25-05905" ref-type="bibr">20</xref>] introduces mutual epipolar attention within an unsupervised stereo framework to emphasize correspondences on the same epipolar line and employs optimal transport to suppress outliers. MVSTER [<xref rid="B23-sensors-25-05905" ref-type="bibr">23</xref>] extends epipolar guidance into the multi-view stereo (MVS) domain by proposing an epipolar Transformer that constrains cross-view attention along epipolar lines. It further introduces a detachable monocular-depth branch and cascade strategies to build data-dependent 3D associations, thereby improving reconstruction efficiency and completeness. Inspired by MVSTER, EI-MVSNet [<xref rid="B21-sensors-25-05905" ref-type="bibr">21</xref>] introduces epipolar guidance volume construction and interval-aware depth labeling to refine the candidate depth intervals and cost volume aggregation. The geometry-enhanced attentive approach [<xref rid="B22-sensors-25-05905" ref-type="bibr">22</xref>] further exploits geometric cues to produce multi-view consistent features under challenging illumination and low-texture scenarios. While these methods [<xref rid="B20-sensors-25-05905" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05905" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05905" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05905" ref-type="bibr">23</xref>] exploit epipolar priors at the cross-view matching or cost aggregation stages, they seldom integrate such priors into single-view feature extraction. Most intra-view streams still rely on standard isotropic backbones (CNNs or ViTs), so redundant nonepipolar context may be encoded before matching, which limits the potential benefits of geometry guidance.</p></sec><sec id="sec2dot4-sensors-25-05905"><title>2.4. Kolmogorov&#8211;Arnold Networks</title><p>Traditional neural networks, including MLP within Transformers, rely on fixed and nonlearnable activation functions and linear transformations, which can limit their capability to model highly complex, nonlinear functions. Recently, Kolmogorov&#8211;Arnold Networks (KANs) [<xref rid="B25-sensors-25-05905" ref-type="bibr">25</xref>], derived from the superposition theorem, enhance expressivity and interpretability by replacing node-based MLP with edge-wise learnable univariate functions. Recent studies demonstrate their effectiveness in computer vision. U-KAN [<xref rid="B27-sensors-25-05905" ref-type="bibr">27</xref>] integrates KAN layers into U-Net bottlenecks, achieving higher segmentation accuracy with reduced parameters and FLOPs. Extensions such as KM-UNet [<xref rid="B29-sensors-25-05905" ref-type="bibr">29</xref>] and MM-UKAN++ [<xref rid="B30-sensors-25-05905" ref-type="bibr">30</xref>] further combine KAN with state-space models or multiscale fusion, showing consistent gains on medical image benchmarks. Recently, Yang and Wang proposed the Kolmogorov&#8211;Arnold Transformer (KAT) [<xref rid="B28-sensors-25-05905" ref-type="bibr">28</xref>], which substitutes Transformer feed-forward layers with variant GR-KAN and introduces rational function bases and grouped parameter sharing to improve scalability on large-scale datasets. However, it often requires flattening features and does not inherently handle the 2D spatial structure of images, which discards crucial topological information. As a result, valuable topological relationships and relative positional cues are often lost, making it difficult for KANs to fully exploit geometric structures in dense prediction tasks. These limitations suggest that further progress depends on integrating KANs into architectures that explicitly preserve spatial awareness, for example, by combining them with convolutional backbones or attention mechanisms designed for structured 2D data.</p></sec></sec><sec id="sec3-sensors-25-05905"><title>3. Materials and Methods</title><p>To address these limitations, we propose the Adaptive Geometry-aware Stereo-KANformer Network (AGSK-Net), which integrates geometry-aware attention, enhanced nonlinear modeling, and adaptive global and local fusion. Therefore, we first design a novel Stereo-KANformer feature extraction architecture, which employs Adaptive Geometry-aware Multi-head Self-Attention (AG-MSA) and Spatial Group-Rational KAN (SGR-KAN), systematically addressing the key challenges of insufficient global reasoning capability, lack of geometric prior information, and limited nonlinear expressive capacity. In addition, we further propose a Coordinate Attention-enhanced Dynamic Gating Fusion module (DCGF) to achieve efficient and adaptive fusion of multi-scale global and local features. This section will elaborate on the overall architecture of AGSK-Net and the specific design of the aforementioned core modules.</p><sec id="sec3dot1-sensors-25-05905"><title>3.1. Overview</title><p>This paper proposes an Adaptive Geometry-aware Stereo-KANformer Network (AGSK-Net) for dense unsupervised stereo matching. The overall architecture is illustrated in <xref rid="sensors-25-05905-f001" ref-type="fig">Figure 1</xref>a. Given a pair of stereo images from the left and right views, <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> as input, multi-scale features of the left and right views are first extracted through an encoder&#8211;decoder backbone network based on CNN and Stereo-KANformer. The backbone network utilizes convolutional layers to extract local detail features. Meanwhile, the proposed Stereo-KANformer Block (SKB) is introduced into the deeper layers and bottleneck layers of the encoder and decoder networks to capture geometry-aware global contextual information, enhancing the nonlinear expressive capability in complex regions of the spatial structure. Then, to fully fuse the fine-grained local detail features <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> extracted by the CNN with the geometry-aware global context features <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> captured by SKB, we employ the further designed DCGF to adaptively fuse these two heterogeneous features at multiple scales, generating a more informative and discriminative fused feature <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>final</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Finally, the fused features are used to estimate the disparity map.</p></sec><sec id="sec3dot2-sensors-25-05905"><title>3.2. Stereo-KANformer</title><p>The Swin Transformer architecture typically overlooks a fundamental anisotropic geometric prior in stereo matching. Its isotropic relative position bias mechanism assigns equal weights to relative displacements in all directions, ignoring the geometric characteristics of binocular stereo matching. The neglect of critical geometric constraints leads to redundant context learning, which introduces noise in occluded or textureless regions and ultimately degrades matching accuracy. Additionally, Swin Transformer architecture often relies on MLP [<xref rid="B24-sensors-25-05905" ref-type="bibr">24</xref>] to achieve nonlinear feature mapping. This limits its feature representation capability, particularly when handling complex geometric correspondences and subtle texture variations in stereo matching. Based on this, this paper introduces anisotropic epipolar geometric priors into the multi-head self-attention mechanism to achieve effective geometric perception. Leveraging the powerful nonlinear feature representation capability of KAN, we propose the Stereo-KANformer architecture, a KAN-based geometry-aware Swin Transformer for unsupervised stereo matching. By guiding feature learning with epipolar geometric priors, the model enhances geometric consistency along the disparity dimension, thereby capturing global contextual information and enhancing its capacity for modeling nonlinear correspondences in complex scenes.</p><p>Specifically, an Adaptive Geometry-enhanced Multi-head Self Attention (AG-MSA) mechanism tailored for unsupervised stereo matching is first designed, drawing inspiration from the efficient window-based architecture of Swin Transformer [<xref rid="B18-sensors-25-05905" ref-type="bibr">18</xref>]. The proposed Window-based Adaptive Geometry-enhanced Multi-head Self Attention (W-AGMSA) and Shifted Window-based Adaptive Geometry-enhanced Multi-head Self Attention (SW-AGMSA) are applied consecutively to enhance geometric perception, which enables the extraction of global contextual information within each window. Secondly, Spatial Group-Rational KAN (SGR-KAN) is directly applied on the 2D feature maps to replace the MLP module in the traditional Swin Transformer, improving nonlinear expressive capability in complex regions, preserving the spatial characteristics of features, and making it more suitable for the dense unsupervised stereo matching.</p><sec id="sec3dot2dot1-sensors-25-05905"><title>3.2.1. Stereo-KANformer Block (SKB)</title><p>A standard Stereo-KANformer Block (SKB), as shown in <xref rid="sensors-25-05905-f001" ref-type="fig">Figure 1</xref>b, introduces geometric priors and the powerful nonlinear capability of KAN through four stages. A key design principle of the SKB is the consecutive application of two complementary attention modules, Window-based AG-MSA (W-AGMSA) and Shifted-Window-based AG-MSA (SW-AGMSA), to efficiently model both local and global dependencies. Their distinct roles are as follows:</p><p>W-AGMSA performs self-attention strictly within nonoverlapping local windows. This approach is highly efficient, as it limits the quadratic complexity of the attention computation to the size of each window. However, this efficiency comes at the cost of lacking information flow between windows, confining the receptive field of this step. SW-AGMSA is designed specifically to overcome this limitation. By applying a cyclic shift to the feature map before partitioning it into windows, SW-AGMSA creates new window configurations that bridge the boundaries of the previous W-AGMSA step. This crucial operation enables cross-window connections, allowing information to be exchanged across the entire feature map.</p><p>The consecutive pairing of a W-AGMSA stage with an SW-AGMSA stage in successive SKBs is therefore essential. This design allows the Stereo-KANformer backbone to build a global receptive field and capture long-range dependencies while maintaining the computational efficiency of the window-based paradigm. The SKB workflow is as follows:</p><p>For the input feature map <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>in</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, it is first processed by Layer Normalization (LN) then fed into the W-AGMSA to capture global contextual information with geometric awareness. The output is added to the original input <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>in</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to form the first residual connection, resulting in <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD1-sensors-25-05905"><label>(1)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="normal">W-AGMSA</mml:mn></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>LN</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>in</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>in</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Next, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is normalized again and passed through the nonlinear transformation module SGR-KAN to enhance the representation of inter-pixel relationships in the image. A residual connection is applied to preserve the original features, yielding <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD2-sensors-25-05905"><label>(2)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>SGR-KAN</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>LN</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The above steps are then repeated: the transformed feature <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is normalized and passed through SW-AGMSA to further expand the receptive field. The result is then added to the input by a residual connection, producing <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD3-sensors-25-05905"><label>(3)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>SW-AGMSA</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>LN</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mfenced><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the feature <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is normalized once more and passed through the second SGR-KAN module to complete the final nonlinear fusion, resulting in the output feature map <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD4-sensors-25-05905"><label>(4)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>SGR-KAN</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>LN</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mfenced><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2dot2-sensors-25-05905"><title>3.2.2. Adaptive Geometry-Aware Multi-Head Self-Attention</title><p>To deeply integrate the epipolar geometry priors of stereo matching while efficiently capturing global context and long-distance dependencies in feature maps, we design the Adaptive Geometry-aware Multi-head Self-Attention (AG-MSA) module. While prior works often apply epipolar priors at cross-view or volume stages, we embed them inside single-view self-attention via AGRP. The core of the module lies in our newly proposed attention geometry bias adjustment mechanism, specifically designed for stereo matching. By consecutively applying Window-based and Shifted Window-based multi-head self-attention (W-AGMSA and SW-AGMSA) structures, the module embeds epipolar geometry priors into every stage of feature learning while maintaining computational efficiency. This guides the network to learn feature representations that are more robust to horizontal matching and less sensitive to vertical noise, thereby improving matching accuracy in globally inferred regions.</p><p>In the standard self-attention mechanism, for a given input feature <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, it is first reshaped into a series of image patches (patch tokens). The standard scaled dot-product self-attention mechanism is defined as:<disp-formula id="FD5-sensors-25-05905"><label>(5)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mfenced><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">V</italic> represent the Query, Key, and Value matrices, respectively. <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the dimensionality of the key vectors, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> represents the attention scores. <italic toggle="yes">B</italic> denotes the relative position bias, which encodes spatial relationships. As a key factor for performance enhancement, the bias remains crucial for dense unsupervised stereo matching. However, the design of the relative position bias is inherently isotropic, treating displacements in all directions equally. As shown in <xref rid="sensors-25-05905-f002" ref-type="fig">Figure 2</xref>a, this fundamentally contradicts the anisotropic physical prior in rectified stereo matching, where the matching search is strictly constrained along horizontal epipolar lines.</p><p>In practical deployment, there remains a gap between ideal calibration and real-world imaging. Camera calibration errors, residual lens distortion, and slight mismatches between left and right view sensors can lead to vertical misalignment across multiple pixels. Imposing a strict zero vertical displacement constraint would compromise system robustness due to its intolerance of these inevitable perturbations. In contrast, adopting a flexible design that allows for minor vertical offsets is crucial for maintaining performance under real imaging conditions.</p><p>To address the above challenges, we propose a novel geometry bias framework based on direct geometry modeling and adaptive learning, which we refer to as Adaptive Geometry-enhanced Relative Positioning (AGRP). As shown in <xref rid="sensors-25-05905-f002" ref-type="fig">Figure 2</xref>b, the framework abandons indirect lookup tables and simple additive biases and instead adopts a hybrid model composed of geometric functions. The approach not only introduces strong epipolar geometric priors into the unsupervised stereo matching model but employs a distance-based dynamic constraint mechanism to overcome the issue of vertical perturbations. As a result, it facilitates the learning of disparity information that is structurally more consistent and detail-precise, particularly in challenging regions.</p><p>The approach constructs the final attention logit <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> directly through two complementary geometric constraint mechanisms. Firstly, the approach encodes fine-grained spatial relationships along the epipolar line using a horizontal spatial bias <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Secondly, a modulation factor <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> determined by the vertical distance <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is introduced and multiplied with <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to dynamically adjust the contribution of the horizontal bias, thereby reducing the reliability of the horizontal bias in nonepipolar directions. Finally, beyond modulation, we introduce an independent penalty term <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, also determined by the vertical distance <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which is subtracted from the total attention logit to explicitly suppress any attention that is not horizontally aligned. The final attention logit is formulated as:<disp-formula id="FD6-sensors-25-05905"><label>(6)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>B</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>&#961;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mi>&#981;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent the relative coordinate differences in width and height directions, respectively, between two image patches (tokens) within a window. The primary horizontal bias <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is retrieved from a learnable parameter tensor <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>h</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">M</italic> denotes the window size.</p><p>The geometric modulation factor <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is controlled by a learnable, head-specific modulation strength factor <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. To reduce the reliability of spatial bias in nonepipolar directions, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is designed using a stable rational function and dynamically scales the contribution of <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> based on the vertical distance <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD7-sensors-25-05905"><label>(7)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The direct geometric penalty <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is controlled by a learnable, per-head penalty strength factor <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This function is bounded and smooth, ensuring stable training while effectively imposing constraints. It is directly subtracted from the total logit to explicitly suppress any nonhorizontal alignment in attention:<disp-formula id="FD8-sensors-25-05905"><label>(8)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>&#183;</mml:mo><mml:mo>|</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, we design the geometric constraint in two complementary forms: the modulation factor <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> and the penalty term <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>. Their synergy is key to achieving robustness. The modulation factor <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> does not directly affect the overall attention score but is specifically used to adjust the confidence of the horizontal spatial bias <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The underlying logic is that when a pixel deviates in the vertical direction, the predefined spatial correlation rule <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in the horizontal direction should accordingly lose its significance. However, relying solely on this modulation may be insufficient to suppress incorrect matches driven by extremely high content similarity <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To this end, by combining it with the strong constraint of the direct penalty term <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>, our AGRP framework is able to incorporate epipolar geometric prior knowledge in a manner that is both flexible and robust.</p><p>In this design, both <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula> are smooth and continuous functions of <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, making the constraint effect of AGRP flexible. For minor vertical displacements caused by calibration errors (e.g., <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), the model applies only a mild suppression. In contrast, for larger vertical displacements (e.g., <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mo>&#8805;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), the suppression effect increases sharply. In this way, AGRP creates a soft epipolar band in the attention space, centered on the horizontal epipolar line and with probabilities smoothly decaying with vertical distance, as illustrated by the yellow dashed box in <xref rid="sensors-25-05905-f002" ref-type="fig">Figure 2</xref>a. This allows the model to focus on high-probability horizontal matches while tolerating small vertical perturbations that are inevitable in real-world scenarios, thereby enhancing the robustness and accuracy of matching without introducing excessive redundant context. Finally, the logarithmic values <italic toggle="yes">L</italic> adjusted by the AGRP framework are normalized via Softmax and then multiplied by the value matrix <italic toggle="yes">V</italic> to compute the attention output:<disp-formula id="FD9-sensors-25-05905"><label>(9)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mfenced><mml:mi>V</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The AG-MSA module in this paper is constructed by alternately stacking W-AGMSA and SW-AGMSA layers. Within each local or shifted window, the complete AGRP computation process is executed. This enables the approach to adaptively learn feature representations that are robust to geometric structure variations at every stage of global context aggregation and information interaction.</p></sec><sec id="sec3dot2dot3-sensors-25-05905"><title>3.2.3. Spatial Group-Rational KAN</title><p>In the standard Swin Transformer, MLP with fixed activation functions and simple linear transformations limit nonlinear modeling. To address this, we draw inspiration from the core idea of the Kolmogorov&#8211;Arnold Network (KAN), which replaces MLP with flexible and learnable univariate functions. Although previous studies have explored similar approaches for specific image tasks [<xref rid="B26-sensors-25-05905" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05905" ref-type="bibr">27</xref>], these approaches often suffer from poor model stability when applying KAN to more complex tasks, leading to difficulties in scaling up large-scale model training. Recently, GR-KAN [<xref rid="B28-sensors-25-05905" ref-type="bibr">28</xref>], a novel variant of KAN, has attracted widespread attention. The variant replaces the B-spline basis with rational functions and introduces a parameter sharing mechanism across edge groups, thereby improving performance and scalability. However, GR-KAN typically requires flattening two-dimensional feature maps into one-dimensional sequences when handling visual tasks. This operation disrupts the inherent spatial structure and local topological relationships of images, resulting in the loss of relative positional information of features. This is particularly detrimental in stereo matching, where pixel-level relative positions and local structural distributions are crucial for accurate disparity estimation.</p><p>As shown in <xref rid="sensors-25-05905-f003" ref-type="fig">Figure 3</xref>, to address this issue, we design a Spatial Group-Rational KAN (SGR-KAN), which preserves two-dimensional spatial structural features and serves as a powerful alternative to standard MLP. The core innovation of SGR-KAN lies in its organic integration of the powerful nonlinear modeling capability of rational function activations with the spatial locality awareness of deep convolutions. It performs channel-wise and spatial group modeling directly on 2D feature maps, thereby enhancing nonlinear expressiveness while fully preserving and leveraging critical spatial structural information.</p><list list-type="simple"><list-item><label>(1)</label><p>
<bold>SGR-KAN Core Activation Unit</bold>
</p></list-item></list><p>The core of SGR-KAN lies in its advanced activation unit, which is designed based on the theoretical foundation of Kolmogorov&#8211;Arnold Networks (KANs). The Kolmogorov&#8211;Arnold theorem [<xref rid="B31-sensors-25-05905" ref-type="bibr">31</xref>] states that any multivariate continuous function <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defined on a bounded domain can be decomposed into a weighted sum of several univariate continuous functions <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#934;</mml:mo><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD10-sensors-25-05905"><label>(10)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mo>&#934;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on this idea, the KAN architecture achieves deep mapping through the ordered nesting of <italic toggle="yes">L</italic> layer function matrices <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>):<disp-formula id="FD11-sensors-25-05905"><label>(11)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>KAN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mo>&#934;</mml:mo><mml:mi>L</mml:mi></mml:msub><mml:mo>&#8728;</mml:mo><mml:msub><mml:mo>&#934;</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8728;</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>&#8728;</mml:mo><mml:msub><mml:mo>&#934;</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8728;</mml:mo><mml:msub><mml:mo>&#934;</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mfenced><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#8728; denotes layer-wise nesting, and each layer of function matrix <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#934;</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> consists of several learnable univariate transformations <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which are responsible for information transfer from layer <italic toggle="yes">l</italic> to layer <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD12-sensors-25-05905"><label>(12)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#934;</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#8230;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8945;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#8230;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>However, directly learning the spline basis function form of <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> may face issues, such as scalability. Therefore, our SGR-KAN activation unit draws on the idea of GR-KAN and parameterizes the unary function <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the edge as a rational function of an <italic toggle="yes">m</italic>-order polynomial <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and an <italic toggle="yes">n</italic>-order polynomial <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, to address the potential instability caused by poles (i.e., denominator <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), we adopt the Safe Pad&#233; Activation Unit (PAU) [<xref rid="B32-sensors-25-05905" ref-type="bibr">32</xref>]:<disp-formula id="FD13-sensors-25-05905"><label>(13)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>w</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>&#183;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the learnable coefficients of the rational function, and <italic toggle="yes">w</italic> is a learnable scaling factor. Taking the absolute value of the coefficients <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> in the denominator ensures the boundedness and numerical stability of the function.</p><p>To further improve parameter efficiency, we introduce a grouping mechanism [<xref rid="B28-sensors-25-05905" ref-type="bibr">28</xref>] to divide the input channels into <italic toggle="yes">g</italic> groups, with each group containing <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>in</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels. Each channel shares the same set of learnable coefficients <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">w</italic>, where <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8970;</mml:mo><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>&#8971;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the group index. For an input vector <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula>, the operation can be represented as the product of a weight matrix <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:math></inline-formula> and a Group-Rational function vector <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD14-sensors-25-05905"><label>(14)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>GR-KAN</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#8230;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>in</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8945;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#8230;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>in</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mo>&#8970;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>&#8971;</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mo>&#8970;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>in</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>&#8971;</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>in</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><list list-type="simple"><list-item><label>(2)</label><p>
<bold>Full Architecture of SGR-KAN</bold>
</p></list-item></list><p>Based on the aforementioned activation unit, the overall structure of SGR-KAN is illustrated in <xref rid="sensors-25-05905-f003" ref-type="fig">Figure 3</xref>. Firstly, for an input feature <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> that has been processed by layer normalization (LayerNorm), a <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution layer is applied to expand the channel dimension from <italic toggle="yes">C</italic> to <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>hidden</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, providing a richer feature space for the Group-Rational activation:<disp-formula id="FD15-sensors-25-05905"><label>(15)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>exp</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Next, the expanded feature <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>exp</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is fed into the GR-KAN [<xref rid="B28-sensors-25-05905" ref-type="bibr">28</xref>] activation module to perform Group-Rational Activation:<disp-formula id="FD16-sensors-25-05905"><label>(16)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>act</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>GR-KAN</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>exp</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, a <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> Depthwise Convolution is introduced to perform spatial filtering independently on each feature channel, which effectively integrates contextual information from neighboring pixels and enhances the spatial awareness:<disp-formula id="FD17-sensors-25-05905"><label>(17)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>dw</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>DWConv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>act</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the channel dimension is projected back from <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>hidden</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to the original channel dimension <italic toggle="yes">C</italic> through another <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution layer:<disp-formula id="FD18-sensors-25-05905"><label>(18)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>SGR-KAN</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>dw</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, the proposed SGR-KAN combines learnable rational activation functions with convolution operations that preserve spatial topology, not only retaining the powerful nonlinear expressive capability of KAN but overcoming its inherent limitations when handling structured data including images. Furthermore, the grouping mechanism ensures its parameter efficiency.</p></sec></sec><sec id="sec3dot3-sensors-25-05905"><title>3.3. Dynamic Candidate Gated Fusion</title><p>In unsupervised stereo matching, accurately capturing the geometric structure of complex scenes and ill-posed regions requires effectively fusing CNN features rich in local details with Stereo-KANformer features that provide geometry-aware global context. To address key challenges arising from the fusion of heterogeneous features, such as information redundancy, detail loss, and gradient conflicts, we propose a novel Dynamic Candidate Gated Fusion (DCGF) module. This module abandons the fixed, single-path update paradigm of traditional fusion approaches and introduces a fine-grained fusion architecture composed of three core stages: spatial pre-enhancement, dynamic parallel candidate construction, and adaptive fusion and update.</p><p>First, in the spatial pre-enhancement stage, Coordinate Attention [<xref rid="B33-sensors-25-05905" ref-type="bibr">33</xref>] is utilized to pre-enhance the directional awareness and long-distance spatial dependencies of the concatenated heterogeneous features, achieving preliminary alignment and information enhancement of global and local features at the spatial context level. Second, in the dynamic parallel candidate construction stage, traditional gated fusion approaches typically generate only a fixed candidate state [<xref rid="B34-sensors-25-05905" ref-type="bibr">34</xref>]. To improve the flexibility and expressive capability of the fusion process, we propose a Dynamic Dual-Candidate State mechanism. This mechanism employs separated dual reset gates to independently and finely control the local and global information flows and then generates two complementary update schemes in parallel: a primary candidate state, representing a stable combination of features; and an enhanced candidate state, which is specifically designed to capture deeper and more complex synergistic relationships between the two types of features through nonlinear operations including cross-modulation. Finally, during the adaptive fusion and update phase, the module employs a lightweight channel attention submodule to dynamically learn the fusion weights based on the content of the two candidate states. The optimally weighted candidate state is then fused with the original local features under the control of the update gate. The workflow of the DCGF module is as follows:<list list-type="simple"><list-item><label>(1)</label><p><bold>Attention Enhancement and Gating Signal Generation</bold></p></list-item></list></p><p>First, the input local feature <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and global feature <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are concatenated along the channel dimension. To enhance the spatial awareness of the features, we apply a coordinate attention module [<xref rid="B33-sensors-25-05905" ref-type="bibr">33</xref>] to the concatenated features to capture direction-aware and position-sensitive global contextual information and cross-region disparity consistency, resulting in the enhanced feature <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>att</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>Then, based on this, we generate the current step <italic toggle="yes">t</italic> gating signals using three independent <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions followed by a Sigmoid activation function <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula>, including the update gate <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the local reset gate <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>local</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the global reset gate <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>global</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD19-sensors-25-05905"><label>(19)</label><mml:math id="mm103" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Conv</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>att</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>local</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>local</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>att</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>global</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>global</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>att</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p><list list-type="simple"><list-item><label>(2)</label><p>
<bold>Dynamic Parallel Dual Candidate State Construction</bold>
</p></list-item></list><p>To provide richer fusion possibilities, we design two parallel candidate states. First, dual reset gates are applied to the original features <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> respectively, through element-wise multiplication &#8857; to selectively reset them:<disp-formula id="FD20-sensors-25-05905"><label>(20)</label><mml:math id="mm106" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>local</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>global</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p><p>Subsequently, based on the reset concatenated features, a main candidate state <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>main</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is constructed using a <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution followed by a tanh activation function:<disp-formula id="FD21-sensors-25-05905"><label>(21)</label><mml:math id="mm109" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>main</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">tanh</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Conv</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To capture deeper nonlinear interactions between the two features, the enhanced candidate state <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>enhanced</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is constructed through cross modulation, which involves element-wise multiplication and addition of the reset features:<disp-formula id="FD22-sensors-25-05905"><label>(22)</label><mml:math id="mm111" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>cross</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>&#8857;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>global</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The enhanced candidate state is then activated using a <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution followed by a tanh activation function:<disp-formula id="FD23-sensors-25-05905"><label>(23)</label><mml:math id="mm113" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>enhanced</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">tanh</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>enh</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>cross</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><list list-type="simple"><list-item><label>(3)</label><p>
<bold>Adaptive Fusion and Update</bold>
</p></list-item></list><p>First, to achieve adaptive combination of the two candidate states, a lightweight channel attention submodule SubAtt is introduced, which is implemented using a <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution and global average pooling (GAP). The module takes the concatenated candidate states <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>concat</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as input and outputs the normalized fusion weights <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>main</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>enhanced</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD24-sensors-25-05905"><label>(24)</label><mml:math id="mm118" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>SubAtt</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>concat</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>ReLU</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>GAP</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>concat</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD25-sensors-25-05905"><label>(25)</label><mml:math id="mm119" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>main</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>enhanced</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>SubAtt</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>concat</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Next, the optimal candidate state <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is obtained through weighted fusion:<disp-formula id="FD26-sensors-25-05905"><label>(26)</label><mml:math id="mm121" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>main</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>main</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>enhanced</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>enhanced</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, under the control of the update gate <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the optimal candidate state is combined with the local feature to produce the fused feature <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>final</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD27-sensors-25-05905"><label>(27)</label><mml:math id="mm124" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>final</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>The DCGF dynamically adjusts the degree of local detail preservation and the proportion of global context incorporation based on the characteristics of the input features. This effectively overcomes the shortcomings of traditional fusion strategies and yields more discriminative feature representations, thereby enhancing the fusion capability of global and local information, especially in complex regions.</p></sec><sec id="sec3dot4-sensors-25-05905"><title>3.4. Adaptive Geometry-Aware Stereo-KANformer Network</title><p>The proposed Adaptive Geometry-aware Stereo-KANformer Network (AGSK-Net) is constructed by systematically integrating the designed modules, namely AG-MSA, SGR-KAN, and DCGF, into a unified architecture. This section focuses on how they are organized into the final network and how key parameters are configured. <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, &#8230;, <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> denote feature maps at different hierarchical stages of the network, where deeper levels correspond to lower spatial resolutions and larger channel dimensions. The overall pipeline is illustrated in <xref rid="sensors-25-05905-f001" ref-type="fig">Figure 1</xref>a.</p><p><bold>Shallow stages (1, 1/2 resolutions):</bold> Standard convolutional layers are applied to preserve high-resolution local structures while maintaining computational efficiency.</p><p><bold>Deep stages (1/4, 1/8, 1/16 resolutions):</bold> At these scales, Stereo-KANformer Blocks (SKBs) are employed. Each SKB integrates AG-MSA and SGR-KAN, where the number of attention heads is set to 8. Hierarchical window sizes are assigned as 8 &#215; 8 and 16 &#215; 16 with corresponding shifted windows of 4 &#215; 4 and 8 &#215; 8, respectively, enabling efficient cross-window information exchange. The number of SKBs per stage is set to 1, 1, 2 for progressively deeper modeling.</p><p><bold>Fusion across scales:</bold> DCGF modules are deployed after each SKB stage to merge high-level global features with low-level local details. By dynamically adjusting the fusion weights through candidate gating, these modules ensure consistent feature refinement across different resolutions.</p><p>Through this hierarchical integration, AGSK-Net combines fine-grained local cues, geometry-aware global reasoning, and adaptive multi-scale fusion, delivering high-quality disparity representations with improved robustness in ill-posed regions.</p></sec></sec><sec id="sec4-sensors-25-05905"><title>4. Experiment and Analysis</title><sec id="sec4dot1-sensors-25-05905"><title>4.1. Experimental Data and Environment</title><p><bold>KITTI 2012/2015.</bold> The KITTI 2012 dataset contains 194 pairs of training images and 195 pairs of testing images, all with a resolution of 376 &#215; 1240. The KITTI 2015 dataset includes 200 pairs of dynamic street scene training images and 200 pairs of testing images, also with a resolution of 376 &#215; 1240.</p><p><bold>Scene Flow.</bold> Scene Flow is a synthetic dataset composed of three subsets: Flyingthings3D, Driving, and Monkaa, comprising a total of 35454 pairs of training images and 4370 pairs of testing images, with a resolution of 960 &#215; 540.</p><p><bold>Middlebury 2021.</bold> Middlebury 2021 is a real indoor scene dataset captured using structured light, containing 24 pairs of high-resolution images.</p><p><bold>Experiment details.</bold> The approach in this study is implemented based on Pytorch 1.10 and Python 3.10. The hardware configuration includes an Intel Core i7-11700F 2.50 GHz processor, 32 GB of RAM, and two NVIDIA RTX 4060TI graphics cards with 16 GB of VRAM each. To enable comparison with the baseline, the hyperparameters in this paper are set to be consistent with PASMNet. The batch size is set to 8, and the optimizer used is Adam, with <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. To ensure fair and comprehensive evaluation, we adopted a multi-stage training and testing strategy on four benchmark datasets: Scene Flow, KITTI 2012, KITTI 2015, and Middlebury 2021. The model was first pre-trained on Scene Flow (35,454 training pairs, 4370 test pairs) for 15 epochs, with the learning rate set to <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the first 10 epochs and <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the last 5 epochs. This stage provides dense disparity supervision under diverse synthetic conditions. We then fine-tuned on KITTI 2015 (200 pairs) and KITTI 2012 (194 pairs) for 100 epochs, using a learning rate of <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the first 80 epochs and <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the last 20, to adapt the model to real driving scenes. For evaluation, we tested on the Scene Flow (4370 pairs), KITTI 2012 (195 pairs), and KITTI 2015 (200 pairs) test sets and directly tested on Middlebury 2021 (24 high-resolution pairs) without adaptation to assess detail recovery, robustness in complex regions, and cross-dataset generalization.</p></sec><sec id="sec4dot2-sensors-25-05905"><title>4.2. Ablation Study of AGSK-Net</title><p>To systematically analyze the impact of each component in the AGSK-Net model on unsupervised stereo matching performance, multiple ablation experiments were conducted on the Scene Flow and KITTI 2015 datasets in <xref rid="sensors-25-05905-t001" ref-type="table">Table 1</xref>, with different experimental configurations distinguished by index labels IDi (i = 1, 2, 3, 4, 5).</p><sec id="sec4dot2dot1-sensors-25-05905"><title>4.2.1. Effect on Swin Transformer</title><p>In this experiment, the classic Swin Transformer was first integrated to enhance the feature extraction capability for capturing global context. Comparing the results of ID1 and ID2 (ID1 + Swin Transformer) in <xref rid="sensors-25-05905-t001" ref-type="table">Table 1</xref>, the EPE and 3-pixel error are decreased by 7.78% and 5.48%, respectively. Compared to using only a CNN model, the Swin Transformer enhances the global inference capability by leveraging the window-based and shifted window multi-head attention mechanisms to extract global context information.</p></sec><sec id="sec4dot2dot2-sensors-25-05905"><title>4.2.2. Effect on Stereo-KANformer</title><p>This paper proposes an improved Stereo-KANformer by incorporating AG-MSA and SGR-KAN to introduce epipolar geometric priors and further enhance nonlinear expressive capability in complex regions. According to the comparison between ID2 and ID3 (ID2 + AGRP bias adjustment mechanism) in <xref rid="sensors-25-05905-t001" ref-type="table">Table 1</xref>, the EPE and 3-pixel error are reduced by 7.36% and 4.48%, respectively. From the comparison between ID3 and ID4 (ID3 + SGR-KAN), the EPE and 3-pixel error are reduced by 19.16% and 9.10%, respectively. This demonstrates that the proposed AG-MSA introduces geometric priors into the attention mechanism to obtain geometry-aware global contextual information and that SGR-KAN effectively enhances the nonlinear expressive capability in complex regions through the Spatial Group-Rational KAN architecture.</p></sec><sec id="sec4dot2dot3-sensors-25-05905"><title>4.2.3. Effect on DCGF</title><p>The proposed DCGF adaptively fuses global and local information at multiple scales to extract rich global context and local detail features. The comparison between ID4 and ID5 (ID4+DCGF) in <xref rid="sensors-25-05905-t001" ref-type="table">Table 1</xref> shows that after introducing the DCGF adaptive fusion mechanism, EPE and 3-pixel error are reduced by 23.70% and 6.70%, respectively. This demonstrates that DCGF effectively integrates contextual and detailed information at all scales through coordinate attention enhancement and a multi-candidate dynamic gating mechanism, thereby improving the accuracy of unsupervised stereo matching.</p></sec><sec id="sec4dot2dot4-sensors-25-05905"><title>4.2.4. Detailed Analysis of AGRP and Network Architecture</title><p>To further investigate the specific design choices within our framework, we conducted a series of more detailed ablation studies, with results presented in <xref rid="sensors-25-05905-t002" ref-type="table">Table 2</xref>.</p><p><bold>Analysis of AGRP Components:</bold> To validate our hybrid geometric bias design in AG-MSA, we analyzed the individual contributions of the geometric modulation (<inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula>) and the direct penalty (<inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>) terms. We started with the Swin Transformer baseline (ID2) and added each component separately. The &#8220;M-only&#8221; variant (ID3a), which only uses the modulation factor <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> to scale the horizontal bias, already shows a significant performance improvement, confirming the benefit of down-weighting the spatial prior for off-epipolar pixels. The &#8220;P-only&#8221; variant (ID3b), which only applies the direct penalty <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>, yields an even stronger improvement, highlighting the effectiveness of an explicit, absolute penalty. Our full AGRP model (ID3), which synergistically combines both mechanisms, achieves the best performance. This demonstrates that the two components are complementary: the modulation provides a nuanced, relative adjustment of the spatial bias, while the penalty acts as a robust, absolute deterrent against geometrically inconsistent matches.</p><p><bold>Analysis of Multi-Scale SKB Architecture:</bold> To justify the necessity of our multi-scale design, we compared the full model with three variants. ID4a, which places Stereo-KANformer Blocks (SKBs) only at the deepest 1/16 layer, shows limited performance improvement, likely because global inference information can be effectively extracted only at low resolution. ID4b, which applies SKBs at both the 1/16 and 1/8 resolution levels, performs better, indicating that incorporating higher-resolution SKBs allows simultaneous extraction of both global and local information. Finally, the superior performance of ID4c demonstrates that hierarchically applying SKBs across multiple resolutions is crucial for effectively capturing and integrating fine details with abstract global context.</p><p><bold>Analysis of Stereo-KANformer Depth:</bold> Finally, we analyzed the impact of the number of SKB blocks (i.e., the depth of the transformer backbone). We compared our final configuration &#8220;Medium Depth&#8221; (ID4e) with a &#8220;Shallow&#8221; variant (ID4d) that uses fewer SKB blocks and a &#8220;Deep&#8221; variant (ID4f) that uses more. The &#8220;Shallow&#8221; model slightly improves the performance, indicating insufficient feature transformation capacity. The &#8220;Deep&#8221; model yields a marginal improvement on the synthetic Scene Flow dataset but a slight gain on the real-world KITTI dataset, suggesting a potential for overfitting and a less favorable performance-complexity trade-off. These results validate that our chosen depth provides a well-balanced and effective configuration for the task.</p></sec></sec><sec id="sec4dot3-sensors-25-05905"><title>4.3. Benchmark Evaluation</title><p>The proposed Adaptive Geometry-aware Stereo-KANformer Network (AGSK-Net) aims to enhance the accuracy and generalization performance of unsupervised stereo matching. The model is first trained on the Scene Flow dataset and then fine-tuned separately on the KITTI 2012 and KITTI 2015 training sets.</p><sec id="sec4dot3dot1-sensors-25-05905"><title>4.3.1. Quantitative Evaluation</title><p>To ensure a fair and objective comparison against prior state-of-the-art methods, our quantitative evaluation relies exclusively on the most widely adopted and standardized metrics in the stereo matching field: the End-Point Error (EPE) and the N-pixel error rate. These metrics are universally used by community benchmarks and allow for direct, apples-to-apples performance assessment. <xref rid="sensors-25-05905-t003" ref-type="table">Table 3</xref> presents the testing results of our approach on the KITTI 2015 and Scene Flow benchmarks. In the table, &#8220;D1-bg&#8221;, &#8220;D1-fg&#8221;, and &#8220;D1-all&#8221; correspond to the 3-pixel error metric [<xref rid="B35-sensors-25-05905" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05905" ref-type="bibr">36</xref>] for background pixels, foreground pixels, and all-region pixels, respectively. As shown in <xref rid="sensors-25-05905-t003" ref-type="table">Table 3</xref>, the proposed approach achieves competitive performance across all regions of the KITTI 2015 and Scene Flow datasets compared to other representative state-of-the-art approaches, further demonstrating the effectiveness of AGSK-Net.</p><p>In addition, <xref rid="sensors-25-05905-t004" ref-type="table">Table 4</xref> presents the results on the KITTI 2012 test set. It can be observed that the proposed approach achieves the lowest error across multiple metrics, such as 2, 3, and 5-pixel errors [<xref rid="B37-sensors-25-05905" ref-type="bibr">37</xref>]. AGSK-Net significantly outperforms PASMNet [<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>] in terms of accuracy by extracting and adaptively fusing multi-scale global context and local details through Stereo-KANformer and DCGF.</p><table-wrap position="anchor" id="sensors-25-05905-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05905-t003_Table 3</object-id><label>Table 3</label><caption><p>The results on the KITTI 2015 benchmark. &#8220;All&#8221; refers to the entire image region, &#8220;Noc.&#8221; refers to the nonoccluded regions in the image. The &#8220;-&#8221; symbol indicates that the result is not found in the original paper.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Models</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">KITTI 2015</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scene Flow</th></tr><tr><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
All/%
</th><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
Noc./%
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
EPE/Pixel
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
D1-bg
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
D1-fg
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
D1-All
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
D1-bg
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
D1-fg
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
D1-All
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">3DG-DVO&#160;[<xref rid="B38-sensors-25-05905" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">14.12</td><td align="center" valign="middle" rowspan="1" colspan="1">18.68</td><td align="center" valign="middle" rowspan="1" colspan="1">14.88</td><td align="center" valign="middle" rowspan="1" colspan="1">13.54</td><td align="center" valign="middle" rowspan="1" colspan="1">17.27</td><td align="center" valign="middle" rowspan="1" colspan="1">14.16</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zhou&#160;[<xref rid="B10-sensors-25-05905" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">10.23</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">9.91</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OASM-Net&#160;[<xref rid="B39-sensors-25-05905" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">6.89</td><td align="center" valign="middle" rowspan="1" colspan="1">19.42</td><td align="center" valign="middle" rowspan="1" colspan="1">8.98</td><td align="center" valign="middle" rowspan="1" colspan="1">5.44</td><td align="center" valign="middle" rowspan="1" colspan="1">17.30</td><td align="center" valign="middle" rowspan="1" colspan="1">7.39</td><td align="center" valign="middle" rowspan="1" colspan="1">3.86</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SegStereo&#160;[<xref rid="B40-sensors-25-05905" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">8.79</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">7.70</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Self-SuperFlow&#160;[<xref rid="B41-sensors-25-05905" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">5.78</td><td align="center" valign="middle" rowspan="1" colspan="1">19.76</td><td align="center" valign="middle" rowspan="1" colspan="1">8.11</td><td align="center" valign="middle" rowspan="1" colspan="1">4.69</td><td align="center" valign="middle" rowspan="1" colspan="1">18.29</td><td align="center" valign="middle" rowspan="1" colspan="1">6.93</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AAFS&#160;[<xref rid="B42-sensors-25-05905" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">6.27</td><td align="center" valign="middle" rowspan="1" colspan="1">13.95</td><td align="center" valign="middle" rowspan="1" colspan="1">7.54</td><td align="center" valign="middle" rowspan="1" colspan="1">5.96</td><td align="center" valign="middle" rowspan="1" colspan="1">13.01</td><td align="center" valign="middle" rowspan="1" colspan="1">7.12</td><td align="center" valign="middle" rowspan="1" colspan="1">2.88</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PASMNet&#160;[<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">5.41</td><td align="center" valign="middle" rowspan="1" colspan="1">16.36</td><td align="center" valign="middle" rowspan="1" colspan="1">7.23</td><td align="center" valign="middle" rowspan="1" colspan="1">5.02</td><td align="center" valign="middle" rowspan="1" colspan="1">15.16</td><td align="center" valign="middle" rowspan="1" colspan="1">6.69</td><td align="center" valign="middle" rowspan="1" colspan="1">3.54</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Permutation Stereo&#160;[<xref rid="B43-sensors-25-05905" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">5.53</td><td align="center" valign="middle" rowspan="1" colspan="1">15.47</td><td align="center" valign="middle" rowspan="1" colspan="1">7.18</td><td align="center" valign="middle" rowspan="1" colspan="1">5.18</td><td align="center" valign="middle" rowspan="1" colspan="1">14.51</td><td align="center" valign="middle" rowspan="1" colspan="1">6.72</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SPSMnet&#160;[<xref rid="B44-sensors-25-05905" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">5.42</td><td align="center" valign="middle" rowspan="1" colspan="1">12.84</td><td align="center" valign="middle" rowspan="1" colspan="1">6.65</td><td align="center" valign="middle" rowspan="1" colspan="1">4.94</td><td align="center" valign="middle" rowspan="1" colspan="1">12.01</td><td align="center" valign="middle" rowspan="1" colspan="1">6.10</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UHP&#160;[<xref rid="B45-sensors-25-05905" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">5.00</td><td align="center" valign="middle" rowspan="1" colspan="1">13.70</td><td align="center" valign="middle" rowspan="1" colspan="1">6.45</td><td align="center" valign="middle" rowspan="1" colspan="1">4.65</td><td align="center" valign="middle" rowspan="1" colspan="1">12.37</td><td align="center" valign="middle" rowspan="1" colspan="1">5.93</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CRD-Fusion&#160;[<xref rid="B46-sensors-25-05905" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">4.59</td><td align="center" valign="middle" rowspan="1" colspan="1">13.68</td><td align="center" valign="middle" rowspan="1" colspan="1">6.11</td><td align="center" valign="middle" rowspan="1" colspan="1">4.30</td><td align="center" valign="middle" rowspan="1" colspan="1">12.73</td><td align="center" valign="middle" rowspan="1" colspan="1">5.69</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.44</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>12.40</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.69</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.36</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>10.36</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.68</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.64</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="anchor" id="sensors-25-05905-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05905-t004_Table 4</object-id><label>Table 4</label><caption><p>The results on the KITTI 2012 benchmark. We present the evaluated errors at thresholds of 2-pixel, 3-pixel, and 5-pixel errors on the benchmark.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Models</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">&gt;2-Pixel/%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">&gt;3-Pixel/%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">&gt;5-Pixel/%</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Noc.
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
All
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Noc.
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
All
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Noc.
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
All
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zhou&#160;[<xref rid="B10-sensors-25-05905" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">14.32</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">9.86</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">7.88</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SegStereo&#160;[<xref rid="B40-sensors-25-05905" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">7.89</td><td align="center" valign="middle" rowspan="1" colspan="1">9.64</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OASM-Net&#160;[<xref rid="B39-sensors-25-05905" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">9.01</td><td align="center" valign="middle" rowspan="1" colspan="1">11.17</td><td align="center" valign="middle" rowspan="1" colspan="1">6.39</td><td align="center" valign="middle" rowspan="1" colspan="1">8.60</td><td align="center" valign="middle" rowspan="1" colspan="1">4.32</td><td align="center" valign="middle" rowspan="1" colspan="1">6.50</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Permutation Stereo&#160;[<xref rid="B43-sensors-25-05905" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">11.89</td><td align="center" valign="middle" rowspan="1" colspan="1">13.16</td><td align="center" valign="middle" rowspan="1" colspan="1">7.39</td><td align="center" valign="middle" rowspan="1" colspan="1">8.48</td><td align="center" valign="middle" rowspan="1" colspan="1">4.32</td><td align="center" valign="middle" rowspan="1" colspan="1">5.11</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PASMNet&#160;[<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">8.77</td><td align="center" valign="middle" rowspan="1" colspan="1">10.58</td><td align="center" valign="middle" rowspan="1" colspan="1">5.91</td><td align="center" valign="middle" rowspan="1" colspan="1">6.98</td><td align="center" valign="middle" rowspan="1" colspan="1">3.86</td><td align="center" valign="middle" rowspan="1" colspan="1">4.67</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UHP&#160;[<xref rid="B45-sensors-25-05905" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">9.08</td><td align="center" valign="middle" rowspan="1" colspan="1">10.37</td><td align="center" valign="middle" rowspan="1" colspan="1">6.05</td><td align="center" valign="middle" rowspan="1" colspan="1">7.09</td><td align="center" valign="middle" rowspan="1" colspan="1">3.69</td><td align="center" valign="middle" rowspan="1" colspan="1">4.43</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AAFS&#160;[<xref rid="B42-sensors-25-05905" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">10.64</td><td align="center" valign="middle" rowspan="1" colspan="1">11.69</td><td align="center" valign="middle" rowspan="1" colspan="1">6.10</td><td align="center" valign="middle" rowspan="1" colspan="1">6.94</td><td align="center" valign="middle" rowspan="1" colspan="1">3.28</td><td align="center" valign="middle" rowspan="1" colspan="1">3.81</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>7.24</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>8.85</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.84</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.74</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>3.08</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>3.71</bold>
</td></tr></tbody></table></table-wrap></sec><sec id="sec4dot3dot2-sensors-25-05905"><title>4.3.2. Qualitative Evaluation</title><p><xref rid="sensors-25-05905-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-05905-f005" ref-type="fig">Figure 5</xref>, respectively, illustrate the error maps and depth maps generated by our approach and the baseline PASMNet [<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>] and OASM-Net [<xref rid="B39-sensors-25-05905" ref-type="bibr">39</xref>] on the KITTI 2015 and KITTI 2012 test sets.</p><p>Compared with PASMNet and OASM-Net, our approach leverages Stereo-KANformer to capture multi-scale global context and local detail information and adaptively fuses them through DCGF, resulting in significantly better overall performance. In particular, the yellow rectangles in <xref rid="sensors-25-05905-f004" ref-type="fig">Figure 4</xref> highlight the ill-posed regions, including object geometric structures, traffic lights, edge details, and occlusions, where our approach achieves more accurate disparity estimation.</p><p><xref rid="sensors-25-05905-f005" ref-type="fig">Figure 5</xref> further demonstrates issues such as disparity detail loss caused by fine objects, occlusions, weak textures, and object edges, which are especially common in stereo matching. As shown in the yellow rectangle, compared with other approaches, the proposed approach demonstrates more robust performance in complex ill-posed regions, such as the fine wooden frames along glass edges, repetitive textures on cars, and object boundaries and intricate fence details.</p><p>Furthermore, <xref rid="sensors-25-05905-f006" ref-type="fig">Figure 6</xref> presents a comparison of disparity estimation performance between the proposed approach and the baseline PASMNet on the Scene Flow test set. By comparing the areas marked with yellow rectangular boxes, it is evident that the proposed approach outperforms the baseline in challenging and fine-grained regions, such as occluded cylinders, wheels, motorcycles, and headphones. This further validates the effectiveness of the proposed AGSK-Net with geometric perception capability in jointly modeling multi-scale global information and local geometric details.</p></sec></sec><sec id="sec4dot4-sensors-25-05905"><title>4.4. Generalization Performance</title><p>In unsupervised deep stereo matching, generalization ability plays a key role. Therefore, we evaluate the generalization performance of our approach against the baseline PASMNet on the Middlebury 2021 dataset. For fairness, all models are exclusively pre-trained on the Scene Flow dataset without any fine-tuning on Middlebury 2021.</p><p><xref rid="sensors-25-05905-f007" ref-type="fig">Figure 7</xref> presents the qualitative evaluation results on Middlebury 2021. It can be observed that our approach outperforms the baseline PASMNet in the areas marked by blue circles. Our approach demonstrates excellent generalization in disparity estimation in complex ill-posed regions, such as low-texture areas, fine details, and occlusions. For example, in the second column of <xref rid="sensors-25-05905-f007" ref-type="fig">Figure 7</xref>, the intricate details of the lotus leaf model, as well as the chair backs in the first, third, and fourth columns, show large disparity gaps in the results from the baseline, whereas our approach produces more complete and accurate disparity estimations. The quantitative results in <xref rid="sensors-25-05905-t005" ref-type="table">Table 5</xref> further support this conclusion. Compared with the baseline model, our approach reduces the 3-pixel error and EPE on the Middlebury 2021 dataset by 29.25% and 31.80%, respectively. Overall, by leveraging the geometry-aware global context extraction of Stereo-KANformer and the multi-scale global and local detail fusion of DCGF, our approach demonstrates significantly better generalization performance in unseen scenarios than the baseline PASMNet.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05905"><title>5. Conclusions</title><p>This paper addresses the core challenges in unsupervised stereo matching within existing deep learning paradigms, including the lack of global contextual information guided by geometric priors, limitations in modeling nonlinear relationships, and insufficient multi-scale feature fusion. We propose a novel Adaptive Geometry-aware Stereo-KANformer Network (AGSK-Net). Through the collaborative operation of multiple innovative modules, the network overcomes the limitations of traditional unsupervised stereo matching algorithms, such as the lack of global inference and geometric perception capabilities, restricted nonlinear expressive in complex regions, and inadequate fusion of global and local information. These improvements enhance matching accuracy and robustness in complex ill-posed regions. Experimental results fully validate the effectiveness of AGSK-Net and demonstrate the critical role of each proposed module in improving the performance of unsupervised stereo matching. Although AGSK-Net has made significant progress, there are still several directions worth exploring. For example, while the learnable rational functions in SGR-KAN offer strong expressive capability, they incur higher computational costs compared to standard MLP. Future work could investigate more lightweight implementations of KAN or model distillation techniques. In addition, this paper primarily focuses on binocular static images, and extending the geometric perception capability of AG-MSA to the temporal dimension and applying AG-MSA to multi-view stereo (MVS) or stereo video matching tasks represents a highly promising research direction. We believe that the proposed approach of deeply integrating domain prior knowledge into the attention mechanism will provide valuable insights for future research in 3D computer vision.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Q.F. and X.W.; methodology, Q.F.; software, Q.F. and H.W.; validation, Z.L., T.Q., and H.W.; formal analysis, T.Z.; investigation, T.Z.; resources, Z.L. and T.Q.; data curation, Q.F. and T.Q.; writing&#8212;original draft preparation, Q.F.; writing&#8212;review and editing, X.W.; visualization, H.W. and T.Q.; supervision, Z.L. and H.W.; project administration, Z.L. and H.W.; funding acquisition, X.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding authors.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05905"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name></person-group><article-title>RLStereo: Real-time stereo matching based on reinforcement learning</article-title><source>IEEE Trans.</source><year>2021</year><volume>30</volume><fpage>9442</fpage><lpage>9455</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3126418</pub-id><pub-id pub-id-type="pmid">34780325</pub-id></element-citation></ref><ref id="B2-sensors-25-05905"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>P.</given-names></name><name name-style="western"><surname>Bansal</surname><given-names>N.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name></person-group><article-title>Planemvs: 3D plane reconstruction from multi-view stereo</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>8665</fpage><lpage>8675</lpage></element-citation></ref><ref id="B3-sensors-25-05905"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>N.Z.</given-names></name></person-group><article-title>Pointguard: Provably robust 3D point cloud classification</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>6186</fpage><lpage>6195</lpage></element-citation></ref><ref id="B4-sensors-25-05905"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>P.</given-names></name></person-group><article-title>Adaptive Kernel Convolutional Stereo Matching Recurrent Network</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7386</elocation-id><pub-id pub-id-type="doi">10.3390/s24227386</pub-id><pub-id pub-id-type="pmid">39599162</pub-id><pub-id pub-id-type="pmcid">PMC11598454</pub-id></element-citation></ref><ref id="B5-sensors-25-05905"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>J.-R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.-S.</given-names></name></person-group><article-title>Pyramid stereo matching network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>5410</fpage><lpage>5418</lpage></element-citation></ref><ref id="B6-sensors-25-05905"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Mai</surname><given-names>S.</given-names></name></person-group><article-title>Robust Cost Volume Generation Method for Dense Stereo Matching in Endoscopic Scenarios</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>3427</elocation-id><pub-id pub-id-type="doi">10.3390/s23073427</pub-id><pub-id pub-id-type="pmid">37050489</pub-id><pub-id pub-id-type="pmcid">PMC10098972</pub-id></element-citation></ref><ref id="B7-sensors-25-05905"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>T.</given-names></name><name name-style="western"><surname>Brown</surname><given-names>M.</given-names></name><name name-style="western"><surname>Snavely</surname><given-names>N.</given-names></name><name name-style="western"><surname>Lowe</surname><given-names>D.G.</given-names></name></person-group><article-title>Unsupervised learning of depth and ego-motion from video</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>1851</fpage><lpage>1858</lpage></element-citation></ref><ref id="B8-sensors-25-05905"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Garg</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bg</surname><given-names>V.K.</given-names></name><name name-style="western"><surname>Carneiro</surname><given-names>G.</given-names></name><name name-style="western"><surname>Reid</surname><given-names>I.</given-names></name></person-group><article-title>Unsupervised cnn for single view depth estimation: Geometry to the rescue</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>8&#8211;10 and 15&#8211;16 October 2016</conf-date><fpage>740</fpage><lpage>756</lpage></element-citation></ref><ref id="B9-sensors-25-05905"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zha</surname><given-names>Z.J.</given-names></name></person-group><article-title>Degradation-agnostic correspondence from resolution-asymmetric stereo</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>12962</fpage><lpage>12971</lpage></element-citation></ref><ref id="B10-sensors-25-05905"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Unsupervised learning of stereo matching</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>1567</fpage><lpage>1575</lpage></element-citation></ref><ref id="B11-sensors-25-05905"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name></person-group><article-title>Self-supervised multiscale adversarial regression network for stereo disparity estimation</article-title><source>IEEE Trans. Cybern.</source><year>2020</year><volume>51</volume><fpage>4770</fpage><lpage>4783</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2020.2999492</pub-id><pub-id pub-id-type="pmid">32649284</pub-id></element-citation></ref><ref id="B12-sensors-25-05905"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>An</surname><given-names>W.</given-names></name></person-group><article-title>Parallax attention for unsupervised stereo correspondence learning</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>44</volume><fpage>2108</fpage><lpage>2125</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3026899</pub-id><pub-id pub-id-type="pmid">32976095</pub-id></element-citation></ref><ref id="B13-sensors-25-05905"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tosi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bartolomei</surname><given-names>L.</given-names></name><name name-style="western"><surname>Poggi</surname><given-names>M.</given-names></name></person-group><article-title>A survey on deep stereo matching in the twenties</article-title><source>Int. J. Comput. Vis.</source><year>2025</year><volume>133</volume><fpage>4245</fpage><lpage>4276</lpage><pub-id pub-id-type="doi">10.1007/s11263-024-02331-0</pub-id></element-citation></ref><ref id="B14-sensors-25-05905"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamid</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Manap</surname><given-names>N.A.</given-names></name><name name-style="western"><surname>Hamzah</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Kadmin</surname><given-names>A.F.</given-names></name></person-group><article-title>Stereo matching algorithm based on deep learning: A survey</article-title><source>J. King Saud Univ. Comput. Inf. Sci.</source><year>2022</year><volume>34</volume><fpage>1663</fpage><lpage>1673</lpage><pub-id pub-id-type="doi">10.1016/j.jksuci.2020.08.011</pub-id></element-citation></ref><ref id="B15-sensors-25-05905"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B16-sensors-25-05905"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Drenkow</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>A.</given-names></name><name name-style="western"><surname>Creighton</surname><given-names>F.X.</given-names></name><name name-style="western"><surname>Taylor</surname><given-names>R.H.</given-names></name><name name-style="western"><surname>Unberath</surname><given-names>M.</given-names></name></person-group><article-title>Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>6197</fpage><lpage>6206</lpage></element-citation></ref><ref id="B17-sensors-25-05905"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>D.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>N.</given-names></name></person-group><article-title>A transformer-based architecture for high-resolution stereo matching</article-title><source>IEEE Trans. Comput. Imaging</source><year>2024</year><volume>10</volume><fpage>83</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1109/TCI.2024.3350884</pub-id></element-citation></ref><ref id="B18-sensors-25-05905"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.</given-names></name></person-group><article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>10012</fpage><lpage>10022</lpage></element-citation></ref><ref id="B19-sensors-25-05905"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bing</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Knoll</surname><given-names>A.</given-names></name></person-group><article-title>Efficient Stereo Matching Using Swin Transformer and Multilevel Feature Consistency in Autonomous Mobile Systems</article-title><source>IEEE Trans. Ind. Inform.</source><year>2024</year><volume>20</volume><fpage>7957</fpage><lpage>7965</lpage><pub-id pub-id-type="doi">10.1109/TII.2024.3367033</pub-id></element-citation></ref><ref id="B20-sensors-25-05905"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>J.Q.</given-names></name><name name-style="western"><surname>Giannarou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Elson</surname><given-names>D.S.</given-names></name></person-group><article-title>H-net: Unsupervised attention-based stereo depth estimation leveraging epipolar geometry</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>4460</fpage><lpage>4467</lpage></element-citation></ref><ref id="B21-sensors-25-05905"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>J.</given-names></name><name name-style="western"><surname>He</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>F.</given-names></name></person-group><article-title>EI-MVSNet: Epipolar-guided multi-view stereo network with interval-aware label</article-title><source>IEEE Trans. Image Process.</source><year>2024</year><volume>33</volume><fpage>753</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1109/TIP.2023.3347929</pub-id><pub-id pub-id-type="pmid">38194375</pub-id></element-citation></ref><ref id="B22-sensors-25-05905"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>Geometry-enhanced attentive multi-view stereo for challenging matching scenarios</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>34</volume><fpage>7401</fpage><lpage>7416</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2024.3376692</pub-id></element-citation></ref><ref id="B23-sensors-25-05905"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>MVSTER: Epipolar transformer for efficient multi-view stereo</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Cham, Switzerland</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><fpage>573</fpage><lpage>591</lpage></element-citation></ref><ref id="B24-sensors-25-05905"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Taud</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mas</surname><given-names>J.F.</given-names></name></person-group><source>Multilayer perceptron (MLP). Geomatic Approaches for Modeling Land Change Scenarios</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2017</year><fpage>451</fpage><lpage>455</lpage></element-citation></ref><ref id="B25-sensors-25-05905"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Vaidya</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ruehle</surname><given-names>F.</given-names></name><name name-style="western"><surname>Halverson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Solja&#269;i&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Tegmark</surname><given-names>M.</given-names></name></person-group><article-title>Kan: Kolmogorov-arnold networks</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2404.19756</pub-id></element-citation></ref><ref id="B26-sensors-25-05905"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>M.M.</given-names></name></person-group><article-title>Kac: Kolmogorov-arnold classifier for continual learning</article-title><source>Proceedings of the Computer Vision and Pattern Recognition Conference</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>11&#8211;15 June 2025</conf-date><fpage>15297</fpage><lpage>15307</lpage></element-citation></ref><ref id="B27-sensors-25-05905"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name></person-group><article-title>U-kan makes strong backbone for medical image segmentation and generation</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>25 February&#8211;4 March 2025</conf-date><volume>Volume 39</volume><fpage>4652</fpage><lpage>4660</lpage></element-citation></ref><ref id="B28-sensors-25-05905"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Kolmogorov-arnold transformer</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2409.10594</pub-id></element-citation></ref><ref id="B29-sensors-25-05905"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>KM-UNet KAN Mamba UNet for medical image segmentation</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="doi">10.48550/arXiv.2501.02559</pub-id><pub-id pub-id-type="arxiv">2501.02559</pub-id></element-citation></ref><ref id="B30-sensors-25-05905"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name></person-group><article-title>MM-UKAN++: A Novel Kolmogorov-Arnold Network Based U-shaped Network for Ultrasound Image Segmentation</article-title><source>IEEE Trans. Ultrason. Ferroelectr. Freq. Control.</source><year>2025</year><pub-id pub-id-type="doi">10.1109/TUFFC.2025.3539262</pub-id><pub-id pub-id-type="pmid">40031744</pub-id></element-citation></ref><ref id="B31-sensors-25-05905"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kolmogorov</surname><given-names>A.N.</given-names></name></person-group><article-title>On the representations of continuous functions of many variables by superposition of continuous functions of one variable and addition</article-title><source>Dokl. Akad. Nauk USSR.</source><year>1957</year><volume>114</volume><fpage>953</fpage><lpage>956</lpage></element-citation></ref><ref id="B32-sensors-25-05905"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Molina</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schramowski</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kersting</surname><given-names>K.</given-names></name></person-group><article-title>Pad&#233; activation units: End-to-end learning of flexible activation functions in deep networks</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1907.06732</pub-id></element-citation></ref><ref id="B33-sensors-25-05905"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name></person-group><article-title>Coordinate attention for efficient mobile network design</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>13713</fpage><lpage>13722</lpage></element-citation></ref><ref id="B34-sensors-25-05905"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>DGFEG: Dynamic gate fusion and edge graph perception network for remote sensing change detection</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2025</year><volume>18</volume><fpage>3581</fpage><lpage>3598</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2025.3526208</pub-id></element-citation></ref><ref id="B35-sensors-25-05905"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Su</surname><given-names>Y.</given-names></name></person-group><article-title>Multi-scale graph neural network for global stereo matching</article-title><source>Signal Process. Image Commun.</source><year>2023</year><volume>118</volume><fpage>117026</fpage><pub-id pub-id-type="doi">10.1016/j.image.2023.117026</pub-id></element-citation></ref><ref id="B36-sensors-25-05905"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>HKAN: A Hybrid Kolmogorov&#8211;Arnold Network for Robust Fabric Defect Segmentation</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>8181</elocation-id><pub-id pub-id-type="doi">10.3390/s24248181</pub-id><pub-id pub-id-type="pmid">39771916</pub-id><pub-id pub-id-type="pmcid">PMC11679074</pub-id></element-citation></ref><ref id="B37-sensors-25-05905"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brateanu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Balmez</surname><given-names>R.</given-names></name><name name-style="western"><surname>Orhei</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ancuti</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ancuti</surname><given-names>C.</given-names></name></person-group><article-title>Enhancing low-light images with kolmogorov&#8211;arnold networks in transformer attention</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>327</elocation-id><pub-id pub-id-type="doi">10.3390/s25020327</pub-id><pub-id pub-id-type="pmid">39860697</pub-id><pub-id pub-id-type="pmcid">PMC11768327</pub-id></element-citation></ref><ref id="B38-sensors-25-05905"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zach</surname><given-names>J.</given-names></name><name name-style="western"><surname>Stelldinger</surname><given-names>P.</given-names></name></person-group><article-title>Self-Supervised Deep Visual Stereo Odometry with 3D-Geometric Constraints</article-title><source>Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments</source><conf-loc>New York, NY, USA</conf-loc><conf-date>25&#8211;27 June 2025</conf-date><fpage>336</fpage><lpage>342</lpage></element-citation></ref><ref id="B39-sensors-25-05905"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Z.</given-names></name></person-group><article-title>Occlusion aware stereo matching via cooperative unsupervised learning</article-title><source>Proceedings of the Asian Conference on Computer Vision</source><conf-loc>Perth, Australia</conf-loc><conf-date>2&#8211;6 December 2018</conf-date><fpage>197</fpage><lpage>213</lpage></element-citation></ref><ref id="B40-sensors-25-05905"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Segstereo: Exploiting semantic information for disparity estimation</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>636</fpage><lpage>651</lpage></element-citation></ref><ref id="B41-sensors-25-05905"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bendig</surname><given-names>K.</given-names></name><name name-style="western"><surname>Schuster</surname><given-names>R.</given-names></name><name name-style="western"><surname>Stricker</surname><given-names>D.</given-names></name></person-group><article-title>Self-superflow: Self-supervised scene flow prediction in stereo sequences</article-title><source>Proceedings of the 2022 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Bordeaux, France</conf-loc><conf-date>16&#8211;19 October 2022</conf-date><fpage>481</fpage><lpage>485</lpage></element-citation></ref><ref id="B42-sensors-25-05905"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>P.C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.S.</given-names></name></person-group><article-title>Attention-aware feature aggregation for real-time stereo matching on edge devices</article-title><source>Proceedings of the Asian Conference on Computer Vision</source><conf-loc>Kyoto, Japan</conf-loc><conf-date>30 November&#8211;4 December 2020</conf-date></element-citation></ref><ref id="B43-sensors-25-05905"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Brousseau</surname><given-names>P.A.</given-names></name><name name-style="western"><surname>Roy</surname><given-names>S.</given-names></name></person-group><article-title>A permutation model for the self-supervised stereo matching problem</article-title><source>Proceedings of the 2022 19th Conference on Robots and Vision (CRV)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>31 May&#8211;2 June 2022</conf-date><fpage>122</fpage><lpage>131</lpage></element-citation></ref><ref id="B44-sensors-25-05905"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Long</surname><given-names>Q.</given-names></name></person-group><article-title>Self-supervised Learning of PSMNet via Generative Adversarial Networks</article-title><source>Proceedings of the International Conference on Intelligent Computing</source><conf-loc>Singapore</conf-loc><conf-date>25&#8211;27 September 2024</conf-date><fpage>469</fpage><lpage>479</lpage></element-citation></ref><ref id="B45-sensors-25-05905"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cong</surname><given-names>R.</given-names></name><name name-style="western"><surname>Du</surname><given-names>J.</given-names></name></person-group><article-title>Unsupervised hierarchical iterative tile refinement network with 3D planar segmentation loss</article-title><source>IEEE Robot. Autom. Lett.</source><year>2024</year><volume>9</volume><fpage>2678</fpage><lpage>2685</lpage><pub-id pub-id-type="doi">10.1109/LRA.2024.3359545</pub-id></element-citation></ref><ref id="B46-sensors-25-05905"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fidan</surname><given-names>B.</given-names></name></person-group><article-title>Occlusion-aware self-supervised stereo matching with confidence guided raw disparity fusion</article-title><source>Proceedings of the 2022 19th Conference on Robots and Vision (CRV)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>31 May&#8211;2 June 2022</conf-date><fpage>132</fpage><lpage>139</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05905-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall architecture of the proposed approach. <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, &#8230;, <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> denote the feature maps at different hierarchical stages of the network, where deeper levels correspond to lower spatial resolutions and larger channel dimensions. (<bold>a</bold>) Overall architecture of AGSK-Net. The network combines CNN and Stereo-KANformer to extract multi-scale local and global features from left and right images, achieves adaptive fusion through the DCGF module, and ultimately generates fused feature maps for disparity estimation. (<bold>b</bold>) The structure of Stereo-KANformer Block. It utilizes AG-MSA and SGR-KAN to extract geometry-aware global contextual information and enhance nonlinear expressive capability in complex regions. (<bold>c</bold>) The structure of DCGF module. The module adaptively fuses global and local features at different scales to generate more informative fused features.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05905-g001.jpg"/></fig><fig position="float" id="sensors-25-05905-f002" orientation="portrait"><label>Figure 2</label><caption><p>The illustration of matching along the epipolar line and the illustration of our self-attention mechanism based on AGRP.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05905-g002.jpg"/></fig><fig position="float" id="sensors-25-05905-f003" orientation="portrait"><label>Figure 3</label><caption><p>The illustration of Spatial Group-Rational KAN (SGR-KAN). It mainly consists of GR-KAN and 3 &#215; 3 depthwise convolution.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05905-g003.jpg"/></fig><fig position="float" id="sensors-25-05905-f004" orientation="portrait"><label>Figure 4</label><caption><p>Predicted disparity maps on the KITTI 2015 test dataset. Our approach achieves better performance than PASMNet and OASM-Net in complex regions, such as occlusions, weak textures, and small objects, etc.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05905-g004.jpg"/></fig><fig position="float" id="sensors-25-05905-f005" orientation="portrait"><label>Figure 5</label><caption><p>Predicted disparity maps on the KITTI 2012 test dataset. The results of our approach perform better than PASMNet and OASM-Net in ill-posed regions, such as occlusions, repetitive textures, and edges, etc.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05905-g005.jpg"/></fig><fig position="float" id="sensors-25-05905-f006" orientation="portrait"><label>Figure 6</label><caption><p>Predicted disparity maps on the Scene Flow test set. The results of our approach perform better than PASMNet in regions with occlusions, edges, and complex geometric details, etc. The GT represents the ground-truth disparity.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05905-g006.jpg"/></fig><fig position="float" id="sensors-25-05905-f007" orientation="portrait"><label>Figure 7</label><caption><p>Predicted disparity maps on the Middlebury 2021 set. The results of our approach perform better than PASMNet in regions with occlusions, edges, and complex geometric details, etc.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05905-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05905-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05905-t001_Table 1</object-id><label>Table 1</label><caption><p>The ablation study of AGSK-Net. We evaluate EPE on the Scene Flow test dataset and the percentage of 3-pixel error on the KITTI 2015 evaluation dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">ID</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AGSK-Net Setting</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scene Flow</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">KITTI 2015</th></tr><tr><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Swin Transformer
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
Stereo-KANformer
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
DCGF
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
EPE/Pixel
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
&gt;3-Pixel/%
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
AG-MSA
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
SGR-KAN
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID1</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">5.01</td><td align="center" valign="middle" rowspan="1" colspan="1">7.438</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID2</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">4.62</td><td align="center" valign="middle" rowspan="1" colspan="1">7.030</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID3</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">4.28</td><td align="center" valign="middle" rowspan="1" colspan="1">6.715</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID4</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">3.46</td><td align="center" valign="middle" rowspan="1" colspan="1">6.104</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.64
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.695
</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05905-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05905-t002_Table 2</object-id><label>Table 2</label><caption><p>Detailed ablation studies are conducted on the components of AGRP and the overall architecture of AGSK-Net. M-only denotes the use of only the Modulation term (<inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula>) in AGRP, while P-only denotes the use of only the Penalty term (<inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>). Single-Scale (1/16) applies SKB solely at the 1/16 resolution. Multi-Scale (1/16, 1/8) applies SKBs at the 1/16 and 1/8 resolutions. Multi-Scale (Fine, 1/16, 1/8, 1/4) applies SKBs at the 1/16, 1/8, and 1/4 resolutions. Shallow, Medium Depth, and Deep refer to reducing or increasing the number of SKB blocks, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ablation Component</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Setting</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scene Flow<break/>EPE/Pixel</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">KITTI 2015<break/>&gt;3-Pixel/%</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Baseline</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Swin-Transformer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.030</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID3a</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">AGRP Components</td><td align="center" valign="middle" rowspan="1" colspan="1">M-only (<inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">4.45</td><td align="center" valign="middle" rowspan="1" colspan="1">6.881</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID3b</td><td align="center" valign="middle" rowspan="1" colspan="1">P-only (<inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">4.36</td><td align="center" valign="middle" rowspan="1" colspan="1">6.802</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Modulation + Penalty</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.715</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID4a</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SKB Architecture</td><td align="center" valign="middle" rowspan="1" colspan="1">Single-Scale (1/16)</td><td align="center" valign="middle" rowspan="1" colspan="1">2.98</td><td align="center" valign="middle" rowspan="1" colspan="1">6.313</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID4b</td><td align="center" valign="middle" rowspan="1" colspan="1">Multi-Scale (1/16, 1/8)</td><td align="center" valign="middle" rowspan="1" colspan="1">2.90</td><td align="center" valign="middle" rowspan="1" colspan="1">6.265</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID4c</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-Scale (1/16, 1/8, 1/4)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.104</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID4d</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SKB Depth</td><td align="center" valign="middle" rowspan="1" colspan="1">Shallow (1,1,1)</td><td align="center" valign="middle" rowspan="1" colspan="1">2.82</td><td align="center" valign="middle" rowspan="1" colspan="1">6.104</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID4e</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium Depth (1,1,2)</td><td align="center" valign="middle" rowspan="1" colspan="1">2.64</td><td align="center" valign="middle" rowspan="1" colspan="1">5.695</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID4f</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep (1,2,2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.693</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05905-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05905-t005_Table 5</object-id><label>Table 5</label><caption><p>The evaluation on Middlebury 2021. We evaluate the generalization performance by percentage of 3-pixel error and EPE.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Models</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Middlebury 2021</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
&gt;3-Pixel/%
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
EPE/Pixel
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">PASMNet&#160;[<xref rid="B12-sensors-25-05905" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">25.568</td><td align="center" valign="middle" rowspan="1" colspan="1">9.854</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>18.089</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>6.720</bold>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>