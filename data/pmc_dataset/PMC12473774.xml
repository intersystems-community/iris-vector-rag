<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473774</article-id><article-id pub-id-type="pmcid-ver">PMC12473774.1</article-id><article-id pub-id-type="pmcaid">12473774</article-id><article-id pub-id-type="pmcaiid">12473774</article-id><article-id pub-id-type="pmid">41012971</article-id><article-id pub-id-type="doi">10.3390/s25185734</article-id><article-id pub-id-type="publisher-id">sensors-25-05734</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MSRS-DETR: End-to-End Object Detection for Multi-Scale Remote Sensing</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Yuan</surname><given-names initials="J">Jie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05734" ref-type="aff">1</xref><xref rid="af2-sensors-25-05734" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Feng</surname><given-names initials="S">Shuyi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05734" ref-type="aff">1</xref><xref rid="af2-sensors-25-05734" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7912-027X</contrib-id><name name-style="western"><surname>Han</surname><given-names initials="H">Hao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05734" ref-type="aff">1</xref><xref rid="c1-sensors-25-05734" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Kwan</surname><given-names initials="C">Chiman</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05734"><label>1</label>College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210024, China; <email>yuanjie_6952@163.com</email> (J.Y.); <email>feng_shu_yi@aliyun.com</email> (S.F.)</aff><aff id="af2-sensors-25-05734"><label>2</label>Shanghai Aerospace Electronic and Communication Equipment Research Institute, Shanghai 201109, China</aff><author-notes><corresp id="c1-sensors-25-05734"><label>*</label>Correspondence: <email>hhan@nuaa.edu.cn</email>; Tel.: +86-571-87953349</corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5734</elocation-id><history><date date-type="received"><day>24</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>04</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>11</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>14</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05734.pdf"/><abstract><p>Remote sensing imagery (RSI) object detection is critical to many applications, yet mainstream detectors analyse only spatial features and, because of spectral bias, fail to learn high-frequency information adequately, resulting in performance bottlenecks under cluttered backgrounds, distractors, and multi-scale targets, especially small ones. To break these limitations, we propose MSRS-DETR, an end-to-end framework that deeply fuses spatial and frequency cues. The approach introduces three key innovations: (1) C2f<sub>FAT</sub>NET, a frequency-attention-enhanced lightweight residual backbone that provides richer dual-domain features with fewer parameters; (2) an Entanglement Transformer Block (ETB) in the encoder that refines deep semantics via cross-domain frequency&#8211;spatial interaction and suppresses background interference; and (3) S2-CCFF, a shallow-feature-extended bidirectional fusion path that markedly improves the retention and utilisation of fine details for small objects. Experiments on HRSC2016 and ShipRSImageNet demonstrate the effectiveness and generalisation of this spatial&#8211;frequency paradigm: relative to the baseline, MSRS-DETR reduces parameters by <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>29.1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, boosts inference speed by <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>12.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and raises mAP<sub>50-95</sub> by <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.69</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2.16</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p></abstract><kwd-group><kwd>remote sensing</kwd><kwd>object detection</kwd><kwd>DETR</kwd><kwd>frequency&#8211;spatial fusion</kwd><kwd>frequency-domain analysis</kwd><kwd>small object detection</kwd><kwd>multi-scale detection</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05734"><title>1. Introduction</title><p>Object detection is a cornerstone of computer vision, and its historical trajectory closely follows the evolution of deep learning paradigms. The field has progressed from early detectors that relied on handcrafted features through convolutional neural network (CNN)-based architectures to the recent emergence of Transformer-based models, steadily improving in accuracy, speed, and generalisation. Within this context, remote sensing object detection (RSOD) supports vital tasks such as urban planning, resource exploration, environmental monitoring, and national security, and it therefore continues to attract significant research and practical attention [<xref rid="B1-sensors-25-05734" ref-type="bibr">1</xref>].</p><p>When state-of-the-art detectors designed for natural images are applied directly to remote sensing imagery (RSI), their performance is often constrained by domain-specific factors, including sensor noise, cluttered backgrounds, high semantic similarity between targets and surroundings, extreme scale variation, and the high density of small objects.</p><p>Mainstream CNN and Transformer detectors learn and represent features solely in the spatial domain, overlooking the information embedded in the frequency domain. According to Fourier theory, low-frequency components encode global contours and smooth regions, whereas high-frequency components capture edges and textures [<xref rid="B2-sensors-25-05734" ref-type="bibr">2</xref>]. Small objects in RSI correspond mainly to high-frequency signals, and cluttered backgrounds exhibit distinctive frequency-domain patterns. Moreover, deep networks show a spectral bias [<xref rid="B3-sensors-25-05734" ref-type="bibr">3</xref>]; they fit low-frequency information first, which hinders the modelling of high-frequency details. A detector that seamlessly fuses spatial and frequency cues is therefore likely to overcome these intrinsic limitations and advance high-precision RSOD.</p><p>Motivated by this insight, and taking the real-time end-to-end detector RT-DETR [<xref rid="B4-sensors-25-05734" ref-type="bibr">4</xref>] as our baseline, we develop MSRS-DETR, a multi-scale detector tailored for RSI.</p><p>The main contributions of this work are as follows:<list list-type="order"><list-item><p>An efficient end-to-end framework for RSOD that integrates frequency-domain analysis into the backbone, encoder, and feature-fusion stages, thereby improving multi-scale detection in complex scenes.</p></list-item><list-item><p>C2f<sub>FAT</sub>NET, a lightweight residual backbone equipped with frequency attention, which reduces parameters while enhancing the discriminative power of extracted features.</p></list-item><list-item><p>An Entanglement Transformer Block (ETB) that performs joint frequency&#8211;spatial attention to distil robust high-level semantics and suppress background interference.</p></list-item><list-item><p>S2-CCFF, a shallow-feature-augmented bidirectional fusion addition that preserves high-resolution detail and boosts the recall of small targets while introducing modest latency overhead along the fusion path.</p></list-item></list></p><p>The remainder of this paper is organised as follows. <xref rid="sec2-sensors-25-05734" ref-type="sec">Section 2</xref> reviews related work. <xref rid="sec3-sensors-25-05734" ref-type="sec">Section 3</xref> details the proposed MSRS-DETR, including a lightweight backbone refinement that augments the C2f/RepC3 blocks to better preserve fine-grained details, the Entanglement Transformer Block (ETB) for jointly modelling frequency and spatial cues, and the S2-CCFF bidirectional fusion across scales. <xref rid="sec4-sensors-25-05734" ref-type="sec">Section 4</xref> briefly summarises the experimental setup and datasets and reports the main quantitative results with concise qualitative examples. <xref rid="sec5-sensors-25-05734" ref-type="sec">Section 5</xref> concludes this paper and discusses limitations and future directions.</p></sec><sec id="sec2-sensors-25-05734"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05734"><title>2.1. CNN-Based Remote Sensing Object Detection</title><p>Since the rise of deep learning, CNN-driven algorithms have rapidly dominated object detection and have evolved along two main trajectories. The first comprises two-stage detectors typified by the R-CNN family [<xref rid="B5-sensors-25-05734" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05734" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05734" ref-type="bibr">7</xref>], which follow a proposal-then-refinement paradigm. By generating high-quality candidate boxes through a region proposal network, these methods achieve strong accuracy, though their multi-stage pipelines restrict real-time deployment. The second trajectory favours speed and is represented by one-stage detectors such as the You Only Look Once (YOLO) series [<xref rid="B8-sensors-25-05734" ref-type="bibr">8</xref>]. By eliminating the proposal stage and casting detection as direct regression and classification, YOLO has, through successive iterations, struck an exceptional balance between speed and precision. The latest version, YOLOv8 [<xref rid="B9-sensors-25-05734" ref-type="bibr">9</xref>], introduces a more efficient C2f backbone, an anchor-free detection head, and advanced loss functions, making it a widely adopted baseline in both academia and industry. To tackle the intrinsic multi-scale challenge of CNNs, Feature Pyramid Networks (FPNs) [<xref rid="B10-sensors-25-05734" ref-type="bibr">10</xref>] add a top-down pathway with lateral connections, merging deep semantic information with shallow spatial detail and becoming a standard component of modern detectors.</p><p>When these advanced CNN detectors are applied directly to remote sensing imagery, their inherent limitations become evident. The local receptive field of convolutions hampers the capture of the global context and long-range dependencies required for large-scene interpretation. Repeated down-sampling, essential for building deep semantic features, inevitably discards information; the faint signatures of small objects, common in RSI, are easily overwhelmed during propagation. Consequently, the upper bound of CNN performance in complex remote sensing scenarios is constrained by the architecture itself, motivating research into paradigms with stronger global modelling capacity.</p><p>In recent years, many remote sensing detectors have extended fast YOLO-style baselines to address domain challenges, such as small, sparse targets and cluttered backgrounds, while keeping real-time efficiency. Representative examples include MSA-YOLO, which introduces multi-scale strip attention, GS-PANet fusion, and a Wise-Focal CIoU loss to suppress background noise and balance sample contributions [<xref rid="B11-sensors-25-05734" ref-type="bibr">11</xref>]; YOLO-RS, which couples ASFF/OSPP-like aggregation with a lightweight head to improve accuracy and throughput on aerial benchmarks [<xref rid="B12-sensors-25-05734" ref-type="bibr">12</xref>]; and YOLO-RSA, a multi-scale design specialised for ship scenarios in optical imagery [<xref rid="B13-sensors-25-05734" ref-type="bibr">13</xref>]. Under adverse weather, CM-YOLO further applies component-decoupling background suppression and local&#8211;global semantic mining to maintain robustness in cloud/mist scenes [<xref rid="B14-sensors-25-05734" ref-type="bibr">14</xref>]. Complementary surveys synthesise that attention modules, lightweight backbones, and enhanced multi-scale fusion (e.g., ASFF/BiFPN variants) are effective methods for improving detection in RSI without sacrificing speed [<xref rid="B15-sensors-25-05734" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05734" ref-type="bibr">16</xref>].</p></sec><sec id="sec2dot2-sensors-25-05734"><title>2.2. Transformer-Based Remote Sensing Object Detection</title><p>To overcome the locality of CNNs, the Transformer architecture, originally devised for natural language processing, has been introduced into computer vision and is now reshaping object detection. Starting with the Vision Transformer (ViT) [<xref rid="B17-sensors-25-05734" ref-type="bibr">17</xref>], its self-attention mechanism enables direct interactions between arbitrary pixels, yielding a true global receptive field.</p><p>DETR (Detection Transformer) [<xref rid="B18-sensors-25-05734" ref-type="bibr">18</xref>] is the seminal work in this line. By adopting an encoder&#8211;decoder Transformer and learnable object queries, DETR reformulates detection as an end-to-end set prediction task, discarding anchors and non-maximum suppression in favour of bipartite matching and set loss supervision. Despite its elegance, the first version converges slowly and struggles with small objects. Deformable DETR [<xref rid="B19-sensors-25-05734" ref-type="bibr">19</xref>] addresses these issues by introducing deformable attention, which focuses computation on a sparse set of sampling points, greatly reducing complexity, accelerating convergence, and improving small-object recall. To extend DETR&#8217;s end-to-end advantages to real-time scenarios, RT-DETR [<xref rid="B4-sensors-25-05734" ref-type="bibr">4</xref>] employs a lightweight hybrid encoder comprising a single Transformer layer on top of deep CNN features and adopts uncertainty-minimal query selection. It thus achieves very high throughput while maintaining accuracy, performing competitively with, and in many cases surpassing, heavily optimised YOLO variants on the reported benchmarks, and establishing a strong standard for real-time end-to-end detection.</p><p>Although Transformer-based detectors excel at global modelling, several limitations remain in remote sensing. First, they typically require large-scale pre-training, whereas well-annotated RSI datasets are scarce. Second, in hybrid designs such as RT-DETR, image features still undergo multiple CNN down-sampling operations before entering the Transformer, so the loss of small-object information persists [<xref rid="B20-sensors-25-05734" ref-type="bibr">20</xref>], leaving the subsequent attention layers with insufficient detail.</p><p>Building on the real-time encoder&#8211;decoder paradigm, RS-DETR adapts RT-DETR to aerial scenarios via cross-scale gating, EBiFPN-style fusion, and refined IoU objectives, reporting improved accuracy while preserving throughput on representative benchmarks [<xref rid="B21-sensors-25-05734" ref-type="bibr">21</xref>]. Recent surveys further emphasise that to stabilise optimisation and accommodate diverse object scales in remote sensing, Transformer components are increasingly combined with CNN pyramids and domain-tailored fusion necks [<xref rid="B16-sensors-25-05734" ref-type="bibr">16</xref>]. Overall, these advances suggest that carefully designed fusion and loss formulations are key to transferring Transformer-style global reasoning to remote sensing imagery.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05734"><title>3. Methodology</title><p>To address the widespread challenges in remote sensing imagery of detecting multi-scale objects, especially small targets, coping with heavy background clutter, and overcoming the limited representational capacity of existing end-to-end detectors, we build on the high-performance real-time detector RT-DETR [<xref rid="B4-sensors-25-05734" ref-type="bibr">4</xref>] and propose MSRS-DETR, a new multi-scale remote sensing detection model that revises the backbone, encoder, and decoder designs. <xref rid="sensors-25-05734-f001" ref-type="fig">Figure 1</xref> gives an overview of the architecture, which comprises a lightweight frequency-aware residual backbone C2f<sub>FAT</sub>NET, an Entanglement Transformer Block (ETB) for frequency&#8211;spatial feature extraction, and an S2-CCFF fusion module that leverages shallow features.</p><sec id="sec3dot1-sensors-25-05734"><title>3.1. Backbone</title><p>The backbone underpins the detector, and the quality of its features directly sets the performance ceiling. An effective backbone should extract discriminative multi-scale features while keeping computational cost low. In MSRS-DETR we introduce C2f<sub>FAT</sub>NET, a frequency-aware backbone illustrated in <xref rid="sensors-25-05734-f001" ref-type="fig">Figure 1</xref>. Classic C2f blocks handle the shallow S2 and deepest S5 stages for generic features, whereas the middle stages S3 and S4 adopt the new C2f<sub>FAT</sub> blocks for stronger discrimination.</p><p>The C2f block, derived from CSPNet [<xref rid="B22-sensors-25-05734" ref-type="bibr">22</xref>], splits the input along channels into a shortcut branch and a bottleneck branch, then concatenates and fuses them with a <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, thus enriching gradients and reducing redundancy. However, its bottleneck relies solely on standard convolutions, a spatially local operation that lacks long-range context.</p><p>We therefore replace the bottleneck with a Frequency-Aware Transformer (FAT) module [<xref rid="B23-sensors-25-05734" ref-type="bibr">23</xref>], which extends analysis from the spatial domain to the joint frequency&#8211;spatial domain. As shown in <xref rid="sensors-25-05734-f002" ref-type="fig">Figure 2</xref>, FAT contains Frequency-Decomposition Window Attention (FDWA) and a Frequency-Modulation Feed-Forward Network (FMFFN).</p><p>FDWA runs four attention windows of different shapes in parallel: a large square window for low-frequency context (LL-WA), a small square window for high-frequency detail (HH-WA), and two rectangular windows for vertical (HL-WA) and horizontal (LH-WA) anisotropic features. Thus, a single block can decompose multi-scale and multi-directional frequency components that describe oriented ships or textured vegetation.</p><p>FMFFN adaptively refines the features as<disp-formula id="FD1-sensors-25-05734"><label>(1)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="script">B</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="script">B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>ffn</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8857;</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>ffn</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the input of the block, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">B</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">B</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denote block partition and merge, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the fast Fourier transform and its inverse. The learnable filter <italic toggle="yes">W</italic> modulates the spectrum, amplifying informative frequencies and suppressing noise.</p><p>By integrating FAT, the C2f<sub>FAT</sub> block evolves from a purely spatial extractor into a dual-domain module. Despite higher internal complexity than a single bottleneck, its stronger representation allows fewer parameters than stacking multiple convolutions and furnishes richer cues for downstream detection, particularly where pure spatial features fall short.</p></sec><sec id="sec3dot2-sensors-25-05734"><title>3.2. Encoder</title><sec id="sec3dot2dot1-sensors-25-05734"><title>3.2.1. Entanglement Transformer Block</title><p>To extract robust features when targets and background share high visual similarity, we append an Entanglement Transformer Block (ETB) [<xref rid="B24-sensors-25-05734" ref-type="bibr">24</xref>] after the deepest backbone stage S5. ETB jointly models features in frequency and spatial domains through parallel processing and deep interaction. <xref rid="sensors-25-05734-f003" ref-type="fig">Figure 3</xref> outlines its three parts: frequency self-attention (FSA), spatial self-attention (SSA), and an Entangled Feed-Forward Network (EFFN). After layer normalisation, the input is sent to FSA and SSA in parallel; their outputs are fused, renormalised, and fed to EFFN.</p><p>FSA captures global frequency dependencies. The input <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is linearly projected, transformed with FFT, and produces complex-valued queries <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, keys <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and values <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The resulting attention map <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#923;</mml:mo><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is split into real and imaginary parts, each soft-maxed and recombined, enabling the model to correlate periodic or textured patterns across the spectrum before transformation back to the spatial domain via IFFT.</p><p>SSA models local spatial context efficiently. We employ multi-scale depthwise separable convolution (MSC) to generate <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, focusing attention on geometry and local relations.</p><p>EFFN performs two-stage cross-domain exchange. First, the fused features are projected separately to spatial and frequency domains. Second, the two outputs are concatenated and re-entered into parallel branches for deeper interaction, forcing repeated alignment between complementary cues. When spatial details are obscured, frequency patterns such as texture still guide discrimination, yielding robust representations with negligible parameter overhead. Parallel FSA and SSA also exploit GPU concurrency for faster inference.</p></sec><sec id="sec3dot2dot2-sensors-25-05734"><title>3.2.2. S2-Enhanced CNN-Based Cross-Scale Feature Fusion</title><p>Severe scale variation, especially tiny targets buried by repeated down-sampling, motivates the S2-Enhanced CNN-based Cross-Scale Feature Fusion (S2-CCFF). Using a bidirectional topology, it exchanges information among backbone outputs <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A top-down semantic path up-samples S5 to S4, concatenates with <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and aggregates them with a RepC3 block, iterating to S2. The fused high-resolution layer <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is then the source of a bottom-up localisation path. After a stride-3 convolution, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is down-sampled, fused with the semantic output at S3, and processed again by RepC3, propagating precise edges upward while semantic cues flow downward. Thus, each scale receives both rich semantics and fine localisation, boosting small-object detection.</p><p>RepC3 is chosen as the fusion unit, see <xref rid="sensors-25-05734-f004" ref-type="fig">Figure 4</xref>. A fixed receptive field limits ordinary convolutions, whereas RepC3 leverages structural re-parameterisation [<xref rid="B25-sensors-25-05734" ref-type="bibr">25</xref>]. During training, its RepConv branch mates a <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, a <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and an identity pathway, akin to an ensemble of receptive fields. At deployment, these branches are algebraically folded into a single <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution by padding the <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> kernel and merging batch-normalisation statistics, giving standard <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> cost yet richer representation.</p></sec></sec><sec id="sec3dot3-sensors-25-05734"><title>3.3. Decoder</title><p>To remain efficient and free of non-maximum suppression, the decoder adopts the query selection strategy from the baseline RT-DETR [<xref rid="B4-sensors-25-05734" ref-type="bibr">4</xref>]. The mechanism, referred to as uncertainty-minimal query selection (UmQS), is designed to provide high-quality initial queries to the decoder. It operates by first using an auxiliary prediction head on the encoder&#8217;s output features to generate initial classification scores. Then, the features corresponding to the top-K highest scores are selected to serve as the initial object queries for the decoder layers. This approach ensures that the decoder focuses its attention on feature map regions with a high probability of containing objects, thereby improving both convergence speed and overall efficiency.</p><p>These selected queries pass through several identical decoder layers, each applying self-attention to suppress redundancy and cross-attention to gather evidence from encoder features. Training is supervised by a set-to-set loss with Hungarian matching between <italic toggle="yes">N</italic> predictions and <italic toggle="yes">M</italic> ground truths: <disp-formula id="FD2-sensors-25-05734"><label>(2)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>decoder</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>GIoU</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>GIoU</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>cls</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the focal loss [<xref rid="B26-sensors-25-05734" ref-type="bibr">26</xref>] to tackle class imbalance, and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>GIoU</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> [<xref rid="B27-sensors-25-05734" ref-type="bibr">27</xref>] supervise bounding box regression. Losses are applied to every decoder layer and summed, realising an end-to-end pipeline without postprocessing.</p></sec></sec><sec id="sec4-sensors-25-05734"><title>4. Experiments and Results</title><sec id="sec4dot1-sensors-25-05734"><title>4.1. Experimental Setup</title><sec id="sec4dot1dot1-sensors-25-05734"><title>4.1.1. Datasets</title><p>To conduct a comprehensive and multi-faceted evaluation of the proposed MSRS-DETR model, we selected two representative public benchmarks for remote sensing ship detection, namely, HRSC2016 and ShipRSImageNet. These datasets span scenarios from simple to highly complex, enabling an effective assessment of the model&#8217;s accuracy, robustness, and generalisation, especially for multi-scale and small-object detection. <xref rid="sensors-25-05734-f005" ref-type="fig">Figure 5</xref> visualises the differences in target scale and spatial distribution between the two datasets.</p><p>HRSC2016 [<xref rid="B28-sensors-25-05734" ref-type="bibr">28</xref>] is a classic benchmark for ship recognition in high-resolution optical remote sensing imagery. It contains 1061 images collected from six major ports, officially split into 436 training, 181 validation, and 444 test images, with 2976 annotated ship instances in total. Resolutions range from 0.4 m to 2.0 m and image sizes vary from <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>300</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>300</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1500</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>900</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels. Images cover open-sea scenes with simple backgrounds and inshore-port scenes with highly cluttered backgrounds, thereby testing detection performance under diverse conditions. As shown in <xref rid="sensors-25-05734-f005" ref-type="fig">Figure 5</xref>a, the target-size distribution is relatively uniform, making HRSC2016 suitable for evaluating multi-scale detection. Imagery is derived from Google Earth across six harbours; the dataset records harbor, data source, image date, geographic coordinates, resolution layer, and scale.</p><p>ShipRSImageNet [<xref rid="B29-sensors-25-05734" ref-type="bibr">29</xref>] is larger, more fine-grained, and more diverse. It comprises 3435 images with 17,573 ship instances sourced from various remote sensing platforms worldwide. Compared with HRSC2016, its target sizes vary more widely and include many small objects (<xref rid="sensors-25-05734-f005" ref-type="fig">Figure 5</xref>b). Complex backgrounds and heterogeneous scenes pose additional challenges for feature representation. Consequently, ShipRSImageNet is used in ablation studies to verify the effectiveness and generalisation of each proposed module for small-object detection in complex scenes. Images are collected from multiple sources including xView (WorldView-3, 0.3 m GSD, chips of <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>930</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>930</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), HRSC2016 and FGSD, the Airbus Ship Detection Challenge, and Chinese satellites such as GaoFen-2 and JiLin-1; the official split is 2198/550/687 (train/validation/test).</p></sec><sec id="sec4dot1dot2-sensors-25-05734"><title>4.1.2. Evaluation Metrics</title><p>Model performance is assessed with a set of accuracy and efficiency metrics. Accuracy follows the COCO protocol and reports precision (P), recall (R), and mean Average Precision (mAP). Efficiency focuses on model complexity and inference speed, quantified by the number of parameters (Params) and theoretical computation (GFLOPs). Definitions are summarised in <xref rid="sensors-25-05734-t001" ref-type="table">Table 1</xref>. For both HRSC2016 and ShipRSImageNet, detections are evaluated using horizontal bounding boxes (HBB) parameterised as <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with standard axis-aligned IoU. Unless otherwise noted, we report AP<sub>50</sub>, AP<sub>75</sub>, and mAP<sub>50:95</sub>.</p></sec><sec id="sec4dot1dot3-sensors-25-05734"><title>4.1.3. Implementation Details</title><p>All experiments were run on a single GeForce RTX 4090 (NVIDIA, Santa Clara, USA) to ensure fair comparison. The software stack consisted of Ubuntu 22.04, PyTorch 2.4.0, CUDA 12.4, and Python 3.12.10. Training employed the AdamW optimiser. Input images were resized to <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the batch size was 8, and training lasted 500 epochs with a cosine-annealing learning-rate schedule. Inference speed (FPS) was measured on single images (batch size = 1) and averaged over the entire test set; preprocessing was included. Key settings were as follows. We trained for 500 epochs with a batch size of 8 at an input size of <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. We used AdamW with an initial learning rate of <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, momentum of <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and weight decay of <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Data augmentation applied horizontal flipping with probability 0.5 together with random translation of 0.1 and random scaling of 0.5. We did not use rotation, mosaic, mixup, or copy&#8211;paste, and automatic mixed precision was off. We used an IoU threshold of 0.7 and limited the number of detections to 300. We fixed the random seed to 0, enabled deterministic training, and used four data-loading workers.</p></sec></sec><sec id="sec4dot2-sensors-25-05734"><title>4.2. Comparative Experiments</title><p>To validate the overall capability of MSRS-DETR, we conducted a horizontal comparison on the HRSC2016 dataset against several mainstream detectors, including our baseline model, RT-DETR-R18. The experimental results are presented in <xref rid="sensors-25-05734-t002" ref-type="table">Table 2</xref>.</p><p>The experimental results demonstrate that our proposed MSRS-DETR strikes an exceptional balance between accuracy, efficiency, and model complexity, yielding significant performance gains. Compared to the baseline RT-DETR-R18, MSRS-DETR achieves a higher core mAP<sub>50-95</sub> score along with improvements in mAP<sub>50</sub> and mAP<sub>75</sub>, all with a remarkable 29.1% reduction in parameters. This result showcases a significant performance boost alongside model lightweighting. It is noteworthy that while the theoretical computation (GFLOPs) of MSRS-DETR is higher than the baseline, our subsequent ablation study shows that its practical inference speed is superior. When compared to YOLOv8m, which has similar computational load, MSRS-DETR achieves a 0.18% (HRSC2016) and 0.28% (ShipRSImageNet) higher mAP<sub>50-95</sub> with fewer GFLOPs. Compared to YOLOv8s, which has a similar parameter count, MSRS-DETR surpasses it by 2.59% (HRSC2016) and 4.92% (ShipRSImageNet) in the core mAP<sub>50-95</sub> metric. Furthermore, when measured against the top-performing RT-DETR-R101, MSRS-DETR uses only 18.9% of its parameters and 29.8% of its GFLOPs while lagging by a mere 0.55% in mAP<sub>50-95</sub> on HRSC2016 and exceeding it by 1.38% on ShipRSImageNet. Evidently, MSRS-DETR maintains high accuracy while significantly reducing model complexity and computational cost, demonstrating its outstanding performance on multi-scale ship remote sensing object detection tasks.</p></sec><sec id="sec4dot3-sensors-25-05734"><title>4.3. Ablation Study</title><p>To clearly validate the individual effectiveness and synergistic effects of the proposed C2f<sub>FAT</sub>NET backbone, ETB module, and S2-CCFF module, we conducted a series of ablation experiments. Starting with RT-DETR-R18 as the baseline, we progressively integrated our proposed components and evaluated the performance on both the HRSC2016 and ShipRSImageNet datasets. The results are detailed in <xref rid="sensors-25-05734-t003" ref-type="table">Table 3</xref>.</p><p>The results of the ablation study show that isolating the introduction of the C2f<sub>FAT</sub>NET backbone reduces the model&#8217;s parameter count from 19.9 M to 15.2 M (a 23.6% decrease) and lowers GFLOPs from 56.9 G to 51.3 G. While making the model more lightweight, the core mAP<sub>50-95</sub> metric improved by 0.98% on HRSC2016 and 0.34% on ShipRSImageNet. This confirms that the FATBlock-enhanced C2f<sub>FAT</sub>NET, with its powerful frequency-domain feature extraction, can achieve more efficient multi-scale feature extraction in RSI with fewer parameters and lower computational cost, thus achieving performance gains alongside backbone lightweighting.</p><p>When we introduced the ETB module for feature extraction at the top of the encoder, the core mAP<sub>50-95</sub> metric improved by 0.67% and 0.34% on the two datasets, respectively, with almost no change in parameters or GFLOPs. Notably, the practical inference speed (FPS) increased by a remarkable 52.1% on HRSC2016 and 40.2% on ShipRSImageNet. This provides strong evidence that ETB&#8217;s unique frequency&#8211;spatial entanglement mechanism can capture more discriminative deep features, and its highly parallel structure effectively utilises the computational power of modern GPUs, leading to a substantial boost in practical inference efficiency without increasing the theoretical computational burden.</p><p>The isolated introduction of the S2-CCFF module yields the most significant accuracy gains on the more challenging ShipRSImageNet dataset, which is rich in small targets, with an improvement of 2.13% in mAP<sub>50-95</sub>. Although this module increases GFLOPs due to processing a higher-resolution feature map, the resulting accuracy benefit is substantial. This demonstrates that injecting the high-resolution details from the S2 layer into the bidirectional fusion path is an effective strategy for preserving and utilising key small-object cues and reducing the missed detection rate.</p><p>The full MSRS-DETR model, combining all three modules, achieved the best mAP<sub>50-95</sub> performance across all configurations on both datasets, improving upon the baseline by 1.69% and 2.16%, respectively. The final performance gain surpasses that of any partial combination, indicating a strong synergistic effect among our designed components. C2f<sub>FAT</sub>NET provides richer features at the source, the ETB module refines deep semantics, and the S2-CCFF module ensures effective detail fusion, together forming an efficient and powerful feature processing pipeline. With a 29.1% reduction in parameters compared to the baseline, the MSRS-DETR model achieves higher detection accuracy and faster inference speed (FPS increased by 12.4% and 8.4% on the two datasets), striking an optimal balance between model complexity, inference efficiency, and detection performance. While the ETB&#8217;s parallel feature extraction demonstrably improves runtime and accuracy in isolation (<xref rid="sensors-25-05734-t003" ref-type="table">Table 3</xref>), the end-to-end speedup of the full model is constrained by the S2-CCFF detail-extraction and fusion strategy on the high-resolution path; consequently, overall throughput improves only modestly over the baseline but yields the best accuracy&#8211;efficiency trade-off.</p></sec><sec id="sec4dot4-sensors-25-05734"><title>4.4. Visual Results</title><p>We further performed qualitative analyses to visualise improvements in challenging scenarios. <xref rid="sensors-25-05734-f006" ref-type="fig">Figure 6</xref> compares MSRS-DETR with the baseline on HRSC2016, illustrating reductions in false positives, increased true positives in cluttered backgrounds, and superior multi-scale detection. Concretely, <xref rid="sensors-25-05734-f006" ref-type="fig">Figure 6</xref>a shows that MSRS-DETR yields fewer onshore false positives (e.g., quayside structures), <xref rid="sensors-25-05734-f006" ref-type="fig">Figure 6</xref>b demonstrates more reliable recognition under background interference and look-alike structures where the baselines exhibit both false alarms and misses, and <xref rid="sensors-25-05734-f006" ref-type="fig">Figure 6</xref>c highlights better recall of small objects in multi-scale scenes. <xref rid="sensors-25-05734-f007" ref-type="fig">Figure 7</xref> shows similar advantages on ShipRSImageNet, particularly for dense small targets, evidencing the efficacy of S2-CCFF. Specifically, in <xref rid="sensors-25-05734-f007" ref-type="fig">Figure 7</xref>a YOLOv8m exhibits imprecise delineation of small targets and RT-DETR shows higher miss rates, whereas MSRS-DETR localises dense, multi-scale vessels more accurately; in <xref rid="sensors-25-05734-f007" ref-type="fig">Figure 7</xref>b, under background interference, MSRS-DETR achieves more true positives with fewer false positives than the baselines. These visual results corroborate the quantitative findings and highlight the balanced accuracy, speed, and efficiency achieved by MSRS-DETR.</p></sec><sec sec-type="discussion" id="sec4dot5-sensors-25-05734"><title>4.5. Discussion</title><p>MSRS-DETR improves detection in cluttered ports and dense small-object scenes by aligning frequency-aware feature extraction with an NMS-free DETR pipeline. The gains follow the roles of the three components and the patterns in <xref rid="sec4dot4-sensors-25-05734" ref-type="sec">Section 4.4</xref>. C2f<sub>FAT</sub>NET strengthens fine-grained cues at the source and stabilises optimisation under heterogeneous backgrounds, which supports higher recall and fewer background-induced false alarms. ETB couples frequency and spatial information and reduces spectral bias, which preserves high-frequency detail that characterises small vessels and textured surroundings and also explains the strong runtime gains when ETB is used alone. S2-CCFF injects shallow high-resolution signals and propagates edges upward while semantics flow downward, which increases small-object recall in crowded regions. This benefit introduces extra work along the fusion path and bounds the end-to-end speedup once all modules are assembled. The trade-off is consistent with the ablation in <xref rid="sensors-25-05734-t003" ref-type="table">Table 3</xref> and with the qualitative panels that show fewer quayside false positives and better localisation of small boats under interference.</p><p>The component-to-failure-mode mapping is clear in practice. Extremely low-contrast targets can still be missed when wakes or shadows dominate the local spectrum, and S2-CCFF reduces this miss rate by recovering edges while ETB sharpens texture cues. Look-alike human-made structures can trigger false alarms, and C2f<sub>FAT</sub>NET improves class separation through more selective early features while ETB suppresses background leakage. Very large targets can be partially detected when effective context is limited, and the multi-scale route mitigates this behaviour by passing both localisation detail and deep semantics across scales. Together, these mechanisms produce the accuracy and efficiency balance reported for the full model.</p><p>The improvements we report come from the frequency&#8211;spatial fusion architecture that operates upstream of the box head and is therefore representation agnostic. To isolate architectural effects and preserve internal validity with the RT DETR baseline, we adopt horizontal bounding boxes, which keep Hungarian matching, losses, and uncertainty minimal query selection unchanged. Introducing oriented boxes would add angle parameterisation, periodicity handling, and rotated IoU and assignment, which could confound attribution of gains to our modules. HRSC2016 and ShipRSImageNet both natively support horizontal annotations and evaluation, which allows stable and directly comparable reporting without redesigning protocols. In future work we will add oriented detection and oriented metrics to complete the picture and to compare against methods that use rotated heads.</p><p>MSRS-DETR is positioned in the real-time end-to-end line that begins with DETR set prediction without NMS and extends to RT DETR for throughput. Our pathway differs from common FPN or EBiFPN necks that fuse only in the spatial domain. Frequency analysis appears in the backbone through FAT and in the encoder through ETB, so the fusion module receives inputs whose fine detail is already enhanced rather than recovered post hoc. The shallow bidirectional fusion is instantiated by RepC<sub>N</sub> with <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in our design and targets small-object detail while leaving the decoder unchanged in the RT DETR sense. This division of labour helps explain why accuracy improves most on ShipRSImageNet, where tiny targets dominate, while the end-to-end property is preserved.</p><p>The framework is practical with public cloud-hosted imagery and can run economically at scale. A typical workflow accesses Sentinel 2 L2A and Landsat ARD in place, applies quality masks such as the SCL layer for Sentinel 2 and QA bits for Landsat, normalises radiometry when mixed sensors are used, and windows space and time to produce chips at the detector input size. This pre-detector adaptation layer documents provenance and explains tolerance to varied spatial resolutions, and the detector then serves as the core of a multi-scale pipeline on cloud-native platforms with object storage and on-the-fly tiling and masking. We expect these steps to generalise beyond ships and to broaden usage in large-area monitoring where repeated coverage and low cost are important.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05734"><title>5. Conclusions</title><p>To overcome the performance bottleneck caused by the exclusive use of spatial information in existing remote sensing detectors, we have presented MSRS-DETR, a paradigm that deeply fuses spatial and frequency cues. The proposed model integrates a frequency-aware backbone network C2f<sub>FAT</sub>NET, an Entanglement Transformer Block (ETB), and a shallow-feature-enhanced bidirectional fusion framework S2-CCFF into the backbone, encoder, and fusion stages, respectively. This design systematically alleviates small-object information loss and background interference from the initial feature extraction stage through multi-scale feature fusion.</p><p>Experiments on the HRSC2016 and ShipRSImageNet benchmarks show that MSRS-DETR markedly reduces parameter count while simultaneously improving detection accuracy and inference speed, confirming the effectiveness of joint spatial&#8211;frequency fusion in complex remote sensing scenarios for multi-scale ship remote sensing object detection. While the proposed method shows strong performance on these ship benchmarks, future work should focus on validating this approach on a wider variety of object classes (e.g., vehicles, aircraft) to rigorously assess its generalisability. This balanced outcome is achieved through the synergy of its components: the C2f<sub>FAT</sub>NET backbone reduces parameters while enriching features, the ETB block significantly boosts practical inference speed, and the S2-CCFF module trades some of that speed gain for critical accuracy improvements on small targets. Future work will explore lighter and more adaptive spatial&#8211;frequency co-learning strategies, aiming to apply the dual-domain feature extraction and fusion scheme more efficiently to even more complex, multi-scenario, and multi-scale remote sensing detection tasks.</p></sec></body><back><ack><title>Acknowledgments</title><p>We would like to thank our students for their work during the algorithm testing.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualisation, H.H.; methodology, J.Y.; software, J.Y.; validation, S.F.; formal analysis, H.H.; investigation, J.Y.; resources, H.H.; data curation, S.F.; writing&#8212;original draft preparation, J.Y.; writing&#8212;review and editing, H.H.; visualisation, S.F.; supervision, H.H.; project administration, H.H.; funding acquisition, H.H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The HRSC2016 and ShipRSImageNet datasets used in this study are publicly available resources. The trained model weights have been made publicly available on GitHub (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/loges00/MSRS-DETR/releases/tag/weights">https://github.com/loges00/MSRS-DETR/releases/tag/weights</uri>, accessed on 25 August 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05734"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>G.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name></person-group><article-title>A survey on object detection in optical remote sensing images</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2016</year><volume>117</volume><fpage>11</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2016.03.014</pub-id></element-citation></ref><ref id="B2-sensors-25-05734"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name></person-group><article-title>Unified Spatial-Frequency Modeling and Alignment for Multi-Scale Small Object Detection</article-title><source>Symmetry</source><year>2025</year><volume>17</volume><elocation-id>242</elocation-id><pub-id pub-id-type="doi">10.3390/sym17020242</pub-id></element-citation></ref><ref id="B3-sensors-25-05734"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Z.-Q.J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name></person-group><article-title>Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks</article-title><source>Commun. Comput. Phys.</source><year>2020</year><volume>28</volume><fpage>1746</fpage><lpage>1767</lpage><pub-id pub-id-type="doi">10.4208/cicp.oa-2020-0085</pub-id></element-citation></ref><ref id="B4-sensors-25-05734"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>DETRs Beat YOLOs on Real-time Object Detection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.08069</pub-id></element-citation></ref><ref id="B5-sensors-25-05734"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Donahue</surname><given-names>J.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>J.</given-names></name></person-group><article-title>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</article-title><source>Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Columbus, OH, USA</conf-loc><conf-date>23&#8211;28 June 2014</conf-date><fpage>580</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.81</pub-id></element-citation></ref><ref id="B6-sensors-25-05734"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Fast R-CNN</article-title><source>Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Santiago, Chile</conf-loc><conf-date>7&#8211;13 December 2015</conf-date><fpage>1440</fpage><lpage>1448</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.169</pub-id></element-citation></ref><ref id="B7-sensors-25-05734"><label>7.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</article-title><source>Advances in Neural Information Processing Systems</source><person-group person-group-type="editor"><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lawrence</surname><given-names>N.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>D.</given-names></name><name name-style="western"><surname>Sugiyama</surname><given-names>M.</given-names></name><name name-style="western"><surname>Garnett</surname><given-names>R.</given-names></name></person-group><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Sydney, Australia</publisher-loc><year>2015</year><volume>Volume 28</volume></element-citation></ref><ref id="B8-sensors-25-05734"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You Only Look Once: Unified, Real-Time Object Detection</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>779</fpage><lpage>788</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id></element-citation></ref><ref id="B9-sensors-25-05734"><label>9.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chaurasia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.</given-names></name></person-group><article-title>YOLO by Ultralytics</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/ultralytics" ext-link-type="uri">https://github.com/ultralytics/ultralytics</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-05-08">(accessed on 8 May 2024)</date-in-citation></element-citation></ref><ref id="B10-sensors-25-05734"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name></person-group><article-title>Feature pyramid networks for object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>2117</fpage><lpage>2125</lpage></element-citation></ref><ref id="B11-sensors-25-05734"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>K.</given-names></name></person-group><article-title>MSA-YOLO: A Remote Sensing Object Detection Model Based on Multi-Scale Strip Attention</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6811</elocation-id><pub-id pub-id-type="doi">10.3390/s23156811</pub-id><pub-id pub-id-type="pmid">37571594</pub-id><pub-id pub-id-type="pmcid">PMC10422293</pub-id></element-citation></ref><ref id="B12-sensors-25-05734"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>T.</given-names></name><name name-style="western"><surname>Han</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name></person-group><article-title>YOLO-RS: A More Accurate and Faster Object Detection Method for Remote Sensing Images</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>3863</elocation-id><pub-id pub-id-type="doi">10.3390/rs15153863</pub-id></element-citation></ref><ref id="B13-sensors-25-05734"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>B.</given-names></name></person-group><article-title>YOLO-RSA: A Multiscale Ship Detection Algorithm Based on Optical Remote Sensing Image</article-title><source>J. Mar. Sci. Eng.</source><year>2024</year><volume>12</volume><elocation-id>603</elocation-id><pub-id pub-id-type="doi">10.3390/jmse12040603</pub-id></element-citation></ref><ref id="B14-sensors-25-05734"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><article-title>CM-YOLO: Typical Object Detection Method in Remote Sensing Cloud and Mist Scene Images</article-title><source>Remote Sens.</source><year>2025</year><volume>17</volume><elocation-id>125</elocation-id><pub-id pub-id-type="doi">10.3390/rs17010125</pub-id></element-citation></ref><ref id="B15-sensors-25-05734"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chehri</surname><given-names>A.</given-names></name></person-group><article-title>Small Object Detection Based on Deep Learning for Remote Sensing: A Comprehensive Review</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>3265</elocation-id><pub-id pub-id-type="doi">10.3390/rs15133265</pub-id></element-citation></ref><ref id="B16-sensors-25-05734"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gui</surname><given-names>S.</given-names></name><name name-style="western"><surname>Song</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name></person-group><article-title>Remote Sensing Object Detection in the Deep Learning Era&#8212;A Review</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>327</elocation-id><pub-id pub-id-type="doi">10.3390/rs16020327</pub-id></element-citation></ref><ref id="B17-sensors-25-05734"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B18-sensors-25-05734"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Han</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>G.</given-names></name><name name-style="western"><surname>He</surname><given-names>S.</given-names></name></person-group><article-title>Detection</article-title><source>Computer Vision&#8212;ECCV 2020</source><person-group person-group-type="editor"><name name-style="western"><surname>Vedaldi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bischof</surname><given-names>H.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name><name name-style="western"><surname>Frahm</surname><given-names>J.M.</given-names></name></person-group><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>212</fpage><lpage>228</lpage></element-citation></ref><ref id="B19-sensors-25-05734"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Su</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Deformable {DETR}: Deformable Transformers for End-to-End Object Detection</article-title><source>Proceedings of the International Conference on Learning Representations</source><conf-loc>Vienna, Austria</conf-loc><conf-date>4 May 2021</conf-date></element-citation></ref><ref id="B20-sensors-25-05734"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>S.</given-names></name></person-group><article-title>Drone-DETR: Efficient Small Object Detection for Remote Sensing Image Using Enhanced RT-DETR Model</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5496</elocation-id><pub-id pub-id-type="doi">10.3390/s24175496</pub-id><pub-id pub-id-type="pmid">39275406</pub-id><pub-id pub-id-type="pmcid">PMC11397902</pub-id></element-citation></ref><ref id="B21-sensors-25-05734"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>RS-DETR: An Improved Remote Sensing Object Detection Model Based on RT-DETR</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>10331</elocation-id><pub-id pub-id-type="doi">10.3390/app142210331</pub-id></element-citation></ref><ref id="B22-sensors-25-05734"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.Y.M.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>P.Y.</given-names></name><name name-style="western"><surname>Hsieh</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>I.H.</given-names></name></person-group><article-title>CSPNet: A new backbone that can enhance learning capability of CNN</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>390</fpage><lpage>391</lpage></element-citation></ref><ref id="B23-sensors-25-05734"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>H.</given-names></name></person-group><article-title>Frequency-Aware Transformer for Learned Image Compression</article-title><source>Proceedings of the The Twelfth International Conference on Learning Representations</source><conf-loc>Vienna, Austria</conf-loc><conf-date>7&#8211;11 May 2024</conf-date></element-citation></ref><ref id="B24-sensors-25-05734"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xuan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>L.</given-names></name></person-group><article-title>Frequency-Spatial Entanglement Learning for Camouflaged Object Detection</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2409.01686</pub-id><pub-id pub-id-type="arxiv">2409.01686</pub-id></element-citation></ref><ref id="B25-sensors-25-05734"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>N.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>RepVGG: Making VGG-style ConvNets Great Again</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2101.03697</pub-id></element-citation></ref><ref id="B26-sensors-25-05734"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Goyal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name></person-group><article-title>Focal Loss for Dense Object Detection</article-title><source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2999</fpage><lpage>3007</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.324</pub-id></element-citation></ref><ref id="B27-sensors-25-05734"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rezatofighi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tsoi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Gwak</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sadeghian</surname><given-names>A.</given-names></name><name name-style="western"><surname>Reid</surname><given-names>I.</given-names></name><name name-style="western"><surname>Savarese</surname><given-names>S.</given-names></name></person-group><article-title>Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>658</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.00075</pub-id></element-citation></ref><ref id="B28-sensors-25-05734"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Weng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name></person-group><article-title>A High Resolution Optical Satellite Image Dataset for Ship Recognition and Some New Baselines</article-title><source>Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods (ICPRAM)</source><conf-loc>Porto, Portugal</conf-loc><conf-date>24&#8211;26 February 2017</conf-date><fpage>324</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.5220/0006120603240331</pub-id></element-citation></ref><ref id="B29-sensors-25-05734"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>P.</given-names></name><name name-style="western"><surname>He</surname><given-names>R.</given-names></name></person-group><article-title>ShipRSImageNet: A Large-Scale Fine-Grained Dataset for Ship Detection in High-Resolution Optical Remote Sensing Images</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2021</year><volume>14</volume><fpage>8458</fpage><lpage>8472</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2021.3104230</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05734-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overall architecture of MSRS-DETR.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05734-g001.jpg"/></fig><fig position="float" id="sensors-25-05734-f002" orientation="portrait"><label>Figure 2</label><caption><p>Structures of the C2f block and the proposed C2f<sub>FAT</sub> block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05734-g002.jpg"/></fig><fig position="float" id="sensors-25-05734-f003" orientation="portrait"><label>Figure 3</label><caption><p>Architecture of the ETB module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05734-g003.jpg"/></fig><fig position="float" id="sensors-25-05734-f004" orientation="portrait"><label>Figure 4</label><caption><p>Internal design of the RepC<sub>N</sub> (<italic toggle="yes">N</italic> = 3) block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05734-g004.jpg"/></fig><fig position="float" id="sensors-25-05734-f005" orientation="portrait"><label>Figure 5</label><caption><p>Label distributions of HRSC2016 and ShipRSImageNet. (<bold>Left</bold>): Normalised target centre coordinates <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>Right</bold>): Normalised target sizes (width, height).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05734-g005.jpg"/></fig><fig position="float" id="sensors-25-05734-f006" orientation="portrait"><label>Figure 6</label><caption><p>Qualitative comparison on HRSC2016. Each triptych shows YOLOv8m (<bold>left</bold>), RT-DETR (<bold>middle</bold>), and MSRS-DETR (<bold>right</bold>). MSRS-DETR reduces false positives and improves recall under clutter and strong scale variation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05734-g006.jpg"/></fig><fig position="float" id="sensors-25-05734-f007" orientation="portrait"><label>Figure 7</label><caption><p>Qualitative comparison on ShipRSImageNet. Each triptych shows YOLOv8m (<bold>left</bold>), RT-DETR (<bold>middle</bold>), and MSRS-DETR (<bold>right</bold>). Our method better localises dense small vessels and suppresses background-induced false alarms.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05734-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05734-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05734-t001_Table 1</object-id><label>Table 1</label><caption><p>Definitions of evaluation metrics. TP, FP, and FN denote true positives, false positives, and false negatives, respectively. <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>cls</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of classes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Category</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Definition</th></tr></thead><tbody><tr><td align="center" rowspan="7" valign="middle" style="border-bottom:solid thin" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">R</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AP</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mi>AP</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mi>mAP</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>cls</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>cls</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>AP</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>50</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">mAP at IoU&#160;=&#160;0.50</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">mAP<sub>75</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">mAP at IoU&#160;=&#160;0.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP<sub>50-95</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average of mAP at IoU&#160;=&#160;0.50:0.05:0.95</td></tr><tr><td align="center" rowspan="2" valign="middle" style="border-bottom:solid thin" colspan="1">Efficiency</td><td align="center" valign="middle" rowspan="1" colspan="1">Params (M)</td><td align="center" valign="middle" rowspan="1" colspan="1">Trainable parameters in millions</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GFLOPs</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Giga floating-point operations</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05734-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05734-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance comparison on HRSC2016 (H)/ShipRSImageNet (S). All metrics in each cell are reported as H/S. <bold>Bold</bold> indicates the best per column; <underline>underline</underline> indicates the second best.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GFLOPs</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP<sub>50</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP<sub>75</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP<sub>50-95</sub></th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>11.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>28.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">93.33/80.05</td><td align="center" valign="middle" rowspan="1" colspan="1">91.04/72.23</td><td align="center" valign="middle" rowspan="1" colspan="1">83.62/62.88</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RTDETR-HGBlock</td><td align="center" valign="middle" rowspan="1" colspan="1">32.0</td><td align="center" valign="middle" rowspan="1" colspan="1">103.4</td><td align="center" valign="middle" rowspan="1" colspan="1">92.99/84.12</td><td align="center" valign="middle" rowspan="1" colspan="1">90.79/75.60</td><td align="center" valign="middle" rowspan="1" colspan="1">84.47/66.07</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RTDETR-R18</td><td align="center" valign="middle" rowspan="1" colspan="1">19.9</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>56.9</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.93/84.09</td><td align="center" valign="middle" rowspan="1" colspan="1">90.58/74.84</td><td align="center" valign="middle" rowspan="1" colspan="1">84.52/65.64</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8m</td><td align="center" valign="middle" rowspan="1" colspan="1">25.8</td><td align="center" valign="middle" rowspan="1" colspan="1">78.7</td><td align="center" valign="middle" rowspan="1" colspan="1">93.62/<underline>84.36</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">91.95/<underline>77.27</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">86.03/<underline>67.52</underline></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RTDETR-R101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">247.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>94.33</bold>/83.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>92.21</bold>/75.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>86.76</bold>/66.42</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSRS-DETR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>14.1</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><underline>94.06</underline>/<bold>85.09</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><underline>92.08</underline>/<bold>78.14</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><underline>86.21</underline>/<bold>67.80</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05734-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05734-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation study of the proposed modules. All metrics are presented in the format of HRSC2016 (H)/ShipRSImageNet (S). The checkmark (&#10003;) indicates the module is included. The final model is highlighted in <bold>bold</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Baseline</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C2f<sub>FAT</sub>NET</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ETB</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">S2-CCFF</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GFLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP<sub>50</sub></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP<sub>75</sub></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP<sub>50-95</sub></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">19.9</td><td align="center" valign="middle" rowspan="1" colspan="1">56.9</td><td align="center" valign="middle" rowspan="1" colspan="1">92.93/84.09</td><td align="center" valign="middle" rowspan="1" colspan="1">90.58/74.84</td><td align="center" valign="middle" rowspan="1" colspan="1">84.52/65.64</td><td align="center" valign="middle" rowspan="1" colspan="1">169/179</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">15.2</td><td align="center" valign="middle" rowspan="1" colspan="1">51.3</td><td align="center" valign="middle" rowspan="1" colspan="1">93.86/84.28</td><td align="center" valign="middle" rowspan="1" colspan="1">92.00/75.50</td><td align="center" valign="middle" rowspan="1" colspan="1">85.50/65.98</td><td align="center" valign="middle" rowspan="1" colspan="1">157/154</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">20.0</td><td align="center" valign="middle" rowspan="1" colspan="1">57.3</td><td align="center" valign="middle" rowspan="1" colspan="1">94.05/83.59</td><td align="center" valign="middle" rowspan="1" colspan="1">91.48/75.16</td><td align="center" valign="middle" rowspan="1" colspan="1">85.19/65.98</td><td align="center" valign="middle" rowspan="1" colspan="1">257/251</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">18.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.1</td><td align="center" valign="middle" rowspan="1" colspan="1">93.03/85.35</td><td align="center" valign="middle" rowspan="1" colspan="1">90.45/78.20</td><td align="center" valign="middle" rowspan="1" colspan="1">84.85/67.73</td><td align="center" valign="middle" rowspan="1" colspan="1">161/151</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">15.4</td><td align="center" valign="middle" rowspan="1" colspan="1">51.6</td><td align="center" valign="middle" rowspan="1" colspan="1">94.45/84.25</td><td align="center" valign="middle" rowspan="1" colspan="1">92.18/75.31</td><td align="center" valign="middle" rowspan="1" colspan="1">86.20/65.25</td><td align="center" valign="middle" rowspan="1" colspan="1">195/232</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">14.0</td><td align="center" valign="middle" rowspan="1" colspan="1">73.3</td><td align="center" valign="middle" rowspan="1" colspan="1">92.81/84.91</td><td align="center" valign="middle" rowspan="1" colspan="1">90.57/76.97</td><td align="center" valign="middle" rowspan="1" colspan="1">84.76/66.77</td><td align="center" valign="middle" rowspan="1" colspan="1">162/166</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.89/84.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.03/77.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.33/67.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">203/213</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>14.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>73.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>94.06</bold>/<bold>85.09</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>92.08</bold>/<bold>78.14</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>86.21</bold>/<bold>67.80</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>190</bold>/<bold>194</bold></td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>