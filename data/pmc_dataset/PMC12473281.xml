<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473281</article-id><article-id pub-id-type="pmcid-ver">PMC12473281.1</article-id><article-id pub-id-type="pmcaid">12473281</article-id><article-id pub-id-type="pmcaiid">12473281</article-id><article-id pub-id-type="pmid">41012939</article-id><article-id pub-id-type="doi">10.3390/s25185702</article-id><article-id pub-id-type="publisher-id">sensors-25-05702</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Monitoring Visual Fatigue with Eye Tracking in a Pharmaceutical Packing Area</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1173-7329</contrib-id><name name-style="western"><surname>Albarr&#225;n Morillo</surname><given-names initials="C">Carlos</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05702" ref-type="aff">1</xref><xref rid="c1-sensors-25-05702" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0896-8134</contrib-id><name name-style="western"><surname>Su&#225;rez-P&#233;rez</surname><given-names initials="JF">John F.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-05702" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5247-7634</contrib-id><name name-style="western"><surname>Demichela</surname><given-names initials="M">Micaela</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05702" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9116-7161</contrib-id><name name-style="western"><surname>Camargo Salinas</surname><given-names initials="MA">M&#243;nica Andrea</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af3-sensors-25-05702" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-3669-5454</contrib-id><name name-style="western"><surname>Miranda Arandia</surname><given-names initials="NY">Nasli Yuceti</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af3-sensors-25-05702" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Soleimani</surname><given-names initials="M">Manuchehr</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Bernal</surname><given-names initials="MV">Maria Valero</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Halog</surname><given-names initials="AB">Anthony Basco</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05702"><label>1</label>Applied Science and Technology Department, Politecnico di Torino, 10129 Turin, Italy; <email>micaela.demichela@polito.it</email></aff><aff id="af2-sensors-25-05702"><label>2</label>Escuela de Ingenieria y Ciencias, Tecnologico de Monterrey, Ave. General Ramon Corona 2514, Zapopan 45138, Mexico; <email>jf.suarez@tec.mx</email></aff><aff id="af3-sensors-25-05702"><label>3</label>Departamento de Ingenier&#237;a Industrial, Universidad de Am&#233;rica, Bogot&#225; 111321, Colombia; <email>monica.camargo@uamerica.edu.co</email> (M.A.C.S.); <email>nasli.miranda@uamerica.edu.co</email> (N.Y.M.A.)</aff><author-notes><corresp id="c1-sensors-25-05702"><label>*</label>Correspondence: <email>carlos.albarran@polito.it</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5702</elocation-id><history><date date-type="received"><day>01</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>01</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>12</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05702.pdf"/><abstract><sec sec-type="highlights"><title>Highlights</title><p>
<bold>What are the main findings?</bold>
<list list-type="bullet"><list-item><p>This study introduces a robust methodology for assessing visual fatigue in real industrial environments, specifically in a pharmaceutical packaging area by integrating subjective reports, biometric eye-tracking data, and environmental lighting measurements.</p></list-item><list-item><p>Results reveal a consistent increase in visual fatigue across the workweek and shifts, with critical fatigue peaks during night shifts and tasks involving high physical and visual demand under suboptimal lighting.</p></list-item></list>
</p><p>
<bold>What is the implication of the main finding?</bold>
<list list-type="bullet"><list-item><p>Combining physiological, perceptual, and environmental data addresses a major gap in real-time visual fatigue monitoring on the shop floor, moving beyond control room and laboratory-based assessments.</p></list-item><list-item><p>The approach contributes to Industry 5.0 goals by enabling adaptive, human-centered interventions that enhance operator well-being, task accuracy, and sustainable ergonomic design.</p></list-item></list>
</p></sec><sec><title>Abstract</title><p>This study investigates visual fatigue in a real-world pharmaceutical packaging environment, where operators perform repetitive inspection and packing tasks under frequently suboptimal lighting conditions. A human-centered methodology was adopted, combining adapted self-report questionnaires, high-frequency eye-tracking data collected with Tobii Pro Glasses 3, and lux-level measurements. Key eye-movement metrics&#8212;including fixation duration, visit patterns, and pupil diameter&#8212;were analyzed within defined work zones (Areas of Interest). To reduce data complexity and uncover latent patterns of visual behavior, Principal Component Analysis was applied. Results revealed a progressive increase in visual fatigue across the workweek and throughout shifts, particularly during night work, and showed a strong association with inadequate lighting. Tasks involving high physical workload under poor illumination emerged as critical risk scenarios. This integrated approach not only confirmed the presence of visual fatigue but also identified high-risk conditions in the workflow, enabling targeted ergonomic interventions. The findings provide a practical framework for improving operator well-being and inspection performance through sensor-based monitoring and environment-specific design enhancements, in alignment with the goals of Industry 5.0.</p></sec></abstract><kwd-group><kwd>visual fatigue</kwd><kwd>eye tracking</kwd><kwd>wearable sensors</kwd><kwd>human-centered design</kwd><kwd>pharmaceutical packaging</kwd><kwd>Industry 5.0</kwd><kwd>occupational health</kwd></kwd-group><funding-group><award-group><funding-source>the European Union&#8217;s Horizon 2020 Research and Innovation Program</funding-source><funding-source>Innovation Program under the Marie Sk&#322;odowska-Curie</funding-source><award-id>955901</award-id></award-group><funding-statement>This work has been performed within the Collaborative Intelligence for Safety-Critical Systems project (CISC). The CISC project has received funding from the European Union&#8217;s Horizon 2020 Research and Innovation Program under the Marie Sk&#322;odowska-Curie grant agreement no. 955901.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05702"><title>1. Introduction</title><p>Visual fatigue, also referred to as eye strain or visual discomfort, is a physical and cognitive condition that arises from prolonged visual activity, particularly under demanding or non-ergonomic conditions [<xref rid="B1-sensors-25-05702" ref-type="bibr">1</xref>]. It commonly occurs when individuals focus on the same target for extended periods, engage in frequent eye movements, or process large volumes of visual information [<xref rid="B2-sensors-25-05702" ref-type="bibr">2</xref>]. Symptoms include ocular discomfort, dryness, blurred vision, headaches, and diminished concentration, all of which can negatively affect cognitive functioning and overall task performance [<xref rid="B3-sensors-25-05702" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05702" ref-type="bibr">4</xref>]. These impacts are increasingly relevant as industries transition toward more intelligent and human-centric paradigms, such as those promoted by Industry 5.0, which emphasizes worker well-being and adaptive technology [<xref rid="B5-sensors-25-05702" ref-type="bibr">5</xref>].</p><p>This type of fatigue presents a significant challenge in occupations that require sustained visual precision and attention to detail. It slows performance, increases the likelihood of missing critical signals, and elevates the risk of errors. Such challenges are particularly problematic in sectors such as healthcare, manufacturing, packaging, and transportation. For example, a quality control operator scanning hundreds of products or a technician inspecting small defects under bright lighting may experience reduced accuracy when visual fatigue sets in [<xref rid="B6-sensors-25-05702" ref-type="bibr">6</xref>]. Despite its consequences, visual fatigue is often underestimated or accepted as inevitable. However, its cumulative effects can lead to chronic stress, long-term eye discomfort, and early manifestations of computer vision syndrome (CVS) [<xref rid="B7-sensors-25-05702" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05702" ref-type="bibr">8</xref>]. This not only compromises worker comfort but also reduces performance and jeopardizes health&#8212;an issue that industries can no longer afford to ignore.</p><p>The urgency of this problem becomes clearer when examined in the context of regional and global data. According to the Colombian Safety Council (2024) [<xref rid="B9-sensors-25-05702" ref-type="bibr">9</xref>], the manufacturing sector accounted for the highest incidence of occupational diseases in 2024 (21.1% of cases), followed by administrative and support service activities (12.4%). Musculoskeletal disorders remain the most frequently reported category [<xref rid="B10-sensors-25-05702" ref-type="bibr">10</xref>]. Although visual health problems are increasingly recognized, the Colombian legal framework (Decree 676 of 2020) does not explicitly classify CVS or visual fatigue as occupational diseases, instead emphasizing more traditional ocular conditions such as conjunctivitis, keratitis, and cataracts. Nevertheless, the legislation allows these emerging conditions to be recognized if a clear cause&#8211;effect relationship is demonstrated. International evidence also points to substantial productivity losses: one study in Saudi Arabia [<xref rid="B11-sensors-25-05702" ref-type="bibr">11</xref>] found that 7.6% of workers reported absences due to fatigue-related dry eye, while 29.4% indicated that it reduced their effective working hours. Globally, the World Health Organization (WHO) and the International Labour Organization (ILO) estimate that uncorrected vision problems result in an annual productivity loss of approximately USD 411 billion [<xref rid="B12-sensors-25-05702" ref-type="bibr">12</xref>]. Within Colombia, a study of pharmaceutical workers in Bogot&#225; reported a 51.4% prevalence of CVS [<xref rid="B13-sensors-25-05702" ref-type="bibr">13</xref>], underscoring the scale of the problem in industrial contexts. High prevalence rates have also been documented among academic populations: a 2022 study of medical students in Tunja reported an 84.4% prevalence of CVS [<xref rid="B14-sensors-25-05702" ref-type="bibr">14</xref>], while research with optometry students at La Salle University in Bogot&#225; found headache (89.9%), red eyes, and dry eyes as the most frequent symptoms [<xref rid="B15-sensors-25-05702" ref-type="bibr">15</xref>]. Despite these findings, the true impact of visual fatigue often remains obscured in absenteeism records, where it is misclassified as musculoskeletal disorders or general illness.</p><p>This context highlights a critical gap in both research and workplace practice, which the present study seeks to address. Despite increasing awareness, visual fatigue continues to be underreported and insufficiently measured, particularly in operational environments. A comprehensive understanding requires robust, multidimensional assessment strategies. However, this is not straightforward: fatigue manifests differently among individuals and does not always produce obvious or immediate signs [<xref rid="B4-sensors-25-05702" ref-type="bibr">4</xref>]. Some workers may perceive eye heaviness, blurred vision, or difficulty maintaining concentration, while others may remain unaware, continuing their tasks as performance silently deteriorates. Often, by the time fatigue is consciously recognized, significant cognitive and perceptual decline has already occurred. For this reason, researchers must adopt integrative approaches that combine subjective perceptions with physiological data using smart technologies, in line with Industry 5.0&#8217;s emphasis on responsive, worker-centered systems.</p><p>Subjective assessments provide valuable insights into the personal experience of visual fatigue. Tools such as the Visual Fatigue Scale (VFS) [<xref rid="B16-sensors-25-05702" ref-type="bibr">16</xref>] or the Simulator Sickness Questionnaire (SSQ) [<xref rid="B17-sensors-25-05702" ref-type="bibr">17</xref>] ask straightforward questions&#8212;for example: Are your eyes sore or tired? Are you struggling to focus? Do you feel mentally drained or slightly dizzy? These questionnaires are quick and easy to administer, offering useful information on how individuals cope with task demands. However, responses can be inconsistent: some participants underreport their symptoms, while others overreport, and subtle signs of fatigue often remain undetected.</p><p>Objective tools complement these limitations by capturing underlying physiological changes. Eye trackers and related devices provide rich data on visual behavior, including fixation duration, saccadic movements, blinking frequency, and pupil responses to light and stress [<xref rid="B18-sensors-25-05702" ref-type="bibr">18</xref>]. These subtle indicators often reveal the early manifestations of visual fatigue before the individual becomes consciously aware of them [<xref rid="B19-sensors-25-05702" ref-type="bibr">19</xref>]. More advanced approaches incorporate electroencephalography (EEG) to directly monitor brain activity [<xref rid="B20-sensors-25-05702" ref-type="bibr">20</xref>]. By tracking changes in brainwave patterns, EEG can detect increases in mental workload or lapses in attention. For example, higher proportions of slower theta waves or reductions in faster alpha waves are indicative of mental fatigue and reduced attentional capacity [<xref rid="B21-sensors-25-05702" ref-type="bibr">21</xref>]. Although EEG is not yet practical for widespread workplace use, it remains a valuable laboratory tool, particularly when combined with eye tracking or heart rate monitoring [<xref rid="B22-sensors-25-05702" ref-type="bibr">22</xref>]. Integrating these multimodal insights advances the understanding of visual fatigue and enables the development of early-intervention strategies.</p><p>In addition to these physiological measures, conventional approaches such as critical flicker fusion frequency (CFF) and accommodative function tests have been widely used as indicators of visual fatigue. While these methods are effective in controlled laboratory or clinical settings, they present practical limitations for industrial environments: they require specialized equipment, interrupt workflow, and are not suitable for continuous or non-intrusive monitoring. By contrast, eye-tracking measures can be integrated seamlessly into real work settings, allowing fatigue to be assessed dynamically without disrupting production tasks. This distinction underscores the value of the present study, which emphasizes real-time, context-sensitive monitoring aligned with the human-centric principles of Industry 5.0.</p><p>Until now, much of the knowledge on visual fatigue has been derived from domains where sustained attention is critical. Research has expanded in areas such as gaming, professional driving&#8212;including truck drivers, train operators, and pilots&#8212;and immersive technologies such as virtual and mixed reality (VR/MR). In these contexts, performance depends heavily on the ability to maintain prolonged visual engagement without loss of accuracy or speed [<xref rid="B23-sensors-25-05702" ref-type="bibr">23</xref>]. Other lines of research have focused on 3D technologies, particularly in training and simulation, where depth perception and screen-based interaction introduce new visual demands [<xref rid="B24-sensors-25-05702" ref-type="bibr">24</xref>]. Office work has also been a major concern, with prolonged exposure to screens being closely linked to digital eye strain, now recognized as a major occupational health issue [<xref rid="B25-sensors-25-05702" ref-type="bibr">25</xref>]. Long-term use of visual display terminals (VDTs) has been associated with discomfort, reduced productivity, and risks to long-term eye health [<xref rid="B26-sensors-25-05702" ref-type="bibr">26</xref>]. Studies in healthcare environments highlight similar issues for diagnostic and monitoring tasks that rely on digital interfaces, while cultural and museum settings are beginning to examine how interactive, screen-based exhibits influence visitor fatigue and engagement [<xref rid="B27-sensors-25-05702" ref-type="bibr">27</xref>].</p><p>Despite these advances, studies specifically targeting industrial environments remain limited. Our review identified 21 relevant articles [<xref rid="B28-sensors-25-05702" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05702" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05702" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05702" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05702" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05702" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05702" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05702" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05702" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05702" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05702" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05702" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05702" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05702" ref-type="bibr">41</xref>,<xref rid="B42-sensors-25-05702" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-05702" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05702" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05702" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05702" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05702" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-05702" ref-type="bibr">48</xref>]. Most of these focus on control rooms, monitoring stations, or screen-intensive roles, where operators engage in prolonged observation of digital displays. These include process industry control rooms, refineries, and display-dominated workstations, with several studies addressing the influence of lighting and shift schedules on visual strain [<xref rid="B33-sensors-25-05702" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05702" ref-type="bibr">34</xref>,<xref rid="B36-sensors-25-05702" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05702" ref-type="bibr">37</xref>,<xref rid="B41-sensors-25-05702" ref-type="bibr">41</xref>]. Other contributions have explored lighting ergonomics, visual comfort, or physiological responses such as EEG and pupil dynamics, though often within laboratory settings disconnected from actual industrial workflows [<xref rid="B32-sensors-25-05702" ref-type="bibr">32</xref>,<xref rid="B38-sensors-25-05702" ref-type="bibr">38</xref>,<xref rid="B40-sensors-25-05702" ref-type="bibr">40</xref>,<xref rid="B42-sensors-25-05702" ref-type="bibr">42</xref>,<xref rid="B44-sensors-25-05702" ref-type="bibr">44</xref>,<xref rid="B48-sensors-25-05702" ref-type="bibr">48</xref>]. A small number of exploratory studies have tested augmented reality systems, smart wearables, or fatigue detection algorithms in industrial applications, but these remain experimental and not yet fully embedded in operational practice [<xref rid="B29-sensors-25-05702" ref-type="bibr">29</xref>,<xref rid="B31-sensors-25-05702" ref-type="bibr">31</xref>,<xref rid="B42-sensors-25-05702" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-05702" ref-type="bibr">43</xref>].</p><p>Only a few studies have directly examined hands-on, visually demanding activities on the shop floor, beyond control room settings. These tasks are typically characterized by suboptimal lighting, strict time pressures, and high repetition [<xref rid="B28-sensors-25-05702" ref-type="bibr">28</xref>,<xref rid="B30-sensors-25-05702" ref-type="bibr">30</xref>,<xref rid="B35-sensors-25-05702" ref-type="bibr">35</xref>,<xref rid="B39-sensors-25-05702" ref-type="bibr">39</xref>,<xref rid="B45-sensors-25-05702" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05702" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05702" ref-type="bibr">47</xref>]. Among them, only a handful have integrated smart technologies&#8212;such as eye tracking, fatigue prediction algorithms, or sensor-driven lighting systems&#8212;to objectively monitor visual fatigue in real time [<xref rid="B35-sensors-25-05702" ref-type="bibr">35</xref>,<xref rid="B45-sensors-25-05702" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05702" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05702" ref-type="bibr">47</xref>].</p><p>This highlights a critical research gap: very few studies have investigated visual fatigue in operational industrial environments using real-time, objective, and accurate measurements. Smart technologies are particularly valuable because they enable continuous, non-intrusive data collection and provide early warning of fatigue, often before observable declines in performance occur. These tools are essential for advancing Industry 5.0, which emphasizes the augmentation of human capabilities through adaptive technologies that support the worker, rather than requiring the worker to adapt to the system [<xref rid="B49-sensors-25-05702" ref-type="bibr">49</xref>]. They are central to human-centered design, early intervention, and proactive strategies that enhance both operator well-being and process reliability [<xref rid="B50-sensors-25-05702" ref-type="bibr">50</xref>].</p><p>In response to this gap, the present study investigates visual fatigue in a pharmaceutical packaging line, applying a human-centered, sensor-based methodology. By combining environmental assessments, operator feedback, and objective eye-tracking data collected with Tobii Pro Glasses 3, the study captures how visual fatigue develops during real work shifts. This approach enables the identification of high-risk tasks, a deeper understanding of fatigue dynamics in context, and the formulation of evidence-based ergonomic interventions to improve operator well-being and inspection performance.</p><p>The following sections describe the methodology adopted in this study, outlining the experimental design and procedures used to assess visual fatigue in a real industrial environment. This is followed by the presentation and discussion of results, focusing on the identification of critical risk points in the workflow. The paper concludes with a summary of key findings and their implications for human-centered industrial design and fatigue prevention.</p></sec><sec id="sec2-sensors-25-05702"><title>2. Materials and Methods</title><p>This section outlines the methodology adopted to investigate visual fatigue in a real-world pharmaceutical packaging environment. Rather than relying on controlled simulations or laboratory experiments, the study was designed to capture how visual fatigue develops naturally during regular work shifts. The following subsections describe the experimental setup, data collection procedures, and analytical methods used to identify fatigue patterns and highlight critical points in the packaging process where ergonomic interventions could have the greatest impact.</p><sec id="sec2dot1-sensors-25-05702"><title>2.1. Experimental Setup</title><p>The study was carried out in the packaging area of a pharmaceutical company, where workers routinely perform tasks requiring sustained visual attention, such as checking, packaging, and sealing pharmaceutical products. These tasks involve grouping items into units of one, ten, or one hundred or on large trays and demand continuous visual focus, rapid hand movements, and precise motor control. This environment provided an ideal context for investigating visual fatigue under realistic working conditions. To illustrate the setup, <xref rid="sensors-25-05702-f001" ref-type="fig">Figure 1</xref> presents a representative heatmap generated from one participant&#8217;s eye-tracking data. The visualization shows how visual attention was concentrated on the trays of pharmaceutical products prior to packaging (areas in read), under real shop-floor conditions.</p><p>To capture both objective and subjective indicators of visual fatigue, we employed the Tobii Pro Glasses 3 eye-tracking system in combination with a modified Visual Fatigue Scale questionnaire. The Tobii Pro Glasses 3 (Tobii AB, Danderyd, Sweden) is a lightweight, wearable device capable of sampling at up to 100 Hz, recording detailed eye-movement data&#8212;including gaze behavior, fixations, saccades, and pupil diameter&#8212;within natural industrial environments. Specific Areas of Interest (AoIs) were defined within the workspace, focusing on stations dedicated to the inspection and packaging of vials and packs. The eye-tracking data collected within these AoIs included the following parameters:<list list-type="bullet"><list-item><p>Total Fixation Duration: Cumulative time fixating on the AoI.</p></list-item><list-item><p>Fixation Count: Total number of fixations.</p></list-item><list-item><p>Total Visit Duration: Total time spent in the AoI per visit.</p></list-item><list-item><p>Average Visit Duration: Mean time per visit within the AoI.</p></list-item><list-item><p>Visit Count: Number of gaze entries into the AoI.</p></list-item><list-item><p>Percentage of Gazes as Fixations: Share of gazes resulting in fixations.</p></list-item><list-item><p>Percentage of Total Activity Time in the AoI.</p></list-item><list-item><p>Pupil Diameter: Monitored during fixations to infer cognitive load and fatigue.</p></list-item></list></p><p>Subjective visual fatigue was measured using an adapted Visual Fatigue Scale [<xref rid="B16-sensors-25-05702" ref-type="bibr">16</xref>], comprising three Likert-scale items:<list list-type="bullet"><list-item><p>Eye tiredness</p></list-item><list-item><p>Vision clarity</p></list-item><list-item><p>Eye discomfort</p></list-item></list></p><p>Each item was rated on a scale from 0 (very fresh/clear) to 4 (very tired/blurred), yielding a total score ranging from 0 to 12.</p><p>Because the success of visual inspection largely depends on the operator&#8217;s eyesight, the illumination system is critical for ensuring optimal performance. To contextualize the eye-tracking data, environmental lighting levels (measured in lux) were recorded using a luxmeter installed near the AoIs. Measurements were taken once at the beginning of each trial, following prior verification that lighting remained stable throughout the session.</p><p>The instruments used to assess visual fatigue included the eye tracker (Tobii Pro Glasses 3), the luxmeter (for illumination levels), and the adapted Visual Fatigue Scale. Together, these tools provided a comprehensive perspective on visual fatigue by integrating subjective perceptions with objective physiological indicators under different operational conditions. Although additional instruments were employed in the broader project to evaluate other human factors&#8212;such as stress, general fatigue, and biomechanical risks&#8212;these aspects fall outside the scope of this paper.</p></sec><sec id="sec2dot2-sensors-25-05702"><title>2.2. Data Collection</title><p>Before the study commenced, all 43 participants (42 females and 1 male) provided informed consent. The research was approved by the Ethics Committee of Universidad de Am&#233;rica (Protocol No. 002-2024), in accordance with the ethical principles of the Declaration of Helsinki. Participation was entirely voluntary, and no financial compensation was offered. With 43 participants out of approximately 80 total operators, the sample represented more than half of the workforce assigned to these packaging tasks. The group was evenly distributed across the four production lines, ensuring balanced representation of different operational settings. The gender distribution also reflected the real workplace context, where only three to four of the ~80 operators are male. Participant ages ranged from 19 to 53 years (mean = 32.2), making this a contextually appropriate and representative sample.</p><p>To capture the progression of visual fatigue during a typical shift, each operator was assessed at two key points: once during the first two hours of the shift (before the scheduled break) and once during the final two hours (after the break). These intervals were chosen to respect operational constraints, minimizing disruption to production while allowing meaningful observation of fatigue dynamics. Data were collected continuously throughout the workweek (Monday to Friday) across all three standard 8-h shifts: morning (06:30&#8211;14:30), afternoon (14:30&#8211;22:30), and night (22:30&#8211;06:30). The study was carried out in four production scenarios: Plant 4 Line 2, Plant 4 Line 3, Plant 4 Line 4, and Plant 8. Workers performed their usual inspection, packaging, and sealing activities under real shop-floor conditions, involving individual units, batches of 10 or 100, and large trays. This repetitive and visually demanding workload, combined with differences in lighting and layout among the lines, provided an important context for evaluating environmental influences on visual fatigue.</p><p>Each data collection session lasted 20 min, during which participants wore the Tobii Pro Glasses 3 to record detailed eye-movement patterns under natural working conditions. At the beginning of each trial, illumination levels were measured using the luxmeter installed near the AoIs. To complement these objective measures, participants also completed the adapted Visual Fatigue Scale. The questionnaire was administered immediately prior to each eye-tracking session, meaning that every operator provided two subjective self-reports: one during the first assessment period (early in the shift) and one during the second assessment period (toward the end of the shift). Each assessment took only a few minutes and was conducted under the same shop-floor conditions, ensuring minimal disruption to workflow. This procedure ensured that the subjective perception of fatigue was captured in real time and paired directly with objective biometric recordings from eye-tracking and environmental measurements. The dual assessment design (early and late shift) enabled the study to track both the accumulation of fatigue across the day and the weekly trend across shifts, while maintaining consistency and comparability across participants.</p></sec><sec id="sec2dot3-sensors-25-05702"><title>2.3. Data Analysis Techniques</title><p>This section describes the analytical techniques applied to examine the collected data, with the aim of understanding how visual fatigue evolves over the course of a work shift. The analysis integrated environmental factors, biometric indicators obtained from eye-tracking, and responses from the subjective Visual Fatigue Scale to identify the most visually demanding moments in the workflow&#8212;those posing the greatest risk to operator comfort, accuracy, and well-being. These insights formed the basis for proposing targeted ergonomic improvements designed to mitigate visual strain and enhance the long-term sustainability of packaging tasks.</p><sec id="sec2dot3dot1-sensors-25-05702"><title>2.3.1. Lighting Levels Analysis</title><p>Before proposing ergonomic interventions, it was essential to evaluate whether the lighting conditions at each workstation complied with internationally recognized standards for visual tasks. Reference was made to the international lighting standard [<xref rid="B51-sensors-25-05702" ref-type="bibr">51</xref>] for indoor work environments, which is also applicable in Colombia. This standard recommends a minimum of 500 lux for precision tasks such as inspection and quality control, while tasks with higher visual demands may require up to 1500 lux to prevent eye strain and ensure sustained performance. Maintaining illumination within this range is critical for minimizing visual fatigue and supporting consistent task execution.</p><p>In this study, lighting levels were measured at the beginning of each trial and compared against the ISO/CIE 8995-1:2025 recommended thresholds [<xref rid="B51-sensors-25-05702" ref-type="bibr">51</xref>]. Deviations from these values were then analyzed in relation to their potential influence on visual fatigue, allowing the identification of environmental risk factors that could compromise operator comfort and visual performance.</p></sec><sec id="sec2dot3dot2-sensors-25-05702"><title>2.3.2. Eye-Tracker Analysis</title><p>Using the Tobii Pro Glasses 3, we collected seven key eye-tracking metrics to assess visual behavior, along with pupil diameter as an additional physiological indicator. The metrics included: Total Fixation Duration (Tot Fix Dur), Fixation Count, Total Visit Duration (Tot Visit Dur), Average Visit Duration (Avr Visit Dur), Visit Count, Percentage of Total Activity Time in the AoI, and Percentage of Gazes as Fixations. Each metric was recorded within defined AoIs to capture fluctuations in visual attention and engagement during actual work tasks.</p><p>Before incorporating pupil diameter into subsequent analyses, its reliability under the study conditions was evaluated. Because pupil size is highly sensitive to external factors&#8212;particularly illumination, but also temperature and humidity [<xref rid="B52-sensors-25-05702" ref-type="bibr">52</xref>]&#8212;it was necessary to determine whether it could function as a valid indicator of fatigue in this real-world setting. To this end, correlations were examined between pupil diameter and environmental variables, including light intensity (Lux), temperature (T), and relative humidity (RH). This step controlled for potential confounding influences and established whether pupil data could be meaningfully interpreted as a marker of visual strain.</p><p>To analyze the relationships among the eye-tracking variables and reduce redundancy, Principal Component Analysis (PCA) was applied. PCA was selected not only to reduce dimensionality but also to address multicollinearity and improve the interpretability of fatigue-related patterns [<xref rid="B53-sensors-25-05702" ref-type="bibr">53</xref>]. Preliminary analyses revealed strong intercorrelations among several metrics&#8212;such as fixation duration, visit duration, and gaze frequency&#8212;that could weaken model robustness. By transforming these correlated variables into uncorrelated principal components, PCA clarified the data structure and enhanced analytical stability. For interpretability, Varimax rotation with Kaiser normalization was employed [<xref rid="B54-sensors-25-05702" ref-type="bibr">54</xref>]. Varimax, an orthogonal rotation method, maximizes the variance of squared loadings within each component, simplifying the structure and highlighting the most influential variables [<xref rid="B54-sensors-25-05702" ref-type="bibr">54</xref>]. This approach is particularly well suited for biometric datasets such as eye-tracking, where multiple indicators often reflect overlapping underlying behaviors [<xref rid="B55-sensors-25-05702" ref-type="bibr">55</xref>]. Varimax rotation was chosen over oblique methods (e.g., Promax) because the aim was to extract distinct, non-overlapping dimensions of visual behavior&#8212;such as visual engagement or search effort&#8212;rather than correlated latent factors [<xref rid="B56-sensors-25-05702" ref-type="bibr">56</xref>]. Kaiser normalization was included to prevent variables with larger variances from dominating the solution and to stabilize the variance structure during rotation [<xref rid="B54-sensors-25-05702" ref-type="bibr">54</xref>]. Only components with eigenvalues greater than 1 were retained (Kaiser criterion), ensuring that each explained a meaningful share of the total variance. This approach produced a reduced set of orthogonal components representing the core dimensions of visual behavior in the packaging workflow, enabling the identification of visually demanding tasks and fatigue-prone conditions with greater precision.</p></sec><sec id="sec2dot3dot3-sensors-25-05702"><title>2.3.3. Visual Fatigue Trends and Critical Risk Detection</title><p>In the final phase of analysis, we examined how visual fatigue varied across operational and individual-level factors to identify the workflow conditions that posed the highest risk. Specifically, variations were analyzed in both objective eye-tracking metrics and subjective scale scores according to: moment within the shift (beginning vs. end), shift type (morning, afternoon, or night), day of the week (Monday&#8211;Friday), production line (Plant 4 Line 2, Plant 4 Line 3, Plant 4 Line 4, and Plant 8), batch size (1, 10, 100, or trays), operator age, and years of experience.</p><p>Prior to statistical comparisons, data normality was assessed using the Kolmogorov&#8211;Smirnov (KS) and Shapiro&#8211;Wilk (SW) tests [<xref rid="B57-sensors-25-05702" ref-type="bibr">57</xref>]. For repeated-measures data, sphericity was evaluated using Mauchly&#8217;s Test of Sphericity when normality assumptions were satisfied [<xref rid="B58-sensors-25-05702" ref-type="bibr">58</xref>]. Statistical significance testing was then applied to determine whether differences across groups or time points were meaningful. Correlation analyses were conducted according to data distribution: Pearson&#8217;s correlation coefficient was used for normally distributed variables, while Spearman&#8217;s rank correlation was applied when distributions deviated from normality [<xref rid="B59-sensors-25-05702" ref-type="bibr">59</xref>].</p><p>This multi-dimensional analysis enabled the detection of fatigue progression patterns and the identification of conditions consistently associated with higher fatigue levels. By combining biometric trends with self-reported fatigue, the approach provided a robust framework for recognizing high-risk scenarios. These findings informed the ergonomic recommendations proposed to the industrial partner, ensuring that interventions targeted the areas with the greatest potential to reduce visual fatigue and improve operator comfort and performance.</p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-25-05702"><title>3. Results</title><p>This section presents the main findings of the study, organized according to the three analytical domains described previously: (1) lighting level assessment, (2) eye-tracking metrics and Principal Component Analysis (PCA), and (3) visual fatigue trends and risk detection across operational and individual factors.</p><sec id="sec3dot1-sensors-25-05702"><title>3.1. Lighting Levels</title><p><xref rid="sensors-25-05702-f002" ref-type="fig">Figure 2</xref> shows the lighting levels recorded across production lines throughout the workday. Each data point represents a measurement taken at the beginning of a trial, with colors and markers distinguishing Line 2 (blue squares), Line 3 (orange circles), Line 4 (green Xs), and Line 8 (purple triangles). The horizontal dashed lines denote the ISO/CIE 8995-1:2025 recommended illumination range for precision tasks: a minimum of 500 lux (blue) and a maximum of 1500 lux (red).</p><p>The results indicate that most measurements fell below the 500 lux threshold, with Line 8 consistently showing the lowest values, suggesting inadequate illumination for visually demanding inspection tasks. In contrast, Line 8 also exhibited occasional values exceeding 1500 lux, highlighting inconsistencies in local lighting conditions.</p></sec><sec id="sec3dot2-sensors-25-05702"><title>3.2. Eye-Tracker Results</title><p>We first assessed whether average pupil diameter could be reliably included in the analysis, given its sensitivity to external lighting conditions [<xref rid="B60-sensors-25-05702" ref-type="bibr">60</xref>]. Normality was tested using the Kolmogorov&#8211;Smirnov (KS) and Shapiro&#8211;Wilk (SW) tests. Results yielded <italic toggle="yes">p</italic>-values of 0.070 (KS) and 0.012 (SW), indicating deviation from normality according to the Shapiro&#8211;Wilk test, which is more sensitive with smaller samples. Similarly, light intensity (lux) was non-normally distributed (<italic toggle="yes">p</italic> &lt; 0.001 in both tests). Due to these deviations, Spearman&#8217;s rank correlation was applied to examine the relationship between pupil diameter and illumination. A significant negative correlation was observed (&#961; = &#8722;0.506, <italic toggle="yes">p</italic> &lt; 0.001), showing that higher light levels were consistently associated with smaller pupil sizes. This result is consistent with known physiological mechanisms, whereby pupil constriction regulates retinal exposure in response to increased ambient light [<xref rid="B60-sensors-25-05702" ref-type="bibr">60</xref>]. Accordingly, pupil diameter was excluded from the final fatigue analysis, as its variation primarily reflected lighting conditions rather than operator fatigue, and no further correlations with environmental data were pursued.</p><p>The remaining seven eye-tracking metrics&#8212;Total Fixation Duration (Tot Fix Dur), Fixation Count, Total Visit Duration (Tot Visit Dur), Average Visit Duration (Avr Visit Dur), Visit Count, Percentage of Total Activity Time in the AoI, and Percentage of Gazes as Fixations&#8212;were analyzed using Principal Component Analysis (PCA). This approach improved interpretability by reducing multicollinearity and revealing latent dimensions of visual engagement. Dataset suitability was evaluated using the Kaiser&#8211;Meyer&#8211;Olkin (KMO) Measure of Sampling Adequacy and Bartlett&#8217;s Test of Sphericity. The KMO value was 0.538, slightly above the accepted minimum of 0.5, indicating marginal adequacy. Bartlett&#8217;s test was statistically significant (<italic toggle="yes">p</italic> &lt; 0.001), confirming sufficient correlations among variables for PCA.</p><p>PCA revealed a clear structure in the dataset, identifying three principal components that together explained 93.64% of the total variance. Following the Kaiser criterion, only components with eigenvalues greater than 1 were retained. As shown in the scree plot (<xref rid="sensors-25-05702-f003" ref-type="fig">Figure 3</xref>), the first three components met this criterion, accounting for 52.38%, 27.46%, and 13.79% of the variance, respectively. The scree plot also displayed a distinct &#8220;elbow&#8221; at the third component, reinforcing this selection by illustrating the diminishing returns in variance explained beyond this point.</p><p>Varimax rotation with Kaiser normalization was applied, and only factor loadings above 0.60 are reported to highlight the most significant contributions of variables to each component. The rotated component matrix (<xref rid="sensors-25-05702-t001" ref-type="table">Table 1</xref>) revealed three distinct components associated with eye-tracking parameters, which can be interpreted as follows:
<list list-type="bullet"><list-item><p>Principal Component 1, &#8220;Overall Visual Engagement&#8221;: This component groups Total Visit Duration, Total Fixation Duration, Fixation Count, and Percentage of Total Activity Time in the AoI. Collectively, these metrics reflect the amount of visual attention and time devoted to task-relevant areas. High loadings on this component suggest sustained visual involvement, likely associated with task complexity or attentional demand. Therefore, this component is interpreted as a general indicator of visual engagement during operational activities.</p></list-item><list-item><p>Principal Component 2, &#8220;Fixation Characteristics&#8221;: This factor includes Average Fixation Duration and Percentage of Gazes as Fixations, both of which describe the nature and stability of gaze behavior.</p></list-item><list-item><p>Principal Component 3, &#8220;Visit Duration Pattern&#8221;: Driven solely by Average Visit Duration, this component highlights how long participants remained within each Area of Interest per visit.</p></list-item></list></p><p>Contextual interpretation is essential across all three Principal Components, as each captures a distinct dimension of visual behavior. Collectively, they provide valuable insights into shifts in visual strategy, attentional load, and early indicators of fatigue. Because this study focused specifically on visual fatigue, the interpretation of these components was grounded in prior literature on eye-movement behavior under fatigue conditions [<xref rid="B61-sensors-25-05702" ref-type="bibr">61</xref>,<xref rid="B62-sensors-25-05702" ref-type="bibr">62</xref>,<xref rid="B63-sensors-25-05702" ref-type="bibr">63</xref>,<xref rid="B64-sensors-25-05702" ref-type="bibr">64</xref>]. <xref rid="sensors-25-05702-t002" ref-type="table">Table 2</xref> summarizes the expected changes in key eye-tracking metrics as visual fatigue progresses.</p><p>As shown in <xref rid="sensors-25-05702-t002" ref-type="table">Table 2</xref>, prior literature consistently reports that visual fatigue is characterized by increases in total and average fixation duration and blinking frequency, accompanied by decreases in fixation frequency and saccadic parameters. Principal Component 2 (Fixation Characteristics)&#8212;comprising Average Fixation Duration and Percentage of Gazes as Fixations&#8212;closely aligns with these empirically observed patterns. Both metrics capture elements of gaze stability and attentional persistence, which intensify as visual fatigue progresses. This convergence between the empirical loadings of PC2 and established theoretical expectations reinforces its interpretation as a valid and sensitive indicator of visual fatigue. Higher scores on this component reflect longer and more stable fixations, a behavior often associated with reduced efficiency in attentional modulation under fatigue.</p><p>Because the cognitive demands of the inspection task remain constant throughout the shift, systematic increases in PC2 values can be attributed to the accumulation of visual fatigue rather than variations in task complexity. Accordingly, PC2 was selected as the primary outcome variable in subsequent analyses examining how visual fatigue evolves under different contextual and operational conditions.</p><p>While all three components provide useful insights into eye-movement behavior, Principal Component 1&#8212;associated with overall visual engagement&#8212;appears to capture attentional demand or task structure rather than fatigue-specific effects. Principal Component 3, dominated by Average Visit Duration, may instead reflect individual exploration style or navigation strategy. Since neither PC1 nor PC3 aligns consistently with established fatigue markers in the literature, the analysis focused exclusively on PC2, which directly reflects fixation stability and duration&#8212;two parameters most reliably linked to the onset and progression of visual fatigue.</p></sec><sec id="sec3dot3-sensors-25-05702"><title>3.3. Visual Fatigue Trends</title><p>This section presents key findings on how visual fatigue, based on Principal Component 2 (Fixation Characteristics), varied according to shift timing, weekday, production line, batch type, age, and experience&#8212;highlighting critical risk scenarios for ergonomic intervention. Subjective data from the Visual Fatigue Scale were also incorporated to strengthen the analysis.</p><p>Normality of PC2 was assessed using both the Kolmogorov&#8211;Smirnov (KS) and Shapiro&#8211;Wilk (SW) tests. The results confirmed a normal distribution (KS: <italic toggle="yes">p</italic> = 0.200; SW: <italic toggle="yes">p</italic> = 0.122).</p><p>Following this validation, PC2 was analyzed across contextual and individual factors, including shift timing, production line, batch type, operator age, and experience. Using parametric tests and correlation analyses, no significant patterns emerged in relation to these variables.</p><p>A clear and consistent trend was, however, observed across the workweek. Because weekdays represent ordinal variables, Spearman&#8217;s rank correlation was applied, revealing a strong positive association between PC2 and the progression of the week (&#961; = 0.529, <italic toggle="yes">p</italic> &lt; 0.001). This finding indicates that visual fatigue accumulates over time, likely because of continuous exposure to visually demanding tasks without sufficient recovery.</p><p><xref rid="sensors-25-05702-f004" ref-type="fig">Figure 4</xref> illustrates this trend. At the beginning of the week (Monday), PC2 scores were negative, reflecting shorter and less stable fixations characteristic of a relatively rested visual state. As the week progressed, PC2 values steadily increased and shifted into positive territory, indicating longer and more stable fixations&#8212;consistent with the established physiological and behavioral signatures of accumulated visual fatigue.</p><p>Pairwise comparisons with Bonferroni correction revealed a statistically significant difference between Monday and Friday (<italic toggle="yes">p</italic> = 0.022), further supporting the hypothesis of fatigue accumulation.</p><p>An additional pattern emerged when analyzing the weekly trend of Overall Engagement (PC1) in relation to Fixation Characteristics (PC2), the designated indicator of visual fatigue. As illustrated in <xref rid="sensors-25-05702-f005" ref-type="fig">Figure 5</xref>, the mean plots suggest an inverse relationship between these two components.</p><p>This inverse trend reinforces the conceptual distinction between the components: while PC1 captures sustained attentional involvement, PC2 reflects fatigue-induced changes in gaze stability and fixation behavior. Their contrasting trajectories across the week empirically validate the use of PC2 as an indicator of visual fatigue and demonstrate how fatigue may undermine engagement during prolonged repetitive tasks.</p><p>The Visual Fatigue Scale provided complementary insights into operators&#8217; subjective perceptions of visual strain, further supporting the biometric findings. Using Spearman&#8217;s rank correlation&#8212;appropriate for ordinal Likert-scale responses&#8212;significant associations were observed between perceived visual fatigue and both shift timing (&#961; = 0.214, <italic toggle="yes">p</italic> = 0.048) and the specific point within the shift (&#961; = 0.392, <italic toggle="yes">p</italic> &lt; 0.001). These results indicate that perceived visual fatigue increases during later shifts (particularly the night shift) and accumulates as the workday progresses.</p><p>Another noteworthy finding from the Visual Fatigue Scale relates to the physical load associated with different lot sizes. In this context, Lot 1 refers to packaging single units, Lot 10 and Lot 100 correspond to batches of 10 and 100 units, and Tray 1 and Tray 2 involve handling larger trays with greater physical demands. As illustrated in <xref rid="sensors-25-05702-f006" ref-type="fig">Figure 6</xref>, the highest mean fatigue score was recorded for Tray 2. This batch type corresponded to operations in Production Line 8, where the lowest illumination levels had previously been observed. These results suggest a compounded effect of high physical workload and inadequate lighting on the accumulation of visual fatigue.</p><p>In addition, the Visual Fatigue Scale revealed a clear weekly trend, with perceived fatigue increasing from Monday (7.83) to Wednesday (7.90), followed by a drop on Thursday (6.50), and then rising again on Friday (6.92). This pattern closely mirrors the behavior of PC2 (Fixation Characteristics), reinforcing its validity as an indicator of visual fatigue. The decrease observed on Thursday can be explained by the fact that data collected on that day corresponded primarily to Lot 1 tasks, which are less visually demanding than the other operations (see <xref rid="sensors-25-05702-f006" ref-type="fig">Figure 6</xref>).</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05702"><title>4. Discussion</title><p>This study highlights several critical risk factors contributing to visual fatigue in pharmaceutical inspection and packaging environments. Among the most pressing concerns was the suboptimal lighting observed in Production Line 8, where illumination levels frequently fell below the ISO/CIE 8995-1:2025 thresholds recommended for tasks requiring high visual precision. Inadequate lighting in such settings significantly elevates the risk of visual fatigue, potentially compromising both operator well-being and the accuracy essential for pharmaceutical inspections. Although some measurements temporarily exceeded the upper recommended limit, these instances were appropriately managed and did not directly affect operator visual comfort. As shown in <xref rid="sensors-25-05702-f006" ref-type="fig">Figure 6</xref>, perceived visual fatigue in Tray 1&#8212;corresponding to the highest recorded lux levels&#8212;remained relatively low compared to Tray 2 in Plant 8. This indicates that lighting variations were managed effectively, without compromising comfort.</p><p>To better understand how visual fatigue evolves under different work conditions, wearable eye-tracking technology (Tobii Pro Glasses 3) was combined with PCA, in line with Industry 5.0&#8217;s emphasis on real-time, worker-adaptive monitoring systems. Principal Component 2 (&#8220;Fixation Characteristics&#8221;)&#8212;comprising Average Fixation Duration and the Percentage of Gazes as Fixations&#8212;emerged as a reliable and objective indicator of visual fatigue. This component aligns closely with established physiological signatures of fatigue, such as longer and more stable fixations. Its validity was further reinforced by its empirical behavior: as Fixation Characteristics increased across the workweek, Principal Component 1 (&#8220;Overall Engagement&#8221;) simultaneously declined, reflecting the inverse relationship between attentional engagement and fatigue accumulation.</p><p>Subjective responses from the Visual Fatigue Scale further validated these biometric findings, together revealing four consistent trends:<list list-type="bullet"><list-item><p>Visual fatigue increased steadily across the workweek, as indicated by rising PC2 scores and a concurrent decline in PC1.</p></list-item><list-item><p>Perceived fatigue accumulated progressively within each shift, becoming more pronounced as operators advanced through their tasks&#8212;suggesting insufficient recovery during work hours.</p></list-item><list-item><p>Night shifts were consistently associated with higher levels of perceived visual fatigue.</p></list-item><list-item><p>Tray 2 tasks on Line 8 represented a critical risk scenario, combining high physical load with the lowest recorded lighting levels. Operators reported the highest fatigue scores in this condition, suggesting a compounded effect of physical workload and inadequate illumination.</p></list-item></list></p><p>These insights illustrate how Industry 5.0 tools, such as wearable sensors, can help identify high-risk conditions and inform proactive ergonomic strategies. Based on the findings, three targeted interventions are proposed, aligned with the human-centric design principles of Industry 5.0. First, enhancing local task lighting offers a cost-effective and high-impact solution: installing high-power desk lamps at workstations&#8212;particularly in underlit areas such as Line 8&#8212;can substantially reduce visual strain during detailed inspection tasks. Second, implementing adjustable overhead LED lighting systems can provide consistent, customizable illumination across shifts and task types, improving both visual comfort and long-term energy efficiency [<xref rid="B65-sensors-25-05702" ref-type="bibr">65</xref>]. Finally, introducing structured visual breaks&#8212;particularly during night shifts or the latter half of work periods&#8212;can help mitigate fatigue accumulation. Evidence suggests that even short pauses every 20&#8211;30 min, allowing workers to rest their eyes by focusing on distant objects, reduce digital eye strain and support sustained visual performance [<xref rid="B66-sensors-25-05702" ref-type="bibr">66</xref>].</p><p>It is important to note that these results are specific to the pharmaceutical packaging environment studied, where repetitive inspection and packaging tasks under variable lighting conditions strongly influenced the emergence of visual fatigue. While this sector provides a highly relevant context due to its demanding visual requirements, the findings should not be generalized to all industrial environments without caution. Instead, they highlight the potential of the proposed methodology, which can be adapted and validated in other operational settings through future research.</p><p>Finally, the relationship between visual fatigue, task engagement, and cognitive stress warrants discussion. In this study, the task itself remained constant across participants, consisting of repetitive inspection and packaging of pharmaceutical products. This ensured that task complexity and cognitive demands did not vary, minimizing the likelihood of cognitive stress as a confounding factor. Although physical demands differed slightly across batch types (1, 10, 100 units, or trays), the underlying activity was the same, reinforcing that observed variations were linked primarily to visual fatigue rather than task load. Moreover, the PCA helped separate Fixation Characteristics (PC2), which aligned with visual fatigue markers, from Overall Engagement (PC1), which reflected attentional involvement. Their inverse relationship across the week&#8212;validated by subjective fatigue reports&#8212;further supports the interpretation that PC2 captured fatigue-specific effects, while PC1 reflected broader engagement.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05702"><title>5. Conclusions</title><p>This study introduced a human-centered, sensor-based approach to assessing visual fatigue in pharmaceutical inspection and packaging environments&#8212;a domain where sustained attention, precision, and ergonomic sustainability are critical. By integrating wearable eye-tracking technology with subjective self-assessments, a novel, data-driven framework was developed to monitor fatigue in real time and identify high-risk working conditions. A key outcome was the identification of &#8220;Fixation Characteristics&#8221; as a robust indicator of visual fatigue, grounded both in theoretical literature and validated empirically through its inverse relationship with overall attentional engagement and alignment with self-reported fatigue levels. The convergence between sensor-based measures and subjective experience exemplifies the type of human-centric integration envisioned in Industry 5.0. Consistent with this paradigm, the findings support proactive ergonomic interventions that extend beyond productivity, emphasizing worker safety, comfort, and long-term well-being.</p><p>The study was limited to a single production facility within the pharmaceutical sector, which may constrain the generalizability of the results. The participant group was also strongly skewed toward female workers, reflecting the actual demographic composition of the packaging workforce. While this enhances ecological validity, it may also introduce gender-related bias, as physiological responses to fatigue could differ across populations. To address this, future studies should include more balanced participant groups to validate the findings across mixed populations and different workplace contexts. Encouragingly, preliminary data from a second case study in the automotive sector, conducted under real work conditions with an all-male participant group, revealed comparable fatigue patterns. These findings further support the cross-domain validity of the &#8220;Fixation Characteristics&#8221; component as a reliable indicator of visual fatigue, suggesting that the PCA-based methodology developed here holds promise across diverse industrial environments. Future research will aim to incorporate further factors known to influence visual fatigue, including glare, contrast ratios, circadian rhythm disruptions, workstation design, and cognitive workload. In addition, future work will focus on developing a quantitative predictive model of visual fatigue, combining eye-tracking features with the Visual Fatigue Scale to classify fatigue into different levels, similar to previous work on physical fatigue modeling [<xref rid="B67-sensors-25-05702" ref-type="bibr">67</xref>,<xref rid="B68-sensors-25-05702" ref-type="bibr">68</xref>].</p><p>Explicitly testing and validating this approach in different industrial settings beyond pharmaceutical packaging will be essential to establish broader applicability and to formulate standardized assessment criteria for visual fatigue in Industry 5.0 contexts. Nonetheless, we acknowledge that in more complex industrial settings, cognitive stress and task engagement may interact more strongly with visual fatigue. Future research should therefore integrate multimodal measures such as heart rate variability and EEG factors to better capture this interplay.</p><p>In conclusion, this study represents an important step toward bridging the gap between controlled laboratory research and the complexities of industrial reality. It provides empirical evidence that smart, non-intrusive technologies can be employed not only to monitor visual fatigue but also to guide proactive ergonomic strategies. In doing so, it advances the objectives of Industry 5.0: creating workplaces that are more efficient, adaptive, inclusive, and centered on human health and performance.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors express their gratitude to Anyela Jineth Mateus Olarte and Diana Cathalina Mu&#241;oz Giraldo from the pharmaceutical company for their invaluable support and collaboration throughout this research.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, C.A.M.; methodology, C.A.M.; software, C.A.M. and J.F.S.-P.; validation, C.A.M., J.F.S.-P., M.A.C.S. and N.Y.M.A.; formal analysis, C.A.M. and J.F.S.-P.; investigation, C.A.M.; resources, M.D. and M.A.C.S.; data curation, C.A.M., J.F.S.-P., M.A.C.S. and N.Y.M.A.; writing&#8212;original draft preparation, C.A.M.; writing&#8212;review and editing, C.A.M., M.D., J.F.S.-P., M.A.C.S. and N.Y.M.A.; visualization, C.A.M., M.D., J.F.S.-P., M.A.C.S. and N.Y.M.A.; supervision, M.D. and J.F.S.-P.; project administration, M.D. and M.A.C.S.; funding acquisition, M.D. and M.A.C.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of Universidad de Am&#233;rica (Colombia) (protocol code 002-2024 June 2024).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The anonymized data files are available in the public Zenodo repository: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doi.org/10.5281/zenodo.15705327">https://doi.org/10.5281/zenodo.15705327</uri>.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">AoIs</td><td align="left" valign="middle" rowspan="1" colspan="1">Areas of Interest</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CVS</td><td align="left" valign="middle" rowspan="1" colspan="1">Computer vision syndrome</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CFF</td><td align="left" valign="middle" rowspan="1" colspan="1">Critical flicker fusion frequency</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EEG</td><td align="left" valign="middle" rowspan="1" colspan="1">Electroencephalography</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ILO</td><td align="left" valign="middle" rowspan="1" colspan="1">International Labour Organization</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KMO</td><td align="left" valign="middle" rowspan="1" colspan="1">Kaiser-Meyer-Olkin</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KS</td><td align="left" valign="middle" rowspan="1" colspan="1">Kolmogorov&#8211;Smirnov</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MR</td><td align="left" valign="middle" rowspan="1" colspan="1">Mixed reality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PCA</td><td align="left" valign="middle" rowspan="1" colspan="1">Principal Component Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RH</td><td align="left" valign="middle" rowspan="1" colspan="1">Relative humidity</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SW</td><td align="left" valign="middle" rowspan="1" colspan="1">Shapiro&#8211;Wilk</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SSQ</td><td align="left" valign="middle" rowspan="1" colspan="1">Simulator Sickness Questionnaire</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">T</td><td align="left" valign="middle" rowspan="1" colspan="1">Temperature</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VDTs</td><td align="left" valign="middle" rowspan="1" colspan="1">Visual display terminals</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VFS</td><td align="left" valign="middle" rowspan="1" colspan="1">Visual Fatigue Scale</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VR</td><td align="left" valign="middle" rowspan="1" colspan="1">Virtual reality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WHO</td><td align="left" valign="middle" rowspan="1" colspan="1">World Health Organization</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05702"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sheedy</surname><given-names>J.E.</given-names></name><name name-style="western"><surname>Hayes</surname><given-names>J.N.</given-names></name><name name-style="western"><surname>Engle</surname><given-names>J.</given-names></name></person-group><article-title>Is all asthenopia the same?</article-title><source>Optom. Vis. Sci.</source><year>2003</year><volume>80</volume><fpage>732</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1097/00006324-200311000-00008</pub-id><pub-id pub-id-type="pmid">14627938</pub-id></element-citation></ref><ref id="B2-sensors-25-05702"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blehm</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vishnu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Khattak</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mitra</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yee</surname><given-names>R.W.</given-names></name></person-group><article-title>Computer Vision Syndrome: A Review</article-title><source>Surv. Ophthalmol.</source><year>2005</year><volume>50</volume><fpage>253</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.survophthal.2005.02.008</pub-id><pub-id pub-id-type="pmid">15850814</pub-id></element-citation></ref><ref id="B3-sensors-25-05702"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaur</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gurnani</surname><given-names>B.</given-names></name><name name-style="western"><surname>Nayak</surname><given-names>S.</given-names></name><name name-style="western"><surname>Deori</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kaur</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jethani</surname><given-names>J.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>D.</given-names></name><name name-style="western"><surname>Agarkar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hussaindeen</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Sukhija</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Digital Eye Strain&#8212;A Comprehensive Review</article-title><source>Ophthalmol. Ther.</source><year>2022</year><volume>11</volume><fpage>1655</fpage><lpage>1680</lpage><pub-id pub-id-type="doi">10.1007/s40123-022-00540-9</pub-id><pub-id pub-id-type="pmid">35809192</pub-id><pub-id pub-id-type="pmcid">PMC9434525</pub-id></element-citation></ref><ref id="B4-sensors-25-05702"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Coles-Brennan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sulley</surname><given-names>A.</given-names></name><name name-style="western"><surname>Young</surname><given-names>G.</given-names></name></person-group><article-title>Management of digital eye strain</article-title><source>Clin. Exp. Optom.</source><year>2019</year><volume>102</volume><fpage>18</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1111/cxo.12798</pub-id><pub-id pub-id-type="pmid">29797453</pub-id></element-citation></ref><ref id="B5-sensors-25-05702"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alves</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lima</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Gaspar</surname><given-names>P.D.</given-names></name></person-group><article-title>Is Industry 5.0 a human-centred approach? A systematic review</article-title><source>Processes</source><year>2023</year><volume>11</volume><elocation-id>193</elocation-id><pub-id pub-id-type="doi">10.3390/pr11010193</pub-id></element-citation></ref><ref id="B6-sensors-25-05702"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gowrisankaran</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sheedy</surname><given-names>J.E.</given-names></name></person-group><article-title>Computer Vision Syndrome: A Review</article-title><source>Work</source><year>2015</year><volume>52</volume><fpage>303</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.3233/WOR-152162</pub-id><pub-id pub-id-type="pmid">26519133</pub-id></element-citation></ref><ref id="B7-sensors-25-05702"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sheppard</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Wolffsohn</surname><given-names>J.S.</given-names></name></person-group><article-title>Digital Eye Strain: Prevalence, Measurement and Amelioration</article-title><source>BMJ Open Ophthalmol.</source><year>2018</year><volume>3</volume><fpage>e000146</fpage><pub-id pub-id-type="doi">10.1136/bmjophth-2018-000146</pub-id><pub-id pub-id-type="pmcid">PMC6020759</pub-id><pub-id pub-id-type="pmid">29963645</pub-id></element-citation></ref><ref id="B8-sensors-25-05702"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cheung</surname><given-names>R.</given-names></name><name name-style="western"><surname>Duong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Paynter</surname><given-names>R.</given-names></name><name name-style="western"><surname>Asper</surname><given-names>L.</given-names></name></person-group><article-title>Viewing Distance and Eyestrain Symptoms with Prolonged Viewing of Smartphones</article-title><source>Clin. Exp. Optom.</source><year>2017</year><volume>100</volume><fpage>133</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1111/cxo.12453</pub-id><pub-id pub-id-type="pmid">27716998</pub-id></element-citation></ref><ref id="B9-sensors-25-05702"><label>9.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Consejo Colombiano de Seguridad</collab></person-group><article-title>Siniestralidad Laboral&#8212;Primer Semestre 2024. Observatorio de Seguridad y Salud en el Trabajo</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ccs.org.co/observatorio/assets/boletin_fasecolda/6707f0df26940.pdf" ext-link-type="uri">https://ccs.org.co/observatorio/assets/boletin_fasecolda/6707f0df26940.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-01">(accessed on 1 April 2024)</date-in-citation></element-citation></ref><ref id="B10-sensors-25-05702"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Piedrah&#237;ta</surname><given-names>H.</given-names></name></person-group><article-title>Costs of Work-Related Musculoskeletal Disorders (MSDs) in Developing Countries: Colombia Case</article-title><source>Int. J. Occup. Saf. Ergon.</source><year>2006</year><volume>12</volume><fpage>379</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1080/10803548.2006.11076696</pub-id><pub-id pub-id-type="pmid">17156613</pub-id></element-citation></ref><ref id="B11-sensors-25-05702"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Binyousef</surname><given-names>F.H.</given-names></name><name name-style="western"><surname>Alruwaili</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Altammami</surname><given-names>A.F.</given-names></name><name name-style="western"><surname>Alharbi</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Alrakaf</surname><given-names>F.A.</given-names></name><name name-style="western"><surname>Almazrou</surname><given-names>A.A.</given-names></name></person-group><article-title>Impact of Dry Eye Disease on Work Productivity among Saudi Workers in Saudi Arabia</article-title><source>Clin. Ophthalmol.</source><year>2021</year><volume>15</volume><fpage>2675</fpage><lpage>2681</lpage><pub-id pub-id-type="doi">10.2147/OPTH.S313158</pub-id><pub-id pub-id-type="pmid">34211260</pub-id><pub-id pub-id-type="pmcid">PMC8239165</pub-id></element-citation></ref><ref id="B12-sensors-25-05702"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burton</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Ramke</surname><given-names>J.</given-names></name><name name-style="western"><surname>Marques</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Bourne</surname><given-names>R.R.</given-names></name><name name-style="western"><surname>Congdon</surname><given-names>N.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>I.</given-names></name><name name-style="western"><surname>Faal</surname><given-names>H.B.</given-names></name><name name-style="western"><surname>Arunga</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bachani</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bascaran</surname><given-names>C.</given-names></name><etal/></person-group><article-title>The Lancet Global Health Commission on Global Eye Health: Vision Beyond 2020</article-title><source>Lancet Glob. Health</source><year>2021</year><volume>9</volume><fpage>e489</fpage><lpage>e551</lpage><pub-id pub-id-type="doi">10.1016/S2214-109X(20)30488-5</pub-id><pub-id pub-id-type="pmid">33607016</pub-id><pub-id pub-id-type="pmcid">PMC7966694</pub-id></element-citation></ref><ref id="B13-sensors-25-05702"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garc&#237;a &#193;lvarez</surname><given-names>P.E.</given-names></name><name name-style="western"><surname>Garc&#237;a Lozada</surname><given-names>D.</given-names></name></person-group><article-title>Factores Asociados con el S&#237;ndrome de Visi&#243;n por el Uso de Computador</article-title><source>Investig. Andin.</source><year>2010</year><volume>12</volume><fpage>107</fpage><lpage>119</lpage></element-citation></ref><ref id="B14-sensors-25-05702"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gerena Pallares</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Vargas Rodr&#237;guez</surname><given-names>L.J.</given-names></name><name name-style="western"><surname>Ni&#241;o Avenda&#241;o</surname><given-names>C.A.</given-names></name><name name-style="western"><surname>Uyaban</surname><given-names>G.C.</given-names></name><name name-style="western"><surname>Ballesteros Virgen</surname><given-names>Y.</given-names></name></person-group><article-title>Prevalencia del S&#237;ndrome Visual por Computadora en los Estudiantes de Medicina de la Ciudad de Tunja Durante la Pandemia</article-title><source>Rev. Colomb. Salud Ocup.</source><year>2022</year><volume>12</volume><fpage>e7916</fpage><pub-id pub-id-type="doi">10.18041/2322-634X/rcso.1.2022.7916</pub-id></element-citation></ref><ref id="B15-sensors-25-05702"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Naranjo Su&#225;rez</surname><given-names>I.D.</given-names></name><name name-style="western"><surname>Villanueva Naranjo</surname><given-names>L.F.</given-names></name></person-group><article-title>Prevalencia del S&#237;ndrome Visual Inform&#225;tico en Estudiantes de Optometr&#237;a de la Universidad de La Salle de S&#233;ptimo y Octavo Semestre en el Primer Ciclo del 2021</article-title><source>Undergraduate Thesis</source><publisher-name>Universidad de La Salle</publisher-name><publisher-loc>Bogot&#225;, Colombia</publisher-loc><year>2021</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ciencia.lasalle.edu.co/server/api/core/bitstreams/848a9b7a-6613-4eb2-83d6-5abff1eaeed6/content" ext-link-type="uri">https://ciencia.lasalle.edu.co/server/api/core/bitstreams/848a9b7a-6613-4eb2-83d6-5abff1eaeed6/content</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2021-04-01">(accessed on 1 April 2021)</date-in-citation></element-citation></ref><ref id="B16-sensors-25-05702"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Benedetto</surname><given-names>S.</given-names></name><name name-style="western"><surname>Carbone</surname><given-names>A.</given-names></name><name name-style="western"><surname>Drai-Zerbib</surname><given-names>V.</given-names></name><name name-style="western"><surname>Pedrotti</surname><given-names>M.</given-names></name><name name-style="western"><surname>Baccino</surname><given-names>T.</given-names></name></person-group><article-title>Effects of Luminance and Illuminance on Visual Fatigue and Arousal During Digital Reading</article-title><source>Comput. Hum. Behav.</source><year>2014</year><volume>41</volume><fpage>112</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.chb.2014.09.023</pub-id></element-citation></ref><ref id="B17-sensors-25-05702"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kennedy</surname><given-names>R.S.</given-names></name><name name-style="western"><surname>Lane</surname><given-names>N.E.</given-names></name><name name-style="western"><surname>Berbaum</surname><given-names>K.S.</given-names></name><name name-style="western"><surname>Lilienthal</surname><given-names>M.G.</given-names></name></person-group><article-title>Simulator Sickness Questionnaire: An Enhanced Method for Quantifying Simulator Sickness</article-title><source>Int. J. Aviat. Psychol.</source><year>1993</year><volume>3</volume><fpage>203</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1207/s15327108ijap0303_3</pub-id></element-citation></ref><ref id="B18-sensors-25-05702"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eckstein</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Guerra-Carrillo</surname><given-names>B.</given-names></name><name name-style="western"><surname>Miller Singley</surname><given-names>A.T.</given-names></name><name name-style="western"><surname>Bunge</surname><given-names>S.A.</given-names></name></person-group><article-title>Beyond Eye Gaze: What Else Can Eyetracking Reveal About Cognition and Cognitive Development?</article-title><source>Dev. Cogn. Neurosci.</source><year>2017</year><volume>25</volume><fpage>69</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.dcn.2016.11.001</pub-id><pub-id pub-id-type="pmid">27908561</pub-id><pub-id pub-id-type="pmcid">PMC6987826</pub-id></element-citation></ref><ref id="B19-sensors-25-05702"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>Exploration of Eye Fatigue Detection Features and Algorithm Based on Eye-Tracking Signal</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>1798</elocation-id><pub-id pub-id-type="doi">10.3390/electronics13101798</pub-id></element-citation></ref><ref id="B20-sensors-25-05702"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Craig</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tran</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wijesuriya</surname><given-names>N.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name></person-group><article-title>Regional brain wave activity changes associated with fatigue</article-title><source>Psychophysiology</source><year>2012</year><volume>49</volume><fpage>574</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2011.01329.x</pub-id><pub-id pub-id-type="pmid">22324302</pub-id></element-citation></ref><ref id="B21-sensors-25-05702"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Trejo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kubitz</surname><given-names>K.</given-names></name><name name-style="western"><surname>Rosipal</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kochavi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Montgomery</surname><given-names>L.</given-names></name></person-group><article-title>EEG-Based Estimation and Classification of Mental Fatigue</article-title><source>Mental Fatigue: Neural Correlates, Assessment and Interventions</source><publisher-name>Wiley</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2021</year><comment>Chapter 13</comment><fpage>272</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1002/9781119386957.ch13</pub-id></element-citation></ref><ref id="B22-sensors-25-05702"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aksu</surname><given-names>&#350;.H.</given-names></name><name name-style="western"><surname>&#199;ak&#305;t</surname><given-names>E.</given-names></name><name name-style="western"><surname>Da&#287;deviren</surname><given-names>M.</given-names></name></person-group><article-title>Mental Workload Assessment Using Machine Learning Techniques Based on EEG and Eye Tracking Data</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>2282</elocation-id><pub-id pub-id-type="doi">10.3390/app14062282</pub-id></element-citation></ref><ref id="B23-sensors-25-05702"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hirzle</surname><given-names>T.</given-names></name><name name-style="western"><surname>Cordts</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rukzio</surname><given-names>E.</given-names></name><name name-style="western"><surname>Bulling</surname><given-names>A.</given-names></name></person-group><article-title>A Survey of Digital Eye Strain in Gaze-Based Interactive Systems</article-title><source>Proceedings of the ACM Symposium on Eye Tracking Research &amp; Applications (ETRA)</source><conf-loc>Stuttgart, Germany</conf-loc><conf-date>2&#8211;5 June 2020</conf-date><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3379155.3391313</pub-id></element-citation></ref><ref id="B24-sensors-25-05702"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lambooij</surname><given-names>M.T.M.</given-names></name><name name-style="western"><surname>Fortuin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heynderickx</surname><given-names>I.</given-names></name><name name-style="western"><surname>IJsselsteijn</surname><given-names>W.A.</given-names></name></person-group><article-title>Visual Discomfort and Visual Fatigue of Stereoscopic Displays: A Review</article-title><source>J. Imaging Sci. Technol.</source><year>2009</year><volume>53</volume><fpage>art00001</fpage><pub-id pub-id-type="doi">10.2352/J.ImagingSci.Technol.2009.53.3.030201</pub-id></element-citation></ref><ref id="B25-sensors-25-05702"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rosenfield</surname><given-names>M.</given-names></name></person-group><article-title>Computer Vision Syndrome: A Review of Ocular Causes and Potential Treatments</article-title><source>Ophthalmic Physiol. Opt.</source><year>2011</year><volume>31</volume><fpage>502</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1111/j.1475-1313.2011.00834.x</pub-id><pub-id pub-id-type="pmid">21480937</pub-id></element-citation></ref><ref id="B26-sensors-25-05702"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Uchino</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yokoi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uchino</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dogru</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kawashima</surname><given-names>M.</given-names></name><name name-style="western"><surname>Komuro</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sonomura</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kato</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kinoshita</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schaumberg</surname><given-names>D.A.</given-names></name><etal/></person-group><article-title>Prevalence of Dry Eye Disease and Its Risk Factors in Visual Display Terminal Users: The Osaka Study</article-title><source>Am. J. Ophthalmol.</source><year>2013</year><volume>156</volume><fpage>759</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.ajo.2013.05.040</pub-id><pub-id pub-id-type="pmid">23891330</pub-id></element-citation></ref><ref id="B27-sensors-25-05702"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>K.</given-names></name></person-group><article-title>Evaluating the Effectiveness of Static and Dynamic Exhibits in Historical Museums Based on Eye-Tracking Experiments</article-title><source>SSRN Electron. J.</source><year>2023</year><pub-id pub-id-type="doi">10.2139/ssrn.4545264</pub-id></element-citation></ref><ref id="B28-sensors-25-05702"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Untimanon</surname><given-names>O.</given-names></name><name name-style="western"><surname>Pacharatrakul</surname><given-names>W.</given-names></name><name name-style="western"><surname>Boonmeepong</surname><given-names>K.</given-names></name><name name-style="western"><surname>Thammagarun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Laemun</surname><given-names>N.</given-names></name><name name-style="western"><surname>Taptagaporn</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chongsuvivatwong</surname><given-names>V.</given-names></name></person-group><article-title>Visual Problems among Electronic and Jewelry Workers in Thailand</article-title><source>J. Occup. Health</source><year>2006</year><volume>48</volume><fpage>407</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1539/joh.48.407</pub-id><pub-id pub-id-type="pmid">17053309</pub-id></element-citation></ref><ref id="B29-sensors-25-05702"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Drouot</surname><given-names>M.</given-names></name><name name-style="western"><surname>Le Bigot</surname><given-names>N.</given-names></name><name name-style="western"><surname>Bolloc&#8217;h</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bricard</surname><given-names>E.</given-names></name><name name-style="western"><surname>de Bougrenet</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Nourrit</surname><given-names>V.</given-names></name></person-group><article-title>The Visual Impact of Augmented Reality during an Assembly Task</article-title><source>Displays</source><year>2021</year><volume>66</volume><fpage>101987</fpage><pub-id pub-id-type="doi">10.1016/j.displa.2021.101987</pub-id></element-citation></ref><ref id="B30-sensors-25-05702"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yeow</surname><given-names>P.H.P.</given-names></name><name name-style="western"><surname>Sen</surname><given-names>R.N.</given-names></name></person-group><article-title>Ergonomics Improvements of the Visual Inspection Process in a Printed Circuit Assembly Factory</article-title><source>Int. J. Ind. Ergon.</source><year>2004</year><volume>33</volume><fpage>369</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1080/10803548.2004.11076622</pub-id><pub-id pub-id-type="pmid">15598361</pub-id></element-citation></ref><ref id="B31-sensors-25-05702"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>M&#228;ttig</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kretschmer</surname><given-names>V.</given-names></name></person-group><article-title>Smart Packaging in Intralogistics: An Evaluation Study of Human&#8211;Technology Interaction in Applying New Collaboration Technologies</article-title><source>Proceedings of the 52nd Hawaii International Conference on System Sciences</source><conf-loc>Maui, HI, USA</conf-loc><conf-date>8&#8211;11 January 2019</conf-date><fpage>739</fpage><lpage>748</lpage></element-citation></ref><ref id="B32-sensors-25-05702"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jung</surname><given-names>D.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>S.</given-names></name><name name-style="western"><surname>An</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>T.</given-names></name></person-group><article-title>Bio-Signals Based Occupant-Centric Lighting Control for Cognitive Performance, Visual Fatigue and Energy Consumption</article-title><source>Build. Environ.</source><year>2025</year><volume>269</volume><fpage>112424</fpage><pub-id pub-id-type="doi">10.1016/j.buildenv.2024.112424</pub-id></element-citation></ref><ref id="B33-sensors-25-05702"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Glimne</surname><given-names>S.</given-names></name><name name-style="western"><surname>Brautaset</surname><given-names>R.</given-names></name><name name-style="western"><surname>&#214;sterman</surname><given-names>C.</given-names></name></person-group><article-title>Visual Fatigue during Control Room Work in Process Industries</article-title><source>Work</source><year>2020</year><volume>65</volume><fpage>903</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.3233/WOR-203141</pub-id><pub-id pub-id-type="pmid">32310219</pub-id><pub-id pub-id-type="pmcid">PMC7242839</pub-id></element-citation></ref><ref id="B34-sensors-25-05702"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>Y.C.</given-names></name><name name-style="western"><surname>Ho</surname><given-names>K.J.</given-names></name></person-group><article-title>Night-Shift Work and Risk of Compromised Visual Acuity Among the Workers in an Electronics Manufacturing Company</article-title><source>Int. J. Occup. Med. Environ. Health</source><year>2018</year><volume>31</volume><fpage>71</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.13075/ijomeh.1896.01020</pub-id><pub-id pub-id-type="pmid">28930301</pub-id></element-citation></ref><ref id="B35-sensors-25-05702"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>J.-T.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>G.-F.</given-names></name><name name-style="western"><surname>Hwang</surname><given-names>S.-L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>E.M.-Y.</given-names></name></person-group><article-title>A Pre-Alarm System for Visual Fatigue in an IC Packaging Factory</article-title><source>Proceedings of the IEEE International Conference on Industrial Engineering and Engineering Management</source><conf-loc>Hong Kong, China</conf-loc><conf-date>8&#8211;11 December 2009</conf-date><fpage>2029</fpage><lpage>2033</lpage></element-citation></ref><ref id="B36-sensors-25-05702"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Muraoka</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ikeda</surname><given-names>H.</given-names></name><name name-style="western"><surname>Nakashima</surname><given-names>N.</given-names></name></person-group><article-title>Electronic Method for Evaluation and Recovery of Visual Strain Caused by Video Data Terminal Operations in Industry</article-title><source>Proceedings of the Conference Record of the 2002 IEEE Industry Applications Conference</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>13&#8211;18 October 2002</conf-date><fpage>485</fpage><lpage>492</lpage></element-citation></ref><ref id="B37-sensors-25-05702"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Muraoka</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ikeda</surname><given-names>H.</given-names></name></person-group><article-title>Estimating Visual Fatigue Caused by Display Operations with Temperature Rise Sensed on Human Eyelids</article-title><source>Proceedings of the 2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)</source><conf-loc>Osaka, Japan</conf-loc><conf-date>27&#8211;30 October 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2015</year><fpage>495</fpage><lpage>496</lpage></element-citation></ref><ref id="B38-sensors-25-05702"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Muraoka</surname><given-names>T.</given-names></name><name name-style="western"><surname>Uchimura</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ikeda</surname><given-names>H.</given-names></name></person-group><article-title>Evaluating Visual Fatigue to Keep Eyes Healthy within Circadian Change during Continuous Display Operations</article-title><source>J. Disp. Technol.</source><year>2016</year><volume>12</volume><fpage>1464</fpage><lpage>1471</lpage><pub-id pub-id-type="doi">10.1109/JDT.2016.2606509</pub-id></element-citation></ref><ref id="B39-sensors-25-05702"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>B&#225;ez</surname><given-names>G.</given-names></name><name name-style="western"><surname>De la Vega</surname><given-names>E.</given-names></name><name name-style="western"><surname>Castro</surname><given-names>C.</given-names></name><name name-style="western"><surname>Elizarrar&#225;s</surname><given-names>R.</given-names></name></person-group><article-title>Use of Color Lights for the Detection of Anomalies in Quality Systems</article-title><source>Work</source><year>2012</year><volume>41</volume><fpage>5889</fpage><lpage>5891</lpage><pub-id pub-id-type="doi">10.3233/WOR-2012-0984-5889</pub-id><pub-id pub-id-type="pmid">22317720</pub-id></element-citation></ref><ref id="B40-sensors-25-05702"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>P.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Han</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Du</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name></person-group><article-title>Research on an Indoor Light Environment Comfort Evaluation Index Based on EEG and Pupil Signals</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>3411</elocation-id><pub-id pub-id-type="doi">10.3390/electronics13173411</pub-id></element-citation></ref><ref id="B41-sensors-25-05702"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Asadi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Torabi Gudarzi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hosseini</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Hashemi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Foladi Dehaghi</surname><given-names>R.</given-names></name></person-group><article-title>The Correlation Between Lighting Intensity, Eye Fatigue, Occupational Stress, and Sleep Quality in the Control Room Operators of Abadan Refinery</article-title><source>Shiraz E-Med. J.</source><year>2024</year><volume>25</volume><fpage>e141536</fpage><pub-id pub-id-type="doi">10.5812/semj-141536</pub-id></element-citation></ref><ref id="B42-sensors-25-05702"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>N.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>T.-Y.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>J.</given-names></name></person-group><article-title>Design and Implementation of a Novel Eye Fatigue Analysis System for Safe Field Workers</article-title><source>Sensors</source><year>2023</year><volume>23</volume><fpage>6515</fpage><pub-id pub-id-type="doi">10.1109/BCD57833.2023.10466333</pub-id></element-citation></ref><ref id="B43-sensors-25-05702"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Herold</surname><given-names>R.</given-names></name><name name-style="western"><surname>Gevorgyan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Damerau</surname><given-names>L.S.</given-names></name><name name-style="western"><surname>Hartmann</surname><given-names>U.</given-names></name><name name-style="western"><surname>Friemert</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ellegast</surname><given-names>R.</given-names></name><name name-style="western"><surname>Schiefer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Karamanidis</surname><given-names>K.</given-names></name><name name-style="western"><surname>Harth</surname><given-names>V.</given-names></name><name name-style="western"><surname>Tersch&#252;ren</surname><given-names>C.</given-names></name></person-group><article-title>Effects of Smart Glasses on the Visual Acuity and Eye Strain of Employees in Logistics and Picking: A Six-Month Observational Study</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6515</elocation-id><pub-id pub-id-type="doi">10.3390/s24206515</pub-id><pub-id pub-id-type="pmid">39459994</pub-id><pub-id pub-id-type="pmcid">PMC11510858</pub-id></element-citation></ref><ref id="B44-sensors-25-05702"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name></person-group><article-title>Fatigue Characterization of EEG Brain Networks Under Mixed Reality Stereo Vision</article-title><source>Brain Sci.</source><year>2024</year><volume>14</volume><elocation-id>1126</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci14111126</pub-id><pub-id pub-id-type="pmid">39595889</pub-id><pub-id pub-id-type="pmcid">PMC11591834</pub-id></element-citation></ref><ref id="B45-sensors-25-05702"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tjolleng</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cha</surname><given-names>M.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>K.</given-names></name></person-group><article-title>Development of a Human-Friendly Visual Inspection Method for Painted Vehicle Bodies</article-title><source>Appl. Ergon.</source><year>2023</year><volume>106</volume><fpage>103911</fpage><pub-id pub-id-type="doi">10.1016/j.apergo.2022.103911</pub-id><pub-id pub-id-type="pmid">36194926</pub-id></element-citation></ref><ref id="B46-sensors-25-05702"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tjolleng</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>K.</given-names></name></person-group><article-title>Development of a Visual Inspection Method for Defects on Metallic Surface Considering Emergent Feature</article-title><source>Appl. Sci.</source><year>2016</year><volume>6</volume><fpage>1234</fpage></element-citation></ref><ref id="B47-sensors-25-05702"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ulutas</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ozkan</surname><given-names>N.F.</given-names></name></person-group><article-title>Assessing Visual Control Activities in Ceramic Tile Surface Defect Detection: An Eye-Tracking Study</article-title><source>Int. J. Intell. Eng. Inform.</source><year>2017</year><volume>5</volume><fpage>342</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1504/IJIEI.2017.087959</pub-id></element-citation></ref><ref id="B48-sensors-25-05702"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name></person-group><article-title>Investigation of Weighted Scales for Measuring Visual Fatigue in Screening Tasks</article-title><source>Proceedings of the 2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</source><conf-loc>Virtual Conference</conf-loc><conf-date>31 October&#8211;4 November 2021</conf-date><fpage>5768</fpage><lpage>5771</lpage><pub-id pub-id-type="doi">10.1109/EMBC46164.2021.9630334</pub-id><pub-id pub-id-type="pmid">34892430</pub-id></element-citation></ref><ref id="B49-sensors-25-05702"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nahavandi</surname><given-names>S.</given-names></name></person-group><article-title>Industry 5.0&#8212;A Human-Centric Solution</article-title><source>Sustainability</source><year>2019</year><volume>11</volume><elocation-id>4371</elocation-id><pub-id pub-id-type="doi">10.3390/su11164371</pub-id></element-citation></ref><ref id="B50-sensors-25-05702"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Trstenjak</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bene&#353;ova</surname><given-names>A.</given-names></name><name name-style="western"><surname>Opetuk</surname><given-names>T.</given-names></name><name name-style="western"><surname>Cajner</surname><given-names>H.</given-names></name></person-group><article-title>Human Factors and Ergonomics in Industry 5.0&#8212;A Systematic Literature Review</article-title><source>Appl. Sci.</source><year>2025</year><volume>15</volume><elocation-id>2123</elocation-id><pub-id pub-id-type="doi">10.3390/app15042123</pub-id></element-citation></ref><ref id="B51-sensors-25-05702"><label>51.</label><element-citation publication-type="book"><std>ISO/CIE 8995-1:2025</std><source>Light and Lighting&#8212;Lighting of Work Places&#8212;Part 1: Indoor</source><publisher-name>ISO</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2025</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.iso.org/standard/76342.html" ext-link-type="uri">https://www.iso.org/standard/76342.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-01">(accessed on 1 January 2025)</date-in-citation></element-citation></ref><ref id="B52-sensors-25-05702"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Winn</surname><given-names>B.</given-names></name><name name-style="western"><surname>Whitaker</surname><given-names>D.</given-names></name><name name-style="western"><surname>Elliott</surname><given-names>D.B.</given-names></name><name name-style="western"><surname>Phillips</surname><given-names>N.J.</given-names></name></person-group><article-title>Factors Affecting Light-Adapted Pupil Size in Normal Human Subjects</article-title><source>Investig. Ophthalmol. Vis. Sci.</source><year>1994</year><volume>35</volume><fpage>1132</fpage><lpage>1137</lpage><pub-id pub-id-type="pmid">8125724</pub-id></element-citation></ref><ref id="B53-sensors-25-05702"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jolliffe</surname><given-names>I.T.</given-names></name><name name-style="western"><surname>Cadima</surname><given-names>J.</given-names></name></person-group><article-title>Principal Component Analysis: A Review and Recent Developments</article-title><source>Philos. Trans. R. Soc. A</source><year>2016</year><volume>374</volume><fpage>20150202</fpage><pub-id pub-id-type="doi">10.1098/rsta.2015.0202</pub-id><pub-id pub-id-type="pmid">26953178</pub-id><pub-id pub-id-type="pmcid">PMC4792409</pub-id></element-citation></ref><ref id="B54-sensors-25-05702"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaiser</surname><given-names>H.F.</given-names></name></person-group><article-title>The Varimax Criterion for Analytic Rotation in Factor Analysis</article-title><source>Psychometrika</source><year>1958</year><volume>23</volume><fpage>187</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1007/BF02289233</pub-id></element-citation></ref><ref id="B55-sensors-25-05702"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mishra</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Vohra</surname><given-names>V.</given-names></name><name name-style="western"><surname>Raja</surname><given-names>K.N.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>Y.</given-names></name></person-group><article-title>Principal Component Analysis of Biometric Traits to Explain Body Conformation in Kajali Sheep of Punjab, India</article-title><source>Indian J. Anim. Sci.</source><year>2017</year><volume>87</volume><fpage>93</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.56093/ijans.v87i1.66914</pub-id></element-citation></ref><ref id="B56-sensors-25-05702"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fabrigar</surname><given-names>L.R.</given-names></name><name name-style="western"><surname>Wegener</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>MacCallum</surname><given-names>R.C.</given-names></name><name name-style="western"><surname>Strahan</surname><given-names>E.J.</given-names></name></person-group><article-title>Evaluating the Use of Exploratory Factor Analysis in Psychological Research</article-title><source>Psychol. Methods</source><year>1999</year><volume>4</volume><fpage>272</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1037/1082-989X.4.3.272</pub-id></element-citation></ref><ref id="B57-sensors-25-05702"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Razali</surname><given-names>N.M.</given-names></name><name name-style="western"><surname>Wah</surname><given-names>Y.B.</given-names></name></person-group><article-title>Power Comparisons of Shapiro&#8211;Wilk, Kolmogorov&#8211;Smirnov, Lilliefors and Anderson&#8211;Darling Tests</article-title><source>J. Stat. Model. Anal.</source><year>2011</year><volume>2</volume><fpage>21</fpage><lpage>33</lpage></element-citation></ref><ref id="B58-sensors-25-05702"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grieve</surname><given-names>A.P.</given-names></name></person-group><article-title>Tests of Sphericity of Normal Distributions and the Analysis of Repeated Measures Designs</article-title><source>Psychometrika</source><year>1984</year><volume>49</volume><fpage>257</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1007/BF02294176</pub-id></element-citation></ref><ref id="B59-sensors-25-05702"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schober</surname><given-names>P.</given-names></name><name name-style="western"><surname>Boer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Schwarte</surname><given-names>L.A.</given-names></name></person-group><article-title>Correlation Coefficients: Appropriate Use and Interpretation</article-title><source>Anesth. Analg.</source><year>2018</year><volume>126</volume><fpage>1763</fpage><lpage>1768</lpage><pub-id pub-id-type="doi">10.1213/ANE.0000000000002864</pub-id><pub-id pub-id-type="pmid">29481436</pub-id></element-citation></ref><ref id="B60-sensors-25-05702"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ellis</surname><given-names>C.J.</given-names></name></person-group><article-title>The Pupillary Light Reflex in Normal Subjects</article-title><source>Br. J. Ophthalmol.</source><year>1981</year><volume>65</volume><fpage>754</fpage><lpage>759</lpage><pub-id pub-id-type="doi">10.1136/bjo.65.11.754</pub-id><pub-id pub-id-type="pmid">7326222</pub-id><pub-id pub-id-type="pmcid">PMC1039657</pub-id></element-citation></ref><ref id="B61-sensors-25-05702"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Eye Movement Characteristics and Visual Fatigue Assessment of Virtual Reality Games with Different Interaction Modes</article-title><source>Front. Neurosci.</source><year>2023</year><volume>17</volume><elocation-id>1173127</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2023.1173127</pub-id><pub-id pub-id-type="pmid">37065908</pub-id><pub-id pub-id-type="pmcid">PMC10102480</pub-id></element-citation></ref><ref id="B62-sensors-25-05702"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marandi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Madeleine</surname><given-names>P.</given-names></name><name name-style="western"><surname>Omland</surname><given-names>&#216;.</given-names></name><name name-style="western"><surname>Vuillerme</surname><given-names>N.</given-names></name><name name-style="western"><surname>Samani</surname><given-names>A.</given-names></name></person-group><article-title>Eye Movement Characteristics Reflected Fatigue Development in Both Young and Elderly Individuals</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><elocation-id>13148</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-31577-1</pub-id><pub-id pub-id-type="pmid">30177693</pub-id><pub-id pub-id-type="pmcid">PMC6120880</pub-id></element-citation></ref><ref id="B63-sensors-25-05702"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kashevnik</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kovalenko</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mamonov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hamoud</surname><given-names>B.</given-names></name><name name-style="western"><surname>Bulygin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kuznetsov</surname><given-names>V.</given-names></name><name name-style="western"><surname>Shoshina</surname><given-names>I.</given-names></name><name name-style="western"><surname>Brak</surname><given-names>I.</given-names></name><name name-style="western"><surname>Kiselev</surname><given-names>G.</given-names></name></person-group><article-title>Intelligent Human Operator Mental Fatigue Assessment Method Based on Gaze Movement Monitoring</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6805</elocation-id><pub-id pub-id-type="doi">10.3390/s24216805</pub-id><pub-id pub-id-type="pmid">39517703</pub-id><pub-id pub-id-type="pmcid">PMC11548258</pub-id></element-citation></ref><ref id="B64-sensors-25-05702"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Connell</surname><given-names>C.</given-names></name><name name-style="western"><surname>Thompson</surname><given-names>B.</given-names></name><name name-style="western"><surname>Turuwhenua</surname><given-names>J.</given-names></name><name name-style="western"><surname>Srzich</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gant</surname><given-names>N.</given-names></name></person-group><article-title>Effects of Dopamine and Norepinephrine on Exercise-Induced Oculomotor Fatigue</article-title><source>Med. Sci. Sports Exerc.</source><year>2017</year><volume>49</volume><fpage>1778</fpage><lpage>1788</lpage><pub-id pub-id-type="doi">10.1249/MSS.0000000000001307</pub-id><pub-id pub-id-type="pmid">28452866</pub-id></element-citation></ref><ref id="B65-sensors-25-05702"><label>65.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Albarr&#225;n Morillo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Demichela</surname><given-names>M.</given-names></name></person-group><article-title>Wearable Devices Enhancing Human Capabilities in Direct Enamel Painting Defect Detection</article-title><source>Proceedings of the 34th European Safety and Reliability Conference (ESREL)</source><conf-loc>Krakow, Poland</conf-loc><conf-date>23&#8211;27 June 2024</conf-date><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://esrel2024.com/wp-content/uploads/articles/ea1/wearable-devices-enhancing-human-capabilities-in-direct-enamel-painting-defect-detection.pdf" ext-link-type="uri">https://esrel2024.com/wp-content/uploads/articles/ea1/wearable-devices-enhancing-human-capabilities-in-direct-enamel-painting-defect-detection.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-06-01">(accessed on 1 June 2024)</date-in-citation></element-citation></ref><ref id="B66-sensors-25-05702"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reddy</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Low</surname><given-names>C.K.</given-names></name><name name-style="western"><surname>Lim</surname><given-names>Y.P.</given-names></name><name name-style="western"><surname>Low</surname><given-names>L.L.</given-names></name><name name-style="western"><surname>Mardina</surname><given-names>F.</given-names></name><name name-style="western"><surname>Nursaleha</surname><given-names>M.P.</given-names></name></person-group><article-title>Computer Vision Syndrome: A Study of Knowledge and Practices in University Students</article-title><source>Nepal J. Ophthalmol.</source><year>2013</year><volume>5</volume><fpage>161</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.3126/nepjoph.v5i2.8707</pub-id><pub-id pub-id-type="pmid">24172549</pub-id></element-citation></ref><ref id="B67-sensors-25-05702"><label>67.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Albarr&#225;n Morillo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Demichela</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jawla</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kelleher</surname><given-names>J.</given-names></name></person-group><article-title>Wearable Technology and Machine Learning for Assessing Physical Fatigue in Industry 4.0</article-title><source>Proceedings of the AHFE 2024 International Conference</source><conf-loc>Nice, France</conf-loc><conf-date>24&#8211;28 July 2024</conf-date><publisher-name>AHFE International</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2024</year><pub-id pub-id-type="doi">10.54941/ahfe1005052</pub-id></element-citation></ref><ref id="B68-sensors-25-05702"><label>68.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Albarr&#225;n Morillo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Su&#225;rez-P&#233;rez</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Demichela</surname><given-names>M.</given-names></name></person-group><article-title>Exploring the Impact of Repetitive Exercise on Physical Fatigue: A Study of Industrial Task Simulation in a Controlled Fitness Setting</article-title><source>Proceedings of the AHFE 2024 International Conference</source><conf-loc>Nice, France</conf-loc><conf-date>24&#8211;28 July 2024</conf-date><publisher-name>AHFE International</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2024</year><pub-id pub-id-type="doi">10.3303/CET2399028</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05702-f001" orientation="portrait"><label>Figure 1</label><caption><p>Representative heatmap from one participant&#8217;s eye-tracking data during pharmaceutical packaging tasks. Areas in red indicate regions of highest visual attention, while green areas reflect lower levels of focus.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05702-g001.jpg"/></fig><fig position="float" id="sensors-25-05702-f002" orientation="portrait"><label>Figure 2</label><caption><p>Lighting levels (in Lux) recorded at the beginning of each trial.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05702-g002.jpg"/></fig><fig position="float" id="sensors-25-05702-f003" orientation="portrait"><label>Figure 3</label><caption><p>Scree plot of PCA for the seven eye-tracking metrics.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05702-g003.jpg"/></fig><fig position="float" id="sensors-25-05702-f004" orientation="portrait"><label>Figure 4</label><caption><p>Weekly trend of mean Fixation Characteristics (PC2), the objective indicator of visual fatigue. Values begin negative on Monday, reflecting shorter and less stable fixations typical of a rested visual system, and increase steadily through Friday, indicating longer and more stable fixations characteristic of cumulative visual fatigue.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05702-g004.jpg"/></fig><fig position="float" id="sensors-25-05702-f005" orientation="portrait"><label>Figure 5</label><caption><p>Weekly trend of mean Overall Visual Engagement (PC1). Engagement is highest at the beginning of the week, drops markedly by midweek (Wednesday), shows partial recovery thereafter, and tapers off again by Friday.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05702-g005.jpg"/></fig><fig position="float" id="sensors-25-05702-f006" orientation="portrait"><label>Figure 6</label><caption><p>Mean Visual Fatigue Scale scores across different lot sizes. Perceived fatigue increased sharply from Lot 1 to Lot 10, stabilized through Lot 100, decreased slightly for Tray 1, and peaked sharply in Tray 2.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05702-g006.jpg"/></fig><table-wrap position="float" id="sensors-25-05702-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05702-t001_Table 1</object-id><label>Table 1</label><caption><p>Rotated Component Matrix for the seven eye-tracking metrics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Variable</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Principal Component</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tot Visit dur</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tot fixation dur</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fixation Count</td><td align="center" valign="middle" rowspan="1" colspan="1">0.944</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">% total activity time</td><td align="center" valign="middle" rowspan="1" colspan="1">0.936</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">% gazes are fixation</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.963</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Avr Fixation dur</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Avr Visit dur</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.942</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05702-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05702-t002_Table 2</object-id><label>Table 2</label><caption><p>Eye movement parameters and their expected change with visual fatigue.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Eye Movement Parameters</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Change with Visual Fatigue</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fixation frequency</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8595;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Total fixation duration</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8593;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average fixation duration</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8593;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Percentage of Gazes as Fixations</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8593;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average fixation angle: left eye</td><td align="center" valign="middle" rowspan="1" colspan="1">~</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average fixation angle: right eye</td><td align="center" valign="middle" rowspan="1" colspan="1">~</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Total saccade duration</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8595;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average saccade duration</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8595;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average saccade magnitude: left eye</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8595;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average saccade magnitude: right eye</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8595;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maximum saccade magnitude: left eye</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8595;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maximum saccade magnitude: right eye</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8595;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Blinking frequency</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8593;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average blinking duration</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8593;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average rotation angle: left eye</td><td align="center" valign="middle" rowspan="1" colspan="1">~</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average rotation angle: right eye</td><td align="center" valign="middle" rowspan="1" colspan="1">~</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pupil diameter</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8595;</td></tr></tbody></table><table-wrap-foot><fn><p>&#8593;: Increase with visual fatigue; &#8595;: Decrease with visual fatigue; ~: Little to no change.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>