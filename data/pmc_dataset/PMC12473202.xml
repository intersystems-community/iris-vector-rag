<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="review-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473202</article-id><article-id pub-id-type="pmcid-ver">PMC12473202.1</article-id><article-id pub-id-type="pmcaid">12473202</article-id><article-id pub-id-type="pmcaiid">12473202</article-id><article-id pub-id-type="pmid">41012898</article-id><article-id pub-id-type="doi">10.3390/s25185658</article-id><article-id pub-id-type="publisher-id">sensors-25-05658</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>Comprehensive Review of Open-Source Fundus Image Databases for Diabetic Retinopathy Diagnosis</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Conquer</surname><given-names initials="V">Val&#233;rian</given-names></name><xref rid="af1-sensors-25-05658" ref-type="aff">1</xref><xref rid="c1-sensors-25-05658" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Lambolais</surname><given-names initials="T">Thomas</given-names></name><xref rid="af2-sensors-25-05658" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6499-5655</contrib-id><name name-style="western"><surname>Andrade-Miranda</surname><given-names initials="G">Gustavo</given-names></name><xref rid="af1-sensors-25-05658" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3458-0552</contrib-id><name name-style="western"><surname>Magnier</surname><given-names initials="B">Baptiste</given-names></name><xref rid="af2-sensors-25-05658" ref-type="aff">2</xref><xref rid="af3-sensors-25-05658" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Smolka</surname><given-names initials="B">Bogdan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05658"><label>1</label>IMT Mines Ales, 30100 Al&#232;s, France; <email>gustavo.andrade-miranda@mines-ales.fr</email></aff><aff id="af2-sensors-25-05658"><label>2</label>EuroMov Digital Health in Motion, University Montpellier, IMT Mines Ales, 30100 Al&#232;s, France</aff><aff id="af3-sensors-25-05658"><label>3</label>Service de M&#233;decine Nucl&#233;aire, Centre Hospitalier Universitaire de N&#238;mes, Universit&#233; de Montpellier, 30000 N&#238;mes, France</aff><author-notes><corresp id="c1-sensors-25-05658"><label>*</label>Correspondence: <email>valerian.conquer@etu.mines-ales.fr</email></corresp></author-notes><pub-date pub-type="epub"><day>11</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5658</elocation-id><history><date date-type="received"><day>04</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>10</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>28</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>11</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05658.pdf"/><abstract><p>Databases play a crucial role in training, validating, and comparing AI models for detecting retinal diseases, as well as in clinical research, technology development, and healthcare professional training. Diabetic retinopathy (DR), a common diabetes complication, is a leading cause of vision impairment and blindness worldwide. Early detection and management are essential to prevent irreversible vision loss. Fundus photography, known for being economical and non-contact, is a widely applicable gold standard method that offers a convenient way to diagnose and grade DR. This paper presents a comprehensive review of 22 open-source fundus retinal image databases commonly used in DR research, highlighting their main characteristics and key features. Most of these datasets were released between 2000 and 2022. These databases are analyzed through an in-depth examination of their images, enabling objective comparison using color space distances and Principal Component Analysis (PCA) based on 16 key statistical features. Finally, this review aims to support informed decision-making for researchers and practitioners involved in DR diagnosis and management, ultimately improving patient outcomes.</p></abstract><kwd-group><kwd>diabetic retinopathy</kwd><kwd>fundus image</kwd><kwd>databases</kwd><kwd>computer science image analysis</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05658"><title>1. Introduction</title><p>Diabetes is a complex disease that can have autoimmune, genetic, or acquired causes, impairing the body&#8217;s ability to regulate blood sugar, often involving problems with insulin production, insulin action, or both. This hormone is crucial for enabling glucose uptake into cells, thereby regulating blood sugar levels [<xref rid="B1-sensors-25-05658" ref-type="bibr">1</xref>]. Persistent high blood sugar levels can result in the production of acetone, which reflects metabolic disturbances associated with diabetes and is often observed in patients with complications such as nephropathy and coronary artery disease. This chronic hyperglycemia leads to biochemical and microvascular changes in the retina and consequently to diabetic retinopathy [<xref rid="B2-sensors-25-05658" ref-type="bibr">2</xref>].</p><p>Notably, diabetic retinopathy (DR), a microvascular complication of diabetes, occurs when high blood sugar levels damage the blood vessels in the retina [<xref rid="B3-sensors-25-05658" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05658" ref-type="bibr">4</xref>]. This light-sensitive layer, essential to vision, can be damaged and eventually lead to blindness if not treated promptly [<xref rid="B5-sensors-25-05658" ref-type="bibr">5</xref>]. Globally, DR affects approximately one-third of all individuals with diabetes, with vision-threatening stages present in approximately 10% of diabetic patients [<xref rid="B6-sensors-25-05658" ref-type="bibr">6</xref>]. Clinically, the progression of DR is typically classified into two main stages [<xref rid="B7-sensors-25-05658" ref-type="bibr">7</xref>]:<list list-type="bullet"><list-item><p><bold>Non-proliferative diabetic retinopathy (NPDR)</bold>: This stage is marked by weakened, bulging, or leaking retinal blood vessels, resulting in microaneurysms, hemorrhages, and fluid accumulation. It can lead to retinal edema and the formation of exudates. As ischemia progresses, additional vascular occlusions worsen the oxygen deprivation in the retina. An example is displayed in <xref rid="sensors-25-05658-f001" ref-type="fig">Figure 1</xref>b.</p></list-item><list-item><p><bold>Proliferative diabetic retinopathy (PDR)</bold>: Characterized by neovascularization as an attempt to compensate for hypoxia, these fragile new blood vessels are prone to bleeding, leading to serious complications such as vitreous hemorrhage, retinal detachment, and neovascular glaucoma. An example is presented in <xref rid="sensors-25-05658-f001" ref-type="fig">Figure 1</xref>c.</p></list-item></list></p><p>Symptoms of DR often develop gradually and may initially go unnoticed. Common symptoms include blurred vision, dark spots, difficulties with color perception, and sudden vision loss. Several risk factors influence the development and progression of DR, including the duration of diabetes, blood sugar control, hypertension, dyslipidemia, pregnancy, and smoking. Effective treatment options depend on the severity and stage of the disease and extend beyond ocular interventions such as laser photocoagulation, intravitreal injections, and vitrectomy [<xref rid="B5-sensors-25-05658" ref-type="bibr">5</xref>,<xref rid="B8-sensors-25-05658" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05658" ref-type="bibr">9</xref>]. Preventive care and management strategies are equally crucial and include regular eye examinations, strict glycemic control, management of comorbid conditions, and healthy lifestyle changes. A key diagnostic tool in DR screening and monitoring is the acquisition of high-resolution retinal photographs of the back of the eye, known as fundus images. These images reveal important anatomical structures such as the retina, optic disc, macula, and blood vessels. Higher-resolution images exhibit greater detail, thereby enabling the detection of early signs of DR and other ocular pathologies. As shown in <xref rid="sec4dot5dot2-sensors-25-05658" ref-type="sec">Section 4.5.2</xref>, the detection of such pathologies can be challenging due to the small size of their lesions. As illustrated in <xref rid="sensors-25-05658-f001" ref-type="fig">Figure 1</xref>a, several important anatomical structures can be observed in a fundus image:<list list-type="bullet"><list-item><p><bold>Blood vessels</bold>: These carry blood to and from the retina, appearing as fine lines in fundus images.</p></list-item><list-item><p><bold>Macula</bold>: Oval-shaped, yellowish area near the center of the retina responsible for detailed central vision; it appears darker in retinal images.</p></list-item><list-item><p><bold>Fovea</bold>: Located within the macula, the fovea has the highest density of cones in the retina.</p></list-item><list-item><p><bold>Optic nerve</bold>: Transmits visual information to the brain, appearing as a large spot positioned medial to the macula.</p></list-item></list></p><p>The anatomical features visible in fundus images are essential for diagnosing and monitoring DR, as they provide critical visual information that complements clinical symptoms and risk factors. In this context, the present paper focuses on publicly available fundus image databases, which serve as reference standards for clinical evaluation and are vital for training artificial intelligence models and conducting research in automated retinal disease detection. The paper is organized as follows: <xref rid="sec2-sensors-25-05658" ref-type="sec">Section 2</xref> presents the main retinal imaging techniques, particularly those used for acquiring fundus photographs; <xref rid="sec3-sensors-25-05658" ref-type="sec">Section 3</xref> introduces how these images are leveraged for disease classification and severity assessment; <xref rid="sec4-sensors-25-05658" ref-type="sec">Section 4</xref> reviews the most widely used public retinal imaging databases, detailing their structure and intended applications; and finally, <xref rid="sec5-sensors-25-05658" ref-type="sec">Section 5</xref> offers a discussion of current challenges and perspectives, concluding the study.</p><fig position="anchor" id="sensors-25-05658-f001" orientation="portrait"><label>Figure 1</label><caption><p>Examples of fundus images. (<bold>a</bold>) Fundus image of a healthy subject showing anatomical structures [<xref rid="B10-sensors-25-05658" ref-type="bibr">10</xref>]. (<bold>b</bold>) Fundus image of an unhealthy subject showing signs of non-proliferative diabetic retinopathy (NPDR) [<xref rid="B11-sensors-25-05658" ref-type="bibr">11</xref>]. (<bold>c</bold>) Fundus image of an unhealthy subject showing signs of proliferative diabetic retinopathy (PDR) [<xref rid="B12-sensors-25-05658" ref-type="bibr">12</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g001.jpg"/></fig><p>In recent years, numerous studies have investigated diabetic retinopathy through segmentation, classification, and generalization tasks using fundus images. Comprehensive surveys and benchmarks have been published to evaluate the performance of various algorithms and to highlight the limitations of generalization across different datasets and disease stages [<xref rid="B13-sensors-25-05658" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05658" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05658" ref-type="bibr">15</xref>]. Recent works have focused on cross-dataset and domain generalization to improve model robustness and transferability [<xref rid="B16-sensors-25-05658" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05658" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05658" ref-type="bibr">18</xref>], reflecting the growing interest in designing models that can adapt to variability in image quality, lesion appearance, and annotation styles across real-world clinical settings.</p><p>In this context, the present paper focuses on publicly available fundus image databases, which serve as reference standards for clinical evaluation and are vital for training artificial intelligence models and conducting research in automated retinal disease detection.</p></sec><sec id="sec2-sensors-25-05658"><title>2. Retinal Picturing Methods</title><p>Accurate imaging is a fundamental step in diagnosing and monitoring retinal diseases, including diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma. Four primary retinal imaging methods are identified in clinical practice, each with distinct procedural characteristics and diagnostic capabilities. These methods lead to the acquisition of fundus images, which are later analyzed for pathological signs. Two of the techniques require pharmacological pupil dilation using mydriatic eye drops to provide an unobstructed view of the retina, while the other two are non-mydriatic and allow for easier deployment in screening programs. Importantly, these imaging modalities are not limited to DR but are widely used across ophthalmology for general retinal evaluation and disease monitoring [<xref rid="B19-sensors-25-05658" ref-type="bibr">19</xref>]. Traditional examination techniques such as indirect ophthalmoscopy and slit-lamp biomicroscopy (with lenses like the Volk or Goldmann lens) remain essential in clinical settings for detailed visualization of the eye, particularly the periphery, and often require mydriasis for optimal performance [<xref rid="B20-sensors-25-05658" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05658" ref-type="bibr">21</xref>].</p><sec><title>Retinal Photography</title><p>Retinal photography is a non-invasive imaging method used to acquire high-resolution digital images of the eye using a specialized fundus camera. It is a cornerstone modality in both clinical and screening settings for the documentation and analysis of retinal health. Standard retinal photographs typically cover a field of view ranging from 30&#176; to 50&#176;, centered on the macula or optic disc, although extended protocols such as the ETDRS 7-field system or widefield (90&#176;&#8211;120&#176;) and ultrawidefield imaging (up to 200&#176;) may be used to capture more peripheral retinal areas [<xref rid="B22-sensors-25-05658" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05658" ref-type="bibr">23</xref>]. Fundus cameras may rely on conventional color photography or scanning laser ophthalmoscopy (SLO), each with specific characteristics in terms of resolution, contrast, and spectral channels (e.g., Optos ultrawidefield devices use only red and green wavelengths).</p><p>Retinal photographs are typically acquired within 1 to 2 min per eye and, in many cases, without requiring pharmacological pupil dilation. However, dilation may be necessary in patients with small pupils or media opacities to ensure sufficient image quality. The captured images are stored for longitudinal monitoring, enabling assessment of disease progression or treatment response in chronic conditions such as DR, glaucoma, and age-related macular degeneration.</p><p>This imaging modality is particularly well suited to automated analysis, including AI-based diagnostic support, which may operate in near real time or be integrated into delayed-reading workflows. In such workflows, images are captured at one location (e.g., by trained technicians in a primary care setting) and interpreted later by specialists, enabling efficient deployment of mass screening programs and telemedicine platforms [<xref rid="B24-sensors-25-05658" ref-type="bibr">24</xref>].</p></sec></sec><sec id="sec3-sensors-25-05658"><title>3. Disease Classification and Assessment</title><p>Once retinal images have been successfully acquired, they can be analyzed to identify and grade the patient&#8217;s ocular condition. This analysis can be performed manually by an ophthalmologist or other qualified professionals such as optometrists, general practitioners, or certified retinal graders, using computer-aided diagnostic tools. In recent years, automated methods&#8212;particularly those based on deep learning&#8212;have gained significant traction for DR detection, as outlined in the diagnostic flowchart in <xref rid="sensors-25-05658-f002" ref-type="fig">Figure 2</xref>. These methods generally fall into two categories: learning-based approaches and sequential (rule-based) processes.</p><sec id="sec3dot1-sensors-25-05658"><title>3.1. Learning-Based Methods</title><p>These approaches analyze retinal images by comparing them to large annotated datasets containing known pathological features. These systems learn complex visual patterns and continuously improve as they are exposed to new data, thereby enhancing both sensitivity and specificity in DR detection. Despite their strong performance, these models require significant computational resources and large volumes of high-quality labeled images for training&#8212;ideally with the highest possible image quality, although this is not always achievable in practice. Popular deep learning architectures for this task include convolutional neural networks (CNNs) and transformer-based models, which have demonstrated high diagnostic accuracy in different tasks [<xref rid="B25-sensors-25-05658" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05658" ref-type="bibr">26</xref>].</p></sec><sec id="sec3dot2-sensors-25-05658"><title>3.2. Sequential (Rule-Based) Processing</title><p>Alternatively, sequential processing methods rely on predefined rules and image-processing steps. These include filtering and segmentation procedures designed to isolate relevant retinal structures while avoiding confusion between pathological signs and normal anatomical features such as the optic disc, macula, or fovea. These steps often include masking of the optic disc and targeted region extraction prior to classification. These methods are computationally efficient and do not require large training datasets. However, they lack adaptability, are less robust to image variability, and do not improve with continued use, limiting their generalization and scalability.</p><p>A common challenge in both methods is the poor quality of fundus images due to issues such as improper lighting, motion blur, or media opacities [<xref rid="B27-sensors-25-05658" ref-type="bibr">27</xref>]. These limitations can hinder the visibility of key features and reduce diagnostic performance. To address this, image enhancement techniques&#8212;particularly those that are contrast-sensitive&#8212;are often employed as a preprocessing step. Such techniques improve the visibility of microaneurysms, hemorrhages, and exudates, thereby aiding in more accurate feature segmentation and classification [<xref rid="B28-sensors-25-05658" ref-type="bibr">28</xref>].</p></sec><sec id="sec3dot3-sensors-25-05658"><title>3.3. DR Severity Grading</title><p>To facilitate diagnosis and clinical decision-making, diabetic retinopathy is typically classified into five standardized levels of severity: the International Clinical Diabetic Retinopathy (ICDR) severity scale. These levels, and their corresponding numbers commonly used in datasets, are detailed in <xref rid="sensors-25-05658-f003" ref-type="fig">Figure 3</xref> and below [<xref rid="B29-sensors-25-05658" ref-type="bibr">29</xref>]:<list list-type="bullet"><list-item><p><bold>No apparent retinopathy (Level 0)</bold>: No abnormalities are present. Patients typically continue with routine diabetes management and regular eye examinations.</p></list-item><list-item><p><bold>Mild (Level 1)</bold>: Microaneurysms only and non-proliferative. Increased monitoring is recommended, and patients may be advised on better glycemic and blood pressure control to prevent progression.</p></list-item><list-item><p><bold>Moderate (Level 2)</bold>: More than just microaneurysms but less than severe non-proliferative diabetic retinopathy. The patient might need more frequent eye exams, and the healthcare team may consider medical interventions to manage diabetes more aggressively.</p></list-item><list-item><p><bold>Severe (Level 3)</bold>: A significant worsening of the condition and a high risk of progression to proliferative DR, presence of signs such as intraretinal hemorrhages, definite venous beading, or prominent intraretinal microvascular abnormalities.</p></list-item><list-item><p><bold>Proliferative State (Level 4)</bold>: The most advanced and sight-threatening stage with one or more of the following: neovascularization, vitreous or preretinal hemorrhage. Urgent treatment is required, which may include intravitreal injections, laser therapy, or vitrectomy to prevent permanent vision loss.</p></list-item></list></p><p>The classification and assessment of diabetic retinopathy (DR) from fundus images depend critically on image quality, the chosen diagnostic methodology, and the precise detection of signs. Learning-based methods, particularly those employing deep learning, have demonstrated strong potential for automated analysis; however, their performance is largely dependent on the availability of large-scale, well-annotated image datasets. Accordingly, the identification and use of publicly available fundus image databases are essential for enabling reliable classification aligned with standardized grading protocols. Such classification is a key component in supporting timely clinical decision-making and improving patient outcomes. The following section reviews the principal public datasets that facilitate the development and validation of automated diagnostic systems for DR.</p></sec></sec><sec id="sec4-sensors-25-05658"><title>4. Retinal Image Databases for DR Research</title><p>This section provides an overview of retinal image databases used in diabetic retinopathy (DR) research that are publicly available for research purposes. In particular, it highlights 22 datasets, each with distinct characteristics, including image count, resolution, format, geographic origin, publication year, and the availability of expert annotations.</p><sec id="sec4dot1-sensors-25-05658"><title>4.1. Key Points</title><list list-type="bullet"><list-item><p><bold>Dataset Variability:</bold> There is significant variability in dataset sizes, ranging from 28 images in AGAR300 to 88,702 images in EyePACS. Image resolutions vary significantly, potentially affecting the reliability and accuracy of subsequent analyses.</p></list-item><list-item><p><bold>Image Formats:</bold> The datasets employ a range of standard image formats&#8212;JPEG, PNG, TIFF, and BMP&#8212;each with varying compression characteristics that affect file size and storage efficiency, as well as image quality, especially the preservation of fine details and the presence of compression artifacts.</p></list-item><list-item><p><bold>Labels and Annotations:</bold> A key aspect of dataset utility for deep learning is the availability of either image-level labels or pixel-level annotations, which correspond to two distinct use cases. Labels typically refer to DR severity grades (e.g., &#8220;moderate NPDR&#8221;, &#8220;proliferative DR&#8221;) and are used in classification tasks. Annotations, in contrast, provide lesion-level localization (e.g., masks for microaneurysms, exudates, hemorrhages) and are used in detection or segmentation tasks. Most deep learning models rely only on labels for severity classification, while others focus on localizing lesions regardless of severity grade. Some datasets offer both, enabling hybrid approaches. Anatomical landmarks such as the optic disc and macula may also be annotated to increase clinical relevance.</p></list-item><list-item><p><bold>Data Distribution:</bold> The distribution of patients affected and unaffected by DR differs markedly between datasets. While large-scale datasets like EyePACS include many unaffected patients, others like <bold>DRiDB</bold> and <bold>AGAR300</bold> focus exclusively on DR patients. Such variability affects model training and may introduce bias if class imbalance is not carefully managed.</p></list-item><list-item><p><bold>Grading Systems:</bold> The grading systems for disease severity are not consistent across datasets. For example, <bold>Messidor</bold> uses its own grading system based on the number of microaneurysms, hemorrhages, and neovascularization, while others use the International Clinical Diabetic Retinopathy Scale.</p></list-item><list-item><p><bold>Annotated Areas:</bold> Datasets provide annotations for various anatomical characteristics and signs, such as blood vessels, exudates, microaneurysms, and hemorrhages. The annotation formats vary, with some datasets providing segmentation masks and others using XML documents. Clarifying whether these annotations are intended for lesion detection or severity classification is essential for selecting appropriate datasets for model development.</p></list-item><list-item><p><bold>Color Analysis:</bold> The section includes an analysis of color histograms and statistical features for each dataset, showing differences and similarities in color distribution and image quality.</p></list-item><list-item><p><bold>Visualization and Analysis:</bold> Techniques like Multi-Dimensional Scaling (MDS) and Principal Component Analysis (PCA) are used to visualize and analyze the similarities and differences between datasets based on color histograms and other features.</p></list-item></list><p>Overall, this section emphasizes the importance of selecting (or combining) appropriate datasets for specific research objectives and the need for precise and consistent annotations to develop robust machine learning models for DR detection.</p></sec><sec id="sec4dot2-sensors-25-05658"><title>4.2. General Overview</title><p><xref rid="sensors-25-05658-t001" ref-type="table">Table 1</xref> provides an overview of the main fundus image databases used in diabetic retinopathy research. A total of 22 datasets are listed, including:<list list-type="bullet"><list-item><p><bold>AGAR300</bold>: Annotated Germs for Automated Recognition [<xref rid="B30-sensors-25-05658" ref-type="bibr">30</xref>]</p></list-item><list-item><p><bold>APTOS</bold>: Asia Pacific Tele-Ophthalmology Society [<xref rid="B31-sensors-25-05658" ref-type="bibr">31</xref>]</p></list-item><list-item><p><bold>BRSET</bold>: Brazilian Multilabel Ophthalmological Dataset [<xref rid="B32-sensors-25-05658" ref-type="bibr">32</xref>]</p></list-item><list-item><p><bold>CHASE DB1</bold>: CHASE DataBase 1 [<xref rid="B33-sensors-25-05658" ref-type="bibr">33</xref>]</p></list-item><list-item><p><bold>DDR</bold>: Dataset for Diabetic Retinopathy [<xref rid="B34-sensors-25-05658" ref-type="bibr">34</xref>]</p></list-item><list-item><p><bold>DiaRetDB0</bold>: Diabetic Retinopathy DataBase 0 [<xref rid="B35-sensors-25-05658" ref-type="bibr">35</xref>]</p></list-item><list-item><p><bold>DiaRetDB1</bold>: Diabetic Retinopathy DataBase 1 [<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>]</p></list-item><list-item><p><bold>DR HAGIS</bold>: Diabetic Retinopathy, Hypertension, Age-related Macular Degeneration and Glaucoma Images [<xref rid="B37-sensors-25-05658" ref-type="bibr">37</xref>]</p></list-item><list-item><p><bold>DRiDB</bold>: Diabetic Retinopathy image DataBase [<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>]</p></list-item><list-item><p><bold>DRIVE</bold>: Digital Retinal Images for Vessel Extraction [<xref rid="B39-sensors-25-05658" ref-type="bibr">39</xref>]</p></list-item><list-item><p><bold>E-ophtha</bold>: E-ophtha [<xref rid="B40-sensors-25-05658" ref-type="bibr">40</xref>]</p></list-item><list-item><p><bold>Eye PACS</bold>: Eye Picture Archive Communication System [<xref rid="B11-sensors-25-05658" ref-type="bibr">11</xref>]</p></list-item><list-item><p><bold>F-DCVP</bold>: Fundus-Data Computer Vision Project [<xref rid="B41-sensors-25-05658" ref-type="bibr">41</xref>]</p></list-item><list-item><p><bold>HEI-MED</bold>: Hamilton Eye Institute Macular Edema Dataset [<xref rid="B42-sensors-25-05658" ref-type="bibr">42</xref>]</p></list-item><list-item><p><bold>HRF</bold>: High-Resolution Fundus Segmentation [<xref rid="B43-sensors-25-05658" ref-type="bibr">43</xref>]</p></list-item><list-item><p><bold>IDRID</bold>: Indian Diabetic Retinopathy Image Dataset [<xref rid="B28-sensors-25-05658" ref-type="bibr">28</xref>]</p></list-item><list-item><p><bold>JSIEC</bold>: Joint Shantou International Eye Centre [<xref rid="B44-sensors-25-05658" ref-type="bibr">44</xref>]</p></list-item><list-item><p><bold>Messidor</bold>: Methods to Evaluate Segmentation and Indexing Techniques in the field of Retinal Ophthalmology [<xref rid="B45-sensors-25-05658" ref-type="bibr">45</xref>]</p></list-item><list-item><p><bold>Messidor2</bold>: Methods to Evaluate Segmentation and Indexing Techniques in the field of Retinal Ophthalmology 2 [<xref rid="B45-sensors-25-05658" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05658" ref-type="bibr">46</xref>]</p></list-item><list-item><p><bold>Retina</bold>: Retina [<xref rid="B47-sensors-25-05658" ref-type="bibr">47</xref>]</p></list-item><list-item><p><bold>ROC</bold>: Online Challenge [<xref rid="B48-sensors-25-05658" ref-type="bibr">48</xref>]</p></list-item><list-item><p><bold>STARE</bold>: Structured Analysis of the Retina [<xref rid="B10-sensors-25-05658" ref-type="bibr">10</xref>]</p></list-item></list></p><p>Each dataset is characterized by the number of images, image resolution, file format, geographic origin, year of publication, and whether the images include annotations. These databases originate from various regions around the world, including India, the UK, Finland, the USA, and France, offering a wide variety of imaging conditions and patient demographics. The databases were also published over a long period, as shown in <xref rid="sensors-25-05658-f004" ref-type="fig">Figure 4</xref>.</p><p>As previously mentioned, there is significant variation in the size of datasets and images. The second column of <xref rid="sensors-25-05658-t001" ref-type="table">Table 1</xref> reports the total number of images and <xref rid="sensors-25-05658-f005" ref-type="fig">Figure 5</xref> shows a visual comparison of the mean resolution to the number of images in each dataset. Most datasets provide low-resolution images. This means that the image resolution is far below the high-definition (HD) standard of 1280 &#215; 720 pixels and is closer to 640 &#215; 480. High resolution allows for more detailed capture of the anatomical structures of the eye in fundus images, which is crucial for precise and comprehensive analysis of DR features. Fine details such as microaneurysms, exudates, and hemorrhages can be better visualized and assessed in high-resolution images, facilitating early detection of lesions and monitoring their progression over time. This can also contribute to improving the accuracy of image analysis algorithms and automated diagnostic tools used in both research and clinical practice. Image formats vary across datasets and include JPEG, PNG, TIFF, and BMP, with JPEG being the most common. The format&#8212;especially JPEG format&#8212;can affect image quality, which may in turn influence the reliability of image processing and feature extraction techniques.</p><p>Lastly, the availability of image annotations is a critical factor in training and validating machine learning models for DR detection. Most of the datasets listed provide some form of expert annotation, ranging from global severity grades to pixel-level lesion labels. These annotations are essential for supervised learning approaches and are further discussed in subsequent sections.</p></sec><sec id="sec4dot3-sensors-25-05658"><title>4.3. Data Distribution and Patient Health Status</title><p>Another important characteristic of these databases is the proportion of patients without DR they include. A balanced representation of patients with or without DR is critical for training robust image analysis algorithms. A sufficient number of patients without DR not only improves classification accuracy but also reduces model bias and enhances generalization to real-world screening scenarios. An imbalanced dataset, particularly one heavily biased toward pathological cases, can lead to overfitting and poor performance in detecting normal retinas.</p><p>The bar chart presented in <xref rid="sensors-25-05658-f006" ref-type="fig">Figure 6</xref> illustrates the distribution of patients with diabetic retinopathy across various databases, distinguishing between those diagnosed with the DR condition and those without it. This distribution varies significantly between the different databases. In most of them, only patients with DR are provided, which highlights a significant imbalance that can hinder the development of robust and generalizable diagnostic models. The inclusion of patients without DR is closely linked to the specific objective of each database. For instance, <bold>F-DCVP</bold> and <bold>Messidor2</bold> do not provide any information regarding the presence or absence of DR [<xref rid="B41-sensors-25-05658" ref-type="bibr">41</xref>]. Therefore, they are not present in <xref rid="sensors-25-05658-f006" ref-type="fig">Figure 6</xref>. While some databases, such as <bold>IDRID</bold> or <bold>DiaRetDB1</bold>, include only a small proportion of patients without DR [<xref rid="B28-sensors-25-05658" ref-type="bibr">28</xref>,<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>]&#8212;limiting their usefulness for training algorithms that require balanced datasets&#8212;others like <bold>E-ophtha</bold>, <bold>HEI-MED</bold>, and <bold>BRSET</bold> stand out by containing a majority of patients without DR, making them more suitable for developing models capable of distinguishing between DR and other cases [<xref rid="B32-sensors-25-05658" ref-type="bibr">32</xref>,<xref rid="B40-sensors-25-05658" ref-type="bibr">40</xref>,<xref rid="B42-sensors-25-05658" ref-type="bibr">42</xref>].</p><p>The <bold>DR HAGIS</bold> and <bold>DRiDB</bold> datasets consist entirely of patients diagnosed with DR [<xref rid="B37-sensors-25-05658" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>]. These datasets have been specifically curated for the study of the condition, providing a concentrated source of pathological cases. Such datasets are particularly valuable for training, validating, and benchmarking analytical or machine learning models focused on disease detection.</p><p>Similarly, the <bold>DRIVE</bold> and <bold>DR HAGIS</bold> datasets only include patients with DR [<xref rid="B37-sensors-25-05658" ref-type="bibr">37</xref>,<xref rid="B39-sensors-25-05658" ref-type="bibr">39</xref>]. They are especially useful for detailed studies on the manifestation and progression of the disease, supporting the development of specialized diagnostic algorithms and fine-grained lesion analysis.</p><p>In contrast, the <bold>APTOS</bold>, <bold>Messidor</bold>, and <bold>E-ophtha</bold> datasets feature a more balanced distribution of patients, including a significant proportion of patients without DR [<xref rid="B31-sensors-25-05658" ref-type="bibr">31</xref>,<xref rid="B40-sensors-25-05658" ref-type="bibr">40</xref>]. This diversity enhances their suitability for developing and evaluating diagnostic tools capable of distinguishing between DR and non-DR cases, particularly in realistic screening scenarios.</p><p>More broadly, the chart highlights the heterogeneous compositions of DR datasets, each designed with distinct research objectives in mind. Datasets predominantly composed of diseased cases are well suited to in-depth pathological studies, such as lesion detection, whereas those that include both cases are essential for developing and validating classification algorithms. This variability underscores the importance of selecting datasets that align with the specific goals and requirements of a given study.</p></sec><sec id="sec4dot4-sensors-25-05658"><title>4.4. Grading and Annotations</title><p><xref rid="sensors-25-05658-t002" ref-type="table">Table 2</xref> provides a comparative overview of various databases used for diabetic retinopathy analysis, emphasizing their key characteristics and differences in annotation formats. Each database is evaluated based on several criteria, including disease severity grading and the presence of annotations for blood vessels, exudates, microaneurysms, and hemorrhages. The comparison highlights the diverse features offered by these databases. They can be broadly categorized into three groups based on their primary focus: blood vessel segmentation, disease severity assessment, and sign-specific annotation.</p><p>Note that <bold>F-DCVP</bold> and <bold>Messidor2</bold> are not included in the table, as they do not provide any of the listed annotated features. The <bold>AGAR300</bold> dataset only contains images of patients with microaneurysms but does not provide any annotations [<xref rid="B30-sensors-25-05658" ref-type="bibr">30</xref>,<xref rid="B41-sensors-25-05658" ref-type="bibr">41</xref>,<xref rid="B45-sensors-25-05658" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05658" ref-type="bibr">46</xref>]. Among the listed datasets, <bold>APTOS</bold>, <bold>DDR</bold>, <bold>Eye PACS</bold>, <bold>JSIEC</bold>, <bold>Messidor</bold>, and <bold>STARE</bold> include disease severity grading [<xref rid="B10-sensors-25-05658" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05658" ref-type="bibr">11</xref>,<xref rid="B31-sensors-25-05658" ref-type="bibr">31</xref>,<xref rid="B34-sensors-25-05658" ref-type="bibr">34</xref>,<xref rid="B44-sensors-25-05658" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05658" ref-type="bibr">45</xref>]. Although <bold>STARE</bold> includes annotations for multiple lesion types, it does not offer segmentation masks&#8212;only severity grading is available. For blood vessel segmentation, <bold>CHASE DB1</bold>, <bold>DR HAGIS</bold>, <bold>DRIVE</bold>, and <bold>HRF Segmentation</bold> are particularly suitable. <bold>DRiDB</bold> and <bold>STARE</bold> also provide segmentation masks of blood vessels [<xref rid="B33-sensors-25-05658" ref-type="bibr">33</xref>,<xref rid="B37-sensors-25-05658" ref-type="bibr">37</xref>,<xref rid="B39-sensors-25-05658" ref-type="bibr">39</xref>,<xref rid="B43-sensors-25-05658" ref-type="bibr">43</xref>].</p><p>To identify the signs of DR, <bold>DiaRetDB</bold>, <bold>DRiDB</bold>, <bold>E-ophtha</bold>, <bold>HEI-MED</bold>, <bold>IDRID</bold>, and <bold>ROC</bold> are the most relevant datasets. However, the annotation format varies significantly between them. For example, <bold>ROC</bold> annotations are reported in an XML format, where each sign, such as a microaneurysm, is represented by a central position and a radius [<xref rid="B48-sensors-25-05658" ref-type="bibr">48</xref>]. Similarly, HEI-MED provides Matlab files. These files are used to generate annotations of the images from the dataset. This approach is generally considered less precise than pixel-wise segmentation masks used in datasets such as <bold>DRiDB</bold> or <bold>IDRID</bold> [<xref rid="B28-sensors-25-05658" ref-type="bibr">28</xref>,<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>].</p><p>Some of the datasets fall into multiple categories. For instance, <bold>DRiDB</bold> includes both blood vessels and lesion segmentation, while <bold>IDRID</bold> provides sign-level annotations along with disease severity grading [<xref rid="B28-sensors-25-05658" ref-type="bibr">28</xref>,<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>]. Additionally, several databases contain annotations related to anatomical structures, further enriching the data available for training and evaluation.</p><p>In summary, annotation formats vary widely and play a crucial role in detecting the presence, severity, and progression of DR. The precision and consistency of annotations are key to developing robust and well-performing algorithms. Some datasets, such as <bold>DRiDB</bold> and <bold>DiaRetDB</bold>, include annotations from multiple experts&#8212;typically five or fewer&#8212;which helps improve reliability through consensus [<xref rid="B35-sensors-25-05658" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>,<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>]. The annotation format facilitates this process by supporting precise and interpretable labeling.</p></sec><sec id="sec4dot5-sensors-25-05658"><title>4.5. Analyses of the Annotations</title><p>Annotations in retinal image analysis are essential for identifying and grading the severity of diabetic retinopathy (DR). They aid in tracking disease progression and are vital for training machine learning models for automated detection and diagnosis. This section examines the grading criteria used across different datasets, highlighting their key similarities and differences.</p><sec id="sec4dot5dot1-sensors-25-05658"><title>4.5.1. Disease Grading</title><p>Among the datasets which show disease severity (i.e., <bold>APTOS</bold>, <bold>BRSET</bold>, <bold>DDR</bold>, <bold>Eye PACS</bold>, <bold>IDRID</bold>, <bold>Messidor</bold>, and <bold>STARE</bold>) the grading systems are not always consistent [<xref rid="B10-sensors-25-05658" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05658" ref-type="bibr">11</xref>,<xref rid="B28-sensors-25-05658" ref-type="bibr">28</xref>,<xref rid="B31-sensors-25-05658" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05658" ref-type="bibr">32</xref>,<xref rid="B34-sensors-25-05658" ref-type="bibr">34</xref>,<xref rid="B45-sensors-25-05658" ref-type="bibr">45</xref>]. The <bold>STARE</bold> dataset provides a text document with the labels of the diagnosis. There are two different labels used for DR: &#8220;Background Diabetic Retinopathy&#8221; and &#8220;Proliferative Diabetic Retinopathy&#8221;. <bold>Messidor</bold> uses its own grading system: there are no differences between severe NPDR and PDR. These grades range from 0 to 3 and are calculated based on the number of microaneurysms (denoted <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), hemorrhages (<italic toggle="yes">H</italic>), and neovascularization (<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">NV</mml:mi></mml:mrow></mml:math></inline-formula>). Disease severity grades are calculated as follows:<disp-formula><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="4pt"/><mml:mo>(</mml:mo><mml:mi>Normal</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#956;</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:mi>AND</mml:mi><mml:mspace width="4pt"/><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>&#956;</mml:mi><mml:mi>A</mml:mi><mml:mo>&#8804;</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:mi>AND</mml:mi><mml:mspace width="4pt"/><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:mn>5</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>&#956;</mml:mi><mml:mi>A</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>15</mml:mn><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:mi>OR</mml:mi><mml:mspace width="4pt"/><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>H</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:mi>AND</mml:mi><mml:mspace width="4pt"/><mml:mo>(</mml:mo><mml:mi mathvariant="italic">NV</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>3</mml:mn></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#956;</mml:mi><mml:mi>A</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>15</mml:mn><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:mi>OR</mml:mi><mml:mspace width="4pt"/><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:mi>OR</mml:mi><mml:mspace width="4pt"/><mml:mo>(</mml:mo><mml:mi mathvariant="italic">NV</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p><xref rid="sensors-25-05658-f007" ref-type="fig">Figure 7</xref> shows the proportion of each grade in the <bold>Messidor</bold> dataset. In other datasets (<bold>DDR</bold>, <bold>IDRID</bold>, <bold>Eye PACS</bold>, <bold>APTOS</bold>, <bold>JSIEC</bold>), these grades range from 0 to 4, and each number corresponds to the progression of the disease, as mentioned in <xref rid="sec3dot3-sensors-25-05658" ref-type="sec">Section 3.3</xref>:<list list-type="bullet"><list-item><p>0&#8212;No apparent retinopathy</p></list-item><list-item><p>1&#8212;Mild</p></list-item><list-item><p>2&#8212;Moderate</p></list-item><list-item><p>3&#8212;Severe</p></list-item><list-item><p>4&#8212;Proliferative DR</p></list-item></list></p><p>This grading system follows the International Clinical Diabetic Retinopathy Scale, which was introduced earlier (see <xref rid="sensors-25-05658-f003" ref-type="fig">Figure 3</xref>). This scale is used by <bold>APTOS</bold>, <bold>DDR</bold>, <bold>Eye PACS</bold> and <bold>IDRID</bold> [<xref rid="B11-sensors-25-05658" ref-type="bibr">11</xref>,<xref rid="B28-sensors-25-05658" ref-type="bibr">28</xref>,<xref rid="B31-sensors-25-05658" ref-type="bibr">31</xref>,<xref rid="B34-sensors-25-05658" ref-type="bibr">34</xref>]. <xref rid="sensors-25-05658-f008" ref-type="fig">Figure 8</xref> shows a comparison of the proportion of each grade in each dataset that uses the International Clinical DR Scale.</p><p>Due to the high quantity of data in <bold>Eye PACS</bold>, that dataset contains many patients without DR [<xref rid="B11-sensors-25-05658" ref-type="bibr">11</xref>]. These databases contain few patients with grade 4 DR, which is the rarest case. Another observation is that grade 2 is more prevalent than grade 1 in these four datasets. Note that the <bold>JSIEC</bold> dataset is based on the ICDR severity scale but does not follow it. In <bold>JSIEC</bold>, 0 corresponds to normal eye fundus, 1 to mild NPDR, 2 to moderate NPDR, 3 to severe NPDR and PD,R and 4 to suspected PDR [<xref rid="B44-sensors-25-05658" ref-type="bibr">44</xref>]. The dataset sample includes 1000 images, only 189 of which are related to DR, as shown in <xref rid="sensors-25-05658-f006" ref-type="fig">Figure 6</xref>.</p></sec><sec id="sec4dot5dot2-sensors-25-05658"><title>4.5.2. Annotated Areas</title><p>A wide variety of annotated areas are in these databases, including anatomical characteristics such as the optic disc, blood vessels, and the macula, as well as signs such as hard and soft exudates, hemorrhages, blot hemorrhages, and small red dots.</p><p>As it was mentioned before, different formats exist to annotate the fundus images. Most of them are segmentation masks with various binary mask formats such as PNG, BMP, GIF or TIFF. These formats are especially useful for annotating blood vessels. However, there are also TXT documents like in the <bold>STARE</bold> dataset and XML documents like in the <bold>ROC</bold> dataset. TXT documents in the <bold>STARE</bold> dataset contain the number and type of signs for each image but not the position of these signs in the fundus image. In contrast, XML documents provide a position, and the images in <xref rid="sensors-25-05658-f009" ref-type="fig">Figure 9</xref> of different sizes show the position and the radius of each microaneurysm that has been annotated. Some datasets such as <bold>HEI-MED</bold> and <bold>DiaRetDB0</bold> provide Matlab files that use a screening method to identify signs.</p><p>In <xref rid="sensors-25-05658-f010" ref-type="fig">Figure 10</xref>, <xref rid="sensors-25-05658-f011" ref-type="fig">Figure 11</xref>, <xref rid="sensors-25-05658-f012" ref-type="fig">Figure 12</xref>, <xref rid="sensors-25-05658-f013" ref-type="fig">Figure 13</xref> and <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref>, images from the <bold>IDRID</bold>, <bold>E-ophtha</bold>, <bold>DiaRetDB1</bold>, and <bold>DRiDB</bold> datasets, are given with segmentation masks of various formats. These segmentation masks were processed in order to isolate and color-code regions that corresponded either to signs of disease or to anatomical features.</p><p>In <xref rid="sensors-25-05658-f009" ref-type="fig">Figure 9</xref>, for each image, only one annotator studied these images, and considering that the area covered by each circle is small, the precision is probably not perfect. <xref rid="sensors-25-05658-f010" ref-type="fig">Figure 10</xref> shows that some annotators provide a very complete analysis of the signs (especially in image (b)), but not all annotators detail the annotations to the same extent, as can be seen in <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref> for the <bold>DRiDB</bold> dataset or in <xref rid="sensors-25-05658-f013" ref-type="fig">Figure 13</xref> [<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>,<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>]. The <bold>E-ophtha</bold> dataset also provides segmentation masks for microaneurysms and exudates. <xref rid="sensors-25-05658-f011" ref-type="fig">Figure 11</xref> shows images from this dataset, and areas of the image where microaneurysms or exudates have been annotated by an expert have been enlarged. This demonstrates the small size of these signs and the importance of having a high-resolution image and several annotators to ensure these details are not missed. In databases that do not feature several annotators per image, it is impossible to know whether the annotations are consensual, as is the case, for example, in the <bold>IDRID</bold> database.</p><p>That is why other datasets such as <bold>DiaRetDB1</bold> and <bold>DRiDB</bold> provide segmentation masks of several doctors to establish a consensus and to ensure that these data are as precise as possible [<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>,<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>]. The image and the masks below are from the <bold>DiaRetDB1</bold> database. Each segmentation mask represents a different annotated sign from several doctors.</p><p>The masks of the <bold>DiaRetDB1</bold> dataset represent the intersection of the different doctor annotations [<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>]. The whiter the area on the mask, the greater the agreement among doctors on that annotation. In this example, there is strong consensus on hard exudates and hemorrhages, while red small dots show less agreement, as only a few doctors identified that sign. It emphasizes that annotators may disagree about the area covered by a sign or whether a sign is present. The Intersection over Union (IoU) or a defined threshold can help to check if doctors reach a consensus (the IoU is equal to the area of the intersection over the area of the union).</p><p>For example, by applying a threshold of 150 (with images encoded with 8 bits) to each segmentation mask, the resulting delimited areas are shown in <xref rid="sensors-25-05658-f012" ref-type="fig">Figure 12</xref>. The Intersection over Union (IoU) shown in <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref> was computed for each annotation type of an image from the <bold>DRiDB</bold> dataset. The image (f) of <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref> helps to visualize this IoU.</p><p>Among the datasets examined in this article, <bold>DRiDB</bold> stands out as the most comprehensive. In addition to sign annotations, it includes detailed markings of anatomical structures such as the optic disc and macula. The patient image presented in <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref> was annotated by five different doctors. Each segmentation mask was processed to isolate and color-code specific regions corresponding to either disease signs or anatomical features. This mechanism allows for a clear visual comparison of the differences between annotators. For instance, the annotation in <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref>c reflects a highly detailed and exhaustive diagnosis, whereas the annotation in <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref>d highlights broader regions that may not exclusively contain the indicated sign. These differences are further illustrated in <xref rid="sensors-25-05658-f014" ref-type="fig">Figure 14</xref>f, which shows the overlap of annotations across all annotators&#8212;darker regions indicate higher agreement.</p><p>Thanks to its richness in both pathological and anatomical annotations, the DRiDB dataset is particularly well suited for developing and evaluating computer vision algorithms in medical image analysis.</p></sec></sec><sec id="sec4dot6-sensors-25-05658"><title>4.6. Annotation Quality and Bias Assessment</title><p>Although these datasets benefit from expert annotation, several issues can compromise the consistency and reliability of those labels. First, inter-observer variability remains a crucial point: while some datasets like <bold>IDRID</bold> rely on a single expert annotator&#8212;preventing any assessment of consistency&#8212;others, such as <bold>DiaRetDB1</bold> and <bold>DRiDB</bold>, provide annotations from multiple clinicians for each image. This enables the derivation of consensus annotations, for example, by intersecting individual masks, and allows for a more quantitative assessment of agreement between observers using metrics like Intersection over Union (IoU), which measures the degree of overlap between independent segmentations. Substantial inter-expert variability in the grading of retinal features is well documented, even among trained specialists, and must be considered in ground-truth generation for automated analyses [<xref rid="B49-sensors-25-05658" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-05658" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-05658" ref-type="bibr">51</xref>].</p><p>In addition, subjective interpretation and varying annotation precision lead to inconsistencies within the same dataset. Annotators may disagree on the exact delineation of lesion borders, particularly for small or poorly defined features such as red dots or soft exudates. These differences are evident in the variety of masks drawn by different experts in datasets like <bold>DRiDB</bold>, as well as in the use of broad, imprecise circles in datasets like <bold>ROC</bold>, which may not accurately follow lesion contours. Studies show that the agreement for identifying subtle pathologies is typically lower than for more prominent findings [<xref rid="B50-sensors-25-05658" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-05658" ref-type="bibr">51</xref>]. Furthermore, it is important to note that annotation errors or ambiguities directly affect not only the training of detection algorithms but also the evaluation itself [<xref rid="B52-sensors-25-05658" ref-type="bibr">52</xref>]. When reference annotations are imprecise or inconsistent, common quantitative metrics&#8212;such as IoU, Dice coefficient, sensitivity, or specificity&#8212;can underestimate the true potential of a model, or conversely, reward predictions that merely match the flaws or biases of the ground truth. As a result, the interpretation of model performance is inseparable from the reliability of the reference annotations.</p><p>Class imbalance represents another important source of bias: certain structures (such as hard exudates or the optic disc) are prevalent, whereas others (like soft exudates or blot hemorrhages) may be rare or even absent from many images. This skewed distribution can negatively affect model training, as infrequent classes are often under-represented and thus more challenging for models to learn. Assessing the extent of this imbalance through label distribution graphs or imbalance ratios can provide insight into its potential impact on model performance and generalization [<xref rid="B51-sensors-25-05658" ref-type="bibr">51</xref>,<xref rid="B53-sensors-25-05658" ref-type="bibr">53</xref>].</p><p>Finally, inconsistencies in annotation formats&#8212;whether segmentation masks, approximate circles, or text labels&#8212;as well as differences in the rigor of quality control (for example, whether expert consensus or independent verification is used) further complicate direct comparison across datasets. Such variation must be thoughtfully considered during training, evaluation, and benchmarking of automated analysis systems to ensure fair and robust comparisons.</p><sec><title>Color Analysis</title><p>The appearance of fundus images is strongly influenced by the type of imaging device used and the principles behind image acquisition. To contextualize, the list below details the imaging devices used across the analyzed datasets, including fundus camera manufacturers and their imaging modalities, when available:<list list-type="bullet"><list-item><p><bold>AGAR300</bold>: NR</p></list-item><list-item><p><bold>APTOS</bold>: NR</p></list-item><list-item><p><bold>BRSET</bold>: Canon CR2 camera, Nikon NF5050 retinal camera</p></list-item><list-item><p><bold>CHASE DB1</bold>: Nidek NM-200-D fundus camera. The images were captured at a 30-degree field of view with a resolution of 1280 &#215; 960 pixels</p></list-item><list-item><p><bold>DDR</bold>: TRC NW48, Nikon D5200, Canon CR 2 cameras</p></list-item><list-item><p><bold>DiaRetDB0</bold>: 50-degree field-of-view digital fundus camera</p></list-item><list-item><p><bold>DiaRetDB1</bold>: 50-degree field-of-view digital fundus camera</p></list-item><list-item><p><bold>DR HAGIS</bold>: TRC-NW6s (Topcon), TRC-NW8 (Topcon), or CR-DGi fundus camera (Canon)</p></list-item><list-item><p><bold>DRiDB</bold>: Zeiss VISUCAM 200 fundus camera at a 45-degree field of view</p></list-item><list-item><p><bold>DRIVE</bold>: Canon CR5 non-mydriatic 3CCD camera with a 45-degree field of view (FOV). Each image was captured using 8 bits per color plane at 768 &#215; 584 pixels.</p></list-item><list-item><p><bold>E-ophtha</bold>: NR</p></list-item><list-item><p><bold>Eye PACS</bold>: Centervue DRS (Centervue, Italy), Optovue iCam (Optovue, USA), Canon CR1/DGi/CR2 (Canon), and Topcon NW (Topcon)</p></list-item><list-item><p><bold>F-DCVP</bold>: NR</p></list-item><list-item><p><bold>HEI-MED</bold>: Visucam PRO fundus camera (ZEISS, Germany)</p></list-item><list-item><p><bold>HRF</bold>: CF-60UVi camera (Canon)</p></list-item><list-item><p><bold>IDRID</bold>: VX-10 alpha digital fundus camera (Kowa, USA)</p></list-item><list-item><p><bold>JSIEC</bold>: NR</p></list-item><list-item><p><bold>Messidor</bold>: Topcon TRC NW6 non-mydriatic fundus camera with a 45-degree field of view using 8 bits per color channel and a resolution of 1440 &#215; 960, 2240 &#215; 1488 or 2304 &#215; 1536 pixels.</p></list-item><list-item><p><bold>Messidor2</bold>: Topcon TRC NW6 non-mydriatic fundus camera with a 45-degree field of view using 8 bits per color channel and a resolution of 1440 &#215; 960, 2240 &#215; 1488 or 2304 &#215; 1536 pixels.</p></list-item><list-item><p><bold>Retina</bold>: NR</p></list-item><list-item><p><bold>ROC</bold>: TRC-NW100 (Topcon), TRC-NW200 (Topcon), or CR5&#8211;45NM (Canon)</p></list-item><list-item><p><bold>STARE</bold>: TRV-50 fundus camera (Topcon)</p></list-item></list></p><p>Most datasets rely on optical fundus photography using traditional retinal cameras (e.g., Topcon, Canon, Zeiss), while scanning laser ophthalmoscopy systems (e.g., Heidelberg, Optos) are not reported in the selected datasets. The captured images generally follow a true-color representation using standard RGB reconstruction, without mention of pseudo-color imaging or multispectral enhancement. When not reported (NR), the dataset documentation provides no details about the acquisition hardware or imaging principles.</p><p>For each image in each dataset, the following features were computed for each color channel (red, green, blue) and the grayscale version (the formula used in the Python library OpenCV to convert an image from RGB to grayscale is as follows: <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="1.em"/><mml:mi>Y</mml:mi><mml:mo>&#8592;</mml:mo><mml:mn>0.299</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>0.587</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mn>0.114</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">R</italic>, <italic toggle="yes">G</italic>, and <italic toggle="yes">B</italic> represent the red, green, and blue color channels of the image, respectively; this weighted sum reflects the human eye&#8217;s higher sensitivity to green and red light, resulting in a perceptually accurate grayscale representation):<list list-type="bullet"><list-item><p>Mean;</p></list-item><list-item><p>Minimum;</p></list-item><list-item><p>Maximum;</p></list-item><list-item><p>Mean of the local variances (computed using a 5 &#215; 5 window);</p></list-item><list-item><p>Histograms (blue, green, red, and grayscale).</p></list-item></list>
These features provide both global and local information about image intensity and color distribution. In particular, the local variance helps to quantify texture, edge sharpening, and detail variation in small regions of the image. Additionally, in the figures provided in <xref rid="app1-sensors-25-05658" ref-type="app">Supplementary Materials</xref>,
<list list-type="order"><list-item><p>The <bold>top row</bold> shows box plots for the first four statistical features listed above;</p></list-item><list-item><p>The <bold>bottom row</bold> presents the normalized histograms, computed as the average of all image histograms within each dataset.</p></list-item></list>
<xref rid="sensors-25-05658-f015" ref-type="fig">Figure 15</xref> synthesizes all these figures. Note: When the minimum value of a color channel is zero for all images in a dataset, the corresponding box plot is reduced to a single line at zero in the middle of the graph (see <xref rid="app1-sensors-25-05658" ref-type="app">Supplementary Materials</xref>).</p><p>These graphs highlight both the differences and similarities among the datasets in terms of image content. To ensure consistent statistical analysis across datasets, a preprocessing step was applied to isolate the retinal region. When available, fundus masks provided by the datasets were used to crop the images, restricting computations to the region of interest. For datasets lacking such masks, a simple background removal heuristic was employed: only pixels with the exact RGB value #000000 (pure black) were excluded from the analysis. However, this method is not always reliable, as background regions are not uniformly black across all datasets&#8212;some may contain near-black values that are not captured by this thresholding, potentially introducing background noise into the statistics. A more common approach to background masking in retinal images is to detect the circular region of interest using Hough transformations or similar techniques such as RANSAC (RANdom SAmple Consensus) [<xref rid="B54-sensors-25-05658" ref-type="bibr">54</xref>].</p><p>Datasets that include a background mask are <bold>DiaRetDB0</bold>, <bold>DiaRetDB1</bold>, <bold>DR HAGIS</bold>, <bold>DRiDB</bold>, <bold>DRIVE</bold>, and <bold>HRF Segmentation</bold> [<xref rid="B35-sensors-25-05658" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05658" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05658" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05658" ref-type="bibr">39</xref>,<xref rid="B43-sensors-25-05658" ref-type="bibr">43</xref>]. For these six datasets, the box plots of minimum values are more reliable and meaningful, since the background was properly excluded. In contrast, for datasets without explicit masks, the background may vary in intensity and not be purely black. For instance, the box plots of minimum values in the <bold>STARE</bold> dataset reveal outliers above zero across all color channels [<xref rid="B10-sensors-25-05658" ref-type="bibr">10</xref>].</p><p>To better visualize the differences and similarities between datasets, histogram distances are shown in <xref rid="sensors-25-05658-f016" ref-type="fig">Figure 16</xref>, and a Principal Component Analysis (PCA) of the remaining features is presented further.</p><p>In <xref rid="sensors-25-05658-f016" ref-type="fig">Figure 16</xref>, each matrix is a square matrix representing the pairwise distances between image datasets based on color histograms. The rows and columns correspond to different datasets, such as <bold>APTOS</bold>, <bold>DRIVE</bold>, <bold>STARE</bold>, and others. There are four matrices in total, each normalized and computed using a different histogram: one for the blue channel, one for the green channel, one for the red channel, and one for the grayscale histogram.</p><p>Each matrix displays two types of distances: the upper triangle contains the <bold>Manhattan distances</bold> [<xref rid="B55-sensors-25-05658" ref-type="bibr">55</xref>], while the lower triangle shows the <bold>Bhattacharyya distances</bold>. In a given color space, let <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> denote the histograms extracted from two different images. Each histogram contains <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> bins, and for an integer <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mn>255</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the value of the <italic toggle="yes">k</italic>th bin of histogram <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The histograms are normalized so that their bin values sum to one:<disp-formula><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>for</mml:mi><mml:mspace width="4.pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This normalization allows for a meaningful comparison between histograms using different distance metrics, defined as follows:<list list-type="bullet"><list-item><p><bold>Manhattan distance (L1 norm)</bold>:<disp-formula><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>Manhattan</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><bold>Bhattacharyya distance</bold> (if histograms are not normalized):<disp-formula><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>Bhattacharyya</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msqrt><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where<disp-formula><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>for</mml:mi><mml:mspace width="4.pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p>To better interpret the computed distance matrices, we can analyze the patterns that emerge from comparing different datasets. The similarities among datasets that include background masks appear more clearly in the red channel matrix. Nonetheless, the <bold>DRIVE</bold> and <bold>HRF Segmentation</bold> datasets do not seem to follow this pattern. According to <xref rid="sensors-25-05658-t002" ref-type="table">Table 2</xref> and the original dataset sources, this discrepancy is likely due to the different purposes of these datasets: <bold>DiaRetDB0</bold>, <bold>DiaRetDB1</bold>, <bold>DR HAGIS</bold>, and <bold>DRiDB</bold> are specifically designed for detecting DR signs, whereas <bold>DRIVE</bold> and <bold>HRF Segmentation</bold> are primarily intended for blood vessel segmentation. Additionally, the <bold>Messidor</bold> dataset appears to differ significantly from the others, particularly in terms of the Bhattacharyya distance in the red channel matrix.</p><p><xref rid="sensors-25-05658-f017" ref-type="fig">Figure 17</xref> presents a 2D visualization of the average matrix (averaged over RGB and grayscale channels) based on the Bhattacharyya distance. The visualization was generated using metric Multi-Dimensional Scaling (MDS), which helped to better interpret the relationships depicted by the distance matrices. To realize this reduction in dimension, MDS uses a symmetric matrix <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#10877;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#10877;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> containing the Bhattacharyya distances between the <italic toggle="yes">n</italic> datasets. Then, the objective of the metric MDS is to find <italic toggle="yes">n</italic> points <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> that minimize the following stress function:<disp-formula><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The same principle was applied to the Manhattan distances in <xref rid="sensors-25-05658-f018" ref-type="fig">Figure 18</xref>. For a more detailed mathematical treatment of MDS algorithms and their applications, see [<xref rid="B56-sensors-25-05658" ref-type="bibr">56</xref>].</p><p>According to <xref rid="sensors-25-05658-f017" ref-type="fig">Figure 17</xref>, the <bold>APTOS</bold> and <bold>JSIEC</bold> datasets, the <bold>DDR</bold> and <bold>F-DCVP</bold> datasets, or the <bold>E-ophtha</bold> and <bold>DRiDB</bold> datasets are positioned close to each other, while other datasets such as <bold>AGAR300</bold> and <bold>DRIVE</bold> appear as outliers. The graph in <xref rid="sensors-25-05658-f019" ref-type="fig">Figure 19</xref> further supports these observations. However, the graph in <xref rid="sensors-25-05658-f018" ref-type="fig">Figure 18</xref>, which uses the Manhattan distance, seems to emphasize the differences even if the outliers remain the same, and the closest datasets are also <bold>APTOS</bold> and <bold>JSIEC</bold>.</p><p>While <xref rid="sensors-25-05658-f016" ref-type="fig">Figure 16</xref> focused on differences between histograms, <xref rid="sensors-25-05658-f019" ref-type="fig">Figure 19</xref> presents a Principal Component Analysis (PCA) based on the 16 key statistical features derived from the box plots in <xref rid="app1-sensors-25-05658" ref-type="app">Supplementary Materials</xref>. Specifically, for each image in a dataset, four features were computed for each color channel (red, green, blue, and grayscale): the mean, minimum, maximum, and mean of the local variances. This resulted in 16 features per image. For each dataset, the median value of each of these 16 features was calculated across all images, yielding a single 16-dimensional feature vector per dataset. PCA was then applied to these vectors, and the datasets were visualized in a two-dimensional space using the first two principal components. The PCA plot reveals a clear separation among datasets, suggesting that the selected features captured meaningful differences in image characteristics. However, this approach has certain limitations: it does not consider retinal structures or pathological lesions, and since PCA was applied to aggregated median values, it does not reflect intra-dataset variability.</p><p>The PCA results offer valuable insights into the relationships between datasets. When datasets are clustered closely in the PCA plot, it suggests that they share similar characteristics in terms of lighting conditions, color distribution, and overall image quality. In contrast, datasets positioned further apart&#8212;such as <bold>AGAR300</bold> and <bold>HEI-MED</bold>&#8212;may represent outliers due to differences in acquisition parameters, including exposure, resolution, or preprocessing workflows. Additional factors such as image compression and background segmentation techniques can also contribute to these discrepancies. For instance, <bold>DiaRetDB0</bold> applies a single background mask uniformly across all images, while <bold>DiaRetDB1</bold> assigns a unique mask to each image. This methodological difference may account for the notable divergence observed between these two otherwise comparable datasets [<xref rid="B35-sensors-25-05658" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05658" ref-type="bibr">36</xref>].</p><p><xref rid="sensors-25-05658-f017" ref-type="fig">Figure 17</xref>, <xref rid="sensors-25-05658-f018" ref-type="fig">Figure 18</xref> and <xref rid="sensors-25-05658-f019" ref-type="fig">Figure 19</xref> reveal some similarities, such as identifying outliers like <bold>AGAR300</bold>, <bold>DiaRetDB0</bold>, <bold>DiaRetDB1</bold>, and <bold>DRIVE</bold>. However, they differ in terms of which datasets are closest to each other. In <xref rid="sensors-25-05658-f017" ref-type="fig">Figure 17</xref> and <xref rid="sensors-25-05658-f018" ref-type="fig">Figure 18</xref>, <bold>JSIEC</bold> and <bold>APTOS</bold> are the closest datasets, whereas in <xref rid="sensors-25-05658-f019" ref-type="fig">Figure 19</xref>, <bold>APTOS</bold> is closer to <bold>DRiDB</bold> than to <bold>JSIEC</bold>. Additionally, <xref rid="sensors-25-05658-f019" ref-type="fig">Figure 19</xref> shows <bold>IDRID</bold> and <bold>HRF Segmentation</bold> as very close, while in <xref rid="sensors-25-05658-f017" ref-type="fig">Figure 17</xref> and <xref rid="sensors-25-05658-f018" ref-type="fig">Figure 18</xref>, they appear quite different. Neither figure establishes a clear correlation between the main characteristics of the datasets (as summarized in <xref rid="sensors-25-05658-t002" ref-type="table">Table 2</xref>) and their color histograms or color variations.</p><p>The PCA visualization provides a compact representation of inter-dataset differences based on statistical color features. While it effectively reveals outliers&#8212;such as <bold>AGAR300</bold>, <bold>HEI-MED</bold>, and <bold>DiaRetDB0</bold>&#8212;and highlights certain similarities among datasets, its utility extends beyond basic clustering. Datasets that cluster closely (e.g., <bold>JSIEC</bold>, <bold>APTOS</bold>, <bold>E-OPHTHA</bold>) likely share imaging conditions or preprocessing strategies, which has direct implications for tasks like unsupervised domain adaptation. In contrast, peripheral datasets may present more substantial domain shifts and thus serve as challenging test sets for assessing generalization.</p><p>The observed divergence between <bold>DiaRetDB0</bold> and <bold>DiaRetDB1</bold>, despite their institutional similarity, emphasizes how preprocessing choices (e.g., global vs. individual background masking) can have pronounced effects on feature distributions. These technical discrepancies can influence the success of model transferability and domain adaptation. However, the PCA&#8217;s reliance on aggregated median features and its exclusion of anatomical or pathological content limits its interpretability for clinical task alignment. Therefore, while useful for dataset comparison, PCA should be supplemented with more task-specific evaluations.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05658"><title>5. Conclusions and Discussion</title><p>This review highlighted the crucial role of open-source fundus image databases in advancing diabetic retinopathy (DR) diagnosis, particularly in the context of machine learning and automated analysis. The diversity of these datasets&#8212;regarding resolution, grading systems, annotations, and patient demographics&#8212;offers valuable opportunities for algorithm development, while also posing significant challenges in terms of standardization and model generalization.</p><p>A key takeaway is the impact of annotation quality and consistency. Datasets with pixel-level segmentation masks from multiple annotators, such as <bold>DiaRetDB1</bold> and <bold>DRiDB</bold>, offer a higher degree of reliability and are particularly suited to tasks requiring fine-grained lesion detection. Conversely, databases that rely on single-expert annotations or less precise formats (e.g., XML coordinates) may introduce variability that hinders model generalization.</p><p>Another important factor is the balance between patients affected and unaffected by DR. While many datasets over-represent DR patients to focus on pathological findings, this can lead to bias in classification models. Datasets like <bold>E-ophtha</bold> and <bold>Eye PACS</bold>, which include a substantial number of patients without DR individuals, are more appropriate for developing robust screening tools aimed at distinguishing between normal and pathological cases. Finally, the color analysis of fundus images reveals significant variability across datasets, which may affect the performance of image enhancement and preprocessing algorithms. The use of fundus masks is recommended to reduce noise and ensure consistency during image processing. The <xref rid="sensors-25-05658-f020" ref-type="fig">Figure 20</xref> summarizes the paper&#8217;s key findings.</p><sec><title>Recommendations</title><list list-type="bullet"><list-item><p><bold>For segmentation tasks:</bold> Favor datasets with pixel-level annotations and central positioning in the PCA space (e.g., <bold>DRiDB</bold> and <bold>IDRID</bold> for lesions segmentation, <bold>HRF Segmentation</bold> and <bold>DRIVE</bold> for blood vessel segmentation). Use caution when incorporating outlier datasets like <bold>AGAR300</bold>, as their distinct characteristics may hinder generalization.</p></list-item><list-item><p><bold>For classification:</bold> Datasets such as <bold>APTOS</bold> and <bold>EYE-PACS</bold>, which lie near the PCA centroid, may provide a good balance of feature diversity and representativeness for model training. They are also based on the ICDR severity scale, and one of the severity classes is not under-represented.</p></list-item><list-item><p><bold>For dataset selection or benchmarking:</bold> Central datasets like <bold>MESSIDOR</bold> and <bold>EYE-PACS</bold> can serve as robust baselines. Outliers such as <bold>HEI-MED</bold> or <bold>DiaRetDB0</bold> can be used to evaluate performance under less-controlled acquisition settings.</p></list-item></list><p>To summarize, this review provides a detailed comparative framework for selecting the most appropriate open-source fundus image database based on specific research or clinical objectives. As AI-driven DR diagnosis tools continue to evolve, standardization in grading systems, annotation formats, and data diversity will be essential to ensure their reliability, interpretability, and clinical acceptance. Future efforts should focus on creating harmonized datasets with detailed, consensus-driven annotations and broad demographic representation to support the development of equitable and accurate diagnostic systems.</p></sec></sec></body><back><ack><title>Acknowledgments</title><p>The authors gratefully acknowledge that part of the Messidor-2 dataset was kindly provided by the Messidor program partners (see <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.adcis.net/en/third-party/messidor/">https://www.adcis.net/en/third-party/messidor/</uri> accessed on 22 July 2025). We also acknowledge that the DR HAGIS dataset (Holm et al., Journal of Medical Imaging, 2017 [<xref rid="B37-sensors-25-05658" ref-type="bibr">37</xref>]) is not currently hosted at the original University of Manchester page. Interested readers may request access directly from the authors of the original publication or consult alternative research repositories (e.g., Kaggle mirrors).</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-05658"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mdpi.com/article/10.3390/s25185658/s1">https://www.mdpi.com/article/10.3390/s25185658/s1</uri>, Figure S1: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the AGAR300 dataset. Figure S2: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the APTOS dataset. Figure S3: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the BRSET dataset. Figure S4: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the CHASE DB1 dataset. Figure S5: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the DDR dataset. Figure S6: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the DiaRetDB0 dataset. Figure S7: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the DiaRetDB1 dataset. Figure S8: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the DR HAGIS dataset. Figure S9: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the DRiDB dataset. Figure S10: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the DRIVE dataset. Figure S11: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the E-Ophtha dataset. Figure S12: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the Eye PACS dataset. Figure S13: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the F-DCVP dataset. Figure S14: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the HEI MED dataset. Figure S15: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the HRF Segmentation dataset. Figure S16: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the IDRID dataset. Figure S17: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the JSIEC dataset. Figure S18: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the MESSIDOR dataset. Figure S19: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the MESSIDOR2 dataset. Figure S20: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the Retina dataset. Figure S21: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the ROC dataset. Figure S22: Average color histograms (RGB and grayscale) and box plots of key statistical features for all images in the STARE dataset.</p><supplementary-material id="sensors-25-05658-s001" position="float" content-type="local-data" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05658-s001.zip" position="float" orientation="portrait"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization, B.M. and T.L.; methodology, B.M., T.L. and G.A.-M.; software, V.C.; validation, B.M., G.A.-M. and T.L.; formal analysis, V.C. and B.M.; investigation, V.C.; resources, B.M. and G.A.-M.; data curation, V.C.; writing&#8212;original draft preparation, V.C. and B.M.; writing&#8212;review and editing, V.C., B.M. and G.A.-M.; visualization, V.C. and B.M.; supervision, B.M.; project administration, B.M. and V.C.; funding acquisition, B.M. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">AI</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Intelligence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DR</td><td align="left" valign="middle" rowspan="1" colspan="1">Diabetic Retinopathy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NPDR</td><td align="left" valign="middle" rowspan="1" colspan="1">Non-Proliferative Diabetic Retinopathy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NR</td><td align="left" valign="middle" rowspan="1" colspan="1">Not Reported</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PDR</td><td align="left" valign="middle" rowspan="1" colspan="1">Proliferative Diabetic Retinopathy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PCA</td><td align="left" valign="middle" rowspan="1" colspan="1">Principal Component Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ICDR</td><td align="left" valign="middle" rowspan="1" colspan="1">International Clinical Diabetic Retinopathy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AGAR</td><td align="left" valign="middle" rowspan="1" colspan="1">Annotated Germs for Automated Recognition</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">APTOS</td><td align="left" valign="middle" rowspan="1" colspan="1">Asia Pacific Tele-Ophthalmology Society</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BRSET</td><td align="left" valign="middle" rowspan="1" colspan="1">Brazilian Multilabel Ophthalmological Dataset</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DDR</td><td align="left" valign="middle" rowspan="1" colspan="1">Dataset for Diabetic Retinopathy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DiaRetDB</td><td align="left" valign="middle" rowspan="1" colspan="1">Diabetic Retinopathy DataBase</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DR HAGIS</td><td align="left" valign="middle" rowspan="1" colspan="1">Diabetic Retinopathy, Hypertension, Age-related macular degeneration<break/>and Glaucoma Images</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DRiDB</td><td align="left" valign="middle" rowspan="1" colspan="1">Diabetic Retinopathy image DataBase</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DRIVE</td><td align="left" valign="middle" rowspan="1" colspan="1">Digital Retinal Images for Vessel Extraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Eye PACS</td><td align="left" valign="middle" rowspan="1" colspan="1">Eye Picture Archive Communication System</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">F-DCVP</td><td align="left" valign="middle" rowspan="1" colspan="1">Fundus-Data Computer Vision Project</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HEI-MED</td><td align="left" valign="middle" rowspan="1" colspan="1">Hamilton Eye Institute Macular Edema Dataset</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HRF</td><td align="left" valign="middle" rowspan="1" colspan="1">High-Resolution Fundus Segmentation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IDRID</td><td align="left" valign="middle" rowspan="1" colspan="1">Indian Diabetic Retinopathy Image Dataset</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">JSIEC</td><td align="left" valign="middle" rowspan="1" colspan="1">Joint Shantou International Eye Center</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ROC</td><td align="left" valign="middle" rowspan="1" colspan="1">Online Challenge</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">STARE</td><td align="left" valign="middle" rowspan="1" colspan="1">Structured Analysis of the Retina</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05658"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>American Diabetes Association</collab></person-group><article-title>Diagnosis and classification of diabetes mellitus</article-title><source>Diabetes Care</source><year>2010</year><volume>33</volume><fpage>S62</fpage><lpage>S69</lpage><comment>Erratum in <italic toggle="yes">Diabetes Care</italic><bold>2010</bold>, <italic toggle="yes">33</italic>, e57.</comment><pub-id pub-id-type="doi">10.2337/dc10-S062</pub-id><pub-id pub-id-type="pmid">20042775</pub-id><pub-id pub-id-type="pmcid">PMC2797383</pub-id></element-citation></ref><ref id="B2-sensors-25-05658"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brownlee</surname><given-names>M.</given-names></name></person-group><article-title>Biochemistry and molecular cell biology of diabetic complications</article-title><source>Nature</source><year>2001</year><volume>414</volume><fpage>813</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1038/414813a</pub-id><pub-id pub-id-type="pmid">11742414</pub-id></element-citation></ref><ref id="B3-sensors-25-05658"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yau</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rogers</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kawasaki</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lamoureux</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kowalski</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bek</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dekker</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fletcher</surname><given-names>A.</given-names></name><name name-style="western"><surname>Grauslund</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Global prevalence and major risk factors of diabetic retinopathy</article-title><source>Diabetes Care</source><year>2012</year><volume>35</volume><fpage>556</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.2337/dc11-1909</pub-id><pub-id pub-id-type="pmid">22301125</pub-id><pub-id pub-id-type="pmcid">PMC3322721</pub-id></element-citation></ref><ref id="B4-sensors-25-05658"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ruta</surname><given-names>L.</given-names></name><name name-style="western"><surname>Magliano</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lemesurier</surname><given-names>R.</given-names></name><name name-style="western"><surname>Taylor</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zimmet</surname><given-names>P.</given-names></name><name name-style="western"><surname>Shaw</surname><given-names>J.</given-names></name></person-group><article-title>Prevalence of diabetic retinopathy in Type 2 diabetes in developing and developed countries</article-title><source>Diabet. Med.</source><year>2013</year><volume>30</volume><fpage>387</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1111/dme.12119</pub-id><pub-id pub-id-type="pmid">23331210</pub-id></element-citation></ref><ref id="B5-sensors-25-05658"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheung</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mitchell</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>T.Y.</given-names></name></person-group><article-title>Diabetic Retinopathy</article-title><source>Lancet</source><year>2010</year><volume>376</volume><fpage>124</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(09)62124-3</pub-id><pub-id pub-id-type="pmid">20580421</pub-id></element-citation></ref><ref id="B6-sensors-25-05658"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>Y.</given-names></name></person-group><article-title>Prediction and analysis of risk factors for diabetic retinopathy based on machine learning and interpretable models</article-title><source>Heliyon</source><year>2024</year><volume>10</volume><fpage>e29497</fpage><pub-id pub-id-type="doi">10.1016/j.heliyon.2024.e29497</pub-id><pub-id pub-id-type="pmid">38699007</pub-id><pub-id pub-id-type="pmcid">PMC11064081</pub-id></element-citation></ref><ref id="B7-sensors-25-05658"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Classification of diabetic retinopathy: Past, present and future</article-title><source>Front. Endocrinol.</source><year>2022</year><volume>13</volume><elocation-id>1079217</elocation-id><pub-id pub-id-type="doi">10.3389/fendo.2022.1079217</pub-id><pub-id pub-id-type="pmcid">PMC9800497</pub-id><pub-id pub-id-type="pmid">36589807</pub-id></element-citation></ref><ref id="B8-sensors-25-05658"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>American Academy of Ophthalmology</collab></person-group><source>Preferred Practice Pattern&#174;: Diabetic Retinopathy</source><publisher-name>American Academy of Ophthalmology</publisher-name><publisher-loc>San Francisco, CA, USA</publisher-loc><year>2022</year></element-citation></ref><ref id="B9-sensors-25-05658"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wong</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kawasaki</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ruamviboonsuk</surname><given-names>P.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>N.</given-names></name><name name-style="western"><surname>Lansingh</surname><given-names>V.C.</given-names></name><name name-style="western"><surname>Maia</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mathenge</surname><given-names>W.</given-names></name><name name-style="western"><surname>Moreker</surname><given-names>M.</given-names></name><name name-style="western"><surname>Muqit</surname><given-names>M.M.</given-names></name><etal/></person-group><article-title>Diabetic Retinopathy</article-title><source>Nat. Rev. Dis. Prim.</source><year>2016</year><volume>2</volume><fpage>16012</fpage><pub-id pub-id-type="doi">10.1038/nrdp.2016.12</pub-id><pub-id pub-id-type="pmid">27159554</pub-id></element-citation></ref><ref id="B10-sensors-25-05658"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Goldbaum</surname><given-names>M.</given-names></name></person-group><article-title>Structured Analysis of the Retina (STARE)</article-title><source>Master&#8217;s Thesis</source><publisher-name>University of California</publisher-name><publisher-loc>San Diego, CA, USA</publisher-loc><year>2003</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://cecas.clemson.edu/~ahoover/stare/" ext-link-type="uri">https://cecas.clemson.edu/~ahoover/stare/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-05-30">(accessed on 30 May 2024)</date-in-citation></element-citation></ref><ref id="B11-sensors-25-05658"><label>11.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Dugas</surname><given-names>E.</given-names></name><name name-style="western"><surname>Jorge</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cukierski</surname><given-names>W.</given-names></name></person-group><article-title>Diabetic Retinopathy Detection. Kaggle</article-title><year>2015</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://kaggle.com/competitions/diabetic-retinopathy-detection" ext-link-type="uri">https://kaggle.com/competitions/diabetic-retinopathy-detection</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-05-22">(accessed on 22 May 2025)</date-in-citation></element-citation></ref><ref id="B12-sensors-25-05658"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kunikata</surname><given-names>H.</given-names></name><name name-style="western"><surname>Aizawa</surname><given-names>N.</given-names></name><name name-style="western"><surname>Fuse</surname><given-names>N.</given-names></name><name name-style="western"><surname>Abe</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nakazawa</surname><given-names>T.</given-names></name></person-group><article-title>25-Gauge Microincision Vitrectomy to Treat Vitreoretinal Disease in Glaucomatous Eyes after Trabeculectomy</article-title><source>J. Ophthalmol.</source><year>2014</year><volume>2014</volume><fpage>306814</fpage><pub-id pub-id-type="doi">10.1155/2014/306814</pub-id><pub-id pub-id-type="pmid">24864192</pub-id><pub-id pub-id-type="pmcid">PMC4016913</pub-id></element-citation></ref><ref id="B13-sensors-25-05658"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Prathibha</surname><given-names>S.</given-names></name></person-group><article-title>Advancing diabetic retinopathy diagnosis with fundus imaging: A comprehensive survey of computer-aided detection, grading and classification methods</article-title><source>Glob. Transit.</source><year>2024</year><volume>6</volume><fpage>93</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.glt.2024.04.001</pub-id></element-citation></ref><ref id="B14-sensors-25-05658"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>L.</given-names></name></person-group><article-title>A benchmark for studying diabetic retinopathy: Segmentation, grading, and transferability</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>40</volume><fpage>818</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.3037771</pub-id><pub-id pub-id-type="pmid">33180722</pub-id></element-citation></ref><ref id="B15-sensors-25-05658"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Quellec</surname><given-names>G.</given-names></name><name name-style="western"><surname>Charriere</surname><given-names>K.</given-names></name><name name-style="western"><surname>Boudi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cochener</surname><given-names>B.</given-names></name><name name-style="western"><surname>Lamard</surname><given-names>M.</given-names></name></person-group><article-title>Deep image mining for diabetic retinopathy screening</article-title><source>Med. Image Anal.</source><year>2017</year><volume>39</volume><fpage>178</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.media.2017.04.012</pub-id><pub-id pub-id-type="pmid">28511066</pub-id></element-citation></ref><ref id="B16-sensors-25-05658"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Playout</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cheriet</surname><given-names>F.</given-names></name></person-group><article-title>Cross-dataset generalization for retinal lesions segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2405.08329</pub-id><pub-id pub-id-type="arxiv">2405.08329</pub-id></element-citation></ref><ref id="B17-sensors-25-05658"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fadugba</surname><given-names>J.</given-names></name><name name-style="western"><surname>K&#246;hler</surname><given-names>P.</given-names></name><name name-style="western"><surname>Koch</surname><given-names>L.</given-names></name><name name-style="western"><surname>Manescu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Berens</surname><given-names>P.</given-names></name></person-group><article-title>Benchmarking Retinal Blood Vessel Segmentation Models for Cross-Dataset and Cross-Disease Generalization</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2406.14994</pub-id></element-citation></ref><ref id="B18-sensors-25-05658"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Miao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>G.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>W.</given-names></name></person-group><article-title>Caudr: A causality-inspired domain generalization framework for fundus-based diabetic retinopathy grading</article-title><source>Comput. Biol. Med.</source><year>2024</year><volume>175</volume><elocation-id>108459</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108459</pub-id><pub-id pub-id-type="pmid">38701588</pub-id></element-citation></ref><ref id="B19-sensors-25-05658"><label>19.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Le Monde</collab></person-group><article-title>More than 1.3 Billion People Will Have Diabetes by 2050</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.lemonde.fr/en/environment/article/2023/06/24/more-than-1-3-billion-people-will-have-diabetes-by-2050_6036228_114.html" ext-link-type="uri">https://www.lemonde.fr/en/environment/article/2023/06/24/more-than-1-3-billion-people-will-have-diabetes-by-2050_6036228_114.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-03">(accessed on 3 July 2025)</date-in-citation></element-citation></ref><ref id="B20-sensors-25-05658"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gollogly</surname><given-names>H.E.</given-names></name><name name-style="western"><surname>Hodge</surname><given-names>D.O.</given-names></name><name name-style="western"><surname>St Sauver</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Erie</surname><given-names>J.C.</given-names></name></person-group><article-title>Increasing incidence of cataract surgery: Population-based study</article-title><source>J. Cataract. Refract. Surg.</source><year>2013</year><volume>39</volume><fpage>1383</fpage><lpage>1389</lpage><pub-id pub-id-type="doi">10.1016/j.jcrs.2013.03.027</pub-id><pub-id pub-id-type="pmid">23820302</pub-id><pub-id pub-id-type="pmcid">PMC4539250</pub-id></element-citation></ref><ref id="B21-sensors-25-05658"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abr&#224;moff</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Garvin</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Sonka</surname><given-names>M.</given-names></name></person-group><article-title>Retinal Imaging and Image Analysis</article-title><source>IEEE Rev. Biomed. Eng.</source><year>2010</year><volume>3</volume><fpage>169</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1109/RBME.2010.2084567</pub-id><pub-id pub-id-type="pmid">22275207</pub-id><pub-id pub-id-type="pmcid">PMC3131209</pub-id></element-citation></ref><ref id="B22-sensors-25-05658"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>International Agency for the Prevention of Blindness (IAPB)</collab></person-group><source>IAPB Technical Specifications</source><comment>Rapport Technique Version Finale 08/04/2021</comment><publisher-name>International Agency for the Prevention of Blindness</publisher-name><publisher-loc>London, UK</publisher-loc><year>2021</year></element-citation></ref><ref id="B23-sensors-25-05658"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ghasemi Falavarjani</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Khadamy</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sadda</surname><given-names>S.R.</given-names></name></person-group><article-title>Ultra-wide-field imaging in diabetic retinopathy; an overview</article-title><source>J. Curr. Ophthalmol.</source><year>2016</year><volume>28</volume><fpage>57</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.joco.2016.04.001</pub-id><pub-id pub-id-type="pmid">27331147</pub-id><pub-id pub-id-type="pmcid">PMC4909710</pub-id></element-citation></ref><ref id="B24-sensors-25-05658"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thomas</surname><given-names>R.L.</given-names></name></person-group><article-title>Delaying and preventing diabetic retinopathy</article-title><source>Pract. Diabetes</source><year>2021</year><volume>38</volume><fpage>31</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1002/pdi.2360</pub-id></element-citation></ref><ref id="B25-sensors-25-05658"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andrade-Miranda</surname><given-names>G.</given-names></name><name name-style="western"><surname>Jaouen</surname><given-names>V.</given-names></name><name name-style="western"><surname>Tankyevych</surname><given-names>O.</given-names></name><name name-style="western"><surname>Cheze Le Rest</surname><given-names>C.</given-names></name><name name-style="western"><surname>Visvikis</surname><given-names>D.</given-names></name><name name-style="western"><surname>Conze</surname><given-names>P.H.</given-names></name></person-group><article-title>Multi-modal medical Transformers: A meta-analysis for medical image segmentation in oncology</article-title><source>Comput. Med. Imaging Graph.</source><year>2023</year><volume>110</volume><fpage>102308</fpage><pub-id pub-id-type="doi">10.1016/j.compmedimag.2023.102308</pub-id><pub-id pub-id-type="pmid">37918328</pub-id></element-citation></ref><ref id="B26-sensors-25-05658"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andrade-Miranda</surname><given-names>G.</given-names></name><name name-style="western"><surname>Vega</surname><given-names>P.S.</given-names></name><name name-style="western"><surname>Taguelmimt</surname><given-names>K.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>H.P.</given-names></name><name name-style="western"><surname>Visvikis</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bert</surname><given-names>J.</given-names></name></person-group><article-title>Exploring transformer reliability in clinically significant prostate cancer segmentation: A comprehensive in-depth investigation</article-title><source>Comput. Med. Imaging Graph.</source><year>2024</year><volume>118</volume><fpage>102459</fpage><pub-id pub-id-type="doi">10.1016/j.compmedimag.2024.102459</pub-id><pub-id pub-id-type="pmid">39566375</pub-id></element-citation></ref><ref id="B27-sensors-25-05658"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tripathi</surname><given-names>R.</given-names></name></person-group><article-title>Automated Early Detection of Diabetic Retinopathy Using Image Analysis Techniques</article-title><source>Int. J. Comput. Appl.</source><year>2010</year><volume>8</volume><fpage>18</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.5120/1186-1648</pub-id></element-citation></ref><ref id="B28-sensors-25-05658"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Porwal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Pachade</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kamble</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kokare</surname><given-names>M.</given-names></name><name name-style="western"><surname>Deshmukh</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sahasrabuddhe</surname><given-names>V.</given-names></name><name name-style="western"><surname>Meriaudeau</surname><given-names>F.</given-names></name></person-group><article-title>Indian Diabetic Retinopathy Image Dataset (IDRiD)</article-title><source>IEEE Dataport</source><year>2018</year><pub-id pub-id-type="doi">10.21227/H25W98</pub-id></element-citation></ref><ref id="B29-sensors-25-05658"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wilkinson</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ferris</surname><given-names>F.L.</given-names></name><name name-style="western"><surname>Klein</surname><given-names>R.E.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>P.P.</given-names></name><name name-style="western"><surname>Agardh</surname><given-names>C.D.</given-names></name><name name-style="western"><surname>Davis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dills</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kampik</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pararajasegaram</surname><given-names>R.</given-names></name><name name-style="western"><surname>Verdaguer</surname><given-names>J.T.</given-names></name></person-group><article-title>Proposed international clinical diabetic retinopathy and diabetic macular edema disease severity scales</article-title><source>Ophthalmology</source><year>2003</year><volume>110</volume><fpage>1677</fpage><lpage>1682</lpage><pub-id pub-id-type="doi">10.1016/S0161-6420(03)00475-5</pub-id><pub-id pub-id-type="pmid">13129861</pub-id></element-citation></ref><ref id="B30-sensors-25-05658"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Derwin</surname><given-names>D.J.</given-names></name><name name-style="western"><surname>Selvi</surname><given-names>S.T.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>O.J.</given-names></name><name name-style="western"><surname>Shan</surname><given-names>B.P.</given-names></name></person-group><article-title>A novel automated system of discriminating Microaneurysms in fundus images</article-title><source>Biomed. Signal Process. Control</source><year>2020</year><volume>58</volume><elocation-id>101839</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2019.101839</pub-id></element-citation></ref><ref id="B31-sensors-25-05658"><label>31.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Karthik</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dane</surname><given-names>S.</given-names></name></person-group><article-title>APTOS 2019 Blindness Detection. Kaggle</article-title><year>2019</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://kaggle.com/competitions/aptos2019-blindness-detection" ext-link-type="uri">https://kaggle.com/competitions/aptos2019-blindness-detection</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-12">(accessed on 12 December 2024)</date-in-citation></element-citation></ref><ref id="B32-sensors-25-05658"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nakayama</surname><given-names>L.F.</given-names></name><name name-style="western"><surname>Goncalves</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zago Ribeiro</surname><given-names>L.</given-names></name><name name-style="western"><surname>Santos</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ferraz</surname><given-names>D.</given-names></name><name name-style="western"><surname>Malerbi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Celi</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Regatieri</surname><given-names>C.</given-names></name></person-group><source>A Brazilian Multilabel Ophthalmological Dataset (BRSET) (Version 1.0.1)</source><comment>RRID:SCR_007345</comment><publisher-name>PhysioNet</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2024</year><pub-id pub-id-type="doi">10.13026/1pht-2b69</pub-id><pub-id pub-id-type="pmcid">PMC11239107</pub-id><pub-id pub-id-type="pmid">38991014</pub-id></element-citation></ref><ref id="B33-sensors-25-05658"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fraz</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Remagnino</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hoppe</surname><given-names>A.</given-names></name><name name-style="western"><surname>Uyyanonvara</surname><given-names>B.</given-names></name><name name-style="western"><surname>Rudnicka</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Owen</surname><given-names>C.G.</given-names></name><name name-style="western"><surname>Barman</surname><given-names>S.A.</given-names></name></person-group><article-title>An Ensemble Classification-Based Approach Applied to Retinal Blood Vessel Segmentation</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2012</year><volume>59</volume><fpage>2538</fpage><lpage>2548</lpage><pub-id pub-id-type="doi">10.1109/TBME.2012.2205687</pub-id><pub-id pub-id-type="pmid">22736688</pub-id></element-citation></ref><ref id="B34-sensors-25-05658"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name></person-group><article-title>Diagnostic Assessment of Deep Learning Algorithms for Diabetic Retinopathy Screening</article-title><source>Inf. Sci.</source><year>2019</year><volume>501</volume><fpage>511</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2019.06.011</pub-id></element-citation></ref><ref id="B35-sensors-25-05658"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kauppi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kalesnykiene</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kamarainen</surname><given-names>J.K.</given-names></name><name name-style="western"><surname>Lensu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sorri</surname><given-names>I.</given-names></name><name name-style="western"><surname>Raninen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Voutilainen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pietil&#228;</surname><given-names>J.</given-names></name><name name-style="western"><surname>K&#228;lvi&#228;inen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Uusitalo</surname><given-names>H.</given-names></name></person-group><source>Diabetic Retinopathy Database&#8212;DIARETDB0</source><comment>Technical Report</comment><publisher-name>Department of Ophthalmology, Faculty of Medicine, University of Kuopio</publisher-name><publisher-loc>Kuopio, Finland</publisher-loc><year>2018</year></element-citation></ref><ref id="B36-sensors-25-05658"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kauppi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kalesnykiene</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kamarainen</surname><given-names>J.K.</given-names></name><name name-style="western"><surname>Lensu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sorri</surname><given-names>I.</given-names></name><name name-style="western"><surname>Raninen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Voutilainen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pietil&#228;</surname><given-names>J.</given-names></name><name name-style="western"><surname>K&#228;lvi&#228;inen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Uusitalo</surname><given-names>H.</given-names></name></person-group><source>Diabetic Retinopathy Database&#8212;DIARETDB1</source><comment>Technical Report</comment><publisher-name>Department of Ophthalmology, Faculty of Medicine, University of Kuopio</publisher-name><publisher-loc>Kuopio, Finland</publisher-loc><year>2018</year></element-citation></ref><ref id="B37-sensors-25-05658"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Holm</surname><given-names>S.</given-names></name><name name-style="western"><surname>Russell</surname><given-names>G.</given-names></name><name name-style="western"><surname>Nourrit</surname><given-names>V.</given-names></name><name name-style="western"><surname>McLoughlin</surname><given-names>N.</given-names></name></person-group><article-title>DR HAGIS&#8212;A fundus image database for the automatic extraction of retinal surface vessels from diabetic patients</article-title><source>J. Med. Imaging</source><year>2017</year><volume>4</volume><fpage>014503</fpage><pub-id pub-id-type="doi">10.1117/1.JMI.4.1.014503</pub-id><pub-id pub-id-type="pmcid">PMC5299858</pub-id><pub-id pub-id-type="pmid">28217714</pub-id></element-citation></ref><ref id="B38-sensors-25-05658"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Prenta&#353;i&#263;</surname><given-names>P.</given-names></name><name name-style="western"><surname>Lon&#269;ari&#263;</surname><given-names>S.</given-names></name><name name-style="western"><surname>Vatavuk</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ben&#269;i&#263;</surname><given-names>G.</given-names></name><name name-style="western"><surname>Suba&#353;i&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Petkovi&#263;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dujmovi&#263;</surname><given-names>L.</given-names></name><name name-style="western"><surname>Malenica-Ravli&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Budimlija</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tadi&#263;</surname><given-names>R.</given-names></name></person-group><article-title>Diabetic retinopathy image database (DRiDB): A new database for diabetic retinopathy screening programs research</article-title><source>Proceedings of the 2013 8th International Symposium on Image and Signal Processing and Analysis (ISPA)</source><conf-loc>Trieste, Italy</conf-loc><conf-date>4&#8211;6 September 2013</conf-date><fpage>711</fpage><lpage>716</lpage></element-citation></ref><ref id="B39-sensors-25-05658"><label>39.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Grand Challenge</collab></person-group><article-title>DRIVE: Digital Retinal Images for Vessel Extraction</article-title><year>2017</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://drive.grand-challenge.org/" ext-link-type="uri">https://drive.grand-challenge.org/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-05-30">(accessed on 30 May 2024)</date-in-citation></element-citation></ref><ref id="B40-sensors-25-05658"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Decenci&#232;re</surname><given-names>E.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cazuguel</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lay</surname><given-names>B.</given-names></name><name name-style="western"><surname>Cochener</surname><given-names>B.</given-names></name><name name-style="western"><surname>Trone</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gainville</surname><given-names>B.</given-names></name><name name-style="western"><surname>Deb-Joardar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Massin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Quellec</surname><given-names>G.</given-names></name><etal/></person-group><article-title>TeleOphta: Machine learning and image processing methods for teleophthalmology</article-title><source>IRBM</source><year>2013</year><volume>34</volume><fpage>196</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.irbm.2013.01.010</pub-id></element-citation></ref><ref id="B41-sensors-25-05658"><label>41.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>FundusData</collab></person-group><article-title>Fundus-Data Dataset</article-title><year>2022</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://universe.roboflow.com/fundusdata/fundus-data" ext-link-type="uri">https://universe.roboflow.com/fundusdata/fundus-data</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-02">(accessed on 2 June 2025)</date-in-citation></element-citation></ref><ref id="B42-sensors-25-05658"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Giancardo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Meriaudeau</surname><given-names>F.</given-names></name><name name-style="western"><surname>Karnowski</surname><given-names>T.P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Garg</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tobin</surname><given-names>K.W.</given-names><suffix>Jr.</suffix></name><name name-style="western"><surname>Chaum</surname><given-names>E.</given-names></name></person-group><article-title>Exudate-based diabetic macular edema detection in fundus images using publicly available datasets</article-title><source>Med. Image Anal.</source><year>2012</year><volume>16</volume><fpage>216</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.media.2011.07.004</pub-id><pub-id pub-id-type="pmid">21865074</pub-id><pub-id pub-id-type="pmcid">PMC10729314</pub-id></element-citation></ref><ref id="B43-sensors-25-05658"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Budai</surname><given-names>A.</given-names></name><name name-style="western"><surname>Odstrcilik</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kondermann</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hornegger</surname><given-names>J.</given-names></name><name name-style="western"><surname>Michelson</surname><given-names>G.</given-names></name></person-group><article-title>Robust Vessel Segmentation in Fundus Images</article-title><source>Int. J. Biomed. Imaging</source><year>2013</year><volume>2013</volume><elocation-id>154860</elocation-id><pub-id pub-id-type="doi">10.1155/2013/154860</pub-id><pub-id pub-id-type="pmid">24416040</pub-id><pub-id pub-id-type="pmcid">PMC3876700</pub-id></element-citation></ref><ref id="B44-sensors-25-05658"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cen</surname><given-names>L.P.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Ju</surname><given-names>S.T.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.F.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Automatic detection of 39 fundus diseases and conditions in retinal photographs using deep neural networks</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>4828</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-25138-w</pub-id><pub-id pub-id-type="pmid">34376678</pub-id><pub-id pub-id-type="pmcid">PMC8355164</pub-id></element-citation></ref><ref id="B45-sensors-25-05658"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Decenci&#232;re</surname><given-names>E.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cazuguel</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lay</surname><given-names>B.</given-names></name><name name-style="western"><surname>Cochener</surname><given-names>B.</given-names></name><name name-style="western"><surname>Trone</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gainville</surname><given-names>B.</given-names></name><name name-style="western"><surname>Massin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Deb-Joardar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Danno</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Feedback on a publicly distributed database: The Messidor database</article-title><source>Image Anal. Stereol.</source><year>2014</year><volume>33</volume><fpage>231</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.5566/ias.1155</pub-id></element-citation></ref><ref id="B46-sensors-25-05658"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abr&#224;moff</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Folk</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Han</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Walker</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Williams</surname><given-names>D.F.</given-names></name><name name-style="western"><surname>Russell</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Massin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cochener</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gain</surname><given-names>P.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>L.</given-names></name><etal/></person-group><article-title>Automated analysis of retinal images for detection of referable diabetic retinopathy</article-title><source>JAMA Ophthalmol.</source><year>2013</year><volume>131</volume><fpage>351</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1001/jamaophthalmol.2013.1743</pub-id><pub-id pub-id-type="pmid">23494039</pub-id></element-citation></ref><ref id="B47-sensors-25-05658"><label>47.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Jr2ngb</collab></person-group><article-title>Cataract Dataset</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/jr2ngb/cataractdataset" ext-link-type="uri">https://www.kaggle.com/datasets/jr2ngb/cataractdataset</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-05-30">(accessed on 30 May 2024)</date-in-citation></element-citation></ref><ref id="B48-sensors-25-05658"><label>48.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Retinopathy Online Challenge</collab></person-group><article-title>Retinopathy Online Challenge (ROC) Dataset</article-title><year>2008</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://roc.healthcare.uiowa.edu/" ext-link-type="uri">https://roc.healthcare.uiowa.edu/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-05-30">(accessed on 30 May 2024)</date-in-citation></element-citation></ref><ref id="B49-sensors-25-05658"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pourjavan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bourguignon</surname><given-names>G.-H.</given-names></name><name name-style="western"><surname>Marinescu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Otjacques</surname><given-names>L.</given-names></name><name name-style="western"><surname>Boschi</surname><given-names>A.</given-names></name></person-group><article-title>Evaluating the Influence of Clinical Data on Inter-Observer Variability in Fundus Photograph Assessment</article-title><source>Clin. Ophthalmol.</source><year>2024</year><volume>18</volume><fpage>3999</fpage><lpage>4009</lpage><pub-id pub-id-type="doi">10.2147/OPTH.S492872</pub-id><pub-id pub-id-type="pmid">39741794</pub-id><pub-id pub-id-type="pmcid">PMC11687089</pub-id></element-citation></ref><ref id="B50-sensors-25-05658"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Du</surname><given-names>K.</given-names></name><name name-style="western"><surname>Shah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bollepalli</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Ibrahim</surname><given-names>M.N.</given-names></name><name name-style="western"><surname>Gadari</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sutharahan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sahel</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chhablani</surname><given-names>J.</given-names></name><name name-style="western"><surname>Vupparaboina</surname><given-names>K.K.</given-names></name></person-group><article-title>Inter-rater reliability in labeling quality and pathological features of retinal OCT scans: A customized annotation software approach</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><elocation-id>e0314707</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0314707</pub-id><pub-id pub-id-type="pmid">39693322</pub-id><pub-id pub-id-type="pmcid">PMC11654994</pub-id></element-citation></ref><ref id="B51-sensors-25-05658"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srinivasan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Suresh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chendilnathan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Prakash V</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sivaprasad</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rajalakshmi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Anjana</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Kulothungan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Raman</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Inter-observer agreement in grading severity of diabetic retinopathy in ultra-wide-field fundus photographs</article-title><source>Eye</source><year>2023</year><volume>37</volume><fpage>1231</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.1038/s41433-022-02107-1</pub-id><pub-id pub-id-type="pmid">35595962</pub-id><pub-id pub-id-type="pmcid">PMC10102141</pub-id></element-citation></ref><ref id="B52-sensors-25-05658"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdulrahman</surname><given-names>H.</given-names></name><name name-style="western"><surname>Magnier</surname><given-names>B.</given-names></name><name name-style="western"><surname>Montesinos</surname><given-names>P.</given-names></name></person-group><article-title>From contours to ground truth: How to evaluate edge detectors by filtering</article-title><source>J. WSCG</source><year>2017</year><volume>25</volume><fpage>133</fpage><lpage>142</lpage></element-citation></ref><ref id="B53-sensors-25-05658"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teoh</surname><given-names>C.S.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>K.H.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>H.C.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>H.W.</given-names></name><name name-style="western"><surname>Yuen</surname><given-names>Y.S.</given-names></name><name name-style="western"><surname>Naing</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yogesan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Koh</surname><given-names>V.T.C.</given-names></name></person-group><article-title>Variability in Grading Diabetic Retinopathy Using Retinal Photography and Its Comparison with an Automated Deep Learning Diabetic Retinopathy Screening Software</article-title><source>Healthcare</source><year>2023</year><volume>11</volume><elocation-id>1697</elocation-id><pub-id pub-id-type="doi">10.3390/healthcare11121697</pub-id><pub-id pub-id-type="pmid">37372815</pub-id><pub-id pub-id-type="pmcid">PMC10298355</pub-id></element-citation></ref><ref id="B54-sensors-25-05658"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dumitrascu</surname><given-names>O.M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Woodruff</surname><given-names>B.K.</given-names></name><name name-style="western"><surname>Nikolova</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sobczak</surname><given-names>J.</given-names></name><name name-style="western"><surname>Youssef</surname><given-names>A.</given-names></name><name name-style="western"><surname>Saxena</surname><given-names>S.</given-names></name><name name-style="western"><surname>Andreev</surname><given-names>J.</given-names></name><name name-style="western"><surname>Caselli</surname><given-names>R.J.</given-names></name><etal/></person-group><article-title>Color Fundus Photography and Deep Learning Applications in Alzheimer Disease</article-title><source>Mayo Clin. Proc. Digit. Health</source><year>2024</year><volume>2</volume><fpage>548</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1016/j.mcpdig.2024.08.005</pub-id><pub-id pub-id-type="pmid">39748801</pub-id><pub-id pub-id-type="pmcid">PMC11695061</pub-id></element-citation></ref><ref id="B55-sensors-25-05658"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Comaniciu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ramesh</surname><given-names>V.</given-names></name><name name-style="western"><surname>Meer</surname><given-names>P.</given-names></name></person-group><article-title>Kernel-based object tracking</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2003</year><volume>25</volume><fpage>564</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2003.1195991</pub-id></element-citation></ref><ref id="B56-sensors-25-05658"><label>56.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Borg</surname><given-names>I.</given-names></name><name name-style="western"><surname>Groenen</surname><given-names>P.J.F.</given-names></name></person-group><source>Modern Multidimensional Scaling: Theory and Applications</source><publisher-name>Springer</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>1997</year></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05658-f002" orientation="portrait"><label>Figure 2</label><caption><p>Flowchart detailing the stages of diabetic retinopathy diagnosis from fundus image acquisition to severity classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g002.jpg"/></fig><fig position="float" id="sensors-25-05658-f003" orientation="portrait"><label>Figure 3</label><caption><p>Diagram detailing the five standardized classification levels of the International Clinical Diabetic Retinopathy disease severity scale with a list of corresponding clinical manifestations and risk progression. For a complete definition, refer to Wilkinson et al. [<xref rid="B29-sensors-25-05658" ref-type="bibr">29</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g003.jpg"/></fig><fig position="float" id="sensors-25-05658-f004" orientation="portrait"><label>Figure 4</label><caption><p>Timeline detailing the date of publication of each dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g004.jpg"/></fig><fig position="float" id="sensors-25-05658-f005" orientation="portrait"><label>Figure 5</label><caption><p>Scatter plot showing the total number of images in each database as a function of mean resolution in megapixels. The mean resolution is the mean of all image resolutions in the dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g005.jpg"/></fig><fig position="float" id="sensors-25-05658-f006" orientation="portrait"><label>Figure 6</label><caption><p>Distribution of patients with DR and without DR across databases. The proportion of patients without DR is compared to all the other fundus images in the dataset that were identified as diseased or not identified. For datasets with training and testing images, in particular, the proportion of patients identified without DR depends on all the images.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g006.jpg"/></fig><fig position="float" id="sensors-25-05658-f007" orientation="portrait"><label>Figure 7</label><caption><p>Proportion of each disease grade in the Messidor dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g007.jpg"/></fig><fig position="float" id="sensors-25-05658-f008" orientation="portrait"><label>Figure 8</label><caption><p>Proportion of each disease grade in <bold>APTOS</bold>, <bold>BRSET</bold>, <bold>DDR</bold>, <bold>IDRID</bold>, <bold>Eye PACS</bold>, and <bold>JSIEC</bold>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g008.jpg"/></fig><fig position="float" id="sensors-25-05658-f009" orientation="portrait"><label>Figure 9</label><caption><p>Images from the <bold>ROC</bold> dataset. The microaneurysms are surrounded with yellow. (<bold>a</bold>) Annotations for patient 23. (<bold>b</bold>) Annotations for patient 29.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g009.jpg"/></fig><fig position="float" id="sensors-25-05658-f010" orientation="portrait"><label>Figure 10</label><caption><p>Images from the <bold>IDRID</bold> dataset. Microaneurysms are outlined in gray, hard exudates in green, hemorrhages in blue, optic disc in white, and soft exudates in red. (<bold>a</bold>) Annotations for patient 1. (<bold>b</bold>) Annotations for patient 3.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g010.jpg"/></fig><fig position="float" id="sensors-25-05658-f011" orientation="portrait"><label>Figure 11</label><caption><p>Images of the <bold>E-ophtha</bold> dataset. Exudates are surrounded in green, and the microaneurysms are surrounded in blue. (<bold>a</bold>) Annotations of patient 404. (<bold>b</bold>) Annotations of patient 20305.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g011.jpg"/></fig><fig position="float" id="sensors-25-05658-f012" orientation="portrait"><label>Figure 12</label><caption><p>Images of the <bold>DiaRetDB1</bold> dataset with delimited areas corresponding to a threshold of 150 applied to the segmentation mask of each sign. The exudates are outlined in blue, and the microaneurysms are surrounded in yellow. (<bold>a</bold>) Annotations for patient 1. (<bold>b</bold>) Annotations for patient 2.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g012.jpg"/></fig><fig position="float" id="sensors-25-05658-f013" orientation="portrait"><label>Figure 13</label><caption><p>Fundus image from the <bold>DiaRetDB1</bold> dataset with segmentation masks showing annotations from several experts. (<bold>a</bold>) Fundus image of patient 1. (<bold>b</bold>) Hard exudates&#8217; segmentation mask. (<bold>c</bold>) Hemorrhages&#8217; segmentation mask. (<bold>d</bold>) Red small dots&#8217; segmentation mask.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g013.jpg"/></fig><fig position="float" id="sensors-25-05658-f014" orientation="portrait"><label>Figure 14</label><caption><p>Blot hemorrhages are surrounded in yellow, hard exudates in green, soft exudates in red, hemorrhages in blue, microaneurysm in grayish blue, the macula in magenta, and the optic disc in white. (<bold>a</bold>) Annotations by doctor 1. (<bold>b</bold>) Annotations by doctor 2. (<bold>c</bold>) Annotations by doctor 3. (<bold>d</bold>) Annotations by doctor 4. (<bold>e</bold>) Annotations by doctor 5. (<bold>f</bold>) The intersection over union (IoU) scores for selected structures are as follows: optic disc: 0.69, blot hemorrhages (BH): 0.00, hard exudates (HE): 0.02, hemorrhages (HEM): 0.08, microaneurysms (MA): 0.00027, macula: 0.15, soft exudates (SE): 0.00.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g014.jpg"/></fig><fig position="float" id="sensors-25-05658-f015" orientation="portrait"><label>Figure 15</label><caption><p>Synthesis of all figures from the <xref rid="app1-sensors-25-05658" ref-type="app">Supplementary Materials</xref>. Box plots aggregate results across all datasets, with outliers omitted due to their high frequency. Histograms represent the weighted average of all dataset-specific histograms, with weights proportional to the number of images per dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g015.jpg"/></fig><fig position="float" id="sensors-25-05658-f016" orientation="portrait"><label>Figure 16</label><caption><p>Colored matrices of the Manhattan distance (upper triangle) and the Bhattacharyya distance (lower triangle) between datasets for the blue, green, red, and gray histograms. Note that HRF Seg corresponds to HRF Segmentation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g016.jpg"/></fig><fig position="float" id="sensors-25-05658-f017" orientation="portrait"><label>Figure 17</label><caption><p>Two-dimensional visualization of dataset similarities (averaged RGB and gray channels, Bhattacharyya distance) using Multi-Dimensional Scaling (MDS).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g017.jpg"/></fig><fig position="float" id="sensors-25-05658-f018" orientation="portrait"><label>Figure 18</label><caption><p>Two-dimensional visualization of dataset similarities (averaged RGB and gray channels, Manhattan distance) using Multi-Dimensional Scaling (MDS).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g018.jpg"/></fig><fig position="float" id="sensors-25-05658-f019" orientation="portrait"><label>Figure 19</label><caption><p>Principal Component Analysis (PCA) of the 22 datasets. The analysis includes 16 components, each corresponding to the median value from the box plots of the different color channels.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g019.jpg"/></fig><fig position="float" id="sensors-25-05658-f020" orientation="portrait"><label>Figure 20</label><caption><p>Summary of the paper&#8217;s key findings on retinal fundus databases for diabetic retinopathy research; it does not include the color analysis section.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05658-g020.jpg"/></fig><table-wrap position="float" id="sensors-25-05658-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05658-t001_Table 1</object-id><label>Table 1</label><caption><p>Overview of the main publicly available retinal fundus image databases used in diabetic retinopathy research (DR), with publication years ranging from 2000 to 2022. The table summarizes key characteristics of each dataset, including the total number of images, image resolution, file format, geographic origin, year of publication, and availability of expert annotations. These datasets vary widely in size, image quality, and annotation detail, reflecting diverse acquisition settings and research purposes. If the dataset itself contains different resolutions, the range of resolutions is provided. Here, #Images represents the total number of images included in the dataset. All datasets were downloaded before 30 May 2025, except for BRSET, DDR and Messidor2, which were downloaded before 31 July 2025.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">#Images</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Resolution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Format</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Geographical Origin</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year of Publication</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DR and Other Diseases</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Annotations</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ieee-dataport.org/open-access/diabetic-retinopathy-fundus-image-datasetagar300" ext-link-type="uri">AGAR300</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 1020&#160;&#215;&#160;1225 to 3696&#160;&#215;&#160;2448</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i001.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">India</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/c/aptos2019-blindness-detection/data" ext-link-type="uri">APTOS</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5590</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 474&#160;&#215;&#160;358 to 4288&#160;&#215;&#160;2848</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i002.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">India</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://physionet.org/content/brazilian-ophthalmological/1.0.1/" ext-link-type="uri">BRSET</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16,266</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 951&#160;&#215;&#160;874 to 2420&#160;&#215;&#160;1880</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG, JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i003.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brazil</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/rashasarhanalharthi/chase-db1" ext-link-type="uri">CHASE DB1</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">999&#160;&#215;&#160;960</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG, PNG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i004.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UK</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2012</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/mariaherrerot/ddrdataset/data?select=DR_grading.csv" ext-link-type="uri">DDR</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12,524</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 512&#160;&#215;&#160;512 to 5184&#160;&#215;&#160;3456</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i005.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">China</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.researchgate.net/publication/326460391_Diabetic_Retinopathy_Database_-_DIARETDB0" ext-link-type="uri">DiaRetDB0</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">130</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1500&#160;&#215;&#160;1152</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i006.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Finland</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.researchgate.net/publication/326460426_Diabetic_Retinopathy_Database_-_DIARETDB1" ext-link-type="uri">DiaRetDB1</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1500&#160;&#215;&#160;1152</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i007.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Finland</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DR HAGIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 2816&#160;&#215;&#160;1880 to 4752&#160;&#215;&#160;3168</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG, PNG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i008.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UK</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/openmedlab/Awesome-Medical-Dataset/blob/main/resources/DRiDB.md" ext-link-type="uri">DRiDB</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">720&#160;&#215;&#160;576</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BMP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i009.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Croatia</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2013</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://drive.grand-challenge.org/" ext-link-type="uri">DRIVE</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">768&#160;&#215;&#160;584</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i010.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Netherlands</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2013</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.adcis.net/en/third-party/e-ophtha/" ext-link-type="uri">E-ophtha</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">463</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 1440&#160;&#215;&#160;960 to 2544&#160;&#215;&#160;1696</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i011.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">France</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2013</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/c/diabetic-retinopathy-detection/data" ext-link-type="uri">Eye PACS</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88,702</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">640&#160;&#215;&#160;640</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i012.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">USA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2015</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://universe.roboflow.com/fundusdata/fundus-data" ext-link-type="uri">F-DCVP</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">181</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">640&#160;&#215;&#160;640</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">?</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">?</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/lgiancaUTH/HEI-MED" ext-link-type="uri">HEI-MED</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">111</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4752&#160;&#215;&#160;3168</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i013.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">USA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www5.cs.fau.de/research/data/fundus-images/" ext-link-type="uri">HRF Segmentation</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3504&#160;&#215;&#160;2336</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i014.jpg"/>
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i015.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Germany and Czech Republic</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2010</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://idrid.grand-challenge.org/Rules/" ext-link-type="uri">IDRID</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">516</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4288&#160;&#215;&#160;2848</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i016.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">India</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/linchundan/fundusimage1000" ext-link-type="uri">JSIEC</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 768&#160;&#215;&#160;576 to 3152&#160;&#215;&#160;3000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i017.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">China</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.adcis.net/en/third-party/messidor/" ext-link-type="uri">Messidor</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1187</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2240&#160;&#215;&#160;1488</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TIFF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i018.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">France</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2004</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.adcis.net/en/third-party/messidor2/" ext-link-type="uri">Messidor2</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1748</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2240&#160;&#215;&#160;1488</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TIFF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i019.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">France</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2014</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/yiweichen04/retina_dataset" ext-link-type="uri">Retina</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">601</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 1848&#160;&#215;&#160;1224 to 2592&#160;&#215;&#160;1728</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i020.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">South Korea</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://webeye.ophth.uiowa.edu/ROC/" ext-link-type="uri">ROC</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">from 768&#160;&#215;&#160;576 to 1394&#160;&#215;&#160;1391</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JPEG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i021.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Netherlands</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2007</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://cecas.clemson.edu/~ahoover/stare/" ext-link-type="uri">STARE</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">397</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">700&#160;&#215;&#160;605</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PPM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" content-type="width:30px" xlink:href="sensors-25-05658-i022.jpg"/>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">USA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">before 2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05658-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05658-t002_Table 2</object-id><label>Table 2</label><caption><p>Summary of diabetic retinopathy database attributes, including disease classification labels, vascular structures, lesion types, and annotation details.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Name</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Labels</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Blood Vessels</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Lesions</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Annotation Format</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Exudates
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Microaneurysms
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Hemorrhages
</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ieee-dataport.org/open-access/diabetic-retinopathy-fundus-image-datasetagar300" ext-link-type="uri">AGAR300</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: grey">None</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/c/aptos2019-blindness-detection/data" ext-link-type="uri">APTOS</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: grey">None</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://physionet.org/content/brazilian-ophthalmological/1.0.1/" ext-link-type="uri">BRSET</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: grey">None</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/rashasarhanalharthi/chase-db1" ext-link-type="uri">CHASE DB1</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/mariaherrerot/ddrdataset/data?select=DR_grading.csv" ext-link-type="uri">DDR</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: grey">None</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.researchgate.net/publication/326460391_Diabetic_Retinopathy_Database_-_DIARETDB0" ext-link-type="uri">DiaRetDB0</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.researchgate.net/publication/326460426_Diabetic_Retinopathy_Database_-_DIARETDB1" ext-link-type="uri">DiaRetDB1</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DR HAGIS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/openmedlab/Awesome-Medical-Dataset/blob/main/resources/DRiDB.md" ext-link-type="uri">DRiDB</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BMP</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://drive.grand-challenge.org/" ext-link-type="uri">DRIVE</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GIF</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.adcis.net/en/third-party/e-ophtha/" ext-link-type="uri">E-ophtha</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PNG</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/c/diabetic-retinopathy-detection/data" ext-link-type="uri">Eye PACS</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: grey">None</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/lgiancaUTH/HEI-MED" ext-link-type="uri">HEI-MED</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Matlab files</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www5.cs.fau.de/research/data/fundus-images/" ext-link-type="uri">HRF Segmentation</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TIFF</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://idrid.grand-challenge.org/Rules/" ext-link-type="uri">IDRID</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TIFF</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/linchundan/fundusimage1000" ext-link-type="uri">JSIEC</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: grey">None</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.adcis.net/en/third-party/messidor/" ext-link-type="uri">Messidor</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: grey">None</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://webeye.ophth.uiowa.edu/ROC/" ext-link-type="uri">ROC</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">XML</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://cecas.clemson.edu/~ahoover/stare/" ext-link-type="uri">STARE</ext-link>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TXT</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>