<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12474938</article-id><article-id pub-id-type="pmcid-ver">PMC12474938.1</article-id><article-id pub-id-type="pmcaid">12474938</article-id><article-id pub-id-type="pmcaiid">12474938</article-id><article-id pub-id-type="pmid">41006343</article-id><article-id pub-id-type="doi">10.1038/s41598-025-13956-7</article-id><article-id pub-id-type="publisher-id">13956</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Optimized YOLO based model for photovoltaic defect detection in electroluminescence images</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Mohamed</surname><given-names initials="A">Achit</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Nacera</surname><given-names initials="Y">Yassa</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ahcene</surname><given-names initials="B">Bouzida</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Teta</surname><given-names initials="A">Ali</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Belabbaci</surname><given-names initials="EO">El Ouanas</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Rabehi</surname><given-names initials="A">Abdelaziz</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Alsabah</surname><given-names initials="YA">Yousef A.</given-names></name><address><email>yousef.alsabah@hau.edu.ye</email></address><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Benghanem</surname><given-names initials="M">Mohamed</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00vsxg936</institution-id><institution-id institution-id-type="GRID">grid.442412.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 4655 050X</institution-id><institution>Laboratoire des Mat&#233;riaux et D&#233;veloppement Durable (LMDD), </institution><institution>University of Bouira, </institution></institution-wrap>Bouira, 10000 Algeria </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/000jvv118</institution-id><institution-id institution-id-type="GRID">grid.442431.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0486 7808</institution-id><institution>Applied Automation and Industrial Diagnostics Laboratory (LAADI), </institution><institution>University of Djelfa, </institution></institution-wrap>Djelfa, Algeria </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03yb2hp88</institution-id><institution-id institution-id-type="GRID">grid.442401.7</institution-id><institution-id institution-id-type="ISNI">0000 0001 0690 7656</institution-id><institution>Laboratory of Medical Informatics and Intelligent and Dynamic Environments (LIMED), </institution><institution>University of Bejaia, </institution></institution-wrap>Bejaia, 06000 Algeria </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/000jvv118</institution-id><institution-id institution-id-type="GRID">grid.442431.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0486 7808</institution-id><institution>Laboratory of Telecommunications and Smart Systems, Faculty of Sciences and Technologies, </institution><institution>University of Djelfa, </institution></institution-wrap>PO Box 3117, Djelfa, 17000 Algeria </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01rpcwa78</institution-id><institution-id institution-id-type="ISNI">0000 0004 9226 1039</institution-id><institution>Physics Department, College of Education and Applied Science, </institution><institution>Hajja University, </institution></institution-wrap>Hajja, Yemen </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03rcp1y74</institution-id><institution-id institution-id-type="GRID">grid.443662.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0417 5975</institution-id><institution>Physics Department, </institution><institution>Islamic University of Madinah, </institution></institution-wrap>Madinah, 42351 Saudi Arabia </aff></contrib-group><pub-date pub-type="epub"><day>26</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>32955</elocation-id><history><date date-type="received"><day>10</day><month>5</month><year>2025</year></date><date date-type="accepted"><day>28</day><month>7</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>28</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 15:25:44.047"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="41598_2025_Article_13956.pdf"/><abstract id="Abs1"><p id="Par1">Ensuring the reliability of photovoltaic (PV) systems requires efficient defect detection to maintain optimal energy production. Deep learning-based object detection models have demonstrated remarkable performance in automating this process. In this study, PV-YOLOv12n is introduced as an optimized variant of YOLOv12n, tailored for defect detection in electroluminescence (EL) images of PV panels. The modifications incorporate an A2C2f module at the P5 scale (1024, True), which enhances feature extraction by prioritizing critical defect regions. This improvement significantly boosts recall and precision for detecting large cracks, significant dislocations, and widespread material inconsistencies. Experimental results on the PVEL-AD and Roboflow datasets demonstrate superior detection performance. PV-YOLOv12n achieves a mAP@50 of 0.91 on both datasets, surpassing the baseline YOLOv12n (mAP@50 of 0.90 and 0.88 for PVEL-AD and Roboflow, respectively). Additionally, mAP@50-95 increases to 0.58 on PVEL-AD and 0.75 on Roboflow, highlighting improved generalization. Despite these improvements, inference speed remains efficient at 4.24 ms for PVEL-AD and 4.43 ms for Roboflow, ensuring suitability for real-time applications. These results validate the effectiveness of PV-YOLOv12n in detecting critical PV panel defects, supporting its deployment in large-scale solar farm inspections.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Photovoltaic defect detection</kwd><kwd>Electroluminescence imaging</kwd><kwd>Deep learning</kwd><kwd>YOLOv12n</kwd><kwd>Feature extraction</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Thermophotovoltaics</kwd><kwd>Photovoltaics</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The widespread integration of photovoltaic (PV) systems as a cornerstone of renewable energy infrastructure has underscored the critical importance of their systematic maintenance to ensure optimal performance and longevity<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. PV panels, continuously exposed to harsh environmental elements, are susceptible to a spectrum of defects, including but not limited to soiling from dust and debris<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, physical damage such as cracks and micro-cracks<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, hotspots, delamination, and bird droppings, all of which can significantly curtail energy conversion efficiency and operational lifespan<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Failure to promptly identify and mitigate these issues can escalate into substantial long-term degradation, diminished power output, and inflated operational expenditures. While traditional manual inspection has long been the standard, its inherent labor intensity, time consumption, and potential for human error render it increasingly impractical and inefficient, especially for expansive utility-scale solar farms<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. This paradigm necessitates a shift towards automated defect detection systems capable of delivering scalable, rapid, and reliable assessments, a challenge that captivates researchers in renewable energy, computer vision, and artificial intelligence<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Automated PV defect detection, primarily relying on the analysis of visual or thermal imagery, presents a complex computer vision task. The visual data captured from PV panels is rich with information, but its effective interpretation is fraught with persistent challenges. These can be broadly categorized into: unique challenges specific to PV inspection, such as the diverse morphology and often subtle visual signatures of different defect types, variations across different panel technologies and aging characteristics, and the dynamic nature of environmental conditions during data acquisition<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>. Concurrently, common imaging challenges further complicate detection, encompassing variations in image resolution, inconsistent illumination, sensor noise, and potential occlusions from structural elements or vegetation<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. Addressing these multifaceted challenges is paramount, as efficient defect detection profoundly impacts the economic viability and reliability of solar energy generation, enabling timely preventative maintenance, optimizing repair schedules, and averting premature system failures. Early attempts at automated PV defect detection predominantly utilized conventional image processing techniques and classical machine learning algorithms<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. While these methods offered initial improvements over purely manual inspection, they often struggled with the high variability and complexity inherent in real-world PV imagery, lacking the robustness required for diverse operational scenarios. The advent of deep learning (DL)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, particularly Convolutional Neural Networks (CNNs), has revolutionized the field of computer vision (CV)<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> and offered powerful new paradigms for object detection. Consequently, numerous DL-based models have been developed and applied to identify and localize PV panel defects with significantly enhanced accuracy and robustness compared to their predecessors<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. These artificial intelligence (AI) powered solutions offer substantial advantages, including superior consistency in defect identification, accelerated processing capabilities for large datasets, and the aptitude to learn complex defect patterns directly from data. Among the diverse array of deep learning architectures for object detection, the You Only Look Once (YOLO) family of models<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> has garnered considerable acclaim for its exceptional balance of speed and accuracy, rendering it highly suitable for real-time or near real-time applications<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. This characteristic is particularly advantageous for scenarios such as drone-based thermographic or visual inspections of large-scale solar installations, where rapid data processing and immediate feedback are crucial for operational efficiency<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Despite these significant advancements in automated inspection technologies, accurately and reliably detecting the full spectrum of PV defects remains a complex problem due to the subtle, varied, and often context-dependent nature of defect manifestations. Existing deep learning methods, including stanvariants, still face limitations, particularly in handling challenging scenarios such as the detection of very small or low-contrast defects, achieving robust performance under fluctuating environmental and imaging conditions, and effectively distinguishing between multiple, sometimes visually similar, defect classes with consistently high precision<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. Furthermore, the demand for lightweight yet powerful models that can be deployed on resource-constrained platforms without a substantial compromise in detection accuracy presents an ongoing research challenge. The motivation for this work arises from the pressing need to overcome these persistent limitations and to further improve the accuracy, robustness, and practical applicability of automated PV defect detection systems. Our aim is to develop an enhanced YOLO-based architecture that leverages targeted optimizations to significantly boost detection performance, especially for difficult-to-detect defects and under variable conditions, while maintaining computational efficiency suitable for real-world deployment. Accordingly, in this study, we propose an enhanced YOLO-based deep learning model, named PV-YOLOv12n, specifically optimized for detecting defects in PV panels. Our key contributions are as follows: <list list-type="order"><list-item><p id="Par3">The YOLOv12n architecture is improved by modifying the feature extraction mechanism to enhance defect detection accuracy while maintaining real-time performance.</p></list-item><list-item><p id="Par4">Well established Roboflow and PVELAD datasets are used to validate the proposed detection model to ensure robustness across diverse PV defect scenarios.</p></list-item><list-item><p id="Par5">A comparative analysis is conducted against state-of-the-art YOLO models, including YOLOv8, YOLOv9, YOLOv10, YOLOv11, and the standard YOLOv12, to validate the effectiveness of our proposed approach.</p></list-item><list-item><p id="Par6">The model&#8217;s inference speed and computational efficiency is assessed, to highlight its suitability for real-world deployment in automated solar farm inspections.</p></list-item></list>The remainder of this paper is structured as follows: Section <xref rid="Sec2" ref-type="sec">2</xref> reviews related work on PV defect detection using deep learning. Section <xref rid="Sec3" ref-type="sec">3</xref> describes our proposed PV-YOLOv12 model, highlighting its architectural enhancements and training methodology. Section <xref rid="Sec10" ref-type="sec">4</xref> presents the experimental setup and results, comparing our approach with existing YOLO models. Finally, Section <xref rid="Sec26" ref-type="sec">5</xref> concludes the paper and outlines directions for future research.</p></sec><sec id="Sec2"><title>Related works</title><p id="Par7">This section reviews seminal and contemporary research in Photovoltaic (PV) defect detection, with a particular focus on the evolution and application of You Only Look Once (YOLO) based methodologies. The aim is to contextualize the contributions of the present study by highlighting existing advancements, identifying prevailing challenges, and underscoring the trajectory of innovation in this critical domain.</p><p id="Par8">Imenes et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> investigated the efficacy of thermal imaging versus multiwavelength composite image processing for enhancing PV module fault detection and classification. They selected YOLOv3 for its optimal balance between computational efficiency and detection prowess. By employing Convolutional Neural Networks (CNNs), thermal and visible color images were combined to generate composite images, achieving a mean Average Precision (mAP) of 0.75. While this method effectively identified faults, it demonstrated limitations in accurately classifying specific defect types. This composite imaging approach had previously shown utility in applications such as shadow detection<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Zou et al.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> presented a 5G-enabled drone system equipped with a thermal imaging camera, leveraging AI-based detection via Python, OpenCV, and Darknet YOLOv4. The drone, fitted with a FLIR DUO PRO R thermal camera on a gimbal, captured real-time images transmitted to a ground station. Analysis of 1000 thermal images, of which 641 depicted solar cell defects, yielded a remarkable mAP of 1.0 (reported as 100%) at an 89% confidence level. This work highlighted a highly efficient, cost-effective, and reliable method for bolstering solar power plant performance. To advance the speed and precision of Electroluminescence (EL) image-based PV defect detection, Meng et al.<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> introduced YOLO-PV, an optimized model derived from YOLOv4. Its backbone was fine-tuned for enhanced low-level defect extraction, and a Spatial Pyramid Attention Network (SPAN) module in the neck facilitated improved feature fusion. YOLO-PV achieved an AP of 91.34% and an inference speed exceeding 35 Frames Per Second (FPS), outperforming CSP-PV by 0.64% and reducing processing time by 36.36% compared to the standard YOLOv4. Data augmentation techniques, including random rotation, mosaic, and exposure adjustment, further elevated accuracy to 94.55%. However, a noted limitation was the uncertain correlation between EL-detected defects and actual PV module performance, underscoring the need for future quantitative analysis. Li et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> acknowledged that while computer vision accelerates PV defect detection, challenges persist due to small defect sizes and high inter-class similarity among defect types. To address these issues, they developed GBH-YOLOv5, integrating Ghost Convolution with BottleneckCSP and an additional prediction head. The Ghost Convolution module streamlined computations, while Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) structures aided feature classification. Using the PV-Multi-Defect dataset (1108 images, 4235 defects across five categories), GBH-YOLOv5 achieved an mAP of 97.8%, significantly outperforming Fast R-CNN<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> by 27.8%. However, the incorporation of these modules increased model complexity. To mitigate this, images were processed in grayscale, a decision that, while simplifying the model, could potentially introduce detection inaccuracies. Future work was suggested to focus on lightweight models for real-time applications and RGB image integration. Hong et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> proposed a Deep Learning (DL) framework employing YOLOv5 and ResNet for PV fault detection, integrating both visible and infrared imagery. A dual infrared camera, flown at low altitudes, captured images under controlled conditions. Their dataset, comprising 3000 images from a solar plant in Hainan, China, was partitioned into training (66%), testing (22%), and validation (11%) sets. The model achieved 95% accuracy, surpassing VGG (93%), with a segmentation speed of 36 FPS, demonstrating its effectiveness in reducing maintenance costs. Zhang et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> presented an enhanced YOLOv5 model tailored for solar cell defect detection, specifically addressing challenges like complex backgrounds, diverse defect morphologies, and scale variations. They replaced traditional convolutions with deformable convolutions for improved feature extraction. The introduction of the ECA-Net attention mechanism and a dedicated small defect prediction head further augmented performance. Techniques such as Mosaic, MixUp, and K-means++ for anchor box clustering accelerated convergence. The improved model reached an mAP of 89.64%, a 7.85% increase over the original YOLOv5, with an inference speed of 36.24 FPS. Zheng<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> developed S-YOLOv5, a lightweight iteration of YOLOv5 designed to balance accuracy and speed. This model utilized adaptive scaling and normalization for superior feature extraction and fusion in the neck and prediction stages. An aerial infrared dataset, captured via a UAV-mounted thermal camera, was pre-processed using adaptive scaling. S-YOLOv5 achieved an impressive mAP of 98.1% with a detection speed of 49 FPS, demonstrating superior efficiency and computational performance compared to several existing models. Hassan et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> developed an automated defect detection system for PV panels using a custom CNN architecture, featuring varying convolutional layers and pooling strategies. Their model achieved a notable validation accuracy of 98.07%, significantly reducing reliance on manual inspections while enhancing quality control and cost-efficiency. In a case study on PV installations affected by Potential-Induced Degradation (PID), the system successfully identified all faulty modules with a precision rate of 96.6%. Further advancing the YOLO lineage, Cao et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> enhanced YOLOv8 by introducing YOLOv8-GD. This model incorporates Depthwise Convolution (DW-Conv) into its backbone and replaces standard convolutions with Group-Shuffle Convolution (GSConv). Additionally, a Bidirectional Feature Pyramid Network (BiFPN) was integrated to refine feature extraction. Experimental results demonstrated that YOLOv8-GD achieved mAP@0.5 and mAP@0.5&#8211;0.95 scores of 92.8% and 63.1%, respectively improvements of 4.2% and 5.7% over the baseline YOLOv8 while also reducing model size by 16.7%. Recent explorations by Ghahremani et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> employed the latest YOLO architectures (v9, v10, and v11) for detecting defects in solar panels using both thermal and optical images. Their study highlighted YOLOv11-X as the top performer, achieving precision, recall, mAP, and F1 scores of 89.7%, 87.7%, 92.7%, and 90%, respectively. This model notably outperformed traditional methods like SVM and Faster R-CNN, underscoring its efficiency for real-time defect detection. Wang et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> proposed MRA-YOLOv8, an enhanced model incorporating a Multi-branch Coordinate Attention Network (MBCANet) and a ResBlock module to bolster feature extraction capabilities. This model achieved a mAP50 of 91.7% on the PVEL-AD dataset, surpassing the standard YOLOv8 by 3.1% and DETR by 16.1%. Their study also investigated synthetic data generation using Generative Adversarial Networks (GANs) to address data imbalance, further enhancing detection accuracy. Similarly, Xu et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> introduced an upgraded YOLOv5 model for detecting solar cell defects in photoluminescence (PL) images, achieving a 91.5% mAP and an inference speed of 24.2 FPS. Their approach integrated data augmentation techniques (Mosaic, Mixup, HSV transformation), a C2f module for enhanced feature fusion, and an SPPF module with soft pooling to minimize redundancy. The model outperformed contemporary architectures like YOLOv7, YOLOv9, and Faster R-CNN, demonstrating robustness across varying lighting conditions and suitability for industrial application.</p><p id="Par9">While the YOLO series is prominent, research has also extensively explored other mainstream architectures, particularly two-stage detectors like Faster R-CNN, which are often favored when high precision is paramount. Chen et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> demonstrated the power of this approach by developing an improved anomaly detection method based on Faster R-CNN for EL images. They enhanced the model by integrating a lightweight channel and spatial attention module (CBAM) to better analyze complex crack defects and utilized a DIoU loss function to improve bounding box regression. Their optimized Faster R-CNN model achieved an impressive mAP of 87.14%, outperforming standard YOLOv5 and SSD implementations in their comparative analysis and highlighting the strong potential of well-tuned two-stage detectors in this domain.</p><p id="Par10">To further delineate the performance boundaries of various architectures, comprehensive benchmark studies are invaluable. Akram et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> conducted such an exploration, evaluating multiple state-of-the-art object detectors for multi-defect detection in EL images. Their study systematically compared different variants of YOLO (v7, v8, v9) against mainstream non-YOLO models, including Faster R-CNN, EfficientDet, and the Detection Transformer (DETR). While their proposed optimized YOLOv9 model achieved the highest performance (94.3% mAP@0.5), their results also showed that architectures like EfficientDet and DETR were highly competitive, outperforming a baseline Faster R-CNN. This work effectively illustrates the performance landscape, confirming the high efficiency of the latest YOLO models while also validating the viability and strength of other modern architectures for PV defect analysis.</p><p id="Par11">The literature reveals a dynamic field where both single-stage detectors like YOLO and two-stage detectors like Faster R-CNN are actively being developed. The YOLO series offers a compelling combination of high speed and accuracy, making it ideal for real-time applications. However, dedicated research on two-stage architectures demonstrates their capacity for high-precision detection, particularly when enhanced with modern techniques like attention mechanisms. A key trend is the ongoing enhancement of all architectures to address challenges such as detecting small or complex defects, improving feature extraction across imaging modalities, and optimizing performance for resource-limited platforms. Despite progress, issues like distinguishing similar defect types, ensuring generalization across conditions, and achieving lightweight, accurate models remain, driving continued research including this study (see Table&#160;<xref rid="Tab1" ref-type="table">1</xref>).<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>State-of-the-art of fault detection techniques based on YOLO architecture.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Study</bold></th><th align="left" colspan="1" rowspan="1"><bold>Model</bold></th><th align="left" colspan="1" rowspan="1"><bold>Key Enhancements</bold></th><th align="left" colspan="1" rowspan="1"><bold>Dataset</bold></th><th align="left" colspan="1" rowspan="1"><bold>mAP/Accuracy</bold></th><th align="left" colspan="1" rowspan="1"><bold>Speed (FPS)</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Li et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></td><td align="left" colspan="1" rowspan="1">GBH-YOLOV5</td><td align="left" colspan="1" rowspan="1">- GhostConv - BottleneckCSP - Extra head</td><td align="left" colspan="1" rowspan="1">PV-Multi-Defect (1108 images)</td><td align="left" colspan="1" rowspan="1">97.8% mAP</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Hong et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td align="left" colspan="1" rowspan="1">YOLOV5 and ResNet</td><td align="left" colspan="1" rowspan="1">- Infrared and visible fusion</td><td align="left" colspan="1" rowspan="1">3000 images (Hainan solar plant)</td><td align="left" colspan="1" rowspan="1">95% accuracy</td><td align="left" colspan="1" rowspan="1">36 FPS</td></tr><tr><td align="left" colspan="1" rowspan="1">Zhang et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td align="left" colspan="1" rowspan="1">Enhanced YOLOV5</td><td align="left" colspan="1" rowspan="1">- Deformable Conv, - ECA-Net - Small defect head</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">89.64% mAP</td><td align="left" colspan="1" rowspan="1">36.24 FPS</td></tr><tr><td align="left" colspan="1" rowspan="1">Zheng<sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td align="left" colspan="1" rowspan="1">S-YOLOV5</td><td align="left" colspan="1" rowspan="1">- Lightweight - adaptive scaling and normalization</td><td align="left" colspan="1" rowspan="1">UAV thermal IR dataset</td><td align="left" colspan="1" rowspan="1">98.1% mAP</td><td align="left" colspan="1" rowspan="1">49 FPS</td></tr><tr><td align="left" colspan="1" rowspan="1">Hassan et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></td><td align="left" colspan="1" rowspan="1">Custom CNN</td><td align="left" colspan="1" rowspan="1">Varying conv/pool layers</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">98.07% accuracy</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Cao et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup></td><td align="left" colspan="1" rowspan="1">YOLOV8-GD</td><td align="left" colspan="1" rowspan="1">- DW-Conv - GSConv - BiFPN</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">92.8% mAP@0.5, 63.1% mAP@0.5-0.95</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Ghahremani et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td align="left" colspan="1" rowspan="1">YOLOV11-X</td><td align="left" colspan="1" rowspan="1">- Latest YOLO versions, - multi-modal input</td><td align="left" colspan="1" rowspan="1">Thermal and optical images</td><td align="left" colspan="1" rowspan="1">92.7% mAP</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Wang et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td align="left" colspan="1" rowspan="1">MRA-YOLOV8</td><td align="left" colspan="1" rowspan="1">- MBCANet - ResBlock - GANs</td><td align="left" colspan="1" rowspan="1">PVEL-AD</td><td align="left" colspan="1" rowspan="1">91.7% mAP50</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Xu et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td align="left" colspan="1" rowspan="1">Upgraded YOLOV5</td><td align="left" colspan="1" rowspan="1">- C2f - SPPF w/soft pooling augmentation</td><td align="left" colspan="1" rowspan="1">PL images</td><td align="left" colspan="1" rowspan="1">91.5% mAP</td><td align="left" colspan="1" rowspan="1">24.2 FPS</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec3"><title>Methods</title><p id="Par12">This study builds upon the foundational YOLOv12<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> architecture by introducing a targeted enhancement to improve object detection performance, particularly in the domain of photovoltaic electroluminescence imaging. The detection of PV defects presents unique challenges, including the subtle nature of some defect classes, high intra-class variance, and the presence of complex background textures. To address these challenges, the YOLOv12 architecture has been modified not by introducing a new type of block, but by strategically altering the composition of its feature fusion neck. Specifically, the standard C3k2 convolutional block at layer 20 was replaced with an A2C2f attention-based block. This decision was motivated by the hypothesis that an attention mechanism at this critical feature fusion stage where features from different scales are combined would enable the model to better focus on salient defect regions. This attention centric adjustment is designed to improve the model&#8217;s feature representation capabilities, leading to more accurate and robust defect detection.</p><sec id="Sec4"><title>Overview of the YOLOv12 architecture</title><p id="Par13">YOLOv12 represents a state-of-the-art evolution in real-time object detection, integrating a highly efficient backbone, feature aggregation modules, and an optimized detection head. The architecture consists of three main components:</p><sec id="Sec5"><title>Advanced convolutional blocks</title><p id="Par14">YOLOv12 introduces a novel class of convolutional blocks designed to prioritize lightweight operations and enhanced parallelism, setting it apart from earlier versions. These blocks employ a sequence of smaller convolutional kernels, generally formulated as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e640">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F_{\text {out}} = \sum _{i=1}^{n} W_i * F_{\text {in}} + b_i \end{aligned}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_13956_Article_Equ1.gif"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="d33e647">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_{out}$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq1.gif"/></alternatives></inline-formula> denotes the output feature map, <inline-formula id="IEq2"><alternatives><tex-math id="d33e653">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_i$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq2.gif"/></alternatives></inline-formula> are the convolutional filters, <inline-formula id="IEq3"><alternatives><tex-math id="d33e659">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_in$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq3.gif"/></alternatives></inline-formula> is the input feature map, and <inline-formula id="IEq4"><alternatives><tex-math id="d33e665">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_i$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq4.gif"/></alternatives></inline-formula> represents the bias term. By replacing fewer large convolutional operations with multiple smaller ones, YOLOv12 enhances processing speed while preserving the effectiveness of feature extraction.</p></sec><sec id="Sec6"><title>Enhanced backbone architecture</title><p id="Par15">In addition to advanced convolutional design, YOLOv12 incorporates architectural improvements such as <inline-formula id="IEq5"><alternatives><tex-math id="d33e675">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$7\times 7$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq5.gif"/></alternatives></inline-formula> separable convolutions, which significantly lower computational demands. This method serves as an efficient alternative to traditional large-kernel convolutions or positional encoding techniques, enabling the network to retain spatial contextual information with fewer parameters. Furthermore, the integration of multi-scale feature pyramids allows the model to effectively capture and distinguish features of objects across different scales, including small or partially obscured targets.</p></sec><sec id="Sec7"><title>Neck</title><p id="Par16">Serving as the bridge between the backbone and the detection head, the neck in YOLOv12 plays a critical role in aggregating and refining features across multiple scales. A key advancement in this component is the integration of an area attention mechanism, optimized using FlashAttention, which improves the model&#8217;s ability to concentrate on important regions within complex or cluttered scenes. This mechanism can be mathematically represented by a segmented attention formulation:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e685">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Attention}(Q, K, V) = \text {softmax}\left( \frac{QK^\top }{\sqrt{d_k}}\right) V \end{aligned}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_13956_Article_Equ2.gif"/></alternatives></disp-formula>where <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">V</italic> denote the query, key, and value matrices, respectively, and <inline-formula id="IEq6"><alternatives><tex-math id="d33e701">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_k$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq6.gif"/></alternatives></inline-formula> is the dimensionality of the key vectors. By partitioning feature maps into spatial segments and applying efficient attention operations, the neck reduces memory bandwidth usage and computational load. This design enables real-time inference performance, even when processing high-resolution inputs.</p><p id="Par17">A key component of YOLOv12 is the C3k2 block, a compact version of the GELAN (Generalized Efficient Layer Aggregation Network) module. This block is strategically placed in the architecture to balance computational efficiency with detection accuracy by optimizing feature aggregation.</p></sec></sec><sec id="Sec8"><title>Integrating A2C2f</title><p id="Par18">To tailor the baseline YOLOv12 architecture for the specific requirements of photovoltaic (PV) defect detection, a key architectural modification was implemented within the network&#8217;s feature fusion neck. In the original model, layer 20 employs a C3k2 block, a convolution-based fusion unit, to aggregate multiscale features. This study proposes that substituting this block with an attention-enhanced module may lead to improved performance, particularly in detecting subtle and complex PV defect patterns. Accordingly, the C3k2 block at layer 20 was replaced with the A2C2f module. While the A2C2f module is an established component within the broader YOLOv12 design space, its targeted application at this specific fusion point constitutes the primary architectural contribution of this work. Layer 20 serves as a pivotal junction where high-level semantic features from the backbone converge with features derived from the upsampling pathway. Enhancing this junction is expected to refine feature selection and aggregation, which is particularly valuable when analyzing high-resolution PV imagery where defects can be faint or morphologically similar. The A2C2f module integrates two crucial components: <list list-type="order"><list-item><p id="Par19"><bold>Area Attention (A2):</bold> This mechanism dynamically assigns higher weights to critical defect regions in the PV images while suppressing irrelevant background noise. By prioritizing significant features, the model improves defect localization.</p></list-item><list-item><p id="Par20"><bold>Residual ELAN (R-ELAN):</bold> This component introduces additional shortcut connections that enhance gradient propagation and stabilize training, especially in deep networks. These residual connections enable efficient learning of complex patterns in PV defect detection, as illustrated in Figure <xref rid="Fig1" ref-type="fig">1</xref>.</p></list-item></list><fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Residual Efficient Layer Aggregation Network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO1" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig1_HTML.jpg"/></fig></p><p id="Par21">Additionally, replacing C3k2 with A2C2f introduces key modifications in feature extraction and attention mechanisms. In the baseline YOLOv12 model, C3k2 is used for feature aggregation by means of standard convolutions and bottleneck structures. By integrating A2C2f instead, the model benefits from advanced attention mechanisms, including Query-Key-Value (QKV) attention, positional encoding with depth-wise convolutions, and multi-layer perceptron (MLP) processing, leading to improved feature learning, as outlined in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Architectural and feature differences between YOLOv12n and PV-YOLOv12n.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Feature</bold></th><th align="left" colspan="1" rowspan="1"><bold>YOLOv12n</bold></th><th align="left" colspan="1" rowspan="1"><bold>PV-YOLOv12n</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Module</td><td align="left" colspan="1" rowspan="1">C3k2</td><td align="left" colspan="1" rowspan="1">A2C2f</td></tr><tr><td align="left" colspan="1" rowspan="1">Feature Aggregation</td><td align="left" colspan="1" rowspan="1">Uses C3k blocks for spatial and depth-wise feature refinement</td><td align="left" colspan="1" rowspan="1">Uses attention-enhanced feature processing for improved localization accuracy</td></tr><tr><td align="left" colspan="1" rowspan="1">Activation Function</td><td align="left" colspan="1" rowspan="1">SiLU (applied to all convolutions)</td><td align="left" colspan="1" rowspan="1">SiLU (applied to all convolutions)</td></tr><tr><td align="left" colspan="1" rowspan="1">Processing Method</td><td align="left" colspan="1" rowspan="1">Bottleneck-based feature fusion for defect detection</td><td align="left" colspan="1" rowspan="1">Attention-based feature refinement</td></tr><tr><td align="left" colspan="1" rowspan="1">Attention Mechanism</td><td align="left" colspan="1" rowspan="1">None</td><td align="left" colspan="1" rowspan="1">AAttn: Implements Query-Key-Value (QKV) attention mechanism Positional Encoding (PE): Uses depth-wise convolutions (7x7) MLP: Two fully connected layers with SiLU activation</td></tr><tr><td align="left" colspan="1" rowspan="1">Computational Adjustments</td><td align="left" colspan="1" rowspan="1">Standard convolution-based processing</td><td align="left" colspan="1" rowspan="1">Introduces additional layers but enhances selective feature weighting for improved defect detection</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec9"><title>Impact of the enhancement on the detection head</title><p id="Par22">As C3k2 module in YOLOv12 is directly connected to the detection head, modifying it significantly affects the model&#8217;s performance. The key impacts include:<list list-type="bullet"><list-item><p id="Par23">Feature representation for object detection: the detection head processes the features extracted from A2C2f to generate bounding boxes and class predictions. Replacing C3k2 with A2C2f provides the detection head with richer, attention-refined features, improving defect localization accuracy.</p></list-item><list-item><p id="Par24">Improved spatial and contextual awareness: C3k2 relies on bottleneck-based feature fusion, whereas A2C2f integrates attention blocks and positional encoding, helping the detection head focus on critical defect areas and reducing misclassification.</p></list-item><list-item><p id="Par25">Better gradient flow and training stability: since A2C2f is close to the detection head, this architectural change influences how gradients flow during backpropagation. A2C2f&#8217;s residual connections ensure smoother gradient updates, leading to improved model stability and convergence.</p></list-item><list-item><p id="Par26">Enhanced detection of large and complex defects: the A2C2f module at the P5 scale (1024, True) prioritizes critical defect regions, which improves recall and precision for detecting large cracks, significant dislocations, and widespread material inconsistencies.</p></list-item></list>These enhancements contribute to improved convergence stability and robustness in defect detection across various PV panel conditions. Figure <xref rid="Fig2" ref-type="fig">2</xref> and Table <xref rid="Tab3" ref-type="table">3</xref> outline the detailed layers of the proposed PV-YOLOv12 architecture.<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>PV-YOLOv12 network structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO2" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig2_HTML.jpg"/></fig><table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Detailed parameters of PV-YOLOv12 architecture.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Index</bold></th><th align="left" colspan="1" rowspan="1"><bold>From</bold></th><th align="left" colspan="1" rowspan="1"><bold>N</bold></th><th align="left" colspan="1" rowspan="1"><bold>Params</bold></th><th align="left" colspan="1" rowspan="1"><bold>Module</bold></th><th align="left" colspan="1" rowspan="1"><bold>Arguments</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">464</td><td align="left" colspan="1" rowspan="1">Conv</td><td align="left" colspan="1" rowspan="1">[3, 16, 3, 2]</td></tr><tr><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">4672</td><td align="left" colspan="1" rowspan="1">Conv</td><td align="left" colspan="1" rowspan="1">[16, 32, 3, 2]</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">6640</td><td align="left" colspan="1" rowspan="1">C3k2</td><td align="left" colspan="1" rowspan="1">[32, 64, 1, False, 0.25]</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">36992</td><td align="left" colspan="1" rowspan="1">Conv</td><td align="left" colspan="1" rowspan="1">[64, 64, 3, 2]</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">26080</td><td align="left" colspan="1" rowspan="1">C3k2</td><td align="left" colspan="1" rowspan="1">[64, 128, 1, False, 0.25]</td></tr><tr><td align="left" colspan="1" rowspan="1">5</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">147712</td><td align="left" colspan="1" rowspan="1">Conv</td><td align="left" colspan="1" rowspan="1">[128, 128, 3, 2]</td></tr><tr><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">180864</td><td align="left" colspan="1" rowspan="1">A2C2f</td><td align="left" colspan="1" rowspan="1">[128, 128, 2, True, 4]</td></tr><tr><td align="left" colspan="1" rowspan="1">7</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">295424</td><td align="left" colspan="1" rowspan="1">Conv</td><td align="left" colspan="1" rowspan="1">[128, 256, 3, 2]</td></tr><tr><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">689408</td><td align="left" colspan="1" rowspan="1">A2C2f</td><td align="left" colspan="1" rowspan="1">[256, 256, 2, True, 1]</td></tr><tr><td align="left" colspan="1" rowspan="1">9</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">Upsample</td><td align="left" colspan="1" rowspan="1">[None, 2, &#8217;nearest&#8217;]</td></tr><tr><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">[&#8722;1, 6]</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">Concat</td><td align="left" colspan="1" rowspan="1">[1]</td></tr><tr><td align="left" colspan="1" rowspan="1">11</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">86912</td><td align="left" colspan="1" rowspan="1">A2C2f</td><td align="left" colspan="1" rowspan="1">[384, 128, 1, False, &#8722;1]</td></tr><tr><td align="left" colspan="1" rowspan="1">12</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">Upsample</td><td align="left" colspan="1" rowspan="1">[None, 2, &#8217;nearest&#8217;]</td></tr><tr><td align="left" colspan="1" rowspan="1">13</td><td align="left" colspan="1" rowspan="1">[&#8722;1, 4]</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">Concat</td><td align="left" colspan="1" rowspan="1">[1]</td></tr><tr><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">24000</td><td align="left" colspan="1" rowspan="1">A2C2f</td><td align="left" colspan="1" rowspan="1">[256, 64, 1, False, &#8722;1]</td></tr><tr><td align="left" colspan="1" rowspan="1">15</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">36992</td><td align="left" colspan="1" rowspan="1">Conv</td><td align="left" colspan="1" rowspan="1">[64, 64, 3, 2]</td></tr><tr><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">[&#8722;1, 11]</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">Concat</td><td align="left" colspan="1" rowspan="1">[1]</td></tr><tr><td align="left" colspan="1" rowspan="1">17</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">74624</td><td align="left" colspan="1" rowspan="1">A2C2f</td><td align="left" colspan="1" rowspan="1">[192, 128, 1, False, &#8722;1]</td></tr><tr><td align="left" colspan="1" rowspan="1">18</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">147712</td><td align="left" colspan="1" rowspan="1">Conv</td><td align="left" colspan="1" rowspan="1">[128, 128, 3, 2]</td></tr><tr><td align="left" colspan="1" rowspan="1">19</td><td align="left" colspan="1" rowspan="1">[&#8722;1, 8]</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">Concat</td><td align="left" colspan="1" rowspan="1">[1]</td></tr><tr><td align="left" colspan="1" rowspan="1">20</td><td align="left" colspan="1" rowspan="1">&#8722;1</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">394240</td><td align="left" colspan="1" rowspan="1">A2C2f</td><td align="left" colspan="1" rowspan="1">[384, 256, 1, True]</td></tr><tr><td align="left" colspan="1" rowspan="1">21</td><td align="left" colspan="1" rowspan="1">[14, 17, 20]</td><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">464912</td><td align="left" colspan="1" rowspan="1">Detect</td><td align="left" colspan="1" rowspan="1">[80, [64, 128, 256]]</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec10"><title>Experiments</title><sec id="Sec11"><title>Experiment introduction</title><p id="Par27">This section provides an overview of the datasets utilized in this study, followed by a description of the experimental setup and training methodology. Finally, it outlines the evaluation metrics used to assess the experimental results.</p><sec id="Sec12"><title>Datasets</title><p id="Par28">To validate the proposed fault detection model, we used two datasets The first dataset, PVEL-AD<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, comprises 5,589 images and is specifically designed for detecting defects in photovoltaic cell. It was jointly released by Hebei University of Technology and Beihang University and contains one class of non-defective images alongside 12 categories of anomalous defects. However, the original class distribution is notably imbalanced, with some defect types being underrepresented. To ensure reliable training and meaningful evaluation, we selected eight defect classes based on the following criteria: (i) adequate availability of annotated instances per class, and (ii) alignment with prior literature focusing on frequently occurring and visually distinctive defects. The selected classes include: black_core, crack, finger, thick_line, star_crack, horizontal_dislocation, vertical_dislocation, and short_circuit, as illustrated in Figure <xref rid="Fig3" ref-type="fig">3</xref>. The dataset was partitioned into training, validation, and test subsets using a 70:20:10 split, resulting in 3,912 images for the training set, 1,118 images for the validation set, and 559 images for the testing set. It is important to note that in EL imaging, a single image can contain multiple instances of various defect types. Consequently, the total number of defect instances exceeds the number of images. Table <xref rid="Tab4" ref-type="table">4</xref> provides a detailed breakdown of the instance counts per defect class and their distribution across the three subsets.<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Defect samples from PVEL-AD Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO3" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig3_HTML.jpg"/></fig><table-wrap id="Tab4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Summary of instances and images across defect categories in the PVEL-AD dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1"><bold>Category</bold></th><th align="left" colspan="3" rowspan="1"><bold>Number of instances</bold></th><th align="left" rowspan="2" colspan="1"><bold>Total</bold></th><th align="left" rowspan="2" colspan="1"><bold>The number of images</bold></th></tr><tr><th align="left" colspan="1" rowspan="1"><bold>Train</bold></th><th align="left" colspan="1" rowspan="1"><bold>Val</bold></th><th align="left" colspan="1" rowspan="1"><bold>Test</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Black_core</td><td align="left" colspan="1" rowspan="1">1601</td><td align="left" colspan="1" rowspan="1">465</td><td align="left" colspan="1" rowspan="1">235</td><td align="left" colspan="1" rowspan="1">2301</td><td align="left" colspan="1" rowspan="1">5589</td></tr><tr><td align="left" colspan="1" rowspan="1">Crack</td><td align="left" colspan="1" rowspan="1">1231</td><td align="left" colspan="1" rowspan="1">378</td><td align="left" colspan="1" rowspan="1">215</td><td align="left" colspan="1" rowspan="1">1824</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Finger</td><td align="left" colspan="1" rowspan="1">2335</td><td align="left" colspan="1" rowspan="1">676</td><td align="left" colspan="1" rowspan="1">339</td><td align="left" colspan="1" rowspan="1">3350</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Horizontal dislocation</td><td align="left" colspan="1" rowspan="1">1474</td><td align="left" colspan="1" rowspan="1">424</td><td align="left" colspan="1" rowspan="1">201</td><td align="left" colspan="1" rowspan="1">2099</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Short circuit</td><td align="left" colspan="1" rowspan="1">1121</td><td align="left" colspan="1" rowspan="1">204</td><td align="left" colspan="1" rowspan="1">125</td><td align="left" colspan="1" rowspan="1">1450</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Star crack</td><td align="left" colspan="1" rowspan="1">201</td><td align="left" colspan="1" rowspan="1">36</td><td align="left" colspan="1" rowspan="1">15</td><td align="left" colspan="1" rowspan="1">252</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Thick line</td><td align="left" colspan="1" rowspan="1">1298</td><td align="left" colspan="1" rowspan="1">324</td><td align="left" colspan="1" rowspan="1">135</td><td align="left" colspan="1" rowspan="1">1757</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Vertical dislocation</td><td align="left" colspan="1" rowspan="1">166</td><td align="left" colspan="1" rowspan="1">48</td><td align="left" colspan="1" rowspan="1">26</td><td align="left" colspan="1" rowspan="1">240</td><td align="left" colspan="1" rowspan="1"/></tr></tbody></table></table-wrap></p><p id="Par29">The second dataset is obtained from the Roboflow Universe<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, an open-source computer vision community. It contains 6,493 images annotated with four distinct classes: bird drop, cracked, dusty, and panel. The panel class represents non-defective panels, serving as a baseline, while the remaining three classes correspond to common solar panel anomalies, including bird droppings, cracks, and dust accumulation. Samples of these defect types are visually represented in Figure <xref rid="Fig4" ref-type="fig">4</xref>, wherein bounding boxes and color coding facilitate differentiation. The dataset was chosen for its large, diverse sample set, offering a realistic representation of PV defects for deep learning-based detection models. For robust training and evaluation, the dataset was subjected to a standard 70:20:10 split, resulting in a training set of 4,545 images, a validation set of 1,298 images, and a testing set of 650 images. The distribution of annotated object instances across different defect classes, along with their respective counts for training, validation, and testing sets, is comprehensively summarized in Table <xref rid="Tab5" ref-type="table">5</xref>. This distribution highlights a significant class imbalance that poses challenges for model generalization and robustness.<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Samples from different classes in the solar panel dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO4" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig4_HTML.jpg"/></fig><table-wrap id="Tab5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Summary of instances and images across categories for Roboflow Dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1"><bold>Category</bold></th><th align="left" colspan="3" rowspan="1"><bold>Number of instances</bold></th><th align="left" rowspan="2" colspan="1"><bold>Total</bold></th><th align="left" rowspan="2" colspan="1"><bold>Number of Images</bold></th></tr><tr><th align="left" colspan="1" rowspan="1"><bold>Train</bold></th><th align="left" colspan="1" rowspan="1"><bold>Val</bold></th><th align="left" colspan="1" rowspan="1"><bold>Test</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Cracked</td><td align="left" colspan="1" rowspan="1">4354</td><td align="left" colspan="1" rowspan="1">1250</td><td align="left" colspan="1" rowspan="1">629</td><td align="left" colspan="1" rowspan="1">6233</td><td align="left" colspan="1" rowspan="1">6493</td></tr><tr><td align="left" colspan="1" rowspan="1">Panel</td><td align="left" colspan="1" rowspan="1">3115</td><td align="left" colspan="1" rowspan="1">905</td><td align="left" colspan="1" rowspan="1">457</td><td align="left" colspan="1" rowspan="1">4477</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Dusty</td><td align="left" colspan="1" rowspan="1">165</td><td align="left" colspan="1" rowspan="1">49</td><td align="left" colspan="1" rowspan="1">21</td><td align="left" colspan="1" rowspan="1">235</td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Bird_drop</td><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">60</td><td align="left" colspan="1" rowspan="1"/></tr></tbody></table></table-wrap></p></sec><sec id="Sec13"><title>Dataset augmentation</title><p id="Par30">To enhance model generalization and robustness, a unified data augmentation strategy was applied to both datasets. The primary motivation for applying a consistent set of augmentations was to build a single, robust model capable of generalizing across different imaging modalities and real-world conditions. The chosen techniques, which are largely standard in state-of-the-art YOLO frameworks, can be categorized into geometric, color/intensity, and occlusion transformations.<list list-type="bullet"><list-item><p id="Par31"><bold>Geometric Transformations (Horizontal Flip, Scale, Translation, Mosaic):</bold> These augmentations are universally beneficial as they teach the model positional and scale invariance. A defect, whether it&#8217;s a micro-crack in an EL image or a dust patch in a visual image, can appear at any location, orientation, or scale. By artificially diversifying the training data in this manner, we ensure the model learns to identify defects based on their intrinsic features rather than their position or size within the frame.</p></list-item><list-item><p id="Par32"><bold>Color/Intensity Transformations (HSV Adjustments):</bold> We acknowledge that the impact of HSV augmentation differs between the two datasets. For the Roboflow dataset, adjusting hue, saturation, and value simulates variations in lighting conditions, shadows, and camera properties, which is critical for real-world visual inspections. For the grayscale PVEL-AD dataset, the primary effect comes from adjusting the value (brightness), which simulates variations in EL imaging intensity and exposure levels. This helps the model become more robust to the quality of the EL capture process.</p></list-item><list-item><p id="Par33"><bold>Occlusion (Random Erasing):</bold> This technique was included to improve the model&#8217;s robustness to occlusions, forcing it to make predictions based on partial information. For EL images, random erasing simulates partial occlusions caused by the solar cell&#8217;s busbars or finger lines. For RGB images, it mimics real-world scenarios where defects are commonly occluded by shadows, leaves, or other debris. In both cases, the technique compels the model to learn a more distributed and resilient feature representation of each defect class. The specifics of the augmentation techniques are detailed in Table&#160;<xref rid="Tab6" ref-type="table">6</xref>.</p></list-item></list><table-wrap id="Tab6" position="float" orientation="portrait"><label>Table 6</label><caption><p>Configuration of Data Augmentation Methods for Model Training.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Augmentation</bold></th><th align="left" colspan="1" rowspan="1"><bold>Parameters</bold></th><th align="left" colspan="1" rowspan="1"><bold>Application Rate</bold></th><th align="left" colspan="1" rowspan="1"><bold>Purpose</bold></th><th align="left" colspan="1" rowspan="1"><bold>Impact on EL/RGB Images</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Mosaic</td><td align="left" colspan="1" rowspan="1">mosaic=1.0</td><td align="left" colspan="1" rowspan="1">100%</td><td align="left" colspan="1" rowspan="1">Combines 4 images into one</td><td align="left" colspan="1" rowspan="1">Helps detect defects in varied contexts; equally beneficial for both modalities</td></tr><tr><td align="left" colspan="1" rowspan="1">HSV Adjustments</td><td align="left" colspan="1" rowspan="1">hsv_h=0.015, hsv_s=0.7, hsv_v=0.4</td><td align="left" colspan="1" rowspan="1">Per-image randomization</td><td align="left" colspan="1" rowspan="1">Modifies color properties</td><td align="left" colspan="1" rowspan="1">More impactful for RGB (color defects); limited effect on EL (grayscale)</td></tr><tr><td align="left" colspan="1" rowspan="1">Horizontal Flip</td><td align="left" colspan="1" rowspan="1">fliplr=0.5</td><td align="left" colspan="1" rowspan="1">50%</td><td align="left" colspan="1" rowspan="1">Flips image left-right</td><td align="left" colspan="1" rowspan="1">Preserves label validity; useful for both EL and RGB</td></tr><tr><td align="left" colspan="1" rowspan="1">Random Erasing</td><td align="left" colspan="1" rowspan="1">erasing=0.4</td><td align="left" colspan="1" rowspan="1">40%</td><td align="left" colspan="1" rowspan="1">Masks random patches</td><td align="left" colspan="1" rowspan="1">Simulates occlusions; helps prevent overfitting to specific patterns</td></tr><tr><td align="left" colspan="1" rowspan="1">Translation</td><td align="left" colspan="1" rowspan="1">translate=0.1</td><td align="left" colspan="1" rowspan="1">Per-image randomization</td><td align="left" colspan="1" rowspan="1">Shifts image by <inline-formula id="IEq7"><alternatives><tex-math id="d33e1645">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq7.gif"/></alternatives></inline-formula>10%</td><td align="left" colspan="1" rowspan="1">Maintains defect features while adding positional variance</td></tr><tr><td align="left" colspan="1" rowspan="1">Scaling</td><td align="left" colspan="1" rowspan="1">scale=0.5</td><td align="left" colspan="1" rowspan="1">Per-image randomization</td><td align="left" colspan="1" rowspan="1">Zooms (0.5x&#8211;1.5x)</td><td align="left" colspan="1" rowspan="1">Helps detect multi-scale defects; critical for small cracks in EL</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec14"><title>Experimental environment</title><p id="Par34">For this study, all experiments were executed on the Kaggle platform, which offers cloud-based Jupyter Notebooks (Kernels) and complimentary access to high-performance GPUs. The computational setup included a Tesla T4 GPU with 15,095 MiB of memory. The software environment was configured with Python 3.10.13 and PyTorch 2.1.2, allowing efficient utilization of GPU parallel processing capabilities. To optimize model performance during training, the experimental parameters were carefully selected based on prior research and dataset characteristics. Stochastic Gradient Descent (SGD) was chosen as the optimizer, with the following hyperparameters: 200 training epochs, a batch size of 16, and an input image resolution of 640 <inline-formula id="IEq8"><alternatives><tex-math id="d33e1668">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_13956_Article_IEq8.gif"/></alternatives></inline-formula> 640 pixels. The initial learning rate was set to 0.01, with a momentum of 0.937, weight decay of 0.0005, and a warm-up period spanning the first three epochs.</p></sec><sec id="Sec15"><title>Evaluation indicators</title><p id="Par35">To comprehensively assess the model&#8217;s performance, we used widely recognized evaluation metrics commonly used in object detection tasks. These include precision (P), recall (R), F1-score (F1), mean average precision (mAP)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, and Giga Floating Point Operations (GFLOPs). Additionally, the total number of model parameters was considered to evaluate computational complexity. The mathematical formulations for these metrics are as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e1682">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P&amp;= \left( \frac{\textrm{TP}}{\textrm{TP}+\textrm{FP}} \right) \end{aligned}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_13956_Article_Equ3.gif"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1688">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R&amp;= \left( \frac{\textrm{TP}}{\textrm{TP}+\textrm{FN}} \right) \end{aligned}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_13956_Article_Equ4.gif"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e1694">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1&amp;= \left( \frac{2 \times P \times R}{P+R} \right) \end{aligned}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_13956_Article_Equ5.gif"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e1700">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textrm{mAP}&amp;= \frac{1}{C} \sum _{i=1}^{C} \textrm{AP}_i \end{aligned}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_13956_Article_Equ6.gif"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e1706">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {GFLOPs}&amp;= \frac{\text {Total floating-point operations}}{10^9} \quad \textit{per image} \end{aligned}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_13956_Article_Equ7.gif"/></alternatives></disp-formula>Where <italic toggle="yes">TP</italic> represents the number of correctly detected PV cell defects, <italic toggle="yes">FP</italic> refers to the number of incorrectly identified defects, and <italic toggle="yes">FN</italic> denotes the number of missed defects. The total number of classes is represented by C. GFLOPs quantify the theoretical computational cost of the model, which is determined based on the input resolution and the complexity of the network architecture. Additionally, the number of parameters reflects the model&#8217;s size and computational requirements, influencing both inference speed and memory consumption.</p></sec></sec><sec id="Sec16"><title>Experiment results</title><sec id="Sec17"><title>Ablation study</title><p id="Par36">To evaluate the effectiveness of the proposed A2C2f module, we conducted a systematic ablation study comparing the baseline YOLOv12n model against the modified PV-YOLOv12n across both datasets. The study isolates the contribution of the A2C2f module by keeping all other hyperparameters, training protocols, and dataset splits identical. Results are summarized in Tables <xref rid="Tab7" ref-type="table">7</xref> and <xref rid="Tab8" ref-type="table">8</xref>.<table-wrap id="Tab7" position="float" orientation="portrait"><label>Table 7</label><caption><p>Ablation Results on PVEL-AD Dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Model</bold></th><th align="left" colspan="1" rowspan="1"><bold>mAP@50</bold></th><th align="left" colspan="1" rowspan="1"><bold>mAP@50-95</bold></th><th align="left" colspan="1" rowspan="1"><bold>Precision</bold></th><th align="left" colspan="1" rowspan="1"><bold>Recall</bold></th><th align="left" colspan="1" rowspan="1"><bold>Inference Speed (ms)</bold></th><th align="left" colspan="1" rowspan="1"><bold>Parameters</bold></th><th align="left" colspan="1" rowspan="1"><bold>GFLOPs</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">YOLOv12n</td><td align="left" colspan="1" rowspan="1">0.90</td><td align="left" colspan="1" rowspan="1">0.57</td><td align="left" colspan="1" rowspan="1">0.83</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">4.26</td><td align="left" colspan="1" rowspan="1">2,602,288</td><td align="left" colspan="1" rowspan="1">6.65</td></tr><tr><td align="left" colspan="1" rowspan="1">PV-YOLOv12n</td><td align="left" colspan="1" rowspan="1"><bold>0.91</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.58</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.84</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.87</bold></td><td align="left" colspan="1" rowspan="1"><bold>4.24</bold></td><td align="left" colspan="1" rowspan="1">2,617,648</td><td align="left" colspan="1" rowspan="1">6.66</td></tr></tbody></table></table-wrap><table-wrap id="Tab8" position="float" orientation="portrait"><label>Table 8</label><caption><p>Ablation Results on Roboflow Dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Model</bold></th><th align="left" colspan="1" rowspan="1"><bold>mAP@50</bold></th><th align="left" colspan="1" rowspan="1"><bold>mAP@50-95</bold></th><th align="left" colspan="1" rowspan="1"><bold>Precision</bold></th><th align="left" colspan="1" rowspan="1"><bold>Recall</bold></th><th align="left" colspan="1" rowspan="1"><bold>Inference Speed (ms)</bold></th><th align="left" colspan="1" rowspan="1"><bold>Parameters</bold></th><th align="left" colspan="1" rowspan="1"><bold>GFLOPs</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">YOLOv12n</td><td align="left" colspan="1" rowspan="1">0.88</td><td align="left" colspan="1" rowspan="1">0.73</td><td align="left" colspan="1" rowspan="1">0.89</td><td align="left" colspan="1" rowspan="1">0.87</td><td align="left" colspan="1" rowspan="1">4.45</td><td align="left" colspan="1" rowspan="1">2,602,288</td><td align="left" colspan="1" rowspan="1">6.65</td></tr><tr><td align="left" colspan="1" rowspan="1">PV-YOLOv12n</td><td align="left" colspan="1" rowspan="1"><bold>0.91</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.75</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.92</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.88</bold></td><td align="left" colspan="1" rowspan="1"><bold>4.43</bold></td><td align="left" colspan="1" rowspan="1">2,617,648</td><td align="left" colspan="1" rowspan="1">6.66</td></tr></tbody></table></table-wrap></p><p id="Par37"><bold>A. PVEL-AD Dataset Analysis:</bold> The integration of the A2C2f module enhances localization accuracy, improving mAP@50 by 1% (from 0.90 to 0.91) and mAP@50-95 by 1% (from 0.57 to 0.58), ensuring more precise defect detection across varying IoU thresholds. Additionally, the module optimizes the precision-recall balance, with a 1% increase in precision (from 0.83 to 0.84) and a 1% boost in recall (from 0.86 to 0.87), effectively reducing false positives while maintaining high defect sensitivity. Despite a slight parameter increase (+15,360 parameters, +0.58%), inference speed improves slightly (from 4.24 ms to 4.26 ms), confirming that the A2C2f module enhances performance without introducing significant computational overhead.</p><p id="Par38"><bold>B. Roboflow Dataset Analysis:</bold> The A2C2f module enhances generalization, leading to a 3.3% increase in mAP@50 (from 0.88 to 0.91) and a 2.6% improvement in mAP@50-95 (from 0.73 to 0.75), demonstrating its effectiveness in detecting diverse real-world defects such as bird droppings and dust. Additionally, precision rises by 3.2% (from 0.89 to 0.92), showcasing the module&#8217;s ability to minimize false alarms in complex backgrounds. The mAP@50-95 improvement further highlights enhanced robustness to scale, particularly for small defects like dust particles (IoU&gt; 0.5), ensuring reliable detection across varying classes.</p></sec><sec id="Sec18"><title>Training dynamics and convergence analysis</title><p id="Par39">To further evaluate the training dynamics of PV-YOLOv12n and YOLOv12n, we analyzed their convergence behavior by tracking the mAP@50 metric across epochs on both datasets. Figures <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> illustrate how the integration of the A2C2f module impacts training stability and final detection accuracy.</p><p id="Par40">On the PVEL-AD dataset (Figure <xref rid="Fig5" ref-type="fig">5</xref>.a), PV-YOLOv12n demonstrates faster convergence for mAP@50, stabilizing at 0.91 by epoch 160, whereas YOLOv12n plateaus at 0.90 mAP@50 after epoch 180. During the early training phase (epochs 0-50), PV-YOLOv12n exhibits a steeper learning curve, achieving an 8% higher mAP@50 compared to baseline YOLOv12n, which can be attributed to the A2C2f module&#8217;s enhanced feature refinement. In the mid-training phase (epochs 50-150), both models stabilize, but PV-YOLOv12n maintains lower variance, indicating greater robustness against overfitting. By the final convergence stage, the A2C2f module enables PV-YOLOv12n to retain higher precision in detecting fine-grained defects such as vertical dislocations. Similarly, for the more stringent mAP@50-95 metric on PVEL-AD (Figure <xref rid="Fig6" ref-type="fig">6</xref>.a), PV-YOLOv12n also shows a consistent, albeit smaller, advantage throughout the training, converging to 0.58, while YOLOv12n reaches 0.57 (Table <xref rid="Tab7" ref-type="table">7</xref>). This indicates that the A2C2f module&#8217;s benefits extend to performance under stricter IoU thresholds, suggesting more accurate bounding box predictions. On the Roboflow dataset (Figure <xref rid="Fig5" ref-type="fig">5</xref>.b), PV-YOLOv12n consistently outperforms YOLOv12n throughout training for mAP@50, achieving a final mAP@50 of 0.91, compared to 0.88 for YOLOv12n, with smoother convergence. The rapid adaptation phase shows PV-YOLOv12n reaching 0.85 mAP@50 by epoch 40, whereas YOLOv12n requires 60 epochs to attain the same performance. As training progresses, the performance gap widens after epoch 100, with PV-YOLOv12n maintaining a 3.4% higher mAP@50 (B). This trend of superior performance is mirrored and amplified in the mAP@50-95 results for the Roboflow dataset (Figure <xref rid="Fig6" ref-type="fig">6</xref>.b). Here, PV-YOLOv12n establishes a more pronounced lead early on and maintains it, ultimately achieving 0.75 mAP@50-95 against YOLOv12n&#8217;s 0.73 (Table <xref rid="Tab8" ref-type="table">8</xref>). This highlights the A2C2f module&#8217;s effectiveness in enhancing feature representation, leading to more precise object localization across a wider range of IoU thresholds, which is particularly beneficial for the diverse Roboflow dataset.<fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>mAP@50 Convergence Analysis of PV-YOLOv12n vs. Baseline YOLOv12n on (<bold>a</bold>) PVEL-AD Dataset and (<bold>b</bold>) Roboflow Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO5" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig5_HTML.jpg"/></fig><fig id="Fig6" position="float" orientation="portrait"><label>Fig. 6</label><caption><p>mAP@50-95 Convergence Analysis of PV-YOLOv12n vs. Baseline YOLOv12n on (<bold>a</bold>) PVEL-AD Dataset and (<bold>b</bold>) Roboflow Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO6" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig6_HTML.jpg"/></fig></p><p id="Par41">Figure <xref rid="Fig7" ref-type="fig">7</xref> illustrates the confusion matrix comparing the classification performance of PV-YOLOv12n (a) and YOLOv12n (b) on the PVEL-AD dataset, highlighting differences in detection accuracy across photovoltaic (PV) defect categories. Both models exhibit strong classification capabilities, with high true positive rates along the diagonal. However, PV-YOLOv12n (a) shows reduced misclassifications compared to YOLOv12n (b), particularly in distinguishing visually similar defects such as cracks and star cracks. The integration of the A2C2f module in PV-YOLOv12n improves feature refinement and selective attention, resulting in enhanced localization accuracy, reduced false positives, and a better balance between precision and recall.<fig id="Fig7" position="float" orientation="portrait"><label>Fig. 7</label><caption><p>Confusion Matrix of PV-YOLOv12n (<bold>a</bold>) and YOLOv12n (<bold>b</bold>) on the PVEL-AD Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO7" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig7_HTML.jpg"/></fig></p><p id="Par42">Figure <xref rid="Fig8" ref-type="fig">8</xref> presents the confusion matrix for PV-YOLOv12n (a) and YOLOv12n (b) on the Roboflow dataset, illustrating the classification performance of both models in detecting photovoltaic (PV) defects. The matrices reveal that PV-YOLOv12n (a) demonstrates improved defect classification, with higher true positive rates along the diagonal and fewer misclassifications compared to YOLOv12n (b). Notably, PV-YOLOv12n (a) shows enhanced accuracy in distinguishing small and complex defects such as bird droppings and dust accumulation, which often lead to confusion in conventional detection models. The A2C2f module in PV-YOLOv12n refines feature extraction and selective attention, leading to improved localization accuracy, a reduction in false positives, and an overall increase in detection robustness across diverse real-world PV defect scenarios.<fig id="Fig8" position="float" orientation="portrait"><label>Fig. 8</label><caption><p>Confusion Matrix of PV-YOLOv12n (<bold>a</bold>) and YOLOv12n (<bold>b</bold>) on the Roboflow Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO8" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig8_HTML.jpg"/></fig></p><p id="Par43">Figure <xref rid="Fig9" ref-type="fig">9</xref> presents the F1-score curves of PV-YOLOv12n (a) and YOLOv12n (b) over the training epochs on the PVEL-AD dataset, demonstrating the models&#8217; ability to balance precision and recall. The F1-score for PV-YOLOv12n (a) starts at 0.52 in the early epochs and steadily rises to 0.87, surpassing YOLOv12n (b), which starts at 0.49 and plateaus at 0.85. The faster convergence and consistently higher F1-score of PV-YOLOv12n (a) indicate improved defect detection stability and robustness. This performance gain is attributed to the A2C2f module, which enhances feature extraction and selective attention, leading to better localization of PV defects. Additionally, PV-YOLOv12n (a) achieves peak performance approximately 20 epochs earlier than YOLOv12n (b), demonstrating superior learning efficiency and model optimization.<fig id="Fig9" position="float" orientation="portrait"><label>Fig. 9</label><caption><p>F1-Score Curves of PV-YOLOv12n (<bold>a</bold>) and YOLOv12n (<bold>b</bold>) on the PVEL-AD Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO9" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig9_HTML.jpg"/></fig></p><p id="Par44">Figure <xref rid="Fig10" ref-type="fig">10</xref> illustrates the F1-score curves of PV-YOLOv12n (a) and YOLOv12n (b) over the training epochs on the Roboflow dataset, reflecting the models&#8217; effectiveness in balancing precision and recall. PV-YOLOv12n (a) starts with an initial F1-score of 0.55, progressively improving to 0.91, whereas YOLOv12n (b) begins at 0.51 and stabilizes at 0.88. The improved F1-score of PV-YOLOv12n (a) highlights its superior defect detection performance, particularly in identifying small and complex anomalies such as bird droppings and dust accumulation. The integration of the A2C2f module enhances feature refinement and selective attention, leading to more precise defect localization. Additionally, PV-YOLOv12n (a) reaches its peak F1-score approximately 15 epochs earlier than YOLOv12n (b), demonstrating faster convergence and greater model efficiency.<fig id="Fig10" position="float" orientation="portrait"><label>Fig. 10</label><caption><p>F1-Score Curves of PV-YOLOv12n (<bold>a</bold>) and YOLOv12n (<bold>b</bold>) on the Roboflow Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO10" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig10_HTML.jpg"/></fig></p></sec><sec id="Sec19"><title>Enhanced performance across modalities</title><p id="Par45"><bold>A. Overall Performance on PVEL-AD Dataset:</bold> On the PVEL-AD dataset, PV-YOLOv12n achieves an overall mAP@0.5 of 91%, demonstrating a consistent improvement over the baseline YOLOv12n (90%). This incremental gain is driven by significant improvements in detecting challenging defect classes, particularly star_crack (Small defects), where PV-YOLOv12n achieves an mAP@0.5 of 77.9%, representing a substantial +5.1 percentage point improvement over YOLOv12n (72.8%). This notable enhancement confirms the efficacy of the A2C2f module in capturing the fine-grained features crucial for small object detection. Minor improvements are also observed for Black_core (+0.9%), crack (+6.8%), and thick_line (+2.1%), indicating a generally positive impact (see Table&#160;<xref rid="Tab9" ref-type="table">9</xref>).</p><p id="Par46">While minor reductions were observed for finger (&#8722;0.5%), horizontal_dislocation (&#8722;5.3%), and vertical_dislocation (&#8722;0.2%), the absolute mAP@0.5 values for these classes remain very high (e.g., 94.0% for horizontal_dislocation and 95.3% for vertical_dislocation). This indicates that the model&#8217;s overall optimization for a broader range of defects led to minor trade-offs in classes that are already highly detectable. For short_circuit, both models achieve an mAP@0.5 of 99.5%, indicating that this &#8217;Large&#8217; defect is robustly detected by both architectures.</p><p id="Par47"><bold>B. Overall Performance on Roboflow Dataset:</bold> The performance trends are even more pronounced on the Roboflow dataset, where PV-YOLOv12n secures an overall mAP@0.5 of 90.9%, yielding a significant +2.7 percentage point improvement compared to YOLOv12n (88.2%). The most striking improvement is seen in the detection of bird_drop defects (Small defects), where PV-YOLOv12n achieves an mAP@0.5 of 73.9%, an impressive +8.4 percentage point gain over YOLOv12n (65.5%). This further corroborates the enhanced capability of PV-YOLOv12n in handling diminutive imperfections. Consistent positive improvements are also observed for cracked (+1.4%), dusty (+1.3%), and panel (+0.1%) (refer to Table&#160;<xref rid="Tab9" ref-type="table">9</xref>).<table-wrap id="Tab9" position="float" orientation="portrait"><label>Table 9</label><caption><p>Comprehensive Defect Detection Performance Comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Dataset</bold></th><th align="left" colspan="1" rowspan="1"><bold>Defect Class</bold></th><th align="left" colspan="1" rowspan="1"><bold>PV-YOLOv12n</bold></th><th align="left" colspan="1" rowspan="1"><bold>YOLOv12n</bold></th><th align="left" colspan="1" rowspan="1"><bold>Improvement</bold></th><th align="left" colspan="1" rowspan="1"><bold>Size Category</bold></th></tr></thead><tbody><tr><td align="left" rowspan="9" colspan="1">PVEL-AD</td><td align="left" colspan="1" rowspan="1">Black_core</td><td align="left" colspan="1" rowspan="1">97.6</td><td align="left" colspan="1" rowspan="1">96.7</td><td align="left" colspan="1" rowspan="1">+0.9</td><td align="left" colspan="1" rowspan="1">Large</td></tr><tr><td align="left" colspan="1" rowspan="1">crack</td><td align="left" colspan="1" rowspan="1">83.5</td><td align="left" colspan="1" rowspan="1">76.7</td><td align="left" colspan="1" rowspan="1">+6.8</td><td align="left" colspan="1" rowspan="1">Small-medium</td></tr><tr><td align="left" colspan="1" rowspan="1">finger</td><td align="left" colspan="1" rowspan="1">89.6</td><td align="left" colspan="1" rowspan="1">90.1</td><td align="left" colspan="1" rowspan="1">&#8722;0.5</td><td align="left" colspan="1" rowspan="1">Medium</td></tr><tr><td align="left" colspan="1" rowspan="1">horizontal_dislocation</td><td align="left" colspan="1" rowspan="1">94.0</td><td align="left" colspan="1" rowspan="1">99.3</td><td align="left" colspan="1" rowspan="1">&#8722;5.3</td><td align="left" colspan="1" rowspan="1">Large</td></tr><tr><td align="left" colspan="1" rowspan="1">short_circuit</td><td align="left" colspan="1" rowspan="1">99.5</td><td align="left" colspan="1" rowspan="1">99.5</td><td align="left" colspan="1" rowspan="1">0.0</td><td align="left" colspan="1" rowspan="1">Large</td></tr><tr><td align="left" colspan="1" rowspan="1">star_crack</td><td align="left" colspan="1" rowspan="1">77.9</td><td align="left" colspan="1" rowspan="1">72.8</td><td align="left" colspan="1" rowspan="1">+5.1</td><td align="left" colspan="1" rowspan="1">Small</td></tr><tr><td align="left" colspan="1" rowspan="1">thick_line</td><td align="left" colspan="1" rowspan="1">93.0</td><td align="left" colspan="1" rowspan="1">90.9</td><td align="left" colspan="1" rowspan="1">+2.1</td><td align="left" colspan="1" rowspan="1">Medium</td></tr><tr><td align="left" colspan="1" rowspan="1">vertical_dislocation</td><td align="left" colspan="1" rowspan="1">95.3</td><td align="left" colspan="1" rowspan="1">95.5</td><td align="left" colspan="1" rowspan="1">&#8722;0.2</td><td align="left" colspan="1" rowspan="1">Large</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>mAP@0.5</bold></td><td align="left" colspan="1" rowspan="1"><bold>91.3</bold></td><td align="left" colspan="1" rowspan="1"><bold>90.2</bold></td><td align="left" colspan="1" rowspan="1"><bold>+1.1</bold></td><td align="left" colspan="1" rowspan="1"><bold>All sizes</bold></td></tr><tr><td align="left" rowspan="5" colspan="1">Roboflow</td><td align="left" colspan="1" rowspan="1">bird_drop</td><td align="left" colspan="1" rowspan="1">73.9</td><td align="left" colspan="1" rowspan="1">65.5</td><td align="left" colspan="1" rowspan="1">+8.4</td><td align="left" colspan="1" rowspan="1">Small</td></tr><tr><td align="left" colspan="1" rowspan="1">cracked</td><td align="left" colspan="1" rowspan="1">93.6</td><td align="left" colspan="1" rowspan="1">92.2</td><td align="left" colspan="1" rowspan="1">+1.4</td><td align="left" colspan="1" rowspan="1">Medium-large</td></tr><tr><td align="left" colspan="1" rowspan="1">dusty</td><td align="left" colspan="1" rowspan="1">98.8</td><td align="left" colspan="1" rowspan="1">97.5</td><td align="left" colspan="1" rowspan="1">+1.3</td><td align="left" colspan="1" rowspan="1">Large</td></tr><tr><td align="left" colspan="1" rowspan="1">panel</td><td align="left" colspan="1" rowspan="1">97.5</td><td align="left" colspan="1" rowspan="1">97.4</td><td align="left" colspan="1" rowspan="1">+0.1</td><td align="left" colspan="1" rowspan="1">Large</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>mAP@0.5</bold></td><td align="left" colspan="1" rowspan="1"><bold>90.9</bold></td><td align="left" colspan="1" rowspan="1"><bold>88.2</bold></td><td align="left" colspan="1" rowspan="1"><bold>+2.7</bold></td><td align="left" colspan="1" rowspan="1"><bold>All sizes</bold></td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec20"><title>Performance comparison of YOLO-based models</title><p id="Par48">Table <xref rid="Tab10" ref-type="table">10</xref> presents the performance comparison across various YOLO-based models trained on the PVEL-AD and Roboflow datasets, using the evaluation metrics mAP@0.5, Precision, Recall, mAP@0.5:0.95, Parameter count (M), and GFLOPs. The proposed PV-YOLOv12n model achieves the highest accuracy (0.91) on both datasets while maintaining a lightweight architecture with 2.6 million parameters and 6.6 GFLOPs. Compared to YOLOv12n, the optimized PV-YOLOv12n improves accuracy slightly while keeping computational complexity manageable. Older versions, such as YOLOv9t and YOLOv10n, show lower accuracy and, in some cases, higher GFLOPs, indicating less efficiency in computation. Interestingly, YOLOv8n also achieves 0.91 mAP on PVELAD, matching the accuracy of our PV-YOLOv12n. The strong performance of YOLOv8n is attributed to its refined architecture, which includes efficient backbone and neck designs, and optimized training strategies, characteristic of the advancements in the YOLO series. However, a critical distinction arises in terms of computational efficiency. YOLOv8n comes with a significantly higher computational cost of 8.7 GFLOPs and a larger parameter count of 3.2 million, compared to PV-YOLOv12n&#8217;s 6.6 GFLOPs and 2.6 million parameters. The results highlight that while YOLOv8n demonstrates high accuracy, PV-YOLOv12n offers a more computationally efficient alternative that achieves comparable accuracy. The lightweight architecture of PV-YOLOv12n, with its reduced model size and lower GFLOPs, leads to lower inference latency, making it more practical and highly suitable for real-time applications in photovoltaic defect detection, especially in resource-constrained environments.<table-wrap id="Tab10" position="float" orientation="portrait"><label>Table 10</label><caption><p>Performance of YOLO-Based Models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1"><bold>Model</bold></th><th align="left" colspan="4" rowspan="1"><bold>PVEL-AD Dataset</bold></th><th align="left" colspan="4" rowspan="1"><bold>Roboflow Dataset</bold></th><th align="left" rowspan="2" colspan="1"><bold>Parameters (M)</bold></th><th align="left" rowspan="2" colspan="1"><bold>GFLOPs</bold></th></tr><tr><th align="left" colspan="1" rowspan="1">mAP@0.5</th><th align="left" colspan="1" rowspan="1">Precision</th><th align="left" colspan="1" rowspan="1">Recall</th><th align="left" colspan="1" rowspan="1">mAP@0.5:0.95</th><th align="left" colspan="1" rowspan="1">mAP@0.5</th><th align="left" colspan="1" rowspan="1">Precision</th><th align="left" colspan="1" rowspan="1">Recall</th><th align="left" colspan="1" rowspan="1">mAP@0.5:0.95</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">PV-YOLOv12n</td><td align="left" colspan="1" rowspan="1">91.3</td><td align="left" colspan="1" rowspan="1">88.7</td><td align="left" colspan="1" rowspan="1">86.1</td><td align="left" colspan="1" rowspan="1">57.8</td><td align="left" colspan="1" rowspan="1">90.9</td><td align="left" colspan="1" rowspan="1">87.3</td><td align="left" colspan="1" rowspan="1">86.8</td><td align="left" colspan="1" rowspan="1">75.5</td><td align="left" colspan="1" rowspan="1">2.6</td><td align="left" colspan="1" rowspan="1">6.6</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOv12n</td><td align="left" colspan="1" rowspan="1">90.2</td><td align="left" colspan="1" rowspan="1">84.4</td><td align="left" colspan="1" rowspan="1">86.3</td><td align="left" colspan="1" rowspan="1">57.1</td><td align="left" colspan="1" rowspan="1">88.2</td><td align="left" colspan="1" rowspan="1">87.0</td><td align="left" colspan="1" rowspan="1">78.9</td><td align="left" colspan="1" rowspan="1">73.3</td><td align="left" colspan="1" rowspan="1">2.6</td><td align="left" colspan="1" rowspan="1">6.5</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOv11n</td><td align="left" colspan="1" rowspan="1">89.5</td><td align="left" colspan="1" rowspan="1">86.1</td><td align="left" colspan="1" rowspan="1">85.8</td><td align="left" colspan="1" rowspan="1">52.3</td><td align="left" colspan="1" rowspan="1">89.2</td><td align="left" colspan="1" rowspan="1">85.1</td><td align="left" colspan="1" rowspan="1">85.6</td><td align="left" colspan="1" rowspan="1">64.6</td><td align="left" colspan="1" rowspan="1">2.6</td><td align="left" colspan="1" rowspan="1">6.5</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOv10n</td><td align="left" colspan="1" rowspan="1">86.8</td><td align="left" colspan="1" rowspan="1">86.6</td><td align="left" colspan="1" rowspan="1">85.9</td><td align="left" colspan="1" rowspan="1">49.4</td><td align="left" colspan="1" rowspan="1">87.4</td><td align="left" colspan="1" rowspan="1">84.5</td><td align="left" colspan="1" rowspan="1">85.8</td><td align="left" colspan="1" rowspan="1">76.8</td><td align="left" colspan="1" rowspan="1">2.3</td><td align="left" colspan="1" rowspan="1">6.7</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOv9t</td><td align="left" colspan="1" rowspan="1">85.3</td><td align="left" colspan="1" rowspan="1">81.5</td><td align="left" colspan="1" rowspan="1">90.4</td><td align="left" colspan="1" rowspan="1">45.5</td><td align="left" colspan="1" rowspan="1">86.9</td><td align="left" colspan="1" rowspan="1">80.8</td><td align="left" colspan="1" rowspan="1">79.5</td><td align="left" colspan="1" rowspan="1">59.6</td><td align="left" colspan="1" rowspan="1">2.0</td><td align="left" colspan="1" rowspan="1">7.7</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOv8n</td><td align="left" colspan="1" rowspan="1">91.3</td><td align="left" colspan="1" rowspan="1">89.3</td><td align="left" colspan="1" rowspan="1">87.3</td><td align="left" colspan="1" rowspan="1">58.3</td><td align="left" colspan="1" rowspan="1">90.2</td><td align="left" colspan="1" rowspan="1">87.1</td><td align="left" colspan="1" rowspan="1">86.9</td><td align="left" colspan="1" rowspan="1">75.3</td><td align="left" colspan="1" rowspan="1">3.2</td><td align="left" colspan="1" rowspan="1">8.7</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec21"><title>Visual presentation</title><sec id="Sec22"><title>Bounding box comparison of defect detection</title><p id="Par49">Figure <xref rid="Fig11" ref-type="fig">11</xref> provides a comparative visualization of the predicted defect regions generated by PV-YOLOv12n and YOLOv12n. The figure showcases detection results on two different datasets: subfigure (a) represents the PVELAD dataset, while subfigure (b) corresponds to the Roboflow dataset. Each pair of images compares the defect localization performance of PV-YOLOv12n (left) and YOLOv12n (right). From the figure, it is evident that PV-YOLOv12n produces more precise and consistent defect detection results. The bounding boxes generated by PV-YOLOv12n appear to be more refined and focused, indicating an improved ability to localize defect-prone regions accurately. In contrast, YOLOv12n exhibits slightly more variation in its predictions, with some bounding boxes being less tightly fitted around defects. Additionally, in both datasets, PV-YOLOv12n shows better detection of fine-grained defects, particularly in complex defect patterns. This highlights the advantages of the proposed modifications in PV-YOLOv12n, leading to enhanced defect detection reliability in real-world photovoltaic applications.<fig id="Fig11" position="float" orientation="portrait"><label>Fig. 11</label><caption><p>Comparative Visualization of PV-YOLOv12n and YOLOv12n Predictions on (<bold>a</bold>) PVELAD and (<bold>b</bold>) Roboflow Datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO11" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig11_HTML.jpg"/></fig></p></sec><sec id="Sec23"><title> Heatmap analysis of defect detection</title><p id="Par50">To further illustrate the improvements in defect localization, Figure <xref rid="Fig12" ref-type="fig">12</xref> presents the heatmap visualizations of PV-YOLOv12n and YOLOv12n on both the PVELAD and Roboflow datasets. These heatmaps highlight the activation intensity of each model, revealing which regions are most influential in their predictions. In Subfigure (a) (PVELAD dataset), PV-YOLOv12n demonstrates stronger and more concentrated activations around defect-prone areas, particularly for structural cracks and horizontal dislocations. The red-highlighted regions in the heatmap indicate a higher confidence in defect localization, whereas YOLOv12n exhibits more diffused activations, suggesting slightly less precise defect identification. PV-YOLOv12n&#8217;s heatmaps are more compact and focused, leading to improved defect boundary detection. Similarly, in Subfigure (b) (Roboflow dataset), PV-YOLOv12n displays higher activation intensity over solar cell defects, particularly in areas with complex degradation patterns. YOLOv12n, while detecting similar defect regions, shows spread-out activations, indicating lower confidence in certain predictions. The ability of PV-YOLOv12n to highlight fine-grained defect patterns reinforces its advantage in detecting subtle and intricate defects.<fig id="Fig12" position="float" orientation="portrait"><label>Fig. 12</label><caption><p>Heatmap Comparison of PV-YOLOv12n and YOLOv12n on (<bold>a</bold>) PVELAD and (<bold>b</bold>) Roboflow Datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO12" position="float" orientation="portrait" xlink:href="41598_2025_13956_Fig12_HTML.jpg"/></fig></p></sec></sec><sec id="Sec24"><title>Discussion</title><p id="Par51">The integration of the A2C2f module into YOLOv12 significantly enhanced photovoltaic (PV) defect detection accuracy while maintaining computational efficiency. PV-YOLOv12n achieved 0.91 mAP@50 on both the PVEL-AD and Roboflow datasets, outperforming the baseline YOLOv12n (0.90 and 0.88 mAP@50, respectively) and other YOLO variants (Table 5). The module&#8217;s Area Attention (A2) mechanism prioritized critical defect regions, improving localization of cracks, dislocations, and dust accumulation, while the Residual ELAN (R-ELAN) component stabilized gradient flow during training. Key contributions include:<list list-type="bullet"><list-item><p id="Par52">Precision-Recall balance: superior F1-scores (Figures <xref rid="Fig10" ref-type="fig">10</xref> and <xref rid="Fig11" ref-type="fig">11</xref>) and faster convergence (Figures <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig7" ref-type="fig">7</xref>) demonstrated robust defect sensitivity and reduced false positives.</p></list-item><list-item><p id="Par53">Lightweight architecture: with only 2.6M parameters and 6.6 GFLOPs, PV-YOLOv12n is ideal for edge devices, enabling real-time drone-based inspections.</p></list-item><list-item><p id="Par54">Generalization: effective performance across 8 defect categories in PVEL-AD dataset and diverse real-world anomalies in Roboflow dataset.</p></list-item></list>A crucial aspect of our findings is the consistent performance improvement of PV-YOLOv12n across two fundamentally different datasets: the electroluminescence (EL) PVEL-AD dataset and the RGB-based Roboflow dataset. This success across modalities is not coincidental but is rooted in the core functionality of the A2C2f module. The module addresses a challenge common to both data types: effectively distinguishing relevant defect features from complex or noisy background information. For the PVEL-AD dataset, EL images feature low-contrast, monochromatic visuals where defects like &#8217;crack&#8217; and &#8217;dislocation&#8217; manifest as subtle variations in luminance. In this context, the Area Attention (A2) mechanism excels by learning to amplify these faint structural patterns while suppressing the uniform, yet potentially noisy, background of the solar cell. This allows the model to achieve a more focused and precise localization of defects defined by their structure rather than by sharp color contrast. Conversely, the Roboflow dataset consists of RGB images where defects such as &#8217;bird drop&#8217; and &#8217;dusty&#8217; are characterized by complex textures, irregular shapes, and occluding color patterns. Here, the same Area Attention mechanism proves effective by learning to prioritize these anomalous textures irrespective of their varied appearance over the regular, repeating grid lines and reflective surfaces of the panel. The model&#8217;s improved ability to distinguish these complex anomalies demonstrates the attention mechanism&#8217;s power to focus on salient regions in visually rich environments. Furthermore, the R-ELAN component&#8217;s role in stabilizing gradient flow provides a data-agnostic benefit that ensures more robust training convergence for the distinct feature distributions of both datasets. Therefore, the A2C2f module&#8217;s enhancement is not tailored to a specific data type but to the fundamental computer vision task of figure-ground separation, which is why it successfully boosts performance on both EL and RGB imagery.</p><p id="Par55">The inherent strength of utilizing Electroluminescence (EL) imaging for photovoltaic (PV) cell anomaly detection stems from the direct correspondence between observed visual patterns and underlying physical or electrical phenomena within the solar cell. Our model&#8217;s architecture, alongside the PVEL-AD dataset&#8217;s defect categorization, is specifically designed to leverage this intrinsic relationship, ensuring high industrial interpretability and facilitating actionable quality control decisions. Each anomaly detected by the model, such as &#8217;Linear Crack,&#8217; &#8217;Star Crack,&#8217; &#8217;Finger Interruption,&#8217; &#8217;Black Core,&#8217; &#8217;Misalignment,&#8217; &#8217;Thick Line,&#8217; and &#8217;Short Circuit,&#8217; represents a distinct visual signature of a specific physical malfunction. For instance, detected cracks and thick lines directly indicate physical wafer fractures causing interrupted current flow, appearing as dark lines. A &#8217;Finger Interruption&#8217; signifies a broken metal contact, visually distinct as a break in the finger grid. &#8217;Misalignments&#8217; manifest as elongated dark regions due to stress-induced structural imperfections. A &#8217;Black Core&#8217; indicates localized poor silicon quality from non-radiative recombination, presenting as a dark area. Finally, the identification of a &#8217;Short Circuit&#8217; points to an anomalously bright spot, marking a low-resistance current path indicative of potential hotspots.</p></sec><sec id="Sec25"><title>Comparison with recent works</title><p id="Par56">To comprehensively evaluate the performance of our proposed PV-YOLOv12 model, we conduct a detailed comparison with state-of-the-art defect detection methods, with the results summarized in Table&#160;<xref rid="Tab11" ref-type="table">11</xref>. This analysis benchmarks our model against others across different datasets, image modalities, and class complexities. From this comparison, we derive several key observations that underscore the strengths of our approach. Our model, PV-YOLOv12n, achieves a robust mAP@50 of 91.0%. A defining strength of our method is its exceptional generalization capability, a feature not demonstrated by most competing models. PV-YOLOv12n maintains this high accuracy across two distinct and challenging public datasets: the Roboflow Universe dataset, comprising 6,493 optical (RGB) images, and the PVEL-AD dataset, with 5,589 Electroluminescence (EL) images. This consistent performance across different imaging modalities (optical vs. EL) and data distributions proves the model&#8217;s versatility and resilience. In contrast, many state-of-the-art methods are highly specialized. For example, S-YOLOv5<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> and GBH-YOLOv5<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> achieve higher mAP scores (98.1% and 97.8%, respectively), but their performance is validated on smaller, self-constructed datasets (3,360 thermal images and 1,108 grayscale images, respectively) and they are not proven to generalize to other image types. Furthermore, our model delivers this strong performance while being an intrinsically efficient &#8220;nano&#8221; architecture (PV-YOLOv12n). When compared to models tested on the same Roboflow dataset, such as YOLOv11m<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> (93.4%), our model&#8217;s slightly lower mAP is an intentional trade-off for significantly reduced computational complexity, making it ideal for real-time, on-drone deployment. The ability to achieve 91.0% mAP on the complex PVEL-AD dataset with 8 defect classes further highlights its effectiveness, especially when compared to models like MRA-YOLOv8<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, which scores a similar 91.7% on the same dataset but is a larger architecture. The comparative analysis confirms that PV-YOLOv12 stands out not merely on accuracy but on its holistic value. While some specialized models may report higher metrics on niche, controlled datasets, our approach demonstrates a superior combination of high accuracy, remarkable generalization across diverse and large-scale public datasets (Optical and EL), and computational efficiency. This positions PV-YOLOv12 as a more practical, scalable, and robust solution for real-world automated PV inspection systems.<table-wrap id="Tab11" position="float" orientation="portrait"><label>Table 11</label><caption><p>Performance Comparison of PV-YOLOv12 with Recent Methods for Solar Panel Defect Detection Across Multiple Datasets (mAP@50 %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Author</bold></th><th align="left" colspan="1" rowspan="1"><bold>Year</bold></th><th align="left" colspan="1" rowspan="1"><bold>Dataset</bold></th><th align="left" colspan="1" rowspan="1"><bold>Image Type</bold></th><th align="left" colspan="1" rowspan="1"><bold>Classes</bold></th><th align="left" colspan="1" rowspan="1"><bold>Images</bold></th><th align="left" colspan="1" rowspan="1"><bold>Method</bold></th><th align="left" colspan="1" rowspan="1"><bold>Accuracy (%)</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Zheng et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td align="left" colspan="1" rowspan="1">2022</td><td align="left" colspan="1" rowspan="1">Self-Constructed</td><td align="left" colspan="1" rowspan="1">Infrared (Thermal)</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">3360</td><td align="left" colspan="1" rowspan="1">S-YOLOv5</td><td align="left" colspan="1" rowspan="1">98.1</td></tr><tr><td align="left" colspan="1" rowspan="1">Zhang &amp; Yin<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td align="left" colspan="1" rowspan="1">2022</td><td align="left" colspan="1" rowspan="1">Self-Constructed</td><td align="left" colspan="1" rowspan="1">EL</td><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">2534</td><td align="left" colspan="1" rowspan="1">Improved YOLOv5</td><td align="left" colspan="1" rowspan="1">89.6</td></tr><tr><td align="left" rowspan="5" colspan="1">Chen et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td><td align="left" rowspan="5" colspan="1">2023</td><td align="left" rowspan="5" colspan="1">PVEL-AD (EL2021)</td><td align="left" rowspan="5" colspan="1">EL</td><td align="left" rowspan="5" colspan="1">3</td><td align="left" rowspan="5" colspan="1">5246</td><td align="left" colspan="1" rowspan="1">SSD</td><td align="left" colspan="1" rowspan="1">78.0</td></tr><tr><td align="left" colspan="1" rowspan="1">Faster R-CNN</td><td align="left" colspan="1" rowspan="1">72.27</td></tr><tr><td align="left" colspan="1" rowspan="1">RCA-Faster R-CNN</td><td align="left" colspan="1" rowspan="1">83.29</td></tr><tr><td align="left" colspan="1" rowspan="1">RetinaNet</td><td align="left" colspan="1" rowspan="1">84.53</td></tr><tr><td align="left" colspan="1" rowspan="1">MCSAM</td><td align="left" colspan="1" rowspan="1">87.14</td></tr><tr><td align="left" colspan="1" rowspan="1">Li et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></td><td align="left" colspan="1" rowspan="1">2023</td><td align="left" colspan="1" rowspan="1">PV-Multi-Defect</td><td align="left" colspan="1" rowspan="1">Grayscale</td><td align="left" colspan="1" rowspan="1">5</td><td align="left" colspan="1" rowspan="1">1108</td><td align="left" colspan="1" rowspan="1">GBH-YOLOv5</td><td align="left" colspan="1" rowspan="1">97.8</td></tr><tr><td align="left" rowspan="7" colspan="1">Qu et al.<sup><xref ref-type="bibr" rid="CR41">41</xref></sup></td><td align="left" rowspan="7" colspan="1">2024</td><td align="left" rowspan="7" colspan="1">PVEL-AD</td><td align="left" rowspan="7" colspan="1">EL</td><td align="left" rowspan="7" colspan="1">6</td><td align="left" rowspan="7" colspan="1">5786</td><td align="left" colspan="1" rowspan="1">PicoDet</td><td align="left" colspan="1" rowspan="1">77.5</td></tr><tr><td align="left" colspan="1" rowspan="1">Cascade-RCNN</td><td align="left" colspan="1" rowspan="1">89.3</td></tr><tr><td align="left" colspan="1" rowspan="1">Sparse-RCNN</td><td align="left" colspan="1" rowspan="1">84.0</td></tr><tr><td align="left" colspan="1" rowspan="1">Deformable-DETR</td><td align="left" colspan="1" rowspan="1">84.2</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO-X</td><td align="left" colspan="1" rowspan="1">89.6</td></tr><tr><td align="left" colspan="1" rowspan="1">Faster-RCNN</td><td align="left" colspan="1" rowspan="1">82.4</td></tr><tr><td align="left" colspan="1" rowspan="1">MS-DCBFE</td><td align="left" colspan="1" rowspan="1">92.0</td></tr><tr><td align="left" colspan="1" rowspan="1">Pan et al.<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></td><td align="left" colspan="1" rowspan="1">2024</td><td align="left" colspan="1" rowspan="1">PVEL-AD</td><td align="left" colspan="1" rowspan="1">EL</td><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">4500</td><td align="left" colspan="1" rowspan="1">YOLO-ACF</td><td align="left" colspan="1" rowspan="1">92.6</td></tr><tr><td align="left" rowspan="2" colspan="1">Wang et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td align="left" rowspan="2" colspan="1">2025</td><td align="left" colspan="1" rowspan="1">PVEL-AD</td><td align="left" colspan="1" rowspan="1">EL</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">&#8212;</td><td align="left" colspan="1" rowspan="1">MRA-YOLOv8</td><td align="left" colspan="1" rowspan="1">91.7</td></tr><tr><td align="left" colspan="1" rowspan="1">SPDI</td><td align="left" colspan="1" rowspan="1">EL/Optical</td><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">&#8212;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">69.3</td></tr><tr><td align="left" rowspan="2" colspan="1">Khanam et al.<sup><xref ref-type="bibr" rid="CR40">40</xref></sup></td><td align="left" rowspan="2" colspan="1">2025</td><td align="left" rowspan="2" colspan="1">Roboflow Universe</td><td align="left" rowspan="2" colspan="1">Optical/RGB</td><td align="left" rowspan="2" colspan="1">4</td><td align="left" rowspan="2" colspan="1">6493</td><td align="left" colspan="1" rowspan="1">YOLOv11m</td><td align="left" colspan="1" rowspan="1">93.4</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOv8</td><td align="left" colspan="1" rowspan="1">92.3</td></tr><tr><td align="left" colspan="1" rowspan="1">Xu et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td align="left" colspan="1" rowspan="1">2025</td><td align="left" colspan="1" rowspan="1">Self-Constructed</td><td align="left" colspan="1" rowspan="1">PL</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">4500</td><td align="left" colspan="1" rowspan="1">YOLOv5</td><td align="left" colspan="1" rowspan="1">91.5</td></tr><tr><td align="left" colspan="1" rowspan="1">Ghahremani et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td align="left" colspan="1" rowspan="1">2025</td><td align="left" colspan="1" rowspan="1">Self-Constructed</td><td align="left" colspan="1" rowspan="1">Thermal</td><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">316</td><td align="left" colspan="1" rowspan="1">YOLOv11-X</td><td align="left" colspan="1" rowspan="1">92.7</td></tr><tr><td align="left" rowspan="2" colspan="1"><bold>Proposed</bold></td><td align="left" rowspan="2" colspan="1"><bold>2025</bold></td><td align="left" colspan="1" rowspan="1"><bold>Roboflow Universe</bold></td><td align="left" colspan="1" rowspan="1"><bold>Optical/RGB</bold></td><td align="left" colspan="1" rowspan="1"><bold>4</bold></td><td align="left" colspan="1" rowspan="1"><bold>6493</bold></td><td align="left" rowspan="2" colspan="1"><bold>PV-YOLOv12n</bold></td><td align="left" colspan="1" rowspan="1"><bold>91.0</bold></td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>PVEL-AD</bold></td><td align="left" colspan="1" rowspan="1"><bold>EL</bold></td><td align="left" colspan="1" rowspan="1"><bold>8</bold></td><td align="left" colspan="1" rowspan="1"><bold>5589</bold></td><td align="left" colspan="1" rowspan="1"><bold>91.0</bold></td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec26"><title>Conclusion and future work</title><p id="Par57">Timely and accurate defect detection in photovoltaic (PV) systems is essential to ensure sustainable and reliable performance. This study presents PV-YOLOv12, an advanced framework for photovoltaic defect detection, achieving state-of-the-art performance through the novel A2C2f module. By synergizing Area Attention (A2) for localized defect prioritization and Residual ELAN for stabilized gradient flow, the model attains 0.91 mAP@50 across diverse datasets while maintaining computational efficiency (2.6M parameters, 6.6 GFLOPs). PV-YOLOv12n demonstrates exceptional precision in identifying both environmental anomalies (e.g., cracks, dust, bird droppings) and critical manufacturing defects such as dislocation faults and microcracks positioning it as a transformative tool for real-time quality control in solar panel production and field inspections. Its ability to reduce false positives while localizing subtle flaws underscores its potential to streamline manufacturing processes and minimize post-production waste. Looking ahead, future efforts will focus on enhancing the framework&#8217;s versatility and robustness. This includes integrating multi-modal data such as thermal and electroluminescence imaging to uncover electrical faults undetectable in RGB spectra, alongside optimizing the model for ultra-low-power edge devices through neural architecture search and quantization. Further validation under extreme environmental conditions sandstorms, heavy rain, and long-term panel degradation will ensure reliability in real-world deployments. Collaborations with industry partners will refine scalability and latency metrics for embedded systems, while advancements in explainable AI will foster technician trust through interpretable defect diagnostics. While our proposed model, PV-YOLOv12n, demonstrates strong performance and generalization across both electroluminescence and visual-light datasets, we recognize the value of a more granular analysis of our data augmentation strategy. As such, a key direction for future work will be to conduct a detailed ablation study on the individual impact of each augmentation technique for each dataset. This would involve systematically training and evaluating the model with different combinations of augmentations to quantify the specific contribution of each technique to the final performance on both the PVEL-AD and Roboflow datasets. Such an analysis would not only provide deeper insights into the model&#8217;s learning process but also enable the development of more tailored, dataset-specific augmentation strategies to further enhance detection accuracy. Additionally, we plan to explore the application of these optimized models on other emerging PV technologies and under an even wider range of environmental conditions to further validate their real-world applicability.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The researchers wish to extend their sincere gratitude to the Deanship of Scientific Research at the Islamic University of Madinah (KSA) for the support provided to the Post-Publishing Program.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>A.M: Methodology, Conceptualization, Software, Coding, Validation, Writing&#8212;Original draft. Y.N: Methodology, Reviewing and Editing, Supervision, Validation. B.A: Methodology, Reviewing and Editing, Supervision, Validation. A.T: Reviewing and Editing, Validation. E.B: Methodology, Reviewing and Editing, Validation. A.R: Reviewing and Editing. Y.A: Reviewing and Editing. M.B: Reviewing and Editing.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All data for the experiments used in this study are available via the web. Hyperlinks at the bottom provide links to the datasets. PVEL-AD dataset: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://github.com/binyisu/PVEL-AD">https://github.com/binyisu/PVEL-AD</ext-link>. Roboflow Universe: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://universe.roboflow.com/susan-ifblr/panel-solar-bw945">https://universe.roboflow.com/susan-ifblr/panel-solar-bw945</ext-link>.</p></notes><notes><title>Declarations</title><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par58">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hern&#225;ndez-Callejo</surname><given-names>L</given-names></name><name name-style="western"><surname>Gallardo-Saavedra</surname><given-names>S</given-names></name><name name-style="western"><surname>Alonso-G&#243;mez</surname><given-names>V</given-names></name></person-group><article-title>A review of photovoltaic systems: Design, operation and maintenance</article-title><source>Sol. Energy</source><year>2019</year><volume>188</volume><fpage>426</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1016/j.solener.2019.06.017</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Hern&#225;ndez-Callejo, L., Gallardo-Saavedra, S. &amp; Alonso-G&#243;mez, V. A review of photovoltaic systems: Design, operation and maintenance. <italic toggle="yes">Sol. Energy</italic><bold>188</bold>, 426&#8211;440 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Zereg, K. <italic toggle="yes">et al.</italic> Dust impact on concentrated solar power: A review. <italic toggle="yes">Environ. Eng. Res.</italic>&#160;<bold>27</bold> (2022).</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alnasser</surname><given-names>T</given-names></name><name name-style="western"><surname>Mahdy</surname><given-names>A</given-names></name><name name-style="western"><surname>Abass</surname><given-names>K</given-names></name><name name-style="western"><surname>Chaichan</surname><given-names>M</given-names></name><name name-style="western"><surname>Kazem</surname><given-names>H</given-names></name></person-group><article-title>Impact of dust ingredient on photovoltaic performance: An experimental study</article-title><source>Sol. Energy</source><year>2020</year><volume>195</volume><fpage>651</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1016/j.solener.2019.12.008</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Alnasser, T., Mahdy, A., Abass, K., Chaichan, M. &amp; Kazem, H. Impact of dust ingredient on photovoltaic performance: An experimental study. <italic toggle="yes">Sol. Energy</italic><bold>195</bold>, 651&#8211;659 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gupta</surname><given-names>V</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>M</given-names></name><name name-style="western"><surname>Pachauri</surname><given-names>R</given-names></name><name name-style="western"><surname>Babu</surname><given-names>K</given-names></name></person-group><article-title>Comprehensive review on effect of dust on solar photovoltaic system and mitigation techniques</article-title><source>Sol. Energy</source><year>2019</year><volume>191</volume><fpage>596</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1016/j.solener.2019.08.079</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Gupta, V., Sharma, M., Pachauri, R. &amp; Babu, K. Comprehensive review on effect of dust on solar photovoltaic system and mitigation techniques. <italic toggle="yes">Sol. Energy</italic><bold>191</bold>, 596&#8211;622 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bo&#353;njakovi&#263;</surname><given-names>M</given-names></name><name name-style="western"><surname>Santa</surname><given-names>R</given-names></name><name name-style="western"><surname>Crnac</surname><given-names>Z</given-names></name><name name-style="western"><surname>Bo&#353;njakovi&#263;</surname><given-names>T</given-names></name></person-group><article-title>Environmental impact of pv power systems</article-title><source>Sustainability</source><year>2023</year><volume>15</volume><fpage>11888</fpage><pub-id pub-id-type="doi">10.3390/su151511888</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Bo&#353;njakovi&#263;, M., Santa, R., Crnac, Z. &amp; Bo&#353;njakovi&#263;, T. Environmental impact of pv power systems. <italic toggle="yes">Sustainability</italic><bold>15</bold>, 11888 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hernandez</surname><given-names>RR</given-names></name><etal/></person-group><article-title>Environmental impacts of utility-scale solar energy</article-title><source>Renew. sustainable energy reviews</source><year>2014</year><volume>29</volume><fpage>766</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.rser.2013.08.041</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Hernandez, R. R. et al. Environmental impacts of utility-scale solar energy. <italic toggle="yes">Renew. sustainable energy reviews</italic><bold>29</bold>, 766&#8211;779 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Hijjawi, U., Lakshminarayana, S., Xu, T., Fierro, G. P. M. &amp; Rahman, M. A review of automated solar photovoltaic defect detection systems: Approaches, challenges, and future orientations. <italic toggle="yes">Sol. Energy</italic><bold>266</bold>, (2023).</mixed-citation></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Denz</surname><given-names>J</given-names></name><etal/></person-group><article-title>Defects and performance of si pv modules in the field-an analysis</article-title><source>Energy &amp; Environ. Sci.</source><year>2022</year><volume>15</volume><fpage>2180</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1039/D2EE00109H</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Denz, J. et al. Defects and performance of si pv modules in the field-an analysis. <italic toggle="yes">Energy &amp; Environ. Sci.</italic><bold>15</bold>, 2180&#8211;2199 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">K&#246;ntges, M. <italic toggle="yes">et al.</italic> Review of failures of photovoltaic modules. (2014).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Chen, S.-Y., Chiu, M.-F. &amp; Zou, X.-W. Real-time defect inspection of green coffee beans using nir snapshot hyperspectral imaging. <italic toggle="yes">Comput. Electron. Agric.</italic><bold>197</bold>, (2022).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Belabbaci, E.&#160;O. <italic toggle="yes">et al.</italic> Improving face kinship verification via tensor representation of multiple deep cnns features combined with 2ddwt histograms. <italic toggle="yes">Multimed. Tools Appl.</italic>&#160;1&#8211;24 (2025).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Constantin, A.-I. <italic toggle="yes">et al.</italic> Importance of preventive maintenance in solar energy systems and fault detection for solar panels based on thermal images. <italic toggle="yes">Electrotehnica, Electron. Autom.&#160;</italic><bold>71&#160;</bold>(2023).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">LeCun, Y., Bengio, Y. &amp; Hinton, G. Deep learning. <italic toggle="yes">Nature</italic><bold>521</bold>, 436&#8211;444 (2015).<pub-id pub-id-type="pmid">26017442</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/nature14539</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Szeliski</surname><given-names>R</given-names></name></person-group><source>Computer Vision: Algorithms and Applications</source><year>2022</year><publisher-name>Springer Nature</publisher-name></element-citation><mixed-citation id="mc-CR14" publication-type="book">Szeliski, R. <italic toggle="yes">Computer Vision: Algorithms and Applications</italic> (Springer Nature, 2022).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>X</given-names></name><name name-style="western"><surname>Sahoo</surname><given-names>D</given-names></name><name name-style="western"><surname>Hoi</surname><given-names>S</given-names></name></person-group><article-title>Recent advances in deep learning for object detection</article-title><source>Neurocomputing</source><year>2020</year><volume>396</volume><fpage>39</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2020.01.085</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Wu, X., Sahoo, D. &amp; Hoi, S. Recent advances in deep learning for object detection. <italic toggle="yes">Neurocomputing</italic><bold>396</bold>, 39&#8211;64 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Ultralytics. Detection tasks documentation. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://docs.ultralytics.com/tasks/detect/">https://docs.ultralytics.com/tasks/detect/</ext-link> (2024). Accessed on 31 December 2024.</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hussain</surname><given-names>M</given-names></name><name name-style="western"><surname>Khanam</surname><given-names>R</given-names></name></person-group><article-title>In-depth review of YOLOv1 to YOLOv10 variants for enhanced photovoltaic defect detection</article-title><source>Solar</source><year>2024</year><volume>4</volume><fpage>351</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.3390/solar4030016</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Hussain, M. &amp; Khanam, R. In-depth review of YOLOv1 to YOLOv10 variants for enhanced photovoltaic defect detection. <italic toggle="yes">Solar</italic><bold>4</bold>, 351&#8211;386 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Wang, Z., Zheng, P., Bahadir Kocer, B. &amp; Kovac, M. Drone-based solar cell inspection with autonomous deep learning. In <italic toggle="yes">Infrastructure Robotics: Methodologies, Robotic Systems and Applications</italic>, 337&#8211;365 (Wiley, Hoboken, NJ, USA, 2023).</mixed-citation></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cha</surname><given-names>Y-J</given-names></name><name name-style="western"><surname>Choi</surname><given-names>W</given-names></name><name name-style="western"><surname>B&#252;y&#252;k&#246;zt&#252;rk</surname><given-names>O</given-names></name></person-group><article-title>Deep learning-based crack damage detection using convolutional neural networks</article-title><source>Comput. Civ. Infrastructure Eng.</source><year>2017</year><volume>32</volume><fpage>361</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1111/mice.12263</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Cha, Y.-J., Choi, W. &amp; B&#252;y&#252;k&#246;zt&#252;rk, O. Deep learning-based crack damage detection using convolutional neural networks. <italic toggle="yes">Comput. Civ. Infrastructure Eng.</italic><bold>32</bold>, 361&#8211;378 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Imenes, A. <italic toggle="yes">et al.</italic> A deep learning approach for automated fault detection on solar modules using image composites. In <italic toggle="yes">Proceedings of the 2021 IEEE 48th Photovoltaic Specialists Conference (PVSC)</italic>, 1925&#8211;1930 (IEEE, Fort Lauderdale, FL, USA, 2021).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Teke, M., Ba&#351;eski, E., Ok, A., Y&#252;ksel, B. spsampsps &#350;enaras, &#199;. Multi-spectral false color shadow detection. In <italic toggle="yes">Proceedings of the ISPRS Conference on Photogrammetric Image Analysis</italic>, 109&#8211;119 (Springer, Munich, Germany, 2011).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Zou, J.-T. &amp; Rajveer, G. Drone-based solar panel inspection with 5g and ai technologies. In <italic toggle="yes">2022 8th International Conference on Applied System Innovation (ICASI)</italic>, 174&#8211;178 (IEEE, 2022).</mixed-citation></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Defect object detection algorithm for electroluminescence image defects of photovoltaic modules based on deep learning</article-title><source>Energy Sci. &amp; Eng.</source><year>2022</year><volume>10</volume><fpage>800</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1002/ese3.1056</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Meng, Z. et al. Defect object detection algorithm for electroluminescence image defects of photovoltaic modules based on deep learning. <italic toggle="yes">Energy Sci. &amp; Eng.</italic><bold>10</bold>, 800&#8211;813 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Li, L., Wang, Z. &amp; Zhang, T. Photovoltaic panel defect detection based on ghost convolution with BottleneckCSP and tiny target prediction head incorporating YOLOv5. <italic toggle="yes">arXiv preprint </italic>arXiv:2303.00886 (2023).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Girshick, R. Fast R-CNN. <italic toggle="yes">arXiv preprint</italic><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://arxiv.org/abs/1504.08083">arXiv:1504.08083</ext-link> (2015).</mixed-citation></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hong</surname><given-names>F</given-names></name><etal/></person-group><article-title>A novel framework on intelligent detection for module defects of PV plant combining the visible and infrared images</article-title><source>Sol. Energy</source><year>2022</year><volume>236</volume><fpage>406</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1016/j.solener.2022.03.018</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Hong, F. et al. A novel framework on intelligent detection for module defects of PV plant combining the visible and infrared images. <italic toggle="yes">Sol. Energy</italic><bold>236</bold>, 406&#8211;416 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>M</given-names></name><name name-style="western"><surname>Yin</surname><given-names>L</given-names></name></person-group><article-title>Solar cell surface defect detection based on improved YOLO v5</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>80804</fpage><lpage>80815</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3195901</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Zhang, M. &amp; Yin, L. Solar cell surface defect detection based on improved YOLO v5. <italic toggle="yes">IEEE Access</italic><bold>10</bold>, 80804&#8211;80815 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Lightweight hot-spot fault detection model of photovoltaic panels in UAV remote-sensing image</article-title><source>Sensors</source><year>2022</year><volume>22</volume><fpage>4617</fpage><pub-id pub-id-type="doi">10.3390/s22124617</pub-id><pub-id pub-id-type="pmid">35746399</pub-id><pub-id pub-id-type="pmcid">PMC9231204</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Zheng, Q. et al. Lightweight hot-spot fault detection model of photovoltaic panels in UAV remote-sensing image. <italic toggle="yes">Sensors</italic><bold>22</bold>, 4617 (2022).<pub-id pub-id-type="pmid">35746399</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s22124617</pub-id><pub-id pub-id-type="pmcid">PMC9231204</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Hassan, S. &amp; Dhimish, M. Enhancing solar photovoltaic modules quality assurance through convolutional neural network-aided automated defect detection. <italic toggle="yes">Renew. Energy</italic><bold>219</bold>, (2023).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Cao, Y. et al. Improved YOLOv8-GD deep learning model for defect detection in electroluminescence images of solar photovoltaic modules. <italic toggle="yes">Eng. Appl. Artif. Intell.</italic><bold>131</bold>, (2024).</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ghahremani</surname><given-names>A</given-names></name><name name-style="western"><surname>Adams</surname><given-names>SD</given-names></name><name name-style="western"><surname>Norton</surname><given-names>M</given-names></name><name name-style="western"><surname>Khoo</surname><given-names>SY</given-names></name><name name-style="western"><surname>Kouzani</surname><given-names>AZ</given-names></name></person-group><article-title>Detecting defects in solar panels using the yolo v10 and v11 algorithms</article-title><source>Electronics</source><year>2025</year><volume>14</volume><fpage>344</fpage><pub-id pub-id-type="doi">10.3390/electronics14020344</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Ghahremani, A., Adams, S. D., Norton, M., Khoo, S. Y. &amp; Kouzani, A. Z. Detecting defects in solar panels using the yolo v10 and v11 algorithms. <italic toggle="yes">Electronics</italic><bold>14</bold>, 344 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>N</given-names></name><etal/></person-group><article-title>Mra-yolov8: A network enhancing feature extraction ability for photovoltaic cell defects</article-title><source>Sensors</source><year>2025</year><volume>25</volume><fpage>1542</fpage><pub-id pub-id-type="doi">10.3390/s25051542</pub-id><pub-id pub-id-type="pmid">40096412</pub-id><pub-id pub-id-type="pmcid">PMC11902818</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Wang, N. et al. Mra-yolov8: A network enhancing feature extraction ability for photovoltaic cell defects. <italic toggle="yes">Sensors</italic><bold>25</bold>, 1542 (2025).<pub-id pub-id-type="pmid">40096412</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s25051542</pub-id><pub-id pub-id-type="pmcid">PMC11902818</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>G</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J</given-names></name><name name-style="western"><surname>Gong</surname><given-names>W</given-names></name><name name-style="western"><surname>Teng</surname><given-names>J</given-names></name></person-group><article-title>Solar cell defects detection based on photoluminescence images and upgraded YOLOv5 model</article-title><source>J. Eng.</source><year>2025</year><volume>2025</volume><fpage>8397362</fpage><pub-id pub-id-type="doi">10.1155/je/8397362</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Xu, G., Huang, J., Gong, W. &amp; Teng, J. Solar cell defects detection based on photoluminescence images and upgraded YOLOv5 model. <italic toggle="yes">J. Eng.</italic><bold>2025</bold>, 8397362 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>A</given-names></name><name name-style="western"><surname>Li</surname><given-names>X</given-names></name><name name-style="western"><surname>Jing</surname><given-names>H</given-names></name><name name-style="western"><surname>Hong</surname><given-names>C</given-names></name><name name-style="western"><surname>Li</surname><given-names>M</given-names></name></person-group><article-title>Anomaly detection algorithm for photovoltaic cells based on lightweight multi-channel spatial attention mechanism</article-title><source>Energies</source><year>2023</year><volume>16</volume><fpage>1619</fpage><pub-id pub-id-type="doi">10.3390/en16041619</pub-id></element-citation><mixed-citation id="mc-CR34" publication-type="journal">Chen, A., Li, X., Jing, H., Hong, C. &amp; Li, M. Anomaly detection algorithm for photovoltaic cells based on lightweight multi-channel spatial attention mechanism. <italic toggle="yes">Energies</italic><bold>16</bold>, 1619 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Akram, M. W. et al. Advancing photovoltaic cells defect detection in electroluminescence images through exploring multiple object detectors. <italic toggle="yes">Sol. Energy Mater. Sol. Cells</italic><bold>292</bold>, (2025).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Tian, Y., Ye, Q. &amp; Doermann, D. YOLOv12: Attention-centric real-time object detectors. <italic toggle="yes">arXiv preprint </italic>arXiv:2502.12524 (2025).</mixed-citation></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>B</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H</given-names></name></person-group><article-title>PVEL-AD: A large-scale open-world dataset for photovoltaic cell anomaly detection</article-title><source>IEEE Transactions on Ind. Informatics</source><year>2022</year><volume>19</volume><fpage>404</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1109/TII.2022.3162846</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Su, B., Zhou, Z. &amp; Chen, H. PVEL-AD: A large-scale open-world dataset for photovoltaic cell anomaly detection. <italic toggle="yes">IEEE Transactions on Ind. Informatics</italic><bold>19</bold>, 404&#8211;413 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Susan. Panel solar dataset. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://universe.roboflow.com/susan-ifblr/panel-solar-bw945">https://universe.roboflow.com/susan-ifblr/panel-solar-bw945</ext-link> (2024). Accessed on 22 November 2024.</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Revaud, J., Almaz&#225;n, J., Rezende, R. &amp; de Souza, C. Learning with average precision: Training image retrieval with a listwise loss. In <italic toggle="yes">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</italic>, 5107&#8211;5116 (Seoul, Republic of Korea, 2019).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Khanam, R., Asghar, T. &amp; Hussain, M. Comparative performance evaluation of yolov5, yolov8, and yolov11 for solar panel defect detection. In <italic toggle="yes">Solar</italic>, vol.&#160;5, 6 (MDPI, 2025).</mixed-citation></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qu</surname><given-names>Z</given-names></name><etal/></person-group><article-title>A photovoltaic cell defect detection model capable of topological knowledge extraction</article-title><source>Sci. Reports</source><year>2024</year><volume>14</volume><fpage>21904</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-72717-0</pub-id><pub-id pub-id-type="pmcid">PMC11413218</pub-id><pub-id pub-id-type="pmid">39300209</pub-id></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Qu, Z. et al. A photovoltaic cell defect detection model capable of topological knowledge extraction. <italic toggle="yes">Sci. Reports</italic><bold>14</bold>, 21904 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-72717-0</pub-id><pub-id pub-id-type="pmcid">PMC11413218</pub-id><pub-id pub-id-type="pmid">39300209</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>W</given-names></name><etal/></person-group><article-title>Enhanced photovoltaic panel defect detection via adaptive complementary fusion in yolo-acf</article-title><source>Sci. Reports</source><year>2024</year><volume>14</volume><fpage>26425</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-75772-9</pub-id><pub-id pub-id-type="pmcid">PMC11531521</pub-id><pub-id pub-id-type="pmid">39488574</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Pan, W. et al. Enhanced photovoltaic panel defect detection via adaptive complementary fusion in yolo-acf. <italic toggle="yes">Sci. Reports</italic><bold>14</bold>, 26425 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-75772-9</pub-id><pub-id pub-id-type="pmcid">PMC11531521</pub-id><pub-id pub-id-type="pmid">39488574</pub-id></mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>