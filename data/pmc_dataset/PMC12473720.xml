<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473720</article-id><article-id pub-id-type="pmcid-ver">PMC12473720.1</article-id><article-id pub-id-type="pmcaid">12473720</article-id><article-id pub-id-type="pmcaiid">12473720</article-id><article-id pub-id-type="pmid">41012991</article-id><article-id pub-id-type="doi">10.3390/s25185753</article-id><article-id pub-id-type="publisher-id">sensors-25-05753</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Lightweight Track Feature Detection Algorithm Based on Element Multiplication and Extended Path Aggregation Networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Qiu</surname><given-names initials="H">Hong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05753" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yang</surname><given-names initials="D">Dayong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05753" ref-type="aff">1</xref><xref rid="c1-sensors-25-05753" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Cao</surname><given-names initials="J">Juanhua</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-05753" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ming</surname><given-names initials="J">Jingqiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05753" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jiang</surname><given-names initials="K">Kun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af3-sensors-25-05753" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="W">Weijun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05753" ref-type="aff">1</xref></contrib></contrib-group><aff id="af1-sensors-25-05753"><label>1</label>School of Advanced Manufacturing, Nanchang University, Nanchang 330031, China</aff><aff id="af2-sensors-25-05753"><label>2</label>Jiangxi Technical College of Manufacturing, Nanchang 330031, China</aff><aff id="af3-sensors-25-05753"><label>3</label>Jiangxi Everbright Measurement and Control Technology Co., Ltd., Nanchang 330031, China</aff><author-notes><corresp id="c1-sensors-25-05753"><label>*</label>Correspondence: <email>dayongyang@ncu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>16</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5753</elocation-id><history><date date-type="received"><day>25</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>30</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>09</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>16</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05753.pdf"/><abstract><p>Aiming at the problems of excessive computational load, insufficient real-time performance, and an excessive amount of model parameters in track inspection, this paper proposes a lightweight track feature detection module (YOLO-LWTD) based on YOLO11n: first, the StarNet module is integrated into the backbone network, and its elemental multiplication operation is utilized to enhance the feature characterization capability; second, in the neck part, a lightweight extended path aggregation network reconstructs the feature pyramid information flow paths by combining with the C3K2-Light module to enhance the efficiency of the multi-scale feature fusion; finally, in the head part, a lighter and more efficient detection header, Detect-LADH, is used to reduce the feature decoding complexity. Experimental validation showed that the improved model outperforms the benchmark model in precision, recall, and mean average precision (MAP) by 0.5%, 2.0%, and 0.8%, respectively, with an inference speed of 163 FPS (a 38.1% improvement). The model volume is compressed to 1.5 MB (a 71.1% lightweight rate). This provides an energy-efficient solution for lightweight track detection tasks geared towards embedded deployment or real-time processing.</p></abstract><kwd-group><kwd>track inspection</kwd><kwd>YOLO11</kwd><kwd>elemental multiplication</kwd><kwd>lightweight network</kwd></kwd-group><funding-group><award-group><funding-source>National Key Research and Development Program of China</funding-source><award-id>2022YFB2602905-02</award-id></award-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>52068052</award-id></award-group><funding-statement>This study was funded by the National Key Research and Development Program of China (2022YFB2602905-02) and the National Natural Science Foundation of China (52068052).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05753"><title>1. Introduction</title><p>As the demand for intelligent operation and maintenance of rail transportation continues to increase, the importance of track inspection is becoming increasingly prominent. The track inspection system describes the geometric position parameters of the track with the running mileage as the independent variable; however, under the influence of cumulative error, the mileage of the geometric deviation position marking in long-distance measurement may have a significant deviation from the actual position. To systematically monitor the condition of key components, such as sleepers, fasteners, roadbeds, and turnouts, the detection accuracy of track inspection is significantly constrained by the cumulative mileage error, which directly leads to deviations in the condition assessment and spatial localization inaccuracy. It significantly affects the reliability of decision-making regarding operation and maintenance [<xref rid="B1-sensors-25-05753" ref-type="bibr">1</xref>].</p><p>To achieve dynamic error correction, a real-time error compensation mechanism based on feature matching can be established by detecting features with fixed patterns in the track as mileage calibration points [<xref rid="B2-sensors-25-05753" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05753" ref-type="bibr">3</xref>]. Therefore, high-precision track feature detection technology has become the core link for improving the robustness of the intelligent inspection system, which is of key significance in guaranteeing the safe operation and maintenance of the entire track life cycle.</p><p>In recent years, researchers have used various sensors and optimization algorithms to extract features from tracks for research on track feature detection [<xref rid="B4-sensors-25-05753" ref-type="bibr">4</xref>]. Wang [<xref rid="B5-sensors-25-05753" ref-type="bibr">5</xref>] used an odometer to detect the speed of the train on the track and the distance traveled, which is still limited due to the acceleration and deceleration skidding of the train and the wear and tear of the wheels; Wei [<xref rid="B6-sensors-25-05753" ref-type="bibr">6</xref>] introduced light detection and ranging (LIDAR) equipment to detect the track plane and used the moving average filter (MAF) algorithm to detect and classify the track, but the method is too computationally intensive and it has very high requirements for the hardware terminal; Wang et al. [<xref rid="B7-sensors-25-05753" ref-type="bibr">7</xref>] fused the radar-acquired multi-cycle tunnel profile point cloud data through the localization algorithm and used the subway tunnel modeling algorithm to establish a standard tunnel profile model to process the fused data; Zhang [<xref rid="B8-sensors-25-05753" ref-type="bibr">8</xref>] used the existing communication optical fiber along the railroad line to detect the track plane and classify the track and proposed an interferometric technique based on Rayleigh backscattered signals in optical fibers for identification and localization of railroad vehicles using existing communication cables along the railroad, but the method still has some limitations; Olaby [<xref rid="B9-sensors-25-05753" ref-type="bibr">9</xref>] proposed a method for railroad localization by using RFID technology to align vehicles to the location of turnouts and crossings on the railroad network; Lian [<xref rid="B10-sensors-25-05753" ref-type="bibr">10</xref>] proposed a new modular visual processing framework based on the multi-target tracking module of dynamic regions of interest to assign a unique identification code to each landmark for continuous train localization; Spinsante [<xref rid="B11-sensors-25-05753" ref-type="bibr">11</xref>] proposed a hybrid GNSS method for train localization, but the method has certain requirements for GNSS signals; Qin [<xref rid="B12-sensors-25-05753" ref-type="bibr">12</xref>] proposed a new method using data fusion techniques with mileage-corrected track geometry inspection data combined with the uncorrected velocity information of axlebox acceleration inspection data to correct the mileage deviation of axlebox acceleration inspection data; and Chen [<xref rid="B13-sensors-25-05753" ref-type="bibr">13</xref>] proposed an on-board railroad positioning system assisted by digital track maps using the Jetlink inertial navigation system (SINS) and OD, which effectively suppresses the accumulation of the train&#8217;s position error. In summary, the current track detection method relies on high-precision equipment to achieve accurate track detection, and the error caused by this equipment also requires higher algorithm complexity. At the same time, the detection model needs to be deployed in the mobile edge device, and its resource constraints and multi-threaded environment also require the model to have a smaller number of parameters. Therefore, lightweight, efficient, and accurate track feature detection technology has become crucial in ensuring line safety and operational and maintenance efficiency.</p><p>The rapid development of deep learning technology has injected unprecedented vitality into the field of computer vision, and vision-based track inspection is gradually becoming a key research direction [<xref rid="B14-sensors-25-05753" ref-type="bibr">14</xref>]. Phaphuangwittayakul [<xref rid="B15-sensors-25-05753" ref-type="bibr">15</xref>] utilized the Dual Attention Visual Transformer (DaViT) to construct RailTrack-DaViT, effectively capturing both global and local information to achieve accurate track detection; Xiao [<xref rid="B16-sensors-25-05753" ref-type="bibr">16</xref>] developed a novel fusion model combining the Segment Anything Model and U-Net network to perform detailed identification and segmentation of track scaling areas; Bottalico [<xref rid="B17-sensors-25-05753" ref-type="bibr">17</xref>] developed a method based on 3D vision to identify inherent features already present on track structures; HU [<xref rid="B18-sensors-25-05753" ref-type="bibr">18</xref>] enhanced the detection in complex slab track scenarios using synthetic images based on the YOLO architecture; Ma [<xref rid="B19-sensors-25-05753" ref-type="bibr">19</xref>] improved the YOLOv8 algorithm for detecting train track fasteners, achieving good detection results; Luo [<xref rid="B20-sensors-25-05753" ref-type="bibr">20</xref>] automated ballast detection using computer vision methods, employing BSV to thoroughly assess continuous track sections; Shen [<xref rid="B21-sensors-25-05753" ref-type="bibr">21</xref>] improved YOLOv7 and Center-Point for detecting visible light images and point clouds, respectively, and used AED as a new metric in the data correlation module to track detection results between images and point clouds, effectively enhancing the correlation robustness and reducing the tracking errors. In terms of algorithm-based applications, Xia [<xref rid="B22-sensors-25-05753" ref-type="bibr">22</xref>] proposed an Odess iteration, significantly reducing the computational overhead of similarity detection while achieving a high detection accuracy and high compression ratios, while Zou [<xref rid="B23-sensors-25-05753" ref-type="bibr">23</xref>] proposed a novel management-friendly duplicate data deletion framework named MFDedup, maximizing the locality; these methods open new horizons for embedded deployment in track detection applications.</p><p>Based on the current research status and existing problems, this paper proposes a lightweight track feature detection algorithm, YOLO-LWTD, for track inspection tasks, building upon YOLO11 [<xref rid="B24-sensors-25-05753" ref-type="bibr">24</xref>]. First, StarNet is used to replace the original backbone network, significantly reducing the model complexity. Second, the feature fusion network is reconfigured to be lightweight, and the efficient C3K2-Light module is introduced. Finally, the model&#8217;s performance is further enhanced by the detection head structure. The structure further enhances the model&#8217;s detection performance.</p><p>The structure of this paper is as follows: <xref rid="sec1-sensors-25-05753" ref-type="sec">Section 1</xref> provides a summary of the current state of research; <xref rid="sec2-sensors-25-05753" ref-type="sec">Section 2</xref> systematically describes the overall architecture of the track detection algorithm based on the improved YOLO11 and the design principle of its optimization module; <xref rid="sec3-sensors-25-05753" ref-type="sec">Section 3</xref> outlines the data acquisition and model-training process; <xref rid="sec4-sensors-25-05753" ref-type="sec">Section 4</xref> analyzes the differences in the performance indexes between the proposed algorithm and the main benchmark methods through comparative experiments and systematically evaluates the experimental results; and <xref rid="sec5-sensors-25-05753" ref-type="sec">Section 5</xref> summarizes the entire paper and presents constructive perspectives for future research directions.</p></sec><sec sec-type="methods" id="sec2-sensors-25-05753"><title>2. Proposed&#160;Methodology</title><sec id="sec2dot1-sensors-25-05753"><title>2.1. YOLO11</title><p>The YOLO (You Only Look Once) series of algorithms, a representative research algorithm in the field of target detection, employs an end-to-end single-stage detection architecture, which enables the efficient detection and precise localization of multiple target objects in images. YOLO11 is the latest model proposed by the Ultralytics team, featuring main innovations that include the introduction of the C3K2 module to optimize the shallow feature extraction process, the incorporation of the C2PSA attention mechanism to enhance feature capture, and the addition of depth-separable convolution (DWConv) to the detection head. In terms of the model architecture, YOLO11 consists of three core components: a feature extraction backbone network (backbone), a multi-scale feature fusion neck network (neck), and a target detection head (head); its overall architecture is shown in <xref rid="sensors-25-05753-f001" ref-type="fig">Figure 1</xref>.</p><p>YOLO11 mainly consists of three parts: backbone, neck, and head. The backbone network part of YOLO11 is used to extract the multi-scale feature maps of the input image. It includes modules such as Conv, C3K2, SPPF, and C2PSA. C3K2 enhances the overall performance of feature extraction. In contrast, the spatial attention (C2PSA) module is combined with the SPPF, which enables the model to adaptively focus on the salient regions in the image to enhance the key feature expression ability; the neck network adopts a combined bi-directional feature fusion structure (PANet) that combines FPN and PAN, in which the C3K2 module fuses features at different scales more efficiently; the detection head part follows the decoupled head of YOLOv8, but YOLO11 adds two depth-separable convolutions (DW-Conv) to the classification detection head to substantially reduce the computation amount without losing accuracy and, at the same time, significantly reduces the computational effort. For the regression loss, a composite loss function combining distribution focal loss and CIoU (complete intersection over union) is used; for the classification loss, distribution focal loss (DFL) is used for the optimization, which adaptively adjusts the weights of positive and negative samples, effectively alleviating the category imbalance problem [<xref rid="B25-sensors-25-05753" ref-type="bibr">25</xref>].</p><p>YOLO11 provides five models with different network depths and widths&#8212;n (nano), s (small), m (medium), l (large), and x (extra-large)&#8212;based on the synergistic tuning of the network depths and widths, as well as the structural parameter counts of each model variant under the condition of an input resolution of 640 &#215; 640 pixels (parameters) and floating point operations (FLOPs) metrics for each model variant at an input resolution of 640 &#215; 640 pixels are shown in <xref rid="sensors-25-05753-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec2dot2-sensors-25-05753"><title>2.2. YOLO-LWTD</title><p>Aiming to enhance the feature extraction efficiency and meet the requirements for lightweight models in multi-target detection tasks for tracking scenes, this study improves the model by utilizing YOLO11n as the base network and proposes a new lightweight track feature detection model. The innovative improvements of the model are mainly reflected in the following three aspects:<list list-type="bullet"><list-item><p>In the backbone feature extraction part, the element-level feature interaction mechanism unique to StarNet is innovatively introduced to enhance the feature extraction;</p></list-item><list-item><p>In the neck part of the model, a lightweight extended path aggregation network and the C3K2-Light module are adopted to achieve efficient fusion of multi-scale features by optimizing the information propagation path of the feature pyramid;</p></list-item><list-item><p>In the head part, a lighter and more efficient detection head Detect-LADH is adopted, and this structure significantly reduces the computational complexity by simplifying the feature-decoding process while ensuring the detection accuracy.</p></list-item></list></p><p>The above improved complete network structure is shown in <xref rid="sensors-25-05753-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec2dot3-sensors-25-05753"><title>2.3. StarNet</title><p>The backbone network architecture of YOLO11 constructs a multilevel feature extraction system by integrating components such as the C3K2 module, SPPF, C2PSA, and the product layer. Although this module combination strategy effectively improves the feature expression capability of the network, it also significantly increases the amount of parameter computation required by the model, resulting in a decrease in the inference speed and negatively impacting the deployment efficiency of the model in real-world application scenarios. To effectively address the performance bottleneck mentioned above, this paper proposes a backbone network reconfiguration scheme based on the StarNet network.</p><p>StarNet [<xref rid="B26-sensors-25-05753" ref-type="bibr">26</xref>] is an efficient neural network architecture based on elementary multiplication operations. Unlike the standard linear dot product operation, it employs element-wise multiplication to construct a mapping relation from a low-dimensional feature space to a high-dimensional, nonlinear space. This operation does not increase the width of the network, and not only preserves the local specificity of the input features but also significantly improves the discriminative ability of the model through nonlinear interactions. For target detection networks, such as YOLO, the elemental multiplication operation is particularly suitable for capturing visual features with subtle structural differences.</p><p>In a single-layer neural network, the neuron inputs can be represented by the representation of the multiplication operation as in Equation (<xref rid="FD1-sensors-25-05753" ref-type="disp-formula">1</xref>), where &#8902; represents the multiplication operation, <italic toggle="yes">W</italic> represents the weights of the input neurons, and <italic toggle="yes">B</italic> represents the bias of the input neurons.<disp-formula id="FD1-sensors-25-05753"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8902;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on Equation (<xref rid="FD1-sensors-25-05753" ref-type="disp-formula">1</xref>), the weight matrix and bias are then combined into a single entity, denoted as <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, corresponding to <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and the fusion of the characteristics of two linear transformations is expressed by element multiplication, resulting in a single output channel conversion and a single-element-input element multiplication, as shown in Equation (<xref rid="FD2-sensors-25-05753" ref-type="disp-formula">2</xref>). This equation defines <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">d</italic> is the input channel number; it can also be extended to the case of multiple output channels and processing of multiple feature elements, i.e., <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Meanwhile, <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> are used to index the channels and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is the coefficient of each item, where the expression for the coefficients is shown in Equation (<xref rid="FD3-sensors-25-05753" ref-type="disp-formula">3</xref>).<disp-formula id="FD2-sensors-25-05753"><label>(2)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>X</mml:mi><mml:mo>&#8902;</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>X</mml:mi><mml:mspace width="1.em"/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:msup><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mfenced><mml:mo>&#8902;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msubsup><mml:msup><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mn>5</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>&#65080;</mml:mo></mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mspace width="4.pt"/><mml:mi>items</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05753"><label>(3)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mphantom><mml:mn>0</mml:mn></mml:mphantom><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mi>w</mml:mi><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>if</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mi>w</mml:mi><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:msubsup><mml:msubsup><mml:mi>w</mml:mi><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>if</mml:mi><mml:mi>i</mml:mi><mml:mo>!</mml:mo><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>From Equation (<xref rid="FD3-sensors-25-05753" ref-type="disp-formula">3</xref>), it can be seen that except for the term <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the rest of the terms all exhibit linear irrelevance to the term <italic toggle="yes">x</italic>, i.e., they are all independent dimensions. Therefore, the element multiplication operation in the <italic toggle="yes">d</italic>-dimensional space yields a representation in the <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mspace width="-0.166667em"/><mml:mo>+</mml:mo><mml:mspace width="-0.166667em"/><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mspace width="-0.166667em"/><mml:mo>+</mml:mo><mml:mspace width="-0.166667em"/><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#8776;</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>d</mml:mi><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> dimensional space, which significantly enhances the dimensionality of the features. This computational mechanism strikes a good balance between computational complexity and model expressiveness, effectively retaining and extracting rich deep semantic information even under low-resolution input conditions, making it particularly suitable for real-time inspection tasks in track or near-track.</p><p>The network structure of StarNet, based on the above elemental multiplication, is shown in <xref rid="sensors-25-05753-f003" ref-type="fig">Figure 3</xref>. It employs an efficient four-stage hierarchical feature extraction framework that achieves progressive expansion of feature dimensions by multiplying the number of channels stage by stage. Specifically, the network first performs basic feature extraction on the input image through the first convolutional layer, followed by deep feature extraction through four-layered architectures. A convolutional layer downsamples each layered architecture, and feature extraction is performed using the StarBlocks module, which consists of two deeply divisible convolutions and three fully connected networks. First, batch normalization is introduced after deep convolution to facilitate information fusion, and batch normalization can improve the computational efficiency of the model; subsequently, the result of batch normalization is passed through the ReLU6 activation function to introduce the nonlinear transformation capability; after this, a new high-dimensional feature space is generated through the elemental multiplication operation; then, the result of the operation is passed through the fully connected layer to integrate the features for preprocessing of the classification task; and, finally, the features are efficiently fused by deep convolution at the end of StarBlocks to further enhance the feature extraction capability. StarNet abandons the traditional method of expanding the network width (i.e., increasing the number of channels) to enhance the expression capability of the model and realizes high-dimensional feature mapping in low-dimensional space, which not only significantly improves the expression capability of the model but also improves the performance of the model, which can also be used for classification tasks. This not only significantly improves the efficiency of the feature extraction and characterization ability but also significantly reduces the computational complexity, achieving the design goal of model lightweighting.</p><p>In YOLO-LWTD, StarNet serves as the front-end component of the backbone, performing basic feature extraction on the input track images to obtain multi-scale feature information. Specifically, the first convolutional layer of StarNet corresponds to the item module in the backbone shown in <xref rid="sensors-25-05753-f002" ref-type="fig">Figure 2</xref>. Subsequent convolutional layers and Star Blocks are alternately stacked to form four stage modules. Finally, SPPF and C2PSA perform additional processing on low-scale features to enhance the model&#8217;s ability to express low-frequency features in images.</p></sec><sec id="sec2dot4-sensors-25-05753"><title>2.4. EPAN</title><p>YOLO11 adopts the path aggregation network (PANet) as its neck part of the pyramid structure, as shown in <xref rid="sensors-25-05753-f004" ref-type="fig">Figure 4</xref>b. Compared with the traditional Feature Pyramid Network (FPN) shown in <xref rid="sensors-25-05753-f004" ref-type="fig">Figure 4</xref>a, PANet adds a bottom-up pathway, where this two-way feature fusion strategy realizes the efficient transfer and fusion of high-level feature information to the low-level, which not only makes the feature map rich in semantic information and precise location information but also breaks through the limitation of unidirectional information flow in the FPN and effectively alleviates the problem of shallow feature information loss. Meanwhile, the feature information is distributed among layers according to the network size, with smaller features assigned to lower layers and larger features to higher layers, thereby optimizing the utilization of multi-scale features. Thanks to this structure, YOLO11 can detect the scale, shape, and class of the target more accurately, while the model&#8217;s characterization ability is further enhanced by gradually increasing the depth and resolution of the feature map.</p><p>However, in practical applications for track detection, YOLO11 shows certain shortcomings in detection. The primary reason is the unsatisfactory effect of feature fusion, which is insufficient for integrating low-level features (e.g., detailed information about the target) and high-level features (e.g., global context information), resulting in limited accuracy and recall in target detection. Therefore, for tracking images with variable background information and irregular image noise, PANet still has limitations in capturing the detailed features of the image, which significantly affects the model&#8217;s performance in complex scenes [<xref rid="B27-sensors-25-05753" ref-type="bibr">27</xref>].</p><p>To address these limitations, this paper further optimizes the feature fusion mechanism based on the architecture of PANet, achieving finer multi-scale feature interactions by introducing an efficient extended path aggregation network (EPAN). As shown in <xref rid="sensors-25-05753-f004" ref-type="fig">Figure 4</xref>c, EPAN&#8217;s front end introduces additional feature-processing modules to perform more refined processing of the main features, making them better suited to the complexity of track scenes and addressing the shortcomings of traditional multi-scale fusion networks in terms of feature depth information mining. Meanwhile, an innovative cross-layer jump connection, similar to a residual structure, has been introduced to enhance the retention and utilization of spatial detail information, enabling the model to more effectively capture the key features of orbital scenes. Additionally, by optimizing the information flow path, the model achieves its lightweighting goal by carrying richer effective information with fewer feature layers.</p><p>In <xref rid="sensors-25-05753-f004" ref-type="fig">Figure 4</xref>c, the feature maps of each row in EPAN have the same scale, but there are differences in how each feature map is processed in detail. The specific implementation process is described as follows: First, three feature maps of different scales&#8212;P1, P2, and P3&#8212;are extracted from the backbone network. Next, the feature maps P1, P2, and P3 output by the backbone network are processed through a 1 &#215; 1 convolution module to generate P4, P5, and P6, respectively, aiming to achieve a nonlinear mapping between the input channels. Subsequently, P6 is upsampled to generate P7; P7 is concatenated with P5, then processed through the C3K2-Light module and a convolutional layer to generate P8; P8 is then concatenated with P4 and fed into the C3K2-Light module for processing to generate P9. Finally, P9 is downsampled and concatenated with P8 and P5, then processed through the C3K2-Light module to generate P11; and P11 is downsampled and concatenated with P3 and P6, then processed through the C3K2-Light module to generate P10. Ultimately, the feature maps P9, P10, and P11 are output from EPAN and fed into the object detection head. The correspondence between feature maps is shown in <xref rid="sensors-25-05753-f005" ref-type="fig">Figure 5</xref>.</p></sec><sec id="sec2dot5-sensors-25-05753"><title>2.5. C3K2-Light</title><p>To construct a more lightweight YOLO11 detection network and achieve the optimal balance between model efficiency and detection accuracy while ensuring track detection performance, this study is inspired by FasterNet and improves the C3K2 module in YOLO11 based on Partial Convolution (PConv [<xref rid="B28-sensors-25-05753" ref-type="bibr">28</xref>]), which makes clever use of the redundancy of channels in the feature map to extract spatial features while keeping other channels undisturbed. Channel redundancy is maintained in the feature map to perform traditional convolution operations on only some of the input channels to extract spatial features while keeping the other channels undisturbed, and this selective computation mechanism significantly reduces the amount of floating-point operations (FLOPs). The structure of PConv is shown in <xref rid="sensors-25-05753-f006" ref-type="fig">Figure 6</xref>.</p><p>For regular convolution with input <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>h</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and output <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>h</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the FLOPs are shown in Equation (<xref rid="FD4-sensors-25-05753" ref-type="disp-formula">4</xref>), where <italic toggle="yes">c</italic> is the channel number, <italic toggle="yes">h</italic> and <italic toggle="yes">w</italic> are the height and width of the input data, and k is the size of the convolution kernel. Furthermore, the FLOPs for PConv are shown in Equation (<xref rid="FD5-sensors-25-05753" ref-type="disp-formula">5</xref>), where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is denoted as the channel number. Since PConv performs conventional convolution operations only on the first and last consecutive channels of the input feature map while keeping the middle channels unchanged, this selective computational strategy makes its FLOPs significantly lower than that of the conventional convolution method and significantly reduces the overall parameter count of the model, thus realizing the balance between the computational efficiency and the feature expression capability.<disp-formula id="FD4-sensors-25-05753"><label>(4)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>w</mml:mi><mml:mo>&#215;</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#215;</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-05753"><label>(5)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>w</mml:mi><mml:mo>&#215;</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#215;</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We designed the C3K2-Light module, shown in <xref rid="sensors-25-05753-f007" ref-type="fig">Figure 7</xref>, based on PConv to optimize the balance between detection accuracy and computational efficiency. The core architecture of the module employs a <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> PConv as the central layer, which not only retains the attention property of standard convolution in the center region of the sensory field but also significantly reduces the computational complexity through a selective channel computation strategy. To enhance the feature characterization capability, the module cascades two <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional layers after the PConv layer, extending the receptive field. It then fuses the features of the PConv and the regular convolution through residual concatenation, ensuring feature diversity while facilitating fast inference. In particular, the module places BatchNormalization after the intermediate convolutional layer and supplements it with the ReLU activation function, which not only accelerates the model&#8217;s convergence but also significantly improves its inference efficiency.</p></sec><sec id="sec2dot6-sensors-25-05753"><title>2.6. Detect-LADH</title><p>The decoupled two-branch detection head structure adopted by YOLO11 improves the task specificity by independently processing classification and regression tasks. However, this architectural design has three significant drawbacks: first, the multiple convolutional operations in the two branches significantly increase the number of model parameters and computational complexity, making it difficult to achieve efficient deployment on low-computing-power devices or meet real-time detection requirements; second, since the feature processing of the classification and regression branches is completely isolated, the network cannot effectively utilize the complementary high-level semantic features extracted by the backbone network, thereby limiting the model&#8217;s representational capability and detection performance in track inspection tasks; additionally, the original decoupled head uses the same convolutional layer at the top of the network for both regression and classification, but these tasks have different focuses, leading to potential conflicts during the detection process.</p><p>We introduce a lightweight asymmetric detection head (LADH [<xref rid="B29-sensors-25-05753" ref-type="bibr">29</xref>]) to address the above issues; the structure is illustrated in <xref rid="sensors-25-05753-f008" ref-type="fig">Figure 8</xref>. This architecture is based on a task-driven design philosophy and uses a three-channel separated network to handle the classification, regression, and IoU prediction tasks separately. LADH consists of two core components: the Asymmetric Head and the Dual Head. The Asymmetric Head of LADH employs asymmetric multi-level compression to apply differentiated compression to features of different categories, thereby adapting to variations in the target complexity. The Dual Head is responsible for integrating the multi-scale feature outputs (P3&#8211;P5) from the Asymmetric Head and generating the final detection results.</p><p>LADH-Head uses depthwise separable convolution (DWConv) instead of standard convolution to avoid performance bottlenecks that shared feature layers may cause. Depthwise separable convolution decomposes traditional convolution into two independent operations: depthwise convolution and pointwise convolution. Specifically, pointwise convolution uses a <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution kernel to fuse cross-channel information, adjusting only the number of channels while maintaining the spatial dimension of the feature map. This design further optimizes the model complexity. In the detection head design, the introduction of <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> depthwise separable convolution decouples the classification task from the bounding box regression task, effectively avoiding task interference caused by differences in the positive sample matching loss. Therefore, replacing the original decoupling head of YOLO11 with LADH-Head ensures the model&#8217;s lightweight nature while significantly improving the detection accuracy and computational efficiency through asymmetric feature processing, making it particularly suitable for track detection scenarios.</p></sec></sec><sec id="sec3-sensors-25-05753"><title>3. Experiments and Results&#160;Analysis</title><p>Current publicly available datasets focused on rail tracks primarily emphasize local features such as the track surface texture and localized defects, severely neglecting the holistic perspective essential for railway inspection tasks. This emphasis on local features results in significant discrepancies between existing datasets and real-world inspection scenarios, making it difficult to fully reflect the actual railway operational environment and thereby limiting the development of object detection algorithms tailored to this task. To address this issue, this study systematically constructed a dataset specifically tailored to the railway inspection context, thereby overcoming the limitations of existing datasets. This paper uses a self-built dataset to train and test the proposed model.</p><sec id="sec3dot1-sensors-25-05753"><title>3.1. Experimental&#160;Datasets</title><p>Track image acquisition experiments were performed in a ballasted track field environment. The experimental site was selected as a standard rail ballasted track section. Fasteners and sleepers are crucial components of the track, serving as the primary connectors between the rails and the sleepers. Due to their unique shapes, this paper identifies and detects them by extracting the features of the fasteners and sleepers.</p><p>Additionally, to enhance the practical usability of the dataset, this study specifically designed the image acquisition system to be mounted on the central section of the track inspection train, using a 45&#176; installation angle to precisely target the bottom edge of the track for image capture. This structural design minimizes the inclusion of excessive external background environments, thereby maximizing the exclusion of external uncontrollable factors such as extreme weather conditions and sudden changes in lighting intensity that could affect the image data. This ensures that the captured track images effectively highlight the key visual features of the track itself. The camera&#8217;s installation position and framing angle are shown in <xref rid="sensors-25-05753-f009" ref-type="fig">Figure 9</xref>.</p><p>However, despite the rigorous image acquisition process, it is still challenging to fully account for the more complex real-world orbital detection environments, for example, dynamic changes in lighting conditions, the presence of random noise, fog caused by weather conditions, and varying degrees of obstruction by foreign objects between track structural components. Therefore, to enhance the completeness of the dataset, enable the model to learn more features, and improve the generalization ability of the deep learning model in complex scenarios, we employed image processing techniques to perform data augmentation on the original track image dataset, thereby striving to approximate and cover the scenarios that may be encountered in reality.</p><p>Specifically, five targeted image transformation methods shown in <xref rid="sensors-25-05753-f010" ref-type="fig">Figure 10</xref> were implemented, and each augmentation method was parametrically controlled to ensure that the generated samples maintained the semantic authenticity of the original images while effectively extending the coverage of the data distribution, as described below:<list list-type="bullet"><list-item><p>Geometric transformation: Performs a horizontal mirror flip of the original image.</p></list-item><list-item><p>Luminance adjustment: Randomly adjust the image luminance value in the range of <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="[" close="]"><mml:mo>&#8722;</mml:mo><mml:mn>30</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mn>30</mml:mn><mml:mo>%</mml:mo></mml:mfenced></mml:mrow></mml:math></inline-formula> by linear transformation to simulate the track scene under different lighting conditions.</p></list-item><list-item><p>Noise injection: A Gaussian noise model <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was used to add random noise to the image to improve the robustness of the model to sensor noise.</p></list-item><list-item><p>Random rotation: Rotate the image at a random angle to increase the spatial diversity of the samples.</p></list-item><list-item><p>Atmospheric interference simulation: Based on the atmospheric scattering model [<xref rid="B30-sensors-25-05753" ref-type="bibr">30</xref>], add the fogging effect of different concentrations to simulate the imaging characteristics under rainy and foggy weather conditions.</p></list-item></list></p><p>During the data collection experiments, a total of 566 valid track images were acquired, with an image resolution of <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1408</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels. After the dataset augmentation, the LabelImg software was used to manually label the target area of the track images, draw bounding boxes around the track features, and assign categories. The annotated dataset consisted of 3396 images, including 3396 sleepers and 3396 fasteners. Finally, all the labeled images are divided into training sets (70%, 2377 images), validation sets (10%, 339 images), and test sets (20%, 680 images).</p></sec><sec id="sec3dot2-sensors-25-05753"><title>3.2. Experimental Environment and Parameter&#160;Configuration</title><p>The experiment was conducted in an environment with Windows 11, CUDA 12.6, Python 3.12.8, and Pytorch 2.5.1. The hardware configuration and training hyperparameters are presented in <xref rid="sensors-25-05753-t002" ref-type="table">Table 2</xref>. During training, the input image size was uniformly adjusted to <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and the L2 regularization term was used to penalize large weights to prevent overfitting.</p></sec><sec id="sec3dot3-sensors-25-05753"><title>3.3. Evaluation&#160;Indicators</title><p>To validate the performance of the algorithm proposed in this paper, we systematically evaluated the model using the following metrics: recall (R), precision (P), mean average precision (MAP), floating-point operations per second (FLOPS, denoted as F), parameter count, and frames per second (FPS), among others. Formulas for some of these metrics are provided below:<disp-formula id="FD6-sensors-25-05753"><label>(6)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05753"><label>(7)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05753"><label>(8)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05753"><label>(9)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05753"><label>(10)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05753"><label>(11)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equations (<xref rid="FD6-sensors-25-05753" ref-type="disp-formula">6</xref>) and (<xref rid="FD7-sensors-25-05753" ref-type="disp-formula">7</xref>), <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> stands for true positive samples, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> stands for false positive samples, and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> stands for false negative samples; in Equation (<xref rid="FD8-sensors-25-05753" ref-type="disp-formula">8</xref>), <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the accuracy of the <italic toggle="yes">i</italic>th category; in Equations (<xref rid="FD9-sensors-25-05753" ref-type="disp-formula">9</xref>) and (<xref rid="FD10-sensors-25-05753" ref-type="disp-formula">10</xref>), <italic toggle="yes">K</italic> stands for the size of the convolutional kernel, <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the number of channels in the input feature layer, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the number of channels in the output feature layer, <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the height of the output feature layer, and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the width of the output feature layer; <italic toggle="yes">B</italic> stands for the bias term; in Equation (<xref rid="FD11-sensors-25-05753" ref-type="disp-formula">11</xref>), <italic toggle="yes">T</italic> is the time required for the model to infer a single sample (in <italic toggle="yes">s</italic>).</p></sec><sec id="sec3dot4-sensors-25-05753"><title>3.4. Convergence&#160;Test</title><p>We evaluate the convergence performance of the model by monitoring the trend of the loss function and optimizing the training strategy accordingly. Specifically, we focus on the change process of the following three types of loss functions:<list list-type="bullet"><list-item><p>Bounding box loss, which is used to assess the regression effect of the target detection box;</p></list-item><list-item><p>Distribution focal loss, which is used to optimize the distributional characteristics of the bounding box prediction;</p></list-item><list-item><p>Classification loss, which is used to measure the classification performance of the model.</p></list-item></list></p><p>The change curves of the above loss functions during the training process are shown in <xref rid="sensors-25-05753-f011" ref-type="fig">Figure 11</xref>.</p><p>Throughout the entire training process, as the number of training epochs increased, the total loss value of the model exhibited a monotonically decreasing trend, and the model displayed neither an obvious overfitting nor an underfitting phenomenon. When the number of training rounds reached 175, the loss curve gradually stabilized and entered a state of convergence. Subsequent performance evaluation and analysis could then be carried out after the loss function converged.</p></sec></sec><sec id="sec4-sensors-25-05753"><title>4. Analysis of Experimental&#160;Results</title><sec id="sec4dot1-sensors-25-05753"><title>4.1. Ablation&#160;Experiment</title><p>To fully validate the effectiveness of the proposed improved method, ablation experiments are conducted on the datasets presented in this paper. Each set of experiments was conducted under the same environmental configuration and parameter settings. The results of the ablation experiments conducted using the six metrics of <italic toggle="yes">MAP</italic>50, <italic toggle="yes">parameters</italic>, model size, <italic toggle="yes">FPS</italic>, precision (<italic toggle="yes">P</italic>), and recall (<italic toggle="yes">R</italic>) for comparison are shown in <xref rid="sensors-25-05753-t003" ref-type="table">Table 3</xref>.</p><p>The results show that the standard YOLO11 model achieves a detection MAP of 84.1%, a model size of 5.2 million parameters, and a detection speed of 118 frames per second. Improvements can be made to increase the model map to 84.9%, reduce the model size to 1.5 M, and increase the detection speed to 163 frames/s. The proposed method in this paper outperforms the YOLO11 model on all datasets, demonstrating its impressive performance in target detection and recognition.</p><p>As shown in <xref rid="sensors-25-05753-t003" ref-type="table">Table 3</xref>, when using YOLO11n as the baseline model, each of the four improvements individually enhances the model&#8217;s lightweight nature. The model, which uses StarNet as the main feature extraction network module (YOLO11-Starnet), achieves a significant improvement in lightweight performance compared with the original model, with a 36.5% reduction in the model size and a 35.6% increase in the detection speed. However, this comes at the cost of a slight decrease in the average accuracy. Given the significant lightweighting achieved, this trade-off is acceptable, indicating that the element multiplication operations in StarNet can characterize object depth features with fewer parameters. The model incorporating the Detect-LADH detection head (YOLO11-Detect-LADH) shows a slight improvement in lightweight design and detection speed compared with the original model, indicating that Detect-LADH effectively balances accuracy and speed. The model incorporating the EPAN structure (YOLO11-EPAN) not only improves the model&#8217;s MAP but also achieves an unexpectedly high degree of lightweight optimization, indicating that EPAN can better fuse image scale features and demonstrate the effectiveness of its residual structure-like design. The model (YOLO11-C3K2-Light) that introduces the PConv module into the C3K2 module in the original neck model achieves a 0.7 percentage point improvement in average accuracy compared with the original model, while reducing the number of parameters and achieving a certain degree of lightweight optimization. When the C3K2-Light module is introduced into StarNet, Detect-LADH, and EPAN, it achieves varying degrees of accuracy improvement, all of which exceed the baseline model, indicating that the C3K2-Light module has good applicability. The lightweight track feature detection model (YOLO-LWTD) proposed in this paper performs well across all evaluation metrics, achieving an average precision, recall, and average precision that are 0.5, 2.0, and 0.8 percentage points higher than the original model, respectively. Meanwhile, the model size is only 1.5 MB, achieving a lightweight degree of 71.1%. The detection speed has improved by 45 frames per second, indicating that the improved model has a more concise and efficient network structure and can effectively enhance the model&#8217;s detection performance.</p></sec><sec id="sec4dot2-sensors-25-05753"><title>4.2. Comparison&#160;Test</title><p>To further validate the advantages of the improved YOLO11n for track feature recognition, a series of target detection models were selected for comparison tests. The constructed track dataset was used for training and evaluated on the test set. The test results are presented in <xref rid="sensors-25-05753-t004" ref-type="table">Table 4</xref>.</p><p>From <xref rid="sensors-25-05753-t004" ref-type="table">Table 4</xref>, the YOLO family of algorithms, which are also one-stage detection algorithms, have iteratively increased performances in terms of the average accuracy. However, YOLOv5, as a classic target detection algorithm, has a simple model structure that makes it have a lower model size and higher inference speed, but its accuracy is lower; YOLOv8, as an update of YOLOv5, has a greater improvement in accuracy; YOLOv9 has a smaller model size, but its detection speed is too low to apply to such a mobile detection task as track detection. YOLOv10 weighs the model size and detection accuracy; YOLO11, as a new generation of target detection algorithms, has been improved compared with the previous algorithms. YOLOv12 and YOLOv13 are iterative versions of YOLO11, with improvements in the average accuracy and lightweight performance, but neither is suitable for track detection scenarios. In this paper, the improved model we propose has satisfactory results in the precision rate, recall rate, average precision rate, model size, and inference speed. Specifically, relative to YOLOv5n, YOLOv8, YOLOv9, YOLOv10, and YOLO11, our model is 6.5, 2.8, 0.7, 0.9, and 0.5 percentage points higher regarding the precision rate; 7.2, 2.2, 7.0, 3.8, and 2.0 percentage points higher regarding the recall rate; and 7.4, 4.4, 3.3, 2.3, and 0.7 percentage points higher regarding the mean average precision rate, respectively. It is also 7% higher than YOLOv5 and YOLOv9 regarding the recall rate. Furthermore, the model size is lightened to 1.5M, which is significantly lower than the rest of the YOLO family of models, and its inference speed is 163 frames/s, which is much higher than the rest.</p><p>We designed and conducted a series of backbone network comparison experiments to evaluate the relative advantages of StarNet in terms of the model performance. We selected and included a variety of representative lightweight neural network modules as benchmark models, including FasterNet, ShuffleNetv2, EfficientNetv2, and MobileNetv3. The experimental results are shown in <xref rid="sensors-25-05753-t005" ref-type="table">Table 5</xref>. The data shows that StarNet demonstrates significant advantages in comparative experiments. With a MAP50 accuracy of 83.1, it not only has the lowest number of parameters and smallest model size but also achieves the highest inference speed and lowest computational overhead. Compared with other networks, StarNet is not the most accurate network, but it performs better in terms of efficiency, speed, and resource utilization while maintaining a competitive accuracy, making it an efficient solution for track detection tasks.</p><p>To systematically evaluate the quality of the EPAN model in terms of its multi-scale feature fusion capabilities, BiFPN and SlimNeck were introduced as benchmarks for the comparative experimental analysis. The detailed comparison results are shown in <xref rid="sensors-25-05753-t006" ref-type="table">Table 6</xref>. As can be seen from the experimental data in the table, EPAN demonstrates significant advantages in multiple key metrics: it achieves efficient inference with the fewest parameters, smallest model size, and lowest computational overhead, while also achieving the highest real-time performance and optimal recall rate. Although its MAP50 is slightly lower than SlimNeck, EPAN leads comprehensively in terms of accuracy and overall efficiency (e.g., FPS is 8.5% higher than PaNet and 33.3% higher than BiFPN), highlighting its superiority in balancing the accuracy, speed, and resource consumption.</p></sec><sec id="sec4dot3-sensors-25-05753"><title>4.3. Visual&#160;Analysis</title><p>The detection results obtained by training the track image dataset using the model proposed in this paper are shown in <xref rid="sensors-25-05753-f012" ref-type="fig">Figure 12</xref>.</p><p>As demonstrated by the seven visualization experiments, the YOLO-LWTD model proposed in this paper achieves good detection performance across various scenarios. In the ordinary scenes shown in the first column, there is little difference in the detection performance between all the models; the second and third columns represent low-light conditions at night and enhanced lighting during the day, respectively. The improved model focuses more closely on the detailed features of the track, whereas the original model is less sensitive to changes in lighting conditions. The fourth column introduces hazy weather conditions that may be encountered during track inspections, revealing that hazy weather significantly impacts the detection performance, with all models struggling to accurately identify objects. However, the proposed model still holds a certain advantage. The subsequent fifth and sixth columns further complicate the background information of the detection scenes. It can be seen that the improved model can better focus on the features of the target object, thereby achieving good detection results. Overall, the track detection model proposed in this paper can effectively focus on track features, and its lightweight structure will also demonstrate a more meaningful performance in practical applications.</p><p>Heatmaps can more intuitively show the key location information learned by the model network. To more clearly evaluate the improved model, Grad-CAM was used to generate corresponding heatmaps, the results of which are shown in <xref rid="sensors-25-05753-f013" ref-type="fig">Figure 13</xref>.</p><p>As can be seen from <xref rid="sensors-25-05753-f013" ref-type="fig">Figure 13</xref>, the improved model exhibits significantly enhances the spatial focusing capability in the orbital feature region, with the CAM peak response region more concentrated around the geometric center of the orbital structure. This phenomenon intuitively confirms the effectiveness of the model optimization.</p></sec><sec id="sec4dot4-sensors-25-05753"><title>4.4. Deployment&#160;Testing</title><p>We built the deployment platform shown in <xref rid="sensors-25-05753-f014" ref-type="fig">Figure 14</xref> based on Rockchip RK3588. RK3588 uses a high-performance processor that integrates quad-core Cortex-A76 and quad-core Cortex-A55 architectures, equipped with an ARM Mali-G610 MC4 GPU, 16 GB LPDDR4X memory, and 64 GB eMMC storage, and supports multiple operating systems, including Linux.</p><p>Experiments were conducted on a mobile terminal platform using the Ubuntu 22.04 to perform Int8 quantization on the model. The results showed that the original model size was 5.2 MB with an FPS of 52, while the improved model size was 1.4 MB with an FPS of 69. The lightweight design of the improved model was proven to be effective and suitable for deployment on mobile platforms, providing a highly energy-efficient solution for lightweight track detection tasks targeting embedded deployment or real-time processing.</p></sec></sec><sec id="sec5-sensors-25-05753"><title>5. Summary and Future&#160;Work</title><sec id="sec5dot1-sensors-25-05753"><title>5.1. Summary</title><p>In this study, the network structure of YOLO11n is enhanced by introducing StarNet into the backbone network, resulting in a lower model size and a significantly higher detection speed. This makes it suitable for multi-threaded work. A work structure is introduced in the neck to improve the feature fusion effectiveness of the enhanced model, and some C3K2-Light attention modules are added to improve the model&#8217;s lightness. Finally, the model is further improved by Detect-LADH to further improve the detection effect of the model.</p><p>The results show that the proposed improved YOLO11n can improve the detection accuracy of the model for track features, which is 0.5, 2.0, and 0.8 percentage points higher than the original model in terms of the mean values of precision, recall, and average precision, respectively; the inference speed is 163 frames/s, which is 38.1% higher than that of the original model, and the size of the model is only 1.5 MB, with a lightweight degree of 71.1%. The performance of the improved model is more balanced in terms of the detection accuracy and degree of lightweight, with a lower model size and significantly higher detection speed. This makes it suitable for multi-threaded, real-time track detection processing tasks and deployment on mobile devices.</p></sec><sec id="sec5dot2-sensors-25-05753"><title>5.2. Future&#160;Work</title><p>This study conducted experimental validation based on a diverse dataset of track images. However, it is worth noting that the current dataset still has limitations in covering all possible track scenarios. Due to the lack of a widely recognized public benchmark dataset specifically designed for track feature detection within the industry, this research faces certain challenges regarding the comprehensiveness and universality of the dataset. This limitation also represents one of the key issues that future research needs to address.</p><p>Looking ahead, our work will focus on the following directions: (1) continuously expanding the scale and diversity of datasets while building an open-access benchmark dataset for track features to foster collaborative development in this field; (2) applying more rigorous statistical methods, such as cross-validation, to further validate the model generalization capabilities on larger datasets; (3) addressing highly heterogeneous challenges in railway inspection operations&#8212;such as dynamic environmental changes, extreme weather, and complex terrain&#8212;by prioritizing research on robust perception and intelligent recognition technologies for complex, multi-variable scenarios (e.g., low-light conditions, rain/fog interference, track debris, and high-speed moving perspectives); (4) explore the deep integration and collaborative analysis of multi-source heterogeneous sensing methods&#8212;including high-resolution machine vision, 3D laser scanning, multispectral/infrared imaging, ground-penetrating radar, acoustic detection, and inertial measurement units&#8212;to build next-generation intelligent track inspection systems characterized by high precision, real-time capability, and reliability.</p></sec></sec></body><back><ack><title>Acknowledgments</title><p>The authors are thankful to the anonymous reviewers and editors for their valuable comments and suggestions.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, H.Q., D.Y. and K.J.; methodology, H.Q.; software, H.Q.; validation, H.Q. and D.Y.; formal analysis, J.M.; investigation, K.J.; resources, K.J.; data curation, K.J. and D.Y.; writing&#8212;original draft preparation, H.Q.; writing&#8212;review and editing, H.Q. and J.M.; visualization, D.Y.; supervision, W.W. and J.C.; project administration, H.Q. and D.Y.; funding acquisition, W.W. and J.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Kun Jiang was employed by the company Jiangxi Everbright Measurement and Control Technology Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05753"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>Q.L.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>G.Q.</given-names></name><name name-style="western"><surname>Lo</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>W.B.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>Z.F.</given-names></name></person-group><article-title>High-speed railway wheel polygon detection framework using improved frequency domain integration</article-title><source>Veh. Syst. Dyn.</source><year>2024</year><volume>62</volume><fpage>1424</fpage><lpage>1445</lpage><pub-id pub-id-type="doi">10.1080/00423114.2023.2235032</pub-id></element-citation></ref><ref id="B2-sensors-25-05753"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>X.</given-names></name></person-group><article-title>Track geometry feature matching method for train positioning</article-title><source>Bull. Surv. Mapp.</source><year>2019</year><fpage>109</fpage><lpage>113</lpage></element-citation></ref><ref id="B3-sensors-25-05753"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Otegui</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bahillo</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lopetegi</surname><given-names>I.</given-names></name><name name-style="western"><surname>D&#237;ez</surname><given-names>L.E.</given-names></name></person-group><article-title>A Survey of Train Positioning Solutions</article-title><source>IEEE Sens. J.</source><year>2017</year><volume>17</volume><fpage>6788</fpage><lpage>6797</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2017.2747137</pub-id></element-citation></ref><ref id="B4-sensors-25-05753"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahimi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.C.</given-names></name><name name-style="western"><surname>Cardenas</surname><given-names>I.D.</given-names></name><name name-style="western"><surname>Starr</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>R.</given-names></name></person-group><article-title>A Review on Technologies for Localisation and Navigation in Autonomous Railway Maintenance Systems</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>4185</elocation-id><pub-id pub-id-type="doi">10.3390/s22114185</pub-id><pub-id pub-id-type="pmid">35684804</pub-id><pub-id pub-id-type="pmcid">PMC9185565</pub-id></element-citation></ref><ref id="B5-sensors-25-05753"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>D.</given-names></name></person-group><article-title>Research on adaptability of train positioning scheme in Virtual Balise with satellite navigation system</article-title><source>J. Railw. Sci. Eng.</source><year>2023</year><volume>20</volume><fpage>1054</fpage><lpage>1065</lpage></element-citation></ref><ref id="B6-sensors-25-05753"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Y.Z.</given-names></name><name name-style="western"><surname>Zong</surname><given-names>K.B.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>B.G.</given-names></name><name name-style="western"><surname>Rizos</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>S.G.</given-names></name></person-group><article-title>A Seamless Train Positioning System Using a Lidar-Aided Hybrid Integration Methodology</article-title><source>IEEE Trans. Veh. Technol.</source><year>2021</year><volume>70</volume><fpage>6371</fpage><lpage>6384</lpage><pub-id pub-id-type="doi">10.1109/TVT.2021.3080393</pub-id></element-citation></ref><ref id="B7-sensors-25-05753"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Su</surname><given-names>G.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>E.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name></person-group><article-title>Research on three-dimensional point cloud reconstruction and deformation detection of tunnel contours based on LiDAR</article-title><source>J. Cent. South Univ. Sci. Technol.</source><year>2024</year><volume>55</volume><fpage>2393</fpage><lpage>2403</lpage></element-citation></ref><ref id="B8-sensors-25-05753"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>X.</given-names></name></person-group><article-title>Real-time positioning technology of train based on optical fiber coherent Rayleigh backscattering</article-title><source>J. Appl. Opt.</source><year>2022</year><volume>43</volume><fpage>994</fpage><lpage>1000</lpage><pub-id pub-id-type="doi">10.5768/jao202243.0508001</pub-id></element-citation></ref><ref id="B9-sensors-25-05753"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Olaby</surname><given-names>O.</given-names></name><name name-style="western"><surname>Hamadache</surname><given-names>M.</given-names></name><name name-style="western"><surname>Soper</surname><given-names>D.</given-names></name><name name-style="western"><surname>Winship</surname><given-names>P.</given-names></name><name name-style="western"><surname>Dixon</surname><given-names>R.</given-names></name></person-group><article-title>Development of a Novel Railway Positioning System Using RFID Technology</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>2401</elocation-id><pub-id pub-id-type="doi">10.3390/s22062401</pub-id><pub-id pub-id-type="pmid">35336573</pub-id><pub-id pub-id-type="pmcid">PMC8954475</pub-id></element-citation></ref><ref id="B10-sensors-25-05753"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lian</surname><given-names>L.R.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Z.W.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>X.Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.Q.</given-names></name></person-group><article-title>A Continuous Autonomous Train Positioning Method Using Stereo Vision and Object Tracking</article-title><source>IEEE Intell. Transp. Syst. Mag.</source><year>2025</year><volume>17</volume><fpage>6</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1109/MITS.2024.3509977</pub-id></element-citation></ref><ref id="B11-sensors-25-05753"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spinsante</surname><given-names>S.</given-names></name><name name-style="western"><surname>Stallo</surname><given-names>C.</given-names></name></person-group><article-title>Hybridized-GNSS Approaches to Train Positioning: Challenges and Open Issues on Uncertainty</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>1885</elocation-id><pub-id pub-id-type="doi">10.3390/s20071885</pub-id><pub-id pub-id-type="pmid">32235292</pub-id><pub-id pub-id-type="pmcid">PMC7181200</pub-id></element-citation></ref><ref id="B12-sensors-25-05753"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>H.Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.D.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.X.</given-names></name></person-group><article-title>Fast mileage deviation correction method for track dynamic inspection system based on data fusion technology</article-title><source>Proceedings of the 3rd IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)</source><conf-loc>Chongqing, China</conf-loc><conf-date>12&#8211;14 October 2018</conf-date><fpage>280</fpage><lpage>286</lpage></element-citation></ref><ref id="B13-sensors-25-05753"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.L.</given-names></name><name name-style="western"><surname>Tu</surname><given-names>Y.Q.</given-names></name></person-group><article-title>A Digital Track Map-Assisted SINS/OD Fusion Algorithm for Onboard Train Localization</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>247</elocation-id><pub-id pub-id-type="doi">10.3390/app14010247</pub-id></element-citation></ref><ref id="B14-sensors-25-05753"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Olivier</surname><given-names>B.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Connolly</surname><given-names>D.P.</given-names></name></person-group><article-title>A Review of Computer Vision for Railways</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2025</year><volume>26</volume><fpage>11034</fpage><lpage>11065</lpage><pub-id pub-id-type="doi">10.1109/TITS.2025.3552011</pub-id></element-citation></ref><ref id="B15-sensors-25-05753"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Phaphuangwittayakul</surname><given-names>A.</given-names></name><name name-style="western"><surname>Harnpornchai</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ying</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>RailTrack-DaViT: A Vision Transformer-Based Approach for Automated Railway Track Defect Detection</article-title><source>J. Imaging</source><year>2024</year><volume>10</volume><elocation-id>192</elocation-id><pub-id pub-id-type="doi">10.3390/jimaging10080192</pub-id><pub-id pub-id-type="pmid">39194981</pub-id><pub-id pub-id-type="pmcid">PMC11355430</pub-id></element-citation></ref><ref id="B16-sensors-25-05753"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>Y.J.</given-names></name><name name-style="western"><surname>Ning</surname><given-names>Y.F.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Y.Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Long</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.D.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.W.</given-names></name></person-group><article-title>A novel machine-vision-based algorithm for quantifying surface fouling of railway ballast beds</article-title><source>Adv. Eng. Inform.</source><year>2025</year><volume>68</volume><fpage>103639</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2025.103639</pub-id></element-citation></ref><ref id="B17-sensors-25-05753"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bottalico</surname><given-names>F.</given-names></name><name name-style="western"><surname>Sabato</surname><given-names>A.</given-names></name></person-group><article-title>Stereo-point tracking of inherent structural features for 3D computer vision measurements</article-title><source>Mech. Syst. Signal Process.</source><year>2025</year><volume>235</volume><fpage>112937</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2025.112937</pub-id></element-citation></ref><ref id="B18-sensors-25-05753"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>W.B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.D.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.W.</given-names></name></person-group><article-title>Robust crack detection in complex slab track scenarios using STC-YOLO and synthetic data with highly simulated modeling</article-title><source>Autom. Constr.</source><year>2025</year><volume>175</volume><fpage>106219</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2025.106219</pub-id></element-citation></ref><ref id="B19-sensors-25-05753"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>S.W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.A.</given-names></name></person-group><article-title>Train track fastener defect detection algorithm based on MGSF-YOLO</article-title><source>J. Supercomput.</source><year>2025</year><volume>81</volume><fpage>494</fpage><pub-id pub-id-type="doi">10.1007/s11227-025-07024-0</pub-id></element-citation></ref><ref id="B20-sensors-25-05753"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>K.L.</given-names></name><name name-style="western"><surname>Qamhia</surname><given-names>I.I.A.</given-names></name><name name-style="western"><surname>Hart</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Tutumluer</surname><given-names>E.</given-names></name></person-group><article-title>Deep Learning Approach for Automated Railroad Ballast Condition Evaluation</article-title><source>International Conference on Transportation Geotechnics</source><comment>Lecture Notes in Civil Engineering</comment><publisher-name>Springer Nature</publisher-name><publisher-loc>Singapore</publisher-loc><year>2024</year><volume>Volume 402</volume><fpage>49</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1007/978-981-97-8213-0_7</pub-id></element-citation></ref><ref id="B21-sensors-25-05753"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.Z.</given-names></name><name name-style="western"><surname>Du</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.N.</given-names></name></person-group><article-title>YCANet: Target Detection for Complex Traffic Scenes Based on Camera-LiDAR Fusion</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>8379</fpage><lpage>8389</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3357826</pub-id></element-citation></ref><ref id="B22-sensors-25-05753"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>W.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>L.F.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>X.Y.</given-names></name><name name-style="western"><surname>Shilane</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>The Design of Fast and Lightweight Resemblance Detection for Efficient Post-Deduplication Delta Compression</article-title><source>ACM Trans. Storage</source><year>2023</year><volume>19</volume><fpage>22</fpage><pub-id pub-id-type="doi">10.1145/3584663</pub-id></element-citation></ref><ref id="B23-sensors-25-05753"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>X.Y.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Shilane</surname><given-names>P.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>From Hyper-dimensional Structures to Linear Structures: Maintaining Deduplicated Data&#8217;s Locality</article-title><source>ACM Trans. Storage</source><year>2022</year><volume>18</volume><fpage>25</fpage><pub-id pub-id-type="doi">10.1145/3507921</pub-id></element-citation></ref><ref id="B24-sensors-25-05753"><label>24.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.</given-names></name></person-group><article-title>Ultralytics YOLO11</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/ultralytics" ext-link-type="uri">https://github.com/ultralytics/ultralytics</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-08">(accessed on 8 September 2025)</date-in-citation></element-citation></ref><ref id="B25-sensors-25-05753"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khanam</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>M.</given-names></name></person-group><article-title>YOLOv11: An Overview of the Key Architectural Enhancements</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2410.17725</pub-id><pub-id pub-id-type="arxiv">2410.17725</pub-id></element-citation></ref><ref id="B26-sensors-25-05753"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name></person-group><article-title>Rewrite the Stars</article-title><source>Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>5694</fpage><lpage>5703</lpage><pub-id pub-id-type="doi">10.1109/CVPR52733.2024.00544</pub-id></element-citation></ref><ref id="B27-sensors-25-05753"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>X.</given-names></name></person-group><article-title>Enhancing UAV Object Detection in Low-Light Conditions with ELS-YOLO: A Lightweight Model Based on Improved YOLOv11</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>4463</elocation-id><pub-id pub-id-type="doi">10.3390/s25144463</pub-id><pub-id pub-id-type="pmid">40732590</pub-id><pub-id pub-id-type="pmcid">PMC12300599</pub-id></element-citation></ref><ref id="B28-sensors-25-05753"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name></person-group><article-title>Lightweight strategy of pipeline detection model based on parameter sharing, pruning and distillation</article-title><source>Signal Image Video Process.</source><year>2025</year><volume>19</volume><fpage>786</fpage><pub-id pub-id-type="doi">10.1007/s11760-025-04386-z</pub-id></element-citation></ref><ref id="B29-sensors-25-05753"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lv</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><article-title>Lightweight Sewer Pipe Crack Detection Method Based on Amphibious Robot and Improved YOLOv8n</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6112</elocation-id><pub-id pub-id-type="doi">10.3390/s24186112</pub-id><pub-id pub-id-type="pmid">39338857</pub-id><pub-id pub-id-type="pmcid">PMC11435957</pub-id></element-citation></ref><ref id="B30-sensors-25-05753"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><article-title>U<sup>2</sup>D<sup>2</sup>Net: Unsupervised Unified Image Dehazing and Denoising Network for Single Hazy Image Enhancement</article-title><source>IEEE Trans. Multimed.</source><year>2024</year><volume>26</volume><fpage>202</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1109/TMM.2023.3263078</pub-id></element-citation></ref><ref id="B31-sensors-25-05753"><label>31.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name></person-group><article-title>Ultralytics YOLOv5 (Version 7.0)</article-title><year>2020</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/yolov5" ext-link-type="uri">https://github.com/ultralytics/yolov5</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-08">(accessed on 8 September 2025)</date-in-citation></element-citation></ref><ref id="B32-sensors-25-05753"><label>32.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chaurasia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.</given-names></name></person-group><article-title>Ultralytics YOLOv8 (Version 8.0.0)</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/ultralytics" ext-link-type="uri">https://github.com/ultralytics/ultralytics</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-08">(accessed on 8 September 2025)</date-in-citation></element-citation></ref><ref id="B33-sensors-25-05753"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>I.H.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.Y.M.</given-names></name></person-group><article-title>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2402.13616</pub-id><pub-id pub-id-type="arxiv">2402.13616</pub-id></element-citation></ref><ref id="B34-sensors-25-05753"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name></person-group><article-title>YOLOv10: Real-Time End-to-End Object Detection</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2405.14458</pub-id></element-citation></ref><ref id="B35-sensors-25-05753"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Doermann</surname><given-names>D.</given-names></name></person-group><article-title>YOLOv12: Attention-Centric Real-Time Object Detectors</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2502.12524</pub-id></element-citation></ref><ref id="B36-sensors-25-05753"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lei</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name><name name-style="western"><surname>Du</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name></person-group><article-title>YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2506.17733</pub-id></element-citation></ref><ref id="B37-sensors-25-05753"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>DETRs Beat YOLOs on Real-time Object Detection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.08069</pub-id></element-citation></ref><ref id="B38-sensors-25-05753"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Kao</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>He</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhuo</surname><given-names>W.P.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>C.H.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>S.H.G.</given-names></name></person-group><article-title>Run, Don&#8217;t Walk: Chasing Higher FLOPS for Faster Neural Networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>12021</fpage><lpage>12031</lpage><pub-id pub-id-type="doi">10.1109/cvpr52729.2023.01157</pub-id></element-citation></ref><ref id="B39-sensors-25-05753"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>N.N.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>H.T.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</article-title><source>Proceedings of the 15th European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><volume>Volume 11218</volume><fpage>122</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-01264-9_8</pub-id></element-citation></ref><ref id="B40-sensors-25-05753"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>M.X.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.</given-names></name></person-group><article-title>EfficientNetV2: Smaller Models and Faster Training</article-title><source>Proceedings of the International Conference on Machine Learning (ICML)</source><conf-loc>Online</conf-loc><conf-date>18&#8211;24 July 2021</conf-date><volume>Volume 139</volume><fpage>7102</fpage><lpage>7110</lpage></element-citation></ref><ref id="B41-sensors-25-05753"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sandler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>M.X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.K.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Vasudevan</surname><given-names>V.</given-names></name><etal/></person-group><article-title>Searching for MobileNetV3</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>1314</fpage><lpage>1324</lpage><pub-id pub-id-type="doi">10.1109/iccv.2019.00140</pub-id></element-citation></ref><ref id="B42-sensors-25-05753"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.F.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.Y.</given-names></name></person-group><article-title>Path Aggregation Network for Instance Segmentation</article-title><source>Proceedings of the 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>8759</fpage><lpage>8768</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2018.00913</pub-id></element-citation></ref><ref id="B43-sensors-25-05753"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Doherty</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gardiner</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kerr</surname><given-names>E.</given-names></name><name name-style="western"><surname>Siddique</surname><given-names>N.</given-names></name></person-group><article-title>BiFPN-YOLO: One-stage object detection integrating Bi-Directional Feature Pyramid Networks</article-title><source>Pattern Recognit.</source><year>2025</year><volume>160</volume><fpage>111209</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2024.111209</pub-id></element-citation></ref><ref id="B44-sensors-25-05753"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>Q.</given-names></name></person-group><article-title>Slim-neck by GSConv: A lightweight-design for real-time detector architectures</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2206.02424</pub-id><pub-id pub-id-type="doi">10.1007/s11554-024-01436-6</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05753-f001" orientation="portrait"><label>Figure 1</label><caption><p>YOLO11 network architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g001.jpg"/></fig><fig position="float" id="sensors-25-05753-f002" orientation="portrait"><label>Figure 2</label><caption><p>YOLO-LWTD network architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g002.jpg"/></fig><fig position="float" id="sensors-25-05753-f003" orientation="portrait"><label>Figure 3</label><caption><p>StarNet network architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g003.jpg"/></fig><fig position="float" id="sensors-25-05753-f004" orientation="portrait"><label>Figure 4</label><caption><p>Diagram of the three neck network structures: (<bold>a</bold>) FPN; (<bold>b</bold>) PANet; (<bold>c</bold>) EPAN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g004.jpg"/></fig><fig position="float" id="sensors-25-05753-f005" orientation="portrait"><label>Figure 5</label><caption><p>Feature relationship diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g005.jpg"/></fig><fig position="float" id="sensors-25-05753-f006" orientation="portrait"><label>Figure 6</label><caption><p>PConv network architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g006.jpg"/></fig><fig position="float" id="sensors-25-05753-f007" orientation="portrait"><label>Figure 7</label><caption><p>C3K2-Light network architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g007.jpg"/></fig><fig position="float" id="sensors-25-05753-f008" orientation="portrait"><label>Figure 8</label><caption><p>Detect-LADH network architecture diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g008.jpg"/></fig><fig position="float" id="sensors-25-05753-f009" orientation="portrait"><label>Figure 9</label><caption><p>Data acquisition schematic.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g009.jpg"/></fig><fig position="float" id="sensors-25-05753-f010" orientation="portrait"><label>Figure 10</label><caption><p>Raw data and data enrichment.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g010.jpg"/></fig><fig position="float" id="sensors-25-05753-f011" orientation="portrait"><label>Figure 11</label><caption><p>Model training loss value change curve.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g011.jpg"/></fig><fig position="float" id="sensors-25-05753-f012" orientation="portrait"><label>Figure 12</label><caption><p>Test result chart.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g012.jpg"/></fig><fig position="float" id="sensors-25-05753-f013" orientation="portrait"><label>Figure 13</label><caption><p>Heatmap results.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g013.jpg"/></fig><fig position="float" id="sensors-25-05753-f014" orientation="portrait"><label>Figure 14</label><caption><p>Experimental platform.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05753-g014.jpg"/></fig><table-wrap position="float" id="sensors-25-05753-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05753-t001_Table 1</object-id><label>Table 1</label><caption><p>YOLO11 model parametric quantities and calculations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs/B</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Param/M</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO11n</td><td align="center" valign="middle" rowspan="1" colspan="1">6.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO11s</td><td align="center" valign="middle" rowspan="1" colspan="1">21.5</td><td align="center" valign="middle" rowspan="1" colspan="1">9.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO11m</td><td align="center" valign="middle" rowspan="1" colspan="1">68.0</td><td align="center" valign="middle" rowspan="1" colspan="1">20.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO11l</td><td align="center" valign="middle" rowspan="1" colspan="1">86.9</td><td align="center" valign="middle" rowspan="1" colspan="1">25.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO11x</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">194.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.9</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05753-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05753-t002_Table 2</object-id><label>Table 2</label><caption><p>Experimental environment parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter Information</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel&#174; Core i5-13490F (Santa Clara, CA, USA)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Memory</td><td align="center" valign="middle" rowspan="1" colspan="1">32 GB</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Display card</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA GeForce RTX 4070&#160;GPU (Santa Clara, CA, USA)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Epoch</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Image size</td><td align="center" valign="middle" rowspan="1" colspan="1">640 &#215; 640</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Model size</td><td align="center" valign="middle" rowspan="1" colspan="1">Depth: 0.25, Width: 0.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Weight decay</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0005</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Initial learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adam</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05753-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05753-t003_Table 3</object-id><label>Table 3</label><caption><p>Table of ablation experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Base</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">StarNet</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Detect-LADH</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">EPAN</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">C3K2-Light</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAP50 (%)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">84.1</td><td align="left" valign="middle" rowspan="1" colspan="1">2,582,542</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2</td><td align="center" valign="middle" rowspan="1" colspan="1">118</td><td align="center" valign="middle" rowspan="1" colspan="1">97.7</td><td align="center" valign="middle" rowspan="1" colspan="1">71.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">83.1</td><td align="left" valign="middle" rowspan="1" colspan="1">1,598,406</td><td align="center" valign="middle" rowspan="1" colspan="1">3.3</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td><td align="center" valign="middle" rowspan="1" colspan="1">96.5</td><td align="center" valign="middle" rowspan="1" colspan="1">70.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">83.6</td><td align="left" valign="middle" rowspan="1" colspan="1">2,281,742</td><td align="center" valign="middle" rowspan="1" colspan="1">4.7</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">97.8</td><td align="center" valign="middle" rowspan="1" colspan="1">69.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">85</td><td align="left" valign="middle" rowspan="1" colspan="1">1,788,782</td><td align="center" valign="middle" rowspan="1" colspan="1">3.7</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td><td align="center" valign="middle" rowspan="1" colspan="1">98.3</td><td align="center" valign="middle" rowspan="1" colspan="1">73.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">84.8</td><td align="left" valign="middle" rowspan="1" colspan="1">2,431,646</td><td align="center" valign="middle" rowspan="1" colspan="1">4.9</td><td align="center" valign="middle" rowspan="1" colspan="1">118</td><td align="center" valign="middle" rowspan="1" colspan="1">96.5</td><td align="center" valign="middle" rowspan="1" colspan="1">72.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">85.9</td><td align="left" valign="middle" rowspan="1" colspan="1">1,447,510</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">152</td><td align="center" valign="middle" rowspan="1" colspan="1">98.8</td><td align="center" valign="middle" rowspan="1" colspan="1">74.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">84.3</td><td align="left" valign="middle" rowspan="1" colspan="1">2,130,846</td><td align="center" valign="middle" rowspan="1" colspan="1">4.4</td><td align="center" valign="middle" rowspan="1" colspan="1">117</td><td align="center" valign="middle" rowspan="1" colspan="1">97.2</td><td align="center" valign="middle" rowspan="1" colspan="1">73.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">85.5</td><td align="left" valign="middle" rowspan="1" colspan="1">1,768,318</td><td align="center" valign="middle" rowspan="1" colspan="1">3.7</td><td align="center" valign="middle" rowspan="1" colspan="1">126</td><td align="center" valign="middle" rowspan="1" colspan="1">97.9</td><td align="center" valign="middle" rowspan="1" colspan="1">73.8</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">620,598</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">163</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05753-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05753-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparative test table.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAP50 (%)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GFLOPs</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv5&#160;[<xref rid="B31-sensors-25-05753" ref-type="bibr">31</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">77.5</td><td align="left" valign="middle" rowspan="1" colspan="1">2,182,054</td><td align="center" valign="middle" rowspan="1" colspan="1">4.4</td><td align="center" valign="middle" rowspan="1" colspan="1">138</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7</td><td align="center" valign="middle" rowspan="1" colspan="1">65.8</td><td align="center" valign="middle" rowspan="1" colspan="1">5.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8&#160;[<xref rid="B32-sensors-25-05753" ref-type="bibr">32</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">80.5</td><td align="left" valign="middle" rowspan="1" colspan="1">2,684,785</td><td align="center" valign="middle" rowspan="1" colspan="1">5.4</td><td align="center" valign="middle" rowspan="1" colspan="1">133</td><td align="center" valign="middle" rowspan="1" colspan="1">95.4</td><td align="center" valign="middle" rowspan="1" colspan="1">70.8</td><td align="center" valign="middle" rowspan="1" colspan="1">6.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv9&#160;[<xref rid="B33-sensors-25-05753" ref-type="bibr">33</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">81.6</td><td align="left" valign="middle" rowspan="1" colspan="1">1,756,950</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">67</td><td align="center" valign="middle" rowspan="1" colspan="1">97.5</td><td align="center" valign="middle" rowspan="1" colspan="1">66</td><td align="center" valign="middle" rowspan="1" colspan="1">6.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10&#160;[<xref rid="B34-sensors-25-05753" ref-type="bibr">34</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">82.6</td><td align="left" valign="middle" rowspan="1" colspan="1">2,695,196</td><td align="center" valign="middle" rowspan="1" colspan="1">5.5</td><td align="center" valign="middle" rowspan="1" colspan="1">104</td><td align="center" valign="middle" rowspan="1" colspan="1">97.3</td><td align="center" valign="middle" rowspan="1" colspan="1">69.2</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO11</td><td align="left" valign="middle" rowspan="1" colspan="1">84.1</td><td align="left" valign="middle" rowspan="1" colspan="1">2,582,542</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2</td><td align="center" valign="middle" rowspan="1" colspan="1">118</td><td align="center" valign="middle" rowspan="1" colspan="1">97.7</td><td align="center" valign="middle" rowspan="1" colspan="1">71.0</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv12&#160;[<xref rid="B35-sensors-25-05753" ref-type="bibr">35</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">84.5</td><td align="left" valign="middle" rowspan="1" colspan="1">2,557,118</td><td align="center" valign="middle" rowspan="1" colspan="1">5.3</td><td align="center" valign="middle" rowspan="1" colspan="1">72</td><td align="center" valign="middle" rowspan="1" colspan="1">98.3</td><td align="center" valign="middle" rowspan="1" colspan="1">71.9</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv13&#160;[<xref rid="B36-sensors-25-05753" ref-type="bibr">36</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">83.7</td><td align="left" valign="middle" rowspan="1" colspan="1">2,448,285</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2</td><td align="center" valign="middle" rowspan="1" colspan="1">52</td><td align="center" valign="middle" rowspan="1" colspan="1">98.8</td><td align="center" valign="middle" rowspan="1" colspan="1">66.4</td><td align="center" valign="middle" rowspan="1" colspan="1">6.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RTDETR (ResNet50) [<xref rid="B37-sensors-25-05753" ref-type="bibr">37</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">84</td><td align="left" valign="middle" rowspan="1" colspan="1">41,938,794</td><td align="center" valign="middle" rowspan="1" colspan="1">82</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td><td align="center" valign="middle" rowspan="1" colspan="1">77.9</td><td align="center" valign="middle" rowspan="1" colspan="1">125</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">620,598</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">163</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05753-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05753-t005_Table 5</object-id><label>Table 5</label><caption><p>Results of the backbone network comparison experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Backbone</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAP50 (%)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GFLOPs</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">StarNet</td><td align="left" valign="middle" rowspan="1" colspan="1">83.1</td><td align="left" valign="middle" rowspan="1" colspan="1">1,598,406</td><td align="center" valign="middle" rowspan="1" colspan="1">3.3</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td><td align="center" valign="middle" rowspan="1" colspan="1">96.5</td><td align="center" valign="middle" rowspan="1" colspan="1">70.8</td><td align="center" valign="middle" rowspan="1" colspan="1">3.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FasterNet&#160;[<xref rid="B38-sensors-25-05753" ref-type="bibr">38</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">82.9</td><td align="left" valign="middle" rowspan="1" colspan="1">2,378,314</td><td align="center" valign="middle" rowspan="1" colspan="1">5.1</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td><td align="center" valign="middle" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" rowspan="1" colspan="1">69.1</td><td align="center" valign="middle" rowspan="1" colspan="1">5.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ShuffleNetv2&#160;[<xref rid="B39-sensors-25-05753" ref-type="bibr">39</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">74.9</td><td align="left" valign="middle" rowspan="1" colspan="1">1,705,318</td><td align="center" valign="middle" rowspan="1" colspan="1">3.6</td><td align="center" valign="middle" rowspan="1" colspan="1">102</td><td align="center" valign="middle" rowspan="1" colspan="1">94.2</td><td align="center" valign="middle" rowspan="1" colspan="1">65</td><td align="center" valign="middle" rowspan="1" colspan="1">4.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EfficientNetv2 [<xref rid="B40-sensors-25-05753" ref-type="bibr">40</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">81.7</td><td align="left" valign="middle" rowspan="1" colspan="1">2,086,978</td><td align="center" valign="middle" rowspan="1" colspan="1">4.4</td><td align="center" valign="middle" rowspan="1" colspan="1">96</td><td align="center" valign="middle" rowspan="1" colspan="1">97.1</td><td align="center" valign="middle" rowspan="1" colspan="1">69</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MobileNetv3&#160;[<xref rid="B41-sensors-25-05753" ref-type="bibr">41</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2,144,716</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">114</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05753-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05753-t006_Table 6</object-id><label>Table 6</label><caption><p>Results of the neck network comparison experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Neck</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAP50 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GFLOPs</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">EPAN</td><td align="left" valign="middle" rowspan="1" colspan="1">85</td><td align="left" valign="middle" rowspan="1" colspan="1">1,788,782</td><td align="center" valign="middle" rowspan="1" colspan="1">3.7</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td><td align="center" valign="middle" rowspan="1" colspan="1">98.3</td><td align="center" valign="middle" rowspan="1" colspan="1">73.2</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PaNet&#160;[<xref rid="B42-sensors-25-05753" ref-type="bibr">42</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">84.1</td><td align="left" valign="middle" rowspan="1" colspan="1">2,582,542</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2</td><td align="center" valign="middle" rowspan="1" colspan="1">118</td><td align="center" valign="middle" rowspan="1" colspan="1">97.7</td><td align="center" valign="middle" rowspan="1" colspan="1">71.0</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BiFPN&#160;[<xref rid="B43-sensors-25-05753" ref-type="bibr">43</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">84.7</td><td align="left" valign="middle" rowspan="1" colspan="1">2,670,538</td><td align="center" valign="middle" rowspan="1" colspan="1">5.4</td><td align="center" valign="middle" rowspan="1" colspan="1">96</td><td align="center" valign="middle" rowspan="1" colspan="1">98.9</td><td align="center" valign="middle" rowspan="1" colspan="1">68.4</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SlimNeck&#160;[<xref rid="B44-sensors-25-05753" ref-type="bibr">44</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2,731,678</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">105</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.3</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>