<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">J Med Ethics Hist Med</journal-id><journal-id journal-id-type="iso-abbrev">J Med Ethics Hist Med</journal-id><journal-id journal-id-type="pmc-domain-id">2152</journal-id><journal-id journal-id-type="pmc-domain">jmehm</journal-id><journal-id journal-id-type="publisher-id">JMEHM</journal-id><journal-title-group><journal-title>Journal of Medical Ethics and History of Medicine</journal-title></journal-title-group><issn pub-type="epub">2008-0387</issn><publisher><publisher-name>Tehran University of Medical Sciences</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12474519</article-id><article-id pub-id-type="pmcid-ver">PMC12474519.1</article-id><article-id pub-id-type="pmcaid">12474519</article-id><article-id pub-id-type="pmcaiid">12474519</article-id><article-id pub-id-type="doi">10.18502/jmehm.v18i2.18812</article-id><article-id pub-id-type="publisher-id">JMEHM-18-2</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>Philosophy of medicine meets AI hallucination and AI drift: moving toward a more gentle medicine</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Namazi</surname><given-names initials="H">Hamidreza</given-names></name><xref rid="aff1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Radfar</surname><given-names initials="MM">Mohammad Mahdi</given-names></name><xref rid="aff2" ref-type="aff">2</xref><xref rid="cor1" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<italic toggle="yes">Assistant Professor, Medical Ethics and History of Medicine Research Center, Tehran University of Medical Sciences, Tehran, Iran.</italic>
</aff><aff id="aff2">
<label>2</label>
<italic toggle="yes">Researcher, Medical Doctor, Medical Ethics and History of Medicine Research Center, Tehran University of Medical Sciences, Tehran, Iran.</italic></aff><author-notes><corresp id="cor1"><label>*</label>Corresponding Author: Mohammad Mahdi Radfar. Address: No. 8, Bahare Dead-end, East Brazil Street, Vanak Sq., Tehran, Iran. Postal Code : 1435713836. Tel: (+98) 21 66 41 96 61. <email>Email: mhrad24@gmail.com</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub"><day>27</day><month>5</month><year>2025</year></pub-date><volume>18</volume><issue-id pub-id-type="pmc-issue-id">497681</issue-id><elocation-id>2</elocation-id><history><date date-type="received"><day>14</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>25</day><month>5</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>27</day><month>05</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>28</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 00:25:15.170"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 Tehran University of Medical Sciences.</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>). Non-commercial uses of the work are permitted, provided the original work is properly cited.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="JMEHM-18-2.pdf"/><abstract><p>The contemporary world is profoundly shaped by technological progress. Among the advancements of our era is the proliferation of artificial intelligence (AI). AI has permeated every facet of human knowledge, including medicine. One domain of AI development is the application of large language models (LLMs) in health-care settings. While these applications hold immense promise, they are not without challenges. Two notable phenomena, AI hallucination and AI drift, pose setbacks. AI hallucination refers to the generation of erroneous information by AI systems, while AI drift is the production of multiple responses to a single query. The emergence of these challenges underscores the crucial role of the philosophy of medicine. By reminding practitioners of the inherent uncertainty that underpins medical interventions, the philosophy of medicine fosters a more receptive stance toward these technological advancements. Furthermore, by acknowledging the inherent fallibility of these technologies, the philosophy of medicine reinforces the importance of gentle medicine and humility in clinical practice. Physicians must not shy away from embracing AI tools due to their imperfections. Acknowledgment of uncertainty fosters a more accepting attitude toward AI tools among physicians, and by constantly highlighting the imperfections, the philosophy of medicine cultivates a deeper sense of humility among practitioners. It is imperative that experts in the philosophy of medicine engage in thoughtful deliberation to ensure that these powerful technologies are harnessed responsibly and ethically, preventing the reins of medical decision-making from falling into the hands of those without the requisite expertise and ethical grounding.</p></abstract><kwd-group><title>Key Words</title><kwd><italic toggle="yes">AI hallucination</italic></kwd><kwd><italic toggle="yes">AI drift</italic></kwd><kwd><italic toggle="yes">Data schizophrenia</italic></kwd><kwd><italic toggle="yes">Medical philosophy</italic></kwd><kwd><italic toggle="yes">Large language models</italic></kwd><kwd><italic toggle="yes">Gentle medicine.</italic></kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro"><title>Introduction</title><p>Our world today is deeply influenced by novel technological advancements. The advent of artificial intelligence (AI) is a paradigm shift comparable to the invention of the microprocessor, the personal computer, the Internet, and the smartphone. AI has the potential to revolutionize the way we work, learn, travel, receive healthcare, and communicate with each other. Entire industries will be reorganized around AI, and businesses will be differentiated by their ability to harness its power. Whether or not we are comfortable with this paradigm shift, our future will be inextricably linked to AI. No field is immune to its influence, including medicine. A recent innovation in the field of artificial intelligence (AI) is the development of large language models (LLMs) known as Chatbots. Chatbots are electronic systems that simulate human conversations using natural language processing and can be integrated into various platforms like websites and apps. Advanced models like ChatGPT, Gemini and Claude2 are designed to assist users across various fields, from customer service to scientific research, offering capabilities such as answering questions and analyzing data. AI hallucination and AI drift are two key challenges of large language models (LLMs). Hallucination occurs when an AI generates incorrect or fictitious responses, while drift refers to inconsistencies in answers to the same question when posed by different users or as a result of variations in how the question is asked. These are caused by a variety of issues, including the inability of large language models to distinguish between correlation and causation, which will be explained in detail in this article. This is where the inherent role of uncertainty in medical interventions comes into play and factors like evidence quality, clinician experience and patient circumstances shape confidence in diagnoses and treatments. Ultimately, uncertainty is intrinsic to medical practice, highlighting the need for humility and a nuanced approach in addressing each patient's unique situation. The concept of gentle medicine encourages a humble approach in medical practice, advocating for caution in diagnoses and treatments while emphasizing disease prevention. It promotes effective interventions but supports a more restrained application. There is a recognition of the limitations of artificial intelligence in medicine, highlighting the inherent uncertainties in decision-making. The philosophy of medicine underscores the need for humility when using AI, acknowledging its dual nature as both helpful and potentially harmful. Therefore, medical institutions should prioritize ethical training for professionals to responsibly navigate the integration of AI tools in healthcare.</p><p>
<italic toggle="yes">What is the role of chatbots in medicine?</italic>
</p><p>A chatbot is an electronic system (generally a software) that simulates conversations by responding to keywords or phrases it recognizes and can be integrated into various platforms, such as websites, mobile apps, and messaging platforms. A chatbot is a computer program that simulates human conversation through natural language processing (NLP). Chatbots are often used in customer service applications, but they can also be used for education, entertainment, and public purposes (<xref rid="B1" ref-type="bibr">1</xref>).</p><p>NLP is a field of computer science that deals with the interaction between computers and human (natural) languages. NLP algorithms are used to understand and generate human language, which is essential for chatbots to simulate human conversation (<xref rid="B15" ref-type="bibr">15</xref>).</p><p>Chatbots can be integrated into a variety of platforms, including websites, mobile apps, and messaging platforms. This allows users to interact with chatbots in a convenient and natural way.</p><p>In this article, we provide a concise overview of several pioneers in the field of artificial intelligence (AI):</p><p>
<italic toggle="yes">1) ChatGPT</italic>
</p><p>ChatGPT, a sophisticated tool developed by OpenAI, has emerged as a prominent player among the diverse array of natural language processing models. This AI-powered chatbot is meticulously crafted to simulate human-like conversations. ChatGPT's functionality stems from its utilization of advanced algorithms programmed to grasp natural language inputs and generate corresponding responses. These responses can be either pre-written or freshly generated by the AI itself (<xref rid="B14" ref-type="bibr">14</xref>). Also, OpenAI recently introduced a new search engine empowered by its large language model known as SearchGPT, which can produce answers based on online data from different sources. ChatGPT's utility has been demonstrated across a spectrum of fields, including medical research and literature review. It can effectively assist medical researchers and scientists in crafting articles and abstracts, compressing data or information, providing structural, referential and titular suggestions, and enhancing the readability of texts. Remarkably, ChatGPT can even generate a complete draft of a paper (<xref rid="B10" ref-type="bibr">10</xref>).</p><p>While incapable of generating novel ideas, artificial intelligence can effectively organize and refine those conceived by researchers, producing an initial draft. However, this should be considered a starting point for human-led text development, as the AI-generated text falls short of being a substitute for human expertise (<xref rid="B8" ref-type="bibr">8</xref>).</p><p>
<italic toggle="yes">2) Gemini</italic>
</p><p>Another promising large language model is Gemini, the Google's AI project that has been launched based on the Palm2 language model. Gemini can search through websites and is empowered by Google search engine, as stated by the Google CEO (<xref rid="B3" ref-type="bibr">3</xref>).</p><p>You can ask Gemini to answer math problems, schedule a meeting in your workplace, and write an email in a foreign language, choosing your favorite tone, wording and so on. Gemini can be used when online medical information is needed, for instance it can help students learn about up-to-date medical approaches. Furthermore, you can ask Gemini to summarize articles in any way you need (<xref rid="B19" ref-type="bibr">19</xref>).</p><p>
<italic toggle="yes">3) Claude2</italic>
</p><p>Claude2 is another large language model introduced by Anthropic, presently available only in the UK and USA. Numerous modifications have been made to Claude2 in order to make it better and faster than the previous large language models. For example, you can upload information to it in the form of a text and ask for analysis. This could be used in medical judgment in complicated cases where analyzing a vast amount of information is hard for the human mind (<xref rid="B12" ref-type="bibr">12</xref>).</p><p>
<italic toggle="yes">4) Replika</italic>
</p><p>There are some other types of AI large language models that are not merely for academic uses. Replika is an AI-based language model that you can use on your cellphone as a friend. First, it asks you some personal questions to learn about your personality and afterward, it will treat you like a friend. Just like Samantha in the movie named "Her".</p><p>Despite its limitations, Replika is a valuable tool for people who are looking for companionship, support, or simply someone to talk to. It can also be a helpful tool for people exploring their identity and looking to learn new things about themselves (<xref rid="B13" ref-type="bibr">13</xref>). In addition to its common application, Replika can be utilized in medicine, for instance medical students can use this tool to strengthen their empathy and communication skills with patients.</p><p>Large language models are advancing daily, with new models being designed and implemented to meet emerging needs. The models released by Meta can be cited as examples. These language models are continuously expanding, but they will not be discussed here in order to avoid lengthening this article.</p><p>LLMs have had significant success in a variety of tasks, including medical research. Some experts believe that the invention of LLMs and their application in medicine has the potential to revolutionize the field, just as the invention of radiology imaging did (<xref rid="B20" ref-type="bibr">20</xref>).</p><p>However, it should be noted that LLMs are not without limitations and like radiology imaging, they can be prone to bias and error. Therefore, it is important to carefully evaluate the output of LLMs before relying on them for medical decision-making.</p><p>
<italic toggle="yes">AI hallucination and AI drift: The Siamese Twins of Large Language Models</italic>
</p><p>Two common challenges associated with large language models (LLMs) are AI hallucination and AI drift. Authors tend to call these challenges "schizophrenic reactions". This metaphor helps us to better understand how humane these challenges are.</p><p>A large language model hallucination is a situation when an AI model generates senseless or imaginative content. For example, when AI is asked about a scientific concept, the answer may be incorrect and sometimes even completely fictitious. In fact, quite often it will produce false data and responses (<xref rid="B4" ref-type="bibr">4</xref>). This phenomenon is referred to as "AI hallucination".</p><p>Another term that needs to be discussed is "AI drift", which is a common enough problem with large language models. AI drift happens when one single question leads to different answers. This different answer may arise either when the question has been asked by a different user, or by the same person in another manner or at a different time (<xref rid="B6" ref-type="bibr">6</xref>).</p><p>
<italic toggle="yes">Why do these phenomena happen?</italic>
</p><p>Researchers have found some of the main reasons for these alterations of reality. Here are some of the most probable causes:</p><p>
<italic toggle="yes">1) Being outdated</italic>
</p><p>There is a lag on data insertion in many of these large language models. The previous version of ChatGPT, for example, strives to generate output that aligns with anticipated outcomes based on its training data. Due to ChatGPT's knowledge cutoff date of September 2021, it is unable to engage in discussions regarding research papers published in 2024. As a result, any forward-looking statement extending beyond September 2021 is likely to be inaccurate, constituting a fabrication devoid of any connection to the 2024 publication. ChatGPT's output solely consists of sentences constructed upon the relationship among preceding words, sometimes resulting in nonsensical content and potentially causing a range of consequences, from misunderstandings to the dissemination of misinformation (<xref rid="B2" ref-type="bibr">2</xref>).</p><p>
<italic toggle="yes">2) Disconnection from open-source information</italic>
</p><p>There have been reports of ChatGPT not being able to search the web and access the National Library of Medicine or other relevant sources with up-to date information (<xref rid="B2" ref-type="bibr">2</xref>). However, this issue has been partially resolved in the newer versions of large language models. As mentioned above, Gemini's advantage over ChatGPT was its access to online databases and real-time articles, but SearchGPT addressed this issue by being able to search online databases.</p><p>
<italic toggle="yes">3) Invalidation of Sources</italic>
</p><p>In ChatGPT and other large language models (LLMs), artificial hallucinations may occur due to a lack of source attribution. The source of specific responses is often unknown, and using different sources with varying information can lead to different answers and conclusions (<xref rid="B2" ref-type="bibr">2</xref>).</p><p>
<italic toggle="yes">4) The Role of temperature</italic>
</p><p>The temperature of a large language model (LLM) significantly influences its output and the likelihood of artificial hallucinations. Temperature can be conceptualized as the LLM's level of certainty in its most probable response. An elevated temperature generates a less confident and more diverse response, which can lead to hallucinations. In the context of LLMs, temperature serves as a hyperparameter that controls the degree of stochasticity in the model's output. This parameter ranges from 0 to 1, with a higher temperature producing more diverse and random outputs, while a lower temperature yields more predictable and conservative outputs. Specifically, when the temperature is high, the LLM is less certain about the most likely next word and more prone to selecting less probable words. This can lead to more creative and unexpected responses, but also increases the risk of producing nonsensical or hallucinatory text. Conversely, when the temperature is low, the LLM is more confident in its most likely next word and less likely to deviate from the most probable sequence. This results in more predictable and conservative outputs, but also limits the LLM's ability to generate creative and original texts (<xref rid="B4" ref-type="bibr">4</xref>).</p><p>Consequently, the temperature parameter serves as a pivotal element in harmonizing the creative and coherent aspects of an LLM's output. By manipulating the temperature, users can deftly adapt the model's behavior to suit specific tasks, encompassing the generation of creative text formats and the production of factual and reliable information.</p><p>The optimal temperature for an LLM is context-dependent and varies in accordance with the specific task at hand. For instance, if an LLM is employed to generate creative text formats such as poems or codes, a higher temperature setting may be preferred to encourage the model to exhibit greater innovation. Conversely, if an LLM is utilized to generate factual text such as a news article or a scientific paper, a lower temperature setting may be employed to guarantee the accuracy of the generated output. For example, ChatGPT uses a temperature of 0.7 for its predictions, which allows it to generate more diverse responses, but also makes it more likely to hallucinate (<xref rid="B4" ref-type="bibr">4</xref>).</p><p>
<italic toggle="yes">5) Keywords and prompts</italic>
</p><p>There is an important difference between a research engine motor such as Google and a large language model such as ChatGPT. A search engine uses keywords provided by users to investigate present articles containing them. On the contrary, an AI language model uses prompts given by users to trawl information from the vast number of ideas living within it. When it comes to prompts and ideas, user's prompt semantically precedes the AI. A prompt provided by the user appears to give meaning to distorted, complex and entropic ideas living within that large language model.</p><p>This could be another reason for AI drifting. When you ask Dr. House (the main character of the series <italic toggle="yes">House, M.D.</italic>) for the best treatment for your rash, he explores the vast amount of his knowledge and prophetically answers, "Get a divorce!" These AI language models angle the idea by the prompt that you give to them. Hence, different answers can arise when the same question has been asked.</p><p>Returning to our metaphor of a schizophrenic person, Dr. House may give a seemingly random answer that would sound nonsensical to an observer who does not understand his thought process. However, this does not necessarily mean that random answers should be treated as low-quality answers.</p><p>In the context of large language models (LLMs), randomness can be used to generate more creative and interesting outputs. For example, an LLM can be configured to generate texts with a higher temperature, which will result in more random and diverse outputs. This can be useful for tasks such as generating poems, codes, or other creative content.</p><p>
<italic toggle="yes">6) Correlation and causation dichotomy</italic>
</p><p>There is an obvious distinction between correlation and causation. For instance, I have noticed that over the past month, whenever I ordered my coffee in a red shirt, the weather was cloudy. However, this does not mean that the weather is tuned to my fashion choices! On the other hand, whenever I oversleep, I get to work late. In this case, I can assume that there is a causation between delayed awakening and being late for work.</p><p>Correlation and causation, while often intertwined in scientific discourse, represent distinct conceptual frameworks. Correlation quantifies the statistical association between two variables, while causation establishes a causal relationship, implying that one variable directly influences the other.</p><p>The philosophical exploration of causation dates back to ancient Greece. In his seminal work, Metaphysics, Aristotle proposed four fundamental causes responsible for natural phenomena: material, formal, efficient, and final. This framework represents the culmination of pre-Socratic and Platonic discussions on causation. The material cause refers to the substance or matter from which an object is composed, while the formal cause embodies the blueprint or design that determines its form and structure. The efficient cause identifies the agent or process that brings the object into existence, and the final cause delineates the purpose or function for which it exists (<xref rid="B11" ref-type="bibr">11</xref>).</p><p>In the realm of natural sciences, understanding the nature of causation is crucial for elucidating causal relationships and connections among natural phenomena. Scientists must be able to distinguish between entities that genuinely cause natural phenomena and those that merely exhibit a statistical association. Distinguishing correlation from causation is essential for avoiding spurious relationships and drawing valid conclusions about natural phenomena. While correlation may suggest a connection between two variables, it does not establish a causal mechanism. Correlation can arise from a common underlying cause, chance, or even a third, unidentified variable. Establishing causation requires a more rigorous approach, often involving controlled experiments, observational studies, and careful consideration of alternative explanations. By isolating the variable of interest and manipulating it under controlled conditions, scientists can determine whether a causal relationship exists (<xref rid="B11" ref-type="bibr">11</xref>).</p><p>The example of ordering coffee in a red shirt on cloudy days is an instance of a correlation. There is a statistical relationship between the two variables, but it is not clear whether one causes the other. There could be a third variable, such as my mood, which is causing both of the other variables.</p><p>The example of oversleeping and being late for work, however, represents causation, since it is more likely that the former is causing the latter, rather than vice versa.</p><p>It appears that AI language models are better in realizing the correlations than causations. When training an AI language model, machine learning is based on a huge amount of data given to the computer. What happens if the data are merely correlated and not caused by each other? Does this make answers of the AI language model vague? How much would the certainty of the answers be under the circumstances?</p><p>The widespread use of data and the rise of big data have given us new capabilities that could usher in a new era. However, they can also lead to the misuse of information, known as "garbage in, garbage out", or GIGO (<xref rid="B18" ref-type="bibr">18</xref>). When users feed a large language model with raw, random or rubbish information, the output cannot be expected to be any better.</p><p>Large language models are not yet able to distinguish between correlation and causation. Therefore, developers should be cautious when using online data such as tweets and blogs. Security concerns may also increase, given that millions of dollars are now being spent by organizations and governments to produce false information.</p><p>Now, is there a moral responsibility for developers and engineers to protect large language models against this type of information? That certainly seems like an important question, since so much is at stake in this regard.</p><p>
<italic toggle="yes">Need for certainty and tolerance of ambiguity</italic>
</p><p>Here we are speaking of "data schizophrenia". When AI has its own brain and neural network, is it not right to entitle AI hallucination as a schizophrenic reaction? Both AI hallucinations and schizophrenic hallucinations can be caused by problems with the way that the brain processes information. Additionally, both can appear to be real or unreal.</p><p>In light of the challenges and interpretations discussed above, a new question arises: What is the role of certainty in medical decision-making?</p><p>The level of certainty with which a clinician establishes a diagnosis and formulates a treatment plan for a patient is a pivotal aspect of medical decision-making. Certainty is shaped by a constellation of factors, including the caliber and abundance of available evidence, the clinician's expertise, and the patient's unique circumstance. The quality and quantity of available evidence hold immense sway over certainty. When robust evidence supports a diagnosis or treatment, clinicians are more apt to exercise confidence in their decision-making. However, when evidence is scarce or contradictory, clinicians may grapple with uncertainty.</p><p>Clinician experience serves as another cornerstone of certainty. Clinicians with extensive experience are generally more likely to exhibit confidence in their decision-making, having encountered a broader spectrum of patients and conditions. Yet, even seasoned clinicians can err, underscoring the significance of meticulously scrutinizing all available evidence before reaching a verdict. The patient's individual circumstances can also modulate certainty. For instance, if a patient presents with a rare or complex condition, clinicians may face heightened uncertainty regarding the most appropriate course of treatment. Moreover, patient preferences and values can also play a crucial role in decision-making (<xref rid="B17" ref-type="bibr">17</xref>).</p><p>To quote Sir William Osler, the father of modern medicine who initiated bedside clinical education: "Medicine is a science of uncertainty and an art of probability" (<xref rid="B16" ref-type="bibr">16</xref>).</p><p>Uncertainty lies at the core of all medical interactions. Medical practice is inherently entwined with uncertainty stemming from a multitude of factors, including technical uncertainties arising from insufficient data, interpersonal uncertainties rooted in the complexities of the physician-patient relationship, and conceptual uncertainties emerging from the application of broad principles to specific scenarios. The inherent generality of scientific knowledge necessitates a nuanced approach to address the unique intricacies of each patient's illness. Consequently, intrinsic uncertainty remains an inextricable component of healthcare. The inherent elusiveness of medical interventions underscores the importance of humility among medical professionals in order to foster a more reciprocal and dialectical approach to the practice of medicine (<xref rid="B17" ref-type="bibr">17</xref>).</p><p>
<italic toggle="yes">Role of automation bias</italic>
</p><p>One of the risks that undermines certainty in clinical decision-making is automation bias. In the past, whenever a physician made a judgment about a patient, that judgment was considered definitive. Physicians would evaluate all aspects and take into account sufficient evidence before arriving at a clinical decision, which was deemed adequate. However, with the daily expansion of artificial intelligence capabilities, the opinions provided by AI are increasingly presented to physicians with a sense of authority.</p><p>In one study, it was observed that after incorrect responses were given by an AI system regarding an electrocardiogram, physicians' treatment decisions for patients changed significantly (<xref rid="B5" ref-type="bibr">5</xref>). The question that arises here is: Can certainty still be leveraged for change, despite awareness of the shortcomings of artificial intelligence? What impact does AI, with all its flaws, have on the automation of patient care and physicians' practices? Can a physician easily alter their opinions about a patient based solely on AI recommendations?</p><p>
<italic toggle="yes">Tolerance of ambiguity leads medicine to be Gentler</italic>
</p><p>In his book <italic toggle="yes">Medical Nihilism</italic>, Jacob Stegenga recommends that doctors adopt a more humble approach in cases such as those mentioned above. Knowing that magic bullets are rare in medicine and that many of the diagnoses and treatments are incorrect, doctors should proceed with more caution. They should prescribe fewer treatments and focus more on disease prevention.</p><p>The concept of gentle medicine is not a call for the wholesale abandonment of medical intervention, and there are always a number of effective treatments that should be utilized. Rather, gentle medicine advocates for a more restrained approach to medical intervention, with physicians intervening less frequently, perhaps even much less frequently than they do today. In addition, gentle medicine emphasizes the importance of improving health through lifestyle and societal changes (<xref rid="B17" ref-type="bibr">17</xref>).</p><p>When we become aware of the problems and limitations of artificial intelligence, it is a mistake to give these achievements more credit than they deserve. Artificial intelligence and large language models can be very helpful in the field of medicine, and their use should definitely be expanded. However, they should always be viewed critically.</p><p>This is where gentle medicine comes in. Physicians should be aware that these tools can have significant limitations, which can add to uncertainty. As proposed by Stegenga, a more effective approach to improving health would be to focus on restructuring society in ways that address the underlying causes of disease, rather than relying on the "magic bullet" model of medical intervention that has dominated medicine in the past century (<xref rid="B17" ref-type="bibr">17</xref>).</p><p>In the absence of absolute certainty, clinicians must weigh the risks and benefits of different treatment options and make a decision that they believe is in the best interests of the patient. This may involve making a trade-off between certainty and other factors, such as the patient's preferences and the availability of resources.</p><p>In the past, some novel breakthroughs have been put aside just because they did not pass the certainty tests. Having said that, there has always been an acceptable amount of uncertainty when doctors face a problem. There are always errors occurring due to human boundaries, exhaustion, overload, burnout, and ideas simply not coming to mind at the right moment.</p><p>It seems that our "schizophrenic friends" (large language models), which can provide significant assistance to doctors, confront us with a new certainty that does not necessarily stem from the old medical traditions. The uncertainty of a doctor's decision-making could be due to a lack of information, and now these language models may also give us incorrect answers as a result of the aforementioned factors.</p><p>By reminding practitioners of the inherent uncertainty that underpins medical interventions, the philosophy of medicine fosters a more receptive stance toward these technological advancements. Furthermore, by acknowledging the inherent fallibility of these technologies, the importance of gentle medicine and humility in clinical practice will be reinforced. Physicians must not shy away from embracing AI tools due to their imperfections, but should develop a more accepting attitude toward them by acknowledging their uncertainty. Moreover, by constantly highlighting the imperfections, the philosophy of medicine cultivates a deeper sense of humility among practitioners.</p><p>Are we facing a new sort of ambiguity? Is there an acceptable range for AI language models to be uncertain? How will the ethics and legislation deal with this new era? Do the advantages of utilizing such language models outweigh their uncertainty?</p><p>Jacques Derrida's concept of the "pharmakon" is a useful lens through which to view the potential impact of AI language models on medicine. A pharmakon is something that can be both beneficial and harmful, depending on how it is used (<xref rid="B9" ref-type="bibr">9</xref>). Similarly, AI language models have the potential to improve medical care, but they also pose some risks.</p><p>Medical colleges and faculties have a responsibility to ensure that AI language models are used in a responsible and ethical way. They should develop training programs for medical professionals on how to use AI language models effectively and safely. They should also conduct research on the potential risks and benefits of AI language models in medicine.</p><p>If we as medical professionals take AI technologies for granted, tech unicorn companies will happily replace our responsibility. This would be a mistake, as tech companies may not have the same values or priorities as medical professionals. For example, a tech unicorn company may be more interested in profits than in providing high-quality medical care.</p><p>It is important to remember that AI language models are tools, and like any tool, they can be used for good or for bad. It is up to us to decide how we will use AI language models in medicine. We must use them in a way that is ethical, responsible, and in the best interests of our patients.</p></sec><sec sec-type="conclusions"><title>Conclusion</title><p>In this article, we have argued that AI hallucination and AI drift are two major challenges that we face as we move toward a future where AI models are increasingly used in medicine. These challenges raise a number of ethical questions. We have explored the philosophical and theoretical basis of the uncertainty of AI models in medicine. We have also discussed how we can interface with this new ambiguous era. There are two different aspects of the case that should not be overlooked. First, we must remember that AI flaws will give medical professionals a more humble and gentle approach, which will take place as soon as there is an acceptance in medical settings for these new technological advancements. Second, by acknowledging the uncertain and ambiguous nature of medicine, we will be more open to these new tools. This is where medical humanities plays an important role in answering such ethical questions.</p><p>The philosophy of medicine plays a vital part in fostering this awareness, reminding practitioners of the uncertainties that accompany medical interventions. By promoting a balanced perspective, we can harness the benefits of AI while ensuring that ethical considerations and human expertise remain at the forefront of medical practice. As we navigate this complex landscape, it is crucial for both medical professionals and philosophers to collaborate, guiding the responsible and ethical implementation of AI in healthcare.</p><p>We started the article with a quote from Aeneid by Virgil: <italic toggle="yes">"Facilis descensus averno: noctes atque dies patet atri ianua ditis; sed revocare gradium superasque evadere ad auras, hoc opus his labor est."</italic> (The gates of hell are open night and day. Smooth the decent and easy is the way, but to return and view the cheerful skies, in this task and mighty labor lies) (<xref rid="B7" ref-type="bibr">7</xref>). We strongly support any sort of progression in the medical field that could possibly palliate patient suffering and pain. It is easy to be completely open and receptive to AI, but if we neglect the possible frauds and problems that this new friend could bring us, we will be faced with new problems and challenges. Under the circumstances, it seems appropriate to practice more gentle medicine. This is where we have to be focused and revise our attempts, as <italic toggle="yes">in this task and mighty labor lies</italic>.</p><p>The limitations encountered while writing this article include the rapid growth and daily updates of large language models. Additionally, exploring practical and technical approaches to reduce errors and mistakes in language models should be addressed in further research.</p></sec></body><back><ack><title>Acknowledgements</title><p>The authors are grateful to Dr. Ali Malaeke and Dr.Younes Shokrkhah for their help with the first drafts of the article.</p></ack><sec><title>Notes:</title><p>
<bold>
<italic toggle="yes">Citation to this article:</italic>
</bold>
</p><p>
<italic toggle="yes">Namazi</italic>
<italic toggle="yes">H, Radfar MM.</italic>
<italic toggle="yes">Philosophy of medicine meets AI hallucination and AI drift: moving toward a more gentle medicine. J Med Ethics Hist Med. 2025; 18: 2.</italic>
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doi.org/10.18502/jmehm.v18i2.18812" ext-link-type="uri">https://doi.org/10.18502/jmehm.v18i2.18812 </ext-link></p></sec><sec sec-type="COI-statement"><title>Conflict of interest</title><p>There is not any conflict of interest among authors</p></sec><sec><title>Funding</title><p>This research did not receive any funds.</p></sec><ref-list><title>References</title><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Adamopoulou</surname><given-names>E</given-names></name><name name-style="western"><surname>Moussiades</surname><given-names>L</given-names></name></person-group><source>An overview of chatbot technology</source><year>2020</year></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alkaissi</surname><given-names>H</given-names></name><name name-style="western"><surname>McFarlane</surname><given-names>SI</given-names></name></person-group><article-title>Artificial hallucinations in ChatGPT: implications in scientific writing</article-title><source> Cureus</source><year>2023</year><volume>15</volume><issue>2</issue><fpage>e35179</fpage><pub-id pub-id-type="pmid">36811129</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.7759/cureus.35179</pub-id><pub-id pub-id-type="pmcid">PMC9939079</pub-id></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aydin</surname><given-names>&#214;</given-names></name></person-group><article-title>Google Bard generated literature review: metaverse</article-title><source> J AI</source><year>2023</year><volume>7</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Beutel</surname><given-names>G</given-names></name><name name-style="western"><surname>Geerits</surname><given-names>E</given-names></name><name name-style="western"><surname>Kielstein</surname><given-names>JT</given-names></name></person-group><article-title>Artificial hallucination: GPT on LSD?</article-title><source> Crit Care</source><year>2023</year><volume>27</volume><issue>1</issue><fpage>148</fpage><pub-id pub-id-type="pmid">37072798</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s13054-023-04425-6</pub-id><pub-id pub-id-type="pmcid">PMC10114308</pub-id></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bond</surname><given-names>RR</given-names></name><name name-style="western"><surname>Novotny</surname><given-names>T</given-names></name><name name-style="western"><surname>Andrsova</surname><given-names>I</given-names></name><name name-style="western"><surname>Koc</surname><given-names>L</given-names></name><name name-style="western"><surname>Sisakova</surname><given-names>M</given-names></name><name name-style="western"><surname>Finlay</surname><given-names>D</given-names></name><etal/></person-group><article-title>Automation bias in medicine: the influence of automated diagnoses on interpreter accuracy and uncertainty when reading electrocardiograms</article-title><source> J Electrocardiol</source><year>2018</year><volume>51</volume><issue>6</issue><fpage>S6</fpage><lpage>S11</lpage><pub-id pub-id-type="pmid">30122457</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jelectrocard.2018.08.007</pub-id></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cambria</surname><given-names>E</given-names></name><name name-style="western"><surname>Mao</surname><given-names>R</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Ho</surname><given-names>SB</given-names></name></person-group><article-title>Seven pillars for the future of AI</article-title><source> IEEE Intell Syst</source><year>2023</year><volume>38</volume><issue>6</issue></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chandon</surname><given-names>G</given-names></name></person-group><source>Virgil. Stories from the Aeneid</source><year>1965</year><publisher-name>World Pub Co</publisher-name></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dave</surname><given-names>T</given-names></name><name name-style="western"><surname>Athaluri</surname><given-names>SA</given-names></name><name name-style="western"><surname>Singh</surname><given-names>S</given-names></name></person-group><article-title>ChatGPT in medicine: an overview of its applications, advantages, limitations, future prospects, and ethical considerations</article-title><source> Front Artif Intell.</source><year>2023</year><volume>6</volume><fpage>1169595</fpage><pub-id pub-id-type="pmid">37215063</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/frai.2023.1169595</pub-id><pub-id pub-id-type="pmcid">PMC10192861</pub-id></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kakoliris</surname><given-names>G</given-names></name></person-group><article-title>The "Undecidable" Pharmakon: Derrida's Reading of Plato's Phaedrus</article-title><source>The New Yearbook for Phenomenology and Phenomenological Philosophy</source><year>2015</year><publisher-name>Routledge</publisher-name><fpage>231</fpage><lpage>42</lpage></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H</given-names></name></person-group><article-title>The rise of ChatGPT: Exploring its potential in medical education</article-title><source> Anat Sci Educ</source><year>2024</year><volume>17</volume><issue>5</issue><fpage>926</fpage><lpage>931</lpage><pub-id pub-id-type="pmid">36916887</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/ase.2270</pub-id></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Marcum</surname><given-names>JA</given-names></name></person-group><source>An introductory philosophy of medicine: Humanizing modern medicine</source><year>2008</year><volume>Vol. 99</volume><publisher-name>Springer Science &amp; Business Media</publisher-name></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Morreel</surname><given-names>S</given-names></name><name name-style="western"><surname>Verhoeven</surname><given-names>V</given-names></name><name name-style="western"><surname>Mathysen</surname><given-names>D</given-names></name></person-group><article-title>Microsoft Bing outperforms five other generative artificial intelligence chatbots in the Antwerp University multiple choice medical license exam</article-title><source> PLOS Digit Health</source><year>2024</year><volume>3</volume><issue>2</issue><fpage>e0000349</fpage><pub-id pub-id-type="pmid">38354127</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pdig.0000349</pub-id><pub-id pub-id-type="pmcid">PMC10866461</pub-id></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Possati</surname><given-names>LM</given-names></name></person-group><article-title>Psychoanalyzing artificial intelligence: The case of Replika</article-title><source> AI Soc</source><year>2023</year><volume>38</volume><issue>4</issue><fpage>1725</fpage><lpage>38</lpage></element-citation></ref><ref id="B14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ray</surname><given-names>PP</given-names></name></person-group><article-title>ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope</article-title><source> Internet Things Cyber Phys Syst</source><year>2023</year></element-citation></ref><ref id="B15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reshamwala</surname><given-names>A</given-names></name><name name-style="western"><surname>Mishra</surname><given-names>D</given-names></name><name name-style="western"><surname>Pawar</surname><given-names>P</given-names></name></person-group><article-title>Review on natural language processing</article-title><source> IRACST Eng Sci Technol Int J</source><year>2013</year><volume>3</volume><issue>1</issue><fpage>113</fpage><lpage>6</lpage></element-citation></ref><ref id="B16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rysavy</surname><given-names>M</given-names></name></person-group><article-title>Evidence-based medicine: a science of uncertainty and an art of probability</article-title><source> Virtual Mentor</source><year>2013</year><volume>15</volume><issue>1</issue><fpage>4</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">23356799</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/virtualmentor.2013.15.1.fred1-1301</pub-id></element-citation></ref><ref id="B17"><label>17</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Stegenga</surname><given-names>J</given-names></name></person-group><source>Medical nihilism</source><year>2018</year><edition>1st ed</edition><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="B18"><label>18</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sukel</surname><given-names>M</given-names></name><name name-style="western"><surname>Rudinac</surname><given-names>S</given-names></name><name name-style="western"><surname>Worring</surname><given-names>M</given-names></name></person-group><article-title>GIGO, Garbage In, Garbage Out: An Urban Garbage Classification Dataset</article-title><source>Int Conf Multimed Model</source><year>2023</year></element-citation></ref><ref id="B19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thapa</surname><given-names>S</given-names></name><name name-style="western"><surname>Adhikari</surname><given-names>S</given-names></name></person-group><article-title>ChatGPT, bard, and large language models for biomedical research: opportunities and pitfalls</article-title><source> Ann Biomed Eng</source><year>2023</year><volume>51</volume><issue>12</issue><fpage>2647</fpage><lpage>2651</lpage><pub-id pub-id-type="pmid">37328703</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s10439-023-03284-0</pub-id></element-citation></ref><ref id="B20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thirunavukarasu</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Ting</surname><given-names>DSJ</given-names></name><name name-style="western"><surname>Elangovan</surname><given-names>K</given-names></name><name name-style="western"><surname>Gutierrez</surname><given-names>L</given-names></name><name name-style="western"><surname>Tan</surname><given-names>TF</given-names></name><name name-style="western"><surname>Ting</surname><given-names>DSW</given-names></name></person-group><article-title>Large language models in medicine</article-title><source> Nat Med</source><year>2023</year><volume>29</volume><issue>8</issue><fpage>1930</fpage><lpage>40</lpage><pub-id pub-id-type="pmid">37460753</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41591-023-02448-8</pub-id></element-citation></ref></ref-list></back></article></pmc-articleset>