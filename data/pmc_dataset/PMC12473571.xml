<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473571</article-id><article-id pub-id-type="pmcid-ver">PMC12473571.1</article-id><article-id pub-id-type="pmcaid">12473571</article-id><article-id pub-id-type="pmcaiid">12473571</article-id><article-id pub-id-type="pmid">41012913</article-id><article-id pub-id-type="doi">10.3390/s25185675</article-id><article-id pub-id-type="publisher-id">sensors-25-05675</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-Camera 3D Digital Image Correlation with Pointwise-Optimized Model-Based Stereo Pairing</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-3240-6687</contrib-id><name name-style="western"><surname>Qin</surname><given-names initials="W">Wenxiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05675" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="F">Feiyue</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05675" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Hu</surname><given-names initials="S">Shaopeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05675" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shimasaki</surname><given-names initials="K">Kohei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05675" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ishii</surname><given-names initials="I">Idaku</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05675" ref-type="aff">1</xref><xref rid="c1-sensors-25-05675" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Du</surname><given-names initials="S">Sheng</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Huang</surname><given-names initials="Z">Zixin</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Jin</surname><given-names initials="L">Li</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Yu</surname><given-names initials="P">Pan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05675"><label>1</label>Graduate School of Advanced Science and Engineering, Hiroshima University, Hiroshima 739-8527, Japan; <email>wenxiang-qin@hiroshima-u.ac.jp</email> (W.Q.); <email>feiyue@hiroshima-u.ac.jp</email> (F.W.); <email>simasaki@hiroshima-u.ac.jp</email> (K.S.)</aff><aff id="af2-sensors-25-05675"><label>2</label>Digital Monozukuri (Manufacturing) Education and Research Center, Hiroshima University, Hiroshima 739-0046, Japan; <email>hsp@hiroshima-u.ac.jp</email></aff><author-notes><corresp id="c1-sensors-25-05675"><label>*</label>Correspondence: <email>iishii@hiroshima-u.ac.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>11</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5675</elocation-id><history><date date-type="received"><day>08</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>08</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>08</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>11</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05675.pdf"/><abstract><p>Dynamic deformation measurement (DDM) is critical across infrastructure and industrial applications. Among various advanced techniques, multi-camera digital image correlation (MC-DIC) stands out due to its ability to achieve wide-range, full-field, and non-contact 3D DDM by pairing camera subsystems. However, existing MC-DIC methods typically rely on inefficient manual pairing or a simplistic strategy that aggregates all visible cameras for measuring specific object regions, leading to camera over-grouping. These limitations often result in cumbersome system setup and ill-measured deformations. To overcome these challenges, we propose a novel MC-DIC method with pointwise-optimized model-based stereo pairing (MPMC-DIC). By automatically evaluating and selecting camera pairs based on five evaluation factors derived from 3D model and calibrated cameras, the proposed method overcomes the over-grouping problem and achieves high-precision DDM of semi-rigid objects. A &#216;5 &#215; 5 cm cylinder experiment demonstrated an accuracy of 0.03 mm for both horizontal and depth displacements in the 0.0&#8211;5.0 mm range, and validated strong robustness against cluttered backgrounds using a 2 &#215; 4 camera array. Vibration measurement of a 9 &#215; 15 &#215; 16 cm PC speaker operating at 50 Hz, using eight surrounding cameras capturing 1920 &#215; 1080 images at 400 fps, confirmed the proposed method&#8217;s capability to perform wide-range dynamic deformation analysis and its robustness against complex object geometries.</p></abstract><kwd-group><kwd>3D dynamic deformation measurement</kwd><kwd>digital image correlation</kwd><kwd>multi-camera geometry</kwd><kwd>automatic camera pairing</kwd><kwd>3D model</kwd></kwd-group><funding-group><award-group><funding-source>JST A-STEP</funding-source><award-id>JPMJTR231A</award-id></award-group><award-group><funding-source>JST SPRING</funding-source><award-id>JPMJSP2132</award-id></award-group><funding-statement>This research was funded in part by JST A-STEP grant number JPMJTR231A and in part by JST SPRING grant number JPMJSP2132.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05675"><title>1. Introduction</title><p>Dynamic deformation measurement (DDM) has significant applications such as structural health monitoring [<xref rid="B1-sensors-25-05675" ref-type="bibr">1</xref>], industrial maintenance [<xref rid="B2-sensors-25-05675" ref-type="bibr">2</xref>], and vehicle refinement [<xref rid="B3-sensors-25-05675" ref-type="bibr">3</xref>]. Several contact and non-contact sensors are widely employed for accurate DDM, such as linear variable differential transducers [<xref rid="B4-sensors-25-05675" ref-type="bibr">4</xref>], accelerometers [<xref rid="B5-sensors-25-05675" ref-type="bibr">5</xref>], fiber optics sensors [<xref rid="B6-sensors-25-05675" ref-type="bibr">6</xref>], and laser sensors [<xref rid="B7-sensors-25-05675" ref-type="bibr">7</xref>]; yet they all suffer from different problems, like sparse measurements, mass loading, and spatial inconsistency. The rapid advancement in camera resolution has propelled the development of vision-based methods that utilize camera pixels as dense arrays of optical sensors. Vision-based methods are favored for their easy installation, as well as their non-contact, fast, and full-field consistent measurement capabilities. Pixel intensity array-tracking methods, such as digital image correlation (DIC) [<xref rid="B8-sensors-25-05675" ref-type="bibr">8</xref>] and template matching [<xref rid="B9-sensors-25-05675" ref-type="bibr">9</xref>], are widely adopted for sub-pixel displacement estimation.</p><p>Although DIC often requires a speckle pattern applied to the object surface to facilitate intensity matching, it is widely accepted as a non-contact displacement measurement technique, with the advantage of being mass load-free and independent of the tested material or the length scale of interest [<xref rid="B10-sensors-25-05675" ref-type="bibr">10</xref>]. DIC was first utilized in the 1980s to monitor aluminum specimens [<xref rid="B11-sensors-25-05675" ref-type="bibr">11</xref>], with tracking motion based on photo-consistency between pre- and post-deformation images. Since then, numerous research studies have advanced this technique in terms of refined shape function [<xref rid="B12-sensors-25-05675" ref-type="bibr">12</xref>], high-order interpolation [<xref rid="B13-sensors-25-05675" ref-type="bibr">13</xref>], enhanced intensity filtering [<xref rid="B14-sensors-25-05675" ref-type="bibr">14</xref>], precise and efficient parameter solving [<xref rid="B15-sensors-25-05675" ref-type="bibr">15</xref>], and boosted initial guess [<xref rid="B16-sensors-25-05675" ref-type="bibr">16</xref>]. To simplify the practical configurations and allow for full-field 3D measurements, 3D-DIC integrated stereo photogrammetry with 2D-DIC was performed in the 1990s [<xref rid="B17-sensors-25-05675" ref-type="bibr">17</xref>]. This integration enables 3D DDM using two overlapping images with the support of calibration [<xref rid="B18-sensors-25-05675" ref-type="bibr">18</xref>], stereo correspondence [<xref rid="B19-sensors-25-05675" ref-type="bibr">19</xref>], and triangulation [<xref rid="B20-sensors-25-05675" ref-type="bibr">20</xref>] from stereo photogrammetry. Meanwhile, speckle pattern also enhances calibration based on its rich features and improves measurement precision in the presence of environmental disturbances [<xref rid="B21-sensors-25-05675" ref-type="bibr">21</xref>]. Recently, the accuracy of stereo correspondence in 3D-DIC has been significantly enhanced through various strategies, such as image feature description and matching [<xref rid="B22-sensors-25-05675" ref-type="bibr">22</xref>], path-guided measurement [<xref rid="B23-sensors-25-05675" ref-type="bibr">23</xref>], geometric constrained semi-global matching [<xref rid="B24-sensors-25-05675" ref-type="bibr">24</xref>], and model-based projection [<xref rid="B25-sensors-25-05675" ref-type="bibr">25</xref>]. Deep learning has further elevated performance by increasing measurement range [<xref rid="B26-sensors-25-05675" ref-type="bibr">26</xref>], improving precision, and enabling super-resolution speckle image generation [<xref rid="B27-sensors-25-05675" ref-type="bibr">27</xref>]. Based on these advancements, 3D-DIC achieves micrometer-level accuracy by tracking the same patterned subset region across stereo image sequences. However, accurate tracking requires sufficient image overlap between stereo camera views, which inherently restricts the measurable region in 3D-DIC systems [<xref rid="B28-sensors-25-05675" ref-type="bibr">28</xref>].</p><p>A widely adopted solution for enlarging the measurement region is to use multi-camera systems supported by multi-camera geometry (MCG; also called multi-view geometry). MCG is a technique that is utilized for recovering 3D information from multiple 2D camera images, such as 3D shape measurement [<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>], 3D DDM [<xref rid="B30-sensors-25-05675" ref-type="bibr">30</xref>], 3D position tracking [<xref rid="B31-sensors-25-05675" ref-type="bibr">31</xref>], and 3D pose estimation [<xref rid="B32-sensors-25-05675" ref-type="bibr">32</xref>]. By capturing images from different locations, MCG strongly compensates for the regions that are difficult to cover by stereo configuration [<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>], with applications for cultural heritage archival practice, surgical planning, structural health monitoring, crime scene reconstruction, and entertainment [<xref rid="B33-sensors-25-05675" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05675" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05675" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05675" ref-type="bibr">36</xref>]. Multi-camera systems are established in two main ways: camera array systems or pseudo-camera systems. Camera array systems place multiple real cameras at different locations, and each camera records an individual view of the object; this configuration ensures high resolution and spatial consistency [<xref rid="B37-sensors-25-05675" ref-type="bibr">37</xref>]. Pseudo-camera systems generate virtual cameras by temporal motion, mirror, or a prism, allowing more cost-effective realization [<xref rid="B38-sensors-25-05675" ref-type="bibr">38</xref>]. The continued efforts to achieve 3D scene representation [<xref rid="B39-sensors-25-05675" ref-type="bibr">39</xref>], calibration [<xref rid="B40-sensors-25-05675" ref-type="bibr">40</xref>], stereo correspondence [<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>], and patchmatch-based depth estimation [<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>] have strongly advanced its development. On the other hand, multi-camera configurations also make camera grouping (selection) an important issue that is firmly related to measurement accuracy [<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>]. For accurate 3D shape measurement, several works have modelled it as an optimization problem aiming to achieve optimal camera grouping, based on properties estimated from 2D images, such as visibility, triangulation angle, incident angle, texture, depth, and normal vector [<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>].</p><p>Multi-camera DIC (MC-DIC), which integrates MCG with DIC, is famous for its strong capability for 3D DDM [<xref rid="B41-sensors-25-05675" ref-type="bibr">41</xref>]. These methods effectively extend the measurement region by fusing results measured at different object areas [<xref rid="B30-sensors-25-05675" ref-type="bibr">30</xref>]. MC-DIC with camera array systems enables wide-range measurements on panoramic and full-region dynamic deformations of column-shaped objects [<xref rid="B42-sensors-25-05675" ref-type="bibr">42</xref>] and beam-shaped structures [<xref rid="B43-sensors-25-05675" ref-type="bibr">43</xref>], respectively. MC-DIC with pseudo-camera systems supports dual-surface and panoramic measurements while maintaining system compactness and low cost [<xref rid="B28-sensors-25-05675" ref-type="bibr">28</xref>]. However, these methods show weakness at camera grouping due to the presence of unavoidable errors in estimated properties [<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>]. Unlike MCG-based 3D shape measurement, which generally aims at large-scale scenes and shows error tolerance, MC-DIC is sensitive to these errors, as it typically targets small-scale dynamic deformations [<xref rid="B28-sensors-25-05675" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05675" ref-type="bibr">29</xref>]. Nowadays, optimal camera grouping in MC-DIC remains a challenge and few methods have been proposed to target it; most MC-DIC methods typically use pre-paired cameras based on experience-guided manual configuration, which increases the complexity and effort required for system setup [<xref rid="B28-sensors-25-05675" ref-type="bibr">28</xref>,<xref rid="B30-sensors-25-05675" ref-type="bibr">30</xref>].</p><p>Recently, 3D models have been integrated into MC-DIC to represent object surfaces in 3D space for isogeometric analysis [<xref rid="B44-sensors-25-05675" ref-type="bibr">44</xref>]; formats include polygon mesh, and non-uniform rational B-spline (NURBS) [<xref rid="B45-sensors-25-05675" ref-type="bibr">45</xref>]. The rich prior spatial knowledge from 3D models enables the precise identification of camera visibility and facilitates accurate DDM by grouping visible cameras [<xref rid="B46-sensors-25-05675" ref-type="bibr">46</xref>]. This integration has been successfully applied to efficient 3D displacement measurement in aeronautical composite structures [<xref rid="B47-sensors-25-05675" ref-type="bibr">47</xref>], with measurement accuracy validated by high consistency with laser scans [<xref rid="B48-sensors-25-05675" ref-type="bibr">48</xref>]. However, these methods adopt a simplistic strategy that groups all visible cameras for a given measurement point, which often results in the inclusion of cameras that yield poor measurements [<xref rid="B49-sensors-25-05675" ref-type="bibr">49</xref>]. Such over-grouping can introduce ill-measured deformations caused by factors such as object&#8211;background discontinuity, self-occlusion, and reflective highlights. Overcoming this issue can significantly improve the robustness against cluttered backgrounds, complex object geometries, and environmental light variations.</p><p>To address this gap, we propose a novel MC-DIC method with pointwise-optimized model-based stereo pairing (MPMC-DIC), comprising model-based MC-DIC (MMC-DIC; an extended version of our previous model-based 3D-DIC [<xref rid="B25-sensors-25-05675" ref-type="bibr">25</xref>]) and a pointwise-optimized model-based stereo pairing strategy (PMSP). By automatically evaluating multiple cameras and selecting the optimal camera pair for each measurement point on the 3D model based on evaluation factors derived from the 3D model and calibrated cameras, MPMC-DIC overcomes the over-grouping problem and achieves high-precision wide-range 3D DDM of semi-rigid objects. Our main contributions are summarized as follows:<list list-type="simple"><list-item><label>(1)</label><p>A novel camera pair evaluation metric is proposed for pointwise-optimized model-based stereo pairing in 3D DDM tasks. Since each camera is evaluated individually prior to pair evaluation, the metric can also be applied to assess individual cameras for 2D-DIC.</p></list-item><list-item><label>(2)</label><p>An MC-DIC method with pointwise-optimized model-based stereo pairing is proposed. To the best of our knowledge, this is the first work dedicated to addressing camera pairing in MC-DIC, enhancing robustness against cluttered backgrounds and complex object geometries.</p></list-item><list-item><label>(3)</label><p>Experiments were conducted to validate the proposed MPMC-DIC method for 3D DDM, demonstrating micrometer-level accuracy and strong robustness against cluttered backgrounds and complex object geometries.</p></list-item></list></p><p>The paper is organized as follows: <xref rid="sec2-sensors-25-05675" ref-type="sec">Section 2</xref> describes our MPMC-DIC method for 3D displacement estimation based on a pre-measured 3D model. <xref rid="sec3-sensors-25-05675" ref-type="sec">Section 3</xref> validates our MPMC-DIC method of micrometer-level accuracy in measuring a centimeter-sized cylinder and robustness against cluttered backgrounds in comparison with the existing method which groups all visible cameras. <xref rid="sec4-sensors-25-05675" ref-type="sec">Section 4</xref> demonstrates the robustness of our MPMC-DIC method against complex geometries and illustrates its ability to precisely measure the vibrations of objects vibrating at audio frequencies by visualizing detailed vibrational characteristics.</p></sec><sec id="sec2-sensors-25-05675"><title>2. Method</title><p>To achieve effective and efficient wide-range 3D DDM, this study proposes a novel MC-DIC method with pointwise-optimized model-based stereo pairing. Specifically, the proposed method introduces five evaluation factors to overcome the over-grouping problem that typically arises when only visibility is considered. The five evaluation factors derived from the 3D model and multiple cameras, along with their respective functions, are as follows:<list list-type="bullet"><list-item><p><bold>Visibility</bold>, which determines the availability of cameras for measurement.</p></list-item><list-item><p><bold>Subset validity rate</bold>, which reflects the subset&#8217;s coverage ratio with measurement object. A lower coverage ratio leads to a greater influence from the background.</p></list-item><list-item><p><bold>Subset gradient</bold>, which reflects the depth inclination of measurement object relative to the camera in the subset region, especially depth discontinuities due to self-occlusion.</p></list-item><list-item><p><bold>Subset ZNCC similarity</bold> (hereinafter referred to as subset similarity), which reflects the matching confidence of correlated subsets in pre- and post-deformation images.</p></list-item><list-item><p><bold>Disparity</bold>, which reflects the angle between a pair of cameras relative to a measurement point. A small disparity often leads to high noise sensitivity, which consequently enlarges the error; a zero disparity disables 3D estimation.</p></list-item></list></p><p>Assuming multi-camera image sequences of a semi-rigid object and an associated reference 3D model, the proposed MPMC-DIC, illustrated in <xref rid="sensors-25-05675-f001" ref-type="fig">Figure 1</xref>, comprises MMC-DIC and PMSP. Following pipeline in [<xref rid="B25-sensors-25-05675" ref-type="bibr">25</xref>] and integrating multiple cameras and visibility determination, MMC-DIC involves four steps: (a1) Camera calibration to ensure precise measurement. (a2) Projection and visibility determination to identify the spatial relationship between measurement points and cameras. (a3) Two-dimensional-DIC to obtain 2D displacements. (a4) Three-dimensional displacement estimation based on camera pairs selected by PMSP. Note that MMC-DIC can also be applied without PMSP by using manual pairing instead. By leveraging the five evaluation factors, PMSP enables automatic and reliable (b1) Individual camera evaluation; (b2) Camera pair evaluation and selection. This ensures robust and precise wide-range 3D DDM.</p><p>MPMC-DIC utilizes multi-view reference images <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">K</italic> multi-view measurement images <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> captured by multiple cameras <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This method assumes that the target object behaves as a semi-rigid body, with a predefined 3D model &#937; serving as the reference framework. The 3D model contains <italic toggle="yes">N</italic> measurement points <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with normal vectors <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Below, we separately outline the detailed algorithm of MMC-DIC in <xref rid="sec2dot1-sensors-25-05675" ref-type="sec">Section 2.1</xref> and PMSP in <xref rid="sec2dot2-sensors-25-05675" ref-type="sec">Section 2.2</xref>.</p><sec id="sec2dot1-sensors-25-05675"><title>2.1. Model-Based MC-DIC (MMC-DIC)</title><p>The MMC-DIC algorithm is outlined in detail here, including steps (a1) to (a4), as shown in <xref rid="sensors-25-05675-f001" ref-type="fig">Figure 1</xref>.</p><list list-type="simple"><list-item><label>(a1)</label><p>Camera Calibration</p></list-item></list><p>Calibration for cameras involves capturing multiple images with known patterns, including the 3D shape of the measurement object. For each camera, intrinsic parameter matrix <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">M</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and distortion parameter vector <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are determined.</p><list list-type="simple"><list-item><label>(a2)</label><p>Projection and Visibility Determination</p></list-item></list><p>The poses of the cameras relative to the 3D model are pre-determined as the extrinsic parameter matrix <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and vector <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD1-sensors-25-05675"><label>(1)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mspace width="0.166667em"/><mml:mo>|</mml:mo><mml:mspace width="0.166667em"/></mml:mrow><mml:msup><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mfenced><mml:mo>=</mml:mo><mml:mi>reg</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>I</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#937;</mml:mo></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>reg</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> registers the 3D model and the reference image to determine extrinsic parameters.</p><p>The <italic toggle="yes">N</italic> measurement points on the 3D model are perspectively projected onto the 2D image planes of the cameras. For each camera, the projected points <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are computed using the intrinsic, distortion, and extrinsic parameters:<disp-formula id="FD2-sensors-25-05675"><label>(2)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mi>proj</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mi mathvariant="bold">M</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>proj</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the perspective projection.</p><p>The visibility of the cameras to each measurement point <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, as a strict guideline for cameras&#8217; availability for camera pairing, is determined by following conditions:<list list-type="bullet"><list-item><p>The point&#8217;s 2D projection is outside the camera image area;</p></list-item><list-item><p>The point&#8217;s normal vector is opposite to the cameras&#8217; orientation;</p></list-item><list-item><p>The point is occluded by the 3D model surfaces.</p></list-item></list></p><p>If any of the above factors apply, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; otherwise, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The <italic toggle="yes">c</italic>-th camera is considered for measuring the <italic toggle="yes">i</italic>-th measurement point only when <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; this consideration also encompasses its evaluation in PMSP.</p><list list-type="simple"><list-item><label>(a3)</label><p>2D-DIC</p></list-item></list><p>Subsets in cameras <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> centered at the <italic toggle="yes">i</italic>-th projected measurement point are set for 2D-DIC. The 2D displacements, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, of the <italic toggle="yes">i</italic>-th projected measurement point are computed at time <italic toggle="yes">t</italic> for each camera via 2D-DIC by correlating the subset region of reference images in measurement images:<disp-formula id="FD3-sensors-25-05675"><label>(3)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mi>DIC</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>DIC</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the 2D-DIC function.</p><list list-type="simple"><list-item><label>(a4)</label><p>Three-Dimensional Displacement Estimation</p></list-item></list><p>The <italic toggle="yes">N</italic> projected measurement points <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are considered to be displaced to <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> at time <italic toggle="yes">t</italic> in the 2D images as follows:<disp-formula id="FD4-sensors-25-05675"><label>(4)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open="(" close=")"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Camera pairs of the <italic toggle="yes">N</italic> measurement points, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, are determined by PMSP, as outlined in <xref rid="sec2dot2-sensors-25-05675" ref-type="sec">Section 2.2</xref>. The 3D positions of the <italic toggle="yes">N</italic> measurement points at time <italic toggle="yes">t</italic>, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">p</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, are estimated via triangulation using their 2D position vectors from selected camera pairs as follows:<disp-formula id="FD5-sensors-25-05675"><label>(5)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">p</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>tri</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">M</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>tri</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the triangulation function. The relative 3D displacement, <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, is computed as the difference between the 3D coordinates of the measurement points:<disp-formula id="FD6-sensors-25-05675"><label>(6)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">p</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot2-sensors-25-05675"><title>2.2. Pointwise-Optimized Model-Based Stereo Pairing (PMSP)</title><p>The PMSP algorithm is outlined in detail here, including steps (b1) and (b2), as shown in <xref rid="sensors-25-05675-f001" ref-type="fig">Figure 1</xref>.</p><list list-type="simple"><list-item><label>(b1)</label><p>Individual Camera Evaluation</p></list-item></list><p>Depth images <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are determined for each camera via 2D rendering of 3D model using intrinsic, distortion, and extrinsic parameters. Each pixel of a depth image records the depth distance of its perspectively corresponding 3D model point, or 0 when this pixel is not covered by the 3D model. Mask images <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> define the coverage region of 3D model rendering as follows:<disp-formula id="FD7-sensors-25-05675"><label>(7)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>otherwise</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The 3D model rendering coverage ratios in subsets <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are computed as subset validity rates <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD8-sensors-25-05675"><label>(8)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced separators="" open="|" close="|"><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mfenced></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="|" close="|"><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:math></inline-formula> computes the pixel number in the subset region.</p><p>The inclination degree of the 3D model region projected in the subsets is computed as subset gradients <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD9-sensors-25-05675"><label>(9)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mstyle><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false" form="prefix">max</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false" form="prefix">min</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are used to search for the maximum and minimum depth distance, respectively, in a 3 &#215; 3 domain of <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> regardless of the pixels uncovered by the 3D model rendering.</p><p>The subset similarities, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, of the <italic toggle="yes">i</italic>-th projected measurement point are computed at time <italic toggle="yes">t</italic> for each camera via 2D-DIC:<disp-formula id="FD10-sensors-25-05675"><label>(10)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>DIC</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mo>&#934;</mml:mo><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Note that <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>DIC</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> here denotes the same processing as in Equation (<xref rid="FD3-sensors-25-05675" ref-type="disp-formula">3</xref>), and subset similarities are computed along with the 2D displacements.</p><p>Camera evaluation scores at time <italic toggle="yes">t</italic>, <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, are computed for each measurement point as follows:<disp-formula id="FD11-sensors-25-05675"><label>(11)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="[" close="]"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the evaluation function for the subset validity rate with a parameter <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<xref rid="sensors-25-05675-f002" ref-type="fig">Figure 2</xref>a) as follows:<disp-formula id="FD12-sensors-25-05675"><label>(12)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>V</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>V</mml:mi><mml:mo>&#8804;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>otherwise</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>Although the subset validity rate is theoretically defined within the range <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, it typically takes values significantly above 0 due to 3D model rendering coverage. Accordingly, its evaluation function <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as a piecewise function, which employs a monotonically increasing linear function from 0 to 1 over the interval <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and a constant zero function otherwise, to enhance distinguishability while maintaining the linear relationship. <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined in the range <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In practice, it is selected close to the lower bound of the computed subset validity rate distribution, which typically lies around 0.5. <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the evaluation function for subset gradient with parameters <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<xref rid="sensors-25-05675-f002" ref-type="fig">Figure 2</xref>b) as follows:<disp-formula id="FD13-sensors-25-05675"><label>(13)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>logistic</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD14-sensors-25-05675"><label>(14)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>logistic</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>To emphasize high evaluation scores for relatively small subset gradients <italic toggle="yes">G</italic> and low scores for relatively large ones, while ensuring a monotonically decreasing trend, and avoiding an abrupt change (e.g., a step function), the subset gradient evaluation function <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as a flipped and shifted logistic function. Logistic function <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>logistic</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a commonly used S-shaped function in machine learning with smooth monotonically values within the range <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B50-sensors-25-05675" ref-type="bibr">50</xref>]. <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>logistic</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> governs the maximum evaluation score at <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. To provide high and distinguishable evaluation scores for small subset gradients, an empirically acceptable value range for <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> defines the point at which the function decreases to 0.5; in other words, this ratio controls the evaluation of big subset gradients. In practice, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is selected so that <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> lies around half of the upper bound of the computed subset gradient distribution. <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the evaluation function for subset similarity with a parameter <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<xref rid="sensors-25-05675-f002" ref-type="fig">Figure 2</xref>c) as follows:<disp-formula id="FD15-sensors-25-05675"><label>(15)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mspace width="1.em"/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>S</mml:mi><mml:mo>&#8804;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>otherwise</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>Similar to <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the subset similarity evaluation function <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as a piecewise function, which employs a monotonically increasing linear function from 0 to 1 over the interval <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and a constant zero function otherwise, to enhance distinguishability, considering that most subset similarity values tend to be concentrated near to 1 in practical applications. <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined in the range <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and, in practice, is selected close to the lower bound of the computed subset similarity distribution.</p><list list-type="simple"><list-item><label>(b2)</label><p>Camera Pair Evaluation and Selection</p></list-item></list><p>Disparity between the <italic toggle="yes">l</italic>-th and the <italic toggle="yes">r</italic>-th cameras for the <italic toggle="yes">i</italic>-th measurement point, <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is computed using extrinsic parameters as follows:<disp-formula id="FD16-sensors-25-05675"><label>(16)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo form="prefix">arccos</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mo>&#8741;</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mrow><mml:mo>&#8741;</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-05675"><label>(17)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:msup><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the direction vector of the <italic toggle="yes">i</italic>-th measurement point relative to the <italic toggle="yes">c</italic>-th camera.</p><p>For a pair containing the <italic toggle="yes">l</italic>-th and <italic toggle="yes">r</italic>-th cameras, the camera pair evaluation score <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is computed as follows:<disp-formula id="FD18-sensors-25-05675"><label>(18)</label><mml:math id="mm90" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the evaluation function for disparity with a parameter <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<xref rid="sensors-25-05675-f002" ref-type="fig">Figure 2</xref>d) as follows:<disp-formula id="FD19-sensors-25-05675"><label>(19)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>logistic</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mi>D</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>For a pair of cameras, a big disparity can effectively mitigate the influence of noise, while a small disparity often enlarges the error, and a zero disparity disables 3D estimation. To reduce the evaluation score of a camera pair with small disparity and guarantee a non-zero disparity value, the disparity evaluation function <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as a scaled and shifted logistic function. Through scaling and shifting, this logistic function ensures the evaluation value is 0 at <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and approaches 1 as <italic toggle="yes">D</italic> increases. Compared to monotonically linear function, it offers a steeper growth rate near <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, followed by a progressively decreasing rate of increase, aligning with the fact that measurement noise sensitivity rises more rapidly as the disparity approaches 0. The parameter <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> controls the maximum rate of increase in the initial phase; in other words, smaller <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> encourages a stronger tendency toward bigger disparities, while larger <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> encourages a weaker tendency. <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is selected according to the desired encouragement level under the given physical conditions. To ensure effective high and low evaluation scores for camera pairs with big and small disparities, respectively, an empirically acceptable value range for <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The camera pair with the highest evaluation score, <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is selected for each measurement point to estimate the 3D displacement:<disp-formula id="FD20-sensors-25-05675"><label>(20)</label><mml:math id="mm104" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">arg max</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:munder><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec3-sensors-25-05675"><title>3. Accuracy Verification</title><sec id="sec3dot1-sensors-25-05675"><title>3.1. Experimental Setting</title><p>To assess the robustness of our proposed MPMC-DIC against cluttered backgrounds for wide-range 3D deformation measurement, we validated its accuracy in measuring the rigid displacements of a moving cylinder within a random-speckle background, as shown in <xref rid="sensors-25-05675-f003" ref-type="fig">Figure 3</xref>a, and compared the results with that obtained from the method considering only visibility (hereinafter referred to as the visibility-only method). A &#216;5 &#215; 5 cm cylinder (<xref rid="sensors-25-05675-f003" ref-type="fig">Figure 3</xref>b), painted with a random pattern and affixed with 7 mm circular markers as references, was connected to an XYZ stage fixed to the platform. Behind the cylinder, a 15 &#215; 15 &#215; 20 cm cuboid was fixed on the platform with its front surface as a part of the background. Printed speckle patterns were affixed to the platform and background. A high-precision 3D scanner (ATOS Compact Scan, ZEISS, Oberkochen, Germany) with 3-&#956;m uncertainty, positioned 53 cm above the cylinder, measured displacements of five markers on the cylinder&#8217;s top surface, with their average as the reference for displacement. The 3D model, reconstructed by the ATOS Compact Scan as shown in <xref rid="sensors-25-05675-f003" ref-type="fig">Figure 3</xref>c,d, included 176 measurement points, consisting of 72 on the top surface (3 circles with 7 mm spacing &#215; 24 in each circle with 15&#176; spacing), and 104 on the front surface (8 semi-circles with 5.5 mm spacing &#215; 13 in each semi-circle with 15&#176; spacing). Eight cameras were arranged in a 2 &#215; 4 curved grid to capture 8-bit Bayer images, covering the cylinder&#8217;s top surface and front 180&#176; surface with around 30 cm spacing and measuring distance. Two PCs recorded these Bayer images using CoaXlink Quad CXP-12 frame grabbers (Euresys, Seraing, Belgium) and converted them to grayscale images via the OpenCV function, with each PC connected to four cameras and equipped with an i9-12900K CPU (Intel, Santa Clara, CA, USA), 64 GB RAM, RTX 3090 GPU (NVIDIA, Santa Clara, CA, USA), and Windows 11 Professional. <xref rid="sensors-25-05675-t001" ref-type="table">Table 1</xref> shows the optical system configurations.</p><p>To verify the accuracy of MPMC-DIC, the cylinder was moved along depth (<italic toggle="yes">z</italic>-) and horizontal (<italic toggle="yes">x</italic>-) direction in 11 steps (0.0&#8211;5.0 mm, around 0.5 mm per step), with images captured by eight cameras and reference displacement measured using the ATOS Compact Scan at each step. <xref rid="sensors-25-05675-f004" ref-type="fig">Figure 4</xref> shows images captured at the initial position as reference images. Prior to measurement, cameras were calibrated using checkerboard and registered using circular marker points from the 3D model and 2D reference images. After capturing, recorded images were post-processed to measure displacements for accuracy verification, comparing MPMC-DIC with the visibility-only method. In both methods, the 2D-DIC method in the open-source 3D-DIC tool OpenCorr (version 1.0) [<xref rid="B22-sensors-25-05675" ref-type="bibr">22</xref>] was used for 2D displacement measurement, with configurations as shown in <xref rid="sensors-25-05675-t002" ref-type="table">Table 2</xref>. In MPMC-DIC, evaluation parameters for PMSP were set as shown in <xref rid="sensors-25-05675-t003" ref-type="table">Table 3</xref>. The mean <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the measured displacement <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> over 176 measurement points are computed for error analysis. By computing with the reference displacement <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and its uncertainty <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the mean and standard deviation of error are defined as <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>ref</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, and used as error indices.</p></sec><sec id="sec3dot2-sensors-25-05675"><title>3.2. Camera Pair Evaluation and Selection</title><p><xref rid="sensors-25-05675-f005" ref-type="fig">Figure 5</xref> shows the PMSP results at the 3.0 mm horizontal movement. For measurement points on the top surface of the cylinder, paired cameras included the first, second, third, and fourth cameras, while the fifth to eighth cameras were excluded from pairing due to lack of visibility; most of these points selected camera pairs with high disparities for measurement precision, such as the camera pair {1, 4}. All measurement points on the front surface paired two cameras oriented to them; this ensured visibility and prevented large object surface inclination and object&#8211;background discontinuity within the subsets for high-quality 2D measurement. <xref rid="sensors-25-05675-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-25-05675-t005" ref-type="table">Table 5</xref> illustrate the PMSP process for an example measurement point on the front surface of the cylinder, as <xref rid="sensors-25-05675-f003" ref-type="fig">Figure 3</xref>c and <xref rid="sensors-25-05675-f005" ref-type="fig">Figure 5</xref> circle, at 3.0 mm horizontal movement. In <xref rid="sensors-25-05675-t004" ref-type="table">Table 4</xref>, each camera was first evaluated individually based on visibility and subset characteristics. As shown in <xref rid="sensors-25-05675-f004" ref-type="fig">Figure 4</xref>, the first, second, fifth, and sixth cameras were visible, while others were not; the subsets from the second and sixth cameras exhibited significant object&#8211;background discontinuity, with noticeable background speckles, and higher object surface inclination compared to those from the first and fifth cameras. As a result, the first and fifth cameras achieved high individual evaluation scores <italic toggle="yes">h</italic> exceeding 0.990, whereas the second and sixth cameras obtained lower values due to reduced subset validity rate <italic toggle="yes">V</italic>, similarity <italic toggle="yes">S</italic>, and increased subset gradient <italic toggle="yes">G</italic>; invisible cameras were excluded from the evaluation. In <xref rid="sensors-25-05675-t005" ref-type="table">Table 5</xref>, the disparity <italic toggle="yes">D</italic> of each camera pair was then evaluated. Most visible camera pairs achieved evaluation scores <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> slightly above 0.9, except for pairs {1, 6} and {2, 5}, which obtained 0.982 and 0.976, respectively. Finally, in the camera pair evaluation stage, the camera pair {1, 5} achieved the highest evaluation score <italic toggle="yes">H</italic> of 0.888, owing to the high individual camera evaluation scores of the first and fifth cameras. This camera pair was therefore selected to measure the example point.</p></sec><sec id="sec3dot3-sensors-25-05675"><title>3.3. Comparison with Visibility-Only Method</title><p><xref rid="sensors-25-05675-f006" ref-type="fig">Figure 6</xref> compares MPMC-DIC with the visibility-only method for measuring the horizontal displacements of the cylinder. In <xref rid="sensors-25-05675-f006" ref-type="fig">Figure 6</xref>a, the visibility-only method exhibits increasing deviations with horizontal movements, particularly on the left and top&#8211;back sides of the cylinder. These deviations were attributed to over-grouped cameras whose subsets include object&#8211;background discontinuity. The presence of background speckles in these subsets led to tracking failures during cylinder movement. In contrast, MPMC-DIC achieved accurate measurements across the entirety of the 0.0&#8211;5.0 mm movements by selecting appropriate camera pairs, as demonstrated in <xref rid="sensors-25-05675-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-25-05675-t005" ref-type="table">Table 5</xref>. Quantitative evaluation in <xref rid="sensors-25-05675-f006" ref-type="fig">Figure 6</xref>b shows that the mean error <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of MPMC-DIC remain below 0.01 mm and 0.02 mm, respectively, whereas the visibility-only method shows rapidly increasing error, reaching a maximum <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.54 mm and <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 1.04 mm at 5.0 mm movement. A similar trend is observed in depth displacement measurement, as illustrated in <xref rid="sensors-25-05675-f007" ref-type="fig">Figure 7</xref>. MPMC-DIC consistently maintains accuracy with <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> below 0.01 mm and 0.02 mm, respectively, throughout the 0.0&#8211;5.0 mm movements, while the visibility-only method exhibits increasing deviations, particularly on the front side due to object&#8211;background discontinuity within the subsets from the first, fourth, fifth, and eighth cameras, with a maximum <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.47 mm and <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.80 mm.</p></sec><sec id="sec3dot4-sensors-25-05675"><title>3.4. Verification on Robustness to Digital Speckle Pattern</title><p>To verify the robustness of our proposed MPMC-DIC to different speckle types, we also measured the displacements of a planar object with a digital speckle pattern and compared the results with those obtained from the visibility-only method. The 3 &#215; 15 cm planar object was part of the front surface of the cuboid behind the cylinder, as shown in <xref rid="sensors-25-05675-f008" ref-type="fig">Figure 8</xref>a. Its 3D model is shown in <xref rid="sensors-25-05675-f008" ref-type="fig">Figure 8</xref>b, which contains 5 &#215; 35 measurement points evenly distributed at horizontal and vertical spacings of 3.75 mm and 3.95 mm, respectively. The same images, experimental configurations, and error metric described in <xref rid="sec3dot1-sensors-25-05675" ref-type="sec">Section 3.1</xref> were adopted for this measurement and analysis. For the error analysis, the reference displacement was set to zero, since the cuboid remained stationary during image acquisition.</p><p>Similar to the measurements of cylinder displacements (<xref rid="sensors-25-05675-f006" ref-type="fig">Figure 6</xref> and <xref rid="sensors-25-05675-f007" ref-type="fig">Figure 7</xref>), MPMC-DIC also outperforms the visibility-only method in measuring planar object displacements (<xref rid="sensors-25-05675-f009" ref-type="fig">Figure 9</xref>), as the visibility-only method was affected by subsets of over-grouped cameras which contained the moving cylinder. Quantitative evaluation at both horizontal and depth cylinder movements shows that the mean error <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of MPMC-DIC remain below 0.03 mm and 0.02 mm, respectively. In contrast, the visibility-only method exhibits increasing errors, with a maximum <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.11 mm and <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.37 mm at 5.0 mm horizontal cylinder movement, and a maximum <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.22 mm and <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.78 mm at 5.0 mm depth cylinder movement.</p></sec><sec id="sec3dot5-sensors-25-05675"><title>3.5. Computational Efficiency Evaluation</title><p>To evaluate the computational efficiency of MPMC-DIC relative to the visibility-only method, we recorded the execution time required to measure cylinder displacements under the same conditions and using the same recording PC as described in <xref rid="sec3dot1-sensors-25-05675" ref-type="sec">Section 3.1</xref>. The time taken to compute the 3D displacements for one frame of 176 measurement points is summarized in <xref rid="sensors-25-05675-t006" ref-type="table">Table 6</xref>. When executed sequentially on the CPU, the visibility-only method required 3816.70 s for processing, whereas MPMC-DIC took 30,906.56 s, with camera pairing incurring an additional execution time about seven times that of the visibility-only method. Nevertheless, the PVD and DID procedures dominated the computation for 3D-model-element-wise and pixel-wise operations, making them well-suited for parallel processing. With GPU acceleration, the execution times of PVD and DID were significantly reduced to 167.81 s and 313.47 s, respectively. Consequently, the total execution times of MPMC-DIC and the visibility-only method were reduced to 633.03 s and 312.24 s, respectively, bringing the additional execution time of MPMC-DIC down to nearly the same level as that of the visibility-only method.</p><p>In conclusion, unlike the visibility-only method, which loses accuracy with over-grouped cameras, MPMC-DIC maintains accurate measurements by selecting camera pairs with high evaluation scores, requiring only an additional execution time equivalent to that of the visibility-only method to exclude the influence of subsets with object&#8211;background discontinuity. These results demonstrate the robustness of our proposed MPMC-DIC against cluttered backgrounds in wide-range 3D deformation measurement.</p></sec></sec><sec id="sec4-sensors-25-05675"><title>4. Vibration Measurement on PC Speaker</title><p>To assess the robustness of our proposed MPMC-DIC against complex object geometries in wide-range 3D DDM, we applied it to measure panoramic vibrations of a 9 &#215; 15 &#215; 16 cm PC speaker featuring a recessed membrane, as shown in <xref rid="sensors-25-05675-f010" ref-type="fig">Figure 10</xref>a, and visually compared it with the visibility-only method. The speaker (<xref rid="sensors-25-05675-f010" ref-type="fig">Figure 10</xref>b) was painted with random pattern and affixed with 7 mm circular markers as references. Its 3D model (<xref rid="sensors-25-05675-f010" ref-type="fig">Figure 10</xref>c,d) was reconstructed by the ATOS Compact Scan, including 32,055 measurement points. The same cameras and PCs as described in <xref rid="sec3-sensors-25-05675" ref-type="sec">Section 3</xref>, as well as the same calibration and registration methods, were utilized in the vibration measurement for image capturing, recording, and processing. The eight cameras surrounding the speaker 99 cm away captured 1920 &#215; 1080-pixel reference and measurement images with a 2.3 ms exposure; 400 fps measurement images were captured for 0.5 s when the PC speaker played 50 Hz audio. <xref rid="sensors-25-05675-f011" ref-type="fig">Figure 11</xref> shows the reference images, where severe self-occlusion can be observed at the membrane part in the second and fifth camera images. Measurement image series were utilized to measure vibration displacements using MPMC-DIC and the visibility-only method based on similar configurations to the accuracy verification: the 2D-DIC method in OpenCorr was performed for 2D measurement using 129 &#215; 129-pixel subsets in both methods, and camera pairs were evaluated and selected using PMSP with the evaluation parameters shown in <xref rid="sensors-25-05675-t007" ref-type="table">Table 7</xref>. Since the speaker was stable throughout the image capturing process, PMSP was applied solely to the first frame in this experiment, with the subsequent frames using the same camera pair selection. The eight-point moving average component was removed from the measured vibration displacements to suppress the artifacts caused by camera self-motion. The peak-to-peak value of <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow></mml:math></inline-formula>-direction displacements is calculated as <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>&#951;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>&#951;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#951;</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The highest value among three directions is defined as the peak-to-peak vibration value.</p><p><xref rid="sensors-25-05675-f012" ref-type="fig">Figure 12</xref> visualizes PMSP results. Most measurement points on the front, back, and lateral surfaces paired adjacent cameras which were oriented to these points, as these cameras ensured visibility, along with high evaluation scores in terms of the subset validity rate and gradient. An exception occurred at a few measurement points located on the lateral surfaces close to the membrane region, which selected camera pair {1, 3} or {4, 6}. This resulted from the fact that the second/fifth camera provided low evaluation scores due to severe self-occlusion in the subset region, while other cameras lacked visibility. Regarding measurement points on the top surface, the visibility and similar subset conditions of all cameras allowed them to select a reliable camera pair with higher disparity; for example, some points selected the camera pair {1, 6} or {6, 8}. For measurement points located on the membrane region, the third and fourth cameras were paired, with the first, sixth, seventh, and eighth cameras excluded due to lack of visibility. Although the second and fifth cameras were also visible to part of these measurement points, they were excluded from pairing due to low camera pair evaluation scores resulting from severe self-occlusion, as illustrated in <xref rid="sensors-25-05675-f011" ref-type="fig">Figure 11</xref>.</p><p>Based on the PMSP, our MPMC-DIC achieved accurate 3D vibration measurements, as shown in <xref rid="sensors-25-05675-f013" ref-type="fig">Figure 13</xref>a, revealing a circular vibration distribution centered on the membrane. The vibration amplitude reaches its peak at the center and gradually diminishes toward the edges. In contrast, the visibility-only method, as shown in <xref rid="sensors-25-05675-f013" ref-type="fig">Figure 13</xref>b, exhibits substantial deviations due to over-grouping of all visible cameras. Ill-measured deformations from the second and fifth cameras led to remarkable errors. As for the measurements on the speaker housing, <xref rid="sensors-25-05675-f014" ref-type="fig">Figure 14</xref> presents the periodic vibration measured using MPMC-DIC. Circular vibration distributions excited by the 50 Hz audio are observed at the left and right lateral surfaces. Due to measurement biases among cameras, minor spatial discontinuities in the measured deformations are observed along the boundary where pairing alters, like the transition region between the top and left lateral surfaces at 7.5 ms. Nevertheless, future work is expected to mitigate these discontinuities by grouping a larger number of cameras and adopting weighted triangulation.</p><p><xref rid="sensors-25-05675-f015" ref-type="fig">Figure 15</xref> presents the 3D vibrations of three points (<inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) located on the speaker membrane. Corresponding to the distribution shown in <xref rid="sensors-25-05675-f013" ref-type="fig">Figure 13</xref>a, these points exhibit gradually decreasing peak-to-peak vibration values of 1.252, 1.032, and 0.598 mm, respectively. Their <italic toggle="yes">z</italic>-direction frequency amplitude spectra reveal a prominent peak at 50 Hz, aligned with the speaker&#8217;s operating frequency, as well as harmonic peaks at 100 and 150 Hz. <xref rid="sensors-25-05675-f016" ref-type="fig">Figure 16</xref> shows the 3D vibrations of eight points located on the speaker housing, with respective peak-to-peak vibration values of 0.009, 0.011, 0.012, 0.018, 0.013, 0.013, 0.012, and 0.009 mm. Although the vibration amplitudes of these points are significantly smaller than those on the membrane, their frequency responses exhibit a similar pattern, with maximum amplitudes at 50 Hz and harmonic peaks at 100 and 150 Hz. Across all 11 points, the frequency amplitude spectra reveal that the amplitudes at 150 Hz are slightly or significantly greater than those at 100 Hz. This indicates the presence of a mechanical resonance in the PC speaker structure at 150 Hz, in addition to the harmonic components.</p><p>These results demonstrate that our proposed MPMC-DIC enables accurate wide-range 3D DDM by selecting camera pairs with high evaluation scores, effectively handling the self-occlusions caused by complex geometries and minimizing the deviations observed in the visibility-only method, thereby confirming its robustness.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05675"><title>5. Conclusions</title><p>In this study, we propose a novel MPMC-DIC method for wide-range 3D DDM of semi-rigid objects. The proposed method enables automatic camera pair evaluation and selection, filtering out cameras which potentially provide ill-measured deformations, thus ensuring accurate measurements. Experiments which measure cylinder rigid displacements and PC speaker vibrations validate its accuracy, along with robustness against cluttered backgrounds and complex object geometries in comparison with the visibility-only method. Visual and quantitative evaluations demonstrate large deviations with a maximum mean error of 0.54 mm for the visibility-only method, whereas MPMC-DIC maintains a mean error below 0.03 mm with an additional execution time approximately equivalent to that of the visibility-only method. Minor spatial discontinuities are observed in the measured deformations along boundaries where camera pairing changes, due to biases among cameras; this issue is expected to be mitigated in future work by grouping multiple cameras. Although this has yet to be proven, MPMC-DIC may enhance robustness in more practical situations, such as environments with light variations, based on its capability to pair reliable cameras.</p><p>Being a newly proposed technique, MPMC-DIC demonstrates considerable promise to applications like in situ monitoring with uncontrollable backgrounds, assessment of products with complex geometries, and measurement in environments with reflective highlights. Our work will further demonstrate its practical performance and continue to extend its measurement capabilities, including local strain measurement. Deeper investigations in the future will be strongly beneficial to develop this method into a highly effective and impactful tool, particularly with regard to the following aspects:<list list-type="bullet"><list-item><p>Evaluation factors and functions: The five evaluation factors were selected based on experience, and evaluation functions were designed based on the requirement for specific factor evaluation, but they have not been comprehensively evaluated to be strictly limited as described in this paper. More forms of evaluation factors or functions are encouraged to be applied in expectation of a better distinguishment between reliable and ill-conditioned camera pairs.</p></list-item><list-item><p>Automatic parameter determination: The evaluation parameters for PMSP in the study were artificially assigned. Future research on automatic parameter determination can further simplify the system setup, e.g., by integrating with machine learning or deep learning.</p></list-item><list-item><p>Multiple-camera grouping: Although cameras were paired into stereo subsystems in this study, a subsystem is able to involve more cameras, and this extension is believed to enable higher-accuracy 3D DDM with weighted triangulation.</p></list-item><list-item><p>Active camera positioning: Our evaluation metric was applied for camera pairing based on static camera positions. In the future, it is also possible that it could be utilized for active camera positioning, or for both of these at the same time.</p></list-item></list></p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, I.I.; methodology, W.Q.; software, W.Q. and S.H.; validation, W.Q., S.H. and I.I.; formal analysis, W.Q. and I.I.; investigation, W.Q. and F.W.; resources, K.S. and I.I.; data curation, W.Q. and K.S.; writing&#8212;original draft preparation, W.Q.; writing&#8212;review and editing, W.Q., F.W., S.H. and I.I.; visualization, W.Q.; supervision, I.I.; project administration, I.I.; funding acquisition, W.Q. and I.I. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05675"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ilanko</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mochida</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tighe</surname><given-names>R.C.</given-names></name></person-group><article-title>A Review on Vibration-Based Damage Detection Methods for Civil Structures</article-title><source>Vibration</source><year>2023</year><volume>6</volume><fpage>843</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.3390/vibration6040051</pub-id></element-citation></ref><ref id="B2-sensors-25-05675"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.</given-names></name></person-group><article-title>A visual vibration characterization method for intelligent fault diagnosis of rotating machinery</article-title><source>Mech. Syst. Signal Proc.</source><year>2023</year><volume>192</volume><fpage>110229</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2023.110229</pub-id></element-citation></ref><ref id="B3-sensors-25-05675"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>L.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name></person-group><article-title>Experimental and numerical investigation on multi-module coupling vibration performance of light rail vehicle</article-title><source>J. Vib. Eng. Technol.</source><year>2024</year><volume>12</volume><fpage>745</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1007/s42417-023-00872-1</pub-id></element-citation></ref><ref id="B4-sensors-25-05675"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nhung</surname><given-names>N.T.C.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>H.Q.</given-names></name><name name-style="western"><surname>Huyen</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>D.B.</given-names></name><name name-style="western"><surname>Quang</surname><given-names>M.T.</given-names></name></person-group><article-title>Development and application of linear variable differential transformer (LVDT) sensors for the structural health monitoring of an urban railway bridge in Vietnam</article-title><source>Eng. Technol. Appl. Sci. Res.</source><year>2023</year><volume>13</volume><fpage>11622</fpage><lpage>11627</lpage><pub-id pub-id-type="doi">10.48084/etasr.6192</pub-id></element-citation></ref><ref id="B5-sensors-25-05675"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jing</surname><given-names>C.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>G.</given-names></name></person-group><article-title>GNSS/accelerometer integrated deformation monitoring algorithm based on sensors adaptive noise modeling</article-title><source>Measurement</source><year>2023</year><volume>218</volume><fpage>113179</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.113179</pub-id></element-citation></ref><ref id="B6-sensors-25-05675"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Khyam</surname><given-names>M.O.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name></person-group><article-title>Recent advances and tendencies regarding fiber optic sensors for deformation measurement: A review</article-title><source>IEEE Sens. J.</source><year>2021</year><volume>22</volume><fpage>2962</fpage><lpage>2973</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2021.3138091</pub-id></element-citation></ref><ref id="B7-sensors-25-05675"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garg</surname><given-names>P.</given-names></name><name name-style="western"><surname>Moreu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ozdagli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Taha</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Mascare&#241;as</surname><given-names>D.</given-names></name></person-group><article-title>Noncontact dynamic displacement measurement of structures using a moving laser Doppler vibrometer</article-title><source>J. Bridge Eng.</source><year>2019</year><volume>24</volume><fpage>04019089</fpage><pub-id pub-id-type="doi">10.1061/(ASCE)BE.1943-5592.0001472</pub-id></element-citation></ref><ref id="B8-sensors-25-05675"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Shimasaki</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>I.</given-names></name></person-group><article-title>HFR-Video-Based Chatter Monitoring Synchronized with Tool Rotation</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2025</year><volume>74</volume><fpage>5033912</fpage><pub-id pub-id-type="doi">10.1109/TIM.2025.3573005</pub-id></element-citation></ref><ref id="B9-sensors-25-05675"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Azimbeik</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mahdavi</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Rofooei</surname><given-names>F.R.</given-names></name></person-group><article-title>Improved image-based, full-field structural displacement measurement using template matching and camera calibration methods</article-title><source>Measurement</source><year>2023</year><volume>211</volume><fpage>112650</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.112650</pub-id></element-citation></ref><ref id="B10-sensors-25-05675"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>E.M.C.</given-names></name><name name-style="western"><surname>Iadicola</surname><given-names>M.A.</given-names></name></person-group><article-title>A good practices guide for digital image correlation</article-title><source>Int. Digit. Image Correl. Soc.</source><year>2018</year><pub-id pub-id-type="doi">10.32720/idics/gpg.ed1</pub-id></element-citation></ref><ref id="B11-sensors-25-05675"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peters</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ranson</surname><given-names>W.</given-names></name></person-group><article-title>Digital imaging techniques in experimental stress analysis</article-title><source>Opt. Eng.</source><year>1982</year><volume>21</volume><fpage>427</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.1117/12.7972925</pub-id></element-citation></ref><ref id="B12-sensors-25-05675"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cary</surname><given-names>P.</given-names></name></person-group><article-title>Deformation measurements by digital image correlation: Implementation of a second-order displacement gradient</article-title><source>Exp. Mech.</source><year>2000</year><volume>40</volume><fpage>393</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1007/BF02326485</pub-id></element-citation></ref><ref id="B13-sensors-25-05675"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Asundi</surname><given-names>A.</given-names></name></person-group><article-title>Two-dimensional digital image correlation for in-plane displacement and strain measurement: A review</article-title><source>Meas. Sci. Technol.</source><year>2009</year><volume>20</volume><fpage>062001</fpage><pub-id pub-id-type="doi">10.1088/0957-0233/20/6/062001</pub-id></element-citation></ref><ref id="B14-sensors-25-05675"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>B.</given-names></name></person-group><article-title>Bias error reduction of digital image correlation using Gaussian pre-filtering</article-title><source>Opt. Lasers Eng.</source><year>2013</year><volume>51</volume><fpage>1161</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2013.04.009</pub-id></element-citation></ref><ref id="B15-sensors-25-05675"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Su</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><article-title>High-efficiency and high-accuracy digital image correlation for three-dimensional measurement</article-title><source>Opt. Lasers Eng.</source><year>2015</year><volume>65</volume><fpage>73</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2014.05.013</pub-id></element-citation></ref><ref id="B16-sensors-25-05675"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>B.</given-names></name></person-group><article-title>Full-automatic seed point selection and initialization for digital image correlation robust to large rotation and deformation</article-title><source>Opt. Lasers Eng.</source><year>2021</year><volume>138</volume><fpage>106432</fpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2020.106432</pub-id></element-citation></ref><ref id="B17-sensors-25-05675"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sutton</surname><given-names>M.</given-names></name><name name-style="western"><surname>Peters</surname><given-names>W.H.</given-names></name></person-group><article-title>Accurate measurement of three-dimensional deformations in deformable and rigid bodies using computer vision</article-title><source>Exp. Mech.</source><year>1993</year><volume>33</volume><fpage>123</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1007/BF02322488</pub-id></element-citation></ref><ref id="B18-sensors-25-05675"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salvi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Armangu&#233;</surname><given-names>X.</given-names></name><name name-style="western"><surname>Batlle</surname><given-names>J.</given-names></name></person-group><article-title>A comparative review of camera calibrating methods with accuracy evaluation</article-title><source>Pattern Recognit.</source><year>2002</year><volume>35</volume><fpage>1617</fpage><lpage>1635</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(01)00126-1</pub-id></element-citation></ref><ref id="B19-sensors-25-05675"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hirschmuller</surname><given-names>H.</given-names></name><name name-style="western"><surname>Scharstein</surname><given-names>D.</given-names></name></person-group><article-title>Evaluation of cost functions for stereo matching</article-title><source>Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Minneapolis, MN, USA</conf-loc><conf-date>17&#8211;22 June 2007</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B20-sensors-25-05675"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hartley</surname><given-names>R.I.</given-names></name><name name-style="western"><surname>Sturm</surname><given-names>P.</given-names></name></person-group><article-title>Triangulation</article-title><source>Comput. Vis. Image Underst.</source><year>1997</year><volume>68</volume><fpage>146</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1006/cviu.1997.0547</pub-id></element-citation></ref><ref id="B21-sensors-25-05675"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>F.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name></person-group><article-title>Auto-calibration and real-time external parameter correction for stereo digital image correlation</article-title><source>Opt. Lasers Eng.</source><year>2019</year><volume>121</volume><fpage>46</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2019.03.018</pub-id></element-citation></ref><ref id="B22-sensors-25-05675"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>OpenCorr: An open source library for research and development of digital image correlation</article-title><source>Opt. Lasers Eng.</source><year>2023</year><volume>165</volume><fpage>107566</fpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2023.107566</pub-id></element-citation></ref><ref id="B23-sensors-25-05675"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Solav</surname><given-names>D.</given-names></name><name name-style="western"><surname>Silverstein</surname><given-names>A.</given-names></name></person-group><article-title>DuoDIC: 3D digital image correlation in MATLAB</article-title><source>J. Open Source Softw.</source><year>2022</year><volume>7</volume><fpage>4279</fpage><pub-id pub-id-type="doi">10.21105/joss.04279</pub-id></element-citation></ref><ref id="B24-sensors-25-05675"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>G.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name></person-group><article-title>Enhance stereo-DIC in low quality speckle pattern by cross-scale stereo matching with application in dental crack detection</article-title><source>Opt. Lasers Eng.</source><year>2025</year><volume>186</volume><fpage>108770</fpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2024.108770</pub-id></element-citation></ref><ref id="B25-sensors-25-05675"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shimasaki</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>I.</given-names></name></person-group><article-title>Model-Based 3-D Vibration Measurement Using Stereo High-Frame-Rate Images</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2025</year><volume>74</volume><fpage>5040812</fpage><pub-id pub-id-type="doi">10.1109/TIM.2025.3593568</pub-id></element-citation></ref><ref id="B26-sensors-25-05675"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Stereo-DICNet: An efficient and unified speckle matching network for stereo digital image correlation measurement</article-title><source>Opt. Lasers Eng.</source><year>2024</year><volume>179</volume><fpage>108267</fpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2024.108267</pub-id></element-citation></ref><ref id="B27-sensors-25-05675"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>Z.</given-names></name></person-group><article-title>Deep learning based speckle image super-resolution for digital image correlation measurement</article-title><source>Opt. Laser Technol.</source><year>2025</year><volume>181</volume><fpage>111746</fpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2024.111746</pub-id></element-citation></ref><ref id="B28-sensors-25-05675"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Multiple-view 3D digital image correlation based on pseudo-overlapped imaging</article-title><source>Opt. Lett.</source><year>2024</year><volume>49</volume><fpage>3733</fpage><lpage>3736</lpage><pub-id pub-id-type="doi">10.1364/OL.529123</pub-id><pub-id pub-id-type="pmid">38950254</pub-id></element-citation></ref><ref id="B29-sensors-25-05675"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>M.</given-names></name></person-group><article-title>Visibility-Aware Pixelwise View Selection for Multi-View Stereo Matching</article-title><source>Proceedings of the International Conference on Pattern Recognition (ICPR)</source><conf-loc>Kolkata, India</conf-loc><conf-date>1&#8211;5 December 2024</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2025</year><fpage>130</fpage><lpage>144</lpage></element-citation></ref><ref id="B30-sensors-25-05675"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ge</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name></person-group><article-title>Point cloud optimization of multi-view images in digital image correlation system</article-title><source>Opt. Lasers Eng.</source><year>2024</year><volume>173</volume><fpage>107931</fpage><pub-id pub-id-type="doi">10.1016/j.optlaseng.2023.107931</pub-id></element-citation></ref><ref id="B31-sensors-25-05675"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shimasaki</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>I.</given-names></name></person-group><article-title>Point Cloud-Based 3D Tracking for Asynchronous and Uncalibrated Multi-Camera Systems</article-title><source>IEEE Sens. Lett.</source><year>2025</year><volume>9</volume><fpage>3503904</fpage><pub-id pub-id-type="doi">10.1109/LSENS.2025.3590157</pub-id></element-citation></ref><ref id="B32-sensors-25-05675"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>W.</given-names></name></person-group><article-title>Multi-person 3D pose estimation from multi-view without extrinsic camera parameters</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>266</volume><fpage>126114</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.126114</pub-id></element-citation></ref><ref id="B33-sensors-25-05675"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ariya</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wongwan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Worragin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Intawong</surname><given-names>K.</given-names></name><name name-style="western"><surname>Puritat</surname><given-names>K.</given-names></name></person-group><article-title>Immersive realities in museums: Evaluating the impact of VR, VR360, and MR on visitor presence, engagement and motivation</article-title><source>Virtual Real.</source><year>2025</year><volume>29</volume><fpage>117</fpage><pub-id pub-id-type="doi">10.1007/s10055-025-01201-5</pub-id></element-citation></ref><ref id="B34-sensors-25-05675"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saikia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Di Vece</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bonilla</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Magbagbeola</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mennillo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Czempiel</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bano</surname><given-names>S.</given-names></name><name name-style="western"><surname>Stoyanov</surname><given-names>D.</given-names></name></person-group><article-title>Robotic Arm Platform for Multi-View Image Acquisition and 3D Reconstruction in Minimally Invasive Surgery</article-title><source>IEEE Robot. Autom. Lett.</source><year>2025</year><volume>10</volume><fpage>3174</fpage><lpage>3181</lpage><pub-id pub-id-type="doi">10.1109/LRA.2025.3540529</pub-id></element-citation></ref><ref id="B35-sensors-25-05675"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>G.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name></person-group><article-title>Visual measurement method for three-dimensional shape of underwater bridge piers considering multirefraction correction</article-title><source>Autom. Constr.</source><year>2023</year><volume>146</volume><fpage>104706</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2022.104706</pub-id></element-citation></ref><ref id="B36-sensors-25-05675"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ospina-Bohorquez</surname><given-names>A.</given-names></name><name name-style="western"><surname>Del Pozo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Courtenay</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Gonz&#225;lez-Aguilera</surname><given-names>D.</given-names></name></person-group><article-title>Handheld stereo photogrammetry applied to crime scene analysis</article-title><source>Measurement</source><year>2023</year><volume>216</volume><fpage>112861</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.112861</pub-id></element-citation></ref><ref id="B37-sensors-25-05675"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xi</surname><given-names>J.</given-names></name></person-group><article-title>A novel method for high dynamic range optical measurement with single shot by multi-view stereo</article-title><source>Opt. Laser Technol.</source><year>2025</year><volume>189</volume><fpage>112930</fpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2025.112930</pub-id></element-citation></ref><ref id="B38-sensors-25-05675"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shimasaki</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>I.</given-names></name></person-group><article-title>An Ultrafast Multi-object Zooming System Based on Low-latency Stereo Correspondence</article-title><source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Abu Dhabi, United Arab Emirates</conf-loc><conf-date>14&#8211;18 October 2024</conf-date><fpage>11552</fpage><lpage>11557</lpage></element-citation></ref><ref id="B39-sensors-25-05675"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>T.J.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>PMVC: Promoting multi-view consistency for 3D scene reconstruction</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2024</conf-date><fpage>3678</fpage><lpage>3688</lpage></element-citation></ref><ref id="B40-sensors-25-05675"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>I.</given-names></name></person-group><article-title>A flexible calibration algorithm for high-speed bionic vision system based on galvanometer</article-title><source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Kyoto, Japan</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><fpage>4222</fpage><lpage>4227</lpage></element-citation></ref><ref id="B41-sensors-25-05675"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>B.</given-names></name></person-group><article-title>Mirror-assisted multi-view high-speed digital image correlation for dual-surface dynamic deformation measurement</article-title><source>Sci. China Technol. Sci.</source><year>2023</year><volume>66</volume><fpage>807</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1007/s11431-022-2136-1</pub-id></element-citation></ref><ref id="B42-sensors-25-05675"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Solav</surname><given-names>D.</given-names></name><name name-style="western"><surname>Moerman</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Jaeger</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Herr</surname><given-names>H.M.</given-names></name></person-group><article-title>A framework for measuring the time-varying shape and full-field deformation of residual limbs using 3-D digital image correlation</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2019</year><volume>66</volume><fpage>2740</fpage><lpage>2752</lpage><pub-id pub-id-type="doi">10.1109/TBME.2019.2895283</pub-id><pub-id pub-id-type="pmid">30676943</pub-id><pub-id pub-id-type="pmcid">PMC6783393</pub-id></element-citation></ref><ref id="B43-sensors-25-05675"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Multi-Camera digital image correlation in deformation measurement of civil components with large slenderness ratio and large curvature</article-title><source>Materials</source><year>2022</year><volume>15</volume><elocation-id>6281</elocation-id><pub-id pub-id-type="doi">10.3390/ma15186281</pub-id><pub-id pub-id-type="pmid">36143591</pub-id><pub-id pub-id-type="pmcid">PMC9503939</pub-id></element-citation></ref><ref id="B44-sensors-25-05675"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Le Gourri&#233;rec</surname><given-names>C.</given-names></name><name name-style="western"><surname>Turpin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Berny</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hild</surname><given-names>F.</given-names></name><name name-style="western"><surname>Roux</surname><given-names>S.</given-names></name></person-group><article-title>Proper Generalized Decomposition stereocorrelation to measure kinematic fields for high speed impact on laminated glass</article-title><source>Comput. Meth. Appl. Mech. Eng.</source><year>2023</year><volume>415</volume><fpage>116217</fpage><pub-id pub-id-type="doi">10.1016/j.cma.2023.116217</pub-id></element-citation></ref><ref id="B45-sensors-25-05675"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gupta</surname><given-names>V.</given-names></name><name name-style="western"><surname>Jameel</surname><given-names>A.</given-names></name><name name-style="western"><surname>Verma</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Anand</surname><given-names>S.</given-names></name><name name-style="western"><surname>Anand</surname><given-names>Y.</given-names></name></person-group><article-title>An insight on NURBS based isogeometric analysis, its current status and involvement in mechanical applications</article-title><source>Arch. Comput. Method Eng.</source><year>2023</year><volume>30</volume><fpage>1187</fpage><lpage>1230</lpage><pub-id pub-id-type="doi">10.1007/s11831-022-09838-0</pub-id></element-citation></ref><ref id="B46-sensors-25-05675"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fouque</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bouclier</surname><given-names>R.</given-names></name><name name-style="western"><surname>Passieux</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>P&#233;ri&#233;</surname><given-names>J.N.</given-names></name></person-group><article-title>Photometric DIC: A unified framework for global Stereo Digital Image Correlation based on the construction of textured digital twins</article-title><source>J. Theor. Comput. Appl. Mech.</source><year>2022</year><pub-id pub-id-type="doi">10.46298/jtcam.7467</pub-id></element-citation></ref><ref id="B47-sensors-25-05675"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Serra</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pierr&#233;</surname><given-names>J.E.</given-names></name><name name-style="western"><surname>Passieux</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>P&#233;ri&#233;</surname><given-names>J.N.</given-names></name><name name-style="western"><surname>Bouvet</surname><given-names>C.</given-names></name><name name-style="western"><surname>Castani&#233;</surname><given-names>B.</given-names></name></person-group><article-title>Validation and modeling of aeronautical composite structures subjected to combined loadings: The VERTEX project. Part 1: Experimental setup, FE-DIC instrumentation and procedures</article-title><source>Compos. Struct.</source><year>2017</year><volume>179</volume><fpage>224</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1016/j.compstruct.2017.07.080</pub-id></element-citation></ref><ref id="B48-sensors-25-05675"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chapelier</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bouclier</surname><given-names>R.</given-names></name><name name-style="western"><surname>Passieux</surname><given-names>J.C.</given-names></name></person-group><article-title>Free-Form Deformation Digital Image Correlation (FFD-DIC): A non-invasive spline regularization for arbitrary finite element measurements</article-title><source>Comput. Meth. Appl. Mech. Eng.</source><year>2021</year><volume>384</volume><fpage>113992</fpage><pub-id pub-id-type="doi">10.1016/j.cma.2021.113992</pub-id></element-citation></ref><ref id="B49-sensors-25-05675"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Genovese</surname><given-names>K.</given-names></name><name name-style="western"><surname>Badel</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cavinato</surname><given-names>C.</given-names></name><name name-style="western"><surname>Pierrat</surname><given-names>B.</given-names></name><name name-style="western"><surname>Bersi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Avril</surname><given-names>S.</given-names></name><name name-style="western"><surname>Humphrey</surname><given-names>J.</given-names></name></person-group><article-title>Multi-view digital image correlation systems for in vitro testing of arteries from mice to humans</article-title><source>Exp. Mech.</source><year>2021</year><volume>61</volume><fpage>1455</fpage><lpage>1472</lpage><pub-id pub-id-type="doi">10.1007/s11340-021-00746-1</pub-id><pub-id pub-id-type="pmid">35370297</pub-id><pub-id pub-id-type="pmcid">PMC8972080</pub-id></element-citation></ref><ref id="B50-sensors-25-05675"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gui</surname><given-names>B.</given-names></name><name name-style="western"><surname>Bhardwaj</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sam</surname><given-names>L.</given-names></name></person-group><article-title>Simulating urban expansion as a nonlinear constrained evolution process: A hybrid logistic&#8211;Monte Carlo cellular automata framework</article-title><source>Chaos Solitons Fractals</source><year>2025</year><volume>199</volume><fpage>116938</fpage><pub-id pub-id-type="doi">10.1016/j.chaos.2025.116938</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05675-f001" orientation="portrait"><label>Figure 1</label><caption><p>Processing flow of the multi-camera (MC) digital image correlation (DIC) method with the pointwise-optimized model-based stereo pairing (MPMC-DIC) in this study: (<bold>a</bold>) Model-based MC-DIC (MMC-DIC), which involves steps (a1)&#8211;(a4). (<bold>b</bold>) Pointwise-optimized model-based stereo pairing strategy (PMSP), which involves steps (b1) and (b2).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g001.jpg"/></fig><fig position="float" id="sensors-25-05675-f002" orientation="portrait"><label>Figure 2</label><caption><p>Evaluation functions for (<bold>a</bold>) The subset validity rate. (<bold>b</bold>) The subset gradient. (<bold>c</bold>) The subset similarity. (<bold>d</bold>) The disparity.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g002.jpg"/></fig><fig position="float" id="sensors-25-05675-f003" orientation="portrait"><label>Figure 3</label><caption><p>Experimental setup of accuracy verification: (<bold>a</bold>) The experimental scene. (<bold>b</bold>) The cylinder to be observed. (<bold>c</bold>,<bold>d</bold>) A three-dimensional model with 176 measurement points (blue). The green point is located at the upper-front part of the cylinder for alignment across figures. The vermilion circle highlights the measurement point selected for PMSP demonstration (unit: cm).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g003.jpg"/></fig><fig position="float" id="sensors-25-05675-f004" orientation="portrait"><label>Figure 4</label><caption><p>Images captured by eight cameras at the initial position for accuracy verification. <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>&#8211;<inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> indicate the first to eighth cameras. Blue points and orange squares indicate the projections and 2D-DIC subsets, respectively, of one measurement point as circled in <xref rid="sensors-25-05675-f003" ref-type="fig">Figure 3</xref>c in each visible camera image.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g004.jpg"/></fig><fig position="float" id="sensors-25-05675-f005" orientation="portrait"><label>Figure 5</label><caption><p>PMSP results for 176 measurement points at 3.0 mm horizontal movement in accuracy verification: (<bold>a</bold>) The first paired camera. (<bold>b</bold>) The second paired camera. The vermilion circle highlights the same measurement point as in <xref rid="sensors-25-05675-f003" ref-type="fig">Figure 3</xref>c selected for PMSP demonstration.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g005.jpg"/></fig><fig position="float" id="sensors-25-05675-f006" orientation="portrait"><label>Figure 6</label><caption><p>Comparison of displacements measured using MPMC-DIC (blue) and the visibility-only method (red) at different horizontal movements in accuracy verification: (<bold>a</bold>) Displaced 3D position distributions of the 176 measurement points. (<bold>b</bold>) Mean errors <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and standard deviations <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g006.jpg"/></fig><fig position="float" id="sensors-25-05675-f007" orientation="portrait"><label>Figure 7</label><caption><p>Comparison of displacements measured using MPMC-DIC (blue) and visibility-only method (red) at different depth movements for accuracy verification: (<bold>a</bold>) Displaced 3D position distributions of the 176 measurement points. (<bold>b</bold>) Mean errors <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and standard deviations <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g007.jpg"/></fig><fig position="float" id="sensors-25-05675-f008" orientation="portrait"><label>Figure 8</label><caption><p>Measurement object in the verification on robustness to digital speckle pattern: (<bold>a</bold>) Planar (vermilion rectangular) object to be observed. (<bold>b</bold>) Its 3D model with 175 evenly assigned (5 &#215; 35) measurement points (blue). The cylinder is considered as background in the measurement (unit: cm).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g008.jpg"/></fig><fig position="float" id="sensors-25-05675-f009" orientation="portrait"><label>Figure 9</label><caption><p>Quantitative comparison of planar object displacements measured using MPMC-DIC (blue) and the visibility-only method (red) in the verification of robustness to digital speckle pattern: mean errors <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and standard deviations <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<bold>a</bold>) at different horizontal cylinder movements; (<bold>b</bold>) at different depth cylinder movements.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g009.jpg"/></fig><fig position="float" id="sensors-25-05675-f010" orientation="portrait"><label>Figure 10</label><caption><p>Experimental setup of vibration measurement: (<bold>a</bold>) Experimental scene. (<bold>b</bold>) PC speaker to be observed. (<bold>c</bold>,<bold>d</bold>) Three-dimensional model with 32,055 measurement points across its surface (unit: cm).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g010.jpg"/></fig><fig position="float" id="sensors-25-05675-f011" orientation="portrait"><label>Figure 11</label><caption><p>Reference images captured by eight cameras in vibration measurement. <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>&#8211;<inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> indicate the first to eighth cameras. The orange square in the first image shows the 129 &#215; 129-pixel subset size.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g011.jpg"/></fig><fig position="float" id="sensors-25-05675-f012" orientation="portrait"><label>Figure 12</label><caption><p>Visualization of the 12 most frequently selected camera pairs with different colors for the first frame processed for vibration measurement. Black regions indicate measurement points which selected other camera pairs or were not measurable due to lack of visibility. In total, 11 of the 32,055 measurement points used for vibration analysis are marked on the 3D model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g012.jpg"/></fig><fig position="float" id="sensors-25-05675-f013" orientation="portrait"><label>Figure 13</label><caption><p>Time-varying 3D shapes of the speaker&#8217;s front surface over 20 ms (one period of 50 Hz audio), placing emphasis on the membrane vibration measured by different methods: (<bold>a</bold>) MPMC-DIC; (<bold>b</bold>) visibility-only method. Deformations are visually magnified 10 times in geometry, and enhanced by colormap based on the actual displacement magnitude (<inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi mathvariant="bold">p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g013.jpg"/></fig><fig position="float" id="sensors-25-05675-f014" orientation="portrait"><label>Figure 14</label><caption><p>Time-varying 3D shapes of the speaker over 20 ms (one period of 50 Hz audio), placing emphasis on the housing vibration measured by MPMC-DIC. Deformations are visually magnified 10 times in geometry, and enhanced by colormap based on the actual displacement magnitude (<inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi mathvariant="bold">p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g014.jpg"/></fig><fig position="float" id="sensors-25-05675-f015" orientation="portrait"><label>Figure 15</label><caption><p>Three-dimensional vibrations measured at three measurement points at the membrane of the speaker (<inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as illustrated in <xref rid="sensors-25-05675-f012" ref-type="fig">Figure 12</xref>) using MPMC-DIC: (<bold>a</bold>) Three-dimensional displacements over 0.2 s; (<bold>b</bold>) frequency&#8211;amplitude spectra derived from 0.5-s <italic toggle="yes">z</italic>-displacements, where a prominent peak at 50 Hz, as well as harmonic peaks at 100 and 150 Hz are identified. <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are located at the center and edge of the membrane, respectively, and <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is at the middle of them.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g015.jpg"/></fig><fig position="float" id="sensors-25-05675-f016" orientation="portrait"><label>Figure 16</label><caption><p>Three-dimensional vibrations measured at eight measurement points on the back, top, and lateral surfaces of the speaker (<inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as illustrated in <xref rid="sensors-25-05675-f012" ref-type="fig">Figure 12</xref>) using MPMC-DIC: (<bold>a</bold>) three-dimensional displacements over 0.2 s; (<bold>b</bold>) frequency&#8211;amplitude spectra derived from 0.5-s <italic toggle="yes">z</italic>-displacements, where a prominent peak at 50 Hz, as well as harmonic peaks at 100 and 150 Hz are identified.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05675-g016.jpg"/></fig><table-wrap position="float" id="sensors-25-05675-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05675-t001_Table 1</object-id><label>Table 1</label><caption><p>Optical system configurations.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Camera</td><td align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Eosens 2.0CXP2 HFR camera (Mikrotron, Unterschleissheim,
Germany)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Resolution</td><td align="left" valign="middle" rowspan="1" colspan="1">1920 &#215; 1080 pixels</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Pixel size</td><td align="left" valign="middle" rowspan="1" colspan="1">10 &#956;m</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Lens</td><td align="left" valign="middle" rowspan="1" colspan="1">CREATOR F2 lens (Zhong Yi Optics, Shenyang,
China)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Focal length</td><td align="left" valign="middle" rowspan="1" colspan="1">35 mm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Aperture</td><td align="left" valign="middle" rowspan="1" colspan="1">f/16</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Stand-off distance (SOD) <sup>1</sup></td><td align="left" valign="middle" rowspan="1" colspan="1">273.03 mm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Stereo angle <sup>2</sup></td><td align="left" valign="middle" rowspan="1" colspan="1">43&#176;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Field-of-view (FOV) <sup>3</sup></td><td align="left" valign="middle" rowspan="1" colspan="1">149.78 &#215; 84.25 mm</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Image scale <sup>3</sup></td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.82 pixel/mm</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> The SOD was calculated as the mean distance of the eight cameras from the cylinder. <sup>2</sup> The stereo angle was calculated as the mean angle between adjacent cameras. <sup>3</sup> The FOV and image scale were theoretically derived based on the SOD. The actual FOV is presented in <xref rid="sensors-25-05675-f004" ref-type="fig">Figure 4</xref>, and the actual image scale may vary slightly depending on the measurement position.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05675-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05675-t002_Table 2</object-id><label>Table 2</label><caption><p>Two-dimensional-DIC configurations.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Subset size</td><td align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">161 &#215; 161 pixels</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Initial guess</td><td align="left" valign="middle" rowspan="1" colspan="1">fast Fourier transformation accelerated cross-correlation (FFTCC)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Non-linear optimization</td><td align="left" valign="middle" rowspan="1" colspan="1">inverse compositional Gauss&#8211;Newton algorithm (ICGN)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Subset shape function</td><td align="left" valign="middle" rowspan="1" colspan="1">first-order shape function</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Convergence criterion</td><td align="left" valign="middle" rowspan="1" colspan="1">0.001 pixels</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Stop condition</td><td align="left" valign="middle" rowspan="1" colspan="1">10 iteration steps</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Correlation criterion</td><td align="left" valign="middle" rowspan="1" colspan="1">zero-mean normalized cross-correlation (ZNCC)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interpolation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">bi-cubic B-spline interpolation</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05675-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05675-t003_Table 3</object-id><label>Table 3</label><caption><p>PMSP evaluation parameters in accuracy verification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mi mathvariant="bold">s</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mi mathvariant="bold">d</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05675-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05675-t004_Table 4</object-id><label>Table 4</label><caption><p>PMSP flow (individual camera evaluation) in accuracy verification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">a</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">V</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">G</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">S</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm163" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold">s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">h</italic>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm164" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.000</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.025</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.996</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.000</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.991</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.993</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.992</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm165" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.592</td><td align="center" valign="middle" rowspan="1" colspan="1">0.430</td><td align="center" valign="middle" rowspan="1" colspan="1">0.899</td><td align="center" valign="middle" rowspan="1" colspan="1">0.184</td><td align="center" valign="middle" rowspan="1" colspan="1">0.406</td><td align="center" valign="middle" rowspan="1" colspan="1">0.799</td><td align="center" valign="middle" rowspan="1" colspan="1">0.437</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm166" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.000</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.005</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.994</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.000</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.993</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.987</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.990</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm167" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.604</td><td align="center" valign="middle" rowspan="1" colspan="1">0.190</td><td align="center" valign="middle" rowspan="1" colspan="1">0.647</td><td align="center" valign="middle" rowspan="1" colspan="1">0.207</td><td align="center" valign="middle" rowspan="1" colspan="1">0.932</td><td align="center" valign="middle" rowspan="1" colspan="1">0.294</td><td align="center" valign="middle" rowspan="1" colspan="1">0.244</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">others</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td></tr></tbody></table><table-wrap-foot><fn><p>Values shown in bold indicate the results associated with the selected camera pair. Evaluations for invisible cameras are excluded and replaced with labels &#8220;*&#8221;.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05675-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05675-t005_Table 5</object-id><label>Table 5</label><caption><p>PMSP flow (camera pair evaluation and selection) in accuracy verification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold">d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="2" colspan="1">
<inline-formula>
<mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="bold">C</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="2" colspan="1">
<inline-formula>
<mml:math id="mm170" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="bold">C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="2" colspan="1">
<inline-formula>
<mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="bold">C</mml:mi><mml:mn>5</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="2" colspan="1">
<inline-formula>
<mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mi mathvariant="bold">C</mml:mi><mml:mn>6</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin;border-right:solid thin" rowspan="2" colspan="1">Others</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="2" colspan="1">Camera Group</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="2" colspan="1">
<italic toggle="yes">H</italic>
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">D</italic>
<inline-formula>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td align="center" valign="middle" colspan="2" rowspan="1">
<inline-formula>
<mml:math id="mm173" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">0.914</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.904</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.982</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.396</td></tr><tr><td align="center" valign="middle" colspan="2" rowspan="1">
<inline-formula>
<mml:math id="mm175" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.775</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">0.976</td><td align="center" valign="middle" rowspan="1" colspan="1">0.908</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm176" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.888</bold>
</td></tr><tr><td align="center" valign="middle" colspan="2" rowspan="1">
<inline-formula>
<mml:math id="mm177" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.747</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.107</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">0.936</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm178" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.237</td></tr><tr><td align="center" valign="middle" colspan="2" rowspan="1">
<inline-formula>
<mml:math id="mm179" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.173</td><td align="center" valign="middle" rowspan="1" colspan="1">0.758</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">*</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm180" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.422</td></tr><tr><td align="center" valign="middle" colspan="2" rowspan="1">others</td><td align="center" valign="middle" rowspan="1" colspan="1">*</td><td align="center" valign="middle" rowspan="1" colspan="1">*</td><td align="center" valign="middle" rowspan="1" colspan="1">*</td><td align="center" valign="middle" rowspan="1" colspan="1">*</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.097</td></tr><tr><td align="center" valign="middle" colspan="2" rowspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm182" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.226</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" colspan="2" rowspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">others</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">*</td></tr></tbody></table><table-wrap-foot><fn><p>Values shown in bold indicate the results associated with the selected camera pair. Evaluations for invisible cameras are excluded and replaced with labels &#8220;*&#8221;.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05675-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05675-t006_Table 6</object-id><label>Table 6</label><caption><p>Computational time of MPMC-DIC and visibility-only method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="right" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="8" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Time [s]</th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
Method
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Calibration
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
PVD <sup>1</sup>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
2D-DIC
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
3D-DE <sup>2</sup>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
DID <sup>3</sup>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
ICE <sup>4</sup>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
CPES <sup>5</sup>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Total
</th></tr></thead><tbody><tr><td align="right" valign="middle" rowspan="1" colspan="1">visibility-only method</td><td align="center" valign="middle" rowspan="1" colspan="1">CPU <sup>6</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">135.89</td><td align="center" valign="middle" rowspan="1" colspan="1">3671.80</td><td align="center" valign="middle" rowspan="1" colspan="1">9.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:mphantom><mml:mn>0</mml:mn></mml:mphantom></mml:mrow></mml:math></inline-formula>3816.70</td></tr><tr><td align="right" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU <sup>7</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm184" overflow="scroll"><mml:mrow><mml:mphantom><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mphantom></mml:mrow></mml:math></inline-formula>167.34</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1">/</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm185" overflow="scroll"><mml:mrow><mml:mphantom><mml:mn>00</mml:mn><mml:mo>,</mml:mo></mml:mphantom></mml:mrow></mml:math></inline-formula>312.24</td></tr><tr><td align="right" valign="middle" rowspan="1" colspan="1">MPMC-DIC</td><td align="center" valign="middle" rowspan="1" colspan="1">CPU <sup>6</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">135.89</td><td align="center" valign="middle" rowspan="1" colspan="1">3677.05</td><td align="center" valign="middle" rowspan="1" colspan="1">8.88</td><td align="center" valign="middle" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" rowspan="1" colspan="1">27,077.75</td><td align="center" valign="middle" rowspan="1" colspan="1">6.87</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">30,906.56</td></tr><tr><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPU <sup>7</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm186" overflow="scroll"><mml:mrow><mml:mphantom><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mphantom></mml:mrow></mml:math></inline-formula>167.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm187" overflow="scroll"><mml:mrow><mml:mphantom><mml:mn>00</mml:mn><mml:mo>,</mml:mo></mml:mphantom></mml:mrow></mml:math></inline-formula>313.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm188" overflow="scroll"><mml:mrow><mml:mphantom><mml:mn>00</mml:mn><mml:mo>,</mml:mo></mml:mphantom></mml:mrow></mml:math></inline-formula>633.03</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> PVD refers to projection and visibility determination. <sup>2</sup> Three-dimensional-DE refers to 3D displacement estimation. <sup>3</sup> DID refers to depth image determination. <sup>4</sup> ICE refers to individual camera evaluation. <sup>5</sup> CPES refers to camera pair evaluation and selection. <sup>6</sup> CPU indicates that execution times were recorded using sequential procedures on the CPU for algorithm implementation without multithreading. <sup>7</sup> GPU indicates that the execution times of PVD and DID were recorded using parallel procedures on the GPU for algorithm implementation with 32,768 threads. Other execution time with labels &#8220;-&#8221; remain the same as those recorded while using CPU.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05675-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05675-t007_Table 7</object-id><label>Table 7</label><caption><p>PMSP evaluation parameters in vibration measurement.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm189" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm192" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#945;</mml:mi><mml:mi mathvariant="bold">s</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm193" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#946;</mml:mi><mml:mi mathvariant="bold">d</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.0</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>