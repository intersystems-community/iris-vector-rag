<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="data-paper" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Data Brief</journal-id><journal-id journal-id-type="iso-abbrev">Data Brief</journal-id><journal-id journal-id-type="pmc-domain-id">2750</journal-id><journal-id journal-id-type="pmc-domain">dib</journal-id><journal-title-group><journal-title>Data in Brief</journal-title></journal-title-group><issn pub-type="epub">2352-3409</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12475404</article-id><article-id pub-id-type="pmcid-ver">PMC12475404.1</article-id><article-id pub-id-type="pmcaid">12475404</article-id><article-id pub-id-type="pmcaiid">12475404</article-id><article-id pub-id-type="doi">10.1016/j.dib.2025.112043</article-id><article-id pub-id-type="pii">S2352-3409(25)00765-6</article-id><article-id pub-id-type="publisher-id">112043</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Data Article</subject></subj-group></article-categories><title-group><article-title>3D human pose point cloud data of light detection and ranging (LiDAR)</article-title></title-group><contrib-group><contrib contrib-type="author" id="au0001"><name name-style="western"><surname>Rahmanti</surname><given-names initials="FZ">Farah Zakiyah</given-names></name><xref rid="aff0001" ref-type="aff">a</xref><xref rid="aff0002" ref-type="aff">b</xref></contrib><contrib contrib-type="author" id="au0002"><name name-style="western"><surname>Riansyah</surname><given-names initials="MI">Moch. Iskandar</given-names></name><xref rid="aff0001" ref-type="aff">a</xref><xref rid="aff0003" ref-type="aff">c</xref></contrib><contrib contrib-type="author" id="au0003"><name name-style="western"><surname>Putra</surname><given-names initials="OV">Oddy Virgantara</given-names></name><xref rid="aff0004" ref-type="aff">d</xref></contrib><contrib contrib-type="author" id="au0004"><name name-style="western"><surname>Yuniarno</surname><given-names initials="EM">Eko Mulyanto</given-names></name><xref rid="aff0001" ref-type="aff">a</xref><xref rid="aff0005" ref-type="aff">e</xref></contrib><contrib contrib-type="author" id="au0005"><name name-style="western"><surname>Purnomo</surname><given-names initials="MH">Mauridhi Hery</given-names></name><email>hery@ee.its.ac.id</email><xref rid="aff0001" ref-type="aff">a</xref><xref rid="aff0005" ref-type="aff">e</xref><xref rid="cor0001" ref-type="corresp">&#8270;</xref></contrib><aff id="aff0001"><label>a</label>Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, 60111, Indonesia</aff><aff id="aff0002"><label>b</label>Department of Information Technology, Telkom University, Surabaya Campus, Surabaya, 60231, Indonesia</aff><aff id="aff0003"><label>c</label>Department of Electrical Engineering, Telkom University, Surabaya Campus, Surabaya, 60231, Indonesia</aff><aff id="aff0004"><label>d</label>Department of Informatics, Universitas Darussalam Gontor, Ponorogo, 63472, Indonesia</aff><aff id="aff0005"><label>e</label>Department of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, 60111, Indonesia</aff></contrib-group><author-notes><corresp id="cor0001"><label>&#8270;</label>Corresponding author. <email>hery@ee.its.ac.id</email></corresp></author-notes><pub-date pub-type="collection"><month>10</month><year>2025</year></pub-date><pub-date pub-type="epub"><day>10</day><month>9</month><year>2025</year></pub-date><volume>62</volume><issue-id pub-id-type="pmc-issue-id">494354</issue-id><elocation-id>112043</elocation-id><history><date date-type="received"><day>16</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>1</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>10</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>28</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 The Authors</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="main.pdf"/><abstract id="abs0001"><p>3D Light Detection and Ranging (LiDAR) sensors are closely related to computer vision and deep learning. 3D LiDAR sensors are commonly embedded in smart vehicles to segment humans, cars, trucks, motors, and other objects. However, 3D LiDAR can also be used indoors to predict human poses that are more friendly to a person's privacy because 3D LiDAR does not capture facial images, but it produces data in the form of point clouds. The point cloud produces spatial, geometric, and temporal information which can be used to predict, detect, and classify human poses and activities. The data output from 3D LiDAR, which includes spatial and temporal data, is in PCAP (.pcap) and JSON (.json) formats. The PCAP file contains the sequence frame of the 3D human pose point cloud, and the JSON file contains the metadata. Each human pose class label has one PCAP file and one JSON file. The raw spatio-temporal data must be processed into PCD format as a 3D human pose point cloud dataset for each human pose.</p><p>The total human pose dataset is 1400 3D point cloud data with PCD format (.pcd) used for the training and testing process in deep learning, consisting of four human pose labels. The label classes are hands-to-the-side, sit-down, squat-down, and stand-up human poses, with each class having 280 3D point cloud data used as training data. While the test data amounted to 280 3D point cloud data. The data collection process uses 3D LiDAR, a tripod, a personal computer/laptop, and a talent, demonstrating basic human poses. The 3D LiDAR used is OS1, a product of Ouster, which has a range of 90&#8211;200 m, 128 channels of resolution, and a temperature of -40 &#8211; 60&#176; C. For talent, there is one person and male gender in this current shooting. However, in its development, it can also take female or children or elderly talent to enrich the human pose dataset. The talent is between 30 and 40 years old. The distance between the 3D LiDAR and the talent position is 120 cm. Data collection took place from 10:00 a.m. to 1:00 pm. indoors.</p><p>This dataset is used for human pose prediction using one of the deep learning algorithms, Convolutional Neural Network (CNN). However, the developers can also use other deep learning algorithms such as transformers, Graph Neural Network (GNN), etc.</p></abstract><kwd-group id="keys0001"><title>Keywords</title><kwd>3D LiDAR</kwd><kwd>Human pose prediction</kwd><kwd>Spatial</kwd><kwd>Geometric</kwd><kwd>Temporal</kwd><kwd>3D point cloud dataset</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p id="para0003">Specifications Table<table-wrap position="float" id="utbl0001" orientation="portrait"><table frame="hsides" rules="groups"><tbody><tr><td valign="top" colspan="1" rowspan="1">Subject</td><td valign="top" colspan="1" rowspan="1">Computer Sciences</td></tr><tr><td valign="top" colspan="1" rowspan="1">Specific subject area</td><td valign="top" colspan="1" rowspan="1">3D human pose point cloud dataset of Light Detection and Ranging (LiDAR).</td></tr><tr><td valign="top" colspan="1" rowspan="1">Type of data</td><td valign="top" colspan="1" rowspan="1">Type of data: Raw and has been processed<break/>Data formats: pcap, json, pcd</td></tr><tr><td valign="top" colspan="1" rowspan="1">Data collection</td><td valign="top" colspan="1" rowspan="1">Our dataset was compiled utilizing a single person as a talent demonstrating several basic human poses using a 3D LiDAR sensor. These basic human poses include hands-to-the-side, sit-down, squat-down, and stand-up, which are also class labels. The data collection process uses a 3D LiDAR, a tripod, a personal computer/laptop, and a talent. The minimum specification of a personal computer is a CPU Intel(R) Core (TM) i5&#8211;8350 U CPU @1.70 GHz and memory 16GB speed 2400 MHz. The distance between the 3D LiDAR and the talent position is about 120 cm. The data output of 3D LiDAR, which includes spatial and temporal data, is in PCAP (.pcap) and JSON (.json) formats. This raw data needs to be human segmented using 3D slicing to obtain human Regions of Interest (ROI) and manual labelling for each segment and ensure accuracy and ease of use, so that it can be applied to decision-making. The minimum software required for this process is PyCharm Community Edition 2022.2.1, but it can be replaced with other software, such as Visual Studio Code, etc. The programming language used is Python and the library used to visualize point cloud data is open3d. The python code for performing the conversion PCAP to PCD (pcap2pcd) can be seen in the GitHub link provided with the file name pcap2pcd.py. The Python conversion pcap2pcd is also in our Mendeley Data page parts step to reproduce. The developer can do normalization of 3D points using the min-max scaler method before starting to the learning process with deep learning algorithms.</td></tr><tr><td valign="top" colspan="1" rowspan="1">Data source location</td><td valign="top" colspan="1" rowspan="1">Geographical Coordinates: &#8722;7.282662,112.7951935<break/>Location: Laboratorium of Multimedia Internet of Things, Department of Electrical Engineering<break/>Institution: Institut Teknologi Sepuluh Nopember<break/>City/Town/Region: Surabaya, East Java, 60,115<break/>Country: Indonesia</td></tr><tr><td valign="top" colspan="1" rowspan="1">Data accessibility</td><td valign="top" colspan="1" rowspan="1">Repository name: Mendeley Data<break/>Data identification number: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://doi.org/10.17632/gpvrnphw66.3" id="interref0001ee">doi:10.17632/gpvrnphw66.3</ext-link><break/>Direct URL to data: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/gpvrnphw66/3" id="interref0003">https://data.mendeley.com/datasets/gpvrnphw66/3</ext-link></td></tr><tr><td valign="top" colspan="1" rowspan="1">Related research article</td><td valign="top" colspan="1" rowspan="1">[<xref rid="bib0001" ref-type="bibr">1</xref>]</td></tr></tbody></table></table-wrap></p><sec id="sec0002"><label>1</label><title>Value of the Data</title><p id="para9004">
<list list-type="simple" id="celist0001"><list-item id="celistitem0001"><label>&#8226;</label><p id="para0004">Spatial Information: This dataset was obtained from a 3D LiDAR sensor, which generates 3D point cloud data, which has information in 3D space. Each point of the human pose is represented in 3D coordinates (x, y, z). This helpful information can be the first step in a feature extraction approach based on points, voxels, graphs, etc. Furthermore, the spatial information includes the relative positioning and the distance. This spatial information is also used in the denoising and the normalization steps before starting the learning process using machine learning or deep learning algorithms.</p></list-item><list-item id="celistitem0002"><label>&#8226;</label><p id="para0005">Geometric Information: a 3D point cloud shows the geometric structure of the human body, such as the body joints, poses, postures, size, orientation, topology, and position in 3D space. The dataset provides geometric information in the form of 3D points that show the position of the head, hands, or feet in 3D space, body size, body orientation facing forward or vice versa, and the distance between the points so that it can distinguish several different human poses. The computer vision application which utilizes geometric information, such as obtaining the skeleton of the human body, human pose prediction, and human pose estimation.</p></list-item><list-item id="celistitem0003"><label>&#8226;</label><p id="para0006">Temporal Information: 3D LiDAR scans objects and captures changes in object movement in a few moments, depending on the scanning time. The dataset provides temporal information in the form of time sequences, durations, and transitions of human poses in 3D space. This potential information can help recognize human pose estimation, human pose tracking, and human activities such as basic activities (stand-up, sit-down, squat-down) and exercising (hands to the side). The human pose was chosen because it is basic for many human activities.</p></list-item><list-item id="celistitem0004"><label>&#8226;</label><p id="para0007">Health Field: Track movement by analyzing human pose for physiotherapy medic which focuses on restoring and improving the body's function. Human pose analysis by professional medical personnel can help provide appropriate recommendations for recovery patients, especially in physical and movement. This dataset can be used as a benchmark for healthy adult human poses, as there are fundamental differences between healthy and unhealthy human poses. However, this dataset only provides human poses for healthy adults.</p></list-item><list-item id="celistitem0005"><label>&#8226;</label><p id="para0008">Monitoring application and safety analytics for anomaly detection: There are several human tracking applications that can be used for various topics, such as pose estimation and gesture recognition, human activity recognition, elderly monitoring from early fall detection, near-fall detection, exercising, and occupational safety by detecting abnormal human poses. However, dataset collection will continue to be improved by the development of research conducted and will not only focus on basic human poses. Potential deployments in the bathroom or bedroom, which prioritize privacy.</p></list-item><list-item id="celistitem0006"><label>&#8226;</label><p id="para0009">The dataset supports anomaly detection under challenging conditions, such as low illumination or backlighting, whereas the reliability of 3D LiDAR is.</p></list-item><list-item id="celistitem0007"><label>&#8226;</label><p id="para0010">Human Pose Prediction, Computer Vision, and Deep Learning: This dataset is invaluable for researchers and developers in human pose prediction, computer vision, and deep learning. It provides an opportunity to develop deep learning models that aim to understand human poses and movements. Computer vision and deep learning development can be combined with other sensors like mmWave radar, camera, etc. Multimodal can be implemented in several computer vision and deep learning case studies.</p></list-item></list>
</p></sec><sec id="sec0003"><label>2</label><title>Background</title><p id="para0011">The development of computer vision-based technology utilizing 3D LiDAR sensors offers spatial, geometric, and temporal information that has the potential to predict and analyze human poses, postures, movements, orientations, and activities with greater accuracy. The availability of public 3D point cloud datasets of humans indoors is still minimal. This dataset contributes to the development of innovations in computer vision and deep learning related to human pose prediction [<xref rid="bib0001" ref-type="bibr">1</xref>], human orientation [<xref rid="bib0002" ref-type="bibr">2</xref>], denoising human pose [<xref rid="bib0003" ref-type="bibr">3</xref>], object classification [<xref rid="bib0003" ref-type="bibr">3</xref>,<xref rid="bib0004" ref-type="bibr">4</xref>], human pose estimation, and human activities, among others. For example, innovations in the health sector, such as physiotherapy and medical rehabilitation, sports, human-computer interaction, and security, have been developed. This 3D human pose point cloud dataset aims to engage a wider audience, including developers and the public. This dataset serves as a means of interdisciplinary research and community service, bridging medical staff with the latest technology and connecting sports coaches with the latest innovations.</p><p id="para0012"><xref rid="tbl0001" ref-type="table">Table 1</xref> shows a comparative overview of several previous datasets. The differences are visible based on the dataset's value, the device used in data collection, limitations, and the dataset gap. Our valuable datasets are privacy-preserving human sensing and spatio-temporal 3D point cloud data.<table-wrap position="float" id="tbl0001" orientation="portrait"><label>Table 1</label><caption><p>Comparative Overview of Different Dataset.</p></caption><alt-text id="alt0001">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top" colspan="1" rowspan="1">Dataset</th><th valign="top" colspan="1" rowspan="1">Value</th><th valign="top" colspan="1" rowspan="1">Device used</th><th valign="top" colspan="1" rowspan="1">Limitation</th><th valign="top" colspan="1" rowspan="1">Gap</th></tr></thead><tbody><tr><td valign="top" colspan="1" rowspan="1">[<xref rid="bib0005" ref-type="bibr">5</xref>]</td><td valign="top" colspan="1" rowspan="1">Dataset for Emergency Medical Services (EMS) vehicles</td><td valign="top" colspan="1" rowspan="1">LiDAR and RGB camera</td><td valign="top" colspan="1" rowspan="1">EMS vehicles,<break/>Created using the CARLA simulator</td><td valign="top" colspan="1" rowspan="1">Outdoor area and focused on enriching specialized objects such as EMS vehicles</td></tr><tr><td valign="top" colspan="1" rowspan="1">[<xref rid="bib0006" ref-type="bibr">6</xref>]</td><td valign="top" colspan="1" rowspan="1">Privacy-Preserving Human Sensing</td><td valign="top" colspan="1" rowspan="1">Radar and RGB-D camera</td><td valign="top" colspan="1" rowspan="1">Supports 3D pose estimation and gesture recognition</td><td valign="top" colspan="1" rowspan="1">Radar produces sparser data, pose details may be lost</td></tr><tr><td valign="top" colspan="1" rowspan="1">[<xref rid="bib0007" ref-type="bibr">7</xref>]</td><td valign="top" colspan="1" rowspan="1">Dataset with high-density point cloud</td><td valign="top" colspan="1" rowspan="1">MOVE4D System</td><td valign="top" colspan="1" rowspan="1">Does not contain temporal data</td><td valign="top" colspan="1" rowspan="1">Focused on a snapshot per pose</td></tr><tr><td valign="top" colspan="1" rowspan="1">[<xref rid="bib0008" ref-type="bibr">8</xref>]</td><td valign="top" colspan="1" rowspan="1">Sequence frame of Video</td><td valign="top" colspan="1" rowspan="1">Digital camera 12 MP</td><td valign="top" colspan="1" rowspan="1">Does not privacy-preserving human sensing</td><td valign="top" colspan="1" rowspan="1">A different way of data collection and focused on 2D data per frame</td></tr><tr><td valign="top" colspan="1" rowspan="1">Current</td><td valign="top" colspan="1" rowspan="1">Sequence frame of 3D human pose point cloud,<break/>Privacy-preserving human sensing, spatial and temporal 3D point cloud data</td><td valign="top" colspan="1" rowspan="1">3D LiDAR</td><td valign="top" colspan="1" rowspan="1">One male as a talent,<break/>Indoor area, specific human pose</td><td valign="top" colspan="1" rowspan="1">Focused on 3D point cloud data and human pose</td></tr></tbody></table></table-wrap></p></sec><sec id="sec0004"><label>3</label><title>Data Description</title><p id="para0013">The data output from the 3D LiDAR sensor is in PCAP (.pcap) and JSON (.json) formats as raw data. The form of data is spatio-temporal point cloud data, one frame 3D point cloud generated of 3D LiDAR as in <xref rid="fig0001" ref-type="fig">Fig 1</xref>. The raw data needs to be segmented to obtain a 3D human pose point cloud that only focuses on humans. The human segmentation process produces a total of 1400 3D point cloud data according to <xref rid="tbl0002" ref-type="table">Table 2</xref> with the class distribution according to <xref rid="fig0002" ref-type="fig">Fig 2</xref>. Each human pose class has 280 data, the total data contained in the four classes is 1120 data points. The proportion between training and testing data is 80 % and 20 %. The amount of test data used is 280 data.<fig id="fig0001" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Raw Data 3D Point Cloud of LiDAR.</p></caption><alt-text id="alt0002">Fig 1</alt-text><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0001" position="float" orientation="portrait" xlink:href="gr1.jpg"/></fig><table-wrap position="float" id="tbl0002" orientation="portrait"><label>Table 2</label><caption><p>Dataset Allocation.</p></caption><alt-text id="alt0003">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top" colspan="1" rowspan="1">Num</th><th valign="top" colspan="1" rowspan="1">Data Class</th><th valign="top" colspan="1" rowspan="1">Number of Data<break/>(3D point cloud)</th><th valign="top" colspan="1" rowspan="1">Allocation</th></tr></thead><tbody><tr><td valign="top" colspan="1" rowspan="1">1</td><td valign="top" colspan="1" rowspan="1">Hands-to-the-side</td><td valign="top" colspan="1" rowspan="1">280</td><td valign="top" colspan="1" rowspan="1">Training Data</td></tr><tr><td valign="top" colspan="1" rowspan="1">2</td><td valign="top" colspan="1" rowspan="1">Sit-down</td><td valign="top" colspan="1" rowspan="1">280</td><td valign="top" colspan="1" rowspan="1">Training Data</td></tr><tr><td valign="top" colspan="1" rowspan="1">3</td><td valign="top" colspan="1" rowspan="1">Squat-down</td><td valign="top" colspan="1" rowspan="1">280</td><td valign="top" colspan="1" rowspan="1">Training Data</td></tr><tr><td valign="top" colspan="1" rowspan="1">4</td><td valign="top" colspan="1" rowspan="1">Stand-up</td><td valign="top" colspan="1" rowspan="1">280</td><td valign="top" colspan="1" rowspan="1">Training Data</td></tr><tr><td valign="top" colspan="1" rowspan="1">5</td><td valign="top" colspan="1" rowspan="1">(Mix Human Poses: hands-to-the-side, sit-down, squat-down, and stand-up)</td><td valign="top" colspan="1" rowspan="1">280</td><td valign="top" colspan="1" rowspan="1">Testing Data</td></tr><tr><td valign="top" colspan="1" rowspan="1"/><td valign="top" colspan="1" rowspan="1"><bold>Total</bold></td><td valign="top" colspan="1" rowspan="1"><bold>1400</bold></td><td valign="top" colspan="1" rowspan="1">-</td></tr></tbody></table></table-wrap><fig id="fig0002" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Balance Data based on Number of Datasets.</p></caption><alt-text id="alt0004">Fig 2</alt-text><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0002" position="float" orientation="portrait" xlink:href="gr2.jpg"/></fig></p><p id="para0014">The dataset owned includes balanced data with the same amount in each class. The advantage of the balance dataset is that the deep learning model will learn fairly for all human pose classes, thereby reducing the risk of bias towards certain classes and minimizing special handling of datasets such as resampling.</p><p id="para0015">The dimensions of length, width, and height of 3D human poses vary. This dimension depends on the poses; for example, a standing pose will have different dimensions than a pose with hands to the side. Despite the differences in size in 3D space, this proves that the existing dataset has challenges in processing the data. This dataset will be particularly useful for developers and researchers in computer vision, deep learning, health, and sports who are interested in pose, posture, and body orientation. The dataset offers opportunities for anyone seeking to understand, explore, and process data using point cloud approaches and deep learning methods.</p></sec><sec id="sec0005"><label>4</label><title>Experimental Design, Materials and Methods</title><p id="para0016">The minimum specification of a personal computer is a CPU Intel(R) Core (TM) i5&#8211;8350 U CPU @1.70 GHz and memory 16GB speed 2400 MHz. The specification device used is the 3D LiDAR sensor used is OS1 is shown on <xref rid="fig0005" ref-type="fig">Fig 5</xref>, a product of Ouster which has a range of 90&#8211;200 m, 128 channels of resolution, and a temperature of &#8722;40 &#8211; 60&#176; C. The minimum software required for this process is PyCharm Community Edition 2022.2.1, but it can be replaced with other software such as Visual Studio Code, etc.</p><p id="para0017">The data output from 3D LiDAR, which includes spatial and temporal data, is in PCAP (.pcap) and JSON (.json) formats. Each human pose class label has one PCAP file and one JSON file. The raw spatio-temporal data must be processed into PCD format as a 3D human pose point cloud dataset for each human pose.</p><p id="para0018">Some examples of human pose visualization on 3D point cloud data using the Open3D library are shown in <xref rid="tbl0003" ref-type="table">Table 3</xref>. The visualization of each pose is based on three representative methods: scanning with the body facing forward, backward, and sideways using 3D LiDAR. The visualization of each scanning is based on the front view, side view, back view, and top view.<table-wrap position="float" id="tbl0003" orientation="portrait"><label>Table 3</label><caption><p>Example of 3D Human Pose Point Cloud Dataset.</p></caption><alt-text id="alt0005">Table 3</alt-text><table frame="hsides" rules="groups"><tbody><tr><td valign="top" colspan="1" rowspan="1"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0007" xlink:href="fx1a.gif"><alt-text id="alt1">Image, table 3</alt-text></inline-graphic></td></tr><tr><td valign="top" colspan="1" rowspan="1"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0007b" xlink:href="fx1b.gif"><alt-text id="alt1b">Image, table 3</alt-text></inline-graphic></td></tr></tbody></table></table-wrap></p><p id="para0019">The total dataset consists of 1400 3D human point cloud data, where 1120 are training data and 280 are testing data. Each class consists of 280 data, according to <xref rid="tbl0002" ref-type="table">Table 2</xref>. The distribution of the dataset for each class is shown on <xref rid="fig0003" ref-type="fig">Fig 3</xref>. Each class comprises 25 % of the training data. According to <xref rid="fig0004" ref-type="fig">Fig 4</xref>, the distribution of the training data and testing data is as follows: the training data has a distribution of 80 %, and the testing data has a distribution of 20 %.<fig id="fig0003" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Distribution of Human Pose Class Label.</p></caption><alt-text id="alt0006">Fig 3</alt-text><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0003" position="float" orientation="portrait" xlink:href="gr3.jpg"/></fig><fig id="fig0004" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Distribution of Training and Testing Data.</p></caption><alt-text id="alt0007">Fig 4</alt-text><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0004" position="float" orientation="portrait" xlink:href="gr4.jpg"/></fig><fig id="fig0005" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>OS1 3D LiDAR Sensor.</p></caption><alt-text id="alt0008">Fig 5</alt-text><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0005" position="float" orientation="portrait" xlink:href="gr5.jpg"/></fig></p><p id="para0020">The data collection process is shown in <xref rid="fig0006" ref-type="fig">Fig 6</xref> 3D LiDAR scans at a distance of 120 cm from humans. The three important things here are getting the file source PCAP with pcap.Pcap(pcap_path, info) and also this meta data source.metadata, the last is scans to convert PCAP and get PCD per frame with client.Scans(source). Import pcap and client from library ouster. The python code for performing the conversion pcap2pcd can be seen in the GitHub link provided with the file name pcap2pcd.py. The Python conversion pcap2pcd is also in our Mendeley Data page parts step to reproduce.<fig id="fig0006" position="float" orientation="portrait"><label>Fig. 6</label><caption><p>Data Collection.</p></caption><alt-text id="alt0009">Fig 6</alt-text><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="celink0006" position="float" orientation="portrait" xlink:href="gr6.jpg"/></fig></p><p id="para0021">Raw data from 3D LiDAR is visualized using Python and open3D. The visualization results can be viewed from various directions, for example, top view, side view, or by rotating; it will get a view with a different perspective. The following process is human segmentation, which is done by cropping 3D to identify the region of interest. The final process is labeling and adjusting each 3D human point cloud data to its human pose label class.</p></sec><sec id="sec0006"><title>Limitations</title><p id="para0022">The dataset has significant limitations, including only representing four basic human poses, the talent involved is one person, and data collection is only indoors. The talent is between 30 and 40 years old. The distance between the 3D LiDAR position and the talent position is 120 cm. Data collection took place from 10:00 a.m. to 1:00 pm. indoors. Our work also considers the early study of human pose prediction. Some improvements in data collection are needed, such as taking other human poses, diverse talent, and multi-person, and data collection is also carried out outdoors. More varied human poses and diverse talent from various sexes (male or female) and ages (children, adults, or the elderly) will add value to the data. Our limitations will impact the diversity of human body shapes and movements later. These highlight limitations for more diverse data collection in the real world, considering the correct pre-processing method based on noise factor will affect the results of the human pose prediction.</p></sec><sec id="sec0007"><title>Ethics Statement</title><p id="para0023">This research complies with ethical standards because the data is in the form of a point cloud that does not display an image of a person and does not display personal information.</p></sec><sec id="sec0008"><title>Credit Author Statement</title><p id="para0024"><bold>Farah Zakiyah Rahmanti:</bold> Software, Methodology, Formal analysis, Resources, Data curation, Writing &#8211;original draft, Writing &#8211;review &amp; editing, Visualization; <bold>Moch. Iskandar Riansyah:</bold> Software, Data curation, Investigation, Writing &#8211;review &amp; editing; <bold>Oddy Virgantara Putra:</bold> Software, Data curation, Investigation, Writing &#8211;review &amp; editing; <bold>Eko Mulyanto Yuniarno:</bold> Conceptualization, Methodology, Validation, Resources, Supervision, Project administration; <bold>Mauridhi Hery Purnomo:</bold> Conceptualization, Validation, Resources, Supervision, Funding acquisition.</p></sec></body><back><ref-list id="cebibl1"><title>References</title><ref id="bib0001"><label>1</label><element-citation publication-type="journal" id="sbref0001"><person-group person-group-type="author"><name name-style="western"><surname>Rahmanti</surname><given-names>F.Z.</given-names></name><name name-style="western"><surname>Riansyah</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>Putra</surname><given-names>O.V.</given-names></name><name name-style="western"><surname>Yuniarno</surname><given-names>E.M.</given-names></name><name name-style="western"><surname>Purnomo</surname><given-names>M.H.</given-names></name></person-group><article-title>Human pose prediction using convolution Neural Network 3D based on binary voxel feature extraction (BIVFE) of light detection and ranging (LiDAR) point cloud data</article-title><source>Int. J. Intell. Eng. Syst.</source><volume>18</volume><issue>6</issue><year>Jul. 2025</year><fpage>301</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.22266/ijies2025.0731.19</pub-id></element-citation></ref><ref id="bib0002"><label>2</label><element-citation publication-type="book" id="sbref0002"><person-group person-group-type="author"><name name-style="western"><surname>Riansyah</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>Putra</surname><given-names>O.V.</given-names></name><name name-style="western"><surname>Priyadi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sardjono</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Yuniarno</surname><given-names>E.M.</given-names></name><name name-style="western"><surname>Purnomo</surname><given-names>M.H.</given-names></name></person-group><part-title>Modified CNN VoxNet based depthwise separable convolution for Voxel-driven body orientation classification</part-title><source>2024 IEEE International Conference on Imaging Systems and Techniques (IST)</source><year>Oct. 2024</year><publisher-name>IEEE</publisher-name><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/IST63414.2024.10759165</pub-id></element-citation></ref><ref id="bib0003"><label>3</label><element-citation publication-type="journal" id="sbref0003"><person-group person-group-type="author"><name name-style="western"><surname>Putra</surname><given-names>O.V.</given-names></name><etal/></person-group><article-title>Enhancing LiDAR-based object recognition through a novel denoising and modified GDANet framework</article-title><source>IEEE Access</source><volume>12</volume><year>2024</year><fpage>7285</fpage><lpage>7297</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3347033</pub-id></element-citation></ref><ref id="bib0004"><label>4</label><element-citation publication-type="book" id="sbref0004"><person-group person-group-type="author"><name name-style="western"><surname>Putra</surname><given-names>O.V.</given-names></name><name name-style="western"><surname>Riansyah</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>Riandini</surname><given-names>A.Priyadi</given-names></name><name name-style="western"><surname>Yuniarno</surname><given-names>E.M.</given-names></name><name name-style="western"><surname>Purnomo</surname><given-names>M.H.</given-names></name></person-group><part-title>Fuzzy lightweight CNN for point cloud object classification based on Voxel</part-title><source>TENCON 2023 - 2023 IEEE Region 10 Conference (TENCON)</source><year>Oct. 2023</year><publisher-name>IEEE</publisher-name><fpage>685</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1109/TENCON58879.2023.10322519</pub-id></element-citation></ref><ref id="bib0005"><label>5</label><element-citation publication-type="journal" id="sbref0005"><person-group person-group-type="author"><name name-style="western"><surname>Jaiswal</surname><given-names>C.</given-names></name><name name-style="western"><surname>Acquaah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nenebi</surname><given-names>C.</given-names></name><name name-style="western"><surname>AlHmoud</surname><given-names>I.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Gokaraju</surname><given-names>B.</given-names></name></person-group><article-title>EMS3D-KITTI: synthetic 3D dataset in KITTI format with a fair distribution of Emergency Medical Services vehicles for autodrive AI model training</article-title><source>Data Br</source><volume>58</volume><year>Feb. 2025</year><object-id pub-id-type="publisher-id">111221</object-id><pub-id pub-id-type="doi">10.1016/j.dib.2024.111221</pub-id><pub-id pub-id-type="pmcid">PMC11730950</pub-id><pub-id pub-id-type="pmid">39811523</pub-id></element-citation></ref><ref id="bib0006"><label>6</label><element-citation publication-type="journal" id="sbref0006"><person-group person-group-type="author"><name name-style="western"><surname>Roshandel</surname><given-names>N.</given-names></name><etal/></person-group><article-title>mmPrivPose3D: a dataset for pose estimation and gesture command recognition in human-robot collaboration using frequency modulated continuous wave 60Hhz RaDAR</article-title><source>Data Br</source><volume>59</volume><year>Apr. 2025</year><object-id pub-id-type="publisher-id">111316</object-id><pub-id pub-id-type="doi">10.1016/j.dib.2025.111316</pub-id><pub-id pub-id-type="pmcid">PMC11803241</pub-id><pub-id pub-id-type="pmid">39925393</pub-id></element-citation></ref><ref id="bib0007"><label>7</label><element-citation publication-type="journal" id="sbref0007"><person-group person-group-type="author"><name name-style="western"><surname>Ruescas-Nicolau</surname><given-names>A.V.</given-names></name><name name-style="western"><surname>Medina-Ripoll</surname><given-names>E.J.</given-names></name><name name-style="western"><surname>Parrilla Bernab&#233;</surname><given-names>E.</given-names></name><name name-style="western"><surname>de Rosario Mart&#237;nez</surname><given-names>H.</given-names></name></person-group><article-title>Multimodal human motion dataset of 3D anatomical landmarks and pose keypoints</article-title><source>Data Br</source><volume>53</volume><year>Apr. 2024</year><object-id pub-id-type="publisher-id">110157</object-id><pub-id pub-id-type="doi">10.1016/j.dib.2024.110157</pub-id><pub-id pub-id-type="pmcid">PMC10875237</pub-id><pub-id pub-id-type="pmid">38375138</pub-id></element-citation></ref><ref id="bib0008"><label>8</label><element-citation publication-type="journal" id="sbref0008"><person-group person-group-type="author"><name name-style="western"><surname>Awad</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Tulaib</surname><given-names>L.F.</given-names></name><name name-style="western"><surname>Saleh</surname><given-names>H.M.</given-names></name></person-group><article-title>Gait recognition by computing fixed body parameters</article-title><source>Babylonian J. Netw.</source><volume>2024</volume><year>Sep. 2024</year><fpage>191</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.58496/BJN/2024/019</pub-id></element-citation></ref></ref-list><sec sec-type="data-availability" id="refdata001"><title>Data Availability</title><p id="para9001">
<list list-type="simple" id="dacelist0001"><list-item id="rdlistitem0001"><p id="para9002">
Mendeley Data<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/gpvrnphw66/3" id="interref0001">3D Human Pose Point Cloud Data of Light Detection and Ranging (LiDAR) (Original data)</ext-link>
</p></list-item><list-item id="rdlistitem0002"><p id="para9003">
Mendeley Data<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/gpvrnphw66/2" id="interref0002">3D Human Pose Point Cloud Dataset of Light Detection and Ranging (LiDAR) (Original data)</ext-link>
</p></list-item></list>
</p></sec><ack id="ack0001"><title>Acknowledgements</title><p id="para0025">This work is supported by <funding-source id="gs0001">Beasiswa Pendidikan Indonesia (BPI)</funding-source> through the PhD completion scholarship program in 2023. BPI is under the Ministry of Education, Culture, Research, and Technology (Kemendikbudristek).</p><sec id="sec0608"><title>Declaration of Competing Interest</title><p id="para0026">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></sec></ack></back></article></pmc-articleset>