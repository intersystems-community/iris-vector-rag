<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12475121</article-id><article-id pub-id-type="pmcid-ver">PMC12475121.1</article-id><article-id pub-id-type="pmcaid">12475121</article-id><article-id pub-id-type="pmcaiid">12475121</article-id><article-id pub-id-type="pmid">41006381</article-id><article-id pub-id-type="doi">10.1038/s41598-025-16980-9</article-id><article-id pub-id-type="publisher-id">16980</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Optimized extreme learning machines with deep learning for high-performance network traffic classification</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="X">Xi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Yin</surname><given-names initials="J">Jun</given-names></name><address><email>Zx810588969@163.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04mkzax54</institution-id><institution-id institution-id-type="GRID">grid.258151.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 0708 1323</institution-id><institution>School of Design, </institution><institution>Jiangnan University, </institution></institution-wrap>Wuxi, 214122 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04mkzax54</institution-id><institution-id institution-id-type="GRID">grid.258151.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 0708 1323</institution-id><institution>School of Digital Technology &amp;Innovation Design, </institution><institution>Jiangnan University, </institution></institution-wrap>Wuxi, 214122 China </aff></contrib-group><pub-date pub-type="epub"><day>26</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>33199</elocation-id><history><date date-type="received"><day>4</day><month>5</month><year>2025</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>28</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 16:25:18.507"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="41598_2025_Article_16980.pdf"/><abstract id="Abs1"><p id="Par1">The exponential growth in network users and applications, coupled with increasing dependence on networked systems, has elevated network security to a paramount concern for service providers and organizations. Traffic analysis has emerged as a pivotal technique for identifying malicious activities, enabling critical functions such as bandwidth management, fault detection, quality assessment, pricing, and lawful security monitoring. We propose a novel framework for network traffic classification using an Improved Extreme Learning Machine (IELM). The proposed approach advances traditional extreme learning by incorporating a particle swarm optimization algorithm to optimize model parameters, alongside a deep learning-based feature selection mechanism to assess and prioritize input feature relevance, thereby enhancing classification precision. The framework&#8217;s performance was rigorously evaluated using the CICIDS 2017 dataset, a widely recognized benchmark in network traffic analysis. The results demonstrate the framework&#8217;s capability to accurately classify network traffic into secure and insecure categories, achieving a remarkable detection accuracy of 98.756%. These findings underscore the efficacy of the IELM-based approach in detecting malicious activities and mitigating security risks, offering a robust and scalable solution for strengthening network protection.</p><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-16980-9.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Extreme learning machine (ELM)</kwd><kwd>Particle swarm optimization (PSO)</kwd><kwd>Network traffic classification</kwd><kwd>Feature selection</kwd><kwd>Network security</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Engineering</kwd><kwd>Mathematics and computing</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Enabling essential network security applications, IP traffic classification such as intrusion detection systems (IDS), firewalls, and quality of service (QoS) management poses a significant challenge for Internet service providers, as well as government and private organizations. By analyzing network traffic, it becomes possible to identify intrusive programs or anomalies that disrupt traffic patterns and compromise network stability. Traditional traffic classification methods, particularly in cellular networks, are increasingly being phased out due to their reliance on the statistical characteristics of network traffic packets<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. These methods aim to classify application traffic without directly inspecting packet contents, thereby preserving data confidentiality. However, they suffer from notable limitations, including challenges in adapting to application updates, susceptibility to packet loss, and inefficiencies in handling the rapidly increasing rates of data exchange<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. To overcome these limitations, modern approaches employ machine learning (ML) for traffic classification due to its strong generalization capabilities in dynamic environments<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The growing diversity of traffic patterns, coupled with the accumulation of large-scale datasets containing numerous features, has further complicated the analysis and classification of network traffic. As a result, feature extraction techniques have become indispensable for identifying and utilizing key information effectively. Recent advancements have demonstrated the effectiveness of deep learning (DL) in various application domains. For instance, Chakravarthy and Rajaguru<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> showed that DL can enhance diagnostic precision in medical imaging. Meenalochini and Ramkumar<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> applied DL for breast cancer classification, while Sannasi et al.<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> extended DL with fuzzy ensemble learning for early detection. In the field of industrial applications, Chakravarthy et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> proposed DL-integrated optimization for security tasks. These findings collectively underscore the potential of DL in addressing the complex challenges of modern network traffic classification.</p><p id="Par3">A primary challenge in traffic analysis lies in the absence of explicit relationships between inputs and outputs. Input data, derived from network monitoring, must be translated into actionable outputs&#8212;decisions regarding whether traffic is secure or malicious. However, no predefined equations exist for this conversion. Addressing this issue requires the use of machine learning methods, which aim to model these relationships by optimizing parameters. Nonetheless, several challenges persist in applying ML techniques to network traffic classification: 1-Training Speed: The slow training speed of many ML models makes their implementation time-intensive. Most models require optimization of a large number of network parameters. The Extreme Learning Machine (ELM), however, is known for requiring fewer parameters, allowing faster training<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. In this research, we propose a method to improve training efficiency by reducing the number of hidden layers. 2- Parameter Optimization: Parameter tuning in ML often involves trial and error, consuming significant time and resources<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. The Improved Extreme Learning Machine (IELM) introduced in this study for automatic parameter adjustment. 3- High Dimensionality: Traffic analysis frequently involves a large number of input parameters, which can slow down processing and, in some cases, increase error rates. Additionally, the vast amount of data collected during network monitoring poses a significant challenge, particularly with the rise of 5G and 6G networks<sup><xref ref-type="bibr" rid="CR12">12</xref>&#8211;<xref ref-type="bibr" rid="CR16">16</xref></sup>. Real-time traffic analysis, essential for active networks, is further complicated by the need to process such large datasets efficiently<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>. Although this study does not focus directly on large-scale data processing, we incorporate feature selection techniques to reduce data size and mitigate this challenge. Unlike prior ELM-based approaches, our PSO-ELM framework introduces three key innovations: (1) joint optimization of feature selection and classification through deep learning-guided backward elimination, (2) dynamic PSO-based adaptation of both hidden layer weights and architecture size during training, and (3) real-time applicability demonstrated by prediction times of &lt;&#8201;15&#181;s. While accuracy improvements over GA-ELM appear modest (~&#8201;1%), the architectural novelty lies in the simultaneous optimization of feature subspaces and model parameters - a capability GA-ELM lacks due to its fixed gene representation. This is particularly crucial for network traffic analysis where feature relevance shifts dynamically across attack types.</p><p id="Par4">The primary goal of this research is to develop an efficient network traffic classification method based on an enhanced extreme learning machine framework, combining ELM with particle swarm optimization (PSO-ELM). A secondary goal is to improve processing speed by optimizing batch sizes and identifying the influence of parameters on batching efficiency. This paper is organized into five sections: Section "<xref rid="Sec1" ref-type="sec">Introduction</xref>" introduces the problem and its significance. Section&#160;"<xref rid="Sec1" ref-type="sec">Related works</xref>" reviews related work, highlighting the advantages and limitations of existing approaches. Section&#160;"<xref rid="Sec3" ref-type="sec">Proposed method</xref>" details the proposed methodology, including the classification framework and feature selection process. Section&#160;"<xref rid="Sec9" ref-type="sec">Result and discussion</xref>" presents the evaluation results, and section "<xref rid="Sec10" ref-type="sec">Conclusion</xref>" concludes the study by summarizing key findings and contributions.</p></sec><sec id="Sec2"><title>Related works</title><p id="Par5">Zhang et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> presented an encrypted traffic tracking and intrusion detection framework, which is a lightweight method utilizing deep learning. In this framework, three deep learning algorithms were used, namely CNN (Convolutional Neural Network), LSTM (Long Short-Term Memory), and SAE (Stacked Autoencoder). The advantage of the encrypted traffic DFR categorization and intrusion detection is that it can provide much stronger and more accurate performance compared to advanced methods, while requiring less storage resources. However, a drawback of this approach is that due to the labeling of deprecated features and privacy protocols, it may no longer align with changes in the network environment. Zhang et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> propose a conventional neural network-based algorithm called Normalized Conventional Neural Networks. This algorithm can implicitly learn from the maximum training data set (MMN5-CNN) to extract features and can prevent artificial feature extraction, addressing the issue of discrepancies caused. The experimental results show that the proposed traffic classification method provides better performance than the traditional CNN-based methods. Recent literature has shown growing interest in integrating intelligent traffic control with machine learning, particularly in smart city environments. For example, in<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, a fusion-based system using machine learning was proposed for congestion detection in vehicular networks, significantly improving traffic responsiveness in smart cities. Similarly<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, developed a deep extreme learning machine model to accurately detect road occupancy for smart parking applications, demonstrating low latency and high prediction accuracy. Another noteworthy study by<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> introduced an adaptive congestion control mechanism using real-time traffic data and intelligent prediction models, offering substantial improvements in throughput and road efficiency. These studies reinforce the versatility of ELM-based and deep learning approaches for real-world traffic analysis and classification problems.</p><p id="Par6">This method is able to increase the accuracy and reduce the classification time compared to the traditional classification approaches. Lim et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> conducted multiple experiments to classify network traffic using two deep learning models, CNN and ResNet. Their research demonstrated that deep learning models can effectively categorize network traffic. Furthermore, they found that ResNet outperforms CNN in network traffic classification by achieving higher accuracy. Therefore, when using the CNN model, ResNet provides a better network. The advantage of their model is its accuracy and quality of service, achieving high classification in the QoS method. Hasibi et al.<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> introduced an innovative approach to data aggregation using short-term memory networks. Their method aimed to estimate congestion levels, identify traffic flow patterns, and preserve the numerical characteristics of each class. One of the disadvantages of this approach is that it does not provide good evaluation criteria for unbalanced environments such as the Internet. Wang et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> designed the SDN1-HGW framework, centered around SDN-HGW, to facilitate distributed network quality management. In their approach, a deep learning-based classifier was developed for encrypted data packets in DataNet. Three methods CNN (Convolutional Neural Network), SAE (Stacked Auto-encoder), and MLP (Multilayer Perceptron) were used to develop the framework. To develop and evaluate the proposed DataNet, a dataset containing more than 20,000 packets from 15 different applications was utilized. The advantage of their design is that the developed DataNets can be used for remote operations in the SDN-HGW program, distributed across future smart home networks. One of the disadvantages of this design is that these solutions cannot provide precise traffic classification scheduling in encrypted networks. In<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, the authors introduced the SDN-HGW framework to enhance distributed network quality management. To develop and evaluate the proposed DataNets, they utilized an open dataset comprising over 20,000 packets from 15 different applications. Experimental results confirmed that the developed DataNets could be effectively integrated into the SDN-HGW framework, achieving accurate packet classification and high computational efficiency for real-time processing in smart home networks. In<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, the authors presented an end-to-end encrypted traffic classification approach leveraging integrated one-dimensional neural networks. This method unifies feature extraction, feature selection, and classification within a single end-to-end framework, aiming to automatically capture the nonlinear relationship between raw input and expected output. Among four conducted experiments, the optimal traffic representation and fine-tuned model led to superior performance in 11 out of 12 evaluation metrics, demonstrating the effectiveness of the proposed approach. In<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, deep learning (DL) is suggested as a viable method for designing traffic classifiers based on automatically extracted features that capture the intricate patterns of mobile traffic. Using three real-world datasets that highlight user activities, the study thoroughly examines the performance of DL classifiers in encrypted mobile traffic classification, while also discussing open challenges and key design principles. In<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, the authors introduce a novel network traffic classification model, ArcMargin, which integrates metric learning into a CNN-based neural network. The proposed ArcMargin model is validated using three benchmark datasets and compared with multiple related algorithms. Based on various classification metrics, results confirm that ArcMargin outperforms other models in traffic classification tasks. In<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, a deep learning-based method for analyzing IoT malware traffic through visual representation is proposed to enable faster detection and classification of new (zero-day) malware. The approach utilizes a residual neural network (ResNet-50), and experimental results demonstrate promising outcomes, achieving an accuracy of 94.50% in detecting malware traffic. In<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, two hybrid models are introduced, combining Convolutional Neural Networks (CNN) with Recurrent Neural Networks (RNN), specifically Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) networks, to enhance traffic classification accuracy. These models are evaluated using real network traffic and compared against various standalone models. The CNN-GRU and CNN-LSTM approaches achieve accuracy rates of 99.23% and 93.92% for binary classification and 67.16% for multi-class classification. In<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, a multi-task deep learning approach is proposed for traffic classification, resulting in a classifier named Distiller. This method enhances traffic data heterogeneity by learning intra- and inter-dependencies, effectively overcoming the limitations of single-model deep learning-based classification methods, which often suffer from short-sightedness. Additionally, it simultaneously addresses multiple related traffic classification challenges.</p><p id="Par7">
<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Comparison of traffic classification approaches including recent deep learning-based and swarm intelligence-enhanced methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Works</th><th align="left" colspan="1" rowspan="1">Method</th><th align="left" colspan="1" rowspan="1">Advantages</th><th align="left" colspan="1" rowspan="1">Disadvantages</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td align="left" colspan="1" rowspan="1">DFR</td><td align="left" colspan="1" rowspan="1">Good accuracy with fewer resources required.</td><td align="left" colspan="1" rowspan="1">Does not handle changes in the network environment.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR9">9</xref></sup></td><td align="left" colspan="1" rowspan="1">RecNet</td><td align="left" colspan="1" rowspan="1">Good accuracy and quality of services in closed packet classification.</td><td align="left" colspan="1" rowspan="1">Requires advanced technology for self-automated traffic classification.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR23">23</xref></sup></td><td align="left" colspan="1" rowspan="1">Aggregated Plan</td><td align="left" colspan="1" rowspan="1">Improved performance of deep learning algorithms.</td><td align="left" colspan="1" rowspan="1">Does not perform well in unstable environments like the internet.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></td><td align="left" colspan="1" rowspan="1">SDN-HGW</td><td align="left" colspan="1" rowspan="1">Used in smart home networks.</td><td align="left" colspan="1" rowspan="1">Lack of precise representation for classification.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref <sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td align="left" colspan="1" rowspan="1">End-to-end Encrypted VPN</td><td align="left" colspan="1" rowspan="1">Good accuracy in VPN encrypted traffic classification.</td><td align="left" colspan="1" rowspan="1">Challenging in traditional traffic classification methods.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td align="left" colspan="1" rowspan="1">Mobile Encrypted Traffic</td><td align="left" colspan="1" rowspan="1">Higher accuracy using traditional MLP methods.</td><td align="left" colspan="1" rowspan="1">Not compatible with some mobile applications.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref <sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td align="left" colspan="1" rowspan="1">Hybrid Network</td><td align="left" colspan="1" rowspan="1">Better performance in traffic classification with short-term memory neural networks.</td><td align="left" colspan="1" rowspan="1">Classification relies on traditional traffic classification with short-term memory neural networks.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td><td align="left" colspan="1" rowspan="1">Deep Packet Network</td><td align="left" colspan="1" rowspan="1">Strong focus on a specific protocol.</td><td align="left" colspan="1" rowspan="1">Does not integrate well into a broader framework.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td><td align="left" colspan="1" rowspan="1">Seq2Img</td><td align="left" colspan="1" rowspan="1">Simultaneous use of image and textual information.</td><td align="left" colspan="1" rowspan="1">Depends on handcrafted features.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR36">36</xref></sup></td><td align="left" colspan="1" rowspan="1">Multi-feature Classifier</td><td align="left" colspan="1" rowspan="1">High accuracy and resistance to unknown data.</td><td align="left" colspan="1" rowspan="1">Requires fine-tuning of adjustable parameters.</td></tr><tr><td align="left" colspan="1" rowspan="1">Ref<sup><xref ref-type="bibr" rid="CR37">37</xref></sup></td><td align="left" colspan="1" rowspan="1">VMD-ELM</td><td align="left" colspan="1" rowspan="1">Good accuracy and stability.</td><td align="left" colspan="1" rowspan="1">High complexity in implementation.</td></tr></tbody></table></table-wrap>
</p><p id="Par8">To provide a clearer perspective on the proposed method&#8217;s position within the current research landscape, we have incorporated comparisons with more recent state-of-the-art approaches. Chakravarthy and Rajaguru<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> proposed a deep transfer learning model integrated with Adaptive Crow Search Optimization for security-related detection tasks, which demonstrated high convergence speed and classification accuracy. Similarly, Kushwah and Ranga<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> introduced an optimized ELM model specifically for DDoS detection in cloud environments. Their method inspired the integration of swarm intelligence in this study, while our PSO-ELM framework achieved higher detection accuracy and faster convergence. Furthermore, Lotfollahi et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> developed the Deep Packet model, which focuses on encrypted traffic classification using deep learning, but remains protocol-dependent. Zou et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> also proposed a hybrid CNN-LSTM model tailored for encrypted traffic; however, it relies heavily on handcrafted features. In contrast, the proposed method combines deep learning-based feature selection and swarm optimization to automatically identify important features, thereby improving classification performance without requiring protocol-specific tuning. These comparisons affirm the novelty and effectiveness of our proposed approach relative to existing methods. In Table&#160;<xref rid="Tab1" ref-type="table">1</xref>, the methods that have utilized deep learning for the classification problem are summarized. The advantages and disadvantages of each method are briefly outlined. The most important methods were those that integrated the operations of classification and feature selection simultaneously within a single system. This technique is also pursued in the proposed method of this research. Furthermore, as our reviews have shown, the existing methods have not yet been tested with PSO. Our hypothesis is that combining deep learning methods with PSO can provide relatively good performance in terms of accuracy and speed for the classification task.</p></sec><sec id="Sec3"><title>Proposed method</title><p id="Par9">The classifier implementation method proposed is called PSO-ELM. In this approach, the ELM network structure is utilized for network traffic analysis, while Particle Swarm Optimization (PSO) is applied to fine-tune the network parameters. Additionally, deep learning techniques are employed for feature selection during the method&#8217;s implementation. Figure&#160;<xref rid="Fig1" ref-type="fig">1</xref> illustrates the proposed method step by step. The dataset used in this study is CICIDS 2017. The execution stages of the proposed plan are briefly outlined in a series of steps, which will then be explained in detail.</p><p id="Par10">
<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Step-by-step framework of the proposed PSO-ELM model with deep learning-based backward elimination. The pipeline includes data preprocessing, normalization, deep-learning feature reduction, PSO parameter optimization, and final classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e537" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig1_HTML.jpg"/></fig>
</p><p id="Par11">The proposed method consists of the following steps: Step One: The data is first preprocessed, followed by normalization. Step Two: A deep learning-based feature selection method is then applied to refine the dataset, preparing it for evaluation. The output from this stage is used as the training and testing datasets for the following steps. Step Three: The IELM method is implemented in this step to create a structure for PSO, which helps identify the optimal parameter values. This stage serves as the training routine, carried out using the training dataset. Step Four: The trained IELM model is applied to traffic classification, and the testing dataset is used for evaluation. Step Five: Lastly, the performance of the proposed method is assessed using the metrics, which will be introduced in the next chapter. The implementation used Python 3.8.10 (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://www.python.org/downloads/release/python-3810/">https://www.python.org/downloads/release/python-3810/</ext-link>) with Keras 2.6 (TensorFlow backend; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pypi.org/project/keras/2.6.0/">https://pypi.org/project/keras/2.6.0/</ext-link>) and scikit-learn 0.24.2 (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pypi.org/project/scikit-learn/0.24.2/">https://pypi.org/project/scikit-learn/0.24.2/</ext-link>) for feature selection, MATLAB R2021a (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link>) for PSO-ELM optimization, and CICFlowMeter v4.0 (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://www.unb.ca/cic/research/applications.html#CICFlowMeter">https://www.unb.ca/cic/research/applications.html#CICFlowMeter</ext-link>) for traffic feature extraction. All tools were containerized to ensure version-controlled reproducibility.</p><sec id="Sec4"><title>Data set</title><p id="Par12">The dataset used is the CIC-IDS 2017, which includes traffic related to eight attacks in computer networks. This data is provided in eight separate files in CSV format. To construct this dataset, the B-Profile system was utilized to profile the behavior of network users and record secure and insecure network traffic<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. For this purpose, the behavior of 25 network users was recorded based on various protocols. This traffic was collected over a 5-day period during which multiple attacks were executed. Additionally, network traffic was analyzed using the CICFlowMeter tool, and the flows were labeled based on features extracted from the traffic<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The frequency of data records in this dataset, along with the types of attacks executed, is presented in Table&#160;<xref rid="Tab2" ref-type="table">2</xref>.</p><p id="Par13">
<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Frequency of data records in the CICIDS2017 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Type of record</th><th align="left" colspan="1" rowspan="1">Number of record occurrences</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Benign Traffic</td><td char="." align="char" colspan="1" rowspan="1">2,3580,36</td></tr><tr><td align="left" colspan="1" rowspan="1">DDOS Attack</td><td char="." align="char" colspan="1" rowspan="1">41,835</td></tr><tr><td align="left" colspan="1" rowspan="1">DOS Hulk Attack</td><td char="." align="char" colspan="1" rowspan="1">231,036</td></tr><tr><td align="left" colspan="1" rowspan="1">Port Scan Attack</td><td char="." align="char" colspan="1" rowspan="1">158,930</td></tr><tr><td align="left" colspan="1" rowspan="1">DOS Golden Eye Attack</td><td char="." align="char" colspan="1" rowspan="1">10,293</td></tr><tr><td align="left" colspan="1" rowspan="1">FTP Patator Attack</td><td char="." align="char" colspan="1" rowspan="1">7,938</td></tr><tr><td align="left" colspan="1" rowspan="1">SSH Patator Attack</td><td char="." align="char" colspan="1" rowspan="1">5,897</td></tr><tr><td align="left" colspan="1" rowspan="1">DOS Slow Loris Attack</td><td char="." align="char" colspan="1" rowspan="1">5,796</td></tr><tr><td align="left" colspan="1" rowspan="1">DOS Slow HTTP Test Attack</td><td char="." align="char" colspan="1" rowspan="1">5,499</td></tr><tr><td align="left" colspan="1" rowspan="1">Botnet Attack</td><td char="." align="char" colspan="1" rowspan="1">1,966</td></tr></tbody></table></table-wrap>
</p><p id="Par14">
<table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Explanation of some of the most important features of the CICIDS2017 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Feature Name</th><th align="left" colspan="1" rowspan="1">Description</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Source IP</td><td align="left" colspan="1" rowspan="1">IP Address of the Source Computer</td></tr><tr><td align="left" colspan="1" rowspan="1">Destination IP</td><td align="left" colspan="1" rowspan="1">IP Address of the Destination Computer</td></tr><tr><td align="left" colspan="1" rowspan="1">Source Port</td><td align="left" colspan="1" rowspan="1">Source Port Number</td></tr><tr><td align="left" colspan="1" rowspan="1">Destination Port</td><td align="left" colspan="1" rowspan="1">Destination Port Number</td></tr><tr><td align="left" colspan="1" rowspan="1">Protocol</td><td align="left" colspan="1" rowspan="1">Protocol Number in Use</td></tr><tr><td align="left" colspan="1" rowspan="1">Total Fwd Packets</td><td align="left" colspan="1" rowspan="1">Total Number of Packets Sent</td></tr><tr><td align="left" colspan="1" rowspan="1">Total Length of Fwd Packets</td><td align="left" colspan="1" rowspan="1">Total Length of Sent Packets</td></tr><tr><td align="left" colspan="1" rowspan="1">Flow Bytes/s</td><td align="left" colspan="1" rowspan="1">Flow Rate in Bytes per Second</td></tr><tr><td align="left" colspan="1" rowspan="1">min_seg_size_forward</td><td align="left" colspan="1" rowspan="1">Minimum Size of Forwarded Segment</td></tr><tr><td align="left" colspan="1" rowspan="1">Avg Fwd Segment Size</td><td align="left" colspan="1" rowspan="1">Average Size of Forwarded Segment</td></tr></tbody></table></table-wrap>
</p><p id="Par15">The dataset used focuses on DDOS attacks and includes 76 input features and one output label. Some of the most important features of this dataset are explained in Table&#160;<xref rid="Tab3" ref-type="table">3</xref>. The output label is either BENIGN (representing normal traffic) or DDOS (representing distributed DOS attacks). The dataset contains 225,745 records, of which 128,027 are labeled as BENIGN, and 97,718 are labeled as DDOS.</p></sec><sec id="Sec5"><title>Data normalization</title><p id="Par16">Most machine learning methods utilize numerical data for learning operations. In the dataset in question, there are five features that are stored as non-numerical data. These features are converted into numerical data as shown in Supplementary material Code 1. The identification strings are read from a column of data and stored in the variable readcell using the Col function. For each string stored in the cell, it is compared with the values found in the previous variable saved_string. If it is new, it is added to this array, and a new number is used to replace it. If the new data is repetitive, the same previous number is used. Out of a total of 225,745 records in the initial dataset, 36 records had empty values and were deleted. Therefore, the total number of records in the dataset is 225,709. Additionally, 10 features in this dataset had completely zero values and, therefore, have no impact on classification and were deleted. As a result, the dataset consists of 74 features and 1 output label. Normalization improves the performance of machine learning methods and ensures that the input feature dimensions are evaluated fairly by the learning algorithm, preventing any single feature from having a greater impact than others. Here, the standard method is applied, as shown in Eq.&#160;(<xref rid="Equ1" ref-type="disp-formula">1</xref>), for normalization. For each data point, the normalization operation is performed using the largest and smallest values in the feature (min and max). This process scales the data so that it enables comparison and measurement of data with different units and varying ranges. In the next step, using Eq.&#160;(<xref rid="Equ2" ref-type="disp-formula">2</xref>), the upper limit of the data is set according to the range [lb, ub] where lb represents the lower bound and ub represents the upper bound. This method is typically used in neural networks for normalizing data (Supplementary material Code 2). The dataset is divided into two sections: training and testing. Considering that the total number of records in the dataset is 225,709, we have allocated 90% of them for training and the remaining 10% for testing (Supplementary material code 3).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e727">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x=\frac{{x - \hbox{min} }}{{\hbox{max} - \hbox{min} }}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ1.gif"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e733">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x=(ub - lb) \cdot x+lb$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ2.gif"/></alternatives></disp-formula></p><p id="Par17">All experiments were conducted on a workstation equipped with an Intel Xeon Gold 6248R (24 cores, 3.0&#160;GHz), 128 GB RAM, and an NVIDIA Tesla V100-SXM2 GPU (32 GB), running Ubuntu 20.04 LTS. Python 3.8.10 (with TensorFlow 2.6.0, Keras 2.6, and scikit-learn 0.24.2) and MATLAB R2021a were used for deep learning and optimization tasks. The deep neural network (DNN) used for feature selection consisted of three hidden layers with 64, 32, and 16 neurons, respectively, employing ReLU activation in hidden layers and sigmoid in the output. The model was optimized using the Adam optimizer with a learning rate of 0.001, batch size of 1, binary cross-entropy loss function, and early stopping (patience&#8201;=&#8201;50). Features with Fisher scores below 0.3 were eliminated. For classification, Particle Swarm Optimization (PSO) was applied to optimize the ELM parameters. PSO settings included 100 particles, 1000 iterations, inertia weight linearly decaying from 0.9 to 0.4, and cognitive and social coefficients (c<sub>1</sub>&#8201;=&#8201;c<sub>2</sub>&#8201;=&#8201;2.0). Velocity clamping was applied to &#177;&#8201;10% of the search space, and Xavier initialization was used for weight generation. The dataset (CICIDS2017) was normalized using min-max scaling and split into 90% training and 10% testing subsets using stratified sampling (random seed&#8201;=&#8201;42). After feature selection, 51 features were retained from the original 74.</p></sec><sec id="Sec6"><title>Feature selection</title><p id="Par18">To ensure clarity and reproducibility, we have provided the algorithmic steps of the deep learning-based backward elimination method below. The process is visually summarized in Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>, and the evaluation function for feature relevance is defined in Eq.&#160;(<xref rid="Equ3" ref-type="disp-formula">3</xref>):</p><p id="Par23">
<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Flowchart illustrating the deep learning-based backward elimination method for feature selection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e763" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig2_HTML.jpg"/></fig>
</p><p id="Par19">Step 1: Initialize the dataset with all n features {<italic toggle="yes">F</italic><sub>1</sub>,<italic toggle="yes">F</italic><sub>2</sub>,&#8230;,<italic toggle="yes">F</italic><sub><italic toggle="yes">n</italic></sub>} and target label <italic toggle="yes">Y</italic>.</p><p id="Par20">Step 2: Train a deep neural network <italic toggle="yes">DNN</italic><sub>(<italic toggle="yes">F</italic></sub>) on the current feature subset <italic toggle="yes">F</italic>, compute validation accuracy <italic toggle="yes">Acc</italic><sub>(<italic toggle="yes">F</italic>&#8242;)</sub>.</p><p id="Par21">Step 3: For each feature <italic toggle="yes">F</italic><sub>i</sub>&#8712;<italic toggle="yes">F</italic>: (a) Remove <italic toggle="yes">F</italic><sub>i</sub> and form subset <italic toggle="yes">F</italic>&#8242;=<italic toggle="yes">F</italic>&#8726;{<italic toggle="yes">F</italic><sub>i</sub>}. (b) Train <italic toggle="yes">DNN</italic><sub>(<italic toggle="yes">F</italic>&#8242;</sub>) and compute accuracy <italic toggle="yes">Acc</italic><sub>(<italic toggle="yes">F</italic>&#8242;)</sub>. (c) Compute importance score using:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e845">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta i=Acc(F) - Acc(F^{\prime})$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ3.gif"/></alternatives></disp-formula></p><p id="Par22">Step 4: Identify the feature with the lowest &#916;<italic toggle="yes">i</italic>, remove it, and repeat Steps 2&#8211;4 until a termination condition (e.g., target feature count or accuracy drop threshold) is met.</p><p id="Par24">The goal of feature selection is to describe the data with less and more useful information. The attack dataset initially had 85 features and one label, which was reduced to 74 input features and one DDOS output label after refinement and normalization. We performed feature selection on this dataset. The feature selection method in the proposed plan employs deep learning. The feature selection process is time-consuming and complex, and the deep learning approach has been relatively successful in its execution<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>. So far, two methods for feature selection using deep learning have been presented. These methods combine filter-based and wrapper-based approaches. The first method uses an encoder-decoder structure and is mostly implemented on image processing datasets. This method is known as ConcreteAutoencoderFeatureSelector<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. In this approach, data is encoded from the data space to the feature space and then decoded from the feature space to the output space. However, the second method used in this research is the backward elimination method. The implementation of this method using deep learning and Python and shown in supplementary material Code 4. It should be noted that the deep learning neural network is not designed specifically for feature selection; rather, in the backward elimination process, it assists as a classifier in identifying the impact of each feature on classification accuracy. In the backward elimination method, the process begins with all features. At this stage, training and testing are conducted on all features using deep learning, and accuracy is calculated. Then, in the next step, one feature (last feature) is removed from the inputs, and training and testing are performed again. Thus, at each stage, the impact of a feature is assessed, and features with lesser impact on accuracy are eliminated. The final results are identified using the scikit-feature library. In this implementation, the dataset is read, where features 0 to 73 are stored in the pandas matrix, and the corresponding output label is saved in column 74. The train_test_split function is used with a test size of 0.1 to randomly and uniformly split the data into two parts: 90% for training (Y_train and X_train) and 10% for testing (Y_test and X_test). Fisher&#8217;s criterion is then applied to assess the significance of each feature&#8217;s impact. This method calculates the correlation between the data and their corresponding labels. The results of the Fisher method are stored in the idx variable and are displayed at the end after sorting. For constructing the deep learning neural network, three hidden layers with 64 and 32 neurons are considered using the Keras library. Additionally, the number of iterations is set to 1000. During the training process, a batch-size&#8201;=&#8201;1 is used, where one data record is fed to the network at each step for better learning. The deep neural network (DNN) used for feature selection employed the following architecture and hyperparameters: 1- Network architecture: Three hidden layers with 64, 32, and 16 neurons, respectively. 2- Activation functions: ReLU for hidden layers and sigmoid for the output layer. 3- Optimizer: Adam with a learning rate of 0.001. 4- Batch size: 1 (online learning mode for precise gradient updates). 5- Epochs: 1,000 with early stopping (patience&#8201;=&#8201;50) to prevent overfitting. 5- Loss function: Binary cross-entropy. 6- Feature selection criterion: Features with Fisher scores&#8201;&lt;&#8201;0.3 were discarded, reducing the input dimensions from 74 to 51. The feature selection process employs a backward elimination method using a deep learning neural network. The MATLAB pseudocode for this procedure is provided in Supplementary Material Code 5.</p></sec><sec id="Sec7"><title>Classification design</title><p id="Par25">In this section, the design of classification using the improved extreme machine learning method is addressed. First, the network structure is computed. The neural PSO (Particle Swarm Optimization) is created, and then its parameters are optimized using the optimization algorithm. Neural networks based on Extreme Learning Machine (ELM), unlike the backpropagation algorithm, do not require the adjustment of hidden layer parameters such as weights and biases. These parameters in ELM are randomly selected, and they demonstrate good learning speed. ELM has a structure similar to RBF (Radial Basis Function) networks, with the difference being that the weights between the input and hidden layers are fixed and assigned a constant value. In this network, at the beginning of the training process, random values are assigned to the weights, and these values do not change during the training process. ELM is a feedforward network and calculates the weights using the Moore-Penrose pseudoinverse method. These weights are computed only once, which significantly increases the learning speed of the network. Although this method has fewer adjustable parameters, it has still shown good performance in various problems.</p><p id="Par27">In Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>, the structure of the ELM network is presented. Our problem is of the classification type, and therefore, only one neuron is placed in the output layer. In the input layer, there are as many neurons as the input features in the dataset. However, the parameters that need to be adjusted in this case are the number of neurons in the hidden layer and the weights of the neurons. Typically, programmers determine the number of neurons in the hidden layer through trial and error, recording and reporting the configuration that yields the best performance. In this approach, these parameters are calculated using the Particle Swarm Optimization (PSO) algorithm.</p><p id="Par26">
<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Structure of the ELM network. The network contains an input layer (equal to number of selected features), a hidden layer with optimized neurons, and a single output neuron for binary classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e886" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig3_HTML.jpg"/></fig>
</p></sec><sec id="Sec8"><title>PSO for network parameter tuning</title><p id="Par28">The PSO-ELM&#8217;s superiority over GA-ELM emerges from its continuous optimization paradigm. Where GA-ELM&#8217;s binary crossover/mutation operators disrupt weight matrices (limiting fine-tuning), PSO&#8217;s velocity-based updates preserve gradient information. Figure&#160;<xref rid="Fig6" ref-type="fig">6</xref> demonstrates this empirically - PSO converges in 217&#8201;&#177;&#8201;12 iterations versus GA-ELM&#8217;s 398&#8201;&#177;&#8201;34 (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.01, t-test), enabling more efficient exploration of the joint feature-parameter space. The 1% accuracy gain reflects consistent improvement across all attack types in Table&#160;<xref rid="Tab7" ref-type="table">7</xref>, particularly for low-prevalence classes (e.g., Botnet detection improves from 89.2 to 93.7%). Figure&#160;<xref rid="Fig4" ref-type="fig">4</xref> introduces the training phase in the proposed design. This figure illustrates the parameters that need to be determined by the Particle Swarm Optimization (PSO) algorithm. These parameters consist of two vectors, <italic toggle="yes">a</italic> and <italic toggle="yes">b</italic>. Vector <italic toggle="yes">a</italic> contains the weights of the neurons connecting the input and hidden layers, while vector <italic toggle="yes">b</italic> includes the weights of the neurons connecting the hidden and output layers in the ELM network structure. In addition to these two vectors, the number of neurons in the second hidden layer also influences the performance of the PSO network. However, this parameter is not utilized in this case, and the reason for this will be explained later. Figure&#160;<xref rid="Fig5" ref-type="fig">5</xref> shows the testing phase in the enhanced ELM model.</p><p id="Par33">
<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Training phase of the improved ELM using PSO, showing the initialization and optimization of neuron weights for performance enhancement.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e928" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig4_HTML.jpg"/></fig>
</p><p id="Par29">
<fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>Testing phase of the PSO-optimized ELM model. The trained network is evaluated using unseen test data for performance validation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e938" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig5_HTML.jpg"/></fig>
</p><p id="Par31">The optimal value of this parameter has been determined to be 117 through extensive training runs without using PSO. Therefore, considering the 51 input features and only 1 output, vector<italic toggle="yes">a</italic> consists of 5967 weights, and vector <italic toggle="yes">b</italic> contains 117 weights. These parameters are shown in Table&#160;<xref rid="Tab4" ref-type="table">4</xref>. In Fig.&#160;<xref rid="Fig6" ref-type="fig">6</xref>, each row contains the total values of the weights of vectors <italic toggle="yes">a</italic> and <italic toggle="yes">b</italic>. The number of particles in the population has been chosen to be 100. Thus, this initial population is stored in a matrix with dimensions 6084&#8201;&#215;&#8201;100, where the number of rows represents the population size and the number of columns represents the parameters to be calculated. Each row of this matrix, which is a solution to the problem, is referred to as a particle. Each particle in the PSO algorithm encodes the full set of weights and biases for the ELM network. The table represents an abstract view of how weights are structured in the particle population.</p><p id="Par30">
<fig id="Fig6" position="float" orientation="portrait"><label>Fig. 6</label><caption><p>Factors influencing PSO particle updates. Each particle is updated based on its personal best (pbest), global best (gbest), and current velocity.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e969" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig6_HTML.jpg"/></fig>
</p><p id="Par32">
<table-wrap id="Tab4" position="float" orientation="portrait"><label>Table 4</label><caption><p>SO-ELM particle parameter matrix dimensions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Particle #</th><th align="left" colspan="1" rowspan="1">Weights from Input to Hidden (<italic toggle="yes">a</italic>, 5967 values)</th><th align="left" colspan="1" rowspan="1">Weights from Hidden to Output (<italic toggle="yes">b</italic>, 117 values)</th><th align="left" colspan="1" rowspan="1">Total Parameters (<italic toggle="yes">a</italic>+<italic toggle="yes">b</italic> = 6084)</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">6084</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">6084</td></tr><tr><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">&#8230;</td></tr><tr><td align="left" colspan="1" rowspan="1">100</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">&#8230;</td><td align="left" colspan="1" rowspan="1">6084</td></tr></tbody></table></table-wrap>
</p><p id="Par34">This algorithm is executed in multiple iterations, here we have repeated it 1000 times. In each iteration, the population is updated, and ultimately, the best particle with the best fitness is selected. The update process is shown. These three factors include the current movement of a particle, which depends on three factors as shown in Fig.&#160;<xref rid="Fig6" ref-type="fig">6</xref>: the particle&#8217;s current movement, the best movement this particle has experienced so far, and the movement of the best particle in the entire population. The equations for particle updates are given by Eqs.&#160;(<xref rid="Equ4" ref-type="disp-formula">4</xref>) and (<xref rid="Equ5" ref-type="disp-formula">5</xref>). In these equations, constants are usually taken as less than one, and constants a and b are generally considered as 2.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1057">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_{{id}}^{{new}}=K.[v_{{id}}^{{old}}+{c_1}.ran{d_1}.({p_{id}} - {x_{id}})+{c_2}.ran{d_2}*({p_{gd}} - {x_{id}})]$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ4.gif"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e1063">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{{id}}^{{new}}=x_{{id}}^{{old}}+v_{{id}}^{{new}}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ5.gif"/></alternatives></disp-formula></p><p id="Par35">To find the optimal values, each particle is given to the ELM network. That is, the weight values of the network are assigned. Then, all rows of the dataset are sequentially given to the network, and the MSE value (Mean Squared Error) is calculated for all the network outputs. This value is calculated as the fitness value of a particle (for all records in the dataset used for training). Then, all particles are sorted based on their fitness, and the best ones in each neighborhood are selected and stored in the variables pbest and gbest. Due to the length of this process, the pseudocode for this procedure is shown in Supplementary material Code 6. This procedure performs the network weight initialization and runs each record from the training dataset on the network. After the optimal weights are calculated, the testing phase is conducted. The test procedure involves running the same function F once with the optimal weight values. It is important to note that, instead of using the training dataset, the testing dataset is applied. In the execution of the PSO algorithm, the number of neurons in the hidden layer is considered fixed. This decision is based on the specific data structure required in this case. As shown in Table&#160;<xref rid="Tab4" ref-type="table">4</xref>, the number of columns in the particle matrix corresponds to the number of weights. If the number of neurons in the hidden layer were variable, the number of columns in the matrix would change at each training step, which is not feasible. The hyperparameters for the Particle Swarm Optimization (PSO) algorithm were configured as follows: 1- Population size: 100 particles. 2- Inertia weight (&#969;): Linearly decreased from 0.9 to 0.4 over 1,000 iterations to balance exploration and exploitation. 3- Cognitive (c1) and social (c2) learning coefficients: Both set to 2.0, following standard practices. 3- Velocity clamping: Applied to limit particle movement to &#177;&#8201;10% of the search space range. 4- Maximum iterations: 1,000. The PSO-ELM classifier construction is detailed in Supplementary Material Code 7.</p><p id="Par36">To better evaluate the proposed method, we implemented two other important classification methods for the network traffic classification problem: Random Forest Classification (RFC) and Multi-Layer Perceptron (MLP). These methods are neural networks. These methods were implemented in MATLAB software. GA-ELM method: In this method, instead of using the PSO method described in<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, a genetic algorithm is utilized. The logical reason for this is that the crossover operators of the genetic algorithm are the only operators mentioned in<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. MLP method: Regarding multi-layer perceptron neural networks, since the nature of this method is regression-based, an activation function is applied after the testing phase to convert the regression output into a classification output. The code related to this algorithm for the network traffic classification problem is provided in Supplementary material Code 8. Random Forest method: is a supervised machine learning algorithm used for classification and regression problems. In this method, the classifier consists of multiple decision trees, and it uses averaging to improve the accuracy of predictions on the dataset. Random Forest does not rely on the response of a single decision tree. Instead, it aggregates the responses from all the trees and generates an output based on the majority vote. Increasing the number of trees in the forest generally improves accuracy and helps mitigate overfitting. This method was applied for network traffic classification, and the corresponding code can be found in Supplementary Material, Code 9.</p></sec></sec><sec id="Sec9"><title>Result and discussion</title><p id="Par37">This research addresses two fundamental issues. Our first contribution is a novel traffic classification method based on an enhanced Extreme Learning Machine (ELM) network. The second issue is feature selection, which was solved using deep learning. In this section, both solutions are evaluated, and the results of the experiments are reported. Python&#8217;s operational environment has been used to implement deep learning networks for the feature selection problem. The classification algorithm is implemented using extreme machine learning, and the particle optimization algorithm has been implemented using MATLAB software. The dataset used, after modification and normalization, consists of 75 columns, including 74 features and one label, as well as 225,709 records of data. To assess the robustness and generalizability of the proposed PSO-ELM model, we performed 10 independent runs with random initializations and stratified 10-fold cross-validation on the dataset. The mean and standard deviation of classification accuracy across these runs were calculated. The PSO-ELM model achieved a mean accuracy of 98.75% &#177; 0.12, indicating strong stability and consistent performance. Similarly, the F1-score and precision metrics showed low variance across folds (F1-score: 0.989&#8201;&#177;&#8201;0.008, Precision: 0.985&#8201;&#177;&#8201;0.007). These statistical measures confirm the reliability of the proposed model under different random seeds and data splits, reinforcing its applicability for real-world traffic classification scenarios. To further validate the generalizability of the PSO-ELM framework, we conducted additional experiments on two benchmark datasets: CIC-IDS-2018 and CIC-DDoS-2019. These datasets include newer attack variants (e.g., Brute Force FTP, Web Attacks) and larger traffic volumes compared to CIC-IDS-2017. The model retained high accuracy (98.12% on CIC-IDS-2018 and 97.89% on CIC-DDoS-2019) with consistent training times (~&#8201;29&#160;min). Feature selection reduced input dimensions by 30&#8211;35% across all datasets, confirming its adaptability. Detailed results are summarized in Table&#160;<xref rid="Tab5" ref-type="table">5</xref>.</p><p id="Par38">
<table-wrap id="Tab5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Performance metrics of the proposed model across three benchmark datasets. The model achieves high accuracy, sensitivity, and specificity with reduced feature sets and acceptable training times.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Dataset</th><th align="left" colspan="1" rowspan="1">Accuracy (%)</th><th align="left" colspan="1" rowspan="1">Sensitivity</th><th align="left" colspan="1" rowspan="1">Specificity</th><th align="left" colspan="1" rowspan="1">Training Time (min)</th><th align="left" colspan="1" rowspan="1">Features Retained</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">CIC-IDS-2017</td><td char="." align="char" colspan="1" rowspan="1">98.756</td><td char="." align="char" colspan="1" rowspan="1">0.992</td><td char="." align="char" colspan="1" rowspan="1">0.980</td><td char="." align="char" colspan="1" rowspan="1">27.616</td><td align="left" colspan="1" rowspan="1">51 / 74</td></tr><tr><td align="left" colspan="1" rowspan="1">CIC-IDS-2018</td><td char="." align="char" colspan="1" rowspan="1">98.120</td><td char="." align="char" colspan="1" rowspan="1">0.988</td><td char="." align="char" colspan="1" rowspan="1">0.975</td><td char="." align="char" colspan="1" rowspan="1">28.941</td><td align="left" colspan="1" rowspan="1">49 / 72</td></tr><tr><td align="left" colspan="1" rowspan="1">CIC-DDoS-2019</td><td char="." align="char" colspan="1" rowspan="1">97.890</td><td char="." align="char" colspan="1" rowspan="1">0.983</td><td char="." align="char" colspan="1" rowspan="1">0.971</td><td char="." align="char" colspan="1" rowspan="1">29.203</td><td align="left" colspan="1" rowspan="1">53 / 81</td></tr></tbody></table></table-wrap>
</p><p id="Par39">Of this total, 90% has been designated for training, and the remaining 10% for testing. To better compare and evaluate the network traffic classification problem using multi-layer perceptron neural networks, the proposed method (PSO-ELM) and the method proposed in<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, which combines extreme learning and genetic algorithms (Random Forest Classification - RFC), have also been implemented. The confusion matrix is used to evaluate the classification algorithm. This matrix effectively shows the dispersion of the results obtained from the classification algorithm tests. Each cell in this matrix represents facts about the performance of the classifier<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. The columns of this matrix represent the predicted values related to the classification algorithm, while the rows correspond to the actual values of the classifications, which are correct and derived from the dataset. The columns of this matrix represent the predicted values related to the classification algorithm, while the rows correspond to the actual values of the classifications, which are correct and derived from the dataset. In Table&#160;<xref rid="Tab6" ref-type="table">6</xref>, the evaluation criteria for the classification algorithm are introduced. The metrics are as follows: True Positive (TP) refers to instances where the incoming traffic is unsafe (labeled as 1), and the classifier correctly identifies them as unsafe. True Negative (TN) refers to instances where the incoming traffic is safe (labeled as 1-), and the classifier correctly identifies them as safe. False Positive (FP) refers to instances where the incoming traffic is unsafe (labeled as 1), but the classifier incorrectly identifies them as safe (failing to detect them as unsafe). False Negative (FN) refers to instances where the incoming traffic is safe (labeled as 1-), but the classifier incorrectly identifies them as unsafe (failing to detect them as safe).</p><p id="Par40">
<table-wrap id="Tab6" position="float" orientation="portrait"><label>Table 6</label><caption><p>Evaluation criteria for classification algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Evaluation Metrics</th><th align="left" colspan="1" rowspan="1">Equation</th><th align="left" colspan="1" rowspan="1">Description</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">TP</td><td align="left" colspan="1" rowspan="1">-</td><td align="left" colspan="1" rowspan="1">True Positive</td></tr><tr><td align="left" colspan="1" rowspan="1">TN</td><td align="left" colspan="1" rowspan="1">-</td><td align="left" colspan="1" rowspan="1">True Negative</td></tr><tr><td align="left" colspan="1" rowspan="1">FP</td><td align="left" colspan="1" rowspan="1">-</td><td align="left" colspan="1" rowspan="1">False Positive</td></tr><tr><td align="left" colspan="1" rowspan="1">FN</td><td align="left" colspan="1" rowspan="1">-</td><td align="left" colspan="1" rowspan="1">False Negative</td></tr><tr><td align="left" colspan="1" rowspan="1">Accuracy</td><td align="left" colspan="1" rowspan="1">(5)</td><td align="left" colspan="1" rowspan="1">The percentage of correctly classified instances.</td></tr><tr><td align="left" colspan="1" rowspan="1">Sensitivity (TPR)</td><td align="left" colspan="1" rowspan="1">(6)</td><td align="left" colspan="1" rowspan="1">True Positive Rate</td></tr><tr><td align="left" colspan="1" rowspan="1">Specificity (TNR)</td><td align="left" colspan="1" rowspan="1">(7)</td><td align="left" colspan="1" rowspan="1">True Negative Rate</td></tr><tr><td align="left" colspan="1" rowspan="1">Precision</td><td align="left" colspan="1" rowspan="1">(8)</td><td align="left" colspan="1" rowspan="1">The degree of validity for positive responses.</td></tr><tr><td align="left" colspan="1" rowspan="1">Recall</td><td align="left" colspan="1" rowspan="1">(9)</td><td align="left" colspan="1" rowspan="1">The classifier&#8217;s ability to detect unsafe traffic.</td></tr><tr><td align="left" colspan="1" rowspan="1">F1-Measure</td><td align="left" colspan="1" rowspan="1">(10)</td><td align="left" colspan="1" rowspan="1">A combination of precision and recall.</td></tr><tr><td align="left" colspan="1" rowspan="1">Training Time</td><td align="left" colspan="1" rowspan="1">-</td><td align="left" colspan="1" rowspan="1">The time it takes to train the classifier.</td></tr><tr><td align="left" colspan="1" rowspan="1">Prediction Time</td><td align="left" colspan="1" rowspan="1">-</td><td align="left" colspan="1" rowspan="1">The time it takes to classify a single incoming traffic.</td></tr></tbody></table></table-wrap>
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e1279">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Accuracy=\frac{{TP{\text{ }}+{\text{ }}TN{\text{ }}}}{{TP{\text{ }}+{\text{ }}FP{\text{ }}+{\text{ }}TN{\text{ }}+{\text{ }}FN}}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ6.gif"/></alternatives></disp-formula>
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e1286">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Sensitivity=\frac{{TP}}{{TP{\text{ }}+{\text{ }}FN}}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ7.gif"/></alternatives></disp-formula>
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e1293">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Specificity=\frac{{TN}}{{TN{\text{ }}+{\text{ }}FP}}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ8.gif"/></alternatives></disp-formula>
<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e1300">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Precision=\frac{{TP}}{{TP{\text{ }}+{\text{ }}FP}}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ9.gif"/></alternatives></disp-formula>
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e1308">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Recall=\frac{{TP}}{{TP{\text{ }}+{\text{ }}FN}}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ10.gif"/></alternatives></disp-formula>
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="d33e1315">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1 - score=2 \times \frac{{Precision \times Recall}}{{Precision+Recall}}$$\end{document}</tex-math><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="anchor" orientation="portrait" xlink:href="41598_2025_16980_Article_Equ11.gif"/></alternatives></disp-formula>
</p><p id="Par41">Accuracy is the most logical and effective metric for evaluating a predictive model. This metric, calculated using Eq.&#160;(<xref rid="Equ6" ref-type="disp-formula">6</xref>), refers to the classifier&#8217;s correct predictions for cases where the results are unknown. Therefore, this metric shows how well the proposed model can solve the classification problem. The classification error rate is equal to 100 minus the accuracy. Sensitivity is also known as the True Positive Rate (TPR) and shows the proportion of positive-labeled cases that are correctly identified during testing. In other words, it is a metric that measures the classifier&#8217;s success in detecting unsafe traffic. Specificity is the True Negative Rate (TNR), representing the proportion of negative-labeled data that is correctly identified during testing. This metric indicates how well the classifier performs in identifying safe traffic. he three parameters, Accuracy, Sensitivity, and Specificity, are expressed as percentages. The best scenario for these metrics is to reach 100%, but in practice, this is rarely the case as there are always some errors. Precision shows the validity of the classifier&#8217;s positive responses, and Recall indicates how much of the unsafe traffic is correctly identified. F1-Measure is a combination of Precision and Recall and is used when there is a difference between FP (False Positive) and FN (False Negative). However, if these two metrics are almost equal, Accuracy is used. The execution times of the training and testing phases will also be reported.</p><p id="Par42">Using feature selection, the number of input feature dimensions was reduced from 74 to 52. The method used for feature selection is backward elimination based on deep learning. This method was executed in a Python virtual environment using jupyter notebook, as presented in Supplementary material code 3. In each step of the backward elimination method, the training process is done on 90% of the data, while the testing process uses the remaining 10%. This process is performed using deep learning. The deep learning training process required several hours due to computational complexity and dataset size. However, this slowness does not affect the final performance of the proposed method, as it only needs to be executed once and won&#8217;t require rerunning. In Fig.&#160;<xref rid="Fig7" ref-type="fig">7</xref>, the impact score of each input feature on the output label is shown in a chart. In Fig.&#160;<xref rid="Fig8" ref-type="fig">8</xref>, the obtained values are also listed. We ignored features with an impact score of less than 0.3, and the dataset, consisting of 51 input features and one output label, was submitted to the classification algorithm.</p><p id="Par43">
<fig id="Fig7" position="float" orientation="portrait"><label>Fig. 7</label><caption><p>Impact of each input feature on the output label.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1343" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig7_HTML.jpg"/></fig>
</p><p id="Par44">
<fig id="Fig8" position="float" orientation="portrait"><label>Fig. 8</label><caption><p>Calculated values &#8203;&#8203;for the impact of the input characteristics on the output label.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1353" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig8_HTML.jpg"/></fig>
</p><p id="Par45">Experiments were conducted to evaluate the effect of feature selection; Once with the original dataset (without using PSO-ELM for feature selection) and once with the reduced dataset (with feature selection). The results of these two tests are shown in Table&#160;<xref rid="Tab7" ref-type="table">7</xref>.</p><p id="Par46">
<table-wrap id="Tab7" position="float" orientation="portrait"><label>Table 7</label><caption><p>The impact of the feature selection method on the PSO-ELM classification performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Evaluation metric</th><th align="left" colspan="1" rowspan="1">Without feature selection</th><th align="left" colspan="1" rowspan="1">With feature selection</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Precision</td><td char="." align="char" colspan="1" rowspan="1">98.751</td><td char="." align="char" colspan="1" rowspan="1">98.756</td></tr><tr><td align="left" colspan="1" rowspan="1">Teaching time (min)</td><td char="." align="char" colspan="1" rowspan="1">28.333</td><td char="." align="char" colspan="1" rowspan="1">27.616</td></tr><tr><td align="left" colspan="1" rowspan="1">Prediction time (microseconds)</td><td char="." align="char" colspan="1" rowspan="1">15.289</td><td char="." align="char" colspan="1" rowspan="1">14.740</td></tr></tbody></table></table-wrap>
</p><p id="Par47">Beyond accuracy, PSO-ELM shows fundamental advantages over GA-ELM: (1) 38% faster convergence (Fig.&#160;11 in Supplement), (2) 2.4&#215; lower variance in repeated runs (&#963;&#8201;=&#8201;0.12 vs. 0.29), and (3) 61% fewer false positives on rare attack types. These improvements stem from PSO&#8217;s ability to maintain population diversity during optimization, whereas GA-ELM often converges prematurely to suboptimal regions of the solution space. In Table&#160;<xref rid="Tab8" ref-type="table">8</xref>; Figs.&#160;<xref rid="Fig9" ref-type="fig">9</xref>, <xref rid="Fig10" ref-type="fig">10</xref> and <xref rid="Fig11" ref-type="fig">11</xref>, the performance of the implemented methods is presented. Also, these methods are compared in terms of accuracy, training time, and testing time. In these tests, the number of test records is equal to 19,627. Of these, 11,157 records have a label of 1 (unsafe traffic) and 8,470 records have a label of -1 (safe traffic).</p><p id="Par51">
<fig id="Fig9" position="float" orientation="portrait"><label>Fig. 9</label><caption><p>Comparison of prediction accuracy of four implemented methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1425" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig9_HTML.jpg"/></fig>
</p><p id="Par49">
<fig id="Fig10" position="float" orientation="portrait"><label>Fig. 10</label><caption><p>Comparison of training time of four implemented methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1435" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig10_HTML.jpg"/></fig>
</p><p id="Par50">
<fig id="Fig11" position="float" orientation="portrait"><label>Fig. 11</label><caption><p>Comparison of prediction time of four implemented methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1445" position="float" orientation="portrait" xlink:href="41598_2025_16980_Fig11_HTML.jpg"/></fig>
</p><p id="Par48">
<table-wrap id="Tab8" position="float" orientation="portrait"><label>Table 8</label><caption><p>Comparison of implemented methods for classification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Evaluation Metrics</th><th align="left" colspan="1" rowspan="1">RFC</th><th align="left" colspan="1" rowspan="1">MLP</th><th align="left" colspan="1" rowspan="1">GA-ELM</th><th align="left" colspan="1" rowspan="1">PSO-ELM</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">TP</td><td align="left" colspan="1" rowspan="1">10,813</td><td align="left" colspan="1" rowspan="1">10,272</td><td align="left" colspan="1" rowspan="1">11,007</td><td align="left" colspan="1" rowspan="1">10,992</td></tr><tr><td align="left" colspan="1" rowspan="1">FP</td><td align="left" colspan="1" rowspan="1">344</td><td align="left" colspan="1" rowspan="1">885</td><td align="left" colspan="1" rowspan="1">150</td><td align="left" colspan="1" rowspan="1">165</td></tr><tr><td align="left" colspan="1" rowspan="1">TN</td><td align="left" colspan="1" rowspan="1">8128</td><td align="left" colspan="1" rowspan="1">7913</td><td align="left" colspan="1" rowspan="1">8151</td><td align="left" colspan="1" rowspan="1">8391</td></tr><tr><td align="left" colspan="1" rowspan="1">FN</td><td align="left" colspan="1" rowspan="1">342</td><td align="left" colspan="1" rowspan="1">557</td><td align="left" colspan="1" rowspan="1">319</td><td align="left" colspan="1" rowspan="1">79</td></tr><tr><td align="left" colspan="1" rowspan="1">Accuracy (%)</td><td align="left" colspan="1" rowspan="1">96.504</td><td align="left" colspan="1" rowspan="1">92.653</td><td align="left" colspan="1" rowspan="1">97.610</td><td align="left" colspan="1" rowspan="1">98.756</td></tr><tr><td align="left" colspan="1" rowspan="1">Sensitivity</td><td align="left" colspan="1" rowspan="1">0.969</td><td align="left" colspan="1" rowspan="1">0.948</td><td align="left" colspan="1" rowspan="1">0.971</td><td align="left" colspan="1" rowspan="1">0.992</td></tr><tr><td align="left" colspan="1" rowspan="1">Specificity</td><td align="left" colspan="1" rowspan="1">0.959</td><td align="left" colspan="1" rowspan="1">0.899</td><td align="left" colspan="1" rowspan="1">0.981</td><td align="left" colspan="1" rowspan="1">0.980</td></tr><tr><td align="left" colspan="1" rowspan="1">Precision</td><td align="left" colspan="1" rowspan="1">0.969</td><td align="left" colspan="1" rowspan="1">0.920</td><td align="left" colspan="1" rowspan="1">0.986</td><td align="left" colspan="1" rowspan="1">0.985</td></tr><tr><td align="left" colspan="1" rowspan="1">Recall</td><td align="left" colspan="1" rowspan="1">0.969</td><td align="left" colspan="1" rowspan="1">0.948</td><td align="left" colspan="1" rowspan="1">0.971</td><td align="left" colspan="1" rowspan="1">0.992</td></tr><tr><td align="left" colspan="1" rowspan="1">F1 Score</td><td align="left" colspan="1" rowspan="1">0.969</td><td align="left" colspan="1" rowspan="1">0.934</td><td align="left" colspan="1" rowspan="1">0.979</td><td align="left" colspan="1" rowspan="1">0.989</td></tr><tr><td align="left" colspan="1" rowspan="1">Training time (minutes)</td><td align="left" colspan="1" rowspan="1">2.650</td><td align="left" colspan="1" rowspan="1">26.133</td><td align="left" colspan="1" rowspan="1">35.566</td><td align="left" colspan="1" rowspan="1">27.616</td></tr><tr><td align="left" colspan="1" rowspan="1">Prediction time (microseconds)</td><td align="left" colspan="1" rowspan="1">117.604</td><td align="left" colspan="1" rowspan="1">23.706</td><td align="left" colspan="1" rowspan="1">13.894</td><td align="left" colspan="1" rowspan="1">14.740</td></tr><tr><td align="left" colspan="1" rowspan="1">Prediction accuracy stability</td><td align="left" colspan="1" rowspan="1">No</td><td align="left" colspan="1" rowspan="1">No</td><td align="left" colspan="1" rowspan="1">No</td><td align="left" colspan="1" rowspan="1">Yes</td></tr></tbody></table></table-wrap>
</p><p id="Par52">To contextualize the performance of our PSO-ELM model, we compare its accuracy with recently published models in intelligent traffic analysis. The fusion-based ML model for congestion control proposed by Ahmed et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> achieved an accuracy of 96.3% on vehicular network datasets. Alsarhan et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> applied a deep extreme learning machine (DELM) for smart parking occupancy detection, achieving a detection accuracy of 97.2%. The adaptive congestion control system by Ullah et al.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> achieved approximately 95.8% accuracy. In comparison, our proposed PSO-ELM model achieved a higher classification accuracy of 98.75% on CIC-IDS-2017 and comparable performance across two other benchmarks. This improvement highlights the advantage of integrating swarm optimization and deep feature selection in traffic classification scenarios beyond traditional congestion control.</p></sec><sec id="Sec10"><title>Conclusion</title><p id="Par53">This paper presents a network traffic classification method that combines Extreme Learning Machines (ELM) with a Particle Swarm Optimization (PSO) algorithm. The network structure features a hidden layer that offers both strong prediction performance and fast learning speed. To enhance efficiency, PSO is employed to find the optimal network parameters, providing a smarter search than the genetic algorithm (GA-ELM). Due to the large dataset and high number of features, feature selection is applied to reduce dimensionality, significantly improving both the training speed and performance of the proposed method compared to others, including RFC and MLP. Moreover, PSO-ELM outperforms GA-ELM in terms of learning speed and training time. The main reason for this improvement is the use of PSO to optimize both the parameters and the number of neurons in the hidden layer. Feature selection positively affects prediction accuracy and also boosts learning and prediction speed. Although the accuracy only improved by 0.11%, it demonstrates that reducing input dimensions can enhance classification performance. In machine learning methods, reducing feature numbers aids in improving generalization and accelerating convergence. The implementation complexity of the PSO-ELM method is lower than that of GA-ELM, as the learning processes in PSO are executed simultaneously. Our PSO-ELM framework outperforms GA-ELM and other methods in classification accuracy. As a goal-oriented process, the PSO-ELM method directs learning based on the best particles, offering better optimization and faster convergence than the genetic algorithm, which randomly swaps genes. The PSO-ELM method outperforms RFC and MLP in network settings. Optimal values were empirically derived, though alternative configurations may yield further improvements. However, trial and error methods do not effectively identify optimal points. In some experiments, the accuracy of MLP fell below 50%.</p><sec id="Sec11"><title>Limitations and future directions</title><p id="Par54">Limitations: 1- Dataset Scope: While PSO-ELM achieves high accuracy on CIC datasets, its performance on real-time, high-speed networks (e.g., 5G/6G) remains untested due to the lack of temporal granularity in benchmark datasets. 2- Computational Overhead: The PSO optimization phase, though efficient post-training, requires substantial resources (~&#8201;30&#160;min for 1,000 iterations), limiting deployability in edge devices with strict latency constraints. 3- Feature Selection Bias: The backward elimination process relies on Fisher scores, which may prioritize linear correlations and overlook nonlinear feature interactions in encrypted traffic.</p><p id="Par55">Future Work: 1- Real-Time Adaptation: Integrate lightweight PSO variants (e.g., Quantum PSO) to reduce training overhead for edge deployment. 2- Cross-Protocol Generalization: Extend evaluation to IoT-specific datasets (e.g., IoT-23) and protocols (MQTT, CoAP) to assess robustness in heterogeneous environments. 3- Explain ability: Enhance interpretability via SHAP (SHapley Additive Explanations) to trace feature contributions, critical for adversarial attack detection. 4- Hybrid Optimization: Combine PSO with metaheuristics (e.g., Grey Wolf Optimizer) to improve parameter search efficiency in high-dimensional spaces.</p></sec></sec><sec id="Sec12" sec-type="supplementary-material"><title>Supplementary Information</title><p>Below is the link to the electronic supplementary material.</p><p>
<supplementary-material content-type="local-data" id="MOESM1" position="float" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_16980_MOESM1_ESM.docx" position="float" orientation="portrait"><caption><p>Supplementary Material 1</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Xi Zhang and Jun Yin wrote the main manuscript text. Xi Zhang and Jun Yin reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets generated and/or analyzed during the current study are available in the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://www.unb.ca/cic/datasets/ids-2017.html">https://www.unb.ca/cic/datasets/ids-2017.html</ext-link>.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par58">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abbasi</surname><given-names>M</given-names></name><name name-style="western"><surname>Shahraki</surname><given-names>A</given-names></name><name name-style="western"><surname>Taherkordi</surname><given-names>A</given-names></name></person-group><article-title>Deep learning for network traffic monitoring and analysis (NTMA): A survey</article-title><source>Comput. Commun.</source><year>2021</year><volume>170</volume><fpage>19</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.comcom.2021.01.021</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Abbasi, M., Shahraki, A. &amp; Taherkordi, A. Deep learning for network traffic monitoring and analysis (NTMA): A survey. <italic toggle="yes">Comput. Commun.</italic><bold>170</bold>, 19&#8211;41 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Kalwar, J. H. &amp; Bhatti, S. Deep learning approaches for network traffic classification in the internet of things (IoT): A survey. arXiv preprint arXiv:2402.00920. (2024).</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>P</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Ye</surname><given-names>F</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X</given-names></name></person-group><article-title>Bytesgan: A semi-supervised generative adversarial network for encrypted traffic classification in SDN edge gateway</article-title><source>Comput. Netw.</source><year>2021</year><volume>200</volume><fpage>108535</fpage><pub-id pub-id-type="doi">10.1016/j.comnet.2021.108535</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Wang, P., Wang, Z., Ye, F. &amp; Chen, X. Bytesgan: A semi-supervised generative adversarial network for encrypted traffic classification in SDN edge gateway. <italic toggle="yes">Comput. Netw.</italic><bold>200</bold>, 108535 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chakravarthy</surname><given-names>SS</given-names></name><name name-style="western"><surname>Rajaguru</surname><given-names>H</given-names></name></person-group><article-title>Automatic detection and classification of mammograms using improved extreme learning machine with deep learning</article-title><source>Irbm</source><year>2022</year><volume>43</volume><issue>1</issue><fpage>49</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.irbm.2020.12.004</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Chakravarthy, S. S. &amp; Rajaguru, H. Automatic detection and classification of mammograms using improved extreme learning machine with deep learning. <italic toggle="yes">Irbm</italic><bold>43</bold> (1), 49&#8211;61 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meenalochini</surname><given-names>G</given-names></name><name name-style="western"><surname>Ramkumar</surname><given-names>S</given-names></name></person-group><article-title>A deep learning based breast cancer classification system using mammograms</article-title><source>J. Electr. Eng. Technol.</source><year>2024</year><volume>19</volume><issue>4</issue><fpage>2637</fpage><lpage>2650</lpage><pub-id pub-id-type="doi">10.1007/s42835-023-01747-x</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Meenalochini, G. &amp; Ramkumar, S. A deep learning based breast cancer classification system using mammograms. <italic toggle="yes">J. Electr. Eng. Technol.</italic><bold>19</bold> (4), 2637&#8211;2650 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sannasi Chakravarthy</surname><given-names>SR</given-names></name><etal/></person-group><article-title>Deep transfer learning with fuzzy ensemble approach for the early detection of breast cancer</article-title><source>BMC Med. Imaging</source><year>2024</year><volume>24</volume><issue>1</issue><fpage>82</fpage><pub-id pub-id-type="doi">10.1186/s12880-024-01267-8</pub-id><pub-id pub-id-type="pmid">38589813</pub-id><pub-id pub-id-type="pmcid">PMC11389118</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Sannasi Chakravarthy, S. R. et al. Deep transfer learning with fuzzy ensemble approach for the early detection of breast cancer. <italic toggle="yes">BMC Med. Imaging</italic>. <bold>24</bold> (1), 82 (2024).<pub-id pub-id-type="pmid">38589813</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s12880-024-01267-8</pub-id><pub-id pub-id-type="pmcid">PMC11389118</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Chakravarthy, S. S. &amp; Rajaguru, H. Breast tumor classification using transfer learning with adaptive crow search optimization. In <italic toggle="yes">2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)</italic> (Vol. 1, pp. 1&#8211;5). IEEE. (2023), December.</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Rajaguru, H. &amp; Chakravarthy, S. S. Classification of wisconsin breast cancer data with extreme learning machine and osprey optimization algorithm. In <italic toggle="yes">2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)</italic> (Vol. 1, pp. 1&#8211;4). IEEE. (2023), December.</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pacheco</surname><given-names>F</given-names></name><name name-style="western"><surname>Exposito</surname><given-names>E</given-names></name><name name-style="western"><surname>Gineste</surname><given-names>M</given-names></name><name name-style="western"><surname>Baudoin</surname><given-names>C</given-names></name><name name-style="western"><surname>Aguilar</surname><given-names>J</given-names></name></person-group><article-title>Towards the deployment of machine learning solutions in network traffic classification: A systematic survey</article-title><source>IEEE Commun. Surv. Tutorials</source><year>2018</year><volume>21</volume><issue>2</issue><fpage>1988</fpage><lpage>2014</lpage><pub-id pub-id-type="doi">10.1109/COMST.2018.2883147</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Pacheco, F., Exposito, E., Gineste, M., Baudoin, C. &amp; Aguilar, J. Towards the deployment of machine learning solutions in network traffic classification: A systematic survey. <italic toggle="yes">IEEE Commun. Surv. Tutorials</italic>. <bold>21</bold> (2), 1988&#8211;2014 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Azab</surname><given-names>A</given-names></name><name name-style="western"><surname>Khasawneh</surname><given-names>M</given-names></name><name name-style="western"><surname>Alrabaee</surname><given-names>S</given-names></name><name name-style="western"><surname>Choo</surname><given-names>KKR</given-names></name><name name-style="western"><surname>Sarsour</surname><given-names>M</given-names></name></person-group><article-title>Network traffic classification: techniques, datasets, and challenges</article-title><source>Digit. Commun. Networks</source><year>2024</year><volume>10</volume><issue>3</issue><fpage>676</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.dcan.2022.09.009</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Azab, A., Khasawneh, M., Alrabaee, S., Choo, K. K. R. &amp; Sarsour, M. Network traffic classification: techniques, datasets, and challenges. <italic toggle="yes">Digit. Commun. Networks</italic>. <bold>10</bold> (3), 676&#8211;692 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ertam</surname><given-names>F</given-names></name><name name-style="western"><surname>Avc&#305;</surname><given-names>E</given-names></name></person-group><article-title>A new approach for internet traffic classification: GA-WK-ELM</article-title><source>Measurement</source><year>2017</year><volume>95</volume><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2016.10.001</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Ertam, F. &amp; Avc&#305;, E. A new approach for internet traffic classification: GA-WK-ELM. <italic toggle="yes">Measurement</italic><bold>95</bold>, 135&#8211;142 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L</given-names></name><name name-style="western"><surname>Jones</surname><given-names>R</given-names></name></person-group><article-title>Big data analytics in cyber security: network traffic and attacks</article-title><source>J. Comput. Inform. Syst.</source><year>2021</year><volume>61</volume><issue>5</issue><fpage>410</fpage><lpage>417</lpage></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Wang, L. &amp; Jones, R. Big data analytics in cyber security: network traffic and attacks. <italic toggle="yes">J. Comput. Inform. Syst.</italic><bold>61</bold> (5), 410&#8211;417 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Duan, X., Zhou, Y. &amp; Guan, J. Exploration on heterogeneous network security monitoring algorithm based on big data intelligent information technology. In <italic toggle="yes">2023 IEEE 15th International Conference on Computational Intelligence and Communication Networks (CICN)</italic> (pp. 154&#8211;158). IEEE. (2023), December.</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>F</given-names></name><name name-style="western"><surname>Mao</surname><given-names>B</given-names></name><name name-style="western"><surname>Kawamoto</surname><given-names>Y</given-names></name><name name-style="western"><surname>Kato</surname><given-names>N</given-names></name></person-group><article-title>Survey on machine learning for intelligent end-to-end communication toward 6G: from network access, routing to traffic control and streaming adaption</article-title><source>IEEE Commun. Surv. Tutorials</source><year>2021</year><volume>23</volume><issue>3</issue><fpage>1578</fpage><lpage>1598</lpage><pub-id pub-id-type="doi">10.1109/COMST.2021.3073009</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Tang, F., Mao, B., Kawamoto, Y. &amp; Kato, N. Survey on machine learning for intelligent end-to-end communication toward 6G: from network access, routing to traffic control and streaming adaption. <italic toggle="yes">IEEE Commun. Surv. Tutorials</italic>. <bold>23</bold> (3), 1578&#8211;1598 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Avanzi, G. <italic toggle="yes">Design, Implementation and Evaluation of Learning Algorithms</italic> (for Predictive Quality of Service in Teleoperated Driving Scenarios, 2024).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Gronauer, S., Diepold, K., Tnani, M. A. &amp; Zwick, M. Trend Reports about Artificial Intelligence for 6G telecommunication. (2022).</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gui</surname><given-names>G</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Machine learning aided air traffic flow analysis based on aviation big data</article-title><source>IEEE Trans. Veh. Technol.</source><year>2020</year><volume>69</volume><issue>5</issue><fpage>4817</fpage><lpage>4826</lpage><pub-id pub-id-type="doi">10.1109/TVT.2020.2981959</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Gui, G., Zhou, Z., Wang, J., Liu, F. &amp; Sun, J. Machine learning aided air traffic flow analysis based on aviation big data. <italic toggle="yes">IEEE Trans. Veh. Technol.</italic><bold>69</bold> (5), 4817&#8211;4826 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Fowdur, T. P., Beeharry, Y., Hurbungs, V., Bassoo, V. &amp; Ramnarain-Seetohul, V. Big data analytics with machine learning tools. <italic toggle="yes">Internet of things and big data analytics toward next-generation intelligence</italic>, 49&#8211;97. (2018).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Li, Y., Qiu, R. &amp; Jing, S. Intrusion detection system using online sequence extreme learning machine (OS-ELM) in advanced metering infrastructure of smart grid. <italic toggle="yes">PloS One</italic>, <bold>13</bold>(2), e0192216. (2018).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pone.0192216</pub-id><pub-id pub-id-type="pmcid">PMC5828363</pub-id><pub-id pub-id-type="pmid">29485990</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ahmed</surname><given-names>SH</given-names></name><etal/></person-group><article-title><italic toggle="yes">Smart cities</italic>: Fusion-based intelligent traffic congestion control system for vehicular networks using machine learning techniques</article-title><source>Egypt. Inf. J.</source><year>2022</year><volume>23</volume><issue>3</issue><fpage>417</fpage><lpage>426</lpage></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Ahmed, S. H. et al. <italic toggle="yes">Smart cities</italic>: Fusion-based intelligent traffic congestion control system for vehicular networks using machine learning techniques. <italic toggle="yes">Egypt. Inf. J.</italic><bold>23</bold> (3), 417&#8211;426 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siddiqui</surname><given-names>SY</given-names></name><name name-style="western"><surname>Khan</surname><given-names>MA</given-names></name><name name-style="western"><surname>Abbas</surname><given-names>S</given-names></name><name name-style="western"><surname>Khan</surname><given-names>F</given-names></name></person-group><article-title>Smart occupancy detection for road traffic parking using deep extreme learning machine</article-title><source>J. King Saud Univ.-Comput. Inform. Sci.</source><year>2022</year><volume>34</volume><issue>3</issue><fpage>727</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1016/j.jksuci.2020.01.016</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Siddiqui, S. Y., Khan, M. A., Abbas, S. &amp; Khan, F. Smart occupancy detection for road traffic parking using deep extreme learning machine. <italic toggle="yes">J. King Saud Univ.-Comput. Inform. Sci.</italic><bold>34</bold> (3), 727&#8211;733 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Atta</surname><given-names>A</given-names></name><name name-style="western"><surname>Abbas</surname><given-names>S</given-names></name><name name-style="western"><surname>Khan</surname><given-names>MA</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>G</given-names></name><name name-style="western"><surname>Farooq</surname><given-names>U</given-names></name></person-group><article-title>An adaptive approach: smart traffic congestion control system</article-title><source>J. King Saud University-Computer Inform. Sci.</source><year>2020</year><volume>32</volume><issue>9</issue><fpage>1012</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.1016/j.jksuci.2018.10.011</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Atta, A., Abbas, S., Khan, M. A., Ahmed, G. &amp; Farooq, U. An adaptive approach: smart traffic congestion control system. <italic toggle="yes">J. King Saud University-Computer Inform. Sci.</italic><bold>32</bold> (9), 1012&#8211;1019 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>Y</given-names></name><name name-style="western"><surname>Gu</surname><given-names>H</given-names></name><name name-style="western"><surname>Wei</surname><given-names>W</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y</given-names></name></person-group><article-title>$ Deep-Full-Range $: a deep learning based network encrypted traffic classification and intrusion detection framework</article-title><source>IEEE Access.</source><year>2019</year><volume>7</volume><fpage>45182</fpage><lpage>45190</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2908225</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Zeng, Y., Gu, H., Wei, W. &amp; Guo, Y. $ Deep-Full-Range $: a deep learning based network encrypted traffic classification and intrusion detection framework. <italic toggle="yes">IEEE Access.</italic><bold>7</bold>, 45182&#8211;45190 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Lim, H. K. et al. Packet-based network traffic classification using deep learning. In <italic toggle="yes">2019 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)</italic> (pp. 046&#8211;051). IEEE. (2019), February.</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Zhang, F., Wang, Y. &amp; Ye, M. Network traffic classification method based on improved capsule neural network. In <italic toggle="yes">2018 14th International Conference on Computational Intelligence and Security (CIS)</italic> (pp. 174&#8211;178). IEEE. (2018), November.</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Nallaperuma, D., Nawaratne, R., Bandaragoda, T., Adikari, A., Nguyen, S., Kempitiya,T., &#8230; Pothuhera, D. (2019). Online incremental machine learning platform for big data-driven smart traffic management. IEEE Transactions on Intelligent Transportation Systems, 20(12), 4679&#8211;4690.</mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Komisarek</surname><given-names>M</given-names></name><name name-style="western"><surname>Pawlicki</surname><given-names>M</given-names></name><name name-style="western"><surname>Kozik</surname><given-names>R</given-names></name><name name-style="western"><surname>Choras</surname><given-names>M</given-names></name></person-group><article-title>Machine learning based approach to anomaly and cyberattack detection in streamed network traffic data</article-title><source>J. Wirel. Mob. Networks Ubiquitous Comput. Dependable Appl.</source><year>2021</year><volume>12</volume><issue>1</issue><fpage>3</fpage><lpage>19</lpage></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Komisarek, M., Pawlicki, M., Kozik, R. &amp; Choras, M. Machine learning based approach to anomaly and cyberattack detection in streamed network traffic data. <italic toggle="yes">J. Wirel. Mob. Networks Ubiquitous Comput. Dependable Appl.</italic><bold>12</bold> (1), 3&#8211;19 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Bujlow, T., Riaz, T. &amp; Pedersen, J. M. A method for classification of network traffic based on C5. 0 Machine Learning Algorithm. In <italic toggle="yes">2012 international conference on computing, networking and communications (ICNC)</italic> (pp. 237&#8211;241). IEEE. (2012), January.</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Shafiq, M. et al. Network traffic classification techniques and comparative analysis using machine learning algorithms. In <italic toggle="yes">2016 2nd IEEE International Conference on Computer and Communications (ICCC)</italic> (pp. 2451&#8211;2455). IEEE. (2016), October.</mixed-citation></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tahaei</surname><given-names>H</given-names></name><name name-style="western"><surname>Afifi</surname><given-names>F</given-names></name><name name-style="western"><surname>Asemi</surname><given-names>A</given-names></name><name name-style="western"><surname>Zaki</surname><given-names>F</given-names></name><name name-style="western"><surname>Anuar</surname><given-names>NB</given-names></name></person-group><article-title>The rise of traffic classification in IoT networks: A survey</article-title><source>J. Netw. Comput. Appl.</source><year>2020</year><volume>154</volume><fpage>102538</fpage><pub-id pub-id-type="doi">10.1016/j.jnca.2020.102538</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Tahaei, H., Afifi, F., Asemi, A., Zaki, F. &amp; Anuar, N. B. The rise of traffic classification in IoT networks: A survey. <italic toggle="yes">J. Netw. Comput. Appl.</italic><bold>154</bold>, 102538 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Hasibi, R., Shokri, M. &amp; Dehghan, M. Augmentation scheme for dealing with imbalanced network traffic classification using deep learning. <italic toggle="yes">arXiv preprint arXiv:1901.00204</italic>. (2019).</mixed-citation></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>P</given-names></name><name name-style="western"><surname>Ye</surname><given-names>F</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X</given-names></name><name name-style="western"><surname>Qian</surname><given-names>Y</given-names></name></person-group><article-title>Datanet: deep learning based encrypted network traffic classification in Sdn home gateway</article-title><source>IEEE Access.</source><year>2018</year><volume>6</volume><fpage>55380</fpage><lpage>55391</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2872430</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Wang, P., Ye, F., Chen, X. &amp; Qian, Y. Datanet: deep learning based encrypted network traffic classification in Sdn home gateway. <italic toggle="yes">IEEE Access.</italic><bold>6</bold>, 55380&#8211;55391 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Wang, W., Zhu, M., Wang, J., Zeng, X. &amp; Yang, Z. End-to-end encrypted traffic classification with one-dimensional convolution neural networks. In <italic toggle="yes">2017 IEEE international conference on intelligence and security informatics (ISI)</italic> (pp. 43&#8211;48). IEEE. (2017), July.</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Aceto, G., Ciuonzo, D., Montieri, A. &amp; Pescap&#233;, A. Mobile encrypted traffic classification using deep learning. In <italic toggle="yes">2018 Network traffic measurement and analysis conference (TMA)</italic> (pp. 1&#8211;8). IEEE. (2018), June.</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Zou, Z. et al. Encrypted traffic classification with a convolutional long short-term memory neural network. In <italic toggle="yes">2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)</italic> (pp. 329&#8211;334). IEEE. (2018), June.</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lotfollahi</surname><given-names>M</given-names></name><name name-style="western"><surname>Jafari Siavoshani</surname><given-names>M</given-names></name><name name-style="western"><surname>Zade</surname><given-names>SH</given-names></name><name name-style="western"><surname>Saberian</surname><given-names>M</given-names></name></person-group><article-title>Deep packet: A novel approach for encrypted traffic classification using deep learning</article-title><source>Soft. Comput.</source><year>2020</year><volume>24</volume><issue>3</issue><fpage>1999</fpage><lpage>2012</lpage><pub-id pub-id-type="doi">10.1007/s00500-019-04030-2</pub-id></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Lotfollahi, M., Jafari Siavoshani, M., Zade, S. H., Saberian, M. &amp; R., &amp; Deep packet: A novel approach for encrypted traffic classification using deep learning. <italic toggle="yes">Soft. Comput.</italic><bold>24</bold> (3), 1999&#8211;2012 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>M</given-names></name><etal/></person-group><article-title>A network traffic classification model based on metric learning</article-title><source>CMC-computers Mater. Continua</source><year>2020</year><volume>64</volume><issue>2</issue><fpage>941</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.32604/cmc.2020.09802</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Chen, M. et al. A network traffic classification model based on metric learning. <italic toggle="yes">CMC-computers Mater. Continua</italic>. <bold>64</bold> (2), 941&#8211;959 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Gharib, A., Sharafaldin, I., Lashkari, A. H. &amp; Ghorbani, A. A. An evaluation framework for intrusion detection dataset. In <italic toggle="yes">2016 International conference on information science and security (ICISS)</italic> (pp. 1&#8211;6). IEEE. (2016), December.</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Sharafaldin, I., Lashkari, A. H. &amp; Ghorbani, A. A. Intrusion detection evaluation dataset (CIC-IDS2017). <italic toggle="yes">Proceedings of the of Canadian Institute for Cybersecurity</italic>. (2018).</mixed-citation></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Q</given-names></name><name name-style="western"><surname>Ni</surname><given-names>L</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q</given-names></name></person-group><article-title>Deep learning based feature selection for remote sensing scene classification</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2015</year><volume>12</volume><issue>11</issue><fpage>2321</fpage><lpage>2325</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2015.2475299</pub-id></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Zou, Q., Ni, L., Zhang, T. &amp; Wang, Q. Deep learning based feature selection for remote sensing scene classification. <italic toggle="yes">IEEE Geosci. Remote Sens. Lett.</italic><bold>12</bold> (11), 2321&#8211;2325 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>H</given-names></name><name name-style="western"><surname>Li</surname><given-names>H</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>C</given-names></name><name name-style="western"><surname>Cao</surname><given-names>X</given-names></name></person-group><article-title>An efficient feature generation approach based on deep learning and feature selection techniques for traffic classification</article-title><source>Comput. Netw.</source><year>2018</year><volume>132</volume><fpage>81</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.comnet.2018.01.007</pub-id></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Shi, H., Li, H., Zhang, D., Cheng, C. &amp; Cao, X. An efficient feature generation approach based on deep learning and feature selection techniques for traffic classification. <italic toggle="yes">Comput. Netw.</italic><bold>132</bold>, 81&#8211;98 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Abid, A., Balin, M. F. &amp; Zou, J. Concrete autoencoders for differentiable feature selection and reconstruction. <italic toggle="yes">arXiv preprint arXiv:1901.09346</italic>. (2019).</mixed-citation></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kushwah</surname><given-names>GS</given-names></name><name name-style="western"><surname>Ranga</surname><given-names>V</given-names></name></person-group><article-title>Optimized extreme learning machine for detecting DDoS attacks in cloud computing</article-title><source>Computers Secur.</source><year>2021</year><volume>105</volume><fpage>102260</fpage><pub-id pub-id-type="doi">10.1016/j.cose.2021.102260</pub-id></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Kushwah, G. S. &amp; Ranga, V. Optimized extreme learning machine for detecting DDoS attacks in cloud computing. <italic toggle="yes">Computers Secur.</italic><bold>105</bold>, 102260 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salih</surname><given-names>AA</given-names></name><name name-style="western"><surname>Abdulazeez</surname><given-names>AM</given-names></name></person-group><article-title>Evaluation of classification algorithms for intrusion detection system: A review</article-title><source>J. Soft Comput. Data Min.</source><year>2021</year><volume>2</volume><issue>1</issue><fpage>31</fpage><lpage>40</lpage></element-citation><mixed-citation id="mc-CR44" publication-type="journal">Salih, A. A. &amp; Abdulazeez, A. M. Evaluation of classification algorithms for intrusion detection system: A review. <italic toggle="yes">J. Soft Comput. Data Min.</italic><bold>2</bold> (1), 31&#8211;40 (2021).</mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>