<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473162</article-id><article-id pub-id-type="pmcid-ver">PMC12473162.1</article-id><article-id pub-id-type="pmcaid">12473162</article-id><article-id pub-id-type="pmcaiid">12473162</article-id><article-id pub-id-type="pmid">41013110</article-id><article-id pub-id-type="doi">10.3390/s25185866</article-id><article-id pub-id-type="publisher-id">sensors-25-05866</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Hybrid Deep Learning Framework for Fault Diagnosis in Milling Machines</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-5807-7056</contrib-id><name name-style="western"><surname>Siddique</surname><given-names initials="MF">Muhammad Farooq</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05866" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-0268-4103</contrib-id><name name-style="western"><surname>Zaman</surname><given-names initials="W">Wasim</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05866" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-1060-9104</contrib-id><name name-style="western"><surname>Umar</surname><given-names initials="M">Muhammad</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05866" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Kim</surname><given-names initials="JY">Jae-Young</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af2-sensors-25-05866" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5185-1062</contrib-id><name name-style="western"><surname>Kim</surname><given-names initials="JM">Jong-Myon</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05866" ref-type="aff">1</xref><xref rid="af2-sensors-25-05866" ref-type="aff">2</xref><xref rid="c1-sensors-25-05866" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Lazaridis</surname><given-names initials="P">Pavlos</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05866"><label>1</label>Department of Electrical, Electronics and Computer Engineering, University of Ulsan, Ulsan 44610, Republic of Korea; <email>mfarooq229@mail.ulsan.ac.kr</email> (M.F.S.); <email>wasim94@mail.ulsan.ac.kr</email> (W.Z.); <email>muhammadumar@mail.ulsan.ac.kr</email> (M.U.)</aff><aff id="af2-sensors-25-05866"><label>2</label>PD Technology Co., Ltd., Ulsan 44610, Republic of Korea; <email>kjy7097@pdtech.co.kr</email></aff><author-notes><corresp id="c1-sensors-25-05866"><label>*</label>Correspondence: <email>jmkim07@ulsan.ac.kr</email>; Tel.: +82-52-259-2217</corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5866</elocation-id><history><date date-type="received"><day>24</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>14</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>19</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05866.pdf"/><abstract><p>This paper presents a hybrid fault-diagnosis framework for milling cutting tools designed to address three persistent challenges in industrial monitoring: noisy vibration signals, limited fault labels, and variability across operating conditions. The framework begins by removing baseline drift from raw signals to improve the signal-to-noise ratio. Logarithmic continuous wavelet scalograms are then constructed to provide precise time-frequency localization and reveal fault-related harmonics. To enhance feature clarity, a Canny edge operator is applied, suppressing minor artifacts and reducing intra-class variation so that key diagnostic structures are emphasized. Feature representation is obtained through a dual-branch encoder, where one pathway captures localized patterns while the other preserves long-range dependencies, resulting in compact and discriminative fault descriptors. These descriptors are integrated by an ensemble decision mechanism that assigns validation-guided weights to individual learners, ensuring reliable fault identification, improved robustness under noise, and stable performance across diverse operating conditions. Experimental validation on real-world cutting tool data demonstrates an accuracy of 99.78%, strong resilience to environmental noise, and consistent diagnostic performance under variable conditions. The framework remains lightweight, scalable, and readily deployable, providing a practical solution for high-precision tool fault diagnosis in data-constrained industrial environments.</p></abstract><kwd-group><kwd>acoustic emission signals</kwd><kwd>hybrid feature extraction</kwd><kwd>fault diagnosis</kwd><kwd>condition monitoring</kwd><kwd>hybrid deep learning model</kwd><kwd>canny edge filter</kwd><kwd>milling machine</kwd></kwd-group><funding-group><award-group><funding-source>Regional Innovation System &amp; Education (RISE)</funding-source><funding-source>Ulsan RISE Center</funding-source><funding-source>Ministry of Education (MOE)</funding-source><funding-source>Ulsan Metropolitan City, Republic of Korea</funding-source><award-id>2025-RISE-07-001</award-id></award-group><award-group><funding-source>Technology Innovation Program</funding-source><funding-source>Ministry of Trade, Industry&amp;Energy (MOTIE, Republic of Korea)</funding-source><award-id>20023566</award-id></award-group><funding-statement>This result was supported by the &#8220;Regional Innovation System &amp; Education (RISE)&#8221; through the Ulsan RISE Center, funded by the Ministry of Education (MOE) and the Ulsan Metropolitan City, Republic of Korea (2025-RISE-07-001). This work was also supported by the Technology Innovation Program (20023566, &#8216;Development and Demonstration of Industrial IoT and AI Based Process Facility Intelligence Support System in Small and Medium Manufacturing Sites&#8217;) funded By the Ministry of Trade, Industry&amp;Energy (MOTIE, Republic of Korea).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05866"><title>1. Introduction</title><p>Milling cutting tools (MCTs) are indispensable in modern manufacturing, enabling the production of intricate and high-precision components across industries such as automotive, aerospace, electronics, and precision engineering [<xref rid="B1-sensors-25-05866" ref-type="bibr">1</xref>]. These tools support complex operations such as cutting, grinding, and drilling, ensuring superior surface finishes while meeting the growing demand for cost-effective, high-performance products [<xref rid="B2-sensors-25-05866" ref-type="bibr">2</xref>]. However, under high-speed and heavy-load operating conditions, MCTs are subjected to severe forces, friction, and thermal stresses, which result in inevitable wear phenomena such as flank wear, crater wear, and edge chipping [<xref rid="B3-sensors-25-05866" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05866" ref-type="bibr">4</xref>]. This progressive degradation compromises product quality, increases machining costs, reduces tool life, and disrupts production reliability. Studies have shown that faults in key machine components, including bearings, gears, and cutting tools, account for nearly 57% of machine failures, with cutting tool defects alone responsible for approximately 20% of unplanned downtime [<xref rid="B5-sensors-25-05866" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05866" ref-type="bibr">6</xref>]. Such failures not only cause extended interruptions to production but also escalate maintenance costs and inflict substantial financial losses, especially in sectors that rely heavily on high-precision milling operations. To overcome these challenges, advanced Tool Condition Monitoring (TCM) systems have been developed to optimize productivity, minimize downtime, and sustain efficiency in industrial manufacturing. In modern Computer Numerical Control (CNC) machining, such systems employ automated fault detection and tool wear monitoring to achieve reliable tool life estimation, reduce unexpected stoppages, and ensure continuous production reliability [<xref rid="B7-sensors-25-05866" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05866" ref-type="bibr">8</xref>]. By enabling early detection of wear and timely intervention, TCM systems play a critical role in maintaining both the productivity and integrity of contemporary manufacturing processes [<xref rid="B9-sensors-25-05866" ref-type="bibr">9</xref>].</p><p>Despite the progress of TCM technologies, significant challenges persist, particularly in real-world industrial environments. One of the primary limitations is the heavy reliance on large volumes of labeled data for model training, which is impractical due to the high costs, labor, and time associated with data collection and annotation [<xref rid="B10-sensors-25-05866" ref-type="bibr">10</xref>]. Furthermore, industrial environments are inherently noisy, and these variable noise conditions introduce distortions and inaccuracies that hinder the performance of conventional monitoring methods [<xref rid="B11-sensors-25-05866" ref-type="bibr">11</xref>]. Indirect sensing techniques, such as those based on dynamometers, accelerometers, or force sensors, are especially susceptible to external disturbances and require sophisticated preprocessing for extracting reliable features [<xref rid="B12-sensors-25-05866" ref-type="bibr">12</xref>]. On the other hand, direct image-based approaches, while capable of delivering superior accuracy in controlled laboratory conditions, are often unsuitable for continuous real-time deployment because of their dependency on stable illumination, optics, and imaging setups [<xref rid="B13-sensors-25-05866" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05866" ref-type="bibr">14</xref>]. Consequently, there is an urgent need for innovative solutions that can overcome data scarcity, remain resilient in noisy environments, and deliver scalable performance under practical industrial constraints [<xref rid="B15-sensors-25-05866" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05866" ref-type="bibr">16</xref>].</p><p>The progression of fault diagnosis methods for MCTs reflects decades of innovation driven by increasing demands for precision, productivity, and automation. Early work in the 1980s and 1990s primarily investigated indirect sensor-based approaches, with vibration, acoustic emission (AE), and cutting force signals explored as indicators of tool wear [<xref rid="B17-sensors-25-05866" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05866" ref-type="bibr">18</xref>]. Cutting force signals, in particular, gained prominence due to their stability and direct correlation with tool&#8211;workpiece interactions. However, such methods were limited by interference from noise, the costs of sensor installation, and the requirement for specialized instruments such as dynamometers. In the 2000s, sensor fusion strategies were introduced, combining signals such as current, force, and vibration to improve detection robustness [<xref rid="B19-sensors-25-05866" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05866" ref-type="bibr">20</xref>]. Li et al. [<xref rid="B21-sensors-25-05866" ref-type="bibr">21</xref>], for example, demonstrated the feasibility of using motor current sensors for real-time tool monitoring without the need for external instrumentation, thereby improving industrial practicality. Nevertheless, indirect methods remained highly sensitive to disturbances and involved computationally intensive preprocessing, motivating the search for more efficient alternatives [<xref rid="B22-sensors-25-05866" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05866" ref-type="bibr">23</xref>].</p><p>By the 2010s, advances in machine vision systems and image processing techniques had enabled direct approaches to tool wear measurement. Researchers applied edge detection, segmentation, and texture analysis to quantify wear with high accuracy. Bagga et al. [<xref rid="B1-sensors-25-05866" ref-type="bibr">1</xref>] proposed an edge-detection-based online wear measurement approach that achieved performance comparable to optical microscopy while minimizing machining interruptions. Around the same time, deep learning architectures, particularly Convolutional Neural Networks (CNNs), emerged as transformative tools, replacing handcrafted feature engineering with automated feature extraction and classification [<xref rid="B24-sensors-25-05866" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05866" ref-type="bibr">25</xref>]. CNNs achieved remarkable performance in wear detection, classification, and segmentation tasks. However, their reliance on local receptive fields limited their ability to capture global contextual dependencies, making them less effective in complex scenarios involving overlapping or subtle wear features [<xref rid="B26-sensors-25-05866" ref-type="bibr">26</xref>].</p><p>The 2020s marked a paradigm shift in TCM with the adoption of attention-based architectures, particularly Vision Transformers (ViTs), which leverage self-attention mechanisms to capture both local and global feature relationships [<xref rid="B27-sensors-25-05866" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05866" ref-type="bibr">28</xref>]. ViTs demonstrated substantial improvements in classification accuracy and robustness, particularly in fault detection and tool wear recognition tasks [<xref rid="B29-sensors-25-05866" ref-type="bibr">29</xref>]. Recent contributions (2023&#8211;2025) have further advanced this area, with hybrid CNN-Transformer frameworks showing superior performance by combining the strengths of convolutional feature extraction with global context modeling [<xref rid="B30-sensors-25-05866" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05866" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05866" ref-type="bibr">32</xref>]. Parallel research has addressed noise-robust preprocessing strategies for AE and vibration signals, as well as domain adaptation and few-shot learning techniques for mitigating data scarcity [<xref rid="B33-sensors-25-05866" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05866" ref-type="bibr">34</xref>]. Nonetheless, limitations persist: many current approaches still degrade significantly under high levels of industrial noise, depend heavily on large annotated datasets, or lack effective feature fusion strategies for leveraging complementary representations. Recent work in robust fault-tolerant control, such as the fuzzy adaptive scheme proposed by Bounemeur and Chemachema [<xref rid="B35-sensors-25-05866" ref-type="bibr">35</xref>], demonstrates resilience to nonlinear actuator and time-varying sensor faults using online adaptation and Lyapunov stability analysis. While these approaches focus on stabilizing nonlinear control systems, the present study advances fault diagnosis by targeting robustness at the signal representation and feature learning levels through logarithmic CWT preprocessing, CNN&#8211;ViT fusion, and ensemble classification.</p><p>In light of these challenges, this study proposes a novel hybrid deep learning framework for milling tool fault diagnosis that integrates ResNet-18 and ViT architectures to exploit their complementary strengths. The framework employs logarithmic Continuous Wavelet Transform (log-CWT) to transform vibration signals into scalograms, enhancing resolution in critical low-frequency regions, while Canny edge filtering is used to suppress noise and emphasize salient features. ResNet-18 captures fine-grained local spatial features through hierarchical convolutional layers, whereas ViT captures long-range dependencies and global contextual information via multi-head self-attention. To further enhance robustness, extracted features are normalized and fused before being classified using a stacking ensemble comprising base learners such as Support Vector Machine (SVM), logistic regression, and a shallow Multilayer Perceptron (MLP), with a meta-learning strategy to optimize predictive accuracy.</p><sec><title>Contributions</title><p>The novelty and main contributions of this study include.</p><list list-type="simple"><list-item><label>1.</label><p>Logarithmic CWT scalograms enhanced with Canny edge filtering are introduced, providing clearer fault-relevant patterns and reducing intra-class variation compared to conventional CWT representations.</p></list-item><list-item><label>2.</label><p>A hybrid dual-branch feature extraction framework is proposed, where ResNet-18 captures local features and the ViT models global dependencies, ensuring complementary and comprehensive feature learning.</p></list-item><list-item><label>3.</label><p>A feature fusion and stacking ensemble classification strategy is designed, integrating linear SVM, logistic regression, and a shallow MLP with validation-guided weighting, significantly improving robustness under noisy and variable operating conditions.</p></list-item><list-item><label>4.</label><p>The framework is validated on real-world vibration datasets involving milling cutting tools, gears, and bearings, achieving superior diagnostic accuracy and demonstrating practical scalability.</p></list-item><list-item><label>5.</label><p>The contributions advance beyond existing CNN- or ViT-only approaches by providing a lightweight yet highly robust diagnostic solution that balances feature diversity, adaptability, and real-world deployability.</p></list-item></list><p>The manuscript is structured as follows. <xref rid="sec2-sensors-25-05866" ref-type="sec">Section 2</xref> details the methodology, outlining the key components of the framework. It also provides the technical background necessary for understanding the proposed approach. <xref rid="sec3-sensors-25-05866" ref-type="sec">Section 3</xref> presents the results and performance evaluation, starting with the experimental setup and dataset details, followed by an analysis of the proposed method&#8217;s effectiveness and a comparison with recent models. Finally, <xref rid="sec4-sensors-25-05866" ref-type="sec">Section 4</xref> concludes the study, summarizing key findings and suggesting future research directions.</p></sec></sec><sec id="sec2-sensors-25-05866"><title>2. Proposed Method for Fault Diagnosis in Milling Machines</title><p>This study introduces a novel hybrid framework that integrates ViTs and CNNs to overcome the challenges of limited datasets and noisy environments in fault diagnosis for milling machines. The block diagram of the proposed hybrid framework has been illustrated in <xref rid="sensors-25-05866-f001" ref-type="fig">Figure 1</xref>, where each module has been carefully designed. Conventional deep learning models frequently suffer from overfitting and limited generalization when training data is scarce, and their robustness decreases significantly in industrial conditions where background noise is unavoidable. To address these issues, the proposed method combines advanced signal preprocessing, log-CWT-based feature transformation, complementary feature extraction via CNNs and ViTs, feature fusion, and a stacking ensemble classifier, as illustrated in <xref rid="sensors-25-05866-f002" ref-type="fig">Figure 2</xref>. Each module has been carefully designed to enhance feature discriminability, improve prediction accuracy, and maintain computational efficiency, thereby providing a scalable and reliable solution suitable for real-world industrial applications.</p><list list-type="simple"><list-item><label><bold>(1)</bold>&#160;</label><p>
<bold>Data Preprocessing</bold>
</p></list-item></list><p>The data preprocessing pipeline begins with the acquisition of vibration signals from a milling machine during different fault conditions. Raw signals are first normalized and subjected to mean removal to eliminate baseline offsets and low-frequency drifts that could bias subsequent feature extraction. The preprocessed signals are then transformed into log-CWT, which provide a time&#8211;frequency domain representation of the non-stationary signals. Unlike standard CWT, logarithmic scaling enhances resolution in low-frequency bands that are often critical for detecting fault-related transients, while compressing higher frequencies to reduce redundancy. This transformation highlights localized bursts of energy associated with tool wear and incipient faults. To further improve robustness, a CEF is applied to the scalograms, sharpening boundaries of significant energy regions while attenuating background noise. These preprocessing steps work together to ensure that the downstream learning modules operate on clean, discriminative, and noise-resilient representations, thereby enabling reliable fault detection even under challenging manufacturing conditions.</p><list list-type="simple"><list-item><label><bold>(2)</bold>&#160;</label><p>
<bold>Feature Extraction</bold>
</p></list-item></list><p>The feature extraction module integrates the strengths of ResNet-18 and ViTs, providing dual-branch architecture. The ResNet-18 branch extracts fine-grained local spatial features through hierarchical convolutional filters and residual connections, capturing wear signatures, micro-cracks, and other localized abnormalities in scalograms. Meanwhile, the ViT branch employs a multi-head self-attention mechanism to model global dependencies across the entire scalogram, effectively capturing long-range contextual relationships that CNNs alone often overlook. The combination of local and global perspectives ensures that the model can detect both subtle and complex wear patterns across different mechanical components, including bearings, gears, and milling cutters. This dual-path design significantly enhances fault recognition accuracy and resilience to variations in operating conditions.</p><list list-type="simple"><list-item><label><bold>(3)</bold>&#160;</label><p>
<bold>Feature Concatenation</bold>
</p></list-item></list><p>To maximize the diagnostic value of extracted representations, the features obtained from ResNet-18 and ViT are passed through normalization and alignment layers before being fused. The fusion strategy is based on concatenation followed by linear projection, which integrates local spatial details with global contextual features into a unified high-dimensional representation. This enriched feature space enables the model to capture diverse fault characteristics more effectively than single-branch architecture. By aligning distributions and reducing redundancy, the fusion step ensures that complementary information from CNN and Transformer pathways is preserved, leading to a more holistic characterization of fault signatures. The fused features are then transferred to a fully connected artificial neural network (ANN) classifier, which acts as an intermediary stage to refine the decision space and provide discriminative embeddings for ensemble classification.</p><list list-type="simple"><list-item><label><bold>(4)</bold>&#160;</label><p>
<bold>Classification</bold>
</p></list-item></list><p>The final classification module employs a meta-learning stacking ensemble that operates on the fused feature embeddings. Instead of relying on a single classifier, the ensemble integrates multiple complementary base learners, including SVMs, logistic regression, and shallow MLPs. Each base learner provides a different decision perspective, capturing linear, probabilistic, and nonlinear relationships. The outputs of these learners are combined through a validation-guided weighted voting scheme, where more reliable classifiers are given greater importance and weaker predictions are suppressed. This strategy significantly improves diagnostic robustness, reduces misclassification under noise, and ensures adaptability to unseen fault patterns. The design is lightweight, computationally efficient, and capable of near real-time operation, making it deployable in industrial environments where scalability and robustness are critical.</p><sec id="sec2dot1-sensors-25-05866"><title>2.1. Logarithmic Continous Wavelet Transform</title><p>Log-CWT is a vital technique for analyzing non-stationary signals, such as vibration data from machines, where signal frequencies evolve dynamically over time. This method plays a crucial role in machine condition monitoring by capturing subtle changes in signal patterns that indicate faults or mechanical wear. By decomposing signals into wavelets&#8212;small oscillating functions scaled logarithmically to focus on specific frequency ranges and shifted over time to capture temporal variations, the log-CWT provides a detailed time-frequency representation of the signal&#8217;s behavior [<xref rid="B36-sensors-25-05866" ref-type="bibr">36</xref>]. Mathematically, the CWT of a signal <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at a given scale <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and translation b is defined as,<disp-formula id="FD1-sensors-25-05866"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8747;</mml:mo><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>&#8734;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8734;</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#183;</mml:mo></mml:mrow></mml:mrow><mml:mi>&#968;</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the CWT coefficient, where a is the scale parameter for frequency resolution, and b is the translation parameter for time. The wavelet <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#968;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> acts as a localized filter, scaled and shifted to extract time-frequency features. While CWT provides robust time-frequency representation, its uniform scaling limits resolution in low-frequency regions.The log-CWT addresses this limitation by introducing logarithmic scaling, enhancing the resolution at lower frequencies. It is expressed as;<disp-formula id="FD2-sensors-25-05866"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8747;</mml:mo><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>&#8734;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8734;</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>&#968;</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, the scale parameter a is replaced with <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> allowing the transform to provide finer details at lower frequencies, which are often critical for detecting faults in vibration signals.</p><p>In this study, log-CWT was applied to vibration data from the MCT dataset to generate scalograms, offering detailed time-frequency visualizations. These scalograms display time on the <italic toggle="yes">x</italic>-axis, frequency on the y-axis, and color intensity representing the amplitude of log-CWT coefficients, as shown in <xref rid="sensors-25-05866-f003" ref-type="fig">Figure 3</xref>. The amplitude highlights the energy distribution of vibration signals across time and frequency, enabling a comprehensive analysis of machine behavior. Under normal operating conditions, scalograms exhibit stable energy patterns within expected frequency ranges, indicating smooth machine performance. In contrast, faulty conditions introduce anomalies, such as sharp energy bursts or irregular patterns, suggesting mechanical stress, impacts, or crack propagation, which act as early indicators of potential faults. The log-CWT provides distinct advantages over traditional CWT for vibration signal analysis. Its logarithmic scaling enhances resolution in low-frequency regions, capturing critical fault features like bearing or gear wear that may be overlooked by CWT. Additionally, the log-CWT balances time-frequency representation, making it better suited for signals with wide-ranging frequency components. These strengths enable more accurate detection of subtle and evolving fault signatures, making the log-CWT an invaluable tool for machine condition monitoring and early fault detection. For comparison, conventional CWT offers a uniform scale that often compresses fault-related low-frequency features, whereas log-CWT enhances resolution in these regions and captures subtle transients more effectively. This advantage justifies the use of log-CWT in the present study for robust fault feature extraction under noisy industrial conditions.</p></sec><sec id="sec2dot2-sensors-25-05866"><title>2.2. Canny Edge Filter</title><p>After generating the log-CWT scalograms, the next important step in preprocessing is applying the Canny Edge Filter (CEF) to enhance important structural details. The canny edge detection algorithm is widely used in image processing for identifying edges, and in the context of fault diagnosis, it helps highlight the most significant features in the scalogram by detecting boundaries between different frequency components [<xref rid="B37-sensors-25-05866" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05866" ref-type="bibr">38</xref>]. The CEF operates in multiple steps to detect edges efficiently. First, the Gaussian filter smooths the scalogram to reduce noise, ensuring that only meaningful frequency transitions are captured. Mathematically, the Gaussian filter is applied as.<disp-formula id="FD3-sensors-25-05866"><label>(3)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the standard deviation controlling the degree of smoothing. This step prevents small variations from being falsely detected as edges. Next, the gradient magnitude and direction of the image are computed using Sobel operators in both the x and y directions:<disp-formula id="FD4-sensors-25-05866"><label>(4)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the logarithmic CWT scalogram. The overall gradient magnitude (M) is then given by.<disp-formula id="FD5-sensors-25-05866"><label>(5)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>&#8730;</mml:mo><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>G</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Applying the canny edge filter to the log-scaled CWT scalogram enhances the most critical frequency transitions, making it easier for deep learning models, CNN, and ViT to extract meaningful features. By emphasizing key spectral structures, this preprocessing step significantly improves the accuracy and reliability of fault classification in milling machine diagnostics.</p></sec><sec id="sec2dot3-sensors-25-05866"><title>2.3. Vision Transformer</title><p>ViTs represent a groundbreaking approach to computer vision, leveraging the transformer architecture originally designed for natural language processing. Unlike CNNs, which rely on convolutional layers to process images, ViTs divide images into fixed-size patches, flatten them, and treat these patches as input tokens for a transformer encoder. This design allows ViTs to capture global relationships between image patches using self-attention mechanisms. ViTs have demonstrated state-of-the-art performance in tasks such as image classification, particularly when pre-trained on large datasets and fine-tuned for specific downstream applications [<xref rid="B39-sensors-25-05866" ref-type="bibr">39</xref>].</p><p>The key advantages of ViTs over traditional CNNs include their ability to model long-range dependencies and their adaptability across diverse datasets. Unlike CNNs, which rely on local receptive fields, ViTs capture global relationships within a single layer, enhancing feature learning. However, ViTs typically require large-scale pre-training data to achieve optimal performance. In cases with limited data, fine-tuning pre-trained ViTs or using hybrid approaches that incorporate CNN-based feature extraction can improve their effectiveness [<xref rid="B40-sensors-25-05866" ref-type="bibr">40</xref>]. Their modular design, inspired by transformer architectures in NLP, allows flexibility in adjusting model complexity based on data availability, making them a viable option even in data-constrained scenarios [<xref rid="B41-sensors-25-05866" ref-type="bibr">41</xref>]. The mathematical steps of ViTs can be summarized as follows:</p><list list-type="simple"><list-item><label>1.</label><p><bold>Patch embedding:</bold> The patch embedding step converts an image into a sequence of patches and em-beds them in a high-dimensional space. The image <inline-formula><mml:math id="mm999" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> where H is the height of the image, W is the width of the image, and C represents the number of the channels. The image is divided into N patches. The mathemati-cal formula for the number of patches is given below.
<disp-formula id="FD6-sensors-25-05866"><label>(6)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
<list list-type="simple"><list-item><p>In Equation (6), N represents the number of patches, <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the height and the width of the image, whereas <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the patch size. Each of these patches is flattened into a vector and linearly projected to a latent dimension D. Mathematically, it can be defined as.</p></list-item></list>
<disp-formula id="FD7-sensors-25-05866"><label>(7)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#160;</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mi>E</mml:mi><mml:mo>&#160;</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<list list-type="simple"><list-item><p>In Equation (7), <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the special token for classification, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the learnable embedding matrix that maps each patch to the latent dimension, whereas <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the positional embedding, added to encode the spatial position of each patch. <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the flattened vector corresponding to the first and second patches of the input image, respectively.</p></list-item></list></p></list-item><list-item><label>2.</label><p><bold>Transformer encoder:</bold> The transformer encoder processes the patch embeddings using self-attention and MLP layers, learning rich contextual representations. The input sequence is processed through L layers of transformer blocks, where each block includes multi-head self-attention, denoted by MSA, and a feedforward neural network such as MLP. Mathematically, it can be expressed as.
<disp-formula id="FD8-sensors-25-05866"><label>(8)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8230;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<list list-type="simple"><list-item><p>where each attention head computes:</p></list-item></list>
<disp-formula id="FD9-sensors-25-05866"><label>(9)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
<list list-type="simple"><list-item><p><inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are query, key, and value matrices derived from the input embeddings.</p></list-item></list>
<disp-formula id="FD10-sensors-25-05866"><label>(10)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<list list-type="simple"><list-item><p>These components are combined with residual connections and normalization, ensuring stable and efficient learning. The resulting contextual embeddings enable the ViT to capture both local and global relationships for fault diagnosis.</p></list-item></list></p></list-item></list><list list-type="simple"><list-item><label>1.</label><p><bold>Classification head:</bold> The classification head of the ViT processes the output embeddings from the transformer encoder to produce the final classification result. It starts with Layer Normalization to improve stability and training, followed by the extraction of features from the classification token x-class learned during encoding. These features are then passed through a Fully Connected (FC) Layer, mapping them to the required output classes and generating prediction probabilities. This design ensures the classification token effectively captures relevant global information from the input image [<xref rid="B42-sensors-25-05866" ref-type="bibr">42</xref>].</p></list-item></list><p>The proposed fault diagnosis method utilizes a ViT architecture, as shown in <xref rid="sensors-25-05866-f004" ref-type="fig">Figure 4</xref>, to effectively extract local and global features from input scalograms. The process begins with dividing the input image (224 &#215; 224 &#215; 3) into fixed-size patches, which are transformed into patch embeddings. A classification token is added to capture global information, and positional embeddings are incorporated to retain spatial relationships. These embeddings are passed through a series of transformer blocks, each consisting of multi-head attention mechanisms to capture dependencies across patches, normalization layers for stability, and MLP blocks for feature refinement. The final transformer block outputs feature embeddings that encapsulate complex spatial dependencies within the input data. These embeddings are then processed by the classification head to predict fault categories with high accuracy, demonstrating the model&#8217;s robustness and effectiveness for fault diagnosis tasks. The architectural summary of the ViT for the current work is presented in <xref rid="sensors-25-05866-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec2dot4-sensors-25-05866"><title>2.4. Features Pool</title><p>Feature extraction is performed using ResNet-18 and ViT, each capturing different aspects of the log-CWT scalograms. The fusion of these feature sets as shown in <xref rid="sensors-25-05866-f005" ref-type="fig">Figure 5</xref> and <xref rid="sensors-25-05866-t002" ref-type="table">Table 2</xref> ensures a more comprehensive understanding of the fault characteristics. Mathematically, if <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the feature vectors extracted from CNN and ViT, the fused feature representation is given by:<disp-formula id="FD11-sensors-25-05866"><label>(11)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:msub><mml:mo>&#8853;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#8853; represents feature concatenation. To ensure balanced feature contribution, Min-Max Scaling is applied to normalize the fused vector:<disp-formula id="FD12-sensors-25-05866"><label>(12)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:msub><mml:mo>&#8853;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the minimum and maximum values of the feature set. This step prevents feature dominance and improves classification performance [<xref rid="B43-sensors-25-05866" ref-type="bibr">43</xref>]. The fused feature vector is then fed into a classifier for fault detection, utilizing both local spatial patterns from CNN and global contextual understanding from ViT, resulting in improved accuracy and robustness in milling machine diagnostics. The concatenated feature vector <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is normalized using Min-Max scaling, which ensures balanced contribution by restricting each feature component to a comparable numerical range [0, 1]. This prevents domination of one feature source i.e., CNN over the other ViT. Formally, for feature <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD13-sensors-25-05866"><label>(13)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#8242;</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot5-sensors-25-05866"><title>2.5. Ensemble Classification with Meta-Learner</title><p>In the current study, a stacking ensemble learning (SEL) approach is adopted as the final classification module for fault diagnosis, replacing the conventional single classifier. This decision is driven by the need to improve generalization, reduce misclassification in overlapping fault categories, and leverage diverse learning strategies. After extracting features from the vibration scalograms using a pretrained ResNet-18 and ViTs, the fused features are passed to multiple base-level classifiers, each offering a unique perspective on the classification task. The ensemble takes the fused feature vector (ResNet-18 + ViT) and feeds it to three base learners-Linear SVM (kernel = &#8216;linear&#8217;), Logistic Regression (solver = &#8216;lbfgs&#8217;), and a shallow MLP (hidden_layer_sizes = (64,)). Each produces an independent prediction, which is combined by a validation-guided, meta-learned weighted voting module (&#8220;Meta Learning Logic Box&#8221;). This two-stage meta-ensemble emphasizes consistently reliable learners and down-weights weaker ones, improving robustness to individual model bias and yielding the final prediction. Mathematically, if F represents the fused feature vector and B1, B2, &#8230;, Bn represent the base learners, the meta-learner M produces the final prediction as:<disp-formula id="FD14-sensors-25-05866"><label>(14)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>^</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>M</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mn>1</mml:mn><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This architecture facilitates effective learning from multiple decision boundaries and improves fault separability, especially in cases where class distributions are imbalanced or highly overlapping. The stacking ensemble is particularly beneficial in industrial fault diagnosis tasks, where real-world data may exhibit noise, non-linearity, and high variability in fault signatures. As depicted in <xref rid="sensors-25-05866-f006" ref-type="fig">Figure 6</xref>, the process begins with feature extraction using ResNet-18 and ViT, followed by the fusion of features and classification through the stacked ensemble. The integration of diverse learners and a robust meta-learner enhance the model&#8217;s capability to classify subtle and overlapping fault categories, making it well-suited for real-world applications in intelligent predictive maintenance. In the stacking ensemble, each base classifier is assigned a weight proportional to its normalized validation accuracy, such that classifiers with higher reliability contribute more strongly to the final decision. This validation-weighted voting scheme contrasts with simple majority voting, where all classifiers are equally weighted. Comparative experiments confirm that validation-weighted voting yields 2&#8211;4% higher accuracy than simple majority voting and 4&#8211;7% higher than the best individual classifier, demonstrating its effectiveness in improving robustness under noisy and limited data conditions.</p><p>Unlike conventional CNN-only or Transformer-only frameworks, the proposed design integrates log-CWT preprocessing, ResNet-18 feature extraction, and ViT global attention, combined with a meta-learning stacking ensemble. This ensures improved robustness in noisy industrial environments and limited-data regimes, which are less frequently addressed in existing hybrid fault diagnosis studies. Our results demonstrate that the integration is theoretically justified (CNN local + ViT global features) where feature fusion improves classification accuracy by over single-branch models.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05866"><title>3. Results and Performance Evaluation</title><p>The proposed method is validated using vibrational data from a real-world milling machine, focusing on fault detection and classification. Its fault detection capability is first evaluated against traditional time-domain indicators, highlighting its advantages. Subsequently, its performance in fault classification is compared with state-of-the-art methods, demonstrating superior accuracy in identifying various fault types.</p><sec id="sec3dot1-sensors-25-05866"><title>3.1. Experimental Setup and Data Acquisition</title><p>Vibrational signals were acquired from a real-world milling machine, as shown in <xref rid="sensors-25-05866-f007" ref-type="fig">Figure 7</xref>. The machining center used for this study was an INTER-SIEG X1 Micro Mill Drill, a compact cast iron machine with capabilities like a small pillar drill. The data collection process focused on a straight parallel-milling operation on a steel workpiece, a commonly used techniques for shaping and machining hard materials. Five steel workpieces, each with dimensions of 20 mm &#215; 35 mm &#215; 35 mm, were used for the experiment. <xref rid="sensors-25-05866-f008" ref-type="fig">Figure 8</xref>a depicts the raw workpieces, while <xref rid="sensors-25-05866-f008" ref-type="fig">Figure 8</xref>b,c displays a machined work-piece post-processing. Vibrational sensors (model R15I-AST, MISTRAS Inc., Princeton Junction, NJ, USA) were securely mounted on the milling machine using adhesive to ensure stability during operation. Data acquisition was conducted using the NI-9223 system from National Instruments, with custom software developed in Python 3.10 by the Ulsan Industrial Artificial Intelligence Laboratory. The vibrational signals were sampled at a frequency of 2.56 KHz, providing 256,000 data points per second. Prior to data acquisition, an HSU-Nelson test was performed to verify the proper functioning of the vibrational sensors and to ensure accurate detection of signals during the experiments. To enhance data quality, two vibrational sensors were deployed. The primary sensor was positioned on the spindle to capture critical signals associated with the tool, bearings, and gears, while the secondary sensor acted as a guard transducer. The guard transducer, illustrated in <xref rid="sensors-25-05866-f007" ref-type="fig">Figure 7</xref>, was designed to detect non-target signals, suppress noise, and ensure that the primary sensor focused exclusively on relevant data. This dual-sensor configuration ensured the collection of high-quality vibrational data, which served as the basis for subsequent fault detection and classification tasks. Data acquisition commenced under the normal operating conditions of the milling machine.</p><p>In accordance with ISO-8688-2 guidelines [<xref rid="B44-sensors-25-05866" ref-type="bibr">44</xref>], the tool&#8217;s lifespan was defined by an average flank wear of 0.3 mm. However, tool breakage, which can occur unpredictably during the machining of hard materials, poses a significant risk of catastrophic failure even in the early stages of tool wear. To ensure the collection of fault data for this study, an average flank wear of 0.3 mm was deliberately induced in the carbide tool prior to the milling operation. In addition, to simulate incipient defects, a fault was introduced in the outer race of the bearing supporting the tool, and vibrational signals were recorded during the milling process to capture relevant fault data. A further defect was created in the gear mechanism responsible for torque transmission from the motor to the spindle by removing a small metal fragment from one of the gear teeth. This defect was carefully monitored, and vibrational signals were collected throughout the machining operation to ensure the inclusion of fault-related data. Furthermore, signals overlapping techniques have been used and the samples of each class of the dataset are made 60 as shown in <xref rid="sensors-25-05866-t003" ref-type="table">Table 3</xref>. These artificially induced defects in the tool, bearing, and gear ensured the dataset comprehensively represented a range of operating conditions, enabling effective fault detection and classification with the proposed method. The details of the dataset are summarized in <xref rid="sensors-25-05866-t003" ref-type="table">Table 3</xref> and illustrated in <xref rid="sensors-25-05866-f009" ref-type="fig">Figure 9</xref>.</p><sec id="sec3dot1dot1-sensors-25-05866"><title>3.1.1. Milling Cutting Tool Dataset</title><p>The MCT Dataset, collected from the MCT testbed, provides comprehensive insights into the health and performance of various machinery components. The experimental setup includes a motor-driven shaft with 16-tooth, 32-tooth, and 30-tooth gears, and a spindle connected to the 30-tooth gear shaft, with a two-flute end mill cutting tool simulating real-world machining. Faults were introduced via laser processing and impact, resulting in three types: 3 mm deep bearing faults, gear faults caused by tooth impact damage, and tool faults from blade breakage as shown in <xref rid="sensors-25-05866-f010" ref-type="fig">Figure 10</xref>. Data was collected under normal and fault conditions during idle and cutting operations. Normal idle conditions involved 15 repetitions of 2-s intervals at 1320 RPM (motor) and 660 RPM (spindle), while cutting conditions used the same speeds and a bed feed rate of 0.4 mm/s. Fault scenarios included 20 idle runs and 15 cutting runs for bearing and gear faults, and 15 cutting runs for tool faults. Vibration signals were sampled at 25,600 Hz using sensors with sensitivities of 100 mV/g for the motor and 500 mV/g for the spindle chuck. Each 2-s segment was recorded via an NI 9234 analog input module with four IEPE channels. This dataset offers valuable information on machinery behavior under diverse conditions, supporting accurate fault detection and classification.</p></sec><sec id="sec3dot1dot2-sensors-25-05866"><title>3.1.2. Bearing Dataset</title><p>The Bearing Dataset, sourced from the RK-4 Bearing Testbed, provides extensive information on bearing health conditions. The experimental setup includes a fault simulation testbed with a vibration sensor mounted on the bearing housing for data collection and a tachometer sensor for measuring rotational speed. Data were recorded for both normal and fault conditions, including outer race faults, inner race faults, and roller faults, achieved by replacing the respective bearing components. FAG NJ206TVP rolling element bearings with dimensions of 30 mm inner diameter, 62 mm outer diameter, and 16 mm race width were used in the experiments. Faults were introduced via electrical discharge machining, creating defects with a uniform length of 3 mm and depth of 1 mm as shown in <xref rid="sensors-25-05866-f011" ref-type="fig">Figure 11</xref>.</p><p>The bearings operated at 1800 RPM under radial load, and vibration signals were sampled at 25.6 kHz using an IEPE acceleration sensor (model 622B01). Data were collected in 1-second intervals through an NI 9234 four-channel IEPE analog input module. The dataset comprises four bearing health states&#8212;normal, inner race fault, outer race fault, and roller fault providing high-resolution data for accurate fault detection and diagnosis.</p></sec><sec id="sec3dot1dot3-sensors-25-05866"><title>3.1.3. Gear Dataset</title><p>The gear dataset, derived from the Gear Fault Testbed, provides comprehensive data on gear health under varying fault severities and operational conditions. The experimental setup includes a three-phase motor, a gearbox, and a fan connected via a belt to apply directional torque. Gear faults were introduced using laser cutting on the driven shaft, resulting in tooth wear severities of 0.45 mm (5% of tooth length), 0.9 mm (10%), 1.8 mm (20%), 2.7 mm (30%), 3.6 mm (40%), and 4.5 mm (50%). The driven shaft gear features 38 teeth with a diameter of 160 mm, while the drive shaft gear has 25 teeth with a diameter of 108 mm, and each tooth measures 9 mm in length as shown in <xref rid="sensors-25-05866-f012" ref-type="fig">Figure 12</xref>.</p><p>Data was captured under rotational speeds of 300 RPM (5 Hz), 600 RPM (10 Hz), 900 RPM (15 Hz), and 1200 RPM (20 Hz), with torque applied through the fan. Vibration signals were recorded at a sampling frequency of 65,536 Hz using an IEPE sensor (model 622B01) for 10-min durations, segmented into 1-s intervals. Sensor placements included Channel 1 on the drive shaft and Channel 2 on the non-drive shaft, providing thorough coverage of gear health states. This dataset offers valuable insights into gear wear progression and fault diagnosis across varying severities and operating speeds.</p></sec></sec><sec id="sec3dot2-sensors-25-05866"><title>3.2. Performance Metrics for Comparisons</title><p>This study presents a fault diagnosis framework integrating ViTs and CNNs to address limited datasets and noisy environments. Using advanced preprocessing, feature fusion, and knowledge distillation, the framework enhances fault detection and classification. Final classifications are carried out using a stacking ensemble with validation-guided weighted voting on the fused features, yielding high accuracy, reduced misclassification, and scalable performance on real-world vibration data. The model&#8217;s effectiveness is evaluated using Equations (15)&#8211;(18).<disp-formula id="FD15-sensors-25-05866"><label>(15)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-05866"><label>(16)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-05866"><label>(17)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-05866"><label>(18)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Accuracy, precision, recall, and F1-score are key metrics for evaluating classification models. Accuracy measures positive predictions and recalls gauges the model&#8217;s ability to identify actual positives. The F1-score balances precision and recall, offering a single performance metric, especially for imbalanced datasets. Together, they provide a comprehensive assessment of model performance.</p></sec><sec id="sec3dot3-sensors-25-05866"><title>3.3. Ablation Studies</title><p>The ViT model, as introduced by Pinghu Xu et al. [<xref rid="B45-sensors-25-05866" ref-type="bibr">45</xref>], leverages self-attention mechanisms to effectively capture global dependencies and complex relationships in data, making it well-suited for fault diagnosis tasks. Evaluated on our lab MCT dataset, the ViT model achieved an accuracy of 93.83%, with precision, recall, and F1-scores of 95.00%, 94.00%, and 94.00%, respectively, as shown in <xref rid="sensors-25-05866-t004" ref-type="table">Table 4</xref>. While these results demonstrate strong overall performance, certain limitations were evident in distinguishing faults with subtle feature differences. For instance, misclassifications between GF and GFI classes were observed in the confusion matrix (Figure 14), and the t-SNE visualization (Figure 15) revealed overlapping clusters for these classes, indicating challenges in learning fine-grained feature separations. Additionally, the ROC curve (Figure 16) showed strong detection performance with an AUC of 1.00 for most classes, but slight reductions were noted for the N and TF classes, with AUC values of 0.97 and 0.98, respectively. These findings highlight the ViT model&#8217;s strengths in capturing global relationships and producing well-separated clusters for most classes, but also its limitations in fully addressing subtle fault variations and optimizing performance with limited labeled data.</p><p>The ablation study evaluates both 1D-CNN [<xref rid="B46-sensors-25-05866" ref-type="bibr">46</xref>] and CWT-CNN [<xref rid="B47-sensors-25-05866" ref-type="bibr">47</xref>] for fault classification. The 1D-CNN model, leveraging sequential feature extraction, achieved 93.67% accuracy with precision, recall, and F1 scores of 93.71% each. While effective, it struggles to distinguish faults with subtle variations, particularly in N, NI, and TF classes, as indicated by overlapping clusters in the confusion matrix (Figure 14) and t-SNE visualization (Figure 15). The ROC curve (Figure 16) shows strong detection capability with an AUC of 1.00 for most classes, though Class N has a slight drop to 0.99, reflecting reduced sensitivity in differentiating normal conditions. Its reliance on local feature extraction limits its ability to capture global fault relationships. In the same manner, the CWT-CNN model, using time-frequency scalograms, achieved 91.48% accuracy, with precision, recall, and F1 scores of 91.72% each. It performs slightly worse than the former 1D-CNN, particularly in handling transient fault variations, leading to misclassifications. A key limitation of this model is the use of CWT instead of log-CWT, which results in uneven time-frequency resolution, potentially reducing the model&#8217;s ability to capture fine-grained fault patterns. Additionally, the relatively small dataset limits deep learning models from effectively learning complex fault characteristics, impacting classification robustness, especially for closely related fault classes. By integrating ViTs with CNNs in the proposed model, we address these limitations by combining global and local feature extraction, improving class separability, and enhancing robustness in noisy environments. The hybrid approach ensures superior fault classification accuracy and reliability, particularly in challenging scenarios with limited labeled data and overlapping feature distributions.</p></sec><sec id="sec3dot4-sensors-25-05866"><title>3.4. Comparison of Proposed with Traditional Methods</title><p>The proposed model processes input log-CWT scalograms through a ViT architecture for global feature extraction as part of a hybrid fault diagnosis pipeline. The input images, sized (3, 224, 224), are divided into non-overlapping 16 &#215; 16 patches and projected into 384-dimensional embeddings via a patch embedding layer. Positional encodings are added to preserve spatial information across the sequence. These tokenized embeddings, along with a learnable classification token, are fed into 12 sequential transformer blocks comprising multi-head self-attention and MLP layers, enabling the capture of long-range dependencies and global contextual relationships. Instead of direct classification, the output from the classification token is extracted as a compact global feature representation. This is subsequently fused with spatial features extracted from ResNet-18 and forwarded to a stacking ensemble classifier, which generates the final class predictions across seven fault categories using probabilistic outputs aggregated from multiple base learners.</p><p>The proposed method is validated through comprehensive experiments, demonstrating high classification accuracy and efficiency. As shown in <xref rid="sensors-25-05866-f013" ref-type="fig">Figure 13</xref>, training converges rapidly, with accuracy stabilizing and flattening after just a few epochs. The classification report in <xref rid="sensors-25-05866-t005" ref-type="table">Table 5</xref> confirms strong results, with a precision of 98.57%, a recall of 99.14%, and an F1-score of 98.86%, culminating in an overall accuracy of 99.78%. The confusion matrix (<xref rid="sensors-25-05866-f014" ref-type="fig">Figure 14</xref>) indicates minimal misclassifications, with many fault classes correctly predicted. Clear class separability is evident in the t-SNE plot (<xref rid="sensors-25-05866-f015" ref-type="fig">Figure 15</xref>), while ROC curves (<xref rid="sensors-25-05866-f016" ref-type="fig">Figure 16</xref>) confirm excellent detection performance, achieving an AUC of 1.00 across all classes. The integration of ViT and ResNet-18 enables the extraction of both global and local fault features, enhancing the model&#8217;s ability to differentiate between subtle fault patterns. Although the results are highly promising, further validation under varied operating conditions is recommended to confirm robustness. Overall, this framework offers a scalable and accurate solution for intelligent fault detection in milling machine applications.</p><p>The CNN-LSTM model, proposed by Ting Huang and Dao et al. [<xref rid="B48-sensors-25-05866" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05866" ref-type="bibr">49</xref>] in their studies, achieved 95.29% accuracy, 97.14% precision, 95.29% recall, and 95.57% F1-score on our lab dataset. This hybrid architecture integrates convolutional layers for spatial feature extraction and LSTM layers for capturing temporal dependencies, making it particularly effective for time-series-based fault detection. By analyzing both spatial and sequential patterns, the model aims to enhance fault classification in industrial applications. However, its performance is significantly affected by the limited number of data samples, leading to challenges in distinguishing fault variations. The confusion matrix highlights misclassifications, particularly in the N and NI classes, where three N samples were incorrectly classified as NI, as shown in <xref rid="sensors-25-05866-f013" ref-type="fig">Figure 13</xref>. This suggests that the model struggles with class overlaps and lacks sufficient training data to generalize well across all fault types. The t-SNE visualization, as illustrated in <xref rid="sensors-25-05866-f014" ref-type="fig">Figure 14</xref>, further supports this, showing overlapping clusters for N, NI, and BFI, indicating that the model is unable to create well-separated feature representations due to data scarcity. The ROC curve in <xref rid="sensors-25-05866-f015" ref-type="fig">Figure 15</xref> shows strong classification performance, with an AUC of 1.00 for most classes, but the N class drops slightly to 0.99, reflecting a minor reduction in sensitivity. While the CNN-LSTM model is designed to capture both spatial and temporal dependencies, its effectiveness is constrained by the limited dataset size, reducing its ability to learn nuanced variations between fault conditions. Enhancing the dataset with more diverse samples and incorporating data augmentation strategies could improve its classification robustness and reliability.</p><p>The CNN-CAE model, proposed by Debasish Jana and Zhiyi et al. [<xref rid="B50-sensors-25-05866" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-05866" ref-type="bibr">51</xref>] in their respective studies achieved 95.08% accuracy, 95.14% precision, 95.71% recall, and 95.26% F1 score on our lab dataset, as shown in <xref rid="sensors-25-05866-t005" ref-type="table">Table 5</xref>. Designed for fault classification in industrial systems, it integrates convolutional layers for spatial feature extraction and an autoencoder for dimensionality reduction and noise filtering. This combination allows the model to learn meaningful patterns from complex datasets, making it particularly useful when dealing with inconsistent data quality. However, the model exhibits misclassifications, particularly in the N, NI, and TF classes, as seen in the confusion matrix in <xref rid="sensors-25-05866-f013" ref-type="fig">Figure 13</xref>. These errors suggest difficulties in differentiating closely related fault types, likely due to overlapping feature representations. The t-SNE visualization distinguishes faults with subtle variations, as shown in <xref rid="sensors-25-05866-f014" ref-type="fig">Figure 14</xref>. Additionally, the ROC curve reveals inconsistencies in sensitivity across different classes, with some faults being detected with high confidence while others remain ambiguous, as illustrated in <xref rid="sensors-25-05866-f015" ref-type="fig">Figure 15</xref>. These findings suggest that while CNN-CAE is effective in extracting local spatial features, it struggles to capture global contextual relations. The model&#8217;s reliance on localized patterns makes it less capable of handling faults with gradual or subtle changes, which are common in real-world applications. A more advanced approach incorporating global dependencies and context-aware learning mechanisms could improve classification performance across all fault types.</p><p>The ViT-Attention model by Lin et al. [<xref rid="B52-sensors-25-05866" ref-type="bibr">52</xref>], which integrates convolutional layers with a self-attention mechanism for multi-sensor data fusion, achieved 97.10% accuracy, 97.85% precision, 96.14% recall, and 96.99% F1 score on our dataset, as presented in <xref rid="sensors-25-05866-t005" ref-type="table">Table 5</xref>. The confusion matrix in <xref rid="sensors-25-05866-f014" ref-type="fig">Figure 14</xref> confirms that most fault classes were correctly classified, though some misclassifications occurred, particularly between the NI and TF categories. The t-SNE visualization in <xref rid="sensors-25-05866-f015" ref-type="fig">Figure 15</xref> reveals well-formed but slightly overlapping feature clusters, indicating challenges in distinguishing closely related faults. The ROC curves in <xref rid="sensors-25-05866-f016" ref-type="fig">Figure 16</xref> show high overall detection confidence, with only minor variations in sensitivity across classes. These outcomes suggest that ViT-Attention is effective in modeling both local and global dependencies but remains sensitive to subtle class overlaps. In contrast, the proposed framework achieves higher robustness and clearer class separation through logarithmic CWT preprocessing combined with a lightweight dual-branch architecture, while avoiding the hardware and computational complexity associated with multi-sensor fusion.</p><p>Finally, the 1D-2D CNN Fusion model with hybrid attention proposed by Meng et al. [<xref rid="B53-sensors-25-05866" ref-type="bibr">53</xref>] achieved 95.00% accuracy, 96.76% precision, 95.51% recall, and 95.79% F1 score on our dataset (<xref rid="sensors-25-05866-t005" ref-type="table">Table 5</xref>). As shown in the confusion matrix in <xref rid="sensors-25-05866-f014" ref-type="fig">Figure 14</xref>, most fault types were classified correctly, though some misclassifications occurred between closely related classes such as NI and N. The t-SNE visualization in <xref rid="sensors-25-05866-f015" ref-type="fig">Figure 15</xref> illustrates reasonable feature clustering, but with minor overlaps that indicate challenges in fully separating subtle fault conditions. The ROC curves in <xref rid="sensors-25-05866-f016" ref-type="fig">Figure 16</xref> confirm strong sensitivity across all categories, with AUC values approaching 1.00, reflecting reliable detection performance. These findings demonstrate the strength of multi-scale feature fusion combined with hybrid attention in enhancing discriminability and noise resistance. However, architecture remains more complex and computationally demanding than conventional CNNs, which may limit its deployment in real-time or embedded industrial settings. In contrast, the proposed framework achieves comparable or superior performance using a more lightweight dual-branch design, making it more scalable and practical for real-world applications.</p></sec><sec id="sec3dot5-sensors-25-05866"><title>3.5. Validation of Proposed Model on Public Dataset</title><p>To assess the generalization capability of the proposed model, it was validated using the publicly available Paderborn bearing dataset. The classification report, summarized in <xref rid="sensors-25-05866-t005" ref-type="table">Table 5</xref>, highlights high accuracy, precision, recall, and F1-scores across all fault categories. The corresponding visual results include confusion matrices <xref rid="sensors-25-05866-f017" ref-type="fig">Figure 17</xref> for both real and artificial fault data, t-SNE plots <xref rid="sensors-25-05866-f017" ref-type="fig">Figure 17</xref> illustrating clear feature separability, and ROC curves demonstrating strong class-wise discriminative performance as shown in <xref rid="sensors-25-05866-f017" ref-type="fig">Figure 17</xref>. These results confirm the model&#8217;s robustness and reliability on benchmark datasets. To assess external validity, the Paderborn dataset was employed without fine-tuning, using comparable health states (healthy, outer race fault, inner race fault) that conceptually align with the in-house milling dataset. The proposed hybrid model achieved 99.83% accuracy under this cross-dataset evaluation, outperforming CNN-only and ViT-only baselines. This demonstrates the framework&#8217;s strong generalization capability across different acquisition setups and operating conditions.</p><p>In summary, the proposed ViT&#8211;ResNet-18 hybrid model is evaluated through ablation studies and comparative analysis with standalone ViT, 1D-CNN, CWT-CNN, CNN-LSTM, CNN-CAE, ViT-Attention, and 1D&#8211;2D CNN fusion architectures. While these individual models demonstrate notable strengths&#8212;for instance, CNN-CAE effectively reduces noise via autoencoding, ViT-Attention captures inter-modal dependencies through self-attention, and 1D&#8211;2D CNN Fusion enhances discriminability with multi-scale feature integration&#8212;they fall short in accurately distinguishing overlapping or subtle fault types, particularly under limited or noisy data conditions. Misclassifications remain evident in closely related categories such as NI, N, and TF, as revealed by their confusion matrices and t-SNE plots. These limitations highlight the need for a unified architecture capable of capturing both detailed local patterns and long-range dependencies, while remaining computationally efficient for deployment. The proposed framework addresses this by combining ResNet-18 for localized spatial features with ViT for global contextual learning, supported by advanced preprocessing techniques such as log-CWT transformation, mean removal filtering, and Canny edge enhancement. Feature fusion produces a rich, discriminative representation, which is classified using a stacking ensemble composed of diverse base learners and a meta-learner to refine predictions. The model&#8217;s effectiveness is confirmed not only on real-world MCT datasets but also through validation on the publicly available Paderborn bearing dataset, where it demonstrates consistent fault classification across both real and artificial conditions. In addition to accuracy and robustness, computational efficiency is critical for industrial deployment. The proposed hybrid ResNet-18 + ViT model demonstrates a favorable trade-off between performance and efficiency, with 27.4 M parameters compared to &gt;85 M in pure ViT models, a training convergence time of 28 min (35% faster than ViT-only), and an average inference latency of 12.3 ms per sample. These results confirm the suitability of the framework for near real-time fault diagnosis under practical hardware constraints. Furthermore, the simulation results demonstrate consistent performance across multiple scenarios, including normal and fault conditions of tools, gears, and bearings. At higher spindle speeds, the framework preserved classification accuracy above 99%, showing resilience to dynamic operating conditions. Under artificially introduced noise, accuracy declined slightly (by 1.2%) but remained significantly higher than conventional CNN-based approaches. Ablation studies further confirm that hybrid CNN&#8211;ViT fusion outperforms single-model baselines, particularly in separating closely related fault categories such as GF vs. GFI. These findings, together with the comparative evaluation against CNN-CAE, ViT-Attention, and 1D&#8211;2D CNN Fusion, validate the scalability, robustness, and practical potential of the proposed framework across a broad spectrum of industrial scenarios. To examine robustness under realistic operating conditions, Gaussian noise was injected into vibration signals at different SNR levels of 30 dB, 20 dB, and 10 dB. The proposed framework maintained excellent performance at 30 dB and 20 dB, with only slight reductions in accuracy and F1-scores relative to clean signals. Even at 10 dB, where noise heavily overlaps with fault signatures, the framework demonstrated stable predictions and significantly outperformed CNN- and ViT-only baselines. These findings highlight that the combination of log-CWT scalograms, Canny edge enhancement, and hybrid dual-branch feature extraction ensures strong resilience to sensor noise and environmental disturbances.</p><p>Despite its strong performance, practical deployment of the proposed framework faces several challenges. First, the computational demands of hybrid CNN&#8211;ViT models may limit real-time use in resource-constrained environments; lightweight variants, pruning, and knowledge distillation can alleviate this issue. Second, sensor noise and environmental disturbances remain unavoidable in industrial setups; robust preprocessing and adaptive filtering are critical to maintain reliability. Third, hardware limitations such as restricted memory and processing power on embedded systems may affect scalability; solutions include edge computing platforms, hardware accelerators, or FPGA/ASIC-based implementations. Addressing these challenges will be essential for translating the proposed framework from controlled lab settings to real-world industrial environments.</p></sec></sec><sec sec-type="conclusions" id="sec4-sensors-25-05866"><title>4. Conclusions</title><p>Milling cutting tools are vital in modern manufacturing, requiring reliable fault diagnosis systems to reduce downtime and ensure product quality. Traditional methods often fall short due to noisy signals, limited labeled data, and the laborious, risky, and costly process of fault data collection. This study proposes a robust fault-diagnosis framework that couples dual-branch deep feature learning with a meta-learning ensemble. Raw vibration signals undergo mean removal, are transformed into logarithmic CWT scalograms, and then are refined with Canny edge filtering to sharpen boundaries and suppress artifacts. A convolutional branch (ResNet-18) and a transformer branch (ViT) extract complementary local and global cues, which are fused into a compact, discriminative embedding. On top of these fused features, three complementary base learners, Linear SVM, Logistic Regression, and a shallow MLP, generate preliminary predictions that are combined by a validation-guided, meta-learned weighted voting module rather than a fixed meta-classifier. The proposed method effectively addresses data scarcity, noise, and computational challenges, achieving 99.78% accuracy on real-world vibration signals. It also adapts well to varying fault conditions and machine components, making it suitable for industrial deployment. An ablation study further validates its performance, showing superior accuracy, precision, recall, and F1-scores compared to 1D-CNN, CWT-CNN, CNN-LSTM, CNN-CAE, ViT-Attention, and 1D&#8211;2D CNN fusion models. These findings underscore the framework&#8217;s robustness, scalability, and efficiency in complex industrial fault diagnosis tasks.</p><p>Despite its performance, the framework has certain limitations that present opportunities for future research. The model validation on diverse datasets and real-time implementation on edge devices for practical use. In future work, this framework can be extended beyond milling machines to other industrial rotating systems, such as pumps, bearings, and gears, to further validate its generalization capability. Additionally, incorporating multimodal sensor fusion (e.g., combining vibration with acoustic emission signals) may improve robustness under complex operating conditions. Lightweight architecture and model compression techniques could also be explored to enable real-time implementation on embedded systems. Investigating self-supervised and semi-supervised learning strategies will help reduce dependence on labeled data, making the approach more practical for large-scale industrial adoption. Finally, resilience against hardware-level disturbances such as sensor degradation and actuator malfunctions will be investigated to further enhance the practical reliability of the framework.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>M.F.S., W.Z., M.U., J.-Y.K. and J.-M.K.; methodology, M.F.S., W.Z., M.U. and J.-M.K.; validation, M.F.S., W.Z., M.U., J.-Y.K. and J.-M.K.; formal analysis, M.F.S., W.Z., M.U., J.-Y.K. and J.-M.K.; resources, M.F.S., W.Z., M.U. and J.-M.K.; writing&#8212;original draft preparation, M.F.S., W.Z., M.U., J.-Y.K. and J.-M.K.; writing&#8212;review and editing, J.-M.K.; visualization, M.F.S. and J.-M.K.; project administration, J.-M.K.; funding acquisition, J.-M.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Jae-Young Kim and Jong-Myon Kim were employed by the company Prognosis and Diagnostic Technologies Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><glossary><title>Nomenclature</title><array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCT</td><td align="left" valign="middle" rowspan="1" colspan="1">Milling Cutting Tool</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ViT</td><td align="left" valign="middle" rowspan="1" colspan="1">Vision Transformer</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ANN</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CAE</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Autoencoder</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ReLU</td><td align="left" valign="middle" rowspan="1" colspan="1">Rectified Linear Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DL</td><td align="left" valign="middle" rowspan="1" colspan="1">Deep Learning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AE</td><td align="left" valign="middle" rowspan="1" colspan="1">Acoustic Emission</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UIAI</td><td align="left" valign="middle" rowspan="1" colspan="1">Ulsan Industrial Artificial Lab</td></tr></tbody></array></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05866"><label>1.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bagga</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>Makhesana</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Mishra</surname><given-names>A.B.</given-names></name><name name-style="western"><surname>Marvaniya</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Patel</surname><given-names>K.M.</given-names></name></person-group><article-title>Tool Wear Detection Using Computer Vision System in Machining</article-title><source>Advances in Manufacturing Technology and Management</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2023</year><fpage>19</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1007/978-981-16-9523-0_3</pub-id></element-citation></ref><ref id="B2-sensors-25-05866"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rezazadeh</surname><given-names>N.</given-names></name><name name-style="western"><surname>De Luca</surname><given-names>A.</given-names></name><name name-style="western"><surname>Perfetto</surname><given-names>D.</given-names></name><name name-style="western"><surname>Salami</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Lamanna</surname><given-names>G.</given-names></name></person-group><article-title>Systematic critical review of structural health monitoring under environmental and operational variability: Approaches for baseline compensation, adaptation, and reference-free techniques</article-title><source>Smart Mater. Struct.</source><year>2025</year><volume>34</volume><fpage>073001</fpage><pub-id pub-id-type="doi">10.1088/1361-665X/ade7db</pub-id></element-citation></ref><ref id="B3-sensors-25-05866"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yue</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>ACWGAN-GP for milling tool breakage monitoring with imbalanced data</article-title><source>Robot. Comput. Integr. Manuf.</source><year>2024</year><volume>85</volume><fpage>102624</fpage><pub-id pub-id-type="doi">10.1016/j.rcim.2023.102624</pub-id></element-citation></ref><ref id="B4-sensors-25-05866"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Umar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Siddique</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Ullah</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.-M.</given-names></name></person-group><article-title>Milling Machine Fault Diagnosis Using Acoustic Emission and Hybrid Deep Learning with Feature Optimization</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>10404</elocation-id><pub-id pub-id-type="doi">10.3390/app142210404</pub-id></element-citation></ref><ref id="B5-sensors-25-05866"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name></person-group><article-title>Study of Tool Wear Monitoring Using Machine Vision</article-title><source>Autom. Control Comput. Sci.</source><year>2020</year><volume>54</volume><fpage>259</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.3103/S0146411620030062</pub-id></element-citation></ref><ref id="B6-sensors-25-05866"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Umar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ullah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Saleem</surname><given-names>F.</given-names></name><name name-style="western"><surname>Farooq Siddique</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.-M.</given-names></name></person-group><article-title>Advanced Fault Diagnosis in Milling Machines Using Acoustic Emission and Transfer Learning</article-title><source>IEEE Access</source><year>2025</year><volume>13</volume><fpage>100776</fpage><lpage>100790</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2025.3578248</pub-id></element-citation></ref><ref id="B7-sensors-25-05866"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>You</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name></person-group><article-title>Machine vision based condition monitoring and fault diagnosis of machine tools using information from machined surface texture: A review</article-title><source>Mech. Syst. Signal Process.</source><year>2022</year><volume>164</volume><fpage>108068</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2021.108068</pub-id></element-citation></ref><ref id="B8-sensors-25-05866"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren&#233; de Jes&#250;s</surname><given-names>R.-T.</given-names></name><name name-style="western"><surname>Gilberto</surname><given-names>H.-R.</given-names></name><name name-style="western"><surname>Iv&#225;n</surname><given-names>T.-V.</given-names></name><name name-style="western"><surname>Juan Carlos</surname><given-names>J.-C.</given-names></name></person-group><article-title>Driver current analysis for sensorless tool breakage monitoring of CNC milling machines</article-title><source>Int. J. Mach. Tools Manuf.</source><year>2003</year><volume>43</volume><fpage>1529</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1016/j.ijmachtools.2003.08.004</pub-id></element-citation></ref><ref id="B9-sensors-25-05866"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pimenov</surname><given-names>D.Y.</given-names></name><name name-style="western"><surname>Bustillo</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wojciechowski</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>V.S.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Kunto&#287;lu</surname><given-names>M.</given-names></name></person-group><article-title>Artificial intelligence systems for tool condition monitoring in machining: Analysis and critical review</article-title><source>J. Intell. Manuf.</source><year>2023</year><volume>34</volume><fpage>2079</fpage><lpage>2121</lpage><pub-id pub-id-type="doi">10.1007/s10845-022-01923-2</pub-id></element-citation></ref><ref id="B10-sensors-25-05866"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bagga</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>Makhesana</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Patel</surname><given-names>K.M.</given-names></name></person-group><article-title>A novel approach of combined edge detection and segmentation for tool wear measurement in machining</article-title><source>Prod. Eng.</source><year>2021</year><volume>15</volume><fpage>519</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1007/s11740-021-01035-5</pub-id></element-citation></ref><ref id="B11-sensors-25-05866"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.-F.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>M.</given-names></name></person-group><article-title>A Hybrid Generalization Network for Intelligent Fault Diagnosis of Rotating Machinery Under Unseen Working Conditions</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2021</year><volume>70</volume><fpage>3520011</fpage><pub-id pub-id-type="doi">10.1109/TIM.2021.3088489</pub-id></element-citation></ref><ref id="B12-sensors-25-05866"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Warke</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bongale</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kamat</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kotecha</surname><given-names>K.</given-names></name><name name-style="western"><surname>Selvachandran</surname><given-names>G.</given-names></name><name name-style="western"><surname>Abraham</surname><given-names>A.</given-names></name></person-group><article-title>Improving the useful life of tools using active vibration control through data-driven approaches: A systematic literature review</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>128</volume><fpage>107367</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.107367</pub-id></element-citation></ref><ref id="B13-sensors-25-05866"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>R.X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>An integrated fault diagnosis and prognosis approach for predictive maintenance of wind turbine bearing with limited samples</article-title><source>Renew. Energy</source><year>2020</year><volume>145</volume><fpage>642</fpage><lpage>650</lpage><pub-id pub-id-type="doi">10.1016/j.renene.2019.06.103</pub-id></element-citation></ref><ref id="B14-sensors-25-05866"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>F.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>F.</given-names></name></person-group><article-title>An Integrated Fault Diagnosis Method for Rotating Machinery Based on Improved Multivariate Multiscale Amplitude-Aware Permutation Entropy and Uniform Phase Empirical Mode Decomposition</article-title><source>Shock. Vib.</source><year>2021</year><volume>2021</volume><fpage>2098892</fpage><pub-id pub-id-type="doi">10.1155/2021/2098892</pub-id></element-citation></ref><ref id="B15-sensors-25-05866"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>N.</given-names></name><name name-style="western"><surname>Nandi</surname><given-names>A.K.</given-names></name></person-group><article-title>Applications of machine learning to machine fault diagnosis: A review and roadmap</article-title><source>Mech. Syst. Signal Process.</source><year>2020</year><volume>138</volume><fpage>106587</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2019.106587</pub-id></element-citation></ref><ref id="B16-sensors-25-05866"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mian</surname><given-names>T.</given-names></name><name name-style="western"><surname>Choudhary</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fatima</surname><given-names>S.</given-names></name></person-group><article-title>Multi-Sensor Fault Diagnosis for Misalignment and Unbalance Detection Using Machine Learning</article-title><source>IEEE Trans. Ind. Appl.</source><year>2023</year><volume>59</volume><fpage>5749</fpage><lpage>5759</lpage><pub-id pub-id-type="doi">10.1109/TIA.2023.3286833</pub-id></element-citation></ref><ref id="B17-sensors-25-05866"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kunto&#287;lu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Aslan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pimenov</surname><given-names>D.Y.</given-names></name><name name-style="western"><surname>Usca</surname><given-names>&#220;.A.</given-names></name><name name-style="western"><surname>Salur</surname><given-names>E.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Mikolajczyk</surname><given-names>T.</given-names></name><name name-style="western"><surname>Giasin</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kap&#322;onek</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>S.</given-names></name></person-group><article-title>A Review of Indirect Tool Condition Monitoring Systems and Decision-Making Methods in Turning: Critical Analysis and Trends</article-title><source>Sensors</source><year>2020</year><volume>21</volume><elocation-id>108</elocation-id><pub-id pub-id-type="doi">10.3390/s21010108</pub-id><pub-id pub-id-type="pmid">33375340</pub-id><pub-id pub-id-type="pmcid">PMC7794675</pub-id></element-citation></ref><ref id="B18-sensors-25-05866"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Datta</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dutta</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pal</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Sen</surname><given-names>R.</given-names></name></person-group><article-title>Progressive cutting tool wear detection from machined surface images using Voronoi tessellation method</article-title><source>J. Mater. Process. Technol</source><year>2013</year><volume>213</volume><fpage>2339</fpage><lpage>2349</lpage><pub-id pub-id-type="doi">10.1016/j.jmatprotec.2013.07.008</pub-id></element-citation></ref><ref id="B19-sensors-25-05866"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jurkovic</surname><given-names>J.</given-names></name><name name-style="western"><surname>Korosec</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kopac</surname><given-names>J.</given-names></name></person-group><article-title>New approach in tool wear measuring technique using CCD vision system</article-title><source>Int. J. Mach. Tools Manuf.</source><year>2005</year><volume>45</volume><fpage>1023</fpage><lpage>1030</lpage><pub-id pub-id-type="doi">10.1016/j.ijmachtools.2004.11.030</pub-id></element-citation></ref><ref id="B20-sensors-25-05866"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haber</surname><given-names>R.E.</given-names></name><name name-style="western"><surname>Jim&#233;nez</surname><given-names>J.E.</given-names></name><name name-style="western"><surname>Peres</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Alique</surname><given-names>J.R.</given-names></name></person-group><article-title>An investigation of tool-wear monitoring in a high-speed machining process</article-title><source>Sens. Actuators A Phys.</source><year>2004</year><volume>116</volume><fpage>539</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/j.sna.2004.05.017</pub-id></element-citation></ref><ref id="B21-sensors-25-05866"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Song</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name></person-group><article-title>Tool Breakage Detection using Deep Learning</article-title><source>Proceedings of the 2018 IEEE International Conference on Big Data, Cloud Computing, Data Science &amp; Engineering (BCD)</source><conf-loc>Yonago, Japan</conf-loc><conf-date>12&#8211;13 July 2018</conf-date></element-citation></ref><ref id="B22-sensors-25-05866"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>G.</given-names></name></person-group><article-title>A Comprehensive Review of Signal Processing and Machine Learning Technologies for UHF PD Detection and Diagnosis (I): Preprocessing and Localization Approaches</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>69876</fpage><lpage>69904</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3077483</pub-id></element-citation></ref><ref id="B23-sensors-25-05866"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siddique</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Ullah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.-M.</given-names></name></person-group><article-title>A Deep Learning Approach for Fault Diagnosis in Centrifugal Pumps through Wavelet Coherent Analysis and S-Transform Scalograms with CNN-KAN</article-title><source>Comput. Mater. Contin.</source><year>2025</year><volume>84</volume><fpage>3577</fpage><lpage>3603</lpage><pub-id pub-id-type="doi">10.32604/cmc.2025.065326</pub-id></element-citation></ref><ref id="B24-sensors-25-05866"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bergs</surname><given-names>T.</given-names></name><name name-style="western"><surname>Holst</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>P.</given-names></name><name name-style="western"><surname>Augspurger</surname><given-names>T.</given-names></name></person-group><article-title>Digital image processing with deep learning for automated cutting tool wear detection</article-title><source>Procedia Manuf.</source><year>2020</year><volume>48</volume><fpage>947</fpage><lpage>958</lpage><pub-id pub-id-type="doi">10.1016/j.promfg.2020.05.134</pub-id></element-citation></ref><ref id="B25-sensors-25-05866"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rezazadeh</surname><given-names>N.</given-names></name><name name-style="western"><surname>De Oliveira</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lamanna</surname><given-names>G.</given-names></name><name name-style="western"><surname>Perfetto</surname><given-names>D.</given-names></name><name name-style="western"><surname>De Luca</surname><given-names>A.</given-names></name></person-group><article-title>WaveCORAL-DCCA: A Scalable Solution for Rotor Fault Diagnosis Across Operational Variabilities</article-title><source>Electronics</source><year>2025</year><volume>14</volume><elocation-id>3146</elocation-id><pub-id pub-id-type="doi">10.3390/electronics14153146</pub-id></element-citation></ref><ref id="B26-sensors-25-05866"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zaman</surname><given-names>W.</given-names></name><name name-style="western"><surname>Siddique</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>S.U.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.-M.</given-names></name></person-group><article-title>A new dual-input CNN for multimodal fault classification using acoustic emission and vibration signals</article-title><source>Eng. Fail. Anal.</source><year>2025</year><volume>179</volume><fpage>109787</fpage><pub-id pub-id-type="doi">10.1016/j.engfailanal.2025.109787</pub-id></element-citation></ref><ref id="B27-sensors-25-05866"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Neurocomputing</surname><given-names>Y.C.</given-names></name></person-group><article-title>A time series transformer based method for the rotating machinery fault diagnosis</article-title><source>Neurocomputing</source><year>2022</year><volume>494</volume><fpage>379</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2022.04.111</pub-id></element-citation></ref><ref id="B28-sensors-25-05866"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name></person-group><article-title>Method for Diagnosis of Acute Lymphoblastic Leukemia Based on ViT-CNN Ensemble Model</article-title><source>Comput. Intell. Neurosci.</source><year>2021</year><volume>2021</volume><fpage>7529893</fpage><pub-id pub-id-type="doi">10.1155/2021/7529893</pub-id><pub-id pub-id-type="pmid">34471407</pub-id><pub-id pub-id-type="pmcid">PMC8405335</pub-id></element-citation></ref><ref id="B29-sensors-25-05866"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name></person-group><article-title>Enhancing multi-type fault diagnosis in lithium-ion battery systems: Vision transformer-based transfer learning approach</article-title><source>J. Power Sources</source><year>2024</year><volume>624</volume><fpage>235610</fpage><pub-id pub-id-type="doi">10.1016/j.jpowsour.2024.235610</pub-id></element-citation></ref><ref id="B30-sensors-25-05866"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name></person-group><article-title>Detection of coronary heart disease based on heart sound and hybrid Vision Transformer</article-title><source>Appl. Acoust.</source><year>2025</year><volume>230</volume><fpage>110420</fpage><pub-id pub-id-type="doi">10.1016/j.apacoust.2024.110420</pub-id></element-citation></ref><ref id="B31-sensors-25-05866"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shuai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>P.</given-names></name></person-group><article-title>Self-Supervised Vision Transformer for Intelligent Fault Diagnosis of Gearbox with Limited Samples</article-title><source>Proceedings of the 2023 Global Reliability and Prognostics and Health Management Conference (PHM-Hangzhou)</source><conf-loc>Hangzhou, China</conf-loc><conf-date>12&#8211;15 October 2023</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/PHM-Hangzhou58797.2023.10482534</pub-id></element-citation></ref><ref id="B32-sensors-25-05866"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>L.</given-names></name></person-group><article-title>Time-series vision transformer based on cross space-time attention for fault diagnosis in fused deposition modelling with reconstruction of layer-wise data</article-title><source>J. Manuf. Process.</source><year>2024</year><volume>115</volume><fpage>240</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/j.jmapro.2024.01.082</pub-id></element-citation></ref><ref id="B33-sensors-25-05866"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shuai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>P.</given-names></name></person-group><article-title>Few-shot fault diagnosis of axial piston pump based on prior knowledge-embedded meta learning vision transformer under variable operating conditions</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>269</volume><fpage>126452</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2025.126452</pub-id></element-citation></ref><ref id="B34-sensors-25-05866"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Few-shot transfer learning for intelligent fault diagnosis of machine</article-title><source>Measurement</source><year>2020</year><volume>166</volume><fpage>108202</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2020.108202</pub-id></element-citation></ref><ref id="B35-sensors-25-05866"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdelhamid</surname><given-names>B.</given-names></name><name name-style="western"><surname>Mohamed</surname><given-names>C.</given-names></name></person-group><article-title>Robust Fuzzy Adaptive Fault-Tolerant Control for a Class of Second-Order Nonlinear Systems</article-title><source>Int. J. Adapt. Control Signal Process.</source><year>2025</year><volume>39</volume><fpage>15</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1002/acs.3916</pub-id></element-citation></ref><ref id="B36-sensors-25-05866"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>ALTobi</surname><given-names>M.A.S.</given-names></name><name name-style="western"><surname>Bevan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wallace</surname><given-names>P.</given-names></name><name name-style="western"><surname>Harrison</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ramachandran</surname><given-names>K.P.</given-names></name></person-group><article-title>Fault diagnosis of a centrifugal pump using MLP-GABP and SVM with CWT</article-title><source>Eng. Sci. Technol. Int. J.</source><year>2019</year><volume>22</volume><fpage>854</fpage><lpage>861</lpage><pub-id pub-id-type="doi">10.1016/j.jestch.2019.01.005</pub-id></element-citation></ref><ref id="B37-sensors-25-05866"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qasim Almaliki</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Abd</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Lafta</surname><given-names>I.A.</given-names></name><name name-style="western"><surname>Din</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ghazali</surname><given-names>O.</given-names></name><name name-style="western"><surname>Almaliki</surname><given-names>J.Q.</given-names></name><name name-style="western"><surname>Utama</surname><given-names>S.</given-names></name></person-group><article-title>Application of the Canny Filter in Digital Steganography</article-title><source>J. Adv. Res. Comput. Appl.</source><year>2024</year><volume>35</volume><fpage>21</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.37934/arca.35.1.2130</pub-id></element-citation></ref><ref id="B38-sensors-25-05866"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>J.</given-names></name></person-group><article-title>An Edge Detection Method for Welding Pool Based on An Improved Canny Algorithm</article-title><source>J. Phys. Conf. Ser.</source><year>2024</year><volume>2785</volume><fpage>012013</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/2785/1/012013</pub-id></element-citation></ref><ref id="B39-sensors-25-05866"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rauscher</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>J.</given-names></name><name name-style="western"><surname>Devaraju</surname><given-names>M.</given-names></name><name name-style="western"><surname>Endisch</surname><given-names>C.</given-names></name></person-group><article-title>Deep learning and data augmentation for partial discharge detection in electrical machines</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>133</volume><fpage>108074</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.108074</pub-id></element-citation></ref><ref id="B40-sensors-25-05866"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>A Survey on Vision Transformer</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>87</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3152247</pub-id><pub-id pub-id-type="pmid">35180075</pub-id></element-citation></ref><ref id="B41-sensors-25-05866"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shahid</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.-C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.-Y.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>K.-L.</given-names></name></person-group><article-title>Hybrid CNN-ViT architecture to exploit spatio-temporal feature for fire recognition trained through transfer learning</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>84</volume><fpage>4703</fpage><lpage>4732</lpage><pub-id pub-id-type="doi">10.1007/s11042-024-18752-5</pub-id></element-citation></ref><ref id="B42-sensors-25-05866"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Azad</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kazerouni</surname><given-names>A.</given-names></name><name name-style="western"><surname>Heidari</surname><given-names>M.</given-names></name><name name-style="western"><surname>Aghdam</surname><given-names>E.K.</given-names></name><name name-style="western"><surname>Molaei</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jose</surname><given-names>A.</given-names></name><name name-style="western"><surname>Roy</surname><given-names>R.</given-names></name><name name-style="western"><surname>Merhof</surname><given-names>D.</given-names></name></person-group><article-title>Advances in medical image analysis with vision Transformers: A comprehensive review</article-title><source>Med. Image Anal.</source><year>2024</year><volume>91</volume><fpage>103000</fpage><pub-id pub-id-type="doi">10.1016/j.media.2023.103000</pub-id><pub-id pub-id-type="pmid">37883822</pub-id></element-citation></ref><ref id="B43-sensors-25-05866"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name></person-group><article-title>A novel framework for motor bearing fault diagnosis based on multi-transformation domain and multi-source data</article-title><source>Knowl. Based Syst.</source><year>2024</year><volume>283</volume><fpage>111205</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2023.111205</pub-id></element-citation></ref><ref id="B44-sensors-25-05866"><label>44.</label><element-citation publication-type="book"><std>ISO 8688-2</std><source>Tool Life Testing in Milling&#8212;Part 2: End Milling</source><publisher-name>ISO</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>1989</year></element-citation></ref><ref id="B45-sensors-25-05866"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>A Fault Diagnosis Method for Rolling Bearing Based on 1D-ViT Model</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>39664</fpage><lpage>39674</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3268534</pub-id></element-citation></ref><ref id="B46-sensors-25-05866"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Bearing fault diagnosis based on vibro-acoustic data fusion and 1D-CNN network</article-title><source>Measurement</source><year>2021</year><volume>173</volume><fpage>108518</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2020.108518</pub-id></element-citation></ref><ref id="B47-sensors-25-05866"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pei</surname><given-names>X.</given-names></name></person-group><article-title>Aeroengine Control System Sensor Fault Diagnosis Based on CWT and CNN</article-title><source>Math. Probl. Eng.</source><year>2020</year><volume>2020</volume><fpage>5357146</fpage><pub-id pub-id-type="doi">10.1155/2020/5357146</pub-id></element-citation></ref><ref id="B48-sensors-25-05866"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>X.</given-names></name></person-group><article-title>A novel fault diagnosis method based on CNN and LSTM and its application in fault diagnosis for complex systems</article-title><source>Artif. Intell. Rev.</source><year>2022</year><volume>55</volume><fpage>1289</fpage><lpage>1315</lpage><pub-id pub-id-type="doi">10.1007/s10462-021-09993-z</pub-id></element-citation></ref><ref id="B49-sensors-25-05866"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>J.</given-names></name></person-group><article-title>Fault diagnosis of hydro-turbine via the incorporation of bayesian algorithm optimized CNN-LSTM neural network</article-title><source>Energy</source><year>2024</year><volume>290</volume><fpage>130326</fpage><pub-id pub-id-type="doi">10.1016/j.energy.2024.130326</pub-id><pub-id pub-id-type="pmcid">PMC11511977</pub-id><pub-id pub-id-type="pmid">39455870</pub-id></element-citation></ref><ref id="B50-sensors-25-05866"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jana</surname><given-names>D.</given-names></name><name name-style="western"><surname>Patil</surname><given-names>J.</given-names></name><name name-style="western"><surname>Herkal</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nagarajaiah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Duenas-Osorio</surname><given-names>L.</given-names></name></person-group><article-title>CNN and Convolutional Autoencoder (CAE) based real-time sensor fault detection, localization, and correction</article-title><source>Mech. Syst. Signal Process.</source><year>2022</year><volume>169</volume><fpage>108723</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2021.108723</pub-id></element-citation></ref><ref id="B51-sensors-25-05866"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>J.</given-names></name></person-group><article-title>An intelligent fault diagnosis method for rotor-bearing system using small labeled infrared thermal images and enhanced CNN transferred from CAE</article-title><source>Adv. Eng. Inform.</source><year>2020</year><volume>46</volume><fpage>101150</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2020.101150</pub-id></element-citation></ref><ref id="B52-sensors-25-05866"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name></person-group><article-title>Multi-Sensor Data Fusion Method Based on Self-Attention Mechanism</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>11992</elocation-id><pub-id pub-id-type="doi">10.3390/app132111992</pub-id></element-citation></ref><ref id="B53-sensors-25-05866"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name></person-group><article-title>A multi-scale feature extraction and fusion method for bearing fault diagnosis based on hybrid attention mechanism</article-title><source>Signal Image Video Process.</source><year>2024</year><volume>18</volume><fpage>31</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1007/s11760-024-03129-w</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05866-f001" orientation="portrait"><label>Figure 1</label><caption><p>High-level block diagram of the proposed hybrid deep learning framework.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g001.jpg"/></fig><fig position="float" id="sensors-25-05866-f002" orientation="portrait"><label>Figure 2</label><caption><p>The flowchart illustrates the proposed method for fault diagnosis in milling machines.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g002.jpg"/></fig><fig position="float" id="sensors-25-05866-f003" orientation="portrait"><label>Figure 3</label><caption><p>Samples of log-CWT scalograms, (<bold>a</bold>) Tool fault and (<bold>b</bold>) Gear fault from the vibration signals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g003.jpg"/></fig><fig position="float" id="sensors-25-05866-f004" orientation="portrait"><label>Figure 4</label><caption><p>Working of vision transformer.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g004.jpg"/></fig><fig position="float" id="sensors-25-05866-f005" orientation="portrait"><label>Figure 5</label><caption><p>Feature pool for ResNet-18 and ViT features.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g005.jpg"/></fig><fig position="float" id="sensors-25-05866-f006" orientation="portrait"><label>Figure 6</label><caption><p>ResNet-18 + ViT Feature Fusion with Meta-Learned, Validation-Weighted Voting (SVM, Logistic Regression, Shallow MLP) for final prediction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g006.jpg"/></fig><fig position="float" id="sensors-25-05866-f007" orientation="portrait"><label>Figure 7</label><caption><p>Experimental setup displaying the milling machine equipped with vibration sensors.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g007.jpg"/></fig><fig position="float" id="sensors-25-05866-f008" orientation="portrait"><label>Figure 8</label><caption><p>Samples used in the study: (<bold>a</bold>) Workpiece samples, (<bold>b</bold>) unprocessed workpieces, (<bold>c</bold>) workpieces after the milling process.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g008.jpg"/></fig><fig position="float" id="sensors-25-05866-f009" orientation="portrait"><label>Figure 9</label><caption><p>Vibration TD signals of various fault classes: (<bold>a</bold>) BF, (<bold>b</bold>) BFI, (<bold>c</bold>) GF, (<bold>d</bold>) GFI, (<bold>e</bold>) N, (<bold>f</bold>) NI, and (<bold>g</bold>) TF.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g009a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g009b.jpg"/></fig><fig position="float" id="sensors-25-05866-f010" orientation="portrait"><label>Figure 10</label><caption><p>The tool faulted during the experiment.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g010.jpg"/></fig><fig position="float" id="sensors-25-05866-f011" orientation="portrait"><label>Figure 11</label><caption><p>Bearing fault during the experiment.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g011.jpg"/></fig><fig position="float" id="sensors-25-05866-f012" orientation="portrait"><label>Figure 12</label><caption><p>Gear fault during the experiment (milling cutting).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g012.jpg"/></fig><fig position="float" id="sensors-25-05866-f013" orientation="portrait"><label>Figure 13</label><caption><p>Training and validation (<bold>a</bold>) accuracy and (<bold>b</bold>) losses against the number of epochs.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g013.jpg"/></fig><fig position="float" id="sensors-25-05866-f014" orientation="portrait"><label>Figure 14</label><caption><p>Confusion matrices of the (<bold>a</bold>) Proposed model, with (<bold>b</bold>) CNN-LSTM, (<bold>c</bold>) CNN-CAE, (<bold>d</bold>) ViT(Ablation Study), (<bold>e</bold>) ViT-Attention, and (<bold>f</bold>) 1D&#8211;2D CNN fusion.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g014.jpg"/></fig><fig position="float" id="sensors-25-05866-f015" orientation="portrait"><label>Figure 15</label><caption><p>t-SNE comparison of the (<bold>a</bold>) Proposed model, with (<bold>b</bold>) CNN-LSTM, (<bold>c</bold>) CNN-CAE, (<bold>d</bold>) ViT (Ablation Study), (<bold>e</bold>) ViT-Attention, and (<bold>f</bold>) 1D&#8211;2D CNN fusion.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g015.jpg"/></fig><fig position="float" id="sensors-25-05866-f016" orientation="portrait"><label>Figure 16</label><caption><p>ROC curves comparison of the (<bold>a</bold>) Proposed model, with (<bold>b</bold>) CNN-LSTM, (<bold>c</bold>) CNN-CAE, (<bold>d</bold>) ViT(Ablation Study), (<bold>e</bold>) ViT-Attention, and (<bold>f</bold>) 1D&#8211;2D CNN fusion.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g016.jpg"/></fig><fig position="float" id="sensors-25-05866-f017" orientation="portrait"><label>Figure 17</label><caption><p>Validation of the proposed model on the Paderborn benchmark dataset: (<bold>a</bold>,<bold>d</bold>) confusion matrices for real and artificial fault data, (<bold>b</bold>,<bold>e</bold>) t-SNE plots showing clear feature separability, and (<bold>c</bold>,<bold>f</bold>) ROC curves demonstrating strong class-wise discriminative performance.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05866-g017.jpg"/></fig><table-wrap position="float" id="sensors-25-05866-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05866-t001_Table 1</object-id><label>Table 1</label><caption><p>Features extraction using vision transformer.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Stage</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Component</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Output Shape</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Input</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Input Images</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 3, 224, 224)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 3, 224, 224)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Patch Embedding</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Patch Embedding</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Splits the image into 16 &#215; 16 patches; projects to a 384-dim embedding</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 196, 384)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Positional Encoding</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Positional Embeddings</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adds positional information to patch embeddings</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 196, 384)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Transformer Blocks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sequential</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Contains 12 transformer encoder blocks with multi-head self-attention and MLP layers</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 196, 384)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification Token</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CLS Token</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A special token representing the whole image</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 1, 384)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05866-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05866-t002_Table 2</object-id><label>Table 2</label><caption><p>Hybrid features fusion and classification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Stage</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Component</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Output Shape</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature Extraction <break/>(ResNet-18)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flatten Layer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-18 outputs are flattened into a 1D vector.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, ResNet-18_Feature_Size)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature Extraction (ViT)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CLS Token</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The ViT CLS token encodes the image as a feature vector.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, ViT_Feature_Size)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature Fusion</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concatenation Layer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concatenates ResNet-18 and ViT features along the feature dimension.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, ResNet-18_Feature_Size + ViT_Feature_Size)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully Connected<break/>Layer 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense Layer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A fully connected layer compresses the <break/>combined features to an intermediate size</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 512)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ReLU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Applies ReLU non-linearity to the <break/>intermediate feature vector.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 512)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully Connected<break/>Layer 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense Layer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maps the intermediate feature vector to<break/>the number of classes.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 7)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Converts logits to probabilities for<break/>classification.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 7)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Output</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Class Probabilities</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Final output probabilities for 7 classes.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Batch, 7)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05866-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05866-t003_Table 3</object-id><label>Table 3</label><caption><p>Data acquisition configuration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Condition</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Samples (VS)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Samples (Per Second)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Acquisition Time</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BFI</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GFI</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NI</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25,600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 min</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05866-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05866-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance metrics of the ablation studies (ViT and 1D CNN) models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Models</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1 Score (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ViT model</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.83</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1D-CNN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.71</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CWT-CNN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.72</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05866-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05866-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of performance metrics of proposed methods with CNN-LSTM, CNN-CAE, ViT-ttention, and 1D-2D CNN fusion models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Models</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Proposed</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CNN-LSTM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CNN-CAE</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ViT-Attention</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">1D-2D CNN Fusion</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Accuracy</bold> (<bold>%</bold>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.83</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Precision</bold> (<bold>%</bold>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.83</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.76</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Recall</bold> (<bold>%</bold>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.51</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>F1 score</bold> (<bold>%</bold>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.79</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Datasets</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Paderborn Artificial</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Paderborn Real</bold>
</td><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>UIAI MCT</bold>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>