<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473648</article-id><article-id pub-id-type="pmcid-ver">PMC12473648.1</article-id><article-id pub-id-type="pmcaid">12473648</article-id><article-id pub-id-type="pmcaiid">12473648</article-id><article-id pub-id-type="pmid">41012917</article-id><article-id pub-id-type="doi">10.3390/s25185678</article-id><article-id pub-id-type="publisher-id">sensors-25-05678</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Automated Remote Detection of Falls Using Direct Reconstruction of Optical Flow Principal Motion Parameters</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-5383-3030</contrib-id><name name-style="western"><surname>Karpuzov</surname><given-names initials="S">Simeon</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05678" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7028-7778</contrib-id><name name-style="western"><surname>Kalitzin</surname><given-names initials="S">Stiliyan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-05678" ref-type="aff">2</xref><xref rid="af3-sensors-25-05678" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5376-6729</contrib-id><name name-style="western"><surname>Georgieva</surname><given-names initials="O">Olga</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05678" ref-type="aff">1</xref><xref rid="af4-sensors-25-05678" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Trifonov</surname><given-names initials="A">Alex</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05678" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Stoyanov</surname><given-names initials="T">Tervel</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05678" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0205-585X</contrib-id><name name-style="western"><surname>Petkov</surname><given-names initials="G">George</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05678" ref-type="aff">1</xref><xref rid="c1-sensors-25-05678" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Khan</surname><given-names initials="JF">Jesmin Farzana</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Ndoye</surname><given-names initials="M">Mandoye</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05678"><label>1</label>GATE Institute, Sofia University, 1164 Sofia, Bulgaria; <email>simeon.karpuzov@gate-ai.eu</email> (S.K.); <email>olga.georgieva@gate-ai.eu</email> (O.G.); <email>aleks.trifonov@gate-ai.eu</email> (A.T.); <email>tervel.stoyanov@gate-ai.eu</email> (T.S.)</aff><aff id="af2-sensors-25-05678"><label>2</label>Stichting Epilepsie Instellingen Nederland (SEIN), Achterweg 5, 2103 SW Heemstede, The Netherlands; <email>skalitzin@sein.nl</email></aff><aff id="af3-sensors-25-05678"><label>3</label>Image Sciences Institute, University Medical Center Utrecht, Heidelberglaan 100, 3584 CX Utrecht, The Netherlands</aff><aff id="af4-sensors-25-05678"><label>4</label>Faculty of Mathematics and Infromatics, Sofia University, St. Kliment Ohridski, 1164 Sofia, Bulgaria</aff><author-notes><corresp id="c1-sensors-25-05678"><label>*</label>Correspondence: <email>georgi.petkov@gate-ai.eu</email>; Tel.: +359-888134817</corresp></author-notes><pub-date pub-type="epub"><day>11</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5678</elocation-id><history><date date-type="received"><day>29</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>02</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>09</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>11</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05678.pdf"/><abstract><sec sec-type="highlights"><title>Highlights</title><p>
<bold>What are the main findings?</bold>
<list list-type="bullet"><list-item><p>Falls can be reliably detected using optical flow video processing algorithms.</p></list-item><list-item><p>Real-time performance is enhanced by direct reconstruction of principal motion parameters.</p></list-item></list>
</p><p>
<bold>What is the implication of the main finding?</bold>
<list list-type="bullet"><list-item><p>The proposed algorithm allows for modular integration into existing patient care observation systems.</p></list-item><list-item><p>It provides non-obstructive, maintenance-free, and privacy-respecting tools for safety.</p></list-item></list>
</p></sec><sec><title>Abstract</title><p>Detecting and alerting for falls is a crucial component of both healthcare and assistive technologies. Wearable devices are vulnerable to damage and require regular inspection and maintenance. Manned video surveillance avoids these problems, but it involves constant labor-intensive attention and, in most cases, may interfere with the privacy of the observed individuals. To address this issue, in this work we introduce and evaluate a novel approach for fully automated fall detection. The presented technique uses direct reconstruction of principal motion parameters, avoiding the computationally expensive full optical flow reconstruction and still providing relevant descriptors for accurate detections. Our method is systematically compared with state-of-the-art techniques. Comparisons of detection accuracy, computational efficiency, and suitability for real-time applications are presented. Experimental results demonstrate notable improvements in accuracy while maintaining a lower computational cost compared to traditional methods, making our approach highly adaptable for real-world deployment. The findings highlight the robustness and universality of our model, suggesting its potential for integration into broader surveillance technologies. Future directions for development will include optimization for resource-constrained environments and deep learning enhancements to refine detection precision.</p></sec></abstract><kwd-group><kwd>fall detection</kwd><kwd>optical flow</kwd><kwd>real-time detection</kwd><kwd>video-surveillance</kwd><kwd>principal motion parameters</kwd></kwd-group><funding-group><award-group><funding-source>Horizon 2020 WIDESPREAD-2018-2020 TEAMING Phase 2 program</funding-source><award-id>857155</award-id></award-group><award-group><funding-source>&#8220;Research, Innovation and Digitalization for Smart Transformation&#8221; 2021-2027 (PRIDST)</funding-source><award-id>BG16RFPR002-1.014-0010-C01</award-id></award-group><award-group><funding-source>&#8220;Anna Teding van Berkhout Stichting&#8221;, Program 35401, Remote Detection of Motor Paroxysms (REDEMP)</funding-source></award-group><funding-statement>This research is part of the GATE project funded by the Horizon 2020 WIDESPREAD-2018-2020 TEAMING Phase 2 program under grant agreement no. 857155 and the program &#8220;Research, Innovation and Digitalization for Smart Transformation&#8221; 2021-2027 (PRIDST) under grant agreement no. BG16RFPR002-1.014-0010-C01. Stiliyan Kalitzin is partially funded by &#8220;Anna Teding van Berkhout Stichting&#8221;, Program 35401, Remote Detection of Motor Paroxysms (REDEMP).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05678"><title>1. Introduction</title><sec id="sec1dot1-sensors-25-05678"><title>1.1. Motivation</title><p>Detection of falls is an increasingly important task in the modern world. Approximately one-third of adults aged 65 and above in the European Union experience one fall annually [<xref rid="B1-sensors-25-05678" ref-type="bibr">1</xref>]. Falls are dangerous and often lead to serious injuries and health complications. In many cases, falling can be fatal [<xref rid="B2-sensors-25-05678" ref-type="bibr">2</xref>]. Falls frequently require medical attention, and people often need to be hospitalized [<xref rid="B2-sensors-25-05678" ref-type="bibr">2</xref>]. This rounds up to an annual cost of treatment around &#8364;25 billion, which is a substantial toll [<xref rid="B3-sensors-25-05678" ref-type="bibr">3</xref>]. Naturally, developing alerting and prevention strategies, protocols, methods, and technologies is an important endeavor that helps reduce the negative outcomes and costs related to falls in the elderly population as well as in specific groups of vulnerable individuals such as epileptic patients.</p><p>There are different approaches for automated fall detection that are used in practice. Frequently used devices include wearable sensors and ambient devices. Wearable sensors, which are the most accurate [<xref rid="B4-sensors-25-05678" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05678" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05678" ref-type="bibr">6</xref>], are widely used. They offer round the clock monitoring and instant response in the event of an emergency. Wearable sensors are affordable and can be customized based on the individual using them. Ambient sensors [<xref rid="B7-sensors-25-05678" ref-type="bibr">7</xref>] also offer substantial benefits&#8212;they require no compliance from the user, provide broad area coverage, and are usable continuously. In the current work, we use a visual-based sensor (camera) for data collection, specifically for the purpose of fall detection. When compared to wearables, camera-based systems offer several advantages. They can successfully identify falls in any individual present within the monitored space. Wearable devices presuppose that their user is already at risk of falling. Camera-based systems have no such assumption, thus broadening their field of application. Camera infrastructure is more robust and, once installed, requires little support and/or maintenance. Wearable devices need to be charged and placed properly. They may also pose certain issues of compromising comfort, privacy, and add stigmatizing effect for the individual wearing them. Remote sensors, such as cameras, will monitor and alert without obstructive effects. When compared to ambient devices, cameras are less expensive and require less maintenance while offering comparable benefits in terms of detection accuracy. There are several disadvantages when using cameras that are worth mentioning. They may be less accurate in certain scenarios due to unfavorable lighting conditions. Obstructions and limited coverage my further impact detection capability. Personal privacy is also impacted when working with video data. These are important points that need to be accounted for when selecting a sensor for the task.</p><p>Our method for fall detection works by processing video data with Optical Flow (OF) [<xref rid="B8-sensors-25-05678" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05678" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05678" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05678" ref-type="bibr">11</xref>] techniques. Such methods are widely used in different computer vision tasks, and their application for fall detection can offer various benefits. OF methods allow us to capture motion without the need for physical markers or wearable devices [<xref rid="B12-sensors-25-05678" ref-type="bibr">12</xref>]. If certain conditions are met, of methods allow for real-time analysis of movement [<xref rid="B13-sensors-25-05678" ref-type="bibr">13</xref>]. To work properly, Optical Flow methods only require simple and cheap hardware such as a basic USB camera, connected to a personal computer [<xref rid="B14-sensors-25-05678" ref-type="bibr">14</xref>]. OF techniques have been shown to perform well under a dynamic environment&#8212;complex scenes with multiple moving objects [<xref rid="B15-sensors-25-05678" ref-type="bibr">15</xref>]. Finally, Optical Flow methods can be combined with machine learning algorithms [<xref rid="B16-sensors-25-05678" ref-type="bibr">16</xref>] in order to better solve different computer vision tasks.</p></sec><sec id="sec1dot2-sensors-25-05678"><title>1.2. Related Works</title><p>Fall detection using camera-based systems is an increasingly important topic in the field of computer vision and digital healthcare. A lot of work is constantly being performed in the field. Many different approaches are available; we refer the reader to the relevant literature [<xref rid="B4-sensors-25-05678" ref-type="bibr">4</xref>,<xref rid="B17-sensors-25-05678" ref-type="bibr">17</xref>]. Here we will go over only those methods that use Optical Flow evaluation in their workflow. In [<xref rid="B18-sensors-25-05678" ref-type="bibr">18</xref>], the motion vector field for each two consecutive frames is calculated. The mean value of the vertical component of the vector field for each two frames is evaluated. From it, certain features such as the maximum vertical velocity and acceleration are derived, and together with the maximum amplitude of the recorded sound, a feature vector is defined. An SVM [<xref rid="B19-sensors-25-05678" ref-type="bibr">19</xref>] classifier is trained on a large dataset, and subsequently the classifier is evaluated on new data. For this work, both video and audio data are used. The method proposed in [<xref rid="B20-sensors-25-05678" ref-type="bibr">20</xref>] utilizes a mixture of background subtraction [<xref rid="B21-sensors-25-05678" ref-type="bibr">21</xref>], standard Optical Flow techniques, and Kalman filtering [<xref rid="B22-sensors-25-05678" ref-type="bibr">22</xref>] to detect falling events. A feature vector that consists of the &#8220;angle&#8221;, the &#8220;ratio&#8221; (defined as width-to-height relationship of a bounding rectangle), and the &#8220;ratio derivative&#8221; (defined as the velocity with which the silhouette changes) is used with a k-Nearest Neighbor [<xref rid="B23-sensors-25-05678" ref-type="bibr">23</xref>] classifier to classify whether to observed movement event is a fall. In [<xref rid="B24-sensors-25-05678" ref-type="bibr">24</xref>], OF is calculated, and various features of points of interest are derived. These are then used by a Convolutional Neural network [<xref rid="B25-sensors-25-05678" ref-type="bibr">25</xref>] for classification of falling events. The system also includes a two-step rule-based motion detection system that handles large, sudden movements and applies rule-based mechanisms when variations in optical flow exceed a threshold.</p><p>In the current paper, Optical Flow is calculated by a novel technique developed in our group [<xref rid="B9-sensors-25-05678" ref-type="bibr">9</xref>]. This OF method provides substantial benefits when compared to traditional OF algorithms. GLORIA is a group parameter reconstruction method known for its computational efficiency and speed. In the following sections, we show how to utilize the benefits of this method in solving the fall detection problem. We would like to point out that the method for fall detection we are presenting has been designed and tested on indoor data with a single moving person in frame but can certainly be applied in open space applications as well. We would also note that, even though we analyze video data, our method is set up in such a way that respects the privacy of the individual being filmed. This is a specific property of the GLORIA algorithm&#8212;it extracts only principal motion information from a video. Any other structure is lost, and the results of the technique cannot be used for any other purposes (unethical or otherwise). This is a central theme in computer vision tasks. We have gone in more detail in a previous work of ours [<xref rid="B13-sensors-25-05678" ref-type="bibr">13</xref>] regarding sensitivity of the video data. Our technique has low computational requirements. It can run in real-time on low to mid-range CPUs. This is a significant advantage when compared to other contemporary methods which require a dedicated GPU for their detection workflow.</p></sec><sec id="sec1dot3-sensors-25-05678"><title>1.3. Organization</title><p>This paper is organized in the following way: Our novel fall detection method is introduced in the following chapter. We present its description, the machine learning methods that are used for classification, the evaluation metrics, and the video datasets on which our algorithm is evaluated. Subsequently, we present our results on said datasets and provide comments about the method based on accuracy, speed, and other conditions such as camera placement. In the final section of the paper, we go over further comments, directions for future improvements, limitations, and the properties of our approach that make it stand out.</p></sec></sec><sec id="sec2-sensors-25-05678"><title>2. Materials and Methods</title><p>Our method consists of three steps. First, the GLORIA technique allows us to extract motion features from a subsection of a video. We use it to calculate principal motion information (two translations, two shears, dilatation, and rotation) and organize this data into a lower-dimensional structure&#8212;a 6 &#215; 150 vector. This data is then used as an input to our CNN. The network classifies events based on that input, and subsequently, the LSTM allows us to better capture short-term dynamics (a key property of these types of methods). The final step increases detection accuracy. We go over each step in more detail in the following subchapters, starting with a brief description of the GLORIA technique.</p><sec id="sec2dot1-sensors-25-05678"><title>2.1. GLORIA Net</title><p>Description of our current method begins by introducing the OF problem in Equation (1). We refer to an individual pixel in a color image frame as <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, we denote with <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> the spatial coordinates and time, and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is an index for the color channel (RGB). If we assume that all temporal changes in the image content arise solely from scene deformation, and we define the local velocity vector field as <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then the corresponding image transformation can be expressed as:<disp-formula id="FD1-sensors-25-05678"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo><mml:mi>v</mml:mi><mml:mo>&#8801;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (1), by <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> we denote the vector field operator, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the current color channel, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the total number of color channels, and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the time. The velocity field can account for a wide variety of objects&#8217; basic motions, such as rotations, dilatations, translations, etc. For the current method, however, we do not need to calculate the velocity vector field for each point, as we can directly reconstruct the global features of the optic flow by considering only specific aggregated values associated with it. Specifically, we aim to identify the global two-dimensional linear non-homogeneous transformations, which comprise rotations, translations, dilatations, and shear transformations that characterize fall movements. For this purpose, we use the GLORIA algorithm. The vector field operator from Equation (1) takes the following form:<disp-formula id="FD2-sensors-25-05678"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>&#8801;</mml:mo><mml:mi>v</mml:mi><mml:mo>&#183;</mml:mo><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo><mml:mo>&#8801;</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>&#8801;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8706;</mml:mo><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The representation in Equation (2) is used for the decomposition of the vector field <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> into a group of several known transformations. In Equation (3), we denote by <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> the transformation generators within the group and by <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> their corresponding parameters.<disp-formula id="FD3-sensors-25-05678"><label>(3)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#8801;</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>With Equation (3), we introduce a set of differential operators for the group of transformations that form a Lie algebra:<disp-formula id="FD4-sensors-25-05678"><label>(4)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>&#8801;</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We can apply Equation (4) to the group of six general linear non-homogeneous transformations in two-dimensional images to derive the global motion operators:<disp-formula id="FD5-sensors-25-05678"><label>(5)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msup><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msup><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The aggregated values related to the translations, shears, rotation, and dilatation <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed through the structural tensor <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the driving vector field <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD10-sensors-25-05678"><label>(6)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo mathvariant="sans-serif-italic">&#8711;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The GLORIA algorithm allows us to skip pointwise OF calculation and instead obtain a 6-element mean-flow vector for each two consecutive frames. This is very advantageous as it allows us to speed up the process significantly while preserving a high degree of accuracy. Another strength of this method is that it works regardless of camera position. It is also important to note that GLORIA can work with spectral data (in our case the color images that make up the video). As seen from Equation (1), all spectral channels contribute to the solution of the inverse problem. It is in general, and especially in the case of using only intensity images, an underdetermined problem because the local velocities in the directions of constant intensity can be arbitrary. This degeneracy is less likely to occur in multichannel images.</p><p>We examine the videos by splitting them into <italic toggle="yes">N</italic> = 150 frame subdivisions. For each subsection, we have a 6 &#215; 150 array. We use our datasets to train a Convolutional Neural Network (CNN) that works with our GLORIA vectors and subsequently classify whether new video data contains a fall event or not.</p><p>The CNN uses a 1D-like architecture by applying 2D convolutional layers, allowing it to capture patterns across the width while preserving vertical structure. The network has an input layer tailored to arrays of size 6 &#215; 150. It uses three convolutional blocks to extract features. The first two blocks each have a convolutional layer, a batch normalization, ReLU activation function, and max pooling step. These layers extract different features and reduce the width of the feature maps. The last convolutional block is used to refine finer details and features and can be optional. Afterwards, the feature maps are flattened into a one-dimensional vector, passed through a dense layer with 64 neurons, and regularized with dropout step to prevent overfitting. The final fully connected layer has two neurons that are used for binary classification, followed by a <italic toggle="yes">softmax</italic> and classification layer to output probabilities and compute the cross-entropy loss. The model is trained using an <italic toggle="yes">Adam</italic> optimizer with a learning rate of 0.001, mini-batches of size 32, and for up to 16 epochs. Validation is performed regularly using test data, and L2 regularization (0.001) is applied to improve generalization. This architecture is specially designed for our type of structured temporal data. Our main idea with this approach is that a fall is a very fast and abrupt event that will have an effect on all six elements of the flow vector. The CNN helps us to additionally refine and classify features that reflect the fall. A diagram of the network&#8217;s layers is presented in <xref rid="sensors-25-05678-f001" ref-type="fig">Figure 1</xref>:</p></sec><sec id="sec2dot2-sensors-25-05678"><title>2.2. LSTM</title><p>The GLORIA algorithm generates a sequence of 6 &#215; 1 vectors, each quantifying the total motion between consecutive video frames, thereby encoding inherent temporal dependencies. To classify videos for fall detection, a model capable of processing this sequential data while preserving historical context was essential. Traditional recurrent neural networks (RNNs) are fundamentally limited in maintaining long-term contextual information. Consequently, we adopted Long Short-Term Memory (LSTM) networks [<xref rid="B26-sensors-25-05678" ref-type="bibr">26</xref>], a robust extension designed to overcome these limitations through its specialized gating mechanism. Despite their prevalent use in natural language processing, LSTMs have demonstrated strong performance in time-series analysis, suggesting their utility for our classification problem. Our solution employs a hybrid CNN-LSTM architecture to leverage the respective strengths of both models. Their specific properties allow them to capture short-term dynamics like sudden movements and long-term tendencies like slow postural movements. This makes them very effective for temporal data such as human activity or in our current case&#8212;fall detection. Convolutional Neural Network (CNN) layers are first utilized to extract local patterns and refine the raw motion vector representation. The output from these CNN layers is then fed into LSTM layers, which process the refined vectors sequentially to identify meaningful temporal features. This integrated approach effectively combines the spatial feature extraction capabilities of CNNs with the temporal modeling power of LSTMs, resulting in enhanced classification accuracy for fall detection. This LSTM approach allows us to additionally increase the accuracy of our method. <xref rid="sensors-25-05678-f002" ref-type="fig">Figure 2</xref> shows where the Bidirectional LSTM block fits in our CNN layer structure:</p></sec><sec id="sec2dot3-sensors-25-05678"><title>2.3. Datasets</title><p>We have used a total of three datasets for method training and evaluations in this work. They are all public and available for download by anyone. We have decided to use widely popular and freely available resources as we believe it is a good way to benchmark our method&#8217;s capabilities.</p><sec id="sec2dot3dot1-sensors-25-05678"><title>2.3.1. UP Fall Detection Dataset</title><p>This dataset [<xref rid="B27-sensors-25-05678" ref-type="bibr">27</xref>] contains 1118 videos in total with FPS of 18 frames per second. There are two camera positions from which falls are recorded. Videos are short, with an average duration of roughly 16 s. The camera is placed horizontally with respect to the floor. Frame size is 640 &#215; 480. The recorded video is in color. The dataset is labeled by activity. These are the motions the recorded person is performing&#8212;jumping, laying on the ground, sitting in a chair, walking, squatting, falling, and standing still upright.</p></sec><sec id="sec2dot3dot2-sensors-25-05678"><title>2.3.2. LE2I Video Dataset</title><p>This dataset [<xref rid="B28-sensors-25-05678" ref-type="bibr">28</xref>] contains 191 videos with FPS of 25 frames per second. There are numerous camera positions, including camera placement in one of the upper corners of the room. This means that sometimes, depending on fall direction and camera placement, the velocities of the person related to the fall will not have a significant y-component. Frame resolution is 320 &#215; 240. Here the videos are labeled by marking during which frames of the video a fall occurs.</p></sec><sec id="sec2dot3dot3-sensors-25-05678"><title>2.3.3. UR Fall Detection Dataset</title><p>This dataset [<xref rid="B29-sensors-25-05678" ref-type="bibr">29</xref>] contains 70 videos with FPS of 30 frames per second. Like the previous dataset, camera positions are numerous. Kinect cameras are used to record fall events. The videos are in color. We use this dataset mainly as out-of-distribution set to further evaluate the performance of the GLORIA Net method.</p></sec></sec><sec id="sec2dot4-sensors-25-05678"><title>2.4. Evaluation Parameters</title><p>In the current work, we are interested in binary classification. In order to evaluate the performance of our models, we introduce a number of statistical values, derived from the confusion matrix. This is a 2 &#215; 2 matrix that is used to compare ground-truth labels to labels predicted from our models. It has the following entries:<list list-type="bullet"><list-item><p>True positives (TP)&#8212;number of times we have correctly predicted an event as positive (e.g., our model predicts a fall, and a fall occurred indeed);</p></list-item><list-item><p>False positives (FP)&#8212;number of times we have incorrectly predicted an event as positive (Type I error);</p></list-item><list-item><p>True negative (TN)&#8212;number of times we have correctly predicted an event as negative (e.g., our model predicts no fall, and a fall did not occur);</p></list-item><list-item><p>False positives (FN)&#8212;number of times we have incorrectly predicted an event as negative (Type II error).</p></list-item></list></p><p>Using these values, we can define the following measures:<disp-formula id="FD13-sensors-25-05678"><label>(7)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><italic toggle="yes">Accuracy</italic> is a useful evaluation parameter as it shows us the overall proportion of correctly predicted events. Another measure we use in the article is the <italic toggle="yes">Sensitivity</italic>:<disp-formula id="FD14-sensors-25-05678"><label>(8)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This measure indicates how often the model correctly classifies positive cases. Similarly, the <italic toggle="yes">Specificity</italic> shows us how good the model&#8217;s prediction is for negative cases:<disp-formula id="FD15-sensors-25-05678"><label>(9)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><italic toggle="yes">Precision</italic> shows us the rate of predicted positives that are classified correctly:<disp-formula id="FD16-sensors-25-05678"><label>(10)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The <italic toggle="yes">F</italic>1-<italic toggle="yes">score</italic> is a good example for a single parameter that can evaluate false positives and false negatives at the same time:<disp-formula id="FD17-sensors-25-05678"><label>(11)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In order to demonstrate the performance of our model graphically, we will use Receiver Operating Characteristic (ROC) curves. They plot the <italic toggle="yes">Sensitivity</italic> (<italic toggle="yes">True Positive Rate</italic>) against the <italic toggle="yes">False positive rate</italic> (1 &#8722; <italic toggle="yes">Specificity</italic>) at different thresholds. They are useful as the Area Under the Curve (AUC) can be used to describe how effective a model is. The closer AUC is to 1, the better the model is. An AUC value close to 0.5 indicates random guessing. The introduced values so far are dimensionless and indicate a percentage.</p><p>Used together, all the measures described so far can give a very detailed evaluation of the performance of our predictive model.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05678"><title>3. Results</title><p>A Lenovo<sup>&#174;</sup> ThinkPad (Lenovo, Hong Kong) with an Intel<sup>&#174;</sup> Core i5 CPU (Intel, Santa Clara, CA, USA) and 16 Gb of RAM is used to process videos from our sets. Algorithm realization is carried out in MATLAB<sup>&#174;</sup> R2023b environment.</p><sec id="sec3dot1-sensors-25-05678"><title>3.1. GLORIA Net Results</title><p>For training and evaluation of this method, we combined the UP-Fall dataset and the LE2I dataset. The CNN was trained on 90% of the data (as well as finetuning the hyperparameters and other network-related settings). Performance evaluation was then carried out on the remaining 10% of the data. We also introduced an out-of-distribution dataset (UR Fall Detection Dataset), in order to give a more detailed and clearer picture on model performance as sometimes CNNs tend to overfit on their training data. This also helps us to verify the quality, adequacy, reusability, and robustness of the selected features. ROC curves for GLORIA Net are available in <xref rid="sensors-25-05678-f003" ref-type="fig">Figure 3</xref>:</p><p>The other evaluation measures, introduced in <xref rid="sec2dot4-sensors-25-05678" ref-type="sec">Section 2.4</xref>, are displayed in <xref rid="sensors-25-05678-t001" ref-type="table">Table 1</xref>:</p><p>With an included LSTM approach, the performance of our method is slightly improved (we register less false negative (FN) events). In this case, the same evaluation measures are presented in <xref rid="sensors-25-05678-t002" ref-type="table">Table 2</xref>:</p><p>The results in <xref rid="sensors-25-05678-t002" ref-type="table">Table 2</xref> show the beneficial effect of the LSTM approach.</p></sec><sec id="sec3dot2-sensors-25-05678"><title>3.2. Comparison in Processing Times</title><p>We also provide a detailed evaluation of the speed with which we process the video data and classify events in a 150-frame window extract from the video. <xref rid="sensors-25-05678-f004" ref-type="fig">Figure 4</xref>a shows us the time our method needs to decide whether a fall event is present or not in a 150-frame time window for different camera resolutions. <xref rid="sensors-25-05678-f004" ref-type="fig">Figure 4</xref>b displays the memory requirements of our method. The biggest contribution is related to the fact that we need to load two images at any given time in order to extract the six-element vector of principal movements.</p><p>This points to the method&#8217;s applicability in real-time scenarios and the overall low resource requirements.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05678"><title>4. Discussion and Conclusions</title><p>We have developed a novel method for detecting falls using video data from standard cameras. Our technique is very robust&#8212;it does not depend on camera placement, which is very advantageous as often cameras are placed in various locations throughout a room. We provide two evaluations with which we can judge overall performance of the method&#8212;one on the 10% remainder of training data and another on an OOD set. From the evaluations, we see that it matches the accuracy of state-of-the-art optical flow methods such as the one presented in [<xref rid="B18-sensors-25-05678" ref-type="bibr">18</xref>], without needing fusion of both video and audio data.</p><p>The method relies on a fast OF algorithm, which makes it very suitable for real-time problems such as the fall detection task. Both of these factors&#8212;freedom to place the camera in different locations and the computational efficiency&#8212;show us the universality of our method and its application for detection of falls in real-time.</p><p>GLORIA Net runs on a low to mid-range laptop CPU with low memory requirements and provides highly accurate results. Many contemporary methods for fall detection rely on processing through a dedicated GPU (both in training and analyzing new data). This is a key feature that demonstrates that it is competitive and practically relevant.</p><p>In addition to the above, it is worth mentioning that in the present work, we show an original approach to apply and use CNNs in video data. Most commonly, CNNs are used with images. A number of possible approaches [<xref rid="B30-sensors-25-05678" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05678" ref-type="bibr">31</xref>] for application of Convolutional Neural networks in videos can be found in the literature, but the integration of the principal component OF parameters with a CNN is a novel contribution.</p><p>Recently, our group has introduced a system for real-time automated detection of epileptic seizures from video. Its capabilities were upgraded to include patient tracking and multiple camera coverage. GLORIA Net has been added to the existing system as it is modular in nature. We are currently observing its work, but so far, the addition has been very beneficial.</p><p>A limitation of our method is that it works with chunks of video data&#8212;collections of 150 frames from the video. This can be restricting in certain scenarios. As a future direction of development, we plan to increase responsiveness of the method so relevant parties can react quicker in the event of a fall. As with all other optical flow methods, GLORIA Net needs constant brightness of the scene in order to function properly. A future direction of work may include sensor fusion with an IR camera, utilizing the multi-spectral nature of the GLORIA technique. This would allow the fall detection method to function in more varied lighting conditions.</p><p>Another direction for future development is to define and extract features from the data that are related to the fall event in a physical sense. For example, we may look at the fall duration, the acceleration of the person, whether or not movement is periodic, and other similar quantities. Such preliminary refinement may help increase the accuracy of our fall detection scheme.</p><p>The datasets we have used in the current article are of videos in which a single person is present. We plan to test our method and upgrade it if needed on data that contains multiple moving people. A more thorough investigation of the additional LSTM block will be conducted on this data.</p><p>Many commercially available fall detection systems exist. Like the method here, they are described as accurate, inobtrusive, real-time, and automatic. What our system can do better is to rely on less sensors than wearable approaches [<xref rid="B6-sensors-25-05678" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05678" ref-type="bibr">7</xref>]&#8212;it can work with a single camera while retaining real-time function with accurate results. In addition, cameras are readily available and can be found in many hospitals and homes, which removes the need to install and maintain special ambient sensors [<xref rid="B5-sensors-25-05678" ref-type="bibr">5</xref>]. These benefits of our method, when compared to commercial products, further highlight its usefulness and ease of applicability.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, G.P. and S.K. (Stiliyan Kalitzin); methodology, S.K. (Stiliyan Kalitzin), S.K. (Simeon Karpuzov), and O.G.; software, S.K. (Simeon Karpuzov), T.S. and A.T.; validation, S.K. (Simeon Karpuzov), T.S. and A.T.; formal analysis, G.P.; investigation, S.K. (Simeon Karpuzov), T.S., G.P., and A.T.; resources, O.G. and S.K. (Stiliyan Kalitzin); data curation, S.K. (Simeon Karpuzov); writing&#8212;original draft preparation, S.K. (Simeon Karpuzov); writing&#8212;review and editing, O.G., S.K. (Stiliyan Kalitzin), and G.P.; visualization, S.K. (Simeon Karpuzov), T.S., and A.T.; supervision, S.K. (Stiliyan Kalitzin) and G.P.; project administration, G.P.; funding acquisition, G.P., O.G., and S.K. (Stiliyan Kalitzin). All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Available upon request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">OF</td><td align="left" valign="middle" rowspan="1" colspan="1">Optical Flow</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SVM</td><td align="left" valign="middle" rowspan="1" colspan="1">Support Vector Machine</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ROC</td><td align="left" valign="middle" rowspan="1" colspan="1">Receiver Operator Characteristic</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AUC</td><td align="left" valign="middle" rowspan="1" colspan="1">Area Under the Curve</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FPS</td><td align="left" valign="middle" rowspan="1" colspan="1">Frames Per Second</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05678"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Husdal</surname><given-names>K.</given-names></name><name name-style="western"><surname>Rice</surname><given-names>J.</given-names></name><name name-style="western"><surname>Loomba</surname><given-names>S.</given-names></name><name name-style="western"><surname>Falck</surname><given-names>R.S.</given-names></name><name name-style="western"><surname>Dimri</surname><given-names>V.</given-names></name><name name-style="western"><surname>Pinheiro</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cameron</surname><given-names>I.</given-names></name><name name-style="western"><surname>Sherrington</surname><given-names>C.</given-names></name><name name-style="western"><surname>Madden</surname><given-names>K.M.</given-names></name><etal/></person-group><article-title>Cost-effectiveness of falls prevention strategies for older adults: Protocol for a living systematic review</article-title><source>BMJ Open</source><year>2024</year><volume>14</volume><fpage>e088536</fpage><pub-id pub-id-type="doi">10.1136/bmjopen-2024-088536</pub-id><pub-id pub-id-type="pmcid">PMC11552585</pub-id><pub-id pub-id-type="pmid">39500610</pub-id></element-citation></ref><ref id="B2-sensors-25-05678"><label>2.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>European Public Health Association</collab></person-group><article-title>Falls in Older Adults in the EU: Factsheet</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://eupha.org/repository/sections/ipsp/Factsheet_falls_in_older_adults_in_EU.pdf" ext-link-type="uri">https://eupha.org/repository/sections/ipsp/Factsheet_falls_in_older_adults_in_EU.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-04">(accessed on 4 June 2025)</date-in-citation></element-citation></ref><ref id="B3-sensors-25-05678"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Robertson</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Ashe</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Liu-Ambrose</surname><given-names>T.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Marra</surname><given-names>C.A.</given-names></name></person-group><article-title>International comparison of cost of falls in older adults living in the community: A systematic review</article-title><source>Osteoporos. Int.</source><year>2010</year><volume>21</volume><fpage>1295</fpage><lpage>1306</lpage><pub-id pub-id-type="doi">10.1007/s00198-009-1162-0</pub-id><pub-id pub-id-type="pmid">20195846</pub-id></element-citation></ref><ref id="B4-sensors-25-05678"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ellul</surname><given-names>J.</given-names></name><name name-style="western"><surname>Azzopardi</surname><given-names>G.</given-names></name></person-group><article-title>Elderly Fall Detection Systems: A Literature Survey</article-title><source>Front. Robot. AI</source><year>2020</year><volume>7</volume><elocation-id>71</elocation-id><pub-id pub-id-type="doi">10.3389/frobt.2020.00071</pub-id><pub-id pub-id-type="pmid">33501238</pub-id><pub-id pub-id-type="pmcid">PMC7805655</pub-id></element-citation></ref><ref id="B5-sensors-25-05678"><label>5.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Alon</surname><given-names>N.</given-names></name></person-group><article-title>Fall Detection and Prevention System and Method. WO2025082457, 24 April 2025</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2025082457" ext-link-type="uri">https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2025082457</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-16">(accessed on 16 June 2025)</date-in-citation></element-citation></ref><ref id="B6-sensors-25-05678"><label>6.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>John</surname><given-names>C.-F.</given-names></name></person-group><article-title>Method and System for Fall Detection. US8217795B2, 10 July 2012</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://patents.google.com/patent/US8217795B2/en" ext-link-type="uri">https://patents.google.com/patent/US8217795B2/en</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-16">(accessed on 16 June 2025)</date-in-citation></element-citation></ref><ref id="B7-sensors-25-05678"><label>7.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Gonz&#225;lez</surname><given-names>C.S.</given-names></name></person-group><article-title>Device, System and Method for Fall Detection. EP 3 796 282 A2, 21 March 2021</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://patentimages.storage.googleapis.com/e9/e8/a1/fc9d181803c231/EP3796282A2.pdf#page=19.77" ext-link-type="uri">https://patentimages.storage.googleapis.com/e9/e8/a1/fc9d181803c231/EP3796282A2.pdf#page=19.77</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-16">(accessed on 16 June 2025)</date-in-citation></element-citation></ref><ref id="B8-sensors-25-05678"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalitzin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Geertsema</surname><given-names>E.</given-names></name><name name-style="western"><surname>Petkov</surname><given-names>G.</given-names></name></person-group><article-title>Scale-Iterative Optical Flow Reconstruction from Multi-Channel Image Sequences</article-title><source>Front. Artif. Intell. Appl.</source><year>2018</year><volume>310</volume><fpage>302</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.3233/978-1-61499-929-4-302</pub-id></element-citation></ref><ref id="B9-sensors-25-05678"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalitzin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Geertsema</surname><given-names>E.</given-names></name><name name-style="western"><surname>Petkov</surname><given-names>G.</given-names></name></person-group><article-title>Optical Flow Group-Parameter Reconstruction from Multi-Channel Image Sequences</article-title><source>Front. Artif. Intell. Appl.</source><year>2018</year><volume>310</volume><fpage>290</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.3233/978-1-61499-929-4-290</pub-id></element-citation></ref><ref id="B10-sensors-25-05678"><label>10.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Lucas</surname><given-names>B.D.</given-names></name><name name-style="western"><surname>Kanade</surname><given-names>T.</given-names></name></person-group><article-title>An Iterative Image Registration Technique with an Application to Stereo Vision (IJCAI)</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.researchgate.net/publication/215458777" ext-link-type="uri">https://www.researchgate.net/publication/215458777</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-01">(accessed on 1 June 2025)</date-in-citation></element-citation></ref><ref id="B11-sensors-25-05678"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Horn</surname><given-names>B.K.P.</given-names></name><name name-style="western"><surname>Schunck</surname><given-names>B.G.</given-names></name></person-group><article-title>Determining optical flow</article-title><source>Artif. Intell.</source><year>1981</year><volume>17</volume><fpage>185</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(81)90024-2</pub-id></element-citation></ref><ref id="B12-sensors-25-05678"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeyasingh-Jacob</surname><given-names>J.</given-names></name><name name-style="western"><surname>Crook-Rumsey</surname><given-names>M.</given-names></name><name name-style="western"><surname>Shah</surname><given-names>H.</given-names></name><name name-style="western"><surname>Joseph</surname><given-names>T.</given-names></name><name name-style="western"><surname>Abulikemu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Daniels</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sharp</surname><given-names>D.J.</given-names></name><name name-style="western"><surname>Haar</surname><given-names>S.</given-names></name></person-group><article-title>Markerless Motion Capture to Quantify Functional Performance in Neurodegeneration: Systematic Review</article-title><source>JMIR Aging</source><year>2024</year><volume>7</volume><fpage>e52582</fpage><pub-id pub-id-type="doi">10.2196/52582</pub-id><pub-id pub-id-type="pmid">39106477</pub-id><pub-id pub-id-type="pmcid">PMC11336506</pub-id></element-citation></ref><ref id="B13-sensors-25-05678"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karpuzov</surname><given-names>S.</given-names></name><name name-style="western"><surname>Petkov</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ilieva</surname><given-names>S.</given-names></name><name name-style="western"><surname>Petkov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kalitzin</surname><given-names>S.</given-names></name></person-group><article-title>Object Tracking Based on Optical Flow Reconstruction of Motion-Group Parameters</article-title><source>Information</source><year>2024</year><volume>15</volume><elocation-id>296</elocation-id><pub-id pub-id-type="doi">10.3390/info15060296</pub-id></element-citation></ref><ref id="B14-sensors-25-05678"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vargas</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>P.</given-names></name><name name-style="western"><surname>Orbe</surname><given-names>E.A.</given-names></name><name name-style="western"><surname>Zapata</surname><given-names>M.</given-names></name><name name-style="western"><surname>Valencia-Arag&#243;n</surname><given-names>K.</given-names></name></person-group><article-title>Low-Cost Non-Wearable Fall Detection System Implemented on a Single Board Computer for People in Need of Care</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5592</elocation-id><pub-id pub-id-type="doi">10.3390/s24175592</pub-id><pub-id pub-id-type="pmid">39275503</pub-id><pub-id pub-id-type="pmcid">PMC11397814</pub-id></element-citation></ref><ref id="B15-sensors-25-05678"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name></person-group><article-title>Robust fall detection in video surveillance based on weakly supervised learning</article-title><source>Neural Netw.</source><year>2023</year><volume>163</volume><fpage>286</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2023.03.042</pub-id><pub-id pub-id-type="pmid">37086545</pub-id></element-citation></ref><ref id="B16-sensors-25-05678"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chhetri</surname><given-names>S.</given-names></name><name name-style="western"><surname>Alsadoon</surname><given-names>A.</given-names></name><name name-style="western"><surname>Al-Dala&#8217;IN</surname><given-names>T.</given-names></name><name name-style="western"><surname>Prasad</surname><given-names>P.W.C.</given-names></name><name name-style="western"><surname>Rashid</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Maag</surname><given-names>A.</given-names></name></person-group><article-title>Deep Learning for Vision-Based Fall Detection System: Enhanced Optical Dynamic Flow</article-title><source>Comput. Intell.</source><year>2021</year><volume>37</volume><fpage>578</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1111/coin.12428</pub-id></element-citation></ref><ref id="B17-sensors-25-05678"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gaya-Morey</surname><given-names>F.X.</given-names></name><name name-style="western"><surname>Manresa-Yee</surname><given-names>C.</given-names></name><name name-style="western"><surname>Buades-Rubio</surname><given-names>J.M.</given-names></name></person-group><article-title>Deep learning for computer vision based activity recognition and fall detection of the elderly: A systematic review</article-title><source>Appl. Intell.</source><year>2024</year><volume>54</volume><fpage>8982</fpage><lpage>9007</lpage><pub-id pub-id-type="doi">10.1007/s10489-024-05645-1</pub-id></element-citation></ref><ref id="B18-sensors-25-05678"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Geertsema</surname><given-names>E.E.</given-names></name><name name-style="western"><surname>Visser</surname><given-names>G.H.</given-names></name><name name-style="western"><surname>Viergever</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Kalitzin</surname><given-names>S.N.</given-names></name></person-group><article-title>Automated remote fall detection using impact features from video and audio</article-title><source>J. Biomech.</source><year>2019</year><volume>88</volume><fpage>25</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.jbiomech.2019.03.007</pub-id><pub-id pub-id-type="pmid">30922611</pub-id></element-citation></ref><ref id="B19-sensors-25-05678"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vapnik</surname><given-names>V.</given-names></name><name name-style="western"><surname>Saitta</surname><given-names>L.</given-names></name></person-group><article-title>Support-vector networks</article-title><source>Mach. Learn.</source><year>1995</year><volume>20</volume><fpage>273</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1023/A:1022627411411</pub-id></element-citation></ref><ref id="B20-sensors-25-05678"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>De Miguel</surname><given-names>K.</given-names></name><name name-style="western"><surname>Brunete</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hernando</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gambao</surname><given-names>E.</given-names></name></person-group><article-title>Home camera-based fall detection system for the elderly</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>2864</elocation-id><pub-id pub-id-type="doi">10.3390/s17122864</pub-id><pub-id pub-id-type="pmid">29232846</pub-id><pub-id pub-id-type="pmcid">PMC5751723</pub-id></element-citation></ref><ref id="B21-sensors-25-05678"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Piccardi</surname><given-names>M.</given-names></name></person-group><article-title>Background subtraction techniques: A review</article-title><source>Proceedings of the 2004 IEEE International Conference on Systems, Man and Cybernetics</source><conf-loc>The Hague, The Netherlands</conf-loc><conf-date>10&#8211;13 October 2004</conf-date><volume>Volume 4</volume><fpage>3099</fpage><lpage>3104</lpage><pub-id pub-id-type="doi">10.1109/ICSMC.2004.1400815</pub-id></element-citation></ref><ref id="B22-sensors-25-05678"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalman</surname><given-names>R.E.</given-names></name></person-group><article-title>A New Approach to Linear Filtering and Prediction Problems</article-title><source>J. Basic Eng.</source><year>1960</year><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="B23-sensors-25-05678"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Hart</surname><given-names>P.E.</given-names></name></person-group><article-title>Nearest Neighbor Pattern Classification</article-title><source>IEEE Trans. Inf. Theory</source><year>1967</year><volume>13</volume><fpage>21</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1109/TIT.1967.1053964</pub-id></element-citation></ref><ref id="B24-sensors-25-05678"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hsieh</surname><given-names>Y.Z.</given-names></name><name name-style="western"><surname>Jeng</surname><given-names>Y.L.</given-names></name></person-group><article-title>Development of Home Intelligent Fall Detection IoT System Based on Feedback Optical Flow Convolutional Neural Network</article-title><source>IEEE Access</source><year>2017</year><volume>6</volume><fpage>6048</fpage><lpage>6057</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2017.2771389</pub-id></element-citation></ref><ref id="B25-sensors-25-05678"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>Computation by neural networks</article-title><source>Nat. Neurosci.</source><year>2000</year><volume>3</volume><fpage>1170</fpage><pub-id pub-id-type="doi">10.1038/81442</pub-id><pub-id pub-id-type="pmid">11127833</pub-id></element-citation></ref><ref id="B26-sensors-25-05678"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hochreiter</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group><article-title>Long Short-Term Memory</article-title><source>Neural Comput.</source><year>1997</year><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="B27-sensors-25-05678"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mart&#237;nez-Villase&#241;or</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ponce</surname><given-names>H.</given-names></name><name name-style="western"><surname>Brieva</surname><given-names>J.</given-names></name><name name-style="western"><surname>Moya-Albor</surname><given-names>E.</given-names></name><name name-style="western"><surname>N&#250;&#241;ez-Mart&#237;nez</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pe&#241;afort-Asturiano</surname><given-names>C.</given-names></name></person-group><article-title>UP-Fall Detection Dataset: A Multimodal Approach</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>1988</elocation-id><pub-id pub-id-type="doi">10.3390/s19091988</pub-id><pub-id pub-id-type="pmid">31035377</pub-id><pub-id pub-id-type="pmcid">PMC6539235</pub-id></element-citation></ref><ref id="B28-sensors-25-05678"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Charfi</surname><given-names>I.</given-names></name><name name-style="western"><surname>Miteran</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dubois</surname><given-names>J.</given-names></name><name name-style="western"><surname>Atri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tourki</surname><given-names>R.</given-names></name></person-group><article-title>Optimized spatio-temporal descriptors for real-time fall detection: Comparison of support vector machine and Adaboost-based classification</article-title><source>J. Electron. Imaging</source><year>2013</year><volume>22</volume><fpage>041106</fpage><pub-id pub-id-type="doi">10.1117/1.JEI.22.4.041106</pub-id></element-citation></ref><ref id="B29-sensors-25-05678"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwolek</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kepski</surname><given-names>M.</given-names></name></person-group><article-title>Human fall detection on embedded platform using depth maps and wireless accelerometer</article-title><source>Comput. Methods Programs Biomed.</source><year>2014</year><volume>117</volume><fpage>489</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2014.09.005</pub-id><pub-id pub-id-type="pmid">25308505</pub-id></element-citation></ref><ref id="B30-sensors-25-05678"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Carreira</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HA, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date></element-citation></ref><ref id="B31-sensors-25-05678"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tran</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bourdev</surname><given-names>L.</given-names></name><name name-style="western"><surname>Fergus</surname><given-names>R.</given-names></name><name name-style="western"><surname>Torresani</surname><given-names>L.</given-names></name><name name-style="western"><surname>Paluri</surname><given-names>M.</given-names></name></person-group><article-title>Learning Spatiotemporal Features with 3D Convolutional Networks</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Santiago, Chile</conf-loc><conf-date>7&#8211;13 December 2015</conf-date><pub-id pub-id-type="doi">10.1109/ICCV.2015.510</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05678-f001" orientation="portrait"><label>Figure 1</label><caption><p>Description of the layers and architecture of GLORIA Net.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05678-g001.jpg"/></fig><fig position="float" id="sensors-25-05678-f002" orientation="portrait"><label>Figure 2</label><caption><p>Addition of the LSTM block to our network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05678-g002.jpg"/></fig><fig position="float" id="sensors-25-05678-f003" orientation="portrait"><label>Figure 3</label><caption><p>ROC curves for the GLORIA Net. Green line is for the performance on UR dataset, and blue line is for the performance on the UP-Fall+LE2I 10% subset. AUC values are provided in the legend.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05678-g003.jpg"/></fig><fig position="float" id="sensors-25-05678-f004" orientation="portrait"><label>Figure 4</label><caption><p>(<bold>a</bold>) Processing time required to make a classification for different frame resolutions. (<bold>b</bold>) Memory requirements for different frame resolutions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05678-g004.jpg"/></fig><table-wrap position="float" id="sensors-25-05678-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05678-t001_Table 1</object-id><label>Table 1</label><caption><p>Accuracy, precision, sensitivity, specificity, and f1-score for both datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specificity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UP-Fall + LE2I</td><td align="center" valign="middle" rowspan="1" colspan="1">97.7%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.1%</td><td align="center" valign="middle" rowspan="1" colspan="1">96.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">97.0%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.0%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05678-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05678-t002_Table 2</object-id><label>Table 2</label><caption><p>Accuracy, precision, sensitivity, specificity, and f1-score for UR dataset with and without the LSTM block.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specificity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UR (+LSTM)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7% (&#8593; <bold>8.4</bold>%)</td><td align="center" valign="middle" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" rowspan="1" colspan="1">100.0% (&#8593; <bold>16.7</bold>%)</td><td align="center" valign="middle" rowspan="1" colspan="1">100.0% (&#8593; <bold>16.7</bold>%)</td><td align="center" valign="middle" rowspan="1" colspan="1">90.9% (&#8593; <bold>7.6</bold>%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UR (no LSTM)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.3%</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>