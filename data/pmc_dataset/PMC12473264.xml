<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473264</article-id><article-id pub-id-type="pmcid-ver">PMC12473264.1</article-id><article-id pub-id-type="pmcaid">12473264</article-id><article-id pub-id-type="pmcaiid">12473264</article-id><article-id pub-id-type="pmid">41012891</article-id><article-id pub-id-type="doi">10.3390/s25185652</article-id><article-id pub-id-type="publisher-id">sensors-25-05652</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Integrating UAV-Derived Diameter Estimations and Machine Learning for Precision Cabbage Yield Mapping</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-0233-5966</contrib-id><name name-style="western"><surname>Arab</surname><given-names initials="ST">Sara Tokhi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05652" ref-type="aff">1</xref><xref rid="c1-sensors-25-05652" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-3547-0391</contrib-id><name name-style="western"><surname>Takezaki</surname><given-names initials="A">Akane</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-05652" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Kogoshi</surname><given-names initials="M">Masayuki</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05652" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9983-6710</contrib-id><name name-style="western"><surname>Nakano</surname><given-names initials="Y">Yuka</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05652" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Kikuchi</surname><given-names initials="S">Sunao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af3-sensors-25-05652" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-4798-4338</contrib-id><name name-style="western"><surname>Tanaka</surname><given-names initials="K">Kei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-05652" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Hayashi</surname><given-names initials="K">Kazunobu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af2-sensors-25-05652" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Chung</surname><given-names initials="Y">Yongwha</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05652"><label>1</label>Research Center for Agricultural Robotics, National Agriculture and Food Research Organization (NARO), Tsukuba 305-0856, Japan</aff><aff id="af2-sensors-25-05652"><label>2</label>Institute of Agriculture Machinery, National Agriculture and Food Research Organization (NARO), Tsukuba 305-0856, Japan</aff><aff id="af3-sensors-25-05652"><label>3</label>Central Region Agricultural Research Center, National Agriculture and Food Research Organization (NARO), Tsukuba 305-8666, Japan</aff><author-notes><corresp id="c1-sensors-25-05652"><label>*</label>Correspondence: <email>tokhiarab.sara359@naro.go.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>10</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5652</elocation-id><history><date date-type="received"><day>24</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>10</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05652.pdf"/><abstract><p>Non-destructive diameter estimation of cabbage heads and yield prediction employing Unmanned Aerial Vehicle (UAV) imagery are superior to conventional approaches, which are labor intensive and time consuming. This approach assesses spatial variability across the field, effective allocation of resources, and supports variable application rates of fertilizer and supply chain management. Here, individual cabbage head diameters were estimated using deep learning-based pose estimation models (YOLOv8s-pose and YOLOv11s-pose) using high spatial resolution RGB images acquired from UAV 6 m during the cabbage-growing season in 2024. With a mean relative error (MRE) of 4.6% and a high mean average precision (mAP) 98.5% at 0.5, YOLOv11s-pose emerged as the best-performing model, verifying its accuracy for pragmatic agricultural use. The approximated diameter was then combined with climatic variables (temperature and rainfall) and canopy reflectance indices (normalized difference vegetation index (NDVI), normalized difference red edge index (NDRE), and green chlorophyll index (CIg)) that were extracted from the multispectral images with 6 m resolution and fed into AI models to develop individual cabbage head fresh weight. Among the machine learning models (MLMs) tested, CatBoost achieved the lowest Mean Squared Error (MSE = 0.025 kg/cabbage), highest R<sup>2</sup> (0.89), and outperformed other models based on the Diebold&#8211;Mariano statistical test (<italic toggle="yes">p</italic> &lt; 0.05). This finding suggests that an integrated AI-powered framework enhances non-invasive and precise yield estimation in cabbage farming.</p></abstract><kwd-group><kwd>unmanned aerial vehicle</kwd><kwd>RGB</kwd><kwd>multispectral images</kwd><kwd>cabbage diameter</kwd><kwd>pose estimation</kwd><kwd>head fresh weight prediction</kwd><kwd>ML algorithms</kwd></kwd-group><funding-group><award-group><funding-source>development and improvement program of strategic smart agricultural technology grants from the Project of the Bio-oriented Technology Research Advancement Institution (BRAIN)</funding-source></award-group><funding-statement>This research was funded by the development and improvement program of strategic smart agricultural technology grants from the Project of the Bio-oriented Technology Research Advancement Institution (BRAIN).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05652"><title>1. Introduction</title><p>Smart agriculture is rapidly transforming traditional farming techniques by optimizing efficiency, maximizing production with minimum inputs, guaranteeing sustainability, and ensuring food security. Societies confronting aging agricultural communities urgently require effective and sustainable food supply management [<xref rid="B1-sensors-25-05652" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05652" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05652" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05652" ref-type="bibr">4</xref>]. In this scenario, intelligent cultivation that integrates artificial intelligence, remote sensing, and data analytics has emerged as a promising solution to increase productivity while minimizing resource consumption.</p><p>Among these aforementioned advanced technologies, the application of deep learning techniques (DLTs) combined with UAV imagery for field-based, high-throughput, non-destructive phenotyping and crop yield prediction has recently increased [<xref rid="B5-sensors-25-05652" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05652" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05652" ref-type="bibr">7</xref>]. These advanced technologies and tools enable the precise identification and monitoring of plant characteristics, such as size, shape, diameter, volume, and health state, through detection, segmentation, and classification, ultimately enabling accurate yield prediction and more efficient supply chain planning [<xref rid="B8-sensors-25-05652" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05652" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05652" ref-type="bibr">10</xref>].</p><p>Recent studies have demonstrated the effectiveness of DLT in cabbage cultivation using UAV imagery by employing models such as DeepLab V3+, YOLOv5, Mask R-CNN, YOLOv8n-seg, UNet, and instance segmentation frameworks [<xref rid="B9-sensors-25-05652" ref-type="bibr">9</xref>,<xref rid="B11-sensors-25-05652" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05652" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05652" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05652" ref-type="bibr">14</xref>]. For instance, Kim et al. [<xref rid="B11-sensors-25-05652" ref-type="bibr">11</xref>] developed a deep learning-based semantic segmentation framework to automate cabbage field identification in the highlands of South Korea. Similarly, a prior study proposed object segmentation on multispectral images and employed YOLOv5 to detect individual Chinese cabbage plants in UAV-derived RGB orthomosaic images [<xref rid="B12-sensors-25-05652" ref-type="bibr">12</xref>]. Moreover, object-based image analysis (OBIA) and deep learning models such as Mask R-CNN have been applied to extract and count individual cabbage plants, with Mask R-CNN outperforming OBIA in terms of accuracy and robustness [<xref rid="B9-sensors-25-05652" ref-type="bibr">9</xref>]. Additionally, CabbageNet, a model developed for real-time segmentation of cabbage heads, demonstrated the potential of intelligent automation in precision horticulture [<xref rid="B13-sensors-25-05652" ref-type="bibr">13</xref>].</p><p>Despite these improvements in cabbage detection and segmentation, there is still a significant gap in the literature regarding precise cabbage diameter measurements for yield prediction using UAV-derived RGB and multispectral imagery. Majority of yield prediction methods rely on either physical sampling or indirect indicators, which restrict scalability and accuracy in commercial farming. These methods require considerable amounts of time and effort. However, drone multispectral and RGB sensors provide farmers with high-resolution images at a lower cost, making them more beneficial and readily available tools for precision farming [<xref rid="B15-sensors-25-05652" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05652" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05652" ref-type="bibr">17</xref>].</p><p>The integration of UAV-derived RGB images with multispectral canopy reflectance indices and climatic variables (temperature and precipitation) into an AI model has remarkably improved crop feature extraction, biomass estimation, plant disease detection, and yield forecasting [<xref rid="B18-sensors-25-05652" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05652" ref-type="bibr">19</xref>]. Generally used canopy reflectance indicators, such as the Normalized Difference Vegetation Index (NDVI), help estimate agricultural production [<xref rid="B20-sensors-25-05652" ref-type="bibr">20</xref>]. Other indices such as NDRE and CIg also provide significant information on plant health, nutrition, canopy structure, stress, and chlorophyll content. This allowed the accurate phenotyping and monitoring of crop conditions in the field [<xref rid="B21-sensors-25-05652" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05652" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05652" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05652" ref-type="bibr">24</xref>].</p><p>In addition to canopy reflectance indices, incorporating meteorological data derived from satellite remote sensing including LandSat8 land surface temperature (LST) and precipitation data from the Japan Aerospace Exploration Agency (JAXA), further strengthens the analytical foundation for precision agriculture. These climatic variables have been used in various studies on grapes, rice, maize, and cassava [<xref rid="B25-sensors-25-05652" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05652" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05652" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05652" ref-type="bibr">28</xref>]. Furthermore, integrating vegetation indices with other crop parameters and plant characteristics obtained from RGB images help us to better understand agricultural situations at the field level. Additionally, combining this information with statistical and ML models enable the prediction of plant features and yields across many other crops, including fruits and vegetables [<xref rid="B29-sensors-25-05652" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05652" ref-type="bibr">30</xref>].</p><p>Although UAV images and DLT are increasingly being utilized for crop monitoring and yield prediction, to date, no study has focused on estimating individual cabbage head weight directly from the diameter acquired from UAV-based RGB images. Cabbage is an economically valuable vegetable of which market price is largely determined by head size and homogeneity, qualities that are difficult to evaluate non-destructively at large scales. In this study, we have proposed a scalable, non-invasive method for estimating the fresh weight of cabbage heads in the field using vegetation indicators, climate data, and diameters obtained from high-resolution RGB UAV images. In contrast to conventional segmentation techniques, such as thresholding, edge detection, and region-based methods, which are commonly used to identify crop boundaries, this study employed a pose estimation (keypoint detection) model to estimate cabbage diameter [<xref rid="B31-sensors-25-05652" ref-type="bibr">31</xref>]. In this context, pose estimation is a more efficient and accurate way to find certain geometric features, such as the outside edges of cabbage heads, which allow for accurate diameter measurement without requiring complete object segmentation. This innovative usage of poses enables a feature extraction approach that is both resilient and efficient, making it ideal for wide-field applications.</p><p>Therefore, the main goal of this study was to enable smart agricultural initiatives by building a deep learning-based system that employs a pose estimation model to recognize and locate individual cabbage heads in UAV RGB images. Specifically, the objectives of this study were to (1) estimate the cabbage head diameter from UAV RGB imagery using a pose estimation model; (2) predict individual cabbage head fresh weight using various ML algorithms by integrating head diameter, environmental data, and biophysical properties of cabbage; and (3) develop a spatial cabbage head fresh weight map based on the best-performing model output. This approach allows the precise, non-destructive, and large-scale measurement of cabbage yield and diameter estimation. Spatial yield maps provide vital information for harvest planning, resource allocation, and nutrient management. This improves both smart agriculture and data-informed decision-making in cabbage cultivation.</p></sec><sec id="sec2-sensors-25-05652"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-05652"><title>2.1. Experimental Field Summary and Cultivation Method</title><p>The experiment was performed in a field of the National Agriculture and Food Research Organization (NARO) located in Tsukuba, Ibaraki Prefecture, Japan. The plots, covering an area of approximately 115.2 m<sup>2</sup> (24 m &#215; 4.8 m), are located at 36&#176;1&#8242;30.53&#8243; N latitude and 140&#176;6&#8242;16.37&#8243; E longitude (<xref rid="sensors-25-05652-f001" ref-type="fig">Figure 1</xref>). Three Japanese cabbage varieties (Renbu (TOHOKU SEED Co., Ltd., Utsunomiya, Japan), Tenku (TAKII SEED Co., Ltd., Kyoto, Japan), and Mikuni (KANEKO SEEDS Co., Ltd., Maebashi, Japan)) were transplanted on 4 September and 9 September 2024, with a total growth period of 128 days from seeding to harvesting (plot-1 Seeding on 7 August and harvesting on 13 December). A total of 60 cabbage seedlings were planted in each row. Each variety comprised 20 plants per row, yielding 160 cabbages per variety and 480 cabbages in total. The distance between plants was 0.4 m, and the distance between rows was 0.6 m. The harvest survey was conducted on 13 December and 16 December. During the surveys a total of 186 cabbages were randomly selected, labeled, placed in their designated bags, and transported to the laboratory under controlled conditions. Cabbage head diameter, total fresh aboveground weight, outer leaf weight, and head fresh weight were determined using quantitative assessments with a precision scale (A&amp;D model) and a vernier caliper. The sample size was determined using a 95% confidence interval to ensure statistical reliability and representativeness of the collected data.</p></sec><sec id="sec2dot2-sensors-25-05652"><title>2.2. Data Acquisition</title><sec id="sec2dot2dot1-sensors-25-05652"><title>2.2.1. Synopsis of Drone Images and Data Collection</title><p>During the cabbage-growing season, aerial photography was conducted using a DJI Phantom 4 Multispectral (P4M) drone on biweekly flights to evaluate and track the state of cabbage development with a total of 12 flights conducted. To measure cabbage head diameter and weight, high-resolution (6 m) RGB, and multispectral images were collected by using six ground control points (GCPs). These control points significantly improve data quality and contribute to more cost-effective surveys. Additionally, a Calibrated Reflectance Panel (CRP) was used following the DJI P4M guidelines to account for abnormalities caused by variations in lighting conditions during multispectral image capture. Images were captured under ideal lighting conditions, typically between 10:00 and 14:00 to ensure consistency and high-quality data. Moreover, each flight took about twenty minutes, and the drone was operated at six meters above ground with a ground pixel, resolution of 3.06 mm/pixel. This altitude allowed precise and detailed photography of the cabbage field. Agisoft Metashape software version 2.0.3 (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.agisoft.com">https://www.agisoft.com</uri>) was used to mosaic approximately 300 original RGB images and 1500 multispectral images to produce an orthomosaic map of the entire field of the experiment. These efforts provide a fundamental basis for further research and applications in precision agriculture.</p></sec><sec id="sec2dot2dot2-sensors-25-05652"><title>2.2.2. Data Processing and Preparation for Cabbage Head Diameter Estimation Using Pose Estimation Techniques</title><p>Individual photographs of cabbages extracted from the orthophoto-GeoTIFF images were selected for this study. Two orthophotos were taken during the harvest period, with dimensions of 6513 &#215; 6072 pixels for plot-1 and 6380 &#215; 5948 pixels for plot-2. Initially, the precise locations of the centers of each cabbage were identified using ArcGIS, with a spacing of 0.4 m between plants and 0.6 m between rows. A square-shaped polygon 35 cm &#215; 35 cm graphic buffer was created around each cabbage. Square-shaped polygons were then applied to mask each cabbage, and the data format was changed from GeoTIFF to PNG. The original images, which had dimensions of 176 &#215; 176 pixels, were first padded to 192 &#215; 192 pixels and finally resized to 640 &#215; 640 pixels before being fed into the YOLO-pose models. The dataset was divided into 70, 20, and 10% for training, validation, and testing, respectively. A Roboflow interface (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://roboflow.com/">https://roboflow.com/</uri>) was used to annotate the data. Different augmentation methods were used to expand the number of datasets, such as changing the hue and saturation (25%), adding noise (8.9%), changing the brightness (46%), adding blurring (10%), changing the exposure (15%), and converting the data to grayscale (25%). These augmentations resulted in a training dataset of 1217 images. Following this, the YOLO (You Only Look Once)-pose estimation model was employed to estimate the cabbage head diameter. To fully cover all cabbage heads, seventeen random keypoints (reference points), including one at the center of each cabbage, were manually labeled using RoboFlow. After augmentation, the dataset was expanded, and the cabbage-pose model (cab-pose) was trained on the training dataset and validated using the test dataset. Subsequently, the center point was joined to the remaining 16 keypoints to calculate the radius. The ground sampling distance (GSD), defined as the real-world distance represented by a single pixel, was approximately 0.306 cm/pixel in this study. The pixel distances from the center to each keypoint were averaged and then multiplied by the GSD. Finally, the cabbage head diameter was determined by doubling the average radius (<xref rid="sensors-25-05652-f002" ref-type="fig">Figure 2</xref>).</p></sec><sec id="sec2dot2dot3-sensors-25-05652"><title>2.2.3. Climatic Variables</title><p>Land surface temperature and precipitation were used to assess their effects on cabbage production. The Landsat 8 Operational Land Imager and Thermal Infrared Sensor (OLI and TIRS) bands were used for temperature estimation using the Google Earth Engine platform (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://earthengine.google.com/">https://earthengine.google.com/</uri>). A cloud mask was applied to the Landsat 8 Surface Reflectance (L8SR) data using the pixel quality assessment (QA) band [<xref rid="B26-sensors-25-05652" ref-type="bibr">26</xref>,<xref rid="B32-sensors-25-05652" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05652" ref-type="bibr">33</xref>]. During this procedure, bits three and five of the QA band, representing shadows and clouds, respectively, were examined. A clear-sky mask was established by setting both cloud shadows and flags to zero, signifying cloud-free scenes [<xref rid="B33-sensors-25-05652" ref-type="bibr">33</xref>]. Furthermore, downscaling methods were used to downscale the spatial resolution of the Landsat 8 data to 10 m. Subsequently, we employed drone-derived NDVI, which was resampled in ArcGIS to match the desired spatial resolution [<xref rid="B34-sensors-25-05652" ref-type="bibr">34</xref>]. The data from September to December 2024 were averaged, resulting in a final temperature map for the research area, created using ArcGIS Desktop 10.8.2. Temperature data was gathered for each cabbage location in the field with ArcGIS Desktop 10.8.2 (Esri, Redlands, CA, USA). Rainfall data were downloaded from the JAXA Global Rainfall Watch (GSMaP), including 12 h cumulative rainfall records collected from September to December 2024. Ten random points (x, y) were collected from the GSM and applied Inverse Distance Weighting (IDW) interpolation to generate rainfall maps for the entire field. These maps were then averaged and masked to each cabbage plot, providing a single rainfall value per cabbage. Seasonal averages of precipitation and temperature were utilized for the yield estimation in the machine learning models (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://global.jaxa.jp/">https://global.jaxa.jp/</uri>, accessed on 2 September 2025) (<xref rid="sensors-25-05652-f002" ref-type="fig">Figure 2</xref>).</p></sec><sec id="sec2dot2dot4-sensors-25-05652"><title>2.2.4. Canopy Reflectance Indices</title><p>Multispectral imagery captured by the DJI P4 drone was used to compute critical canopy reflectance indices, including NDVI, NDRE, and CIg, which are essential for assessing plant growth and overall vitality. These indices provide valuable insights into the physiological status of crops and directly influence the accuracy of yield predictions. In this study, the P4 DJI drone equipped with a multispectral sensor captured data across specific spectral wavelengths: near-infrared (NIR) at 840 nm &#177; 26 nm, Red (RED) at 650 nm &#177; 26 nm, Green (G) at 560 nm &#177; 26 nm, and red edge (RE) at 730 nm &#177; 26 nm. Each of these bands serves a distinct purpose for evaluating different aspects of plant health (DJI, 2020) (<xref rid="sensors-25-05652-t001" ref-type="table">Table 1</xref>) [<xref rid="B35-sensors-25-05652" ref-type="bibr">35</xref>].</p><p>Consequently, the individual cabbage areas in the imagery were masked using certain indices (NDVI, NDRE, and CIg) in order to focus on specific plant regions [<xref rid="B12-sensors-25-05652" ref-type="bibr">12</xref>,<xref rid="B36-sensors-25-05652" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05652" ref-type="bibr">37</xref>]. The average values of each index for individual cabbage heads were then extracted and used as independent variables in the machine learning models aimed at predicting cabbage yield. By incorporating these vegetation indices, this study sought to enhance the precision and reliability of yield estimation by leveraging detailed spectral information provided by multispectral drone imagery (<xref rid="sensors-25-05652-f002" ref-type="fig">Figure 2</xref>).</p></sec></sec><sec id="sec2dot3-sensors-25-05652"><title>2.3. AI Models for Cabbage Head Diameter Estimation</title><p>The YOLOv8s- and YOLOv11s-pose estimation models were used to determine the diameter of the cabbage head. Two dates&#8212;3 December and 9 December 2024&#8212;were selected for both fields (Plots #1 and #2), corresponding to the period when cabbage heads had reached their maximum growth stage before harvesting. The pose models were learned from the COCO dataset and the classification models were learned from the ImageNet dataset (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/ultralytics/issues/1915">https://github.com/ultralytics/ultralytics/issues/1915</uri>, accessed on 2 September 2025); <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://docs.ultralytics.com/tasks/pose/">https://docs.ultralytics.com/tasks/pose/</uri>, accessed on 2 September 2025). Pose estimation is the process of finding and locating keypoints in images to determine the arrangement of an object in space. Keypoints represent different parts of an object, such as joints, landmarks, and other important features. A set of two- or three-dimensional coordinates usually indicates the locations of these keypoints. Pose estimation models identify the essential points of an object and provide associated confidence scores. This method facilitates the identification of components within a scene and their spatial relationships (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://docs.ultralytics.com/tasks/pose/">https://docs.ultralytics.com/tasks/pose/</uri>, accessed on 2 September 2025) [<xref rid="B38-sensors-25-05652" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05652" ref-type="bibr">39</xref>].</p></sec><sec id="sec2dot4-sensors-25-05652"><title>2.4. Machine Learning Models for Cabbage Head Fresh Weight Prediction</title><p>In this experiment, ML models were used to estimate the fresh weight of the cabbage heads. Several vegetation reflectance variables (green vegetation index (GNDVI), soil-adjusted vegetation index (SAVI), modified soil-adjusted vegetation index (MSAVI), modified chlorophyll absorption reflectance index (MCARI), chlorophyll index based on the red band (CIred) and modified simple ratio index (MSRI)) showed substantial multicollinearity; in particular, those with correlation values over 90% were not included in the analysis and representative indices&#8212;NDVI, NDRE, and CIg were selected to reduce redundancy. For the head fresh weight prediction, the last set of predictor variables was the mean average per cabbage NDVI, NDRE, CIg, mean seasonal rainfall (average September&#8211;December), mean seasonal temperature (average September&#8211;December), and head diameter. Several ML models such as Random Forest (RF), Support Vector Regressor (SVR), K-Neighbors Regressor (KNR), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM), and Categorical Boosting (CatBoost) were among the many machine learning techniques used in this study. Fivefold cross-validation was performed to guarantee consistent performance ratings and prevent overfitting. A random search method was used to determine the best hyperparameters for the models [<xref rid="B39-sensors-25-05652" ref-type="bibr">39</xref>] (<xref rid="sensors-25-05652-t002" ref-type="table">Table 2</xref>). Finally, using a pose estimation model, the head diameters of 226 cabbage plants that were not explicitly recorded during the field survey were approximated to facilitate a thorough yield projection. The fresh weight of the head was computed using the expected diameter, along with other independent variables. The performances of the models were evaluated using the Diebold&#8211;Mariano test. Finally, ArcGIS Desktop 10.8.2 was used to create the final spatial map of cabbage head fresh weight. A 0.4 m &#215; 0.6 m grid mesh was generated across the entire field using the Spatial Analyst tool. Subsequently, the predicted cabbage head fresh weight values were assigned to each grid cell, and the vector map was converted to a raster format representing cabbage head fresh weight in kilogram. The overall research schematic frameworks are explained in <xref rid="sensors-25-05652-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec2dot5-sensors-25-05652"><title>2.5. Accuracy Assessment</title><sec id="sec2dot5dot1-sensors-25-05652"><title>2.5.1. Head Keypoint Accuracy Assessment</title><p>The accuracy of the cabbage head diameter in the DLM was assessed using precision (P), recall (R), and mean average precision (mAP, calculated at the intersection across union criteria ranging from 50% to 95%) as assessment metrics to gauge the detection performance of the model for both boxes and poses [<xref rid="B40-sensors-25-05652" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05652" ref-type="bibr">41</xref>]. The assessment metrics are stated as follows:<disp-formula id="FD1-sensors-25-05652"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">&#160;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05652"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#160;</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">&#160;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05652"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>mAP</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8747;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mfenced><mml:mi>dr</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where TP, FP, and FN denote the number of true positives, false positives, and false negatives, respectively. P and R refer to precision and recall. <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the average precision, while <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> calculates the area under the precision&#8211;recall curve from recall 0 to 1. N refers to the total number of classes, which in this study is 1, corresponding to the target class &#8220;cabbage head.&#8221;</p></sec><sec id="sec2dot5dot2-sensors-25-05652"><title>2.5.2. Cabbage Head Diameter and Fresh Weight Accuracy Assessment</title><p>The cabbage head diameter was evaluated using the mean absolute error and relative error. The assessment metrics were as follows:<disp-formula id="FD4-sensors-25-05652"><label>(4)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#7923;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where MAE is the mean absolute error (cm), n is the total number of cabbage head diameter observations, y<sub>i</sub> is the actual (measured) diameter of the ith cabbage head, and &#7923;<sub>i</sub> denotes the predicted diameter of the ith cabbage head.<disp-formula id="FD5-sensors-25-05652"><label>(5)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#7923;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mrow><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where RE denotes the relative error (cm), n represents the total number of observations of cabbage head diameter, y<sub>i</sub> is the actual (measured) diameter of the ith cabbage head, and &#7923;<sub>i</sub> is the predicted diameter of the ith cabbage head.</p><p>The machine learning models were evaluated based on R<sup>2</sup> (the coefficient of determination) and root mean square error (RMSE). The equations are as follows:<disp-formula id="FD6-sensors-25-05652"><label>(6)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#160;</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#7923;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mo>&#956;</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05652"><label>(7)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>=</mml:mo><mml:mroot><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#7923;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow/></mml:mroot></mml:mrow></mml:mrow></mml:math></disp-formula>
where the R<sup>2</sup> indicates percentage variation in fresh cabbage head weight as explained by the model. It ranges from 0 to 1. A score of 0 denotes a poor fit and 1 indicates that the model perfectly fits the data. The &#956; represents the mean. The RMSE indicates root mean square error which measures the average magnitude of prediction errors. This represents the typical difference between the measured y<sub>i</sub> and the predicted fresh cabbage &#7923;<sub>i</sub> head weights.</p></sec><sec id="sec2dot5dot3-sensors-25-05652"><title>2.5.3. Performance Testing Using Diebold&#8211;Mariano Test</title><p>The Diebold&#8211;Mariano (DM) Test is a statistical technique employed to estimate the comparative prediction accuracy of two rival models. It assesses whether the differences in forecasting performance between models are statistically significant, based on a quantitative analysis framework. The DM statistic is calculated using the mean loss differential of the forecast errors from the two models, standardized by their estimated variance. A test is deemed significant when the <italic toggle="yes">p</italic>-value is less than the predetermined significance threshold (e.g., 0.05). In such instances, the null hypothesis is rejected, signifying that the prediction accuracy of the two models differs. A non-significant result indicates the absence of statistical evidence for a difference in predicting performance between the models. The efficacies of CatBoost with RF, CatBoost with XGBoost, and RF with XGBoost were evaluated by DM test. This test assists in identifying a model that yields accurate predictions. The DM formula is expressed as follows:<disp-formula id="FD8-sensors-25-05652"><label>(8)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mroot><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">&#224;</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow/></mml:mroot></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where DM signifies the Diebold&#8211;Mariano statistic, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the mean difference in the forecast errors (loss differential), T represents the total number of observations, and V&#224;r(d) describes the variance of the loss differential [<xref rid="B42-sensors-25-05652" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-05652" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05652" ref-type="bibr">44</xref>].</p></sec></sec><sec id="sec2dot6-sensors-25-05652"><title>2.6. Details of the Experimental Environment</title><p>In this study, the DL and ML models were executed in a system that was set up with the NVIDIA<sup>&#174;</sup> GeForce RTX 4070 SUPER GPU and an Intel(R) Core (TM) i7-14700KF CPU with 64 GB of RAM. This setup offers enhanced computational performance, facilitating efficient training and evaluation of models. Python 3.10.9 and PyTorch 2.5.1 were used for the configuration inside the Conda environment. To guarantee compatibility and the best performance for GPU-accelerated deep learning activities, the configuration also included CUDA 11.8 and cuDNN 8.8.0. Additionally, scikit-learn version 1.6.1 was used for the ML processes. This setup facilitates efficient model evaluation and training by improving performance.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05652"><title>3. Results</title><sec id="sec3dot1-sensors-25-05652"><title>3.1. Cabbage Head Diameter Estimation Using Pose Estimation Model</title><p>The models were trained for 800 epochs using the input images. The model had dimensions of 640 &#215; 640 pixels and a batch size of 32 (<xref rid="sensors-25-05652-t003" ref-type="table">Table 3</xref>). However, an early stopping strategy was applied, which automatically terminated training once the validation loss stopped improving. Additionally, the models were assessed over time in order to establish that both techniques progressively enhanced their capacity to identify keypoints around the cabbage head. The results indicated that in the YOLOv8s-pose model, the loss converged after approximately 120 epochs, and the training loss for the YOLOv11s-pose model stabilized after almost 400 epochs. For validation, the YOLOv11s model showed a steady loss after 250 epochs and the YOLOv8s model stabilized for approximately 120 epochs (<xref rid="sensors-25-05652-f003" ref-type="fig">Figure 3</xref>). This constant improvement in the detection accuracy during training revealed that the networks learned the spatial characteristics required for an exact CHD (cabbage head diameter) estimate.</p><p>YOLOv8s-pose model box detection attained a mean Average Precision (mAP@0.5) of 0.995, recall of 1.00, and precision of 0.89. Keypoint detection achieved a recall of 0.99, an mAP@0.5 of 0.98, and precision of 0.95 (<xref rid="sensors-25-05652-t004" ref-type="table">Table 4</xref>, <xref rid="sensors-25-05652-f004" ref-type="fig">Figure 4</xref>). However, YOLOv11s box detection had a precision of 0.96, recall of 1.00, and mAP@0.5 of 0.99, whereas keypoint detection had an mAP@0.5 of approximately 0.99, a precision of 0.97, and a recall of 0.99 (<xref rid="sensors-25-05652-t004" ref-type="table">Table 4</xref>, <xref rid="sensors-25-05652-f005" ref-type="fig">Figure 5</xref>). In conclusion, the results verified that the YOLOv11s-pose models generally showed improved stability and accuracy compared to YOLOv8s-pose models, which indicated a strong performance in estimating the cabbage head diameter (<xref rid="sensors-25-05652-f006" ref-type="fig">Figure 6</xref>a,b).</p><p>Following the identification of the best-performing model based on the validation criteria, particularly mAP@0.5, and loss plateauing behavior, the model was independently tested (<xref rid="sensors-25-05652-f006" ref-type="fig">Figure 6</xref>a,b). The test results showed that the highest confidence score was approximately 0.95, whereas the lowest score (recorded for only one image) was 0.80. This low confidence score caused the relative error to increase by approximately 12%, and the main reason was that part of the cabbage head was covered by leaves (<xref rid="sensors-25-05652-t005" ref-type="table">Table 5</xref>). At this point, the expected performance could be assessed statistically. Specifically, 16 keypoints detected on each cabbage head were connected to the central reference point, and the average radius was calculated. This radius, initially measured in pixels, was then converted to real-world units (cm) and multiplied by two in order to obtain the final diameter for each cabbage (<xref rid="sensors-25-05652-f006" ref-type="fig">Figure 6</xref>c). The predicted cabbage head diameter showed a mean relative error (MRE) of approximately 4.6% across all the test samples, indicating high accuracy. This minimal error margin demonstrates the model&#8217;s efficiency and strong potential for real-world agricultural applications that require precise and automated measurements of the cabbage head diameter (<xref rid="sensors-25-05652-f006" ref-type="fig">Figure 6</xref>c, <xref rid="sensors-25-05652-t005" ref-type="table">Table 5</xref>).</p></sec><sec id="sec3dot2-sensors-25-05652"><title>3.2. Cabbage Head Fresh Weight Prediction Using Machine Learning Models</title><p>The final set of explanatory variables included the mean individual cabbage NDVI, NDRE, CIg, cabbage head diameter, temperature, and rainfall; cabbage head fresh weight was designated as the response variable. Using field survey data, several machine learning (ML) models, including RF, SVR, KMN, LightGBM, XGBoost, and CatBoost, were developed, validated, and subsequently evaluated for previously unseen cabbage samples (<xref rid="sensors-25-05652-f007" ref-type="fig">Figure 7</xref>).</p><sec id="sec3dot2dot1-sensors-25-05652"><title>3.2.1. Cabbage Head Fresh Weight Machine Learning Model Performance Evaluation Incorporating R<sup>2</sup> and MSE</title><p>The performance of several machine learning models regarding cabbage head fresh weight prediction is listed in <xref rid="sensors-25-05652-t006" ref-type="table">Table 6</xref>. The RF and XGBoost R<sup>2</sup> values were 0.94; for CatBoost, it was 0.93; for both SVR and KNR, they were 0.90; and LightGBM produced an R<sup>2</sup> value of 0.88. However, the R<sup>2</sup> value alone does not fully show how well the models perform, so other measures such as MSE were also considered during both the training and testing phases, as presented in <xref rid="sensors-25-05652-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-05652-f007" ref-type="fig">Figure 7</xref> and <xref rid="sensors-25-05652-f008" ref-type="fig">Figure 8</xref>. Furthermore, the MSE values for the training phase were RF (0.011 kg/cabbage), XGBoost (0.012 kg/cabbage), SVR (0.019 kg/cabbage), KNR (0.021 kg/cabbage), LightGBM (0.022 kg/cabbage) and CatBoost ( kg/cabbage 0.014). The corresponding MSE values for the testing phase were CatBoost (0.025 kg/cabbage), XGBoost (0.027 kg/cabbage), RF (0.031 kg/cabbage), SVR (0.032 kg/cabbage), KNR (0.033 kg/cabbage), and LightGBM (0.046 kg/cabbage). Among these models, CatBoost demonstrated the lowest MSE in the testing stage, including the best generalized performance in terms of prediction loss (<xref rid="sensors-25-05652-f007" ref-type="fig">Figure 7</xref>).</p><p>Additionally, ArcGIS Desktop 10.8.2 (Esri, Redlands, CA, USA) was used to create a spatial prediction map of individual cabbage head fresh weights (kg per cabbage). The results indicated that the dark green areas in the produced spatially predicted map showed the highest yield&#8212;between 2.6 and 3.1 kg&#8212;whereas the red areas showed the lowest yield, between 0 and 0.7 kg (<xref rid="sensors-25-05652-f009" ref-type="fig">Figure 9</xref>).</p></sec><sec id="sec3dot2dot2-sensors-25-05652"><title>3.2.2. Evaluation of Best Model Performance (Random Forest, Extreme Gradient Boosting and Categorical Boosting) by Diebold&#8211;Mariano Statistical Test</title><p>This study assessed several MLMs based on their predicted efficacy. As shown in <xref rid="sensors-25-05652-t007" ref-type="table">Table 7</xref>, CatBoost outperformed all the other models. The test results indicated that CatBoost had significantly fewer errors than both RF and XGBoost. Moreover, there was no significant difference in predictive performance between the RF and XGBoost models. Although RF and XGBoost exhibited the highest R<sup>2</sup> values, their prediction errors were higher than those of CatBoost (<xref rid="sensors-25-05652-t007" ref-type="table">Table 7</xref>).</p></sec></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05652"><title>4. Discussion</title><p>Deep learning models have been used to identify plant features, such as recognizing individual cabbages or drawing bounding boxes around them. They have also been used to predict the aboveground fresh weight of cabbage using RGB and multispectral images from cameras or UAVs [<xref rid="B8-sensors-25-05652" ref-type="bibr">8</xref>,<xref rid="B11-sensors-25-05652" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05652" ref-type="bibr">12</xref>]. Building on this foundation, our study advanced these approaches in two stages. In the first stage, a deep learning model was used to estimate the head diameters of individual cabbages. In the second stage, the diameter derived from the pose model was combined with multispectral and climatic data to accurately predict the fresh head weight of individual cabbages. These values reflect the physiological and environmental conditions of cabbage plants; thus, they help forecast the vital predictive ability of the head for fresh weight, which is crucial for supply chain management.</p><p>A comparison between deep learning models, namely the YOLOv11s-pose and YOLOv8s-pose, confirmed that the YOLOv11s-pose achieved better keypoint detection accuracy and greater model stability, with a low mean relative error (MRE) of 4.6% and a high mAP@0.5 of 98.5%. These results underscore the high potential of precision agriculture applications. These findings demonstrate that YOLOv11s-pose can successfully recognize and estimate spatial features from RGB images captured by UAVs, enabling agriculture to be monitored accurately and on a larger scale. These results matched recent improvements in orchard [<xref rid="B41-sensors-25-05652" ref-type="bibr">41</xref>], in which YOLOv11-pose and vision transformer-based depth estimation models (such as Depth Anything V2) were very successful in estimating the 3D positions of young green apples. Whereas Depth Anything V2 produced the lowest RMSE (1.52) and MAE (1.28) for depth estimation [<xref rid="B45-sensors-25-05652" ref-type="bibr">45</xref>], YOLOv11n obtained a pose accuracy of 0.92 and box accuracy of 0.91. These results demonstrate that YOLOv11 designs are useful for a wide range of horticultural tasks.</p><p>A recent study [<xref rid="B46-sensors-25-05652" ref-type="bibr">46</xref>] presented a method for precise blueberry detection, 3D spatial localization, and pose estimation using visual perception with the YOLOv11 model. The experimental results demonstrated a high detection accuracy of 95.8% mAP50-95, a positioning error of 7.2 mm within 0.5 m, and an average pose error of 19.2&#176; [<xref rid="B46-sensors-25-05652" ref-type="bibr">46</xref>]. Although another study focused on the robotic thinning of green apples, it highlighted the strong cross-application potential of artificial intelligence by using similar AI models for yield prediction in open-field vegetable crops [<xref rid="B41-sensors-25-05652" ref-type="bibr">41</xref>]. Overall, these studies have pointed to a growing trend toward non-invasive, AI-enabled crop-monitoring systems that can drastically reduce labor needs, greatly increase the spatial resolution of measurements, and improve decision-making in field-based production.</p><p>Among the machine learning models evaluated in our study, the CatBoost model emerged as the most effective, with the lowest test-phase mean squared error (MSE) of 0.025 kg/cabbage and a strong R<sup>2</sup> value of 0.89. Random Forest and XGBoost achieved slightly lower R<sup>2</sup> values test-phase (both testing 0.85) (<xref rid="sensors-25-05652-f008" ref-type="fig">Figure 8</xref>), and the Diebold&#8211;Mariano test confirmed that CatBoost&#8217;s predictions were statistically more accurate (<italic toggle="yes">p</italic> &lt; 0.05), illustrating its strength in handling complex feature interactions in agricultural data (<xref rid="sensors-25-05652-t007" ref-type="table">Table 7</xref>). This finding is consistent with previous studies [<xref rid="B47-sensors-25-05652" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-05652" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05652" ref-type="bibr">49</xref>], who combined satellite data, applied the Random Forest, Support Vector, and CatBoost models and obtained an R<sup>2</sup> of 0.95 and an MAE of 0.31 for the CatBoost model [<xref rid="B47-sensors-25-05652" ref-type="bibr">47</xref>]. Climate and pesticide data utilized by [<xref rid="B48-sensors-25-05652" ref-type="bibr">48</xref>] were also employed with CatBoost for rice yield prediction, achieving an R<sup>2</sup> of 0.80 and RMSE of 0.24 [<xref rid="B48-sensors-25-05652" ref-type="bibr">48</xref>]. Another study reported that CatBoost outperformed both LightGBM and XGBoost in predicting eggplant yield [<xref rid="B49-sensors-25-05652" ref-type="bibr">49</xref>]. These consistent results highlight CatBoost&#8217;s strength in modeling agricultural yields because it was developed based on a mechanism called ordering boosting, which makes it resistant to overfitting. This feature enables the model to make better predictions and generalizations.</p><p>The primary limitation of this study is that only three Japanese cabbage varieties, Ranbu, Tenku, and Mikuni, were used to estimate cabbage diameter. Our next step is to apply the model to additional Japanese cabbage varieties, which will allow us to validate its generalizability and robustness for future studies. Furthermore, in the field, we plan to implement variable-rate fertilizers using the yield map developed in this study. By comparing the next year&#8217;s yield map with the current yield map at the end of the maturity stage, we aim to assess temporal yield variability and evaluate the impact of variable-rate input techniques on cabbage production.</p><p>Overall, our research findings have conclusively shown that a framework using AI, and an integrated UAV is applicable for precision vegetable cultivation. This technique enhances decision-making in the supply chain by delivering precise and spatially explicit yield predictions. This study advances the evolution of agriculture using intelligent data-driven approaches that promote sustainable and efficient crop management.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05652"><title>5. Conclusions</title><p>This study has revealed how well computer vision and machine learning technique (MLT) can be combined to accurately estimate cabbage head diameter and predict fresh weight. Pose estimation models were tested in this research, and YOLOv11s-pose stood out for its stability and higher accuracy compared to YOLOv8s-pose, achieving strong keypoint detection performance and high mAP@0.5 scores. The diameter estimated using YOLOv11 had a low MARE of 4.6%, confirming the reliability of the model for practical use in agriculture. Moreover, the head fresh weight was predicted using machine learning models incorporating climatic and agronomic features. CatBoost emerged as a superior machine learning model, with the lowest mean squared error (MSE) of 0.014 for training and 0.025 for testing, and R<sup>2</sup> values of 0.93 for training and 0.89 for testing, respectively. While Random Forest and XGBoost had higher R<sup>2</sup> values during training (0.94), their performance during testing was worse, yielding R<sup>2</sup> values of 0.85 and elevated MSEs of 0.011 kg (RF training), 0.031 kg (RF testing), 0.012 kg (XGBoost training), and 0.027 kg (XGBoost testing). ArcGIS software was used to map spatial variations in cabbage production and to further use the map for precise production. The findings indicate that integrating artificial intelligence methodologies with UAV footage offers a non-destructive, precise, and scalable approach to monitor cabbage diameters and predict yields in actual agricultural settings.</p></sec></body><back><ack><title>Acknowledgments</title><p>We express our sincere gratitude to the United States Geological Survey (USGS) for supplying satellite data, the developers of the Google Earth Engine platform for enabling rapid computation and access to satellite datasets, and the Japan Aerospace Exploration Agency (JAXA) for making global rainfall datasets publicly available for research purposes. The team supported us during the harvest survey, and we thank M. Yokoyama for his invaluable support during drone operations.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, S.T.A.; methodology, S.T.A. and A.T.; software, S.T.A. and M.K.; formal analysis, S.T.A.; resources, M.K., Y.N., K.T., S.K. and K.H.; drone data collection, M.K. and Y.N.; ground survey, S.T.A., A.T. and K.T.; creating orthomosic images, M.K.; writing&#8212;original draft preparation, S.T.A.; writing&#8212;review and editing, K.H., A.T., M.K.,Y.N., S.K. and K.T.; supervision, K.H., A.T., K.T., Y.N. and S.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Access to the data used in this study is restricted due to licensing agreements. As such, the data is not publicly available. However, they may be obtained from the corresponding author upon reasonable request and with appropriate permission.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">UAV</td><td align="left" valign="middle" rowspan="1" colspan="1">Unmanned aerial vehicle</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO</td><td align="left" valign="middle" rowspan="1" colspan="1">You only look once</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MARE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean absolute relative error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NDVI</td><td align="left" valign="middle" rowspan="1" colspan="1">Normalized difference vegetation index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NDRE</td><td align="left" valign="middle" rowspan="1" colspan="1">Normalized differences red edge index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CIg</td><td align="left" valign="middle" rowspan="1" colspan="1">Green chlorophyll Index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MLMs</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine learning models</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DLT</td><td align="left" valign="middle" rowspan="1" colspan="1">Deep learning techniques</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LST</td><td align="left" valign="middle" rowspan="1" colspan="1">Land surface temperature</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">JAXA</td><td align="left" valign="middle" rowspan="1" colspan="1">Japan aerospace exploration agency</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CRP</td><td align="left" valign="middle" rowspan="1" colspan="1">Calibrated reflectance panel</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OLI </td><td align="left" valign="middle" rowspan="1" colspan="1">Operational land imager</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TIRS</td><td align="left" valign="middle" rowspan="1" colspan="1">Thermal infrared sensor</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">QA</td><td align="left" valign="middle" rowspan="1" colspan="1">quality assessment</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNDVI</td><td align="left" valign="middle" rowspan="1" colspan="1">Green normalized vegetation index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAVI</td><td align="left" valign="middle" rowspan="1" colspan="1">Soil-adjusted vegetation index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCARI</td><td align="left" valign="middle" rowspan="1" colspan="1">Modified chlorophyll absorption reflectance index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSAVI</td><td align="left" valign="middle" rowspan="1" colspan="1">Modified soil-adjusted vegetation index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RF</td><td align="left" valign="middle" rowspan="1" colspan="1">Random forest</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SVR</td><td align="left" valign="middle" rowspan="1" colspan="1">Support vector regressor</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KNR</td><td align="left" valign="middle" rowspan="1" colspan="1">K-neighbors regressor</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">XGBoost</td><td align="left" valign="middle" rowspan="1" colspan="1">Extreme gradient boosting</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LightGBM</td><td align="left" valign="middle" rowspan="1" colspan="1">Light gradient boosting Machine</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CatBoost</td><td align="left" valign="middle" rowspan="1" colspan="1">Categorical gradient boosting</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DM</td><td align="left" valign="middle" rowspan="1" colspan="1">Diebold&#8211;Mariano</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mAP</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean average precision</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CHD</td><td align="left" valign="middle" rowspan="1" colspan="1">Cabbage head diameter</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05652"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mugisha</surname><given-names>B.</given-names></name><name name-style="western"><surname>Agole</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ewing</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Wacal</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kule</surname><given-names>E.B.</given-names></name></person-group><article-title>Determinants of Adoption of Climate-Smart Agriculture Practices among Farmers in Sheema District, Western Uganda</article-title><source>J. Int. Agric. Ext. Educ.</source><year>2025</year><volume>32</volume><fpage>204</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.4148/2831-5960.1164</pub-id></element-citation></ref><ref id="B2-sensors-25-05652"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Levi&#228;kangas</surname><given-names>P.</given-names></name><name name-style="western"><surname>S&#248;nvisen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Casado-Mansilla</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mikalsen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cimmino</surname><given-names>A.</given-names></name><name name-style="western"><surname>Drosou</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>S.</given-names></name></person-group><article-title>Towards Smart, Digitalised Rural Regions and Communities&#8212;Policies, Best Practices and Case Studies</article-title><source>Technol. Soc.</source><year>2025</year><volume>81</volume><fpage>102824</fpage><pub-id pub-id-type="doi">10.1016/j.techsoc.2025.102824</pub-id></element-citation></ref><ref id="B3-sensors-25-05652"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bissadu</surname><given-names>K.D.</given-names></name><name name-style="western"><surname>Sonko</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hossain</surname><given-names>G.</given-names></name></person-group><article-title>Society 5.0 Enabled Agriculture: Drivers, Enabling Technologies, Architectures, Opportunities, and Challenges</article-title><source>Inf. Process. Agric.</source><year>2025</year><volume>12</volume><fpage>112</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.inpa.2024.04.003</pub-id></element-citation></ref><ref id="B4-sensors-25-05652"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nanseki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chomei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kuang</surname><given-names>J.</given-names></name></person-group><article-title>A Review of Smart Agriculture and Production Practices in Japanese Large-Scale Rice Farming</article-title><source>J. Sci. Food Agric.</source><year>2023</year><volume>103</volume><fpage>1609</fpage><lpage>1620</lpage><pub-id pub-id-type="doi">10.1002/jsfa.12204</pub-id><pub-id pub-id-type="pmid">36069313</pub-id></element-citation></ref><ref id="B5-sensors-25-05652"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Subeesh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Prakash Kumar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kumar Chakraborty</surname><given-names>S.</given-names></name><name name-style="western"><surname>Upendar</surname><given-names>K.</given-names></name><name name-style="western"><surname>Singh Chandel</surname><given-names>N.</given-names></name><name name-style="western"><surname>Jat</surname><given-names>D.</given-names></name><name name-style="western"><surname>Dubey</surname><given-names>K.</given-names></name><name name-style="western"><surname>Modi</surname><given-names>R.U.</given-names></name><name name-style="western"><surname>Mazhar Khan</surname><given-names>M.</given-names></name></person-group><article-title>UAV Imagery Coupled Deep Learning Approach for the Development of an Adaptive In-House Web-Based Application for Yield Estimation in Citrus Orchard</article-title><source>Measurement</source><year>2024</year><volume>234</volume><fpage>114786</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2024.114786</pub-id></element-citation></ref><ref id="B6-sensors-25-05652"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.-M.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>C.-P.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.-H.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>J.-X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.-M.</given-names></name><name name-style="western"><surname>Xuan</surname><given-names>Y.-M.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>D.-M.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.-F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.-C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>X.-Q.</given-names></name></person-group><article-title>Tea Yield Estimation Using UAV Images and Deep Learning</article-title><source>Ind. Crops Prod.</source><year>2024</year><volume>212</volume><fpage>118358</fpage><pub-id pub-id-type="doi">10.1016/j.indcrop.2024.118358</pub-id></element-citation></ref><ref id="B7-sensors-25-05652"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tanaka</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Watanabe</surname><given-names>T.</given-names></name><name name-style="western"><surname>Katsura</surname><given-names>K.</given-names></name><name name-style="western"><surname>Tsujimoto</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Takai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tanaka</surname><given-names>T.S.T.</given-names></name><name name-style="western"><surname>Kawamura</surname><given-names>K.</given-names></name><name name-style="western"><surname>Saito</surname><given-names>H.</given-names></name><name name-style="western"><surname>Homma</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mairoua</surname><given-names>S.G.</given-names></name><etal/></person-group><article-title>Deep Learning Enables Instant and Versatile Estimation of Rice Yield Using Ground-Based RGB Images</article-title><source>Plant Phenomics</source><year>2023</year><volume>5</volume><fpage>0073</fpage><pub-id pub-id-type="doi">10.34133/plantphenomics.0073</pub-id><pub-id pub-id-type="pmid">38239736</pub-id><pub-id pub-id-type="pmcid">PMC10795498</pub-id></element-citation></ref><ref id="B8-sensors-25-05652"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aguilar-Ariza</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>M.</given-names></name><name name-style="western"><surname>Miyazaki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Saito</surname><given-names>A.</given-names></name><name name-style="western"><surname>Khaing</surname><given-names>H.P.</given-names></name><name name-style="western"><surname>Phoo</surname><given-names>H.W.</given-names></name><name name-style="western"><surname>Kondo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Fujiwara</surname><given-names>T.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kamiya</surname><given-names>T.</given-names></name></person-group><article-title>UAV-Based Individual Chinese Cabbage Weight Prediction Using Multi-Temporal Data</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>20122</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-47431-y</pub-id><pub-id pub-id-type="pmid">37978327</pub-id><pub-id pub-id-type="pmcid">PMC10656565</pub-id></element-citation></ref><ref id="B9-sensors-25-05652"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>A Comparison between Pixel-Based Deep Learning and Object-Based Image Analysis (OBIA) for Individual Detection of Cabbage Plants Based on UAV Visible-Light Images</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>209</volume><fpage>107822</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2023.107822</pub-id></element-citation></ref><ref id="B10-sensors-25-05652"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>C.-J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.-D.</given-names></name><name name-style="western"><surname>Tseng</surname><given-names>H.-H.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>Y.-C.</given-names></name><name name-style="western"><surname>Sung</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.-L.</given-names></name></person-group><article-title>Single-Plant Broccoli Growth Monitoring Using Deep Learning with UAV Imagery</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>207</volume><fpage>107739</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2023.107739</pub-id></element-citation></ref><ref id="B11-sensors-25-05652"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>D.-W.</given-names></name><name name-style="western"><surname>Yun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>S.-J.</given-names></name><name name-style="western"><surname>Kwon</surname><given-names>Y.-S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.-G.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H.-J.</given-names></name></person-group><article-title>Modeling and Testing of Growth Status for Chinese Cabbage and White Radish with UAV-Based RGB Imagery</article-title><source>Remote Sens.</source><year>2018</year><volume>10</volume><elocation-id>563</elocation-id><pub-id pub-id-type="doi">10.3390/rs10040563</pub-id></element-citation></ref><ref id="B12-sensors-25-05652"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>D.-H.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.-H.</given-names></name></person-group><article-title>Development of a UAS-Based Multi-Sensor Deep Learning Model for Predicting Napa Cabbage Fresh Weight and Determining Optimal Harvest Time</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>3455</elocation-id><pub-id pub-id-type="doi">10.3390/rs16183455</pub-id></element-citation></ref><ref id="B13-sensors-25-05652"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name></person-group><article-title>CabbageNet: Deep Learning for High-Precision Cabbage Segmentation in Complex Settings for Autonomous Harvesting Robotics</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>8115</elocation-id><pub-id pub-id-type="doi">10.3390/s24248115</pub-id><pub-id pub-id-type="pmid">39771849</pub-id><pub-id pub-id-type="pmcid">PMC11678936</pub-id></element-citation></ref><ref id="B14-sensors-25-05652"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yokoyama</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Matsui</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tanaka</surname><given-names>T.S.T.</given-names></name></person-group><article-title>An Instance Segmentation Dataset of Cabbages over the Whole Growing Season for UAV Imagery</article-title><source>Data Brief</source><year>2024</year><volume>55</volume><fpage>110699</fpage><pub-id pub-id-type="doi">10.1016/j.dib.2024.110699</pub-id><pub-id pub-id-type="pmid">39044907</pub-id><pub-id pub-id-type="pmcid">PMC11263960</pub-id></element-citation></ref><ref id="B15-sensors-25-05652"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garc&#237;a-Fern&#225;ndez</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sanz-Ablanedo</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rodr&#237;guez-P&#233;rez</surname><given-names>J.R.</given-names></name></person-group><article-title>High-Resolution Drone-Acquired RGB Imagery to Estimate Spatial Grape Quality Variability</article-title><source>Agronomy</source><year>2021</year><volume>11</volume><elocation-id>655</elocation-id><pub-id pub-id-type="doi">10.3390/agronomy11040655</pub-id></element-citation></ref><ref id="B16-sensors-25-05652"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kovacs</surname><given-names>J.M.</given-names></name></person-group><article-title>The Application of Small Unmanned Aerial Systems for Precision Agriculture: A Review</article-title><source>Precis. Agric.</source><year>2012</year><volume>13</volume><fpage>693</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1007/s11119-012-9274-5</pub-id></element-citation></ref><ref id="B17-sensors-25-05652"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lobell</surname><given-names>D.B.</given-names></name><name name-style="western"><surname>Cassman</surname><given-names>K.G.</given-names></name><name name-style="western"><surname>Field</surname><given-names>C.B.</given-names></name></person-group><article-title>Crop Yield Gaps: Their Importance, Magnitudes, and Causes</article-title><source>Annu. Rev. Environ. Resour.</source><year>2009</year><volume>34</volume><fpage>179</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1146/annurev.environ.041008.093740</pub-id></element-citation></ref><ref id="B18-sensors-25-05652"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mia</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Tanabe</surname><given-names>R.</given-names></name><name name-style="western"><surname>Habibi</surname><given-names>L.N.</given-names></name><name name-style="western"><surname>Hashimoto</surname><given-names>N.</given-names></name><name name-style="western"><surname>Homma</surname><given-names>K.</given-names></name><name name-style="western"><surname>Maki</surname><given-names>M.</given-names></name><name name-style="western"><surname>Matsui</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tanaka</surname><given-names>T.S.T.</given-names></name></person-group><article-title>Multimodal Deep Learning for Rice Yield Prediction Using UAV-Based Multispectral Imagery and Weather Data</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>2511</elocation-id><pub-id pub-id-type="doi">10.3390/rs15102511</pub-id></element-citation></ref><ref id="B19-sensors-25-05652"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Herrero-Huerta</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rodriguez-Gonzalvez</surname><given-names>P.</given-names></name><name name-style="western"><surname>Rainey</surname><given-names>K.M.</given-names></name></person-group><article-title>Yield Prediction by Machine Learning from UAS-Based Multi-Sensor Data Fusion in Soybean</article-title><source>Plant Methods</source><year>2020</year><volume>16</volume><fpage>78</fpage><pub-id pub-id-type="doi">10.1186/s13007-020-00620-6</pub-id><pub-id pub-id-type="pmid">32514286</pub-id><pub-id pub-id-type="pmcid">PMC7268475</pub-id></element-citation></ref><ref id="B20-sensors-25-05652"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Walsh</surname><given-names>O.S.</given-names></name><name name-style="western"><surname>Nambi</surname><given-names>E.</given-names></name><name name-style="western"><surname>Shafian</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jayawardena</surname><given-names>D.M.</given-names></name><name name-style="western"><surname>Ansah</surname><given-names>E.O.</given-names></name><name name-style="western"><surname>Lamichhane</surname><given-names>R.</given-names></name><name name-style="western"><surname>McClintick-Chess</surname><given-names>J.R.</given-names></name></person-group><article-title>UAV-based NDVI Estimation of Sugarbeet Yield and Quality under Varied Nitrogen and Water Rates</article-title><source>Agrosyst. Geosci. Environ.</source><year>2023</year><volume>6</volume><fpage>e20337</fpage><pub-id pub-id-type="doi">10.1002/agg2.20337</pub-id></element-citation></ref><ref id="B21-sensors-25-05652"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaur</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Shellenbarger</surname><given-names>H.</given-names></name><name name-style="western"><surname>Griffin</surname><given-names>W.</given-names></name><name name-style="western"><surname>Serrano</surname><given-names>T.</given-names></name><name name-style="western"><surname>Brym</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sandhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>L.K.</given-names></name></person-group><article-title>Drone and Handheld Sensors for Hemp: Evaluating NDVI and NDRE in Relation to Nitrogen Application and Crop Yield</article-title><source>Agrosyst. Geosci. Environ.</source><year>2025</year><volume>8</volume><fpage>e70075</fpage><pub-id pub-id-type="doi">10.1002/agg2.70075</pub-id></element-citation></ref><ref id="B22-sensors-25-05652"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pipatsitee</surname><given-names>P.</given-names></name><name name-style="western"><surname>Tisarum</surname><given-names>R.</given-names></name><name name-style="western"><surname>Taota</surname><given-names>K.</given-names></name><name name-style="western"><surname>Samphumphuang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Eiumnoh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>H.P.</given-names></name><name name-style="western"><surname>Cha-um</surname><given-names>S.</given-names></name></person-group><article-title>Effectiveness of Vegetation Indices and UAV-Multispectral Imageries in Assessing the Response of Hybrid Maize (<italic toggle="yes">Zea mays</italic> L.) to Water Deficit Stress under Field Environment</article-title><source>Environ. Monit. Assess.</source><year>2023</year><volume>195</volume><fpage>128</fpage><pub-id pub-id-type="doi">10.1007/s10661-022-10766-6</pub-id><pub-id pub-id-type="pmid">36402920</pub-id></element-citation></ref><ref id="B23-sensors-25-05652"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Timmer</surname><given-names>B.</given-names></name><name name-style="western"><surname>Reshitnyk</surname><given-names>L.Y.</given-names></name><name name-style="western"><surname>Hessing-Lewis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Juanes</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gendall</surname><given-names>L.</given-names></name><name name-style="western"><surname>Costa</surname><given-names>M.</given-names></name></person-group><article-title>Capturing Accurate Kelp Canopy Extent: Integrating Tides, Currents, and Species-Level Morphology in Kelp Remote Sensing</article-title><source>Front. Environ. Sci.</source><year>2024</year><volume>12</volume><elocation-id>1338483</elocation-id><pub-id pub-id-type="doi">10.3389/fenvs.2024.1338483</pub-id></element-citation></ref><ref id="B24-sensors-25-05652"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meivel</surname><given-names>S.</given-names></name><name name-style="western"><surname>Maheswari</surname><given-names>S.</given-names></name></person-group><article-title>Monitoring of Potato Crops Based on Multispectral Image Feature Extraction with Vegetation Indices</article-title><source>Multidimens. Syst. Signal Process.</source><year>2022</year><volume>33</volume><fpage>683</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1007/s11045-021-00809-5</pub-id></element-citation></ref><ref id="B25-sensors-25-05652"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shamsuzzoha</surname><given-names>M.</given-names></name><name name-style="western"><surname>Noguchi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ahamed</surname><given-names>T.</given-names></name></person-group><article-title>Damaged Area Assessment of Cultivated Agricultural Lands Affected by Cyclone Bulbul in Coastal Region of Bangladesh Using Landsat 8 OLI and TIRS Datasets</article-title><source>Remote Sens. Appl.</source><year>2021</year><volume>23</volume><fpage>100523</fpage><pub-id pub-id-type="doi">10.1016/j.rsase.2021.100523</pub-id></element-citation></ref><ref id="B26-sensors-25-05652"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arab</surname><given-names>S.T.</given-names></name><name name-style="western"><surname>Noguchi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ahamed</surname><given-names>T.</given-names></name></person-group><article-title>Yield Loss Assessment of Grapes Using Composite Drought Index Derived from Landsat OLI and TIRS Datasets</article-title><source>Remote Sens. Appl.</source><year>2022</year><volume>26</volume><fpage>100727</fpage><pub-id pub-id-type="doi">10.1016/j.rsase.2022.100727</pub-id></element-citation></ref><ref id="B27-sensors-25-05652"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Oyoshi</surname><given-names>K.</given-names></name><name name-style="western"><surname>Sobue</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kimura</surname><given-names>S.</given-names></name></person-group><article-title>Rice Growth Outlook Using Satellite-Based Information in Southeast Asia</article-title><source>Remote Sensing of Agriculture and Land Cover/Land Use Changes in South and Southeast Asian Countries</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>91</fpage><lpage>104</lpage></element-citation></ref><ref id="B28-sensors-25-05652"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aka</surname><given-names>K.S.R.</given-names></name><name name-style="western"><surname>Akpavi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dibi</surname><given-names>N.H.</given-names></name><name name-style="western"><surname>Kabo-Bah</surname><given-names>A.T.</given-names></name><name name-style="western"><surname>Gyilbag</surname><given-names>A.</given-names></name><name name-style="western"><surname>Boamah</surname><given-names>E.</given-names></name></person-group><article-title>Toward Understanding Land Use Land Cover Changes and Their Effects on Land Surface Temperature in Yam Production Area, C&#244;te d&#8217;Ivoire, Gontougo Region, Using Remote Sensing and Machine Learning Tools (Google Earth Engine)</article-title><source>Front. Remote Sens.</source><year>2023</year><volume>4</volume><elocation-id>1221757</elocation-id><pub-id pub-id-type="doi">10.3389/frsen.2023.1221757</pub-id></element-citation></ref><ref id="B29-sensors-25-05652"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>He</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>Land Surface Temperature Retrieval for Arid Regions Based on Landsat-8 TIRS Data: A Case Study in Shihezi, Northwest China</article-title><source>J. Arid. Land</source><year>2014</year><volume>6</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1007/s40333-014-0071-z</pub-id></element-citation></ref><ref id="B30-sensors-25-05652"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Skofronick-Jackson</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kirschbaum</surname><given-names>D.</given-names></name><name name-style="western"><surname>Petersen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huffman</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kidd</surname><given-names>C.</given-names></name><name name-style="western"><surname>Stocker</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kakar</surname><given-names>R.</given-names></name></person-group><article-title>The Global Precipitation Measurement (GPM) Mission&#8217;s Scientific Achievements and Societal Contributions: Reviewing Four Years of Advanced Rain and Snow Observations</article-title><source>Q. J. R. Meteorol. Soc.</source><year>2018</year><volume>144</volume><fpage>27</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1002/qj.3313</pub-id><pub-id pub-id-type="pmid">31213729</pub-id><pub-id pub-id-type="pmcid">PMC6581458</pub-id></element-citation></ref><ref id="B31-sensors-25-05652"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name></person-group><article-title>Applications of Deep Learning for Dense Scenes Analysis in Agriculture: A Review</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>1520</elocation-id><pub-id pub-id-type="doi">10.3390/s20051520</pub-id><pub-id pub-id-type="pmid">32164200</pub-id><pub-id pub-id-type="pmcid">PMC7085505</pub-id></element-citation></ref><ref id="B32-sensors-25-05652"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tariq</surname><given-names>A.</given-names></name><name name-style="western"><surname>Riaz</surname><given-names>I.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Amin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kausar</surname><given-names>R.</given-names></name><name name-style="western"><surname>Andleeb</surname><given-names>S.</given-names></name><name name-style="western"><surname>Farooqi</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Rafiq</surname><given-names>M.</given-names></name></person-group><article-title>Land Surface Temperature Relation with Normalized Satellite Indices for the Estimation of Spatio-Temporal Trends in Temperature among Various Land Use Land Cover Classes of an Arid Potohar Region Using Landsat Data</article-title><source>Environ. Earth Sci.</source><year>2020</year><volume>79</volume><fpage>40</fpage><pub-id pub-id-type="doi">10.1007/s12665-019-8766-2</pub-id></element-citation></ref><ref id="B33-sensors-25-05652"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Woodcock</surname><given-names>C.E.</given-names></name></person-group><article-title>Object-Based Cloud and Cloud Shadow Detection in Landsat Imagery</article-title><source>Remote Sens. Environ.</source><year>2012</year><volume>118</volume><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2011.10.028</pub-id></element-citation></ref><ref id="B34-sensors-25-05652"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ona&#269;illov&#225;</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gallay</surname><given-names>M.</given-names></name><name name-style="western"><surname>Paluba</surname><given-names>D.</given-names></name><name name-style="western"><surname>P&#233;liov&#225;</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tokar&#269;&#237;k</surname><given-names>O.</given-names></name><name name-style="western"><surname>Laubertov&#225;</surname><given-names>D.</given-names></name></person-group><article-title>Combining Landsat 8 and Sentinel-2 Data in Google Earth Engine to Derive Higher Resolution Land Surface Temperature Maps in Urban Environment</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>4076</elocation-id><pub-id pub-id-type="doi">10.3390/rs14164076</pub-id></element-citation></ref><ref id="B35-sensors-25-05652"><label>35.</label><element-citation publication-type="webpage"><article-title>DJI Phantom 4 Multispectral User Manual</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dl.djicdn.com/downloads/p4-multispectral/20190927/P4_Multispectral_User_Manual_v1.0_EN.pdf" ext-link-type="uri">https://dl.djicdn.com/downloads/p4-multispectral/20190927/P4_Multispectral_User_Manual_v1.0_EN.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-02">(accessed on 2 September 2025)</date-in-citation></element-citation></ref><ref id="B36-sensors-25-05652"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sellers</surname><given-names>P.J.</given-names></name></person-group><article-title>International Journal of Remote Sensing Canopy Reflectance, Photosynthesis and Transpiration</article-title><source>Int. J. Remote Sens.</source><year>1985</year><volume>6</volume><fpage>1335</fpage><lpage>1372</lpage><pub-id pub-id-type="doi">10.1080/01431168508948283</pub-id></element-citation></ref><ref id="B37-sensors-25-05652"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>W.</given-names></name></person-group><article-title>Estimating Chlorophyll Content from Hyperspectral Vegetation Indices: Modeling and Validation</article-title><source>Agric. For. Meteorol.</source><year>2008</year><volume>148</volume><fpage>1230</fpage><lpage>1241</lpage><pub-id pub-id-type="doi">10.1016/j.agrformet.2008.03.005</pub-id></element-citation></ref><ref id="B38-sensors-25-05652"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Maji</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nagori</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mathew</surname><given-names>M.</given-names></name><name name-style="western"><surname>Poddar</surname><given-names>D.</given-names></name></person-group><article-title>YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date></element-citation></ref><ref id="B39-sensors-25-05652"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bergstra</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ca</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Ca</surname><given-names>Y.B.</given-names></name></person-group><article-title>Random Search for Hyper-Parameter Optimization Yoshua Bengio</article-title><source>J. Mach. Learn. Res.</source><year>2012</year><volume>13</volume><fpage>281</fpage><lpage>305</lpage></element-citation></ref><ref id="B40-sensors-25-05652"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>J.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>W.</given-names></name></person-group><article-title>Research on Human Posture Estimation Algorithm Based on YOLO-Pose</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3036</elocation-id><pub-id pub-id-type="doi">10.3390/s24103036</pub-id><pub-id pub-id-type="pmid">38793891</pub-id><pub-id pub-id-type="pmcid">PMC11124920</pub-id></element-citation></ref><ref id="B41-sensors-25-05652"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Niu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Q.</given-names></name></person-group><article-title>Apple Pose Estimation Based on SCH-YOLO11s Segmentation</article-title><source>Agronomy</source><year>2025</year><volume>15</volume><elocation-id>900</elocation-id><pub-id pub-id-type="doi">10.3390/agronomy15040900</pub-id></element-citation></ref><ref id="B42-sensors-25-05652"><label>42.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Mohammed</surname><given-names>F.A.</given-names></name><name name-style="western"><surname>Mousa</surname><given-names>M.A.</given-names></name></person-group><article-title>Applying Diebold&#8211;Mariano Test for Performance Evaluation Between Individual and Hybrid Time-Series Models for Modeling Bivariate Time-Series Data and Forecasting the Unemployment Rate in the USA</article-title><source>Theory and Applications of Time Series Analysis</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>443</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-56219-9_29</pub-id></element-citation></ref><ref id="B43-sensors-25-05652"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Diebold</surname><given-names>F.X.</given-names></name><name name-style="western"><surname>Mariano</surname><given-names>R.S.</given-names></name></person-group><article-title>Comparing Predictive Accuracy</article-title><source>J. Bus. Econ. Stat.</source><year>2002</year><volume>20</volume><fpage>134</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1198/073500102753410444</pub-id></element-citation></ref><ref id="B44-sensors-25-05652"><label>44.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Diebold</surname><given-names>F.X.</given-names></name><name name-style="western"><surname>Rudebusch</surname><given-names>G.D.</given-names></name></person-group><source>Business Cycles: Durations, Dynamics, and Forecasting</source><publisher-name>Princeton University Press</publisher-name><publisher-loc>Princeton, NJ, USA</publisher-loc><year>2020</year><isbn>9780691219585</isbn></element-citation></ref><ref id="B45-sensors-25-05652"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sapkota</surname><given-names>R.</given-names></name><name name-style="western"><surname>Karkee</surname><given-names>M.</given-names></name></person-group><article-title>YOLO11 and Vision Transformers Based 3D Pose Estimation of Immature Green Fruits in Commercial Apple Orchards for Robotic Thinning</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2410.19846</pub-id></element-citation></ref><ref id="B46-sensors-25-05652"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name></person-group><article-title>Three-Dimensional Spatial Perception of Blueberry Fruits Based on Improved YOLOv11 Network</article-title><source>Agronomy</source><year>2025</year><volume>15</volume><elocation-id>535</elocation-id><pub-id pub-id-type="doi">10.3390/agronomy15030535</pub-id></element-citation></ref><ref id="B47-sensors-25-05652"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Uribeetxebarria</surname><given-names>A.</given-names></name><name name-style="western"><surname>Castell&#243;n</surname><given-names>A.</given-names></name><name name-style="western"><surname>Aizpurua</surname><given-names>A.</given-names></name></person-group><article-title>Optimizing Wheat Yield Prediction Integrating Data from Sentinel-1 and Sentinel-2 with CatBoost Algorithm</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>1640</elocation-id><pub-id pub-id-type="doi">10.3390/rs15061640</pub-id></element-citation></ref><ref id="B48-sensors-25-05652"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mahesh</surname><given-names>P.</given-names></name><name name-style="western"><surname>Soundrapandiyan</surname><given-names>R.</given-names></name></person-group><article-title>Yield Prediction for Crops by Gradient-Based Algorithms</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><elocation-id>e0291928</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0291928</pub-id><pub-id pub-id-type="pmid">39186769</pub-id><pub-id pub-id-type="pmcid">PMC11346950</pub-id></element-citation></ref><ref id="B49-sensors-25-05652"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>A.</given-names></name><name name-style="western"><surname>Islam Shanto</surname><given-names>M.N.</given-names></name><name name-style="western"><surname>Mahabub Rabby</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Sikder</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Sayem Uddin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Arefin</surname><given-names>M.N.</given-names></name><name name-style="western"><surname>Patwary</surname><given-names>M.J.A.</given-names></name></person-group><article-title>Eggplant Yield Prediction Utilizing 130 Locally Collected Genotypes and Machine Learning Model</article-title><source>Proceedings of the 2023 26th International Conference on Computer and Information Technology (ICCIT)</source><conf-loc>Cox&#8217;s Bazar, Bangladesh</conf-loc><conf-date>13&#8211;15 December 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05652-f001" orientation="portrait"><label>Figure 1</label><caption><p>Geographical scope of the study: (<bold>a</bold>) location of Ibaraki Prefecture within the administrative boundaries of Japan, highlighting its regional context; (<bold>b</bold>) position of Tsukuba City within Ibaraki Prefecture; (<bold>c</bold>) location of the NARO experimental field, including the flight path, ground control points (GCPs), and home point.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g001.jpg"/></fig><fig position="float" id="sensors-25-05652-f002" orientation="portrait"><label>Figure 2</label><caption><p>Schematic framework for estimating cabbage diameter and fresh head weight using RGB and multispectral UAV imagery combined with deep learning and machine learning techniques.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g002.jpg"/></fig><fig position="float" id="sensors-25-05652-f003" orientation="portrait"><label>Figure 3</label><caption><p>Loss trends across training epochs for both the YOLOv8s and YOLOv11s models: (<bold>a</bold>) YOLOv8s training losses for bounding box and pose estimation, (<bold>b</bold>) YOLOv8s validation losses for bounding box and pose estimation, (<bold>c</bold>) YOLOv11s training losses for bounding box and pose estimation, and (<bold>d</bold>) YOLOv11s validation losses for bounding box and pose estimation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g003a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g003b.jpg"/></fig><fig position="float" id="sensors-25-05652-f004" orientation="portrait"><label>Figure 4</label><caption><p>Performance evaluation parameters for the YOLOv8s-pose model: (<bold>a</bold>) precision, recall, and mAP for boxes; (<bold>b</bold>) precision, recall, and mAP for poses.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g004.jpg"/></fig><fig position="float" id="sensors-25-05652-f005" orientation="portrait"><label>Figure 5</label><caption><p>Performance evaluation parameters for the YOLOv11s-pose model: (<bold>a</bold>) precision, recall, and mAP for boxes; (<bold>b</bold>) precision, recall, and mAP for poses.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g005.jpg"/></fig><fig position="float" id="sensors-25-05652-f006" orientation="portrait"><label>Figure 6</label><caption><p>Training and validation results with confidence scores: (<bold>a</bold>) YOLOv8s-pose model performance during training and validation, including associated confidence scores; (<bold>b</bold>) YOLOv11s-pose model performance during training and validation, with corresponding confidence scores; and (<bold>c</bold>) cabbage head diameter (CHD) estimation result based on the YOLOv11s-pose model, illustrating the differences between predicted (P) and measured (M) values, along with the associated estimation error expressed as absolute error (AE).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g006a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g006b.jpg"/></fig><fig position="float" id="sensors-25-05652-f007" orientation="portrait"><label>Figure 7</label><caption><p>MSE of ML predicted models for training and testing phase.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g007.jpg"/></fig><fig position="float" id="sensors-25-05652-f008" orientation="portrait"><label>Figure 8</label><caption><p>Comparison of regression performance among machine learning models in predicting cabbage head weight: (<bold>a</bold>) RF training, (<bold>b</bold>) XGBoost training, (<bold>c</bold>) CatBoost training, (<bold>d</bold>) RF testing, (<bold>e</bold>) XGBoost testing, (<bold>f</bold>) CatBoost testing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g008.jpg"/></fig><fig position="float" id="sensors-25-05652-f009" orientation="portrait"><label>Figure 9</label><caption><p>Spatial distribution map of cabbage head fresh weight (kg/cabbage) in the NARO experimental field.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05652-g009.jpg"/></fig><table-wrap position="float" id="sensors-25-05652-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05652-t001_Table 1</object-id><label>Table 1</label><caption><p>Specifications of the Unmanned Aerial Vehicle (UAV).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specification</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UAV type</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DJI Phantom 4M Drone</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Camera sensor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB and multispectral images</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Working frequency</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.400&#8211;2.483 GHz</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Filters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Blue (B): 450 nm &#177; 16 nm; Green (G): 560 nm &#177; 16 nm, Red (R): 650 nm &#177; 16 nm; red edge (RE): 730 nm &#177; 16 nm, near-infrared (NIR): 840 nm &#177; 26 nm.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lenses</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FOV (Field of View): 62.7&#176;<break/>Focal Length: 5.74 mm (35 mm format equivalent: 40 mm),<break/>autofocus set at &#8734;<break/>Aperture: f/2.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Battery</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6000 mAh LiPo 2S</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum takeoff weight</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1487 g</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum flight time</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Approx. 27&#8211;28 min</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05652-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05652-t002_Table 2</object-id><label>Table 2</label><caption><p>Selected parameters for the ML models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Random Forest (RF)</td><td align="center" valign="middle" rowspan="1" colspan="1">n_estimators</td><td align="center" valign="middle" rowspan="1" colspan="1">184</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">max_depth</td><td align="center" valign="middle" rowspan="1" colspan="1">21</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">random_state</td><td align="center" valign="middle" rowspan="1" colspan="1">42</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">min_samples_split</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">min_samples_leaf</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Support Vector Regressor (SVR)</td><td align="center" valign="middle" rowspan="1" colspan="1">Gamma</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Scale</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">kernel</td><td align="center" valign="middle" rowspan="1" colspan="1">rbf</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-Neighbors Regressor (KNR)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n_neighbors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Light Gradient Boosting Machine (LightGBM)</td><td align="center" valign="middle" rowspan="1" colspan="1">n_estimators</td><td align="center" valign="middle" rowspan="1" colspan="1">150</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">max_depth</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">random_state</td><td align="center" valign="middle" rowspan="1" colspan="1">42</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning_rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.045</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Extreme Gradient Boosting (XGB)</td><td align="center" valign="middle" rowspan="1" colspan="1">n_estimators</td><td align="center" valign="middle" rowspan="1" colspan="1">375</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">max_depth</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">random_state</td><td align="center" valign="middle" rowspan="1" colspan="1">42</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning_rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.07</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Categorical Boosting (CatBoost)</td><td align="center" valign="middle" rowspan="1" colspan="1">n_estimators</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">max_depth</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">random_state</td><td align="center" valign="middle" rowspan="1" colspan="1">42</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning_rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05652-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05652-t003_Table 3</object-id><label>Table 3</label><caption><p>Training parameters for the cabbage head diameter estimation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Input Image Dimensions</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Batch Size</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Epoch</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">640 &#215; 640</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td><td align="center" valign="middle" rowspan="1" colspan="1">800</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">640 &#215; 640</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">800</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05652-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05652-t004_Table 4</object-id><label>Table 4</label><caption><p>Pose estimation best result for the boxes and pose.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean Average Precision&#8211;Recall (mAP)@0.5</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">YOLOV8s</td><td align="center" valign="middle" rowspan="1" colspan="1">Boxes</td><td align="center" valign="middle" rowspan="1" colspan="1">0.888</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" rowspan="1" colspan="1">0.995</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pose</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.954</td><td align="center" valign="middle" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" rowspan="1" colspan="1">0.979</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">YOLOv11s</td><td align="center" valign="middle" rowspan="1" colspan="1">Boxes</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">0.994</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pose</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.966</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.985</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05652-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05652-t005_Table 5</object-id><label>Table 5</label><caption><p>Test results for predicted cabbage head diameter (cm), including mean absolute error (MAE) and mean absolute relative error (MARE).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">No.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Variety</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Measured CHD * <break/>(cm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Radius <break/>(cm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Predicted CHD * <break/>(cm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Absolute Error<break/>(cm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Relative Error<break/>(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mikuni</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mikuni</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mikuni</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Renbu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tenku</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.6</td></tr></tbody></table><table-wrap-foot><fn><p>* CHD means cabbage head diameter. Average Radius &#215; 2 = Predicted CHD.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05652-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05652-t006_Table 6</object-id><label>Table 6</label><caption><p>Coefficient of determination (R<sup>2</sup>) and mean squared error (MSE) of machine learning models during training and testing phases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Train</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Test</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.938</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.011</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.847</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.031</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.830</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.032</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KNR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.896</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.813</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.033</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LightGBM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.884</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.751</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.046</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">XGBoost</td><td align="center" valign="middle" rowspan="1" colspan="1">0.942</td><td align="center" valign="middle" rowspan="1" colspan="1">0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.852</td><td align="center" valign="middle" rowspan="1" colspan="1">0.027</td></tr><tr><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CatBoost</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">0.925</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">0.014</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">0.889</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">0.025</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05652-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05652-t007_Table 7</object-id><label>Table 7</label><caption><p>Performance evaluation of CatBoost, XGBoost, and RF by Diebold&#8211;Mariano test for comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Comparison</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DM Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DM Statistic</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Significance (<italic toggle="yes">p</italic> &lt; 0.05)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Better Model</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RF vs. XGBoost</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Squared Errors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4756</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6344</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not Significant</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost vs. RF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Squared Errors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.7514</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0059</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Significant</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost vs. XGBoost</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Squared Errors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0078</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Significant</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RF vs. XGBoost</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Absolute Errors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.2266</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not Significant</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost vs. RF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Absolute Errors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.9753</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0029</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Significant</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost vs. XGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Absolute Errors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.0725</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0382</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Significant</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>