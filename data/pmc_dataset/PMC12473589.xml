<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473589</article-id><article-id pub-id-type="pmcid-ver">PMC12473589.1</article-id><article-id pub-id-type="pmcaid">12473589</article-id><article-id pub-id-type="pmcaiid">12473589</article-id><article-id pub-id-type="pmid">41012999</article-id><article-id pub-id-type="doi">10.3390/s25185761</article-id><article-id pub-id-type="publisher-id">sensors-25-05761</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>WeldLight: A Lightweight Weld Classification and Feature Point Extraction Model for Weld Seam Tracking</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-8125-6218</contrib-id><name name-style="western"><surname>Gao</surname><given-names initials="A">Ang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05761" ref-type="aff">1</xref><xref rid="af2-sensors-25-05761" ref-type="aff">2</xref><xref rid="af3-sensors-25-05761" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="A">Anning</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05761" ref-type="aff">1</xref><xref rid="af2-sensors-25-05761" ref-type="aff">2</xref><xref rid="af3-sensors-25-05761" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Su</surname><given-names initials="F">Fukang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05761" ref-type="aff">1</xref><xref rid="af2-sensors-25-05761" ref-type="aff">2</xref><xref rid="af3-sensors-25-05761" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yang</surname><given-names initials="X">Xinqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05761" ref-type="aff">1</xref><xref rid="af2-sensors-25-05761" ref-type="aff">2</xref><xref rid="af3-sensors-25-05761" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="W">Wenping</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05761" ref-type="aff">1</xref><xref rid="af2-sensors-25-05761" ref-type="aff">2</xref><xref rid="af3-sensors-25-05761" ref-type="aff">3</xref><xref rid="c1-sensors-25-05761" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9781-9677</contrib-id><name name-style="western"><surname>Du</surname><given-names initials="F">Fuxin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05761" ref-type="aff">1</xref><xref rid="af2-sensors-25-05761" ref-type="aff">2</xref><xref rid="af3-sensors-25-05761" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4353-3283</contrib-id><name name-style="western"><surname>Chen</surname><given-names initials="C">Chao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05761" ref-type="aff">1</xref><xref rid="af4-sensors-25-05761" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Luo</surname><given-names initials="Q">Qiwu</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05761"><label>1</label>School of Mechanical Engineering, Shandong University, Jinan 250061, China; <email>202414363@mail.sdu.edu.cn</email> (A.G.); <email>202214806@mail.sdu.edu.cn</email> (A.L.); <email>202334436@mail.sdu.edu.cn</email> (F.S.); <email>202314368@mail.sdu.edu.cn</email> (X.Y.); <email>dufuxin@sdu.edu.cn</email> (F.D.); <email>chaochen@sdjtu.edu.cn</email> (C.C.)</aff><aff id="af2-sensors-25-05761"><label>2</label>Key Laboratory of High-Efficiency and Clean Mechanical Manufacture, Ministry of Education, Shandong University, Jinan 250061, China</aff><aff id="af3-sensors-25-05761"><label>3</label>State Key Laboratory of Advanced Equipment and Technology for Metal Forming, Beijing 100083, China</aff><aff id="af4-sensors-25-05761"><label>4</label>School of Rail Transportation, Shandong Jiaotong University, Jinan 250357, China</aff><author-notes><corresp id="c1-sensors-25-05761"><label>*</label>Correspondence: <email>liuwp@sdu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>16</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5761</elocation-id><history><date date-type="received"><day>30</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>05</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>06</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>16</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05761.pdf"/><abstract><p>To address the issues of intense image noise interference and computational intensity faced by traditional vision-based weld tracking systems, we propose WeldLight, a lightweight and noise-resistant convolutional neural network for precise classification and positioning of welding seam feature points using single-line structured light vision. Our approach includes (1) an online data augmentation method to enhance training samples and improve noise adaptability; (2) a one-stage lightweight network for simultaneous positioning and classification; and (3) an attention module to filter features corrupted by intense noise, thereby improving stability. Experiments show that WeldLight achieves an F1-score of 0.9668 for seam classification on an adjusted test set, with mean absolute positioning errors of 1.639 pixels and 1.736 pixels on low-noise and high-noise test sets, respectively. With an inference time of 29.32 ms on a CPU platform, it meets real-time seam tracking requirements.</p></abstract><kwd-group><kwd>feature point extraction</kwd><kwd>laser vision sensor</kwd><kwd>lightweight network</kwd><kwd>seam tracking</kwd><kwd>welding seam classification</kwd></kwd-group><funding-group><award-group><funding-source>Central Funds Guiding Local Science and Technology Development</funding-source><award-id>YDZX2024115</award-id></award-group><award-group><funding-source>Key Research and Development Program of Shandong Province</funding-source><award-id>2020CXGC010206</award-id></award-group><award-group><funding-source>Shandong Provincial Natural Science Foundation</funding-source><award-id>ZR2024QE326</award-id><award-id>ZR2024QE494</award-id></award-group><funding-statement>Central Funds Guiding Local Science and Technology Development (Grant No. YDZX2024115), the Key Research and Development Program of Shandong Province (Grant No. 2020CXGC010206), and the Shandong Provincial Natural Science Foundation (Project Nos. ZR2024QE326 and ZR2024QE494).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05761"><title>1. Introduction</title><p>With the wide application of robots in industrial manufacturing, a&#160;welding method based on teaching and reproduction has emerged. Traditional robot welding relies on workers to manually define an a priori torch position trajectory, which is inefficient and easily affected by some time-varying dimensional errors of workpieces, such as thermal deformation&#160;[<xref rid="B1-sensors-25-05761" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05761" ref-type="bibr">2</xref>]. To track the welding seam in real time and realize the automatic adjustment of the welding robots, the&#160;researchers designed some advanced sensors to obtain the spatial position information and category information of the welding seam. With the rapid development of machine vision, structured light vision has found extensive application in intelligent robots [<xref rid="B3-sensors-25-05761" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05761" ref-type="bibr">4</xref>] due to its high precision advantages [<xref rid="B5-sensors-25-05761" ref-type="bibr">5</xref>] and the rich characteristic information it provides regarding welding processes [<xref rid="B6-sensors-25-05761" ref-type="bibr">6</xref>].</p><p>As shown in <xref rid="sensors-25-05761-f001" ref-type="fig">Figure 1</xref>a,c, the&#160;characteristics of single-line structured light projection stripes vary significantly based on different welding seams. This variability is valuable for acquiring position information related to welding seam feature points and facilitates the convenient classification of stripes. Such classification is crucial for adjusting the welding process parameters effectively. However, as&#160;shown in <xref rid="sensors-25-05761-f001" ref-type="fig">Figure 1</xref>b, common noises in the welding process, including splash, smog, and&#160;reflection light of arc, will be confused with stripes collected by industrial cameras. Several morphology-based methods have been proposed to realize the extraction of welding seam features. Li&#160;et&#160;al.&#160;[<xref rid="B7-sensors-25-05761" ref-type="bibr">7</xref>] used a Kalman filter to track the meaningful laser stripes on the image through a window to reduce the influence of image noise during welding, and&#160;the laser stripes are decomposed into line&#8211;junction&#8211;line combination fragments to obtain the weld type and the position of feature points on the image. Yang&#160;et&#160;al.&#160;[<xref rid="B8-sensors-25-05761" ref-type="bibr">8</xref>] applied a kernelized correlation filters&#8217; algorithm to realize seam tracking with feature point marks, which can adapt to different types of weld seams. Two common challenges exist in morphology-based research. One is that the tasks of feature point (or region) extraction and weld classification are usually two-stage. On&#160;the flip side, while these models possess a degree of resistance to noise, their tracking accuracy degrades under sustained high-noise conditions&#160;[<xref rid="B9-sensors-25-05761" ref-type="bibr">9</xref>].</p><p>The evolution of deep learning technology in computer vision has led to the refinement of object detection and semantic segmentation, enabled by advances in computing hardware and the development of convolutional neural networks (CNNs). These tasks are frequently transferred to seam feature extraction tasks utilizing line structured light, contributing to the development of an anti-noise model for effective feature extraction, as&#160;evidenced by existing&#160;research.</p><p>Gao&#160;et&#160;al.&#160;[<xref rid="B10-sensors-25-05761" ref-type="bibr">10</xref>] built the YOLO-WELD model based on YOLOv5 in the task of detecting weld feature points. RepVGG is used as the backbone network, and&#160;the NAM attention mechanism and lightweight head layer, RD-Head, are introduced to improve the detection effect. Deng&#160;et&#160;al.&#160;[<xref rid="B11-sensors-25-05761" ref-type="bibr">11</xref>] improved CenterNet, used DenseNet as the backbone network, and&#160;adjusted the head layer to separate the feature point position regression task from the weld classification task. This operation prevented the general feature detection network from associating different categories with feature points on a laser stripe image. Liu&#160;et&#160;al.&#160;[<xref rid="B12-sensors-25-05761" ref-type="bibr">12</xref>] reported a method to extract the feature points of multi-layer and multi-pass welding seams by using a conditional generative adversarial network (CGAN) and improving the CNN model. Cacarion&#160;et&#160;al.&#160;[<xref rid="B13-sensors-25-05761" ref-type="bibr">13</xref>] presented a detection model without NMS (non-maximum suppression) and DETR (detection transformer), which considers CNN as a feature extractor, and used an improved Transformer [<xref rid="B14-sensors-25-05761" ref-type="bibr">14</xref>] to realize the further mining of the global information of the input picture. Because&#160;of the self-attention mechanism of the Transformer, the DETR incurs considerable computational complexity (in this paper, DETR is used as a typical algorithm to compare the inference time of the model).</p><p>However, these methods based on neural networks require considerable computing resources; in turn, they necessitate higher computing performance from the central control equipment integrated into welding robot systems. Notably, such systems commonly lack GPU acceleration support. To&#160;address this challenge, lightweight neural network techniques have been extensively explored in computer vision. Typical approaches include pruning, knowledge distillation, weight quantization, and&#160;especially structural simplification. A&#160;variety of effective lightweight backbones have emerged, ranging from MobileNet&#160;[<xref rid="B15-sensors-25-05761" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05761" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05761" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05761" ref-type="bibr">18</xref>] and ShuffleNet&#160;[<xref rid="B19-sensors-25-05761" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05761" ref-type="bibr">20</xref>] to newer designs such as Shvit&#160;[<xref rid="B21-sensors-25-05761" ref-type="bibr">21</xref>] and LSNet&#160;[<xref rid="B22-sensors-25-05761" ref-type="bibr">22</xref>]. Alongside these developments, many studies have begun integrating attention modules [<xref rid="B23-sensors-25-05761" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05761" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05761" ref-type="bibr">25</xref>] into lightweight architectures to counterbalance the performance degradation often induced by aggressive model&#160;simplification.</p><p>Liu&#160;et&#160;al. [<xref rid="B26-sensors-25-05761" ref-type="bibr">26</xref>] extended the concept of depthwise separable convolution from MobileNet to 3-D convolution. They combined the 2-D input image with temporal information from image sequences to assess the current welding state. Additionally, three attention mechanisms were introduced to enhance the model&#8217;s robustness against welding noise. Ma&#160;et&#160;al. [<xref rid="B27-sensors-25-05761" ref-type="bibr">27</xref>] developed the WeldNet model specifically for welding tasks, encompassing starting point detection and seam classification. The&#160;model&#8217;s architecture employs ShuffleNet as the backbone network. However, although&#160;these methods adopt lightweight network architectures and demonstrate real-time performance on GPUs, their CPU performance remains limited, and&#160;they require large-scale training datasets, thereby hindering the cost-effective deployment of neural networks in industrial applications.</p><p>There are also some feature extraction methods based on the semantic segmentation model. Zou&#160;et&#160;al.&#160;[<xref rid="B28-sensors-25-05761" ref-type="bibr">28</xref>] developed a lightweight laser stripe image segmentation model by replacing the backbone with ShuffleNetv2. They achieved laser stripe segmentation and improved the model by a pruning operation based on trainable parameters of the Batch Normalization (BN) layer, combined with a welding seam tracking algorithm. Despite maintaining a high inference speed on the CPU, this method does not directly provide the specific position of the welding seam in a one-stage manner.</p><p>Additionally, an&#160;efficient ViT (Vision Transformer)&#160;[<xref rid="B29-sensors-25-05761" ref-type="bibr">29</xref>] based on cascaded grouping attention was proposed, a&#160;substitute for multi-head self-attention (MHSA). The&#160;structure of this attention mechanism can have fewer parameters and obtain higher inference speed because it is&#160;memory-efficient.</p><p>Utilizing a lightweight model in welding tasks can decrease the number of network parameters and floating-point operations (FLOPs), making it more suitable for deployment on edge computing devices with limited computational capabilities. Nevertheless, this comes at the cost of diminished performance in abstract feature extraction, potentially weakening the model&#8217;s noise&#160;robustness.</p><p>To address the issues above, the objectives of this paper are as follows:<list list-type="order"><list-item><p>To propose a one-stage model for (1) extracting the position of light stripe feature points of weld structures and (2) performing weld classification (e.g., identifying groove types) by analyzing distinctive geometric patterns in the light stripe.</p></list-item><list-item><p>The proposed method can be used to enhance the model&#8217;s robustness to image noise during welding without affecting the real-time performance of the model.</p></list-item><list-item><p>To reduce the model&#8217;s parameters and computational cost can be deployed on the computing platform where GPU acceleration is unavailable, such as an embedded industrial computer (EIC), and&#160;at the same time maintain a high inference speed to meet the real-time requirements of welding tracking systems in the industrial field.</p></list-item><list-item><p>Compared with typical one-stage target detection models, the&#160;study aims to test and validate the feature point positioning performance under different noise conditions, weld classification performance, and&#160;real-time performance of the proposed model.</p></list-item></list></p><p>The paper comprises five sections. <xref rid="sec2-sensors-25-05761" ref-type="sec">Section 2</xref> details the hardware devices and datasets employed in the study, while <xref rid="sec3-sensors-25-05761" ref-type="sec">Section 3</xref> provides a structural description of WeldLight. Following this, <xref rid="sec4-sensors-25-05761" ref-type="sec">Section 4</xref> outlines the specific configuration of the experiment and results obtained from the comparative verification of various models. <xref rid="sec5-sensors-25-05761" ref-type="sec">Section 5</xref> presents the obtained conclusions.</p></sec><sec id="sec2-sensors-25-05761"><title>2. Hardware Platform and&#160;Dataset</title><sec id="sec2dot1-sensors-25-05761"><title>2.1. Hardware Platform and Measuring&#160;Principle</title><p>As shown in <xref rid="sensors-25-05761-f002" ref-type="fig">Figure 2</xref>, the&#160;self-designed welding seam positioning and tracking sensor based on single-line structured light includes one industrial camera, a single-line laser transmitter, a narrow band filter, and&#160;others. The&#160;model of the used industrial camera is CB016 from HIKROBOT with a sensor size of 1/2.9<sup>&#8243;</sup> and 1440 &#215; 1080 resolution, equipped with a lens with a focal length of 25 mm. The&#160;wavelength of both the single-line laser transmitter and narrow band filter is about 660&#160;nm.</p><p>As shown in <xref rid="sensors-25-05761-f003" ref-type="fig">Figure 3</xref>, a&#160;pixel coordinate frame (PCF) and a camera coordinate frame (CCF) are established in the line structured light imaging system illustrated in the figure; a random feature point in 2-D PCF can be transformed into 3-D CCF by<disp-formula id="FD1-sensors-25-05761"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>q</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the coordinate of points on the laser stripe relative to CCF. <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the 2-D coordinate of the feature point of the welding seam on the PCF, with&#160;the upper left corner of the image as the origin of the coordinate and the right and downward directions as positive, respectively. <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the projection value on the <italic toggle="yes">Z</italic> axis of the feature point with respect to CCF, which is an unknown variable. <italic toggle="yes">M</italic> denotes the intrinsic matrix of the camera, which can be calculated by [<xref rid="B30-sensors-25-05761" ref-type="bibr">30</xref>].</p><p>The laser plane equation of the line structured light with respect to the CCF can be determined by<disp-formula id="FD2-sensors-25-05761"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for the normal vector of the plane and the distance between the origin of CCF and the laser plane, respectively, which can be calculated by the calibration method proposed in [<xref rid="B31-sensors-25-05761" ref-type="bibr">31</xref>] to obtain the general equation of the structured light plane relative to the CCF. According to (<xref rid="FD1-sensors-25-05761" ref-type="disp-formula">1</xref>) and (<xref rid="FD2-sensors-25-05761" ref-type="disp-formula">2</xref>), the&#160;coordinates of feature points concerning CCF can be represented by<disp-formula id="FD3-sensors-25-05761"><label>(3)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>q</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>q</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (<xref rid="FD3-sensors-25-05761" ref-type="disp-formula">3</xref>) will be employed in <xref rid="sec4dot3-sensors-25-05761" ref-type="sec">Section 4.3</xref> to assess the comprehensive performance of the model and hardware platform on feature point&#160;positioning.</p></sec><sec id="sec2dot2-sensors-25-05761"><title>2.2. Welding Image&#160;Dataset</title><p>In complex and variable industrial settings, neural networks often require new training data to adapt to evolving environments, and&#160;precise annotation typically demands substantial human effort. Consequently, developing models suitable for small-batch datasets becomes necessary. A&#160;total dataset of 1500 images (<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1440</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1080</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels), comprising Y-seam, lap-seam, and&#160;butt-seam welding stripes captured by CMOS cameras, was collected. We divided the test images into low-noise and high-noise subsets to better evaluate the robustness of the network under different operating conditions. The&#160;low-noise subset consists of noise-free and slightly noisy images, in&#160;which light stripes remain clear, and the minor disturbances can still be reliably recognized using traditional morphological methods, as&#160;illustrated in <xref rid="sensors-25-05761-f004" ref-type="fig">Figure 4</xref>a,b. The&#160;high-noise subset includes images with extensive splash and fume noise that severely interfere with recognition and even obscure critical features. Such cases are generally difficult to identify using morphological methods, as&#160;shown in <xref rid="sensors-25-05761-f004" ref-type="fig">Figure 4</xref>c, and&#160;they account for approximately 50% of the test set. The&#160;dataset was manually annotated, where the positions of occluded keypoints in the high-noise subset were estimated based on the continuity of light stripe variations.</p><p>It is worth noting that we excluded extremely noisy examples in the high-noise subset (e.g., <xref rid="sensors-25-05761-f001" ref-type="fig">Figure 1</xref>e), which may result from occasional extreme welding noise, highly reflective components, or&#160;hardware design limitations. In&#160;such cases, most light-stripe features become invisible, making single-image recognition ineffective. Fortunately, such extreme noise conditions are rarely encountered in typical welding scenarios. When they do arise, mitigation can be achieved through specialized hardware improvements (e.g., adding optical filters or adjusting laser wavelengths) or through downstream processing. The&#160;latter may involve trackers that integrate temporal information to predict the current occluded point or,&#160;alternatively, skipping the frame and continuing with welding path fitting. In&#160;this study, Kalman filtering was adopted for such predictions, although&#160;its details are not elaborated here.</p><p>The dataset division is detailed in <xref rid="sensors-25-05761-t001" ref-type="table">Table 1</xref>. Although the evaluation in this work is restricted to three weld seam types, the&#160;approach is not confined to these cases; with appropriate specification of feature points, it can be generalized to a wider variety of seam&#160;geometries.</p></sec><sec id="sec2dot3-sensors-25-05761"><title>2.3. Online Data&#160;Augmentation</title><p>The quality of neural network training is highly dependent on the quality and scale of the collected dataset; however, data collection and annotation are often labor-intensive and time-consuming. Therefore, in&#160;many real-world applications, data augmentation is commonly employed to simulate variations in working environments, sensor models, and&#160;sensor positions, thereby enabling effective training on limited data. For&#160;example, previous studies [<xref rid="B32-sensors-25-05761" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05761" ref-type="bibr">33</xref>] designed data augmentation techniques tailored to their respective application domains, which improved the dataset quality and enhanced the performance of neural network models.</p><p>In this study, we similarly adopt data augmentation to enhance the robustness of the neural network model. Specifically, data augmentation is applied to increase the diversity of training samples and improve the model&#8217;s generalization ability through techniques such as image flipping, warping, affine transformations, and&#160;variations in hue, saturation, and&#160;value. In&#160;addition, common noise patterns observed in welding processes are also simulated to further improve the training quality of the network.</p><p>Splashes are the main interference factor in detecting feature points captured by the camera. To&#160;mitigate the impact of splash noise on the neural network model, a&#160;method involving the random generation of splashes is introduced into the traditional online data augmentation step to enhance the learning of splash features by the network model. So, in every epoch, the&#160;images loaded from the dataset undergo different forward passes and backward propagations. It is crucial to note that this process is distinct from the offline splash addition introduced in this work, as&#160;detailed in [<xref rid="B10-sensors-25-05761" ref-type="bibr">10</xref>]. As illustrated in Algorithm&#160;1, for&#160;each noise parameter, we initialize an empty splash image and generate <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> random lines. Each line is assigned a random starting point near the light source, a&#160;random direction, and&#160;random attributes including width and intensity. The&#160;line is then randomly segmented and rendered on the splash image. After&#160;applying Gaussian blur to the splash image, it is superimposed on the original image. Finally, the&#160;original images are blended with a noise template and subjected to a sequence of augmentation operations, including cropping, scaling, affine transformations, and&#160;brightness adjustments. The overall effect of this noise generation process is shown in <xref rid="sensors-25-05761-f005" ref-type="fig">Figure 5</xref>.
<array orientation="portrait"><tbody><tr><td style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1&#160;</bold>Random Noise Generation&#160;Method</td></tr><tr><td style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>1:</label><p><bold>procedure&#160;</bold><sc>RandomNoise</sc>(<inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>)</p></list-item><list-item><label>2:</label><p>&#160;&#160;&#160;&#160;<bold>for</bold> each parameter <italic toggle="yes">p</italic> in <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>3:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8592;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#8195;&#8195;&#8195;&#8195; &#9655;zero matrix with same size as <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>4:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>for</bold>&#160;<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8592;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>5:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<italic toggle="yes">P<sub>start</sub></italic>&#8592;<sc>GeneratePoint</sc>(light source region)</p></list-item><list-item><label>6:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<italic toggle="yes">d</italic>&#8592;<sc>RandomDirection</sc>()</p></list-item><list-item><label>7:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(<italic toggle="yes">w, &#945;</italic>)&#8592;<sc>RandomWidthIntensity</sc>()</p></list-item><list-item><label>8:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<italic toggle="yes">L</italic>&#8592;<sc>ConstrustLine</sc>(<italic toggle="yes">P<sub>start</sub>, d, w, &#945;</italic>)</p></list-item><list-item><label>9:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<italic toggle="yes">L</italic>&#8592;<sc>TruncateLine</sc>(<italic toggle="yes">L</italic>)</p></list-item><list-item><label>10:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<sc>DrawLine</sc>(<inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>)</p></list-item><list-item><label>11:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>12:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;<italic toggle="yes">splash_image</italic>&#8592;<sc>GaussianBlur</sc><italic toggle="yes">(splash_image)</italic></p></list-item><list-item><label>13:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8592;</mml:mo><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>14:</label><p>&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>15:</label><p>&#160;&#160;&#160;<bold>return</bold>&#160;<inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>16:</label><p><bold>end procedure</bold></p></list-item></list></td></tr></tbody></array></p></sec></sec><sec id="sec3-sensors-25-05761"><title>3. WeldLight&#160;Structure</title><sec id="sec3dot1-sensors-25-05761"><title>3.1. Overall&#160;Structure</title><p>As shown in <xref rid="sensors-25-05761-f006" ref-type="fig">Figure 6</xref>, WeldLight can be divided into three parts: the backbone layer, the neck layer, and the head layer.</p><p>The backbone layer is responsible for extracting features and outputting feature map branches with different downsample factors, which are then delivered to the neck layer. MobileNetV3-Small is chosen in the proposed work. In the MobileNetV3 block, a sequence consisting of a <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> depthwise convolution (DWC) with a BN layer and different nonlinearities (i.e., h-swish and ReLU), followed by a channel-fusing <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pointwise convolution (PWC) with a BN layer, is proposed to replace one standard 3 &#215; 3 convolution with BN and ReLU. Notably, a similar DP block (<xref rid="sensors-25-05761-f006" ref-type="fig">Figure 6</xref>) is used in the neck and head layers of WeldLight, employing the ReLU6 function as the nonlinearity to better suit feature-point localization. In terms of convolution computation, the quantitative relation between DP block and one standard 3 &#215; 3 convolution is given by (<xref rid="FD4-sensors-25-05761" ref-type="disp-formula">4</xref>).<disp-formula id="FD4-sensors-25-05761"><label>(4)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where the numerator and denominator of the fraction represent the computations of the DP block and the standard <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, respectively, <italic toggle="yes">k</italic> denotes the kernel size, and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the input channels, output channels, and the spatial dimensions of the input feature map. Generally, there are many output channels of convolution operation, especially the number of channels in the deep layer, which is much larger than the size of the convolution kernel. Consequently, the calculation can be reduced to about <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>.</p><p>In the neck layer, a feature pyramid network (FPN) [<xref rid="B34-sensors-25-05761" ref-type="bibr">34</xref>] construction is applied. Due to many stages of backbone producing feature maps of the different resolutions with different semantic levels, the deepest three branches produced by the backbone with downsampling strides specifically of <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>32</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with respect to the input image will be fused by FPN, as shown in <xref rid="sensors-25-05761-f006" ref-type="fig">Figure 6</xref>. Though more branches merged in the neck layer indicates the model can integrate deep abstract semantic information and superficial coarse-grained feature information to achieve better small-scale feature detection capability, the number of branches should not increase without limit, because more branches will seriously slow down the inference speed [<xref rid="B20-sensors-25-05761" ref-type="bibr">20</xref>]. Through experimentation, the optimal branch number of three was determined to minimize computational complexity and latency while preserving a certain level of positioning accuracy.</p><p>As depicted in <xref rid="sensors-25-05761-f007" ref-type="fig">Figure 7</xref>b, the improved top-down pathway and lateral connection employ concatenation instead of addition, as seen in the original approach shown in <xref rid="sensors-25-05761-f007" ref-type="fig">Figure 7</xref>a. Channel adjustment facilitates merging the feature map from the PWC with the upsampled output of the coarser-resolution feature map containing stronger semantic information. A DP block is also utilized to substitute the standard <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution. Ultimately, the neck layer will generate a feature map of size <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which will subsequently be input into the head layer.</p><p>Additionally, within the neck layer, the network links a proposed attention module to the output of <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>32</mml:mn><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> downsampled feature map, aiming to enhance the model&#8217;s robustness against noise, as detailed in <xref rid="sec3dot2-sensors-25-05761" ref-type="sec">Section 3.2</xref>.</p><p>In the head layer, it is necessary to predict the position of feature points, which is realized by a heatmap tensor and an offset tensor. The task also requires classifying the laser stripes to determine the weld type, which requires the head layer to output a one-hot tensor. The standard <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution block for decoupling the input tensor of the head layer is also replaced by a DP block. At this stage, the PWC not only fuses features across channels but also adjusts the number of channels. Since WeldLight does not classify each feature point separately but instead predicts a category for the entire laser stripe image, the categorical output of the head layer mainly derives from the lower-resolution feature map in the deepest backbone layer, enabling large-scale perception and producing a global category prediction rather than point-specific classification.</p></sec><sec id="sec3dot2-sensors-25-05761"><title>3.2. Cascaded Channel Attention Module (CCAM)</title><p>The deepest output shape from the backbone network obtained by FPN is <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, specifically <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>16</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>16</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. It should be noted that a significant portion of the features are related to noise. These features will be merged in the neck layer and finally affect outputs from the head layer. In order to weaken the influence of noise, CCAM is proposed to filter the noise.</p><p>As shown in <xref rid="sensors-25-05761-f008" ref-type="fig">Figure 8</xref>, the core of CCAM is a channel attention (CA) block, which serves as a feature selector. Initially, a global average pooling layer is used to reduce the input feature map to a single vector, which is then fed to the first fully connected (FC) layer to reduce its dimensionality. A second FC layer restores the feature dimension. The resulting vector is passed through a softmax function to map it to the range of [0, 1], which enables channel-wise weighting of the input feature map through element-wise multiplication. Channels with higher weights are considered more significant.</p><p>In the CCAM design, we adopt softmax rather than sigmoid for channel weighting. While sigmoid is commonly used, it treats channels independently and may assign similar weights across them, limiting discrimination. Softmax instead normalizes the weights to sum to one, emphasizing the relative differences among channels and thereby supporting more effective suppression of noise and highlighting informative features. This choice is also consistent with established deep learning practices, where softmax is widely employed for normalized weighting in multi-channel scenarios [<xref rid="B35-sensors-25-05761" ref-type="bibr">35</xref>]. To summarize, the refined feature map <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by CA block is computed as<disp-formula id="FD5-sensors-25-05761"><label>(5)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8727;</mml:mo><mml:mi mathvariant="normal">z</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow></mml:math></inline-formula> denotes the input feature, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mi>g</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mi>g</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denote the parameters of two FC stages, in which <italic toggle="yes">r</italic>, as shown in <xref rid="sensors-25-05761-f008" ref-type="fig">Figure 8</xref>, stands for the reduction ratio with 16 in WeldLight, and <italic toggle="yes">C</italic> denotes the input tensor number of CCAM, while <italic toggle="yes">g</italic> stands for the number of groups (splits) for the input tensor.</p><p>In addition, CCAM adopts a cascaded group structure to further reduce the computational overhead. As illustrated in <xref rid="sensors-25-05761-f006" ref-type="fig">Figure 6</xref> and <xref rid="sensors-25-05761-f008" ref-type="fig">Figure 8</xref>, the deepest output feature map of the backbone is split into g parts, which are then fed into the CCAM. In the CCAM, the output from each CA block is merged with the next split by element-wise addition to enhance the information fusion between channels. The process is repeated across all g groups.</p><p>For CCAM, the computation generated by the FC layer is <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>. Instead of relying on the cascaded group structure, the computation generated by the FC layer through direct utilization of channel attention leads to a calculation amount of <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>r</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>. The computation of the CA module, when employing a cascaded group structure, is approximately <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>g</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> of the unused capacity. However, this structure also enables multi-layer attention effects, allowing the model to refine its feature selection at each stage.</p></sec><sec id="sec3dot3-sensors-25-05761"><title>3.3. Loss&#160;Function</title><p>The head layer produces three outputs, a heatmap tensor <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, an offset tensor <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mi>O</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and a classification tensor (i.e., one-hot encoding) <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, where <italic toggle="yes">W</italic> and <italic toggle="yes">H</italic> are the input image dimensions of the model, and <italic toggle="yes">c</italic> is the number of categories. Throughout the training process, the loss is determined by comparing the predictions generated by the head layer with the ground truth label values, emphasizing that the shapes of the label values should match those of the predictions. This contrast aims to optimize the neural network parameters to achieve an optimal score or regression value prediction for the model&#8217;s output. Consequently, it is essential to address the generation of labels in this context. The coordinate of the feature points location, following their mapping to the corresponding bin in the label of heatmap tensor, is <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mfenced separators="" open="&#x230A;" close="&#x230B;"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced separators="" open="&#x230A;" close="&#x230B;"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math></inline-formula> is the coordinate of the <italic toggle="yes">k</italic>-th feature point with respect to the input image size of the model (i.e., 512 &#215; 512 for WeldLight), and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the downsampling stride factor. On the label heatmap, positive sample points are represented using Gaussian circles <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> to accelerate the convergence of the model [<xref rid="B36-sensors-25-05761" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05761" ref-type="bibr">37</xref>], where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a factor that varies with the size of the object [<xref rid="B37-sensors-25-05761" ref-type="bibr">37</xref>], and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> stands for the center of Gaussian circles, which is the ideal positive location of feature points mapping on the heatmap.</p><p>In light of the inherent rounding effect of floor division, relying solely on the heatmap to restore position information for feature points can result in accuracy loss. Offsets are introduced to address this issue. The label value of offsets of every feature point is denoted by<disp-formula id="FD6-sensors-25-05761"><label>(6)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>R</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a 2-D coordinate representing the relative position proportion of a feature point in a corresponding bin of the label heatmap. Therefore, the offset is represented by a two-channel tensor. Additionally, the offset tensor bins are activated only at the corresponding positive locations within the heatmap tensor.</p><p>Let <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> be the predicted score at location <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mfenced></mml:mrow></mml:math></inline-formula> of the heatmap produced by WeldLight, and let <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> be the value at the same location on the label heatmap with Gaussian circles; a variant of focal loss [<xref rid="B38-sensors-25-05761" ref-type="bibr">38</xref>] is used to optimize the heatmap prediction, denoted by<disp-formula id="FD7-sensors-25-05761"><label>(7)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>hm</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>&#945;</mml:mi></mml:msup><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="0.277778em"/><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>&#946;</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>&#945;</mml:mi></mml:msup><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the number of feature points in a structured light stripe image, <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> stand for the spatial size of the heatmap, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is utilized to fine-tune the weights for challenging and easily locatable points, while <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> governs the weighting of non-central values within the Gaussian circle. Importantly, the loss weight assigned to a predicted score in <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mover><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> increases, as the distance from the Gaussian center in the label grows.</p><p>The essence of offset prediction is rooted in a regression task. In order to realize the accurate offset prediction, a smooth L1 loss function [<xref rid="B39-sensors-25-05761" ref-type="bibr">39</xref>] is utilized as follows: <disp-formula id="FD8-sensors-25-05761"><label>(8)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>SmoothL</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>Loss</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover><mml:mi>O</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the number of feature points in a structured light stripe image, too. The smooth L1 loss leverages the advantages of both the L1 and L2 loss functions. This method prevents gradient explosion in the initial stages of training and promotes the acquisition of a more tempered gradient during back-propagation at the end of training. This contributes significantly to improved model convergence.</p><p>Cross-entropy loss (CE loss) can be applied to the weld classification task. Here, let <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represent the predicted score of structured light stripe image for class <italic toggle="yes">i</italic>, predicted by WeldLight, and boolean variable <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the label value of class <italic toggle="yes">i</italic> in the one-hot encoding <italic toggle="yes">K</italic>; hence, the loss produced by classification is denoted by<disp-formula id="FD9-sensors-25-05761"><label>(9)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>CEloss</mml:mi><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">c</italic> stands for the total number of welding seam types. Finally, the total loss <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the overall training objective, which is derived as follows: <disp-formula id="FD10-sensors-25-05761"><label>(10)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the constant weights assigned to the offset and classification loss, configuring to a value of 1.</p></sec><sec id="sec3dot4-sensors-25-05761"><title>3.4. Post-Processing</title><p>A correlation exists between the prediction of point numbers and laser stripe classification, as discussed in [<xref rid="B11-sensors-25-05761" ref-type="bibr">11</xref>]. Specifically, if the classification result is predicted as &#8220;lap-seam,&#8221; the model identifies the first two bins on the heatmap tensor with the highest predicted scores. These are then combined with the corresponding bins on the offset tensor, leading to the identification of the final predicted positions. In this scenario, there is no need to consider the suppression by predicted score thresholds. However, as depicted in <xref rid="sensors-25-05761-f009" ref-type="fig">Figure 9</xref>, the heatmap tensor undergoes resizing to match the uniform shape of the original image, covering it entirely. The colors in the heatmap indicate the predicted scores assigned to the potential feature points.</p><p>Notably, the figure illustrates a broken laser stripe, where both ends are confidently identified as potential feature points by WeldLight, despite their proximity. An empirical solution to this issue involves applying a <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> max-pooling operation to the heatmap. Bins with unchanged values correspond to effective potential feature points by comparing the heatmap tensor before and after pooling. In cases of closely located potential feature points, only the one with the highest predicted score is retained, effectively resolving the issue.</p></sec></sec><sec id="sec4-sensors-25-05761"><title>4. Verification</title><sec id="sec4dot1-sensors-25-05761"><title>4.1. Test&#160;Environment</title><p>The network was constructed using the PyTorch 2.0.0 deep learning framework based on Python 3.8. The dataset utilized for training and testing the model was obtained from the welding seam positioning and tracking sensor mentioned earlier. The model was trained on a computer with an Nvidia 2080Ti GPU and an Intel Xeon (R) E5-2683 CPU. The model was converted to the ONNX format and used OpenVINO for DNN inference on an EIC with an Intel Core i7-8650U CPU. The network initializes its backbone with the weights of MobileNetv3 pretrained on ImageNet to expedite training and achieve rapid convergence. The FC layer was implemented using a standard 1 &#215; 1 convolution, and the convolution layer parameters were initialized using the He initialization method. As for the BN layer parameters, the weights were initialized to one, and the biases were initialized to zero.</p><p>The initial learning rate was set to <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> using Adam as the optimizer (momentum = 0.9, weight decay = 0), and a cosine annealing strategy was adopted for learning rate scheduling. The batch size of the training process was set to 32. The maximum number of training epochs was set to 500, and an early stopping strategy was applied such that the training was terminated at the 50th epoch once the validation loss had stabilized. These hyperparameter values were determined empirically to balance convergence speed and training stability.</p><p>Upon observing convergence in the loss function of the validation set during the training stage, the model was utilized to evaluate both the positioning performance of the feature points and the classification performance of the welding seams. The comparative model opted for YOLOv5n and DETR, trained on the COCO dataset, to obtain initial weights, which were subsequently fine-tuned using the default training configurations provided by the respective open-source projects.</p></sec><sec id="sec4dot2-sensors-25-05761"><title>4.2. Weld Classification&#160;Performance</title><p>For welding tasks, the baseline network used in WeldLight is CenterNet, which, similar to DETR and YOLOv5n employed for comparison, belongs to the family of object detection networks. Since a welding image should contain only a single class, previous studies have commonly adopted the class of the point with the highest classification confidence as the image-level label. WeldLight, however, incorporates a dedicated prediction branch to directly infer the class of the entire image rather than distinguishing the classes of individual feature points, thereby better adapting to the requirements of weld seam classification in welding operations. In contrast, DETR, through its end-to-end prediction mechanism, is able to model the relationship between the global image context and the classification of feature points, thereby reducing the risk of assigning multiple classes to different points within the same image. By design, YOLOv5n assigns a class label to each detected point, which necessitates determining the overall image category based on the point with the highest classification confidence.</p><p>Notably, all three models accurately classified 150 images in the test set. In pursuit of a more granular comparison, the images underwent adjustments, including random cropping, translation, deformation, and brightness modification. For a visual representation, a selection of the test set images utilized in the classification performance evaluation is presented in <xref rid="sensors-25-05761-f010" ref-type="fig">Figure 10</xref>.</p><p>The adjusted test set was used to evaluate the three models, respectively. Finally, the classification performance was evaluated by the confusion matrix obtained by each model, presented in <xref rid="sensors-25-05761-f011" ref-type="fig">Figure 11</xref>.</p><p>The core of the weld classification involves a multi-classification task. To streamline comparisons and address discrepancies in sample numbers, the evaluation of the three models in welding seam classification employed the following three metrics: <disp-formula id="FD11-sensors-25-05761"><label>(11)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05761"><label>(12)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05761"><label>(13)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> stand for the weighted-average precision, recall, and F1-score, respectively. <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the precision, recall, and F1-score of class <italic toggle="yes">i</italic>, respectively. <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the ratio of the sample size of category i to the total sample size. True positive (TP), false negative (FN), and false positive (FP) are values that can be determined from the confusion matrix [<xref rid="B40-sensors-25-05761" ref-type="bibr">40</xref>]. <xref rid="sensors-25-05761-t002" ref-type="table">Table 2</xref> displays the metric values of the three models on the adjusted test set. Notably, WeldLight exhibited the highest values across the three key metrics assessing classification performance, with a weighted-average precision of 0.9674, a weighted-average recall of 0.9666, and a weighted-average F1-score of 0.9668.</p></sec><sec id="sec4dot3-sensors-25-05761"><title>4.3. Feature Point Positioning&#160;Performance</title><p>Welding-related noise, such as splash, reflection, and smog, can compromise the quality of images captured by industrial cameras. <xref rid="sensors-25-05761-f012" ref-type="fig">Figure 12</xref> demonstrates that all three models reliably distinguish laser stripes from noise and precisely localize feature points, maintaining robust performance even under noise conditions.</p><p>To evaluate the robustness under different noise levels, the test dataset was divided into high-noise and low-noise subsets, each accounting for 50% of the samples. After post-processing, the three models were evaluated for their feature point positioning performance on both subsets. The predictions were generated for the coordinates of the feature points with respect to the original image resolution of <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1440</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1080</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The purpose of this evaluation is to assess the models&#8217; feature point positioning accuracy under varying noise conditions and to analyze their noise robustness. The performance metrics include the mean absolute error (MAE), root mean square error (RMSE), standard deviation (<inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and projecting the predicted coordinates and the label coordinates to compute the mean Euclidean distance (<inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) with respect to the CCF. The positioning errors of the feature points were calculated along both the <italic toggle="yes">X</italic>-axis and <italic toggle="yes">Y</italic>-axis of the image, as follows: <disp-formula id="FD14-sensors-25-05761"><label>(14)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="0.277778em"/><mml:mrow><mml:mi>axis</mml:mi><mml:mspace width="3.33333pt"/><mml:mo>=</mml:mo><mml:mspace width="3.33333pt"/></mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="0.277778em"/><mml:mrow><mml:mi>axis</mml:mi><mml:mspace width="3.33333pt"/><mml:mo>=</mml:mo><mml:mspace width="3.33333pt"/></mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stand for the coordinate of the label and the predicted coordinate of the feature points, respectively.</p><p>The MAE was used to evaluate the static precision of the positioning, which is derived as follows: <disp-formula id="FD15-sensors-25-05761"><label>(15)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>MAE</mml:mi><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msub><mml:mi mathvariant="normal">e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> denotes the total number of feature points in the designated test set and keeps the same definition in the following formulas.</p><p>The RMSE would highlight the impact of predicted outliers, which is derived as follows: <disp-formula id="FD16-sensors-25-05761"><label>(16)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>RMSE</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:munderover><mml:msubsup><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was used to evaluate the stability/robustness of the model [<xref rid="B7-sensors-25-05761" ref-type="bibr">7</xref>], which is derived as follows: <disp-formula id="FD17-sensors-25-05761"><label>(17)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the average value of <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> at all feature points.</p><p>The metric <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was employed to assess the error level of three models after projecting 2-D positioning results into the 3-D space. It is derived as follows: <disp-formula id="FD18-sensors-25-05761"><label>(18)</label><mml:math id="mm98" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where (<inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), which stand for the label and predicted coordinates of feature points with respect to CCF, can be calculated by Equation (<xref rid="FD3-sensors-25-05761" ref-type="disp-formula">3</xref>), based on <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. An a priori assumption is made that the calibration result of the line structured light plane equation and the intrinsic matrix of the camera is ideal, attributing any error solely to inaccuracies in the feature points&#8217; locations within the PCF, as predicted by the positioning models.</p><p>The absolute error curve, which represents the test outcomes using three models, is depicted in <xref rid="sensors-25-05761-f013" ref-type="fig">Figure 13</xref>, <xref rid="sensors-25-05761-f014" ref-type="fig">Figure 14</xref> and <xref rid="sensors-25-05761-f015" ref-type="fig">Figure 15</xref>, respectively. The green curve corresponds to the absolute error measured from the low-noise test set, the blue curve corresponds to absolute error measurements from the high-noise test set, and the red horizontal line denotes the MAE. All metrics related to the positioning errors of the feature points were calculated and arranged in <xref rid="sensors-25-05761-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05761-t004" ref-type="table">Table 4</xref>; the MAE, RMSE, and <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> were assessed along the X and Y directions of the image, and the average values along the two directions were calculated, represented as the average-MAE, average-RMSE, and average-<inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Simultaneously, the outcomes of WeldLight without CCAM are included in <xref rid="sensors-25-05761-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05761-t004" ref-type="table">Table 4</xref>, too. Notably, there was no specialized positioning performance analysis for different weld types in the test set due to the inherent uncertainty in weld types during welding processes.</p><p>The outcomes of the positioning performance evaluation for the weld feature points on a low-noise test dataset are listed in <xref rid="sensors-25-05761-t003" ref-type="table">Table 3</xref>. Within the low-noise scenario, the three models are basically at the same level for the MAE and RMSE. Notably, regarding the average-<inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, WeldLight with and without CCAM exhibited outstanding performance compared to the other two models, achieving values of 1.943 pixels and 1.922 pixels, respectively. In terms of <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, WeldLight showed the best performance, reaching 0.197 mm. The utilization of CCAM did not yield a significant difference in the low-noise test set condition.</p><p>The experimental findings from the positioning performance assessment of the welding seam feature points in a high-noise test set are illustrated in <xref rid="sensors-25-05761-t004" ref-type="table">Table 4</xref>. Within the high-noise environment, WeldLight exhibited an average MAE, RMSE, and <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 1.736 pixels, 2.407 pixels, and 0.205 mm, respectively. These metrics outperformed both YOLOv5 and DETR. Moreover, compared to WeldLight without CCAM, the model integrating CCAM showcased enhanced performance across nearly all the positioning metrics, reflecting more stable and accurate localization results. This improvement suggests that CCAM contributes to filtering out noise interference and enhancing the robustness of feature extraction, which is particularly beneficial in high-noise welding scenarios. This phenomenon contrasts the measurement results observed in the low-noise test set. WeldLight excels in average <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, registering a value of 2.217 pixel, underscoring WeldLight&#8217;s superior stability in accurately localizing weld feature points within high-noise welding images.</p></sec><sec id="sec4dot4-sensors-25-05761"><title>4.4. Lightweight&#160;Level</title><p>The model&#8217;s lightweight characteristics were evaluated based on multiple metrics, including the total number of model parameters (Params), floating point operations (FLOPs), mean latency, and frames per second (FPS). Params serve as an indirect measure of the computational complexity and memory utilization, while FLOPs represent the computational cost of the model [<xref rid="B41-sensors-25-05761" ref-type="bibr">41</xref>]. A lower mean latency (or a higher FPS) implies reduced inference time, a critical factor for optimal real-time performance in the welding seam tracking system employing this model. This aspect is particularly crucial for complex seam tracking applications, welding speed, and welding process control. The mean latency of the three models was determined through 100 tests using a single image, and the frames per second (FPS) value was calculated by dividing 1000 ms by the mean latency.</p><p>It is worth noting that, despite a consistent evaluation criterion, DETR was not designed for real-time tasks and thus lacks advantages in inference speed. Moreover, Transformer-based architectures such as DETR face inherent challenges in learning low-level features from scratch on limited datasets, making large-scale pretraining essential to achieve competitive performance with CNNs [<xref rid="B42-sensors-25-05761" ref-type="bibr">42</xref>]. In this study, these limitations were mitigated by pretraining on the COCO dataset, followed by local fine-tuning until full convergence. Nevertheless, such architectural characteristics remain important considerations when interpreting the results. Naturally, these factors were also taken into account during the initial design of WeldLight, to avoid similar limitations.</p><p><xref rid="sensors-25-05761-t005" ref-type="table">Table 5</xref> provides an overview of the lightweight metrics for the three models. WeldLight outperformed the others across all metrics, with the Params being 1.3 M, translating to 72% of YOLOv5n, 3.5% of DETR, and 0.87 GFLOPs, representing 21% of YOLOv5n and 1.5% of DETR. With a mean latency of 29.32 ms and FPS at 34.11 Hz, WeldLight fulfills stringent requirements for real-time weld seam tracking.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05761"><title>5. Conclusions</title><list list-type="order"><list-item><p>An improved CenterNet framework was proposed for positioning welding light-stripe feature points. The average MAE in the X and Y directions was 1.639 and 1.736 pixels, respectively, on both low- and high-noise test sets, demonstrating high prediction accuracy. On a single-line structured light platform, the mean Euclidean distance (<inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) of the 3-D feature point coordinates in the CCF was 0.197 mm and 0.205 mm for the two test sets, further confirming the robustness of the proposed method.</p></list-item><list-item><p>In the context of welding seam classification, an approach was proposed that directly derived one-hot encoding for welding seam classification predictions from coarse-resolution features selected by the attention module. This method differed from the conventional method of upsampling to the high-resolution features for prediction. WeldLight achieved an F1-score of 0.9668 on the adjusted test set.</p></list-item><list-item><p>An online data augmentation method, focused on randomly generated splash noise, was proposed to adapt the model to natural splash noise arising from welding processes, thereby reducing its dependence on the quality of the dataset. Furthermore, a lightweight attention module, CCAM, was introduced as a feature selector to enhance the model&#8217;s robustness against welding noise. The results show that WeldLight exhibited improved performance, particularly in high-noise scenes, upon incorporating CCAM.</p></list-item><list-item><p>The model was deployed on an EIC, utilizing the CPU for inference. The average processing speed for a single image was 29.32 ms, translating to 34.11 Hz, thereby satisfying the real-time requirements of the seam tracking system.</p></list-item></list><p>This study, rooted in real-world applications, reduces the computational and dataset requirements, thereby providing manufacturers with practical guidelines for deploying weld-seam tracking on edge devices without the need for costly GPUs. This capability not only lowers the deployment costs but also enhances automation and reduces the reliance on skilled labor.</p><p>Nevertheless, some limitations remain. Future research will focus on evaluating the model in more diverse welding scenarios and employing statistical methods to strengthen the reliability of the results. Moreover, the current architecture still has room for refinement; integrating advanced lightweight designs and exploring Transformer- or Mamba-based frameworks may further improve the performance. Finally, because the present approach still relies on manual annotation, future studies will investigate unsupervised and semi-supervised strategies to better adapt to evolving welding environments.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank all colleagues who contributed to the discussions and writing of this manuscript. We also sincerely appreciate the anonymous reviewers for their constructive comments and suggestions, which greatly improved the quality of this work.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.G. and W.L.; methodology, A.G., A.L. and F.S.; software, A.G. and X.Y.; validation, A.L. and A.G.; formal analysis, A.G. and F.S.; investigation, A.G. and A.L.; resources, F.D.; data curation, C.C. and A.L.; writing&#8212;original draft preparation, A.G. and W.L.; writing&#8212;review and editing, A.G., X.Y. and C.C.; visualization, F.S.; supervision, F.D.; project administration, A.G.; funding acquisition, F.D. and C.C. All authors have read and agreed to the published version of the manuscript</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05761"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Rong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>P.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name></person-group><article-title>Cross-lines laser aided machine vision in tube-to-tubesheet welding for welding height control</article-title><source>Opt. Laser Technol.</source><year>2020</year><volume>121</volume><fpage>105796</fpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2019.105796</pub-id></element-citation></ref><ref id="B2-sensors-25-05761"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name></person-group><article-title>Electric arc length control of circular seam in welding robot based on arc voltage sensing</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>3326</fpage><lpage>3333</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3143113</pub-id></element-citation></ref><ref id="B3-sensors-25-05761"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>Autonomous detection of weld seam profiles via a model of saliency-based visual attention for robotic arc welding</article-title><source>J. Intell. Robot. Syst.</source><year>2016</year><volume>81</volume><fpage>395</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1007/s10846-015-0226-y</pub-id></element-citation></ref><ref id="B4-sensors-25-05761"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wong</surname><given-names>X.I.</given-names></name><name name-style="western"><surname>Majji</surname><given-names>M.</given-names></name></person-group><article-title>A structured light system for relative navigation applications</article-title><source>IEEE Sens. J.</source><year>2016</year><volume>16</volume><fpage>6662</fpage><lpage>6679</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2016.2585438</pub-id></element-citation></ref><ref id="B5-sensors-25-05761"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>LSFP-tracker: An autonomous laser stripe feature point extraction algorithm based on siamese network for robotic welding seam tracking</article-title><source>IEEE Trans. Ind. Electron.</source><year>2023</year><volume>71</volume><fpage>1037</fpage><lpage>1048</lpage><pub-id pub-id-type="doi">10.1109/TIE.2023.3243265</pub-id></element-citation></ref><ref id="B6-sensors-25-05761"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name></person-group><article-title>A teaching-free welding position guidance method for fillet weld based on laser vision sensing and EGM technology</article-title><source>Optik</source><year>2022</year><volume>262</volume><fpage>169291</fpage><pub-id pub-id-type="doi">10.1016/j.ijleo.2022.169291</pub-id></element-citation></ref><ref id="B7-sensors-25-05761"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Khyam</surname><given-names>M.O.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>C.</given-names></name></person-group><article-title>Automatic welding seam tracking and identification</article-title><source>IEEE Trans. Ind. Electron.</source><year>2017</year><volume>64</volume><fpage>7261</fpage><lpage>7271</lpage><pub-id pub-id-type="doi">10.1109/TIE.2017.2694399</pub-id></element-citation></ref><ref id="B8-sensors-25-05761"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>E.</given-names></name><name name-style="western"><surname>Long</surname><given-names>T.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name></person-group><article-title>A high-speed seam extraction method based on the novel structured-light sensor for arc welding robot: A review</article-title><source>IEEE Sens. J.</source><year>2018</year><volume>18</volume><fpage>8631</fpage><lpage>8641</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2018.2867581</pub-id></element-citation></ref><ref id="B9-sensors-25-05761"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kalaichelvi</surname><given-names>V.</given-names></name><name name-style="western"><surname>Karthikeyan</surname><given-names>R.</given-names></name></person-group><article-title>Application of convolutional neural network for classification and tracking of weld seam shapes for TAL Brabo manipulator</article-title><source>Mater. Today Proc.</source><year>2020</year><volume>28</volume><fpage>491</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/j.matpr.2019.12.149</pub-id></element-citation></ref><ref id="B10-sensors-25-05761"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>A.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Du</surname><given-names>F.</given-names></name></person-group><article-title>YOLO-Weld: A Modified YOLOv5-Based Weld Feature Detection Network for Extreme Weld Noise</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>5640</elocation-id><pub-id pub-id-type="doi">10.3390/s23125640</pub-id><pub-id pub-id-type="pmid">37420805</pub-id><pub-id pub-id-type="pmcid">PMC10301933</pub-id></element-citation></ref><ref id="B11-sensors-25-05761"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name></person-group><article-title>A weld seam feature real-time extraction method of three typical welds based on target detection</article-title><source>Measurement</source><year>2023</year><volume>207</volume><fpage>112424</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.112424</pub-id></element-citation></ref><ref id="B12-sensors-25-05761"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name></person-group><article-title>Seam tracking system based on laser vision and CGAN for robotic multi-layer and multi-pass MAG welding</article-title><source>Eng. Appl. Artif. Intell.</source><year>2022</year><volume>116</volume><fpage>105377</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2022.105377</pub-id></element-citation></ref><ref id="B13-sensors-25-05761"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Carion</surname><given-names>N.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Synnaeve</surname><given-names>G.</given-names></name><name name-style="western"><surname>Usunier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name></person-group><article-title>End-to-end object detection with transformers</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B14-sensors-25-05761"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Advances in Neural Information Processing Systems 30, Proceedings of the NIPS&#8217;17: Proceedings of the 31st International Conference on Neural Information Processing System, Red Hook, NY, USA, 4&#8211;9 December 2017</source><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2017</year></element-citation></ref><ref id="B15-sensors-25-05761"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>A.G.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kalenichenko</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Weyand</surname><given-names>T.</given-names></name><name name-style="western"><surname>Andreetto</surname><given-names>M.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Mobilenets: Efficient convolutional neural networks for mobile vision applications</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1704.04861</pub-id><pub-id pub-id-type="arxiv">1704.04861</pub-id></element-citation></ref><ref id="B16-sensors-25-05761"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sandler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Howard</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhmoginov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name></person-group><article-title>Mobilenetv2: Inverted residuals and linear bottlenecks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City</source><conf-loc>UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>4510</fpage><lpage>4520</lpage></element-citation></ref><ref id="B17-sensors-25-05761"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sandler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Vasudevan</surname><given-names>V.</given-names></name><etal/></person-group><article-title>Searching for mobilenetv3</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>1314</fpage><lpage>1324</lpage></element-citation></ref><ref id="B18-sensors-25-05761"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Leichner</surname><given-names>C.</given-names></name><name name-style="western"><surname>Delakis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fornoni</surname><given-names>M.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Banbury</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>C.</given-names></name><name name-style="western"><surname>Akin</surname><given-names>B.</given-names></name><etal/></person-group><article-title>MobileNetV4-Universal Models for the Mobile Ecosystem</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2404.10518</pub-id></element-citation></ref><ref id="B19-sensors-25-05761"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Shufflenet: An extremely efficient convolutional neural network for mobile devices</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City</source><conf-loc>UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>6848</fpage><lpage>6856</lpage></element-citation></ref><ref id="B20-sensors-25-05761"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>H.T.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Shufflenet v2: Practical guidelines for efficient cnn architecture design</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>116</fpage><lpage>131</lpage></element-citation></ref><ref id="B21-sensors-25-05761"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yun</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ro</surname><given-names>Y.</given-names></name></person-group><article-title>Shvit: Single-head vision transformer with memory efficient macro design</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>5756</fpage><lpage>5767</lpage></element-citation></ref><ref id="B22-sensors-25-05761"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name></person-group><article-title>LSNet: See Large, Focus Small</article-title><source>Proceedings of the Computer Vision and Pattern Recognition Conference</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>10&#8211;17 June 2025</conf-date><fpage>9718</fpage><lpage>9729</lpage></element-citation></ref><ref id="B23-sensors-25-05761"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ouyang</surname><given-names>D.</given-names></name><name name-style="western"><surname>He</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name></person-group><article-title>Efficient multi-scale attention module with cross-spatial learning</article-title><source>Proceedings of the ICASSP 2023&#8212;2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Rodos, Greece</conf-loc><conf-date>4&#8211;10 June 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B24-sensors-25-05761"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Si</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>SCSA: Exploring the synergistic effects between spatial and channel attention</article-title><source>Neurocomputing</source><year>2025</year><volume>634</volume><fpage>129866</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2025.129866</pub-id></element-citation></ref><ref id="B25-sensors-25-05761"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Munir</surname><given-names>M.</given-names></name><name name-style="western"><surname>Marculescu</surname><given-names>R.</given-names></name></person-group><article-title>Emcad: Efficient multi-scale convolutional attention decoding for medical image segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>11769</fpage><lpage>11779</lpage></element-citation></ref><ref id="B26-sensors-25-05761"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>J.</given-names></name></person-group><article-title>3DSMDA-Net: An improved 3DCNN with separable structure and multi-dimensional attention for welding status recognition</article-title><source>J. Manuf. Syst.</source><year>2022</year><volume>62</volume><fpage>811</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1016/j.jmsy.2021.01.017</pub-id></element-citation></ref><ref id="B27-sensors-25-05761"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>F.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name></person-group><article-title>WeldNet: A deep learning based method for weld seam type identification and initial point guidance</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>238</volume><fpage>121700</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.121700</pub-id></element-citation></ref><ref id="B28-sensors-25-05761"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>G.</given-names></name></person-group><article-title>Light-weight segmentation network based on SOLOv2 for weld seam feature extraction</article-title><source>Measurement</source><year>2023</year><volume>208</volume><fpage>112492</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.112492</pub-id></element-citation></ref><ref id="B29-sensors-25-05761"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>N.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name></person-group><article-title>EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>14420</fpage><lpage>14430</lpage></element-citation></ref><ref id="B30-sensors-25-05761"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><source>A Flexible New Technique for Camera Calibration. Microsoft Research Technical Report</source><publisher-name>Microsoft Corporation</publisher-name><publisher-loc>Redmond, WA, USA</publisher-loc><year>1998</year></element-citation></ref><ref id="B31-sensors-25-05761"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name></person-group><article-title>A simple calibration method of structured light plane parameters for welding robots</article-title><source>Proceedings of the 2016 35th Chinese Control Conference (CCC)</source><conf-loc>Chengdu, China</conf-loc><conf-date>27&#8211;29 July 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2016</year><fpage>6127</fpage><lpage>6132</lpage></element-citation></ref><ref id="B32-sensors-25-05761"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>Automated non-PPE detection on construction sites using YOLOv10 and transformer architectures for surveillance and body worn cameras with benchmark datasets</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><elocation-id>27043</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-025-12468-8</pub-id><pub-id pub-id-type="pmid">40715598</pub-id><pub-id pub-id-type="pmcid">PMC12297540</pub-id></element-citation></ref><ref id="B33-sensors-25-05761"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sonwane</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chiddarwar</surname><given-names>S.</given-names></name></person-group><article-title>Enhancing weld defect detection and classification with MDCBNet: A multi-scale dense cross block network for improved explainability</article-title><source>NDT E Int.</source><year>2024</year><volume>142</volume><fpage>103029</fpage><pub-id pub-id-type="doi">10.1016/j.ndteint.2023.103029</pub-id></element-citation></ref><ref id="B34-sensors-25-05761"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name></person-group><article-title>Feature pyramid networks for object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>2117</fpage><lpage>2125</lpage></element-citation></ref><ref id="B35-sensors-25-05761"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nwankpa</surname><given-names>C.</given-names></name></person-group><article-title>Activation functions: Comparison of trends in practice and research for deep learning</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1811.03378</pub-id><pub-id pub-id-type="arxiv">1811.03378</pub-id></element-citation></ref><ref id="B36-sensors-25-05761"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kr&#228;henb&#252;hl</surname><given-names>P.</given-names></name></person-group><article-title>Objects as points</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1904.07850</pub-id></element-citation></ref><ref id="B37-sensors-25-05761"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Law</surname><given-names>H.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name></person-group><article-title>Cornernet: Detecting objects as paired keypoints</article-title><source>Proceedings of the European conference on computer vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>734</fpage><lpage>750</lpage></element-citation></ref><ref id="B38-sensors-25-05761"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Goyal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name></person-group><article-title>Focal loss for dense object detection</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2980</fpage><lpage>2988</lpage></element-citation></ref><ref id="B39-sensors-25-05761"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Fast r-cnn</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Santiago, Chile</conf-loc><conf-date>7&#8211;13 December 2015</conf-date><fpage>1440</fpage><lpage>1448</lpage></element-citation></ref><ref id="B40-sensors-25-05761"><label>40.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Shmueli</surname><given-names>B.</given-names></name></person-group><source>Multiclass Metrics Made Simple, Part I: Precision and Recall</source><publisher-name>Medium</publisher-name><publisher-loc>San Francisco, CA, USA</publisher-loc><year>2020</year></element-citation></ref><ref id="B41-sensors-25-05761"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Arnab</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tay</surname><given-names>Y.</given-names></name></person-group><article-title>The efficiency misnomer</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2110.12894</pub-id></element-citation></ref><ref id="B42-sensors-25-05761"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raghu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kornblith</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dosovitskiy</surname><given-names>A.</given-names></name></person-group><article-title>Do vision transformers see like convolutional neural networks?</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>12116</fpage><lpage>12128</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05761-f001" orientation="portrait"><label>Figure 1</label><caption><p>Discerning signal and noise characteristics in industrial camera images for y-seam, lap-seam, and butt-seam welds. (<bold>a</bold>) Noise-free structured light stripe image. (<bold>b</bold>) Structured light stripe image affected by noise. (<bold>c</bold>) Characteristic point of the structured light stripe. (<bold>d</bold>) Part of the feature points affected by noise. (<bold>e</bold>) All feature points affected by noise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g001.jpg"/></fig><fig position="float" id="sensors-25-05761-f002" orientation="portrait"><label>Figure 2</label><caption><p>Laser vision sensor experimental platform.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g002.jpg"/></fig><fig position="float" id="sensors-25-05761-f003" orientation="portrait"><label>Figure 3</label><caption><p>The effect diagram of structured light projection on different welds. (<bold>a</bold>) illustrates the projection effect on a Y-seam and embodies the measurement principle of a single-line structured light vision sensor, and (<bold>b</bold>,<bold>c</bold>) depicts the projection effect of structured light on the lap-seam and butt-seam, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g003.jpg"/></fig><fig position="float" id="sensors-25-05761-f004" orientation="portrait"><label>Figure 4</label><caption><p>Images with different noise levels in the dataset. (<bold>a</bold>) Noise-free image; (<bold>b</bold>) slightly noisy image; (<bold>c</bold>) high-noise image.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g004.jpg"/></fig><fig position="float" id="sensors-25-05761-f005" orientation="portrait"><label>Figure 5</label><caption><p>The operation flow of the proposed data augmentation method. (<bold>a</bold>) is a randomly generated simulated splash light noise map; (<bold>b</bold>) is the result of Gaussian filtering on (<bold>a</bold>); (<bold>c</bold>) is the original image from the dataset; (<bold>d</bold>) is the result of superimposing (<bold>b</bold>) on (<bold>c</bold>); (<bold>e</bold>) indicates the results of other data augmentation operations.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g005.jpg"/></fig><fig position="float" id="sensors-25-05761-f006" orientation="portrait"><label>Figure 6</label><caption><p>Overall layout of the proposed network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g006.jpg"/></fig><fig position="float" id="sensors-25-05761-f007" orientation="portrait"><label>Figure 7</label><caption><p>Architecture of top-down pathway and lateral connection. (<bold>a</bold>) is the original version of FPN; (<bold>b</bold>) is the proposed version.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g007.jpg"/></fig><fig position="float" id="sensors-25-05761-f008" orientation="portrait"><label>Figure 8</label><caption><p>Specific structure of cascade channel attention module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g008.jpg"/></fig><fig position="float" id="sensors-25-05761-f009" orientation="portrait"><label>Figure 9</label><caption><p>A case in which some candidate feature points may not be filtered out when the pool filter is small.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g009.jpg"/></fig><fig position="float" id="sensors-25-05761-f010" orientation="portrait"><label>Figure 10</label><caption><p>Part of the pictures in the adjusted test set used in the performance test of welding classification. (<bold>a</bold>&#8211;<bold>c</bold>) are examples of the adjusted Y-seam, lap-seam, and butt-seam images in the classification performance test.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g010.jpg"/></fig><fig position="float" id="sensors-25-05761-f011" orientation="portrait"><label>Figure 11</label><caption><p>Confusion matrix obtained by different models on the adjusted test set. (<bold>a</bold>&#8211;<bold>c</bold>) correspond to the prediction results of WeldLight, YOLOv5n, and DETR, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g011.jpg"/></fig><fig position="float" id="sensors-25-05761-f012" orientation="portrait"><label>Figure 12</label><caption><p>Part of the regression results obtained by different models on the test set.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g012.jpg"/></fig><fig position="float" id="sensors-25-05761-f013" orientation="portrait"><label>Figure 13</label><caption><p>The absolute error curve of the WeldLight. (<bold>a</bold>,<bold>b</bold>) illustrate the feature points regression absolute errors of the model in the X and Y directions, respectively, evaluated on the low-noise test set. (<bold>c</bold>,<bold>d</bold>) showcase the feature points regression absolute errors of the WeldLight in the X and Y directions, respectively, tested on the high-noise test set. The peak value, the peak position of the absolute error, and the MAE are annotated.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g013.jpg"/></fig><fig position="float" id="sensors-25-05761-f014" orientation="portrait"><label>Figure 14</label><caption><p>The absolute error curve of the YOLOv5. (<bold>a</bold>,<bold>b</bold>) illustrate the feature points regression absolute errors of the model in the X and Y directions, respectively, evaluated on the low-noise test set. (<bold>c</bold>,<bold>d</bold>) showcase the feature points regression absolute errors of the YOLOv5 in the X and Y directions, respectively, tested on the high-noise test set. The peak value, the peak position of the absolute error, and the MAE, are annotated.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g014.jpg"/></fig><fig position="float" id="sensors-25-05761-f015" orientation="portrait"><label>Figure 15</label><caption><p>The absolute error curve of the DETR. (<bold>a</bold>,<bold>b</bold>) illustrate the feature points regression absolute errors of the model in the X and Y directions, respectively, evaluated on the low-noise test set. (<bold>c</bold>,<bold>d</bold>) showcase the feature points regression absolute errors of the DETR in the X and Y directions, respectively, tested on the high-noise test set. The peak value, the peak position of the absolute error, and the MAE are annotated.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05761-g015.jpg"/></fig><table-wrap position="float" id="sensors-25-05761-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05761-t001_Table 1</object-id><label>Table 1</label><caption><p>Partition of the&#160;dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Y</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Lap</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Butt</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Train</td><td align="center" valign="middle" rowspan="1" colspan="1">415</td><td align="center" valign="middle" rowspan="1" colspan="1">404</td><td align="center" valign="middle" rowspan="1" colspan="1">396</td><td align="center" valign="middle" rowspan="1" colspan="1">1215</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Val</td><td align="center" valign="middle" rowspan="1" colspan="1">33</td><td align="center" valign="middle" rowspan="1" colspan="1">45</td><td align="center" valign="middle" rowspan="1" colspan="1">57</td><td align="center" valign="middle" rowspan="1" colspan="1">135</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Test</td><td align="center" valign="middle" rowspan="1" colspan="1">52</td><td align="center" valign="middle" rowspan="1" colspan="1">51</td><td align="center" valign="middle" rowspan="1" colspan="1">47</td><td align="center" valign="middle" rowspan="1" colspan="1">150</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1500</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05761-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05761-t002_Table 2</object-id><label>Table 2</label><caption><p>Test overall metric scores of different models on adjusted test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">WeldLight</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">YOLOv5n</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DETR</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9674</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9281</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9336</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9666</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9067</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9333</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9668</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9090</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9334</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05761-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05761-t003_Table 3</object-id><label>Table 3</label><caption><p>Statistical characteristics of the positioning performance of the three models for light stripe feature points of the low-noise weld structure. <sup>&#8224;</sup>: WeldLight omited the utilization of CCAM. The units for MAE, RMSE, and <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> were all expressed in pixels, while the <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was measured in mm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ours</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ours <sup>&#8224;</sup></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">YOLOv5n</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DETR</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">X-MAE</td><td align="center" valign="middle" rowspan="1" colspan="1">1.898</td><td align="center" valign="middle" rowspan="1" colspan="1">1.859</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.813</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.917</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Y-MAE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.380</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.502</td><td align="center" valign="middle" rowspan="1" colspan="1">1.545</td><td align="center" valign="middle" rowspan="1" colspan="1">1.402</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average-MAE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.639</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.681</td><td align="center" valign="middle" rowspan="1" colspan="1">1.679</td><td align="center" valign="middle" rowspan="1" colspan="1">1.660</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">X-RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">2.544</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.326</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.401</td><td align="center" valign="middle" rowspan="1" colspan="1">2.535</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Y-RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.740</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.885</td><td align="center" valign="middle" rowspan="1" colspan="1">1.926</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.740</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average-RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">2.142</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.106</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.164</td><td align="center" valign="middle" rowspan="1" colspan="1">2.138</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">X-<inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">2.258</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.183</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.392</td><td align="center" valign="middle" rowspan="1" colspan="1">2.422</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Y-<inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.627</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.660</td><td align="center" valign="middle" rowspan="1" colspan="1">1.911</td><td align="center" valign="middle" rowspan="1" colspan="1">1.730</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average-<inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">1.943</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.922</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.152</td><td align="center" valign="middle" rowspan="1" colspan="1">2.076</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.197</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.216</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.232</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.209</td></tr></tbody></table><table-wrap-foot><fn><p><bold>Note:</bold> The best results are highlighted in bold.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05761-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05761-t004_Table 4</object-id><label>Table 4</label><caption><p>Statistical characteristics of the positioning performance of the three models for light stripe feature points of a high-noise weld structure. <sup>&#8224;</sup>: WeldLight omited the utilization of CCAM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ours</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ours <sup>&#8224;</sup></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">YOLOv5n</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DETR</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">X-MAE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.034</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.123</td><td align="center" valign="middle" rowspan="1" colspan="1">2.146</td><td align="center" valign="middle" rowspan="1" colspan="1">2.080</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Y-MAE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.437</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.646</td><td align="center" valign="middle" rowspan="1" colspan="1">1.760</td><td align="center" valign="middle" rowspan="1" colspan="1">1.668</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average-MAE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.736</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.885</td><td align="center" valign="middle" rowspan="1" colspan="1">1.953</td><td align="center" valign="middle" rowspan="1" colspan="1">1.874</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">X-RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">2.931</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.927</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">3.023</td><td align="center" valign="middle" rowspan="1" colspan="1">3.043</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Y-RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.883</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.206</td><td align="center" valign="middle" rowspan="1" colspan="1">2.275</td><td align="center" valign="middle" rowspan="1" colspan="1">2.172</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average-RMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.407</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.567</td><td align="center" valign="middle" rowspan="1" colspan="1">2.649</td><td align="center" valign="middle" rowspan="1" colspan="1">2.608</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">X-<inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.642</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.731</td><td align="center" valign="middle" rowspan="1" colspan="1">3.029</td><td align="center" valign="middle" rowspan="1" colspan="1">2.919</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Y-<inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.792</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.032</td><td align="center" valign="middle" rowspan="1" colspan="1">2.254</td><td align="center" valign="middle" rowspan="1" colspan="1">2.176</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average-<inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.217</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.382</td><td align="center" valign="middle" rowspan="1" colspan="1">2.646</td><td align="center" valign="middle" rowspan="1" colspan="1">2.548</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.205</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.231</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.253</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.238</td></tr></tbody></table><table-wrap-foot><fn><p><bold>Note:</bold> The best results are highlighted in bold.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05761-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05761-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of lightweight metrics of three models on Intel Core i7-8650U CPU.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">WeldLight</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">YOLOv5n</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DETR</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Params (M)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.8</td><td align="center" valign="middle" rowspan="1" colspan="1">37</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GFLOPs</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.87</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">4.1</td><td align="center" valign="middle" rowspan="1" colspan="1">57</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mean latency (ms)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>29.32</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">53.07</td><td align="center" valign="middle" rowspan="1" colspan="1">681.43</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FPS (Hz)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>34.11</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.83</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.47</td></tr></tbody></table><table-wrap-foot><fn><p><bold>Note:</bold> The best results are highlighted in bold.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>