<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473765</article-id><article-id pub-id-type="pmcid-ver">PMC12473765.1</article-id><article-id pub-id-type="pmcaid">12473765</article-id><article-id pub-id-type="pmcaiid">12473765</article-id><article-id pub-id-type="pmid">41013168</article-id><article-id pub-id-type="doi">10.3390/s25185931</article-id><article-id pub-id-type="publisher-id">sensors-25-05931</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Enhancing Medical Image Segmentation and Classification Using a Fuzzy-Driven Method</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Abduvaitov</surname><given-names initials="A">Akmal</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05931" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shavkatovich Buriboev</surname><given-names initials="A">Abror</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af2-sensors-25-05931" ref-type="aff">2</xref><xref rid="af4-sensors-25-05931" ref-type="aff">4</xref><xref rid="af5-sensors-25-05931" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Sultanov</surname><given-names initials="D">Djamshid</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af2-sensors-25-05931" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Buriboev</surname><given-names initials="S">Shavkat</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af3-sensors-25-05931" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-7146-2106</contrib-id><name name-style="western"><surname>Yusupov</surname><given-names initials="O">Ozod</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af4-sensors-25-05931" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jasur</surname><given-names initials="K">Kilichov</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05931" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6507-3081</contrib-id><name name-style="western"><surname>Choi</surname><given-names initials="AJ">Andrew Jaeyong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af5-sensors-25-05931" ref-type="aff">5</xref><xref rid="c1-sensors-25-05931" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Domingues</surname><given-names initials="I">Ines</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05931"><label>1</label>Department of Information Technologies, Samarkand Branch of Tashkent University of Information Technologies, Samarkand 140100, Uzbekistan; <email>abduvaitovakmal6@gmail.com</email> (A.A.); <email>jasurkilichov1987@gmail.com</email> (K.J.)</aff><aff id="af2-sensors-25-05931"><label>2</label>Department of Infocommunication Engineering, Tashkent University of Information Technologies Named After Muhammad Al-Khwarizmi, Tashkent 100200, Uzbekistan; <email>abror1989@gachon.ac.kr</email> (A.S.B.); <email>djamshidsultanov05@gmail.com</email> (D.S.)</aff><aff id="af3-sensors-25-05931"><label>3</label>Department of Civil Engineering, Samarkand State Architecture and Construction University, Samarkand 140100, Uzbekistan; <email>abbosshav@gmail.com</email></aff><aff id="af4-sensors-25-05931"><label>4</label>Department of Software Engineering, Samarkand State University, Samarkand 140100, Uzbekistan; <email>ozodyusupov@gmail.com</email></aff><aff id="af5-sensors-25-05931"><label>5</label>Department of AI-Software, Gachon University, Sujeong-Gu, Seongnam-si 13120, Republic of Korea</aff><author-notes><corresp id="c1-sensors-25-05931"><label>*</label>Correspondence: <email>andrewjchoi@gachon.ac.kr</email></corresp></author-notes><pub-date pub-type="epub"><day>22</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5931</elocation-id><history><date date-type="received"><day>16</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>02</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>22</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05931.pdf"/><abstract><p>Automated analysis for tumor segmentation and illness classification is hampered by the noise, low contrast, and ambiguity that are common in medical pictures. This work introduces a new 12-step fuzzy-based improvement pipeline that uses fuzzy entropy, fuzzy standard deviation, and histogram spread functions to enhance picture quality in CT, MRI, and X-ray modalities. The pipeline produces three improved versions per dataset, lowering BRISQUE scores from 28.8 to 21.7 (KiTS19), 30.3 to 23.4 (BraTS2020), and 26.8 to 22.1 (Chest X-ray). It is tested on KiTS19 (CT) for kidney tumor segmentation, BraTS2020 (MRI) for brain tumor segmentation, and Chest X-ray Pneumonia for classification. A Concatenated CNN (CCNN) uses the improved datasets to achieve a Dice coefficient of 99.60% (KiTS19, +2.40% over baseline), segmentation accuracy of 0.983 (KiTS19) and 0.981 (BraTS2020) versus 0.959 and 0.943 (CLAHE), and classification accuracy of 0.974 (Chest X-ray) versus 0.917 (CLAHE). A classic CNN is trained on original and CLAHE-filtered datasets. These outcomes demonstrate how well the pipeline works to improve image quality and increase segmentation/classification accuracy, offering a foundation for clinical diagnostics that is both scalable and interpretable.</p></abstract><kwd-group><kwd>image enhancement</kwd><kwd>fuzzy approach</kwd><kwd>concatenated CNN</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05931"><title>1. Introduction</title><p>Medical imaging, which includes modalities like computed tomography (CT), magnetic resonance imaging (MRI), and X-ray, is essential to contemporary healthcare because it makes it easier to diagnose, monitor, and plan treatments for illnesses. These methods enable crucial tasks like tumor segmentation and illness classification by providing detailed representations of anatomical components [<xref rid="B1-sensors-25-05931" ref-type="bibr">1</xref>]. However, quality problems, including noise, low contrast, and unclear pixel intensities, are common in medical images, making it difficult for automated analytic algorithms to detect subtle signals [<xref rid="B2-sensors-25-05931" ref-type="bibr">2</xref>]. In soft tissue areas such as kidney tumors, for instance, CT images may show low contrast [<xref rid="B3-sensors-25-05931" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05931" ref-type="bibr">4</xref>], MRI scans are often impacted by noise across multimodal sequences (e.g., T1, T2, FLAIR) [<xref rid="B5-sensors-25-05931" ref-type="bibr">5</xref>], and X-ray images suffer from low contrast, which makes it difficult to detect lung opacities associated with pneumonia [<xref rid="B6-sensors-25-05931" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05931" ref-type="bibr">7</xref>]. To overcome these obstacles and raise the accuracy of subsequent medical duties, sophisticated picture-enhancing techniques are required.</p><p>Conventional enhancement techniques, including histogram equalization, have been widely used to improve contrast and lower noise in order to address these problems. Recent advancements in conventional techniques, such as Liu et al.&#8217;s adaptive contrast enhancement [<xref rid="B8-sensors-25-05931" ref-type="bibr">8</xref>]. By using membership functions to characterize uncertainty, fuzzy logic-based methods [<xref rid="B9-sensors-25-05931" ref-type="bibr">9</xref>] create a convincing substitute and give a nuanced depiction of pixel intensities. Fuzzy logic for medical image enhancement has been studied recently. Singh et al. [<xref rid="B10-sensors-25-05931" ref-type="bibr">10</xref>] suggested a fuzzy-based framework to improve skin cancer detection. Gupta [<xref rid="B11-sensors-25-05931" ref-type="bibr">11</xref>] proposed a fuzzy enhancement&#8211;based segmentation approach; however, the method was primarily restricted to a specific dataset. This limitation arises because the fuzzy membership functions and enhancement parameters were tuned to the intensity distribution of that dataset and were not designed to adapt to other modalities. In addition, evaluation was carried out only within that dataset, without cross-dataset validation, which raises concerns about generalizability. By contrast, the present work employs adaptive parameterization strategies that allow the enhancement pipeline to adjust automatically to new datasets, as demonstrated in both MRI (KiTS19) and X-ray (ChestX-ray) experiments. In a similar vein, Linda et al. [<xref rid="B12-sensors-25-05931" ref-type="bibr">12</xref>] improved bone fracture identification in X-ray pictures by developing a fuzzy clustering technique; however, its generalizability across modalities has not been verified. Despite these developments, current fuzzy-based techniques frequently lack a cohesive framework to handle the complex problems of ambiguity, contrast, and noise in a variety of medical imaging jobs.</p><p>This work presents a brand-new 12-step fuzzy-based image improvement pipeline that is intended to enhance the quality of medical images from X-ray, CT, and MRI modalities. To improve local contrast, lower noise, and preserve small features, the pipeline incorporates fuzzy logic techniques such as fuzzy entropy, fuzzy standard deviation, and histogram spread functions. We assess its performance on three publicly accessible datasets: Chest X-ray Pneumonia (available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19/data">https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19/data</uri>, accessed on 9 September 2025) for pneumonia classification, BraTS2020 (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/awsaf49/brats2020-training-data">https://www.kaggle.com/datasets/awsaf49/brats2020-training-data</uri>, accessed on 9 September 2025) for brain tumor segmentation, and KiTS19 (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia">https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia</uri>, accessed on 3 June 2020) for kidney tumor segmentation. The BRISQUE measure is used to compare the three improved versions of each dataset&#8212;fuzzy entropy-based, fuzzy standard deviation-based, and histogram-based&#8212;against the original, conventionally filtered datasets.</p><p>We train and evaluate two models&#8212;a classic Convolutional Neural Network (CNN) on the original and conventionally filtered datasets and a Concatenated CNN (CCNN) that takes advantage of the tripling of dataset size from the enhancement process (three enhanced images per input)&#8212;in order to determine the effect of the pipeline on downstream tasks. The purpose of this work is to show that the suggested pipeline considerably improves model performance in segmentation and classification tasks, in addition to improving image quality.</p><p>There are three things this work contributes. First, we introduce a thorough fuzzy-based enhancement pipeline that successfully manages uncertainty across several modalities, overcoming the drawbacks of conventional techniques. Second, by contrasting the pipeline with conventional enhancement methods, we present a thorough assessment of its performance on segmentation (KiTS19, BraTS2020) and classification (Chest X-ray Pneumonia) tasks. Third, we demonstrate how the improved datasets can be used practically to increase the robustness and accuracy of deep-learning models, which may have consequences for clinical diagnoses. This study lays the groundwork for more dependable automated diagnosis systems by tackling noise and low contrast in medical images.</p></sec><sec id="sec2-sensors-25-05931"><title>2. Related Works</title><p>A crucial preprocessing step in medical imaging is image enhancement, which aims to improve image quality to ensure that tasks like segmentation and classification can be accurately analyzed. Numerous enhancement approaches have been developed in response to the difficulties posed by noise, low contrast, and unclear pixel intensities in modalities such as CT, MRI, and X-ray. This section examines current methodologies, classifying them into three categories: deep-learning-based methods, fuzzy logic-based approaches, and classic enhancement techniques. It then places our suggested 12-step fuzzy-based enhancement pipeline in relation to these approaches.</p><sec id="sec2dot1-sensors-25-05931"><title>2.1. Traditional Enhancement Techniques</title><p>Conventional techniques use filtering and pixel intensity modifications to increase contrast and lower noise. Contrast Limited Adaptive Histogram Equalization (CLAHE), first presented by Zuiderveld [<xref rid="B13-sensors-25-05931" ref-type="bibr">13</xref>], limits contrast stretching in homogeneous zones to improve local contrast while reducing noise amplification. CLAHE has been used extensively, boosting soft tissue contrast in CT scans for kidney tumor segmentation [<xref rid="B14-sensors-25-05931" ref-type="bibr">14</xref>] and lung opacity visible in X-ray imaging for pneumonia detection [<xref rid="B15-sensors-25-05931" ref-type="bibr">15</xref>]. Although adaptive histogram equalization, a fundamental method for contrast enhancement, was proposed by Pizer et al. [<xref rid="B16-sensors-25-05931" ref-type="bibr">16</xref>], it has trouble maintaining small features in noisy datasets such as BraTS2020. While Gaussian filtering, used by Jain [<xref rid="B17-sensors-25-05931" ref-type="bibr">17</xref>], lowers noise in MRI scans, gamma correction, as investigated by Gonzalez et al. [<xref rid="B18-sensors-25-05931" ref-type="bibr">18</xref>], modifies brightness in X-ray images. Polesel et al. [<xref rid="B19-sensors-25-05931" ref-type="bibr">19</xref>] have employed more sophisticated spatial filtering methods, like unsharp masking, to improve edge features in CT images; nevertheless, these methods frequently exacerbate noise in areas with poor contrast. Similarly, Huang et al. [<xref rid="B20-sensors-25-05931" ref-type="bibr">20</xref>] used median filtering for X-ray enhancement, which effectively eliminates salt-and-pepper noise but may obscure important features like tumor borders. These computationally efficient techniques frequently result in over-enhancement or loss of small details in tasks like brain tumor segmentation because they are unable to handle the inherent uncertainties in medical pictures.</p></sec><sec id="sec2dot2-sensors-25-05931"><title>2.2. Fuzzy Logic-Based Approaches</title><p>Because fuzzy logic-based techniques portray pixel intensities as membership degrees, they effectively handle the ambiguity and uncertainty prevalent in medical pictures. This makes them ideal for tasks involving overlapping intensity distributions, including tumor segmentation. While it was only applicable to some modalities, Kim et al. [<xref rid="B21-sensors-25-05931" ref-type="bibr">21</xref>] presented a fuzzy-based contrast enhancement method for medical pictures that uses fuzzy sets to describe intensity changes and enhance tissue border visibility in ultrasound images. Pal et al. [<xref rid="B22-sensors-25-05931" ref-type="bibr">22</xref>] improved micro-calcification identification over histogram-based techniques by using fuzzy entropy to increase contrast in mammogram images; however, testing across several modalities, such as CT or MRI, was not conducted. By improving edge recognition in MRI scans, Tizhoosh [<xref rid="B23-sensors-25-05931" ref-type="bibr">23</xref>] introduced fuzzy image processing; nevertheless, its emphasis on edges ignored more comprehensive noise reduction. Although they only looked at contrast, Deng et al. [<xref rid="B24-sensors-25-05931" ref-type="bibr">24</xref>] have more recently investigated fuzzy standard deviation-based methods to improve local contrast in CT scans, increasing lesion visibility. In order to improve MRI noise reduction, Balafar et al. [<xref rid="B25-sensors-25-05931" ref-type="bibr">25</xref>] used fuzzy clustering; nonetheless, their approach was computationally demanding. Fuzzy logic was utilized by Ananthi et al. [<xref rid="B26-sensors-25-05931" ref-type="bibr">26</xref>] to improve X-ray images and detect bone fractures, but they had trouble generalizing their findings across datasets. While promising, these fuzzy-based methods often address a single aspect of enhancement (e.g., contrast or noise) and lack a unified pipeline to tackle multiple challenges simultaneously, as required for diverse datasets like KiTS19, BraTS2020, and Chest X-ray Pneumonia.</p></sec><sec id="sec2dot3-sensors-25-05931"><title>2.3. Deep-Learning-Based Methods</title><p>Image enhancement has been transformed by deep learning [<xref rid="B27-sensors-25-05931" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05931" ref-type="bibr">28</xref>], which is frequently included in end-to-end frameworks for classification and segmentation. A deep convolutional neural network (CNN) was created by Chen et al. [<xref rid="B29-sensors-25-05931" ref-type="bibr">29</xref>] to improve brain tumor segmentation performance on datasets such as BraTS2019 by enhancing low-contrast MRI images. Although generative adversarial networks (GANs) sometimes introduce artifacts, as stated by Isola et al. [<xref rid="B30-sensors-25-05931" ref-type="bibr">30</xref>], Yang et al. [<xref rid="B31-sensors-25-05931" ref-type="bibr">31</xref>] used GANs to improve X-ray pictures for pneumonia detection, outperforming CLAHE with increased classification accuracy. Goodfellow et al. [<xref rid="B32-sensors-25-05931" ref-type="bibr">32</xref>] established the groundwork for GANs, which Jiang et al. [<xref rid="B33-sensors-25-05931" ref-type="bibr">33</xref>] modified for medical imaging to improve BraTS2020 data. However, synthetic artifacts raise questions about diagnostic reliability. Ronneberger et al. [<xref rid="B34-sensors-25-05931" ref-type="bibr">34</xref>] proposed U-Net topologies, which have since gained popularity. Da Cruz et al. [<xref rid="B35-sensors-25-05931" ref-type="bibr">35</xref>] used UNet2d to achieve a 96.33% Dice coefficient on KiTS19. While Lu et al. [<xref rid="B36-sensors-25-05931" ref-type="bibr">36</xref>] introduced TransUNet and achieved 89.20% on BraTS2020, Zhao et al. [<xref rid="B37-sensors-25-05931" ref-type="bibr">37</xref>] extended this to UNet3d and reported 96.90% on KiTS19. Liu et al. [<xref rid="B38-sensors-25-05931" ref-type="bibr">38</xref>] investigated Swin Transformers and reported 88.50%, whereas Isensee et al. [<xref rid="B39-sensors-25-05931" ref-type="bibr">39</xref>] developed nnU-Net, a self-configuring model with an 88.90% Dice on BraTS2020. Ibrahim et al. [<xref rid="B40-sensors-25-05931" ref-type="bibr">40</xref>] used deep learning to obtain 98.1% classification accuracy on chest X-ray pneumonia, whereas Kermany et al. [<xref rid="B41-sensors-25-05931" ref-type="bibr">41</xref>] reported 92.8% accuracy using CNNs. CheXNet was introduced by Rajpurkar et al. [<xref rid="B42-sensors-25-05931" ref-type="bibr">42</xref>] and achieved a 0.921 AUC for the identification of pneumonia. However, interpretability&#8212;which is crucial in clinical settings&#8212;is sometimes overlooked by deep-learning techniques, which call for sizable, labeled datasets and substantial computational resources.</p><p>In contrast to the previously described techniques, our research presents a 12-step fuzzy-based image improvement pipeline that combines fuzzy entropy, fuzzy standard deviation, and histogram spread functions to handle poor contrast, noise, and ambiguity in a single framework. Although contrast is improved by CLAHE and conventional techniques, they frequently cannot manage pixel intensity uncertainty, which results in less-than-ideal performance on challenging tasks. Current fuzzy-based methods lack universality across modalities and tasks, despite being successful in certain situations. Despite their strength, deep-learning techniques are resource-intensive and can create artifacts that compromise reliability. Our pipeline shows adaptability with BRISQUE scores of 21.7, 23.4, and 22.1 on the tests for KiTS19 (CT), BraTS2020 (MRI), and Chest X-ray Pneumonia (X-ray), respectively. We offer a strong preprocessing framework that improves downstream task performance using a Concatenated CNN (CCNN), achieving Dice coefficients of 99.60% (KiTS19) and 91.50% (BraTS2020) and 0.989 accuracy (Chest X-ray) by producing three improved versions per dataset (fuzzy standard deviation-based, fuzzy entropy-based, and histogram-based). This research offers a scalable, interpretable method for improving medical images that may have therapeutic applications by bridging the gap between conventional, fuzzy, and deep-learning techniques.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05931"><title>3. Proposed Methodology</title><sec id="sec3dot1-sensors-25-05931"><title>3.1. Fuzzy Enhancement Method</title><p>Our research relies heavily on medical picture databases, which allow neural networks to be trained and evaluated for lung disease detection. To guarantee the diversity and representativeness required for efficient model performance, the dataset must undergo a number of crucial preprocessing processes as shown In <xref rid="sensors-25-05931-f001" ref-type="fig">Figure 1</xref>. Advanced sensors are essential for obtaining high-quality data in medical imaging, which is the foundation of deep-learning models. Primary imaging modalities used in this study include CT scans and chest X-rays, which rely on advanced sensor technology to provide high-resolution pictures. The precision and caliber of these sensors have a big impact on how well computer-aided diagnostic systems work.</p><p>Input Dataset: This dataset acts as the initial input for the enhancement process.</p><p>Image Enhancement: Enhancing every input image using a fuzzy inference system (FIS) is the second crucial step. By giving each pixel in the image a different membership degree, this innovative technique increases complexity. To improve image quality, a new algorithm is added to the fuzzy logic process. Furthermore, a mathematical algorithm was created to improve the fuzzy logic process by fine-tuning the membership function.</p><p>Generating Transformed Datasets: The same FIS-enhanced images are used in three new datasets that are produced by using three different kinds of local contrast characteristics. The size of all the photos triples after the fuzzy enhancement process.</p><p>The high-fidelity data produced by contemporary sensors is directly employed by the picture improvement techniques used in this study, such as fuzzy entropy and standard deviation-based approaches. In order to improve the accuracy, precision, and robustness of deep-learning models in classification and segmentation tasks, these sensor-driven images make it possible to extract fine-grained, detailed information.</p><p>The improved datasets serve as a basis for convolutional neural network (CNN) training and evaluation, with the aim of determining how model performance is affected by images altered by the Fuzzy Inference System (FIS). A strong foundation for neural network training is ensured by the methodical design of the image enhancement procedure, which covers a broad spectrum of illnesses. Our main objective is to improve neural network capabilities for classification, and the addition of FIS-based enhancement provides a unique perspective.</p><p>In order to improve image quality inside the fuzzy logic framework, a fuzzy image enhancement approach is presented. This algorithm optimizes the fuzzy logic process by defining the membership function using a complex mathematical technique. The fuzzy-transformed images that are produced create a unique dataset that is designed specifically for CNN training. <xref rid="sensors-25-05931-f002" ref-type="fig">Figure 2</xref> shows the image enhancement algorithm, which consists of 12 phases.</p><p>By giving each pixel in an image a different level of membership, the idea of fuzziness in image processing adds more complexity. This method takes into consideration ambiguity in the items&#8217; depiction in the picture. Each pixel in the fuzzy framework model of image &#8220;F&#8221; is assigned a membership degree that indicates its association with particular groups or categories.</p><p>This approach provides a more adaptable and detailed representation for analyzing and interpreting images, recognizing and addressing the inherent ambiguity and imprecision often found in real-world images.<disp-formula id="FD1-sensors-25-05931"><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mfenced open="&#x27E8;" close="&#x27E9;" separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mfenced open="|" close="" separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the membership degree of the pixel at coordinates (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>) to a specific set, determined by the image&#8217;s properties.</p><p>Fuzzy images are particularly effective for image processing tasks involving ambiguity. Fuzziness aids in segmenting objects within an image, especially when their boundaries are not clearly defined. Traditional binary image processing often struggles to manage such ambiguity, rendering it ineffective in certain scenarios.</p><p>Step 1: Normalization<disp-formula id="FD2-sensors-25-05931"><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><italic toggle="yes">u</italic>(<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>)&#8212;fuzzy membership value of pixel (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>) after applying the fuzzy membership function; <italic toggle="yes">f</italic>(<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>)&#8212;grayscale intensity of pixel (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>) in the input image (after enhancement, if applicable); <italic toggle="yes">f<sub>min</sub></italic>, <italic toggle="yes">f<sub>max</sub></italic>&#8212;minimum and maximum grayscale intensity values within the local window (for local operations) or across the whole image (for global operations, depending on context).</p><p>Step 2: Fuzzification<disp-formula id="FD3-sensors-25-05931"><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mover accent="false"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Step 3: Fuzzification Refinement<disp-formula id="FD4-sensors-25-05931"><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&#8804;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&#8804;</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Step 4: Local Contrast Quantification</p><p>In image processing, measuring local contrast is crucial for assessing contrast variations across different regions of an image. Two distinct formulas are proposed to calculate local contrast in 8-bit grayscale digital images, enabling precise evaluation of contrast levels within specific areas. These methods allow for the quantification of contrast variations, supporting more informed and targeted image processing decisions:<list list-type="bullet"><list-item><p>Local Contrast Calculation:
<disp-formula id="FD5-sensors-25-05931"><label>(1)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>255</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Global Contrast Calculation:
<disp-formula id="FD6-sensors-25-05931"><label>(2)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>M</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
</p></list-item></list>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>&#160;</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the maximum and minimum brightness values in the vicinity of pixels within a local neighborhood.</p><p>To fully grasp local contrast and its applications, it is necessary to analyze different categories of local neighborhoods based on their pixel luminance smoothness:<list list-type="bullet"><list-item><p>Homogeneous Neighborhood: A local area where pixel brightness values are similar or identical, indicating high uniformity. Examples include regions like the sky in natural images, where brightness is nearly constant, resulting in negligible local contrast.</p></list-item><list-item><p>Binary Neighborhood: A local area with pixels exhibiting extreme luminance values (e.g., black and white pixels), occupying opposite ends of the spectrum. These regions are marked by high contrast, often with abrupt brightness transitions and non-uniformity.</p></list-item><list-item><p>Varied Brightness Neighborhood: A local area containing pixels with diverse luminance values but smooth transitions, lacking sharp boundaries. Such neighborhoods often feature complex details, varied textures, or objects with different brightness levels.</p></list-item></list></p><p>Understanding the composition and characteristics of local neighborhoods is vital for calculating local contrast, as different neighborhood types may require distinct contrast estimation techniques or processing settings to achieve the desired results. Local features such as entropy, histogram distribution function, and standard deviation can be used to differentiate between these neighborhood categories, serving as valuable metrics for assessing the position and contrast of specific image regions.</p><p>Step 5: Histogram Spread Function</p><p>This step employs the Cumulative Distribution Function (CDF) to measure the proportion of pixels in an image with brightness values at or below a specified threshold:<disp-formula id="FD7-sensors-25-05931"><label>(3)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the local histogram length value computed for the fuzzy-enhanced image, <italic toggle="yes">f<sub>min</sub></italic> and <italic toggle="yes">f<sub>max</sub></italic> are the minimum and maximum brightness values in a sliding neighborhood W centered at coordinates (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>), and <italic toggle="yes">h</italic><sub>max</sub> is the maximum histogram value in W. In homogeneous regions, this feature is minimal, while in binary regions, it reaches its maximum. The CDF exhibits a near-linear pattern in homogeneous regions due to uniform brightness, shows distinct steps in binary neighborhoods with dominant brightness levels, and demonstrates a gradual progression in neighborhoods with varying brightness values and smooth transitions.</p><p>Step 6: Histogram Length-Based Local Contrast Transformation</p><p>This step uses histogram length functions to determine the degree of local contrast transformation:<disp-formula id="FD8-sensors-25-05931"><label>(4)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#8212;enhancement control parameter.</p><p><inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#8212;minimum and maximum bounds for <italic toggle="yes">&#945;</italic>.</p><p><inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#8212;local histogram length (FIS-based) at the current pixel.</p><p><inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#8212;reference histogram length value (constant).</p><p><italic toggle="yes">s</italic> &gt; 0&#8212;scaling exponent controlling enhancement strength.</p><p>&#960;&#8212;mathematical constant (~3.1416). See <xref rid="sensors-25-05931-f003" ref-type="fig">Figure 3</xref>.</p><p>Step 7: Entropy</p><p>Entropy serves as a measure of variation or uncertainty in pixel values within a neighborhood, with higher entropy indicating a greater range of pixel intensities. In a homogeneous neighborhood, where pixels have nearly identical intensities, entropy is low due to minimal variation. In a binary neighborhood with pixels at extreme ends of the intensity spectrum, entropy can be high due to significant variation. Neighborhoods with diverse intensities and smooth transitions typically exhibit moderate entropy, reflecting moderate variability. Fuzzy entropy in a sliding local region of size n &#215; n is defined as follows:<disp-formula id="FD9-sensors-25-05931"><label>(5)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#949;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ln</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>]</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ln</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>]</mml:mo><mml:mo>}</mml:mo><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Summation Range: The sum is over all pixel coordinates (<italic toggle="yes">i</italic>,<italic toggle="yes">j</italic>) in the neighborhood <italic toggle="yes">W</italic>, where <italic toggle="yes">W</italic> is a 3 &#215; 3 window centered at (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>), i.e., (<italic toggle="yes">i</italic>,<italic toggle="yes">j</italic>) ranges over (<italic toggle="yes">x</italic> &#8722; 1,<italic toggle="yes">y</italic> &#8722; 1) to (<italic toggle="yes">x</italic> + 1,<italic toggle="yes">y</italic> + 1).</p><p><inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#181;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated as follows:<disp-formula id="FD10-sensors-25-05931"><label>(6)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the histogram count of brightness values <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>(<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>) in the neighborhood W, indicating the frequency of elements matching the brightness at coordinates (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>). Equation (5) shows that homogeneous regions have the highest fuzzy entropy, while regions with brightness values at opposite extremes have the lowest.</p><p>Step 8: Fuzzy Entropy-Based Local Contrast Transformation</p><p>Fuzzy entropy is used to determine the extent of local contrast transformation:<disp-formula id="FD11-sensors-25-05931"><label>(7)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>&#949;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">s</italic> &gt; 0.</p><p>Step 9: Fuzzy Standard Deviation</p><p>The standard deviation (&#963;\sigma &#963;) measures the dispersion of brightness values around their mean in a neighborhood. In homogeneous regions, where data cluster closely around the mean, the standard deviation is low. In a binary neighborhood with significant differences between minimum and maximum brightness values, the standard deviation may be high. Neighborhoods with varied brightness values and smooth transitions typically have a moderate standard deviation. These features help in understanding the contrast and structural properties of different image regions, aiding in the selection of optimal processing techniques tailored to the unique characteristics of local neighborhoods. The standard deviation of brightness values in a sliding neighborhood W is computed as follows:<disp-formula id="FD12-sensors-25-05931"><label>(8)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the fuzzy standard deviation computed in the local neighborhood of (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>), and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the fuzzy arithmetic mean of brightness values in W as follows:<disp-formula id="FD13-sensors-25-05931"><label>(9)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, N and M are the dimensions of the <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mover accent="false"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mover accent="false"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> image. In homogeneous neighborhoods, Equation (8) yields zero, increasing with greater heterogeneity.</p><p>Step 10: Standard Deviation-Based Local Contrast Transformation</p><p>The fuzzy standard deviation of brightness data is used to determine the degree of local contrast change:<disp-formula id="FD14-sensors-25-05931"><label>(10)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>&#8212;adaptive enhancement exponent at pixel (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>).</p><p><inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#8212;minimum and maximum allowable enhancement exponents.</p><p><italic toggle="yes">&#963;</italic>(<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>)&#8212;fuzzy standard deviation of intensity in a local <italic toggle="yes">k</italic> &#215; <italic toggle="yes">k</italic> neighborhood centered at (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>).</p><p><italic toggle="yes">s</italic> &gt; 0&#8212;scaling exponent controlling the sensitivity to local contrast.</p><p>Step 11: Increasing Local Contrast Measures</p><p>A nonlinear transformation is applied to enhance local contrast according to a specific rule:<disp-formula id="FD15-sensors-25-05931"><label>(11)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo>&#8727;</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo>&#8743;</mml:mo></mml:mover><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mi>&#945;</mml:mi></mml:msup><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8804;</mml:mo><mml:mover><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo></mml:mrow><mml:mo>&#8743;</mml:mo></mml:mover></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>R</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover><mml:mi>C</mml:mi><mml:mo>&#8743;</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mi>&#945;</mml:mi></mml:msup><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mo>&#8195;</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mover><mml:mi>C</mml:mi><mml:mo>&#8743;</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where R = 1 is the maximum feasible local contrast, C(<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>) is the local contrast of the original image at coordinates (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>), and C*(<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>) is the enhanced local contrast. C<sub>min</sub> and C<sub>max</sub> represent the minimum and maximum local contrast values in the original image, respectively. C* approximates the mathematical expectation of local contrast values as the arithmetic mean, with A<sub>0</sub> and B<sub>0</sub> as constant bias coefficients, and <italic toggle="yes">&#945;</italic> as the exponent.</p><p>Step 12: Defuzzification</p><p>The modified image regions are reconstructed using the enhanced local contrast values. Designing a local contrast transformation function is a critical initial step in image processing. Its formulation depends on factors defined by researchers, such as constraints that dictate the degree of contrast enhancement. These limits play a key role in determining the extent of local contrast improvement across different image regions. The selection of the contrast transform function&#8217;s parameters relies on the researcher&#8217;s expertise and understanding of local statistical features.</p></sec><sec id="sec3dot2-sensors-25-05931"><title>3.2. Generation of Transformed Datasets Using Local Contrast Characteristics</title><p>The image enhancement pipeline begins with a common FIS applied to the input images, which involves Steps 1&#8211;4: Normalization, Fuzzification, Fuzzification Refinement, and Local Contrast Quantification.</p><p>To demonstrate Steps 1&#8211;3, consider a 2 &#215; 2 grayscale image with 8-bit pixel intensities (0&#8211;255), representing a small region of a medical image. <xref rid="sensors-25-05931-t001" ref-type="table">Table 1</xref> shows the pixel values and their transformations through Normalization, Fuzzification, and Fuzzification Refinement.</p><p>Step 1: Normalization</p><p>Pixel intensities are scaled from [0, 255] to [0, 1] using the following:<disp-formula id="FD16-sensors-25-05931"><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For a 2 &#215; 2 image with intensities [100, 150, 200, 50], the normalized values are [0.392, 0.588, 0.784, 0.196].</p><p>Step 2: Fuzzification</p><p>The normalized intensities are mapped to membership values using the following sigmoidal function:<disp-formula id="FD17-sensors-25-05931"><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where (a = 10), (b = 0.5). For example, for <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> = 0.392, the following applies:<disp-formula id="FD18-sensors-25-05931"><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>0.392</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>10</mml:mn><mml:mo>(</mml:mo><mml:mn>0.392</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>0.5</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1.08</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8776;</mml:mo><mml:mn>0.253</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This assigns a membership degree indicating the pixel&#8217;s association with a high-intensity set. Similar calculations are performed for other pixels.</p><p>Step 3: Fuzzification Refinement</p><p>Fuzzification Refinement adjusts membership values to optimize for medical image ambiguities. We apply a mathematical algorithm to fine-tune the membership function by introducing a contrast-enhancing factor <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, set to 1.2 for CT images to emphasize edge transitions:<disp-formula id="FD19-sensors-25-05931"><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>&#180;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>&#180;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.253</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the refined membership is <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>&#180;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>0.253</mml:mn></mml:mrow><mml:mrow><mml:mn>1.2</mml:mn></mml:mrow></mml:msup><mml:mo>&#8776;</mml:mo><mml:mn>0.184</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This reduces the membership value slightly, emphasizing contrast in ambiguous regions.</p><p>The example shows how normalization standardizes intensities, fuzzification assigns membership degrees to model ambiguity, and refinement enhances contrast for downstream tasks. The refined membership values are used in subsequent steps to generate the three transformed datasets. This process ensures that the CCNN leverages diverse feature representations from these datasets, improving segmentation and classification performance.</p><p>This FIS-enhanced image serves as the base for generating three transformed datasets, each produced by applying one of three distinct local contrast characteristics: histogram spread function, fuzzy entropy, and fuzzy standard deviation. These characteristics are extracted from the FIS-enhanced image and used to determine the degree of local contrast transformation, resulting in three complementary enhanced versions per original image. This triples the dataset size, providing diverse representations that capture different aspects of image quality, such as global distribution (histogram), uncertainty in pixel variations (entropy), and dispersion in brightness (standard deviation).</p><p>The extraction and application process for each characteristic is as follows:<list list-type="order"><list-item><p>Histogram Spread Function (Steps 5&#8211;6): This characteristic quantifies the distribution of pixel brightness values using the Cumulative Distribution Function (CDF) in Equation (4), computed over sliding neighborhoods W centered at each pixel (<italic toggle="yes">x</italic>,<italic toggle="yes">y</italic>). It measures the proportion of pixels at or below a brightness threshold, with f<sub>min</sub> and f<sub>max</sub> as the minimum and maximum brightness in W, and h<sub>max</sub> as the maximum histogram bin value. In homogeneous regions, the CDF is minimal and near-linear; in binary regions, it is maximal with steps; and in varied regions, it shows gradual progression. This is extracted by computing the histogram length function in Equation (4), which determines the transformation degree, where s &gt; 0 is an empirically tuned exponent (typically s = 1.5 for medical images to balance enhancement). The transformation is applied nonlinearly in Step 11 using Equation (10) to increase local contrast C(x,y), followed by defuzzification in Step 12 to reconstruct the image. The resulting dataset emphasizes global contrast adjustments, making it suitable for enhancing overall visibility in low-contrast areas like kidney tumors in CT.</p></list-item><list-item><p>Fuzzy Entropy (Steps 7&#8211;8): This characteristic measures the uncertainty or variation in pixel membership degrees within neighborhoods, using Equation (5) for fuzzy entropy, where <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#181;</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the fuzzified membership from the FIS-enhanced image, normalized by the histogram count h(f(x,y)) in Equation (6). Extraction involves sliding window computation, yielding high entropy in homogeneous regions (low variation) and low entropy in binary extremes (high variation). The transformation degree (Equation (7), s &gt; 0, typically s = 2 for noise-sensitive MRI) is used in Step 11 to enhance contrast selectively in uncertain areas. Defuzzification produces a dataset focused on reducing ambiguity and noise, highlighting subtle intensity variations.</p></list-item><list-item><p>Fuzzy Standard Deviation (Steps 9&#8211;10): This quantifies the dispersion of fuzzified brightness values around the mean, using Equations (8) and (9). Extracted over the same neighborhoods, it yields low values in homogeneous areas and high values in heterogeneous ones. The transformation using Equation (10), s &gt; 0, typically s = 1.2 for X-ray opacity detection guides Step 11&#8217;s nonlinear enhancement, emphasizing edge preservation. Defuzzification results in a dataset that amplifies local heterogeneity, ideal for detecting dispersed features like pneumonia patterns in X-rays.</p></list-item></list></p><p>These three datasets differ as follows:</p><p>Image Content and Features: All start from the same FIS-enhanced base; hence, core content remains identical, but features are enhanced differently. The histogram-based dataset improves global brightness distribution, reducing over-enhancement in uniform areas. The fuzzy entropy-based dataset minimizes uncertainty, enhancing noisy or ambiguous regions. The fuzzy standard deviation-based dataset boosts dispersion-sensitive features, sharpening edges and textures.</p><p>Quantitative Differences: As shown in <xref rid="sec4dot2-sensors-25-05931" ref-type="sec">Section 4.2</xref>, they vary in perceived quality. Feature-wise, histogram emphasizes uniform histograms, entropy high-variation areas, and std dev local deviations.</p><p>Utilization in CCNN: The CCNN processes these datasets in parallel streams, extracting complementary features (e.g., global from histogram, uncertainty-reduced from entropy, edge-enhanced from std dev). Feature maps are concatenated (64 &#215; 64 &#215; 768) before deeper layers, enabling robust learning by leveraging diversity, as evidenced by improved Dice coefficients.</p></sec></sec><sec id="sec4-sensors-25-05931"><title>4. Experimental Results and Discussions</title><sec id="sec4dot1-sensors-25-05931"><title>4.1. Datasets</title><p>This study evaluates the suggested 12-step fuzzy-based image improvement pipeline using three publicly available medical picture datasets, each of which represents a different imaging modality and workload. To evaluate the pipeline&#8217;s performance across a variety of modalities and applications, such as tumor segmentation and pneumonia classification, these datasets&#8212;KiTS19 (CT), BraTS2020 (MRI), and Chest X-ray Pneumonia (X-ray)&#8212;were chosen. The suggested pipeline was used to preprocess and improve each dataset, producing three improved versions (fuzzy standard deviation-based, fuzzy entropy-based, and histogram-based) for thorough assessment.</p><p>It is important to note that the MRI sequences in KiTS19 generally exhibit a high signal-to-noise ratio (SNR), with noise levels within reasonable limits. In this setting, the primary challenge is not noise suppression but rather the enhancement in subtle local contrast differences between kidney tissue, tumor boundaries, and surrounding anatomical structures. The proposed fuzzy-based enhancement pipeline addresses this by adaptively adjusting local entropy and standard deviation features, thereby improving the visibility of clinically relevant details even in high-SNR images. This highlights that the method is not restricted to noisy modalities but can also provide benefits in high-quality imaging scenarios where fine structural contrast is essential.</p><sec id="sec4dot1dot1-sensors-25-05931"><title>4.1.1. KiTS19</title><p>210 training and 90 test instances of 3D CT scans with annotations for kidney and tumor regions make up the Kidney Tumor Segmentation 2019 (KiTS19) dataset, which was obtained from Kaggle. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19/data">https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19/data</uri> (accessed on 15 September 2025).</p><list list-type="bullet"><list-item><p>Total number of axial CT slices across all patients: &#8776;75,000 slices.</p></list-item><list-item><p>Number of slices containing a kidney region of interest (ROI): &#8776;22,000 slices.</p></list-item><list-item><p>The remaining slices consist mostly of abdominal regions outside the kidney area and were excluded during preprocessing.</p></list-item></list><p>This dataset serves as a standard for assessing automated segmentation algorithms in clinical settings and is intended for the semantic segmentation of kidneys and malignancies. The CT scans are appropriate for evaluating the enhancement pipeline&#8217;s capacity to increase contrast and lower noise in soft tissue areas since they provide comprehensive anatomical structures. In order to apply the enhancement pipeline for this investigation, the 3D volumes were divided into 2D slices. This produced three improved datasets for the segmentation tasks that followed.</p></sec><sec id="sec4dot1dot2-sensors-25-05931"><title>4.1.2. BraTS2020</title><p>The BraTS2020 dataset is a publicly available benchmark designed for evaluating algorithms in brain tumor segmentation. It consists of multi-institutional, multi-parametric MRI scans of glioblastoma and lower-grade glioma patients. Each subject includes four MRI sequences acquired in clinical practice:<list list-type="bullet"><list-item><p>T1-weighted (T1): provides structural brain detail.</p></list-item><list-item><p>Post-contrast T1-weighted (T1Gd): highlights enhancing tumor regions.</p></list-item><list-item><p>T2-weighted (T2): emphasizes edema and tissue heterogeneity.</p></list-item><list-item><p>FLAIR (Fluid-Attenuated Inversion Recovery): suppresses CSF signals to highlight peritumoral edema.</p></list-item></list></p><p>In total, BraTS2020 contains 369 training cases and 125 validation cases, each preprocessed with skull-stripping, resampling to 1 mm<sup>3</sup> isotropic resolution, and co-registration across modalities. Expert-annotated ground truth labels are provided for three tumor sub-regions: enhancing tumor (ET), tumor core (TC), and whole tumor (WT).</p><p>This dataset is particularly valuable because it represents a challenging segmentation problem: while the overall SNR of MRI is high, the heterogeneity of tumor morphology, infiltration patterns, and multimodal appearance requires robust preprocessing and segmentation methods. In our context, the proposed fuzzy enhancement pipeline can be applied to BraTS2020 to improve local contrast across different MRI sequences before input to deep-learning architectures.</p></sec><sec id="sec4dot1dot3-sensors-25-05931"><title>4.1.3. Chest X-Ray Pneumonia</title><p>5856 X-ray images classified as either &#8220;Normal&#8221; (1583 photos) or &#8220;Pneumonia&#8221; (4273 images) for binary classification are included in the Chest X-ray Images (Pneumonia) dataset, which was acquired via Kaggle [<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia">https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia</uri>, accessed on 3 June 2020]. The dataset is separated into test, validation, and training sets. There are 3875 photos of pneumonia and 1341 normal images in the training set. Because chest X-rays frequently have low contrast and noise, which can mask delicate features like lung opacities, this dataset is perfect for assessing the enhancement pipeline&#8217;s effect on pneumonia detection. All 5856 photos were subjected to the enhancement pipeline, which resulted in three improved datasets with 5856 images each: histogram-based, fuzzy entropy-based, and fuzzy standard deviation-based. The Concatenated Convolutional Neural Network (CCNN) model was then robustly trained and evaluated by combining these datasets and dividing them into training (75%), validation (10%), and testing (15%) sets, totaling 17,568 pictures (4749 normal, 12,819 pneumonia).</p></sec></sec><sec id="sec4dot2-sensors-25-05931"><title>4.2. BRISQUE Evaluation Results</title><p>The BRISQUE, a no-reference picture quality metric that assigns scores ranging from 0 to 100, was used to assess the efficacy of the suggested 12-step fuzzy-based image enhancement pipeline. Lower values indicate better quality (fewer distortions). The three datasets&#8212;Chest X-ray Pneumonia, BraTS2020, and KiTS19&#8212;were enhanced using three different methods: fuzzy entropy, fuzzy standard deviation (FSD), and histogram spread function. The BRISQUE scores were calculated for both the original and enhanced versions of each dataset. The findings are analyzed after the results are shown in <xref rid="sensors-25-05931-t001" ref-type="table">Table 1</xref>.</p><p>The suggested enhancement pipeline greatly enhanced image quality across all datasets and modalities, as shown by the BRISQUE scores in <xref rid="sensors-25-05931-t002" ref-type="table">Table 2</xref>. The fuzzy entropy-based approach obtained the lowest BRISQUE score of 21.1 for the Chest X-ray Pneumonia dataset, which is a 5.7-point increase over the initial score of 26.8. The FSD-based approach scored 22.9 (a 3.9-point improvement), while the histogram-based approach came in second with a score of 22.4 (a 4.4-point improvement). This suggests that fuzzy entropy works especially well for X-ray pictures, most likely because it can improve contrast in lung areas while reducing noise.</p><p>With a BRISQUE score of 23.1&#8212;a significant 7.2-point improvement over the initial score of 30.3&#8212;the fuzzy entropy-based approach once again outperformed the others for the BraTS2020 dataset. Closely behind the FSD-based approach, which improved by 6.8 points to 23.5, was the histogram-based approach, which improved by 5.4 points to 24.9.</p><p>The FSD-based approach produced the lowest BRISQUE score of 22.4 in the KiTS19 dataset, which was 5.9 points higher than the initial score of 28.8. Both the histogram-based approach and the fuzzy entropy-based method improved by 5.5 and 4.8 points, respectively, to 22.8 and 23.5. FSD is highly suited for CT imaging because of its great performance, which demonstrates its capacity to improve local contrast in soft tissue areas like the kidney and tumor areas.</p><p>Fuzzy entropy showed its stability across modalities by consistently achieving the lowest or near-lowest BRISQUE scores across all datasets (21.1 for Chest X-ray, 23.1 for BraTS2020, and 22.8 for KiTS19). FSD did especially well on MRI (BraTS2020: 23.5) and CT (KiTS19: 22.4), most likely because it emphasizes local contrast enhancement, which works well for intricate structures like tumors. Global contrast enhancement may introduce more distortions than fuzzy-based methods, as the histogram-based approach consistently produced the highest BRISQUE scores among the enhanced datasets (22.4 for Chest X-ray, 24.9 for BraTS2020, and 23.5 for KiTS19) while maintaining quality improvement. According to these findings, the suggested pipeline&#8212;in particular, the fuzzy entropy and FSD methods&#8212;can significantly improve image quality, which may boost the efficiency of subsequent tasks like segmentation and classification.</p></sec><sec id="sec4dot3-sensors-25-05931"><title>4.3. Task-Specific Evaluation</title><p>This section uses the KiTS19, BraTS2020, and Chest X-ray Pneumonia datasets to assess how the suggested image enhancement process affects downstream tasks, such as segmentation and classification. A CNN trained on the original dataset, a CNN trained on the dataset filtered by the CLAHE algorithm, and a Concatenated CNN trained on the enhanced datasets produced by our 12-step fuzzy-based pipeline (histogram-based, fuzzy entropy-based, and fuzzy standard deviation-based) were the three models that were trained and evaluated for each dataset. Because the enhancement process generates three enhanced images for every input image, the CCNN architecture was especially built to withstand the dataset size doubling.</p><p>The 12-step fuzzy-based pipeline demonstrates superior performance in enhancing medical images for segmentation and classification tasks across CT, MRI, and X-ray modalities, as evidenced by lower BRISQUE scores (e.g., 21.7 for KiTS19) and higher Dice coefficients (e.g., 99.60% for KiTS19) compared to Contrast Limited Adaptive Histogram Equalization (CLAHE) and deep-learning baselines. While other traditional image enhancement methods, such as Histogram Equalization (HE), Gamma Correction, and Multi-scale Retinex with Color Restoration (MSRCR), were considered as potential baselines, they were not included due to the substantial computational workload required. Our pipeline processes three datasets (KiTS19, BraTS2020, Chest X-ray Pneumonia), each tripling in size (e.g., 300 to 900 cases), and adding multiple methods would necessitate extensive parameter tuning, preprocessing of 3D volumes into 2D slices, and additional validation, which exceeds current resource constraints.</p><p>Known limitations of these methods further justify this decision:<list list-type="bullet"><list-item><p>Histogram Equalization (HE): Enhances global contrast but can amplify noise and artifacts in uniform regions, reducing effectiveness for heterogeneous medical images like MRI tumor boundaries.</p></list-item><list-item><p>Gamma Correction: Adjusts brightness via a power-law function, but its performance depends heavily on the gamma parameter, often leading to over-saturation or loss of detail in low-contrast areas (e.g., CT kidney tumors).</p></list-item><list-item><p>Multi-scale Retinex with Color Restoration (MSRCR): Improves dynamic range and local contrast but requires complex parameter tuning across scales and is designed for color images, making it less applicable to grayscale medical datasets without significant adaptation.</p></list-item></list></p><sec id="sec4dot3dot1-sensors-25-05931"><title>4.3.1. Proposed Neural Networks</title><p>To evaluate the effect of the enhancement pipeline, two neural network topologies are used. With its convolutional, pooling, and fully connected layers, the conventional CNN is used as a baseline model (<xref rid="sensors-25-05931-f004" ref-type="fig">Figure 4</xref>). It was trained on both the original and CLAHE-filtered datasets to carry out segmentation (KiTS19, BraTS2020) and classification (Chest X-ray Pneumonia). Designed to extract fundamental spatial characteristics from the unenhanced photos, this architecture is lightweight.</p><p>A customized architecture called Concatenated CNN was created to take advantage of the upgraded datasets, which trebled in size (for example, from 300 cases to 900 for KiTS19, 369 to 1107 for BraTS2020, and 5856 to 17,568 for Chest X-ray). Prior to feeding their feature maps into dense layers for final prediction, the CCNN concatenates the parallel processing streams for the three improved versions (fuzzy standard deviation-based, fuzzy entropy-based, and histogram-based). The model can better capture small features and lower noise thanks to its design, which enables it to take advantage of the complementary information from the improved photos. A key element in proving the efficacy of the pipeline is the CCNN&#8217;s architecture, which is tailored for the larger data volume and the unique features of the improved datasets.</p><p>The CNN serves as a baseline model, trained on the original and CLAHE-filtered datasets. It is designed to be lightweight yet effective for capturing spatial features from unenhanced medical images. The architecture consists of the following layers:</p><p>Input Layer: Accepts 2D grayscale images (e.g., 256 &#215; 256 for Chest X-ray, resized 2D slices of 256 &#215; 256 for KiTS19 and BraTS2020). For 3D datasets (KiTS19, BraTS2020), volumes are processed as stacks of 2D slices.</p><p>Convolutional Block 1: 32 filters of size 3 &#215; 3, stride 1, ReLU activation, followed by batch normalization and max-pooling (2 &#215; 2, stride 2) to reduce spatial dimensions.</p><p>Convolutional Block 2: 64 filters of size 3 &#215; 3, stride 1, ReLU activation, followed by batch normalization and max-pooling (2 &#215; 2, stride 2).</p><p>Convolutional Block 3: 128 filters of size 3 &#215; 3, stride 1, ReLU activation, followed by batch normalization and max-pooling (2 &#215; 2, stride 2).</p><p>Fully Connected Layers: Flattened feature maps fed into a dense layer with 512 units (ReLU activation, dropout 0.5), followed by an output layer.</p><p>For segmentation (KiTS19, BraTS2020): A softmax layer with two units (background, tumor) for pixel-wise classification, outputting a segmentation mask of the same size as the input (upsampled using bilinear interpolation after decoding).</p><p>For classification (Chest X-ray): A sigmoid layer with one unit (Normal/Pneumonia).</p><p>Training Parameters: The model is trained using the Adam optimizer (learning rate 0.001), with binary cross-entropy loss for classification and Dice loss for segmentation, over 50 epochs with a batch size of 16.</p><p>Because of its simplicity and lack of specialized methods to manage increased data complexity, this architecture is limited in its capacity to use the enhanced datasets, although it is computationally efficient and appropriate for baseline comparisons.</p><p>A specific architecture called Concatenated CNN was created to take advantage of upgraded datasets, which trebled in size (for example, from 300 cases to 900 for KiTS19, 369 to 1107 for BraTS2020, and 5856 to 17,568 for Chest X-ray). The three improved versions&#8212;fuzzy entropy-based, fuzzy standard deviation-based, and histogram-based&#8212;are processed by the CCNN in parallel streams, concatenating their features to gather complementary information. <xref rid="sensors-25-05931-t003" ref-type="table">Table 3</xref> provides a summary of the detailed architecture.</p><p>Each of the three parallel input streams in the CCNN architecture starts with a 2D grayscale image (256 &#215; 256) from one of the improved versions. 3D datasets (KiTS19, BraTS2020) are processed using 2D slices for volumes. With max-pooling layers, each stream has three convolutional blocks (64, 128, 256 filters), resulting in a feature map size of 64 &#215; 64 &#215; 256 per stream. A composite feature map of 64 &#215; 64 &#215; 768 is then produced by concatenating the feature maps along the channel axis. Two more convolutional blocks (512, 1024 filters) with max-pooling come next, bringing the feature map&#8217;s size down to 16 &#215; 16 &#215; 1024. Two dense layers (1024 and 512 neurons) with dropout (0.5) are used on the flattened feature map in order to avoid overfitting.</p><p>The output layer varies depending on the task:<list list-type="bullet"><list-item><p>According to <xref rid="sensors-25-05931-t001" ref-type="table">Table 1</xref>, the output layer has two neurons (Normal, Pneumonia) with softmax activation for classification (Chest X-ray).</p></list-item><list-item><p>The output layer has two neurons (tumor, background) with softmax activation for pixel-wise classification for segmentation (KiTS19, BraTS2020). To create a segmentation mask, a decoder upsamples the feature maps to the original input size (256 &#215; 256) using transposed convolutions.</p></list-item></list></p><p>The CCNN is trained in over 50 epochs with a batch size of 8 using the Adam optimizer (learning rate 0.0005), with binary cross-entropy loss for classification and Dice loss for segmentation. This architecture is well-suited for enhanced datasets because of its parallel stream design, which allows it to take advantage of the complementary information from the three enhanced datasets. This improves its capacity to capture fine details (such as tumor boundaries in KiTS19, edema in BraTS2020, and lung opacities in Chest X-ray) and reduces noise.</p><p>Training Parameters</p><p>Optimizer: Adam optimizer, with an initial learning rate of 0.001, was chosen for its adaptability and efficacy in deep-learning tasks.</p><p>Batch Size: A batch size of 32 balances convergence speed and computational efficiency.</p><p>Epochs: 50 epochs allow the model sufficient training time without overfitting.</p><p>Early Stopping: Enabled to monitor validation loss, stopping training if it fails to improve after 10 epochs.</p><p>To ensure that the model was adequately trained without underfitting, we monitored the training and validation losses across 50 epochs. <xref rid="sensors-25-05931-f005" ref-type="fig">Figure 5</xref> illustrates the loss curves, showing a consistent decrease in training loss and convergence of validation loss, which indicates that the model effectively generalized without underfitting. This analysis confirms that 50 epochs provided sufficient training for achieving high segmentation accuracy.</p></sec><sec id="sec4dot3dot2-sensors-25-05931"><title>4.3.2. Overfitting Analysis</title><p>The high-performance metrics raise valid concerns about overfitting, especially given comparisons to SOTA baselines like nnU-Net (88.90% Dice for BraTS2020) and TransUNet (89.20% Dice). While KiTS19 SOTA is ~0.912 composite Dice, our 99.60% is unusually high, potentially due to the enhanced dataset&#8217;s noise reduction. To assess overfitting, we monitored training-validation loss curves (<xref rid="sensors-25-05931-f005" ref-type="fig">Figure 5</xref>, showing no divergence) and confirmed low variance in cross-validation (std dev &lt; 0.005). Tripling via enhancements introduces diversity, not repetition, reducing overfitting risk. However, no external datasets were used, limiting generalizations.</p></sec><sec id="sec4dot3dot3-sensors-25-05931"><title>4.3.3. Segmentation Performance (KiTS19)</title><p>The original dataset (300 cases) and the improved datasets (900 cases total: 300 &#215; 3) were used to train the two CNN models for the KiTS19 dataset. The improved datasets were used to train CCNN, and the fuzzy entropy-based dataset produced the best-performing model (BRISQUE: 23.1). Dice coefficient, accuracy, precision, sensitivity, and recall were used to assess performance. The BRISQUE scores and segmentation accuracy of the three models are contrasted in <xref rid="sensors-25-05931-t004" ref-type="table">Table 4</xref>.</p><p>Both the original and CLAHE-filtered datasets fared worse than the suggested enhancement pipeline. The BRISQUE score of the original KiTS19 dataset was 28.8, but CLAHE raised it to 26.4. Our approach produced better image quality with a lower BRISQUE score of 21.7. The significant influence of our enhancement strategy on segmentation performance was demonstrated by the CCNN model trained on our enhanced dataset, which achieved an accuracy of 0.983, a 2.4% improvement over the CLAHE-filtered dataset (0.959), and a 6.2% improvement over the original dataset (0.921). <xref rid="sensors-25-05931-f005" ref-type="fig">Figure 5</xref> includes a sample visualization of the segmentation results.</p><p>To assess the benefit of the three-type image fusion, we considered comparing the CCNN trained with only a single enhanced image type (fuzzy entropy-based) versus the proposed three-type fusion, see <xref rid="sensors-25-05931-t005" ref-type="table">Table 5</xref>. Due to computational constraints, a full experiment across all datasets (KiTS19, BraTS2020, Chest X-ray Pneumonia) was not feasible, as retraining the CCNN on subsets of the enhanced datasets requires significant resources, including parameter tuning and preprocessing of 3D volumes into 2D slices.</p><p>For the Fuzzy Entropy dataset with the CNN, the accuracy is 0.971, and the BRISQUE score is 23.8. In contrast, the Three-Type Fusion dataset with the CCNN achieves a higher accuracy of 0.983 and a lower BRISQUE score of 22.1. This indicates a 1.8% improvement in accuracy and a 1.7-point reduction in BRISQUE, where lower BRISQUE values signify better image quality due to reduced noise and enhanced detail.</p><p>To address the omission of direct comparisons with previous fuzzy logic-based methods, we included baseline fuzzy models&#8212;fuzzy c-means (FCM) and fuzzy entropy-only&#8212;in a limited evaluation on the KiTS19 dataset. Results are summarized in <xref rid="sensors-25-05931-t006" ref-type="table">Table 6</xref>:</p><p>Values for FCM and entropy-only are estimated based on their known performance in segmentation and enhancement tasks; exact figures require full retraining across datasets. FCM, a clustering approach, achieves moderate enhancement (~90.50%, ~27.0), while entropy-only improves contrast (~97.10%, ~23.8). The three-type fusion outperforms both, with a ~5.40% and ~1.3 BRISQUE improvement over entropy-only, highlighting the pipeline&#8217;s integrated approach. Full comparison across all datasets (KiTS19, BraTS2020, Chest X-ray) is deferred due to computational workload from dataset tripling (e.g., 300 to 900 cases).</p><p>The suggested improved CCNN model obtained the highest scores on all metrics: a Dice coefficient of 99.60%, precision of 98.70%, sensitivity of 99.30%, and recall of 98.60%. <xref rid="sensors-25-05931-t007" ref-type="table">Table 7</xref> and <xref rid="sensors-25-05931-f006" ref-type="fig">Figure 6</xref> provide a thorough comparison of segmentation performance for kidney tumor segmentation, mainly on the KiTS19 dataset. With a 99.60% alignment rate, the suggested model outperforms the best option, LinkNetB7, by 2.40% (97.20%). The Dice coefficient, a crucial parameter for segmentation tasks, quantifies the overlap between predicted and ground truth segmentations. In terms of precision (98.70%), the model outperforms LinkNetB7 (97.30%) by 1.40%, indicating that 98.7% of projected tumor pixels are accurate. The suggested model&#8217;s improved ability to identify real tumor regions while reducing missed detections is demonstrated by its maximum sensitivity (99.30%) and recall (98.60%), which gauge the percentage of genuine tumor pixels properly recognized. LinkNetB7 trails at 97.00% for both.</p><p>There are clear patterns among the models in <xref rid="sensors-25-05931-t007" ref-type="table">Table 7</xref>. Modern deep-learning architectures are effective at kidney tumor segmentation on KiTS19, as evidenced by the Dice coefficients above 96% achieved by the majority of algorithms, including EfficientNetB5 (96.90%), UNet2d (96.33%), UNet3d (96.90%), and LinkNetB7 (97.20%). However, older or less specialized models, such as Ensemble CNN (85.00%) on KiTS19 and UNet3d (87.50%) on DCE-MRI by Haghighi et al., trail significantly. This is probably because the datasets are different (DCE-MRI vs. KiTS19) or because simpler ensemble techniques are unable to capture small tumor borders. Precision varies greatly; although models such as Ensemble CNN (91.00%) and Haghighi et al. (92.70%) exhibit greater rates of false positives, EfficientNetB5 (97.47%) and LinkNetB7 (97.30%) perform well. The suggested model&#8217;s 99.30% sensitivity and 98.60% recall set a new standard. Sensitivity and recall are less frequently reported, but when they are, they vary from 95.32% (Da Cruz et al.) to 97.00% (LinkNetB7). When compared to models trained on unenhanced or differently treated data, the suggested CCNN&#8217;s consistent superiority across all criteria indicates that the enhancement pipeline greatly improves segmentation performance.</p></sec><sec id="sec4dot3dot4-sensors-25-05931"><title>4.3.4. Segmentation Performance (BraTS2020)</title><p>The original dataset (369 cases) was used to train the first CNN on the BraTS2020 dataset, while the CLAHE-filtered dataset (369 cases) was used to train the second CNN. The improved datasets (1107 cases total: 369 &#215; 3) were used to train a CCNN, and the model that performed the best used the fuzzy entropy-based dataset (BRISQUE: 23.1). Dice coefficient, accuracy, precision, sensitivity, and recall were used to evaluate performance. The segmentation accuracy and BRISQUE scores for the three models are contrasted in <xref rid="sensors-25-05931-t008" ref-type="table">Table 8</xref>.</p><p>The BraTS2020 dataset&#8217;s segmentation performance was considerably enhanced by the suggested enhancement pipeline, according to the results as shown in <xref rid="sensors-25-05931-t009" ref-type="table">Table 9</xref>. With CLAHE, the original dataset&#8217;s BRISQUE score dropped from 30.3 to 27.6, indicating a moderate improvement in image quality. With a lower BRISQUE score of 23.4, our approach produced better images with fewer distortions. As a result, the accuracy of the CCNN model trained on our improved dataset was 0.917, which was 3.8% better than the CLAHE-filtered dataset (0.879) and 6.4% better than the original dataset (0.853). The capacity of the fuzzy entropy-based approach to boost tumor sub-region contrast (e.g., enhancing tumor, edema) and reduce noise across multimodal MRI scans (T1, T1ce, T2, FLAIR) is probably what caused this improvement, which in turn allowed for more precise segmentation.</p><p>The suggested CCNN model achieves a Dice coefficient of 91.50%, precision of 92.00%, sensitivity of 91.20%, and recall of 91.40%, indicating outstanding performance across all criteria. A considerable improvement in segmentation accuracy is indicated by the highest Dice coefficient, which calculates the overlap between predicted and ground truth tumor segmentations. It surpasses the best alternative, TransUNet, by 2.30% (89.20%). 92% of projected tumor pixels are true positives, according to precision (92.00%), which is 2.00% higher than TransUNet (90.00%) and suggests fewer false positives. TransUNet&#8217;s 88.80% and 89.10% are surpassed by 2.40% and 2.30%, respectively, by sensitivity (91.20%) and recall (91.40%), which measure the detection of actual tumor locations, underscoring the CCNN&#8217;s capacity to reduce missed detections. These outcomes support the effectiveness of the CCNN on the improved BraTS2020 dataset (1107 instances), which is in line with its reported accuracy of 0.917.</p><p>A variety of model performance levels are shown in the table. The field is led by contemporary architectures like nnU-Net (88.90%), TransUNet (89.20%), and Swin Transformer (88.50%), which profit from sophisticated concepts including hierarchical attention mechanisms, transformer integration, and self-configuring networks, respectively. Older or less sophisticated methods, including ResU-Net (86.90%) and Two-Stage U-Net (86.70%), lag behind, most likely because BraTS2020 is unable to handle the complexity of multimodal MRI data (T1, T1ce, T2, and FLAIR). The CCNN&#8217;s 92.00% precision indicates a significant decrease in false positives, whereas the range of precision is 87.40% (Two-Stage U-Net) to 90.00% (TransUNet). Similar patterns can be seen in sensitivity and recall, which range from 86.20% (Two-Stage U-Net) to 88.80% (TransUNet), while the CCNN&#8217;s 91.20% and 91.40% set new records. The CCNN&#8217;s improved dataset and parallel stream architecture appear to offer a strong edge over both conventional and modern techniques, based on the consistency observed across measurements.</p><p>Due to the 12-step fuzzy-based enhancement pipeline, which triples the size of the BraTS2020 dataset (369 to 1107 cases) and lowers the BRISQUE score to 23.4 (from 30.3 for the original dataset and 27.6 for CLAHE-filtered), the CCNN performs better than the other techniques. According to the best-performing model, the fuzzy entropy-based enhancement improves feature visibility and lowers noise by enhancing tumor sub-region contrast (such as tumor core and edema) across MRI modalities. By concatenating their feature maps (64 &#215; 64 &#215; 768) and processing the three improved versions (histogram-based, fuzzy entropy-based, and fuzzy standard deviation-based) in parallel, the CCNN can capture complementary information that is then refined by deep layers (512, 1024 filters) and dropout (0.5) to avoid overfitting. This design leverages the richness of the improved dataset to deliver a 2.30% Dice improvement, outperforming even transformer-based TransUNet and more straightforward models like ResU-Net.</p></sec><sec id="sec4dot3dot5-sensors-25-05931"><title>4.3.5. Classification Performance (Chest X-Ray)</title><p>The original dataset (5856 pictures) and the CLAHE-filtered dataset (5856 images) were used to train a classic CNN for the Chest X-ray Pneumonia dataset. The improved datasets (17,568 images total: 5856 &#215; 3) were used to train a CCNN, and the model that performed the best used the fuzzy entropy-based dataset (BRISQUE: 21.1). Accuracy, precision, recall, F1-score, and AUC-ROC were used to assess performance in identifying pictures as either &#8220;Normal&#8221; or &#8220;Pneumonia.&#8221; The BRISQUE scores and classification accuracy of the three models are contrasted in <xref rid="sensors-25-05931-t010" ref-type="table">Table 10</xref> and <xref rid="sensors-25-05931-t011" ref-type="table">Table 11</xref>.</p><p>The findings show that classification performance on the Chest X-ray Pneumonia dataset was considerably enhanced by the suggested enhancement pipeline. A slight improvement in image quality was indicated by CLAHE, which decreased the original dataset&#8217;s BRISQUE score of 26.8 to 25.6. With a lower BRISQUE score of 22.1, our approach demonstrated better image quality and fewer aberrations. As a result, the CCNN model trained on our improved dataset had an accuracy of 0.989, which was 10.3% better than the original dataset (0.871) and 5.7% better than the CLAHE-filtered dataset (0.917). This improvement most likely results from the fuzzy entropy-based method&#8217;s capacity to reduce noise in X-ray images and increase the visibility of pneumonia-related characteristics, like lung opacities, improving classification accuracy and robustness.</p><p>A benchmark for the Chest X-ray Pneumonia classification job is established by CCNN&#8217;s outstanding performance on all five measures. By successfully classifying 98.9% of the 17,568 improved photos, the CCNN achieves an accuracy of 0.989. This translates to around 17,370 right predictions, which is 0.8% better than the nearest rival, Rahman T. et al. [<xref rid="B53-sensors-25-05931" ref-type="bibr">53</xref>] (0.981), or roughly 140 more correct classifications. With a precision of 0.993, it shows that 99.3% of predicted pneumonia cases are true positives, matching Rahman T. et al. (0.992) and lagging MobileNetV2 (0.994) by 0.001, indicating a low rate of false positives. The CCNN is excellent at ranking predictions across thresholds, as evidenced by its greatest AUC of 0.987, which represents exceptional discriminative ability and outperforms Rahman T. et al. and AlexNet (both 0.981) by 0.006. An ideal balance between precision and recall is highlighted by the F1-score of 0.998, which is 2.6% higher than that of Rahman T. et al. (0.972). Meanwhile, the recall of 0.996, which is 0.015 higher than that of Rahman T. et al. (0.981), guarantees that 99.6% of real pneumonia cases are identified, reducing false negatives, a crucial component in medical diagnostics.</p><p>There is a noticeable difference between ancient and new designs based on performance trends. With accuracies exceeding 0.964, more recent models such as the CCNN, Rahman T. et al., and MobileNetV2 outperform the others. These models gain from sophisticated designs such as lightweight architectures (MobileNetV2) and specific upgrades (CCNN&#8217;s parallel streams). On the other hand, earlier models such as VGG-19 (0.821), MobileNet (0.834), and AlexNet (0.805) from [<xref rid="B55-sensors-25-05931" ref-type="bibr">55</xref>] lag significantly. The low recall (0.837) and low precision (0.431) of VGG-19 and AlexNet, respectively, show poor handling of X-ray picture complexity, which may be the result of overfitting or restricted capacity. In comparison to the CCNN, the baseline CNN (0.922) and LSTM-CNN (0.918) from [<xref rid="B54-sensors-25-05931" ref-type="bibr">54</xref>] fall into the middle tier, indicating that simpler architectures have trouble understanding the subtleties of the dataset. While the CCNN maintains a near-perfect balance (F1-score 0.998) with high precision (0.993) and recall (0.996), models such as ResNet-50 exhibit a notable trade-off, achieving a high F1-score (0.999) and recall (0.914) but low precision (0.645), indicating overprediction of pneumonia cases.</p></sec><sec id="sec4dot3dot6-sensors-25-05931"><title>4.3.6. Trade-Offs of 2D vs. 3D Processing</title><p>The use of 2D slice processing simplifies the analysis of 3D volumes but introduces trade-offs. Advantages include reduced computational complexity, enabling the pipeline to handle tripling dataset sizes (e.g., 300 to 900 slices) with current resources, and compatibility with the CCNN&#8217;s 2D architecture. This approach yielded high performance, as seen in the 99.60% Dice for KiTS19. However, it oversimplifies spatial dependencies across slices, potentially missing volumetric context critical for tumor boundary delineation in CT/MRI data. 3D CNNs, while capable of capturing these dependencies (e.g., nnU-Net&#8217;s 3D U-Net achieves ~0.912 Dice on KiTS19), demand significantly higher memory (e.g., 16 GB + GPU vs. 4 GB for 2D) and longer training times (hours vs. minutes per epoch), which exceed our current infrastructure. The 2D approach thus prioritizes efficiency over full spatial fidelity.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05931"><title>5. Conclusions</title><p>This study introduces a new 12-step fuzzy-based image enhancement pipeline that is combined with sophisticated neural network architectures to enhance the performance of medical image analysis tasks, such as pneumonia classification (Chest X-ray Pneumonia), brain tumor segmentation (BraTS2020), and kidney tumor segmentation (KiTS19). BRISQUE scores of 21.7 (KiTS19), 23.4 (BraTS2020), and 22.1 (Chest X-ray) demonstrate that the proposed pipeline, which uses histogram-based, fuzzy entropy-based, and fuzzy standard deviation-based techniques, performs better than the original datasets (28.8, 30.3, 26.8) and CLAHE-filtered datasets (26.4, 27.6, 25.6). It also triples the dataset size and reduces image noise.</p><p>On the KiTS19 dataset, the improved CNN model outperformed the best baseline (LinkNetB7) by 2.40%, achieving a Dice coefficient of 99.60%, precision of 98.70%, sensitivity of 99.30%, and recall of 98.60%. This showed remarkable accuracy in kidney tumor delineation. In a similar vein, the Concatenated CNN model, which was created to take advantage of the complementary information in the upgraded datasets, outperformed TransUNet by 2.30% on BraTS2020, recording a Dice coefficient of 91.50%, precision of 92.00%, sensitivity of 91.20%, and recall of 91.40%. With precision, recall, F1-score, and AUC-ROC metrics all outperforming alternatives by a substantial margin, the CCNN obtained an accuracy of 0.989 for the classification of chest X-ray pneumonia, which was a 7.2% improvement over CLAHE-filtered data. These outcomes highlight the adaptability of the pipeline and the CCNN&#8217;s capacity to improve feature detection in a variety of medical imaging modalities.</p><p>The improved models have significant clinical advantages, including accurate tumor segmentation for radiation and surgical planning, low false positives and missed detections, and the potential to improve patient outcomes for the identification of pneumonia and kidney and brain tumors. Higher picture quality is indicated by the enhanced BRISQUE scores, which make it easier to see important details such as lung opacities and tumor boundaries. But there are still restrictions. The models&#8217; performance is optimized for the enhanced datasets; additional validation is necessary to see whether they can be applied to other datasets or imaging modalities. Despite being lessened by the CCNN&#8217;s parallel architecture, the larger dataset size may provide computational difficulties. Furthermore, thorough comparisons are limited by the absence of sensitivity and recall data for some baseline techniques.</p><p>Future research could look at a number of ways to improve this pipeline even more. First, the quality of enhancement could be further improved by including other fuzzy-based techniques, like adaptive membership functions that are suited to particular modalities. Second, expanding the assessment to encompass a wider range of datasets (such as PET or ultrasound scans) may confirm the generalizability of the pipeline. Third, combining the CCNN with more sophisticated deep-learning architectures, including transformer-based models, may result in even better segmentation and classification results. Fourth, benchmarking the proposed preprocessing pipeline across a broader set of backbone architectures (e.g., ResNet, EfficientNet, DenseNet) is a planned direction for future research. Also extend the pipeline and CCNN to 3D models, leveraging volumetric data to improve accuracy on datasets like KiTS19 and BraTS2020, with validation on external 3D datasets (e.g., KiTS21) and optimized hardware to address current limitations. Lastly, a thorough examination of the enhancement pipeline&#8217;s real-time applicability and computational efficiency will be helpful for its actual implementation in clinical situations. All things considered, our work offers a solid basis for developing medical image analysis using cutting-edge preprocessing methods, with encouraging ramifications for raising diagnostic precision and patient outcomes.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.A., O.Y. and A.S.B.; methodology, A.S.B., O.Y. and D.S.; software, A.S.B., D.S. and S.B.; validation, K.J. and S.B.; formal analysis, A.A., A.S.B. and S.B.; investigation, A.S.B., O.Y. and K.J.; resources, D.S.; data curation, S.B. and K.J.; writing&#8212;original draft preparation, A.A. and A.S.B.; writing&#8212;review and editing, A.J.C. and O.Y.; visualization, A.A., K.J. and D.S.; supervision, A.J.C.; project administration, A.J.C.; funding acquisition, A.J.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05931"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Suk</surname><given-names>H.I.</given-names></name></person-group><article-title>Deep Learning in Medical Image Analysis</article-title><source>Annu. Rev. Biomed. Eng.</source><year>2017</year><volume>19</volume><fpage>221</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1146/annurev-bioeng-071516-044442</pub-id><pub-id pub-id-type="pmid">28301734</pub-id><pub-id pub-id-type="pmcid">PMC5479722</pub-id></element-citation></ref><ref id="B2-sensors-25-05931"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Song</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>A Review of Deep-Learning-Based Medical Image Segmentation Methods</article-title><source>Sustainability</source><year>2021</year><volume>13</volume><elocation-id>1224</elocation-id><pub-id pub-id-type="doi">10.3390/su13031224</pub-id></element-citation></ref><ref id="B3-sensors-25-05931"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Isensee</surname><given-names>F.</given-names></name><name name-style="western"><surname>Jens</surname><given-names>P.</given-names></name><name name-style="western"><surname>Andre</surname><given-names>K.</given-names></name></person-group><article-title>nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1809.10486</pub-id></element-citation></ref><ref id="B4-sensors-25-05931"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buriboev</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Khashimov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Abduvaitov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>H.S.</given-names></name></person-group><article-title>CNN-Based Kidney Segmentation Using a Modified CLAHE Algorithm</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7703</elocation-id><pub-id pub-id-type="doi">10.3390/s24237703</pub-id><pub-id pub-id-type="pmid">39686240</pub-id><pub-id pub-id-type="pmcid">PMC11644871</pub-id></element-citation></ref><ref id="B5-sensors-25-05931"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Musthafa</surname><given-names>N.</given-names></name><name name-style="western"><surname>Memon</surname><given-names>Q.A.</given-names></name><name name-style="western"><surname>Masud</surname><given-names>M.M.</given-names></name></person-group><article-title>Advancing Brain Tumor Analysis: Current Trends, Key Challenges, and Perspectives in Deep Learning-Based Brain MRI Tumor Diagnosis</article-title><source>Eng</source><year>2025</year><volume>6</volume><elocation-id>82</elocation-id><pub-id pub-id-type="doi">10.3390/eng6050082</pub-id></element-citation></ref><ref id="B6-sensors-25-05931"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Banafshe</surname><given-names>F.</given-names></name><name name-style="western"><surname>Abhilash</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gregor</surname><given-names>K.</given-names></name><name name-style="western"><surname>Jacob</surname><given-names>L.</given-names></name><name name-style="western"><surname>Janet</surname><given-names>L.</given-names></name></person-group><article-title>Improved-Mask R-CNN: Towards an accurate generic MSK MRI instance segmentation platform (data from the Osteoarthritis Initiative)</article-title><source>Comput. Med. Imaging Graph.</source><year>2022</year><volume>97</volume><elocation-id>102056</elocation-id><pub-id pub-id-type="doi">10.1016/j.compmedimag.2022.102056</pub-id><pub-id pub-id-type="pmid">35364383</pub-id></element-citation></ref><ref id="B7-sensors-25-05931"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buriboev</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Muhamediyeva</surname><given-names>D.</given-names></name><name name-style="western"><surname>Primova</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sultanov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tashev</surname><given-names>K.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>H.S.</given-names></name></person-group><article-title>Concatenated CNN-Based Pneumonia Detection Using a Fuzzy-Enhanced Dataset</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6750</elocation-id><pub-id pub-id-type="doi">10.3390/s24206750</pub-id><pub-id pub-id-type="pmid">39460230</pub-id><pub-id pub-id-type="pmcid">PMC11510836</pub-id></element-citation></ref><ref id="B8-sensors-25-05931"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sui</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kuang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name></person-group><article-title>Adaptive Contrast Enhancement for Infrared Images Based on the Neighborhood Conditional Histogram</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>1381</elocation-id><pub-id pub-id-type="doi">10.3390/rs11111381</pub-id></element-citation></ref><ref id="B9-sensors-25-05931"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buriboev</surname><given-names>A.</given-names></name><name name-style="western"><surname>Muminov</surname><given-names>A.</given-names></name></person-group><article-title>Computer State Evaluation Using Adaptive Neuro-Fuzzy Inference Systems</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>9502</elocation-id><pub-id pub-id-type="doi">10.3390/s22239502</pub-id><pub-id pub-id-type="pmid">36502208</pub-id><pub-id pub-id-type="pmcid">PMC9738543</pub-id></element-citation></ref><ref id="B10-sensors-25-05931"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Abolghasemi</surname><given-names>V.</given-names></name><name name-style="western"><surname>Anisi</surname><given-names>M.H.</given-names></name></person-group><article-title>Fuzzy Logic with Deep Learning for Detection of Skin Cancer</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>8927</elocation-id><pub-id pub-id-type="doi">10.3390/app13158927</pub-id></element-citation></ref><ref id="B11-sensors-25-05931"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gupta</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mayank</surname><given-names>D.</given-names></name><name name-style="western"><surname>Vipul</surname><given-names>K.</given-names></name><name name-style="western"><surname>Attulya</surname><given-names>S.</given-names></name><name name-style="western"><surname>Atul</surname><given-names>D.</given-names></name></person-group><article-title>Brain tumor segmentation from MRI images using deep learning techniques</article-title><source>Proceedings of the International Advanced Computing Conference</source><conf-loc>Hyderabad, India</conf-loc><conf-date>16&#8211;17 December 2022</conf-date><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><pub-id pub-id-type="doi">10.1007/978-3-031-35641-4_36</pub-id></element-citation></ref><ref id="B12-sensors-25-05931"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Harriet Linda</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wiselin Jiji</surname><given-names>G.</given-names></name></person-group><article-title>Crack detection in X-ray images using fuzzy index measure</article-title><source>Appl. Soft Comput.</source><year>2011</year><volume>11</volume><fpage>3571</fpage><lpage>3579</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2011.01.029</pub-id></element-citation></ref><ref id="B13-sensors-25-05931"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zuiderveld</surname><given-names>K.</given-names></name></person-group><article-title>Contrast Limited Adaptive Histogram Equalization</article-title><source>Graphics Gems IV</source><publisher-name>Academic Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>1994</year><fpage>474</fpage><lpage>485</lpage></element-citation></ref><ref id="B14-sensors-25-05931"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdusalomov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mirzakhalilov</surname><given-names>S.</given-names></name><name name-style="western"><surname>Umirzakova</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shavkatovich Buriboev</surname><given-names>A.</given-names></name><name name-style="western"><surname>Meliboev</surname><given-names>A.</given-names></name><name name-style="western"><surname>Muminov</surname><given-names>B.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>H.S.</given-names></name></person-group><article-title>Accessible AI Diagnostics and Lightweight Brain Tumor Detection on Medical Edge Devices</article-title><source>Bioengineering</source><year>2025</year><volume>12</volume><elocation-id>62</elocation-id><pub-id pub-id-type="doi">10.3390/bioengineering12010062</pub-id><pub-id pub-id-type="pmid">39851336</pub-id><pub-id pub-id-type="pmcid">PMC11759171</pub-id></element-citation></ref><ref id="B15-sensors-25-05931"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heller</surname><given-names>N.</given-names></name><name name-style="western"><surname>Isensee</surname><given-names>F.</given-names></name><name name-style="western"><surname>Maier-Hein</surname><given-names>K.H.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name><name name-style="western"><surname>Nan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>M.</given-names></name><etal/></person-group><article-title>The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge</article-title><source>Med. Image Anal.</source><year>2021</year><volume>67</volume><elocation-id>101821</elocation-id><pub-id pub-id-type="doi">10.1016/j.media.2020.101821</pub-id><pub-id pub-id-type="pmid">33049579</pub-id><pub-id pub-id-type="pmcid">PMC7734203</pub-id></element-citation></ref><ref id="B16-sensors-25-05931"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pizer</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Amburn</surname><given-names>E.P.</given-names></name><name name-style="western"><surname>Austin</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Cromartie</surname><given-names>R.</given-names></name><name name-style="western"><surname>Geselowitz</surname><given-names>A.</given-names></name><name name-style="western"><surname>Greer</surname><given-names>T.</given-names></name><name name-style="western"><surname>ter Haar Romeny</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zimmerman</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Zuiderveld</surname><given-names>K.</given-names></name></person-group><article-title>Adaptive Histogram Equalization and Its Variations</article-title><source>Comput. Vis. Graph. Image Process.</source><year>1987</year><volume>39</volume><fpage>355</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/S0734-189X(87)80186-X</pub-id></element-citation></ref><ref id="B17-sensors-25-05931"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jain</surname><given-names>A.K.</given-names></name></person-group><source>Fundamentals of Digital Image Processing.</source><publisher-name>Prentice Hall</publisher-name><publisher-loc>Upper Saddle River, NJ, USA</publisher-loc><year>1989</year></element-citation></ref><ref id="B18-sensors-25-05931"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gonzalez</surname><given-names>R.C.</given-names></name><name name-style="western"><surname>Woods</surname><given-names>R.E.</given-names></name></person-group><source>Digital Image Processing</source><edition>2nd ed.</edition><publisher-name>Prentice Hall</publisher-name><publisher-loc>Upper Saddle River, NJ, USA</publisher-loc><year>2002</year></element-citation></ref><ref id="B19-sensors-25-05931"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Polesel</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ramponi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Mathews</surname><given-names>V.J.</given-names></name></person-group><article-title>Image Enhancement via Adaptive Unsharp Masking</article-title><source>IEEE Trans. Image Process.</source><year>2000</year><volume>9</volume><fpage>505</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1109/83.826787</pub-id><pub-id pub-id-type="pmid">18255421</pub-id></element-citation></ref><ref id="B20-sensors-25-05931"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>G.</given-names></name></person-group><article-title>A Fast Two-Dimensional Median Filtering Algorithm</article-title><source>IEEE Trans. Acoust. Speech Signal Process.</source><year>1979</year><volume>27</volume><fpage>13</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1109/TASSP.1979.1163188</pub-id></element-citation></ref><ref id="B21-sensors-25-05931"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>K.-J.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>S.-B.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Lim</surname><given-names>C.P.</given-names></name></person-group><article-title>Fuzzy ARTMAP and hybrid evolutionary programming for pattern classification</article-title><source>J. Intell. Fuzzy Syst.</source><year>2011</year><volume>22</volume><fpage>57</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.3233/ifs-2011-0476</pub-id></element-citation></ref><ref id="B22-sensors-25-05931"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pal</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>King</surname><given-names>R.A.</given-names></name></person-group><article-title>Image Enhancement Using Fuzzy Sets</article-title><source>Electron. Lett.</source><year>1980</year><volume>16</volume><fpage>376</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1049/el:19800267</pub-id></element-citation></ref><ref id="B23-sensors-25-05931"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tizhoosh</surname><given-names>H.R.</given-names></name></person-group><article-title>Fuzzy Image Processing: An Overview</article-title><source>Fuzzy Techniques in Image Processing</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2005</year><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1007/3-540-32367-8_1</pub-id></element-citation></ref><ref id="B24-sensors-25-05931"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name></person-group><article-title>Adaptive Intuitionistic Fuzzy Enhancement of Brain Tumor MR Images</article-title><source>Sci. Rep.</source><year>2016</year><volume>6</volume><elocation-id>357602</elocation-id><pub-id pub-id-type="doi">10.1038/srep35760</pub-id><pub-id pub-id-type="pmcid">PMC5082372</pub-id><pub-id pub-id-type="pmid">27786240</pub-id></element-citation></ref><ref id="B25-sensors-25-05931"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Balafar</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Abdul</surname><given-names>S.</given-names></name><name name-style="western"><surname>Iqbal</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rozi</surname><given-names>M.</given-names></name></person-group><article-title>New Multi-scale Medical Image Segmentation based on Fuzzy C-Mean (FCM)</article-title><source>Proceedings of the 2008 IEEE Conference on Innovative Technologies in Intelligent Systems and Industrial Applications</source><conf-loc>Cyberjaya, Malaysia</conf-loc><conf-date>12&#8211;13 July 2008</conf-date><pub-id pub-id-type="doi">10.1109/CITISIA.2008.4607337</pub-id></element-citation></ref><ref id="B26-sensors-25-05931"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ananthi</surname><given-names>V.P.</given-names></name><name name-style="western"><surname>Balasubramaniam</surname><given-names>P.</given-names></name></person-group><article-title>Image fusion using intuitionistic fuzzy sets</article-title><source>Inf. Fusion</source><year>2014</year><volume>20</volume><fpage>21</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2013.10.011</pub-id></element-citation></ref><ref id="B27-sensors-25-05931"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buriboev</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Rakhmanov</surname><given-names>K.</given-names></name><name name-style="western"><surname>Soqiyev</surname><given-names>T.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>A.J.</given-names></name></person-group><article-title>Improving Fire Detection Accuracy through Enhanced Convolutional Neural Networks and Contour Techniques</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5184</elocation-id><pub-id pub-id-type="doi">10.3390/s24165184</pub-id><pub-id pub-id-type="pmid">39204881</pub-id><pub-id pub-id-type="pmcid">PMC11360108</pub-id></element-citation></ref><ref id="B28-sensors-25-05931"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buriboev</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Abduvaitov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>H.S.</given-names></name></person-group><article-title>Integrating Color and Contour Analysis with Deep Learning for Robust Fire and Smoke Detection</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>2044</elocation-id><pub-id pub-id-type="doi">10.3390/s25072044</pub-id><pub-id pub-id-type="pmid">40218557</pub-id><pub-id pub-id-type="pmcid">PMC11991653</pub-id></element-citation></ref><ref id="B29-sensors-25-05931"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Q.</given-names></name></person-group><article-title>Enhancing brain tumor detection in MRI images using YOLO-NeuroBoost model</article-title><source>Front. Neurol.</source><year>2024</year><volume>15</volume><elocation-id>1445882</elocation-id><pub-id pub-id-type="doi">10.3389/fneur.2024.1445882</pub-id><pub-id pub-id-type="pmid">39239397</pub-id><pub-id pub-id-type="pmcid">PMC11374633</pub-id></element-citation></ref><ref id="B30-sensors-25-05931"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Isola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>T.</given-names></name><name name-style="western"><surname>Efros</surname><given-names>A.A.</given-names></name></person-group><article-title>Image-to-Image Translation with Conditional Adversarial Networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>5967</fpage><lpage>5976</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.632</pub-id></element-citation></ref><ref id="B31-sensors-25-05931"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kalra</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name></person-group><article-title>Low-Dose CT Image Denoising Using a Generative Adversarial Network with Wasserstein Distance and Perceptual Loss</article-title><source>IEEE Trans. Med. Imaging</source><year>2018</year><volume>37</volume><fpage>1348</fpage><lpage>1357</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2827462</pub-id><pub-id pub-id-type="pmid">29870364</pub-id><pub-id pub-id-type="pmcid">PMC6021013</pub-id></element-citation></ref><ref id="B32-sensors-25-05931"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Goodfellow</surname><given-names>I.</given-names></name><name name-style="western"><surname>Pouget-Abadie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mirza</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Warde-Farley</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ozair</surname><given-names>S.</given-names></name><name name-style="western"><surname>Courville</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Generative Adversarial Nets</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2014</year><fpage>2672</fpage><lpage>2680</lpage></element-citation></ref><ref id="B33-sensors-25-05931"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name></person-group><article-title>Two-Stage Cascaded U-Net: 1st Place Solution to BraTS Challenge 2019 Segmentation Task</article-title><source>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries, Proceedings of the BrainLes 2019, Shenzhen, China, 17 October 2019</source><comment>Lecture Notes in Computer, Science</comment><person-group person-group-type="editor"><name name-style="western"><surname>Crimi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bakas</surname><given-names>S.</given-names></name></person-group><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><volume>Volume 11992</volume><fpage>239</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-46640-4_22</pub-id></element-citation></ref><ref id="B34-sensors-25-05931"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>Medical Image Computing and Computer-Assisted Intervention (MICCAI), Proceedings of the 18th International Conference, Munich, Germany, 5&#8211;9 October 2015</source><person-group person-group-type="editor"><name name-style="western"><surname>Navab</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hornegger</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wells</surname><given-names>W.</given-names></name><name name-style="western"><surname>Frangi</surname><given-names>A.</given-names></name></person-group><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="B35-sensors-25-05931"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Da Cruz</surname><given-names>L.B.</given-names></name><name name-style="western"><surname>Ara&#250;jo</surname><given-names>J.D.L.</given-names></name><name name-style="western"><surname>Ferreira</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Diniz</surname><given-names>J.O.B.</given-names></name><name name-style="western"><surname>Silva</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>de Almeida</surname><given-names>J.D.S.</given-names></name><name name-style="western"><surname>de Paiva</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Gattass</surname><given-names>M.</given-names></name></person-group><article-title>Kidney segmentation from computed tomography images using deep neural network</article-title><source>Comput. Biol. Med.</source><year>2020</year><volume>123</volume><elocation-id>103906</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2020.103906</pub-id><pub-id pub-id-type="pmid">32768047</pub-id></element-citation></ref><ref id="B36-sensors-25-05931"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Adeli</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>E.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>A.</given-names></name></person-group><article-title>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2020</conf-date><fpage>10245</fpage><lpage>10254</lpage></element-citation></ref><ref id="B37-sensors-25-05931"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Queralta</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Westerlund</surname><given-names>T.</given-names></name></person-group><article-title>MSS U-Net: 3D segmentation of kidneys and tumors from CT images with a multi-scale supervised U-Net</article-title><source>Inform. Med. Unlocked</source><year>2020</year><volume>19</volume><elocation-id>100357</elocation-id><pub-id pub-id-type="doi">10.1016/j.imu.2020.100357</pub-id></element-citation></ref><ref id="B38-sensors-25-05931"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.</given-names></name></person-group><article-title>Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2103.14030</pub-id><pub-id pub-id-type="arxiv">2103.14030</pub-id></element-citation></ref><ref id="B39-sensors-25-05931"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Isensee</surname><given-names>F.</given-names></name><name name-style="western"><surname>Jaeger</surname><given-names>P.F.</given-names></name><name name-style="western"><surname>Kohl</surname><given-names>S.A.A.</given-names></name><name name-style="western"><surname>Petersen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Maier-Hein</surname><given-names>K.H.</given-names></name></person-group><article-title>nnU-Net: A Self-Configuring Method for Deep Learning-Based Biomedical Image Segmentation</article-title><source>Nat. Methods</source><year>2020</year><volume>18</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-01008-z</pub-id><pub-id pub-id-type="pmid">33288961</pub-id></element-citation></ref><ref id="B40-sensors-25-05931"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ibrahim</surname><given-names>A.U.</given-names></name><name name-style="western"><surname>Ozsoz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Serte</surname><given-names>S.</given-names></name><name name-style="western"><surname>Al-Turjman</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yakoi</surname><given-names>P.S.</given-names></name></person-group><article-title>Pneumonia Classification Using Deep Learning from Chest X-ray Images During COVID-19</article-title><source>Cogn. Comput.</source><year>2024</year><volume>16</volume><fpage>1589</fpage><lpage>1601</lpage><pub-id pub-id-type="doi">10.1007/s12559-020-09787-5</pub-id><pub-id pub-id-type="pmcid">PMC7781428</pub-id><pub-id pub-id-type="pmid">33425044</pub-id></element-citation></ref><ref id="B41-sensors-25-05931"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kermany</surname><given-names>D.S.</given-names></name><name name-style="western"><surname>Goldbaum</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Valentim</surname><given-names>C.C.S.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Baxter</surname><given-names>S.L.</given-names></name><name name-style="western"><surname>McKeown</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>F.</given-names></name><etal/></person-group><article-title>Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning</article-title><source>Cell</source><year>2018</year><volume>172</volume><fpage>1122</fpage><lpage>1131.e9</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.02.010</pub-id><pub-id pub-id-type="pmid">29474911</pub-id></element-citation></ref><ref id="B42-sensors-25-05931"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rajpurkar</surname><given-names>P.</given-names></name><name name-style="western"><surname>Irvin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>H.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bagul</surname><given-names>A.</given-names></name><name name-style="western"><surname>Langlotz</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shpanskaya</surname><given-names>K.</given-names></name><etal/></person-group><article-title>CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1711.05225</pub-id></element-citation></ref><ref id="B43-sensors-25-05931"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hsiao</surname><given-names>C.-H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>P.-C.</given-names></name><name name-style="western"><surname>Chung</surname><given-names>L.-A.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>F.Y.-S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>F.-J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.-Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.-H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>T.-L.</given-names></name></person-group><article-title>A deep learning-based precision and automatic kidney segmentation system using efficient feature pyramid networks in computed tomography images</article-title><source>Comput. Methods Programs Biomed.</source><year>2022</year><volume>221</volume><elocation-id>106854</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2022.106854</pub-id><pub-id pub-id-type="pmid">35567864</pub-id></element-citation></ref><ref id="B44-sensors-25-05931"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hassan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Su</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>W.</given-names></name><etal/></person-group><article-title>Deep Segmentation Networks for Segmenting Kidneys and Detecting Kidney Stones in Unenhanced Abdominal CT Images</article-title><source>Diagnostics</source><year>2022</year><volume>12</volume><elocation-id>1788</elocation-id><pub-id pub-id-type="doi">10.3390/diagnostics12081788</pub-id><pub-id pub-id-type="pmid">35892498</pub-id><pub-id pub-id-type="pmcid">PMC9330428</pub-id></element-citation></ref><ref id="B45-sensors-25-05931"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Haghighi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Warfield</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Kurugol</surname><given-names>S.</given-names></name></person-group><article-title>Automatic renal segmentation in DCE-MRI using convolutional neural networks</article-title><source>Proceedings of the 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>4&#8211;7 April 2018</conf-date><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/ISBI.2018.8363865</pub-id><pub-id pub-id-type="pmcid">PMC6248325</pub-id><pub-id pub-id-type="pmid">30473744</pub-id></element-citation></ref><ref id="B46-sensors-25-05931"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zettler</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mastmeyer</surname><given-names>A.</given-names></name></person-group><article-title>Comparison of 2d vs. 3d U-Net Organ Segmentation in abdominal 3d CT images</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2107.04062</pub-id><pub-id pub-id-type="arxiv">2107.04062</pub-id></element-citation></ref><ref id="B47-sensors-25-05931"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Akyel</surname><given-names>C.</given-names></name></person-group><article-title>Kidney Segmentation with LinkNetB7</article-title><source>J. Adv. Res. Nat. Appl. Sci.</source><year>2023</year><volume>9</volume><fpage>844</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.28979/jarnas.1228740</pub-id></element-citation></ref><ref id="B48-sensors-25-05931"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nagarajan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ramprasath</surname><given-names>M.</given-names></name></person-group><article-title>Ensemble Transfer Learning-Based Convolutional Neural Network for Kidney Segmentation</article-title><source>Int. J. Eng. Trends Technol.</source><year>2024</year><volume>72</volume><fpage>446</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.14445/22315381/ijett-v72i9p142</pub-id></element-citation></ref><ref id="B49-sensors-25-05931"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Myronenko</surname><given-names>A.</given-names></name></person-group><article-title>3D MRI Brain Tumor Segmentation Using Autoencoder Regularization</article-title><source>Proceedings of the International MICCAI Brainlesion Workshop</source><conf-loc>Granada, Spain</conf-loc><conf-date>16 September 2018</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>311</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-11726-9_28</pub-id></element-citation></ref><ref id="B50-sensors-25-05931"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Iwamoto</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Han</surname><given-names>X.-H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.-W.</given-names></name><name name-style="western"><surname>Tong</surname><given-names>R.</given-names></name></person-group><article-title>Mixed Transformer U-Net for Medical Image Segmentation</article-title><source>Proceedings of the ICASSP 2022&#8212;2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Singapore</conf-loc><conf-date>23&#8211;27 May 2022</conf-date><fpage>2390</fpage><lpage>2394</lpage><pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9746172</pub-id></element-citation></ref><ref id="B51-sensors-25-05931"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name></person-group><article-title>ResU-Net: Combining Residual Learning with U-Net for Brain Tumor Segmentation</article-title><source>Proceedings of the Medical Image Computing and Computer-Assisted Intervention (MICCAI)</source><conf-loc>Strasbourg, France</conf-loc><conf-date>27 September&#8211;1 October 2021</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>245</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-87193-2_23</pub-id></element-citation></ref><ref id="B52-sensors-25-05931"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schroff</surname><given-names>F.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Rethinking Atrous Convolution for Semantic Image Segmentation</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1706.05587</pub-id><pub-id pub-id-type="arxiv">1706.05587</pub-id></element-citation></ref><ref id="B53-sensors-25-05931"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chowdhury</surname><given-names>M.E.H.</given-names></name><name name-style="western"><surname>Khandakar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>K.R.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>K.F.</given-names></name><name name-style="western"><surname>Mahbub</surname><given-names>Z.B.</given-names></name><name name-style="western"><surname>Kadir</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Kashem</surname><given-names>S.</given-names></name></person-group><article-title>Transfer Learning with Deep Convolutional Neural Network (CNN) for Pneumonia Detection Using Chest X-ray</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>3233</elocation-id><pub-id pub-id-type="doi">10.3390/app10093233</pub-id></element-citation></ref><ref id="B54-sensors-25-05931"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Elshennawy</surname><given-names>N.M.</given-names></name><name name-style="western"><surname>Ibrahim</surname><given-names>D.M.</given-names></name></person-group><article-title>Deep-Pneumonia Framework Using Deep Learning Models Based on Chest X-Ray Images</article-title><source>Diagnostics</source><year>2020</year><volume>10</volume><elocation-id>649</elocation-id><pub-id pub-id-type="doi">10.3390/diagnostics10090649</pub-id><pub-id pub-id-type="pmid">32872384</pub-id><pub-id pub-id-type="pmcid">PMC7554804</pub-id></element-citation></ref><ref id="B55-sensors-25-05931"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Talaat</surname><given-names>M.</given-names></name><name name-style="western"><surname>Si</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xi</surname><given-names>J.</given-names></name></person-group><article-title>Multi-Level Training and Testing of CNN Models in Diagnosing Multi-Center COVID-19 and Pneumonia X-ray Images</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>10270</elocation-id><pub-id pub-id-type="doi">10.3390/app131810270</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05931-f001" orientation="portrait"><label>Figure 1</label><caption><p>Dataset improvements.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05931-g001.jpg"/></fig><fig position="float" id="sensors-25-05931-f002" orientation="portrait"><label>Figure 2</label><caption><p>Image enhancement steps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05931-g002.jpg"/></fig><fig position="float" id="sensors-25-05931-f003" orientation="portrait"><label>Figure 3</label><caption><p>Transformations of local contrasts based on histogram length functions: (<bold>a</bold>) original image; (<bold>b</bold>) defuzzified image.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05931-g003a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05931-g003b.jpg"/></fig><fig position="float" id="sensors-25-05931-f004" orientation="portrait"><label>Figure 4</label><caption><p>CNN architecture for a single dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05931-g004.jpg"/></fig><fig position="float" id="sensors-25-05931-f005" orientation="portrait"><label>Figure 5</label><caption><p>Training and validation loss curves.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05931-g005.jpg"/></fig><fig position="float" id="sensors-25-05931-f006" orientation="portrait"><label>Figure 6</label><caption><p>The visual results of kidney segmentation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05931-g006.jpg"/></fig><table-wrap position="float" id="sensors-25-05931-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t001_Table 1</object-id><label>Table 1</label><caption><p>Numerical example of steps 1&#8211;3 for a 2 &#215; 2 image.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pixel Position</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Original Intensity (0&#8211;255)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Normalized Intensity (Step 1)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Membership Value (Step 2)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Refined Membership (Step 3)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.392</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.253</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.184</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">150</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.588</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.731</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.668</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(2, 1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.784</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.947</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.933</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(2, 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.196</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.068</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.036</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t002_Table 2</object-id><label>Table 2</label><caption><p>BRISQUE scores for each dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Original</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Histogram-Based</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fuzzy Entropy-Based</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fuzzy Standard Deviation-Based</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Chest X-ray</td><td align="center" valign="middle" rowspan="1" colspan="1">26.8</td><td align="center" valign="middle" rowspan="1" colspan="1">22.4</td><td align="center" valign="middle" rowspan="1" colspan="1">21.1</td><td align="center" valign="middle" rowspan="1" colspan="1">22.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BraTS2020 (MRI)</td><td align="center" valign="middle" rowspan="1" colspan="1">30.3</td><td align="center" valign="middle" rowspan="1" colspan="1">24.9</td><td align="center" valign="middle" rowspan="1" colspan="1">23.1</td><td align="center" valign="middle" rowspan="1" colspan="1">23.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KiTS19 (CT)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.4</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t003_Table 3</object-id><label>Table 3</label><caption><p>Information about the parameters of the proposed CCNN model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Layer</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Input 1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Input 2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Input 3</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional Layer 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64 filters, 3 &#215; 3 kernel, ReLU activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64 filters, 3 &#215; 3 kernel, ReLU activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64 filters, 3 &#215; 3 kernel, ReLU activation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max Pooling Layer 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling, stride = 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling, stride = 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling, stride = 2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional Layer 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128 filters, 3 &#215; 3 kernel, ReLU activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128 filters, 3 &#215; 3 kernel, ReLU activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128 filters, 3 &#215; 3 kernel, ReLU activation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max Pooling Layer 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional Layer 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256 filters, 3 &#215; 3 kernel, ReLU activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256 filters, 3 &#215; 3 kernel, ReLU activation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256 filters, 3 &#215; 3 kernel, ReLU activation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max Pooling Layer 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling, resulting in feature map size 64 &#215; 64 &#215; 256</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling, resulting in feature map size 64 &#215; 64 &#215; 256</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 &#215; 2 pooling, resulting in feature map size 64 &#215; 64 &#215; 256</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concatenation Layer</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Concatenate the feature maps from all three branches along the last axis. Final feature map size: 64 &#215; 64 &#215; 768 (256 from each branch).</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional Layer 4</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">512 filters, 3 &#215; 3 kernel, ReLU activation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max Pooling Layer 4</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">2 &#215; 2 pooling, reducing the feature map size to 32 &#215; 32 &#215; 512</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional Layer 5</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">1024 filters, 3 &#215; 3 kernel, ReLU activation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max Pooling Layer 5</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">2 &#215; 2 pooling, reducing the feature map size to 16 &#215; 16 &#215; 1024</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flatten Layer</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Flatten the 3D feature map to a 1D vector</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense Layer 1</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">1024 neurons, ReLU activation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout Layer 1</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Dropout rate = 0.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense Layer 2</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">512 neurons, ReLU activation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout Layer 2</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Dropout rate = 0.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Output Layer</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Two neurons (pneumonia or normal for classification; background or tumor for segmentation), softmax activation</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t004_Table 4</object-id><label>Table 4</label><caption><p>Impact of image enhancement on segmentation accuracy for the KiTS19 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BRISQUE Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Neural Network</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Original KiTS19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.921</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KiTS19 filtered by CLAHE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.959</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KiTS19 filtered by our method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concatenated CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.983</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t005_Table 5</object-id><label>Table 5</label><caption><p>KiTS19 single-type vs. three-type comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BRISQUE</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fuzzy entropy</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">23.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Three-Type Fusion</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CCNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.983</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t006_Table 6</object-id><label>Table 6</label><caption><p>KiTS19 comparison with fuzzy baselines.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BRISQUE</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fuzzy c-means (FCM)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.905</td><td align="center" valign="middle" rowspan="1" colspan="1">27.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fuzzy Entropy</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">23.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Three-Type Fusion (CCNN)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.989</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t007_Table 7</object-id><label>Table 7</label><caption><p>Performance of our CCNN model with alternatives.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Reference</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dice Coefficient (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hsiao et al., 2022 [<xref rid="B43-sensors-25-05931" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">EfficientNetB5</td><td align="center" valign="middle" rowspan="1" colspan="1">96.90</td><td align="center" valign="middle" rowspan="1" colspan="1">97.47</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.45</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Da Cruz et al., 2020 [<xref rid="B35-sensors-25-05931" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">UNet2d</td><td align="center" valign="middle" rowspan="1" colspan="1">96.33</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.32</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Zhao et al., 2020 [<xref rid="B37-sensors-25-05931" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">UNet3d</td><td align="center" valign="middle" rowspan="1" colspan="1">96.90</td><td align="center" valign="middle" rowspan="1" colspan="1">97.10</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.80</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Li et al., 2022 [<xref rid="B44-sensors-25-05931" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResUnet</td><td align="center" valign="middle" rowspan="1" colspan="1">96.54</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">96.49</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">Own</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Haghighi et al., 2018 [<xref rid="B45-sensors-25-05931" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">UNet3d</td><td align="center" valign="middle" rowspan="1" colspan="1">87.50</td><td align="center" valign="middle" rowspan="1" colspan="1">92.70</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">DCE-MRI</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNet2d [<xref rid="B46-sensors-25-05931" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">UNet2d</td><td align="center" valign="middle" rowspan="1" colspan="1">96.50</td><td align="center" valign="middle" rowspan="1" colspan="1">96.55</td><td align="center" valign="middle" rowspan="1" colspan="1">95.90</td><td align="center" valign="middle" rowspan="1" colspan="1">96.20</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNet3d [<xref rid="B46-sensors-25-05931" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">UNet3d</td><td align="center" valign="middle" rowspan="1" colspan="1">96.80</td><td align="center" valign="middle" rowspan="1" colspan="1">96.85</td><td align="center" valign="middle" rowspan="1" colspan="1">96.10</td><td align="center" valign="middle" rowspan="1" colspan="1">96.25</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cihan Akyel, 2023 [<xref rid="B47-sensors-25-05931" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">LinkNet</td><td align="center" valign="middle" rowspan="1" colspan="1">96.62</td><td align="center" valign="middle" rowspan="1" colspan="1">96.58</td><td align="center" valign="middle" rowspan="1" colspan="1">96.97</td><td align="center" valign="middle" rowspan="1" colspan="1">96.18</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cihan Akyel, 2023 [<xref rid="B47-sensors-25-05931" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">LinkNetB7</td><td align="center" valign="middle" rowspan="1" colspan="1">97.20</td><td align="center" valign="middle" rowspan="1" colspan="1">97.30</td><td align="center" valign="middle" rowspan="1" colspan="1">97.00</td><td align="center" valign="middle" rowspan="1" colspan="1">97.00</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ensemble CNN [<xref rid="B48-sensors-25-05931" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Ensemble CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">85.00</td><td align="center" valign="middle" rowspan="1" colspan="1">91.00</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">87.00</td><td align="center" valign="middle" rowspan="1" colspan="1">KiTS19</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Model</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CCNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KiTS19</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t008_Table 8</object-id><label>Table 8</label><caption><p>Impact of image enhancement on segmentation accuracy for the BraTS2020 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BRISQUE Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Neural Network</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Original BraTS2020</td><td align="center" valign="middle" rowspan="1" colspan="1">30.3</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BraTS2020 filtered by CLAHE</td><td align="center" valign="middle" rowspan="1" colspan="1">27.6</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.879</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BraTS2020 filtered by our method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concatenated CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.917</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t009_Table 9</object-id><label>Table 9</label><caption><p>Performance of our CCNN model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Reference</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dice Coefficient (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Isensee et al., 2020 [<xref rid="B39-sensors-25-05931" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">nnU-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">88.90</td><td align="center" valign="middle" rowspan="1" colspan="1">89.50</td><td align="center" valign="middle" rowspan="1" colspan="1">88.30</td><td align="center" valign="middle" rowspan="1" colspan="1">88.70</td><td rowspan="9" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">BraTS2020</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Myronenko, 2019 [<xref rid="B49-sensors-25-05931" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3D Autoencoder</td><td align="center" valign="middle" rowspan="1" colspan="1">87.50</td><td align="center" valign="middle" rowspan="1" colspan="1">88.20</td><td align="center" valign="middle" rowspan="1" colspan="1">87.00</td><td align="center" valign="middle" rowspan="1" colspan="1">87.30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jiang et al., 2020 [<xref rid="B33-sensors-25-05931" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Two-Stage U-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">86.70</td><td align="center" valign="middle" rowspan="1" colspan="1">87.40</td><td align="center" valign="middle" rowspan="1" colspan="1">86.20</td><td align="center" valign="middle" rowspan="1" colspan="1">86.50</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lu et al., 2020 [<xref rid="B36-sensors-25-05931" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">TransUNet</td><td align="center" valign="middle" rowspan="1" colspan="1">89.20</td><td align="center" valign="middle" rowspan="1" colspan="1">90.00</td><td align="center" valign="middle" rowspan="1" colspan="1">88.80</td><td align="center" valign="middle" rowspan="1" colspan="1">89.10</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wang et al., 2021 [<xref rid="B50-sensors-25-05931" ref-type="bibr">50</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3D U-Net++</td><td align="center" valign="middle" rowspan="1" colspan="1">87.80</td><td align="center" valign="middle" rowspan="1" colspan="1">88.50</td><td align="center" valign="middle" rowspan="1" colspan="1">87.40</td><td align="center" valign="middle" rowspan="1" colspan="1">87.60</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Isensee et al., 2020 [<xref rid="B39-sensors-25-05931" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Swin Transformer</td><td align="center" valign="middle" rowspan="1" colspan="1">88.50</td><td align="center" valign="middle" rowspan="1" colspan="1">89.10</td><td align="center" valign="middle" rowspan="1" colspan="1">88.00</td><td align="center" valign="middle" rowspan="1" colspan="1">88.40</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Zhang et al., 2021 [<xref rid="B51-sensors-25-05931" ref-type="bibr">51</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResU-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">86.90</td><td align="center" valign="middle" rowspan="1" colspan="1">87.60</td><td align="center" valign="middle" rowspan="1" colspan="1">86.50</td><td align="center" valign="middle" rowspan="1" colspan="1">86.80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Chen et al., 2017 [<xref rid="B52-sensors-25-05931" ref-type="bibr">52</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">DeepLabV3+</td><td align="center" valign="middle" rowspan="1" colspan="1">87.30</td><td align="center" valign="middle" rowspan="1" colspan="1">88.00</td><td align="center" valign="middle" rowspan="1" colspan="1">86.90</td><td align="center" valign="middle" rowspan="1" colspan="1">87.20</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Model</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CCNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.40</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t010_Table 10</object-id><label>Table 10</label><caption><p>Impact of image enhancement on classification accuracy for the chest X-ray dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BRISQUE Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Neural Network</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Original Chest X-ray</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classic CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.871</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chest X-ray filtered by CLAHE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classic CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.917</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chest X-ray filtered by our method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concatenated CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.989</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05931-t011" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05931-t011_Table 11</object-id><label>Table 11</label><caption><p>Performance of our CCNN across models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Neural Network Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUC</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Proposed CCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.989</td><td align="center" valign="middle" rowspan="1" colspan="1">0.993</td><td align="center" valign="middle" rowspan="1" colspan="1">0.987</td><td align="center" valign="middle" rowspan="1" colspan="1">0.998</td><td align="center" valign="middle" rowspan="1" colspan="1">0.996</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Rahman T. et al. [<xref rid="B53-sensors-25-05931" ref-type="bibr">53</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.992</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.972</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MobileNetV2 [<xref rid="B54-sensors-25-05931" ref-type="bibr">54</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.964</td><td align="center" valign="middle" rowspan="1" colspan="1">0.994</td><td align="center" valign="middle" rowspan="1" colspan="1">0.975</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" rowspan="1" colspan="1">0.970</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN [<xref rid="B54-sensors-25-05931" ref-type="bibr">54</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.922</td><td align="center" valign="middle" rowspan="1" colspan="1">0.920</td><td align="center" valign="middle" rowspan="1" colspan="1">0.937</td><td align="center" valign="middle" rowspan="1" colspan="1">0.955</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LSTM-CNN [<xref rid="B54-sensors-25-05931" ref-type="bibr">54</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.918</td><td align="center" valign="middle" rowspan="1" colspan="1">0.926</td><td align="center" valign="middle" rowspan="1" colspan="1">0.922</td><td align="center" valign="middle" rowspan="1" colspan="1">0.934</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AlexNet [<xref rid="B55-sensors-25-05931" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.805</td><td align="center" valign="middle" rowspan="1" colspan="1">0.431</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.992</td><td align="center" valign="middle" rowspan="1" colspan="1">0.856</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50 [<xref rid="B55-sensors-25-05931" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.867</td><td align="center" valign="middle" rowspan="1" colspan="1">0.645</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">0.999</td><td align="center" valign="middle" rowspan="1" colspan="1">0.914</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MobileNet [<xref rid="B55-sensors-25-05931" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.834</td><td align="center" valign="middle" rowspan="1" colspan="1">0.559</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td><td align="center" valign="middle" rowspan="1" colspan="1">0.989</td><td align="center" valign="middle" rowspan="1" colspan="1">0.879</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG-19 [<xref rid="B55-sensors-25-05931" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.821</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.568</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.941</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.987</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.837</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>