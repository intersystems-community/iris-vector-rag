<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473709</article-id><article-id pub-id-type="pmcid-ver">PMC12473709.1</article-id><article-id pub-id-type="pmcaid">12473709</article-id><article-id pub-id-type="pmcaiid">12473709</article-id><article-id pub-id-type="pmid">41013089</article-id><article-id pub-id-type="doi">10.3390/s25185849</article-id><article-id pub-id-type="publisher-id">sensors-25-05849</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Fault Recovery Through Online Adaptation of Boolean Network Robots</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3625-5414</contrib-id><name name-style="western"><surname>Baldini</surname><given-names initials="P">Paolo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05849" ref-type="aff">1</xref><xref rid="c1-sensors-25-05849" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2335-2514</contrib-id><name name-style="western"><surname>Braccini</surname><given-names initials="M">Michele</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05849" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9891-5441</contrib-id><name name-style="western"><surname>Roli</surname><given-names initials="A">Andrea</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05849" ref-type="aff">1</xref><xref rid="af2-sensors-25-05849" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Cheneler</surname><given-names initials="D">David</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Monk</surname><given-names initials="S">Stephen</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Meli</surname><given-names initials="E">Enrico</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05849"><label>1</label>Department of Computer Science and Engineering (DISI), Alma Mater Studiorum&#8212;Universit&#224; di Bologna, Campus of Cesena, 47521 Cesena, Italy; <email>m.braccini@unibo.it</email> (M.B.); </aff><aff id="af2-sensors-25-05849"><label>2</label>European Centre for Living Technology, 30123 Venice, Italy</aff><author-notes><corresp id="c1-sensors-25-05849"><label>*</label>Correspondence: <email>p.baldini@unibo.it</email></corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5849</elocation-id><history><date date-type="received"><day>22</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>11</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>15</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>19</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05849.pdf"/><abstract><p>Being able to recover from faults is a desired capability in robotics. This requires identifying ineffective behaviors and making some changes so as to display the desired one. In this work, we consider the problem of adjusting the controller of a robot so as to produce the desired behavior. Instead of considering complex and ad-hoc modifications, we leverage the automatic discovery of suitable solutions by means of online adaptation, a mechanism for the modification of the robot control strategy in runtime. Specifically, we use a performance function to identify ineffective behaviors and drive the controller design to an effective one. We also discuss the technical requirements for this procedure to succeed. The results suggest that online adaptation is suitable for the automatic recovery of functions after the occurrence of damages. Additionally, we show that adapting an existing controller to overcome a fault is faster than searching for a new controller from scratch.</p></abstract><kwd-group><kwd>robotics</kwd><kwd>fault recovery</kwd><kwd>fault tolerance</kwd><kwd>online adaptation</kwd><kwd>Boolean Networks</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05849"><title>1. Introduction</title><p>The capability to recover and overcome damages is a fundamental ability of living beings [<xref rid="B1-sensors-25-05849" ref-type="bibr">1</xref>]. This allows humans, animals, and plants to survive despite damages, recovering or reconfiguring their brain and behaviors so as to overcome them [<xref rid="B2-sensors-25-05849" ref-type="bibr">2</xref>]. This greatly increases the chances of individuals&#8217; survival and the continuity of the species. Human-made artifacts have not yet managed to reach a level of autonomy comparable to that of biological systems [<xref rid="B3-sensors-25-05849" ref-type="bibr">3</xref>]. Damages are often fatal to human-made products such as robots, requiring manual collection and repair.</p><p>Enhancing the autonomy of robots is currently of considerable interest in robotics. Many works tackle the problem of maintaining good behavior in changing contexts [<xref rid="B4-sensors-25-05849" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05849" ref-type="bibr">5</xref>]. Among those stand works on fault tolerance and recovery [<xref rid="B6-sensors-25-05849" ref-type="bibr">6</xref>]. We describe fault tolerance as the ability to maintain the desired behavior despite damages, and fault recovery as the modification of some aspects of the robot or controller so as to recover the behavior after the occurrence of damages. For instance, a fault-tolerant robot might rely on redundant sensors so that the occurrence of a single damage would have a reduced impact on the overall control [<xref rid="B7-sensors-25-05849" ref-type="bibr">7</xref>]. Differently, a robot displaying fault recovery could overcome issues in movement actuation by decreasing the speed of the movement or by changing its strategy altogether [<xref rid="B8-sensors-25-05849" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05849" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05849" ref-type="bibr">10</xref>]. The result is that fault tolerance mitigates the impact of the damage but does not necessarily permit to recover the performance. Meanwhile, fault recovery usually displays a more abrupt decrease in effectiveness but, given some time, permits to recover the performance to a level comparable to the original.</p><p>In this work, we are more interested in robots displaying fault recovery. In this line of research, previous works have considered the design of an internal state of the robot [<xref rid="B9-sensors-25-05849" ref-type="bibr">9</xref>]. When a fault occurs, the system recognizes an unexpected behavior and starts a recovery procedure. This recognizes the new state of the robot and produces a suitable motion plan. Cully et al. [<xref rid="B10-sensors-25-05849" ref-type="bibr">10</xref>] created a similar solution, but they stored a library of controllers that the recovery procedure can exploit. More in the line of our work is that of Mahdavi and Bentley [<xref rid="B11-sensors-25-05849" ref-type="bibr">11</xref>]. The authors used an evolutionary strategy, which is an optimization algorithm inspired by biological evolution, on a worm-like robot that underwent actuation faults. When damage occurred, the algorithm found new solutions that worked with the current state of the robot. Similarly, Silva et al. [<xref rid="B12-sensors-25-05849" ref-type="bibr">12</xref>] modified the controller during operation so as to overcome faults on the actuators, obtaining good results from the point of view of the resilience of the system and its performance. Nevertheless, they did not explore the effect of sensory damages. In Braccini et al. [<xref rid="B13-sensors-25-05849" ref-type="bibr">13</xref>], the authors assessed the performance attainable by a robot with different degrees of damages that learned to act online (i.e., in runtime). Nevertheless, their work considered a single task and completely ignored fault identification. Additionally, they did not investigate the advantage of adapting a solution with respect to creating a new one from scratch.</p><p>In this work, we consider the use of Online Adaptation (OA), that is, the modification of the control software of a robot in runtime, with the aim of improving a performance measure. We assess whether this induces the recovery of behavioral capabilities affected by the occurrence of different types of faults. Our scenario of investigation is that of minimal robots with constrained computational and power capabilities, with the future goal of developing adaptable miniaturized robots. This limits the possible solutions we can implement, driving the choice to simple computational components with the possibility of realistic hardware implementation. Specifically, we propose to use Boolean Networks (BNs), a model of Gene Regulatory Networks (GRNs). These latter model the interconnection of molecular regulators that governs the expression of genes and have been previously used as controllers for robots [<xref rid="B14-sensors-25-05849" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05849" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05849" ref-type="bibr">16</xref>]. BNs are indeed dynamical systems with high computational capabilities and possible hardware implementation [<xref rid="B17-sensors-25-05849" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05849" ref-type="bibr">18</xref>], making them suitable for our needs. As a consequence of this research scenario, the adaptive mechanism must also be minimal, implicitly preventing the use of state-of-the-art systems for fault recovery. Herein, we propose a mechanism based on the simple redirection of input signals to different parts of the BN controller, making it easily implementable even in hardware [<xref rid="B5-sensors-25-05849" ref-type="bibr">5</xref>]. An embedded evaluation function drives the adaptation, also in this case enabling simple hardware implementation. In this research context, we explore the requirements and limitations for using OA, and we analyze its performance. Additionally, we check the performance drop when the damage occurs and whether the recovery is faster than re-learning a suitable behavior from scratch. The goal is not to produce the best fault-recovery strategy based on OA, but to verify the aforementioned aspects.</p><p>This work is organized as follows: <xref rid="sec2-sensors-25-05849" ref-type="sec">Section 2</xref> describes the experiment and the experimental set up. <xref rid="sec3-sensors-25-05849" ref-type="sec">Section 3</xref> presents and explains the produced results. <xref rid="sec4-sensors-25-05849" ref-type="sec">Section 4</xref> discusses the obtained results and draws general considerations on the use of OA for fault recovery. Finally, <xref rid="sec5-sensors-25-05849" ref-type="sec">Section 5</xref> summarizes the work carried out and presents possible future research directions.</p></sec><sec sec-type="methods" id="sec2-sensors-25-05849"><title>2. Methods</title><p>The goal of this work was to understand if OA can be used for the recovery of robot performance after the occurrence of damages. To this end, we experimented with different types of faults in two tasks and arenas. We employed a two-wheeled robot endowed with proximity and light sensors. The adaptive mechanism used operates on BN-based controllers.</p><sec id="sec2dot1-sensors-25-05849"><title>2.1. Robot&#160;Controller</title><p>Generally speaking, in order to perform the desired task, a physical agent needs to act in and react to its environment. The robot perceives the state of the world, and possibly its own state, through the use of sensors. This perception is then processed to produce an action that is expected to help complete the task. Often, this control benefits from the use of internal memory that the robot can exploit.</p><p>Among many possible controllers, we proposed one using a BN for its core computation. A BN can be described as a set of interconnected nodes that assume a value in {0, 1} [<xref rid="B19-sensors-25-05849" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05849" ref-type="bibr">20</xref>]. A node value <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#957;</mml:mi><mml:mi>&#945;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> depends on the value <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#957;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#957;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of other <italic toggle="yes">k</italic> &#8220;input&#8221; nodes of the network: <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#957;</mml:mi><mml:mi>&#945;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>&#945;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#957;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#957;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The inputs of each node are selected at random among all the nodes of the BN. This creates internal loops that enable storing information like a memory. Indeed, the system evolves in time according to its own past state, implicitly storing information and displaying some sort of fading memory [<xref rid="B21-sensors-25-05849" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05849" ref-type="bibr">22</xref>]. Potentially, a node value could depend even on its own state at previous iterations. Nevertheless, in this work, we prevented this situation due to the peculiar effects it induces in the computation [<xref rid="B23-sensors-25-05849" ref-type="bibr">23</xref>].</p><p>After deciding the topology of the network (i.e., the <italic toggle="yes">k</italic> inputs for each node), it is important to decide the update function. This maps each possible state of the inputs to the new state of the node, and it is usually represented as a transition table. The update function can be the same for every node, or specific to each one. Herein, we used the latter approach, as for every node, the outputs of the transition table were generated at random according to a bias <italic toggle="yes">p</italic>. Specifically, for a given input state, we selected the resulting output at random: 1 with probability <italic toggle="yes">p</italic>, and 0 with probability <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The update of the state of the BN can be synchronous or asynchronous. In the first case, all the nodes update according to the state of the network at a previous step. In the second case, the nodes update asynchronously according to the current state of the network. In this work, we used synchronous BNs. Additionally, we selected <italic toggle="yes">k</italic> and <italic toggle="yes">p</italic> so as to maximize the computational capabilities of the network&#8212;3 and 0.79, respectively [<xref rid="B24-sensors-25-05849" ref-type="bibr">24</xref>]. Indeed, <italic toggle="yes">k</italic> and <italic toggle="yes">p</italic> determine the dynamic behavior of the BN and its information processing and storing [<xref rid="B21-sensors-25-05849" ref-type="bibr">21</xref>,<xref rid="B25-sensors-25-05849" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05849" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05849" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05849" ref-type="bibr">28</xref>]. Finally, we used a BN with 500 nodes.</p><p>One obvious limitation of BNs is that they are not designed to work with non-Boolean values. This means that in order to work with analog values such as those produced by robot sensors, they first need to be converted. Many approaches exist to feed non-Boolean values into a BN. For instance, one could feed the digital representation of a number to different nodes of the network. Alternatively, this digital representation could be fed into subsequent time steps. In this work, we used a simple approach that consisted in binarizing values according to a threshold. Specifically, if the perceived value was greater than 0.2, it was converted to 1; otherwise, it was converted to 0. Similarly, the state of a BN can be re-converted to non-Boolean values using strategies opposite to those presented earlier. For instance, the state could be interpreted as a digital representation of a number. Alternatively, the succession of values of a node in time could be averaged or, again, considered as the digital representation of a number. In this work, we selected the values of two BN nodes and multiplied them by 2 so as to control the actuators [<xref rid="B13-sensors-25-05849" ref-type="bibr">13</xref>]. Therefore, the control value for the motors of our robot was in {0, 2}.</p></sec><sec id="sec2dot2-sensors-25-05849"><title>2.2. Adaptive&#160;Mechanism</title><p>The binarized sensory inputs and the control values for the actuators were, respectively, the inputs and outputs of the BN controller. Specifically, the former overwrote the values of some selected nodes, while the values of other nodes controlled the latter. We call each of these input-to-node and node-to-output mappings a &#8220;coupling&#8221; [<xref rid="B29-sensors-25-05849" ref-type="bibr">29</xref>].</p><p>At the start of the experiment, the robot selected a random set of couplings, with the constraint that no coupling insists on the same BN node. This guaranteed that the sensors did not overwrite each other value, and that the outputs did not take the value of an input directly. Throughout the experiment, the robot underwent some adaptation phases in which the couplings changed. Specifically, this mechanism re-coupled up to six inputs of the best-known couplings set to different BN nodes. If the new set of couplings attained better performance than their original set, it became the new best set and the starting point for subsequent adaptations. The output couplings remained unmodified throughout the whole experiment.</p><p>In an offline setting, the fastest way to select a controller for the robot is to try different solutions and to select the best. This is possible because the testing environment is predefined and does not usually change during the design. This provides knowledge about the global performance of the robot, reducing the influence of situational conditions on the evaluation [<xref rid="B30-sensors-25-05849" ref-type="bibr">30</xref>]. However, in an online setting such as the one we considered, it is also important to maintain the performance of the robot in time [<xref rid="B5-sensors-25-05849" ref-type="bibr">5</xref>]. This means somehow balancing exploration and exploitation of the best controller found. In this work, we employed the simple strategy of alternating exploration and exploitation. We did not aim to devise the most effective or balanced strategy for orchestrating these two phases, but rather to present a clear approach to proceed with our investigations. The robot performed an explorative step immediately followed by an exploitative one, thus avoiding extended periods of underperformance while still allowing it to explore. During exploitation, the robot used the best controllers it knew. Nevertheless, over time, these could have stopped being effective due to changes in operational conditions. For instance, the external environment or the robot itself could have changed, making the controller ineffective. To face this kind of situation, the robot continuously updated the performance of the best known controller. Specifically, it averaged the previous performance with the new one. Over time, this reduced the performance of the controller previously considered the best (but that was no longer effective), enabling a better one to replace it.</p></sec><sec id="sec2dot3-sensors-25-05849"><title>2.3. Tasks</title><p>In this work, we considered two frequently used tasks [<xref rid="B31-sensors-25-05849" ref-type="bibr">31</xref>]: (i) collision avoidance and (ii) phototaxis. While performing collision avoidance, the robot had to move as fast and straight as possible while avoiding collisions. The evaluation function, therefore, had to consider the maximum proximity to obstacles perceived, as well as the speed and direction of the movement. To compute the performance of the robot in an epoch, we employed the function proposed by Floreano and Mondada [<xref rid="B32-sensors-25-05849" ref-type="bibr">32</xref>] and used in many other subsequent works [<xref rid="B33-sensors-25-05849" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05849" ref-type="bibr">34</xref>]: <disp-formula id="FD1-sensors-25-05849"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>perf</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mspace width="0.277778em"/><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="0.166667em"/><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.277778em"/><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mfenced open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where:<list list-type="bullet"><list-item><p><italic toggle="yes">e</italic> is the epoch under evaluation;</p></list-item><list-item><p><italic toggle="yes">i</italic> is the index of a step in the epoch;</p></list-item><list-item><p><italic toggle="yes">N</italic> is the number of steps in the epoch, equal to 1500 (i.e., 150 s);</p></list-item><list-item><p><inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum proximity perceived by the robot, in <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, in step <italic toggle="yes">i</italic>;</p></list-item><list-item><p><inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicate whether the left and right motors of the robot are active in step <italic toggle="yes">i</italic>, with 0 indicating a still motor and 1 indicating a turning one.</p></list-item></list></p><p>We experimented with the collision avoidance task in a squared arena with a central block (see <xref rid="sensors-25-05849-f001" ref-type="fig">Figure 1</xref>a). The robot started in a random position of the arena and had to travel the circuit as fast as possible in a straight line. The best strategy consisted of going straight in the side corridors and turning as fast as possible at the corners.</p><p>The second task that we considered was phototaxis. The robot had to follow a light gradient to get as near as possible to its source. This required using light sensors to perceive the signal and wheeled motors to move toward it. We calculated the quality of the behavior in function of the distance traveled towards the light. Specifically, we simply evaluated the performance of the robot as the difference between the distance from the light at the beginning and at the end of an evaluation epoch, as described by the following function:<disp-formula id="FD2-sensors-25-05849"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>perf</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where:<list list-type="bullet"><list-item><p><italic toggle="yes">e</italic> is the epoch under evaluation;</p></list-item><list-item><p><inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the distance of the robot from the light source at the start of the epoch;</p></list-item><list-item><p><inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the distance of the robot from the light source at the end of the epoch.</p></list-item></list></p><p>This is a proxy for the equivalent local evaluation function employing the light sensors to understand the distance. Indeed, the robot can understand if it is moving in the correct direction and the distance from the light thanks to changes in its perception. An increase in the maximum perceived intensity indicates a movement toward the light, while a decrease indicates that it is going away from it. Similarly, the change in the radiation intensity between the start and the end of an epoch indicates the distance traveled toward the light. This evaluation function is conceptually similar to functions used in other works [<xref rid="B33-sensors-25-05849" ref-type="bibr">33</xref>,<xref rid="B35-sensors-25-05849" ref-type="bibr">35</xref>].</p><p>An epoch of the phototaxis task is composed of 450 steps, for a total of 45 s. We experimented with the phototaxis task in a long empty arena with a light source in the top-right corner (see <xref rid="sensors-25-05849-f001" ref-type="fig">Figure 1</xref>b). The robot always started in the bottom-left corner, which was the furthest from the light, and had to get as near as possible to it. We made sure that the robot sensors perceived the luminous radiation in any point of the arena. The size of one side of the arena was 1000 m, permitting the robot to move without ever reaching the luminous source during the duration of the experiment. This avoided a condition in which the robot could not achieve a positive performance due to having already reached the source.</p></sec><sec id="sec2dot4-sensors-25-05849"><title>2.4. Damage&#160;Types</title><p>In this experiment, we explored the capability to recover from different types of faults. Specifically, we imagined issues with both perception and actuation so as to mimic a realistic scenario. Nevertheless, the need to perform self-evaluation to drive the OA required us to focus on internal errors, which are errors occurring on the connection between sensors/actuators and the controller of the robot (see <xref rid="sensors-25-05849-f002" ref-type="fig">Figure 2</xref>). These could undermine the robot behavior but still permit self-evaluation and thus adaptation. The first type of damage we considered was on the wheeled motors. Obviously, zeroing the ability to move would completely impair the robot. Therefore, we imagined a softer damage that just slowed down the motors speed. For instance, imagining a rotation speed of <italic toggle="yes">X</italic> rad/s without damage, its speed while damaged would be <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>&#183;<italic toggle="yes">X</italic> rad/s. The result is that the robot moves slower on the side presenting the fault, modifying its trajectory and speed. We tested if the robot was able to maintain the correct behavior with zero, one, or both motors functioning at half of their original speed.</p><p>The other types of damage that we explored affect the connection of the sensors to the controller. Specifically, we assessed the effect of (i) blind (or missing) perception, (ii) fixed perception, and (iii) random perception. Blind perception occurs when the sensor&#8211;controller connections fail, becoming unable to forward the sensory signal. Practically, we implemented this by avoiding overwriting the BN input nodes with the perceived signals. This reduced the information available to the controller, making it partially blind and damaging the feedback loop. Meanwhile, fixed perception occurs when the overwriting process of BN input nodes blocks. We implemented this by selecting a random value and by continuously feeding it to the BN controller. Even in this case, the robot became partially blind to the external world. Finally, we imagined the sensor&#8211;controller connection to behave erratically, perturbing the BN controller with random values.</p><p>All these types of sensory faults represent possible real-world issues. Whatever the problem, they make the robot at least partially blind to the real world. Nevertheless, there are motivations to assess the effect of each of them separately. Indeed, the management of each type of damage might differ, with some being more complicated to overcome. For instance, blind perception removes information from the computation, but it does not negatively perturb the BN. Fixed perception affects the computation but can be easily nullified by connecting it to a node that ignores its (fixed) value. Again, it can even be used to favorably modify the dynamics of the BN. Finally, random perception might have the biggest impact, requiring one to ignore the signal regardless of its value. Therefore, we assessed the effect of all these types of sensory faults. For each, we tested the effect of a different number of damaged sensors, starting with 0 and increasing the number by 3 up to a total of 24: 0, 3, 6, &#8230;, 21, 24. All the faults occurred on contiguous sensors so as to mimic localized faults.</p></sec><sec id="sec2dot5-sensors-25-05849"><title>2.5. Experimental&#160;Setting</title><p>All the experiments of this work used a foot-bot [<xref rid="B36-sensors-25-05849" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05849" ref-type="bibr">37</xref>], which is a two-wheeled robot endowed with multiple sensory capabilities. In this work, we used just two of them: proximity sensors for the collision avoidance task and light sensors for the phototaxis task. The robot has 24 sensors for each of these two types, distributed homogeneously around the chassis. The robot acts by means of two differential steering wheels, whose speed we limited to 2 cm/s. All the experiments took place in a simulated environment powered by the ARGoS3 simulator, beta 48 [<xref rid="B38-sensors-25-05849" ref-type="bibr">38</xref>].</p><p>For each type and intensity of damage, we tested 2500 robot controllers that represented our replicas of the experiment. The controllers were all different inside the same experiment but were the same across different ones. For instance, replica 3 from the experiment with 6 blind sensors had the same starting controller as replica 3 from the experiment with 24 fixed sensors. This guaranteed that the experiments were comparable. After running the experiments, we filtered out the replicas in which the robot did not learn the behavior without damages. Indeed, being able to complete the task is a necessary condition for discussing fault recovery. Additionally, since the controller of the robot used a random BN with fixed output couplings, it is possible that for some replicas, a working controller did not exist at all. This might be due to topological pathologies or to the connection of the motors to fixed output nodes. The result is that for these pathological cases, the adaptive mechanism that we employed was inadequate. In order to avoid considering these situations, we set a threshold for each task discriminating between robots that learned the behavior and robots that did not. We selected the thresholds considering the characteristics of the evaluation functions and empirically observing multiple runs. Specifically, in the collision avoidance task, a performance greater than 500 indicates the ability to avoid obstacles, even though not necessarily efficiently. Just imagine a robot traveling a straight corridor at its maximum speed. This is the only condition in which it could attain the maximum performance: 1500. In this situation, a robot constantly perceiving an obstacle at 1/3 of its perception distance would obtain a performance of 500. Meanwhile, a robot attaining the maximum step performance for 1/3 of the evaluation epoch would also attain a performance of 500. Both situations thus represent a reasonable behavior. Furthermore, both assume that the robot follows a straight trajectory at its maximum speed, which is not a reasonable assumption. This means that a robot attaining a performance of 500 would likely avoid obstacles quite effectively. Therefore, we state that the selected threshold is theoretically sound for the identification of effective behaviors in the collision avoidance task. We confirmed this choice after visual inspection of some runs. In the phototaxis task, we selected 0 as the threshold, identifying robots that averagely move toward the light during the experiment. This is because the evaluation function for the phototaxis task represents movements toward the light as positive values and movements away from it as negative values. Higher performance indicates more effective behaviors. We considered only replicas whose average performance during exploitation epochs (i.e., the ones using the best behavior known) in the first phase of the experiment (i.e., without damages) was higher than the threshold.</p><p>In this work, we were also interested in the recovery time of the performance when adapting a previously working controller. To this end, we decided to compare the results to those of a controller designed from scratch for the damaged robot. We refer to the first set of experiments as &#8220;informed&#8221; and to the second as &#8220;clueless&#8221; so as to represent the ability of the robot when damage occurs. Clueless robots started in the same position as the informed ones when damage occurred. Additionally, we provided clueless robots the same time to adapt to damages as informed ones. Specifically, informed robots had 480 epochs to learn the behavior and another 480 to adapt it to overcome the faults. Clueless robots had 480 epochs to develop a controller from scratch considering the damages.</p><p>All the code used in this work and the produced data are available online [<xref rid="B39-sensors-25-05849" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05849" ref-type="bibr">40</xref>].</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05849"><title>3. Results</title><p>In this section, we assess the capability of OA to recover the robot performance after a fault. We consider only the performance of the robots during the exploitation of the best solution, and we ignore the performance during exploration. The results show that OA was generally able to recover or maintain the robot performance after undergoing faults (see <xref rid="sensors-25-05849-f003" ref-type="fig">Figure 3</xref>). Nevertheless, we can distinguish the impact on the maximum recovered performance caused by different types of damage in each of the two tasks.</p><p>In the collision avoidance task, having a damaged actuator seemed to improve the performance (see <xref rid="sensors-25-05849-f004" ref-type="fig">Figure 4</xref>). The difference in the two wheels speed indeed tended to curve the trajectory of the robot, inducing a turn. Since the arena was a circuit, a turning behavior effectively navigated the environment. Normally, this is discouraged by the evaluation function, which penalizes unnecessary turns. However, this considers only if the motors are active, and not their effective rotation speed. The result is that the evaluation function we employed was not able to discriminate a turn due to a slowed motor from a straight movement, and thus could not penalize it. This enabled the robot to achieve a higher performance despite showing a suboptimal behavior. This hypothesis held when considering two faulty actuators. In this case, the robot no longer displayed the turning behavior. Instead, all its movements slowed down by half. This increased the time to perform a turn and move away whenever it encountered an obstacle, decreasing the performance with respect to the undamaged scenario. Additionally, OA did not recover the performance in the two-damages scenario. Instead, it decreased slowly until it stabilized.</p><p>Still referring to the collision avoidance task, the occurrence of sensory damages caused a sudden drop in the performance (see <xref rid="sensors-25-05849-f005" ref-type="fig">Figure 5</xref>, <xref rid="sensors-25-05849-f006" ref-type="fig">Figure 6</xref> and <xref rid="sensors-25-05849-f007" ref-type="fig">Figure 7</xref>). Nevertheless, OA recovered the behavior in a short time regardless of the type of damage incurred. The differences occurred in the intensity of the performance drop immediately after the damage, and in the recovered performance. Specifically, blind robots experienced a substantial decrease with the slowest recovery (see <xref rid="sensors-25-05849-f005" ref-type="fig">Figure 5</xref>). Fixed perception caused a comparable drop in performance after damage but recovered faster (see <xref rid="sensors-25-05849-f006" ref-type="fig">Figure 6</xref>). Additionally, the performance attained after adaptation was slightly higher than that of blind robots, indicating a more manageable fault. Finally, random perception caused a reduced drop in performance with respect to the two other types of damage (see <xref rid="sensors-25-05849-f007" ref-type="fig">Figure 7</xref>). Additionally, the recovery was faster and led to a performance similar to that of the undamaged robot.</p><p>In the phototaxis task, the peculiar trend displayed in the presence of damaged actuators was not replicated (see <xref rid="sensors-25-05849-f008" ref-type="fig">Figure 8</xref>). Indeed, the performance decreased gracefully according to the intensity of the fault. This is because, unlike the collision avoidance task, the phototaxis arena did not favor turning behaviors; instead, it required a straight trajectory toward the light to attain the highest evaluation. In the same way, a slower movement decreased the maximum traveled distance. This also limited the performance that the robot could attain, as it was bound to half of the maximum without damages. The results showed a modest recovery of performance in the case of just one instance of damage, with even a slight decrease in the case of two damages.</p><p>The performance trend in the phototaxis task with faulty sensors differed slightly from that in the collision avoidance task (see <xref rid="sensors-25-05849-f009" ref-type="fig">Figure 9</xref>, <xref rid="sensors-25-05849-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05849-f011" ref-type="fig">Figure 11</xref>). When damages occurred, we still observed a drop in performance. However, this recovered more slowly throughout the entire duration of the experiment, often without stabilizing. Interestingly, with some degree of damage, the recovery even seemed enhanced. Indeed, slightly and heavily damaged robots recovered less and slower than moderately damaged ones. For instance, the recovery when subjected to 12 or 15 blind sensors was much greater than that with 3 or 24 faulty sensors. This trend was robust across all the experiments. The motivation is probably that a few damages cause a negligible drop in performance and therefore a limited possible recovery. Meanwhile, with many damages, recovering the performance becomes harder. A mean amount of damage causes a sensible drop but still permits recovering the performance to a great extent.</p><p>Even in the phototaxis case, we can distinguish the effect of different types of damage on the performance. With blind sensors, the performance drop was significant and did not notably recover for slightly or heavily damaged robots (see <xref rid="sensors-25-05849-f009" ref-type="fig">Figure 9</xref>). Differently, the recovery seemed to be at its maximum at approximately half faulty sensors. The experiments with fixed perception showed a similar trend, but even highly damaged robots were able to recover their performance (see <xref rid="sensors-25-05849-f010" ref-type="fig">Figure 10</xref>). Additionally, the recovered performance was greater than that in the case of blind robots, indicating a more manageable fault. Finally, with random perception, the drop in performance was the greatest and recovery was minimal (see <xref rid="sensors-25-05849-f011" ref-type="fig">Figure 11</xref>). Indeed, OA seems to be able to recover the performance only for slightly faulty robots. This differs from the results obtained in the collision avoidance task, where random perception was the least detrimental type of fault.</p><p>In addition to the trends just analyzed, we also investigated the effects of adaptation in &#8220;clueless&#8221; robots, which began damaged and with completely random input couplings (see <xref rid="sensors-25-05849-f004" ref-type="fig">Figure 4</xref>, <xref rid="sensors-25-05849-f005" ref-type="fig">Figure 5</xref>, <xref rid="sensors-25-05849-f006" ref-type="fig">Figure 6</xref>, <xref rid="sensors-25-05849-f007" ref-type="fig">Figure 7</xref>, <xref rid="sensors-25-05849-f008" ref-type="fig">Figure 8</xref>, <xref rid="sensors-25-05849-f009" ref-type="fig">Figure 9</xref>, <xref rid="sensors-25-05849-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05849-f011" ref-type="fig">Figure 11</xref>). This permitted to compare the recovery time of &#8220;informed&#8221; robots that started with a previously working behavior, and to verify that this does not trap the controller into an ill solution. All experiments showed that the performance attained starting from a working controller (i.e., informed) was similar to that attained starting from a clueless robot. This suggests that adapting a working controller does not prevent a successful exploration of alternative solutions. Instead, it speeds up the search, attaining better performance soon after damage occurs. This is especially true for limited faults, which affect the performance less and thus require fewer modifications to become effective again. Additionally, this is especially true for the collision avoidance task, as in the phototaxis task the recovery speed-up was reduced.</p></sec><sec sec-type="discussion" id="sec4-sensors-25-05849"><title>4. Discussion</title><p>In this work, we investigated whether OA enables recovering the performance of a robot after the occurrence of damage and the time it requires. We found that OA is a valid strategy for overcoming damages in most situations (see <xref rid="sensors-25-05849-f0A1" ref-type="fig">Figure A1</xref>, <xref rid="sensors-25-05849-f0A2" ref-type="fig">Figure A2</xref>, <xref rid="sensors-25-05849-f0A3" ref-type="fig">Figure A3</xref>, <xref rid="sensors-25-05849-f0A4" ref-type="fig">Figure A4</xref>, <xref rid="sensors-25-05849-f0A5" ref-type="fig">Figure A5</xref>, <xref rid="sensors-25-05849-f0A6" ref-type="fig">Figure A6</xref>, <xref rid="sensors-25-05849-f0A7" ref-type="fig">Figure A7</xref>, <xref rid="sensors-25-05849-f0A8" ref-type="fig">Figure A8</xref>, <xref rid="sensors-25-05849-f0A9" ref-type="fig">Figure A9</xref>, <xref rid="sensors-25-05849-f0A10" ref-type="fig">Figure A10</xref>, <xref rid="sensors-25-05849-f0A11" ref-type="fig">Figure A11</xref> and <xref rid="sensors-25-05849-f0A12" ref-type="fig">Figure A12</xref>). This permits to recover the performance to a degree proportional to the intensity of the fault. Indeed, more damages induced a greater drop in performance, which was usually followed by a recovery proportionally greater than that of fewer damages. Meanwhile, limited faults caused an almost negligible drop, which was soon recovered to a slightly better value. In the phototaxis task, this trend was slightly different, with heavily damaged robots recovering their performance less. We also identified how different types of damage produce different effects in the recovery. Specifically, we noticed that a fixed perception damage affected the robot less in the two tasks (i.e., the drop was limited and the recovery was fast). This is somewhat surprising, as we would expect a blind (or missing) perception to do so. The motivation is probably that the former enables modifying the dynamics of the BN in favor of the robot, eliciting the desired behavior in a specific environment. Not modifying the controller, blind perception removes the possibility to influence its dynamics and, therefore, to modify the intrinsic behavior of the robot. Random perception produced different results in the collision avoidance and phototaxis tasks. Specifically, it permitted attaining the highest recovered performance in the former, while it produced the worst behavior in the latter. The underlying cause of this difference remains unclear. Finally, damages to the actuation were generally the hardest to recover but could enhance the performance by exploiting the peculiarities of the evaluation function. While this does not actually imply an effective improvement in the generated behavior, it is still an interesting point to consider while devising the function itself. Interestingly, this peculiar trend has been observed even in other articles [<xref rid="B12-sensors-25-05849" ref-type="bibr">12</xref>].</p><p>The aforementioned differences in the impact of different types of damage were not universal but depended on the environment in which the robot acted. Clearly, a robot with completely damaged sensors should not be able to act properly. Nevertheless, it succeeded in doing so thanks to OA, which exploited the peculiarities of the environment to select a suitable behavior. Similarly, when a robot was only partially damaged, OA permitted using both sensory perception and the peculiarities of the environment to improve the performance. This can be seen as a sort of overfitting of the behavior for a specific environment and for a specific type of damage. Usually, this would be considered negative, but due to the characteristics of OA, this is not particularly problematic. Indeed, the ability to generalize is useful for facing unexpected challenges, but it decreases in importance when we can directly adapt to them. Obviously, generalizing remains important, but it is no longer fundamental to succeed. All of these considerations suggest that the difference in the performance with different types of damages could arise from their synergy with the environment and by how easily OA exploits them. A notable example is that of one faulty actuator in collision avoidance, which curved the trajectory, simplifying the navigation of the circuit arena.</p><p>One interesting result is in the analysis of the time to recover the performance after the occurrence of damage. Indeed, trying to fix an already known controller might slow down the recovery, or even trap it in an unfavorable section of the search space. If this were the case, generating a new controller from scratch could lead to better results more quickly. Our results showed, instead, that modifying a known solution to fix the behavior led to a faster performance recovery with respect to learning from scratch. This even suggests that starting from known behaviors might speed up the discovery of controllers for different tasks.</p><p>One important aspect we want to highlight is the need to constantly re-evaluate the best solution [<xref rid="B30-sensors-25-05849" ref-type="bibr">30</xref>]. Indeed, from preliminary experiments, we noticed that keeping only the best evaluation did not work in changing contexts. A &#8220;best controller&#8221; at time <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> might not be the best at time <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. If the maximum performance obtainable by the latter is lower than that achievable by the former, then the adaptation will keep searching for an impossible solution. Re-evaluating the best controller permits identifying when it loses effectiveness, and thus enables replacing it with a better one. Calibrating the re-evaluation weight (i.e., how much our valuation of the controller depends on its last assessment) finally determines the repulsion to change of the robot.</p><p>Just re-evaluating the controller is, however, not enough for an effective adaptation. The evaluation function itself has a primary role in the success of the performance recovery. Indeed, in order to work effectively, OA needs to properly assess the performance of the robot. Doing so requires that the evaluation function understands the quality of the behavior thanks to perception, internal, and/or external feedback. In this work, we imagined damages affecting the internal connections of the robot, from the sensors to the controller, and from the controller to the actuators. The result is that the robot can still evaluate its behavior and thus drive the adaptation to the desired outcome. Nevertheless, damages occurring on the sensors themselves would in that case affect even the capability of self-evaluation. Facing this problem, therefore, requires an evaluation system able to automatically identify inconsistent states [<xref rid="B41-sensors-25-05849" ref-type="bibr">41</xref>]. Alternatively, it could rely on external evaluation or on multiple measures. Nevertheless, this aspect is out of the scope of this work, and thus it is not discussed here. Indeed, we only assessed OA as a mechanism to recover the performance in case of faults, finding it effective.</p><p>As the final point of this discussion, we want to highlight that the whole control system proposed and used in this work is extremely simple. This is because our goal is to start developing a mechanism able to operate on microscopic artificial agents. Obviously, this limits the complexity of the system we propose, which should be easily implementable even in hardware. This excludes the use of all the state-of-the-art mechanisms for fault recovery we know about, which are typically much more complex and computationally demanding with respect to the method we propose here. We claim that our controller fulfills the characteristics for straightforward hardware implementation, making it potentially suitable for adoption in microscopic robots. However, this constrained scenario prevents a direct comparison with other solutions existing in the literature.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05849"><title>5. Conclusions</title><p>In this work, we discussed the use of Online Adaptation to provide fault recovery to extremely simple and limited robots controlled by Boolean Networks. We explored its use in two tasks: collision avoidance and phototaxis. The results showed that OA is effective in recovering the performance of robots to a level similar to that before the damage. We found that adapting a previously working controller recovered the performance of robots faster than devising a new controller from scratch. Additionally, we found that larger damages caused a greater drop in performance but also permitted a greater recovery. We discussed the role of re-evaluation of the best controller as a necessary condition for OA to work. Finally, we proposed adaptation as a way to leverage the peculiarities of the environment to reach a desired goal more effectively.</p><p>Our approach focused on the performance recovery through an adaptation driven by an embedded evaluation function. Nevertheless, we did not consider the case in which the evaluation itself fails. Even though it was not the focus of this work, we recognize that this could limit the application of the proposed mechanism. Therefore, we intend to further explore this issue by investigating the best way to combine fault diagnosis with the fault recovery provided by OA.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, P.B., M.B. and A.R.; methodology, P.B., M.B. and A.R.; software, P.B. and A.R.; validation, P.B.; formal analysis, P.B., M.B. and A.R.; investigation, P.B.; writing&#8212;original draft preparation, P.B.; writing&#8212;review and editing, M.B. and A.R.; visualization, P.B.; supervision, M.B. and A.R. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The code used during the experiments and the resulting data are available in publicly accessible repositories [<xref rid="B39-sensors-25-05849" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05849" ref-type="bibr">40</xref>].</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><app-group><app id="app1-sensors-25-05849"><title>Appendix A. Run Length Distribution</title><p>Plots of the Run Length Distribution (RLD), that is, the rate of replicas in which the robot attained a performance greater than a threshold in time.</p><sec id="secAdot1-sensors-25-05849"><title>Appendix A.1. Collision Avoidance</title><fig position="anchor" id="sensors-25-05849-f0A1" orientation="portrait"><label>Figure A1</label><caption><p>RLD according to the number of slowed motors in the collision avoidance task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 500 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A1.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A2" orientation="portrait"><label>Figure A2</label><caption><p>RLD according to the number of blind perceptions in the collision avoidance task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 500 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A2.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A3" orientation="portrait"><label>Figure A3</label><caption><p>RLD according to the number of fixed perceptions in the collision avoidance task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 500 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A3.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A4" orientation="portrait"><label>Figure A4</label><caption><p>RLD according to the number of random perceptions in the collision avoidance task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 500 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A4.jpg"/></fig></sec><sec id="secAdot2-sensors-25-05849"><title>Appendix A.2. Phototaxis</title><fig position="anchor" id="sensors-25-05849-f0A5" orientation="portrait"><label>Figure A5</label><caption><p>RLD according to the number of slowed motors in the phototaxis task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 0 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A5.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A6" orientation="portrait"><label>Figure A6</label><caption><p>RLD according to the number of blind perceptions in the phototaxis task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 0 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A6.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A7" orientation="portrait"><label>Figure A7</label><caption><p>RLD according to the number of fixed perceptions in the phototaxis task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 0 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A7.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A8" orientation="portrait"><label>Figure A8</label><caption><p>RLD according to the number of random perceptions in the phototaxis task. This plot considers only replicas that learned the behavior without damages. Every performance greater than 0 is considered a success.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A8.jpg"/></fig></sec></app><app id="app2-sensors-25-05849"><title>Appendix B. Distance from Light in Phototaxis</title><fig position="anchor" id="sensors-25-05849-f0A9" orientation="portrait"><label>Figure A9</label><caption><p>Robot distance from the light in time when subjected to slowed motion. This plot considers only replicas that learned the behavior without damages.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A9.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A10" orientation="portrait"><label>Figure A10</label><caption><p>Robot distance from the light in time when subjected to blind perception. This plot considers only replicas that learned the behavior without damages.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A10.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A11" orientation="portrait"><label>Figure A11</label><caption><p>Robot distance from the light in time when subjected to fixed perception. This plot considers only replicas that learned the behavior without damages.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A11.jpg"/></fig><fig position="anchor" id="sensors-25-05849-f0A12" orientation="portrait"><label>Figure A12</label><caption><p>Robot distance from the light in time when subjected to random perception. This plot considers only replicas that learned the behavior without damages.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g0A12.jpg"/></fig></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-05849"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weavers</surname><given-names>H.</given-names></name></person-group><article-title>Biological resilience in health and disease</article-title><source>Dis. Model. Mech.</source><year>2024</year><volume>17</volume><fpage>dmm050799</fpage><pub-id pub-id-type="doi">10.1242/dmm.050799</pub-id><pub-id pub-id-type="pmid">39051470</pub-id><pub-id pub-id-type="pmcid">PMC11552498</pub-id></element-citation></ref><ref id="B2-sensors-25-05849"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wilson</surname><given-names>B.</given-names></name></person-group><article-title>Brain injury: Recovery and rehabilitation</article-title><source>WIREs Cogn. Sci.</source><year>2010</year><volume>1</volume><fpage>108</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1002/wcs.15</pub-id><pub-id pub-id-type="pmid">26272844</pub-id></element-citation></ref><ref id="B3-sensors-25-05849"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bennett</surname><given-names>M.</given-names></name></person-group><article-title>Are Biological Systems More Intelligent Than Artificial Intelligence?</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2405.02325</pub-id></element-citation></ref><ref id="B4-sensors-25-05849"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Baldini</surname><given-names>P.</given-names></name><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name></person-group><article-title>Online Adaptation of Robots Controlled by Nanowire Networks: A Preliminary Study</article-title><source>Communications in Computer and Information Science, Proceedings of the Artificial Life and Evolutionary Computation, Gaeta, Italy, 14&#8211;16 September 2022</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2023</year><volume>Volume 1780</volume><fpage>171</fpage><lpage>182</lpage></element-citation></ref><ref id="B5-sensors-25-05849"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baldini</surname><given-names>P.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name></person-group><article-title>On the Performance of Online Adaptation of Robots Controlled by Nanowire Networks</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>144408</fpage><lpage>144420</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3345224</pub-id></element-citation></ref><ref id="B6-sensors-25-05849"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>M.</given-names></name></person-group><article-title>Resilient Robots: Concept, Review, and Future Directions</article-title><source>Robotics</source><year>2017</year><volume>6</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.3390/robotics6040022</pub-id></element-citation></ref><ref id="B7-sensors-25-05849"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abbaspour</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mokhtari</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sargolzaei</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yen</surname><given-names>K.</given-names></name></person-group><article-title>A Survey on Active Fault-Tolerant Control Systems</article-title><source>Electronics</source><year>2020</year><volume>9</volume><elocation-id>1513</elocation-id><pub-id pub-id-type="doi">10.3390/electronics9091513</pub-id></element-citation></ref><ref id="B8-sensors-25-05849"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bongard</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lipson</surname><given-names>H.</given-names></name></person-group><article-title>Automated damage diagnosis and recovery for remote robotics</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation, ICRA &#8217;04</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>26 April&#8211;1 May 2004</conf-date><publisher-name>Institute of Electrical and Electronics Engineers (IEEE)</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2004</year><volume>Volume 4</volume><fpage>3545</fpage><lpage>3550</lpage></element-citation></ref><ref id="B9-sensors-25-05849"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bongard</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zykov</surname><given-names>V.</given-names></name><name name-style="western"><surname>Lipson</surname><given-names>H.</given-names></name></person-group><article-title>Resilient machines through continuous self-modeling</article-title><source>Science</source><year>2006</year><volume>314</volume><fpage>1118</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1126/science.1133687</pub-id><pub-id pub-id-type="pmid">17110570</pub-id></element-citation></ref><ref id="B10-sensors-25-05849"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cully</surname><given-names>A.</given-names></name><name name-style="western"><surname>Clune</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tarapore</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mouret</surname><given-names>J.</given-names></name></person-group><article-title>Robots that can adapt like animals</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>503</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1038/nature14422</pub-id><pub-id pub-id-type="pmid">26017452</pub-id></element-citation></ref><ref id="B11-sensors-25-05849"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mahdavi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bentley</surname><given-names>P.</given-names></name></person-group><article-title>Innately adaptive robotics through embodied evolution</article-title><source>Auton. Robot.</source><year>2006</year><volume>20</volume><fpage>149</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1007/s10514-006-5941-6</pub-id></element-citation></ref><ref id="B12-sensors-25-05849"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silva</surname><given-names>F.</given-names></name><name name-style="western"><surname>Correia</surname><given-names>L.</given-names></name><name name-style="western"><surname>Christensen</surname><given-names>A.</given-names></name></person-group><article-title>Evolutionary online behaviour learning and adaptation in real robots</article-title><source>R. Soc. Open Sci.</source><year>2017</year><volume>4</volume><fpage>160938</fpage><pub-id pub-id-type="doi">10.1098/rsos.160938</pub-id><pub-id pub-id-type="pmid">28791130</pub-id><pub-id pub-id-type="pmcid">PMC5541525</pub-id></element-citation></ref><ref id="B13-sensors-25-05849"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Baldini</surname><given-names>P.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name></person-group><article-title>An Investigation of Graceful Degradation in Boolean Network Robots Subject to Online Adaptation</article-title><source>Communications in Computer and Information Science, Proceedings of the Artificial Life and Evolutionary Computation, Venice, Italy, 6&#8211;8 September 2023</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><volume>Volume 1977</volume><fpage>202</fpage><lpage>213</lpage></element-citation></ref><ref id="B14-sensors-25-05849"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Quick</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nehaniv</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dautenhahn</surname><given-names>K.</given-names></name><name name-style="western"><surname>Roberts</surname><given-names>G.</given-names></name></person-group><article-title>Evolving Embodied Genetic Regulatory Network-Driven Control Systems</article-title><source>Lecture Notes in Computer Science, Proceedings of the Advances in Artificial Life, Dortmund, Germany, 14&#8211;17 September 2003</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2003</year><fpage>266</fpage><lpage>277</lpage></element-citation></ref><ref id="B15-sensors-25-05849"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>Y.</given-names></name></person-group><article-title>A Hierarchical Gene Regulatory Network for Adaptive Multirobot Pattern Formation</article-title><source>IEEE Trans. Syst. Man Cybern. Part B (Cybern.)</source><year>2012</year><volume>42</volume><fpage>805</fpage><lpage>816</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TSMCB.2011.2178021</pub-id><pub-id pub-id-type="pmid">22311864</pub-id></element-citation></ref><ref id="B16-sensors-25-05849"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Barbieri</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kauffman</surname><given-names>S.A.</given-names></name></person-group><article-title>On the Criticality of Adaptive Boolean Network Robots</article-title><source>Entropy</source><year>2022</year><volume>24</volume><elocation-id>1368</elocation-id><pub-id pub-id-type="doi">10.3390/e24101368</pub-id><pub-id pub-id-type="pmid">37420388</pub-id><pub-id pub-id-type="pmcid">PMC9601899</pub-id></element-citation></ref><ref id="B17-sensors-25-05849"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>de S. Cavalcante</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gauthier</surname><given-names>D.</given-names></name><name name-style="western"><surname>Socolar</surname><given-names>J.</given-names></name><name name-style="western"><surname>Adams</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lathrop</surname><given-names>D.</given-names></name></person-group><article-title>Boolean chaos</article-title><source>Phys. Rev. E</source><year>2009</year><volume>80</volume><fpage>045202</fpage><pub-id pub-id-type="doi">10.1103/PhysRevE.80.045202</pub-id><pub-id pub-id-type="pmid">19905381</pub-id></element-citation></ref><ref id="B18-sensors-25-05849"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rosin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Rontani</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gauthier</surname><given-names>D.</given-names></name><name name-style="western"><surname>Sch&#246;ll</surname><given-names>E.</given-names></name></person-group><article-title>Experiments on autonomous Boolean networks</article-title><source>Chaos Interdiscip. J. Nonlinear Sci.</source><year>2013</year><volume>23</volume><fpage>025102</fpage><pub-id pub-id-type="doi">10.1063/1.4807481</pub-id><pub-id pub-id-type="pmid">23822500</pub-id></element-citation></ref><ref id="B19-sensors-25-05849"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kauffman</surname><given-names>S.</given-names></name></person-group><article-title>Metabolic stability and epigenesis in randomly constructed genetic nets</article-title><source>J. Theor. Biol.</source><year>1969</year><volume>22</volume><fpage>437</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1016/0022-5193(69)90015-0</pub-id><pub-id pub-id-type="pmid">5803332</pub-id></element-citation></ref><ref id="B20-sensors-25-05849"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kauffman</surname><given-names>S.</given-names></name></person-group><article-title>Homeostasis and Differentiation in Random Genetic Control Networks</article-title><source>Nature</source><year>1969</year><volume>224</volume><fpage>177</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/224177a0</pub-id><pub-id pub-id-type="pmid">5343519</pub-id></element-citation></ref><ref id="B21-sensors-25-05849"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lizier</surname><given-names>J.</given-names></name><name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zomaya</surname><given-names>A.</given-names></name></person-group><article-title>The Information Dynamics of Phase Transitions in Random Boolean Networks</article-title><source>Proceedings of the Artificial Life XI</source><conf-loc>Winchester, UK</conf-loc><conf-date>5&#8211;8 August 2008</conf-date><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2008</year><fpage>374</fpage><lpage>381</lpage></element-citation></ref><ref id="B22-sensors-25-05849"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Snyder</surname><given-names>D.</given-names></name><name name-style="western"><surname>Goudarzi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Teuscher</surname><given-names>C.</given-names></name></person-group><article-title>Finding Optimal Random Boolean Networks for Reservoir Computing</article-title><source>Proceedings of the Artificial Life 13</source><conf-loc>East Lansing, MI, USA</conf-loc><conf-date>19&#8211;22 July 2012</conf-date><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2012</year><fpage>259</fpage><lpage>266</lpage></element-citation></ref><ref id="B23-sensors-25-05849"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montagna</surname><given-names>S.</given-names></name><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name></person-group><article-title>The impact of self-loops on Boolean networks attractor landscape and implications for cell differentiation modelling</article-title><source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source><year>2021</year><volume>18</volume><fpage>2702</fpage><lpage>2713</lpage><pub-id pub-id-type="doi">10.1109/TCBB.2020.2968310</pub-id><pub-id pub-id-type="pmid">31985435</pub-id></element-citation></ref><ref id="B24-sensors-25-05849"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luque</surname><given-names>B.</given-names></name><name name-style="western"><surname>Sol&#233;</surname><given-names>R.</given-names></name></person-group><article-title>Lyapunov exponents in random Boolean networks</article-title><source>Phys. A Stat. Mech. Its Appl.</source><year>2000</year><volume>284</volume><fpage>33</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/S0378-4371(00)00184-9</pub-id></element-citation></ref><ref id="B25-sensors-25-05849"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bastolla</surname><given-names>U.</given-names></name><name name-style="western"><surname>Parisi</surname><given-names>G.</given-names></name></person-group><article-title>A Numerical Study of the Critical Line of Kauffman Networks</article-title><source>J. Theor. Biol.</source><year>1997</year><volume>187</volume><fpage>117</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1006/jtbi.1997.0423</pub-id><pub-id pub-id-type="pmid">9236114</pub-id></element-citation></ref><ref id="B26-sensors-25-05849"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lizier</surname><given-names>J.</given-names></name><name name-style="western"><surname>Prokopenko</surname><given-names>M.</given-names></name></person-group><article-title>A Fisher information study of phase transitions in random Boolean networks</article-title><source>Proceedings of the Artificial Life XII</source><conf-loc>Odense, Denmark</conf-loc><conf-date>19&#8211;23 August 2010</conf-date><publisher-name>The MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2010</year><fpage>305</fpage><lpage>312</lpage></element-citation></ref><ref id="B27-sensors-25-05849"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gershenson</surname><given-names>C.</given-names></name></person-group><article-title>Guiding the self-organization of random Boolean networks</article-title><source>Theory Biosci.</source><year>2012</year><volume>131</volume><fpage>181</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1007/s12064-011-0144-x</pub-id><pub-id pub-id-type="pmid">22127955</pub-id><pub-id pub-id-type="pmcid">PMC3414703</pub-id></element-citation></ref><ref id="B28-sensors-25-05849"><label>28.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barbieri</surname><given-names>E.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name></person-group><article-title>The Role of Dynamical Regimes of Online Adaptive BN-Robots in Noisy Environments</article-title><source>Communications in Computer and Information Science, Proceedings of the Artificial Life and Evolutionary Computation, Gaeta, Italy, 14&#8211;16 September 2022</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2023</year><fpage>183</fpage><lpage>194</lpage></element-citation></ref><ref id="B29-sensors-25-05849"><label>29.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kauffman</surname><given-names>S.</given-names></name></person-group><article-title>A Novel Online Adaptation Mechanism in Artificial Systems Provides Phenotypic Plasticity</article-title><source>Communications in Computer and Information Science, Proceedings of the Artificial Life and Evolutionary Computation, Winterthur, Switzerland, 15&#8211;17 September 2021</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><volume>Volume 1722</volume><fpage>121</fpage><lpage>132</lpage></element-citation></ref><ref id="B30-sensors-25-05849"><label>30.</label><element-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Baldini</surname><given-names>P.</given-names></name><name name-style="western"><surname>Roli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Braccini</surname><given-names>M.</given-names></name></person-group><article-title>Online adaptation of heterogeneous robot swarms to a dynamic task</article-title><source>Swarm Intell.</source><year>2025</year><comment><italic toggle="yes">submitted</italic></comment></element-citation></ref><ref id="B31-sensors-25-05849"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nelson</surname><given-names>A.</given-names></name><name name-style="western"><surname>Barlow</surname><given-names>G.</given-names></name><name name-style="western"><surname>Doitsidis</surname><given-names>L.</given-names></name></person-group><article-title>Fitness functions in evolutionary robotics: A survey and analysis</article-title><source>Robot. Auton. Syst.</source><year>2009</year><volume>57</volume><fpage>345</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2008.09.009</pub-id></element-citation></ref><ref id="B32-sensors-25-05849"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Floreano</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mondada</surname><given-names>F.</given-names></name></person-group><article-title>Automatic Creation of an Autonomous Agent: Genetic Evolution of a Neural-Network Driven Robot</article-title><source>Proceedings of the from Animals to Animats 3</source><conf-loc>Brighton, UK</conf-loc><conf-date>8&#8211;12 August 1994</conf-date><publisher-name>The MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>1994</year><fpage>421</fpage><lpage>430</lpage></element-citation></ref><ref id="B33-sensors-25-05849"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jakobi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Husbands</surname><given-names>P.</given-names></name><name name-style="western"><surname>Harvey</surname><given-names>I.</given-names></name></person-group><article-title>Noise and the reality gap: The use of simulation in evolutionary robotics</article-title><source>Lecture Notes in Computer Science, Proceedings of the Advances in Artificial Life, Granada, Spain, 4&#8211;6 June 1995</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>1995</year><fpage>704</fpage><lpage>720</lpage></element-citation></ref><ref id="B34-sensors-25-05849"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nolfi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bongard</surname><given-names>J.</given-names></name><name name-style="western"><surname>Husbands</surname><given-names>P.</given-names></name><name name-style="western"><surname>Floreano</surname><given-names>D.</given-names></name></person-group><article-title>Evolutionary Robotics</article-title><source>Springer Handbook of Robotics</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2016</year><fpage>2035</fpage><lpage>2068</lpage></element-citation></ref><ref id="B35-sensors-25-05849"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Christensen</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Dorigo</surname><given-names>M.</given-names></name></person-group><article-title>Incremental Evolution of Robot Controllers for a Highly Integrated Task</article-title><source>Lecture Notes in Computer Science, Proceedings of the From Animals to Animats 9, Rome, Italy, 25&#8211;29 September 2006</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2006</year><volume>Volume 4095</volume><fpage>473</fpage><lpage>484</lpage></element-citation></ref><ref id="B36-sensors-25-05849"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bonani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Longchamp</surname><given-names>V.</given-names></name><name name-style="western"><surname>Magnenat</surname><given-names>S.</given-names></name><name name-style="western"><surname>R&#233;tornaz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Burnier</surname><given-names>D.</given-names></name><name name-style="western"><surname>Roulet</surname><given-names>G.</given-names></name><name name-style="western"><surname>Vaussard</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bleuler</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mondada</surname><given-names>F.</given-names></name></person-group><article-title>The marXbot, a miniature mobile robot opening new perspectives for the collective-robotic research</article-title><source>Proceedings of the 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>18&#8211;22 October 2010</conf-date><publisher-name>Institute of Electrical and Electronics Engineers (IEEE)</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2010</year><fpage>4187</fpage><lpage>4193</lpage></element-citation></ref><ref id="B37-sensors-25-05849"><label>37.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bonani</surname><given-names>M.</given-names></name><name name-style="western"><surname>R&#233;tornaz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Magnenat</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bleuler</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mondada</surname><given-names>F.</given-names></name></person-group><article-title>Physical Interactions in Swarm Robotics: The Hand-Bot Case Study</article-title><source>Distributed Autonomous Robotic Systems: The 10th International Symposium</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2013</year><fpage>585</fpage><lpage>595</lpage></element-citation></ref><ref id="B38-sensors-25-05849"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pinciroli</surname><given-names>C.</given-names></name><name name-style="western"><surname>Trianni</surname><given-names>V.</given-names></name><name name-style="western"><surname>O&#8217;Grady</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pini</surname><given-names>G.</given-names></name><name name-style="western"><surname>Brutschy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Brambilla</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mathews</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ferrante</surname><given-names>E.</given-names></name><name name-style="western"><surname>Di Caro</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ducatelle</surname><given-names>F.</given-names></name><etal/></person-group><article-title>ARGoS: A Modular, Parallel, Multi-Engine Simulator for Multi-Robot Systems</article-title><source>Swarm Intell.</source><year>2012</year><volume>6</volume><fpage>271</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.1007/s11721-012-0072-5</pub-id></element-citation></ref><ref id="B39-sensors-25-05849"><label>39.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Baldini</surname><given-names>P.</given-names></name></person-group><article-title>Experiment Fault Recovery 2025&#8212;Code</article-title><year>2025</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://zenodo.org/records/16270398" ext-link-type="uri">https://zenodo.org/records/16270398</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-21">(accessed on 21 July 2025)</date-in-citation><pub-id pub-id-type="doi">10.5281/zenodo.16270398</pub-id></element-citation></ref><ref id="B40-sensors-25-05849"><label>40.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Baldini</surname><given-names>P.</given-names></name></person-group><article-title>Experiment Fault Recovery 2025&#8212;Data</article-title><year>2025</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://zenodo.org/records/16270251" ext-link-type="uri">https://zenodo.org/records/16270251</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-21">(accessed on 21 July 2025)</date-in-citation><pub-id pub-id-type="doi">10.5281/zenodo.16270251</pub-id></element-citation></ref><ref id="B41-sensors-25-05849"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Isermann</surname><given-names>R.</given-names></name></person-group><article-title>Supervision, fault-detection and fault-diagnosis methods&#8212;An introduction</article-title><source>Control Eng. Pract.</source><year>1997</year><volume>5</volume><fpage>639</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1016/S0967-0661(97)00046-4</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05849-f001" orientation="portrait"><label>Figure 1</label><caption><p>Representation of the arenas used in the experiment. (<bold>a</bold>) The arena for the collision avoidance task. The block in the center represents an obstacle to be avoided. (<bold>b</bold>) The arena for the phototaxis task. The yellow circle represents the light source, and the concentric lines starting from it represent the gradient of luminous radiation. As the arena is too big to be fully represented, here we just show its main parts. Both arenas contain a green entity representing the robot.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g001.jpg"/></fig><fig position="float" id="sensors-25-05849-f002" orientation="portrait"><label>Figure 2</label><caption><p>Representation of the points of occurrence of the damages considered in this work.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g002.jpg"/></fig><fig position="float" id="sensors-25-05849-f003" orientation="portrait"><label>Figure 3</label><caption><p>Average recovery of performance, calculated as the difference between the average performance at the end of the experiment and that of the exploitation epoch immediately after the occurrence of damage. This plot considers only replicas that learned the behavior without damages. (<bold>a</bold>) The recovery in the collision avoidance task. (<bold>b</bold>) The recovery in the phototaxis task.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g003.jpg"/></fig><fig position="float" id="sensors-25-05849-f004" orientation="portrait"><label>Figure 4</label><caption><p>Performance according to the intensity of the damage on the actuators in the collision avoidance task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 500 indicates a successful robot.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g004.jpg"/></fig><fig position="float" id="sensors-25-05849-f005" orientation="portrait"><label>Figure 5</label><caption><p>Performance according to the number of blind perceptions in the collision avoidance task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 500 indicates a successful robot.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g005.jpg"/></fig><fig position="float" id="sensors-25-05849-f006" orientation="portrait"><label>Figure 6</label><caption><p>Performance according to the number of fixed perceptions in the collision avoidance task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 500 indicates a successful robot.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g006.jpg"/></fig><fig position="float" id="sensors-25-05849-f007" orientation="portrait"><label>Figure 7</label><caption><p>Performance according to the number of random perceptions in the collision avoidance task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 500 indicates a successful robot.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g007.jpg"/></fig><fig position="float" id="sensors-25-05849-f008" orientation="portrait"><label>Figure 8</label><caption><p>Performance according to the intensity of the damage on the actuators in the phototaxis task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 0 indicates a successful controller.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g008.jpg"/></fig><fig position="float" id="sensors-25-05849-f009" orientation="portrait"><label>Figure 9</label><caption><p>Performance according to the number of blind perceptions in the phototaxis task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 0 indicates a successful controller.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g009.jpg"/></fig><fig position="float" id="sensors-25-05849-f010" orientation="portrait"><label>Figure 10</label><caption><p>Performance according to the number of fixed perceptions in the phototaxis task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 0 indicates a successful controller.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g010.jpg"/></fig><fig position="float" id="sensors-25-05849-f011" orientation="portrait"><label>Figure 11</label><caption><p>Performance according to the number of random perceptions in the phototaxis task. The trend is the average of the replicas that learned the behavior. Overall, a performance greater than 0 indicates a successful controller.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05849-g011.jpg"/></fig></floats-group></article></pmc-articleset>