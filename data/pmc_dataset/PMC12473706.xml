<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="review-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473706</article-id><article-id pub-id-type="pmcid-ver">PMC12473706.1</article-id><article-id pub-id-type="pmcaid">12473706</article-id><article-id pub-id-type="pmcaiid">12473706</article-id><article-id pub-id-type="pmid">41013007</article-id><article-id pub-id-type="doi">10.3390/s25185770</article-id><article-id pub-id-type="publisher-id">sensors-25-05770</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Systematic Review</subject></subj-group></article-categories><title-group><article-title>A Systematic Review of Techniques for Artifact Detection and Artifact Category Identification in Electroencephalography from Wearable Devices</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5192-5922</contrib-id><name name-style="western"><surname>Arpaia</surname><given-names initials="P">Pasquale</given-names></name><xref rid="af1-sensors-25-05770" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>De Luca</surname><given-names initials="M">Matteo</given-names></name><xref rid="af1-sensors-25-05770" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-9515-2923</contrib-id><name name-style="western"><surname>Di Marino</surname><given-names initials="L">Lucrezia</given-names></name><xref rid="af1-sensors-25-05770" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Duran</surname><given-names initials="D">Dunja</given-names></name><xref rid="af2-sensors-25-05770" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3305-2515</contrib-id><name name-style="western"><surname>Gargiulo</surname><given-names initials="L">Ludovica</given-names></name><xref rid="af3-sensors-25-05770" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7561-7724</contrib-id><name name-style="western"><surname>Lanteri</surname><given-names initials="P">Paola</given-names></name><xref rid="af4-sensors-25-05770" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1035-5740</contrib-id><name name-style="western"><surname>Moccaldi</surname><given-names initials="N">Nicola</given-names></name><xref rid="af1-sensors-25-05770" ref-type="aff">1</xref><xref rid="c1-sensors-25-05770" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Nalin</surname><given-names initials="M">Marco</given-names></name><xref rid="af5-sensors-25-05770" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Picciafuoco</surname><given-names initials="M">Mauro</given-names></name><xref rid="af5-sensors-25-05770" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Robbio</surname><given-names initials="R">Rachele</given-names></name><xref rid="af1-sensors-25-05770" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1399-2630</contrib-id><name name-style="western"><surname>Visani</surname><given-names initials="E">Elisa</given-names></name><xref rid="af2-sensors-25-05770" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Postolache</surname><given-names initials="O">Octavian</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Duraccio</surname><given-names initials="L">Luigi</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05770"><label>1</label>Department of Electrical Engineering and Information Technology (DIETI), University of Naples Federico II, 80131 Naples, Italy; <email>pasquale.arpaia@unina.it</email> (P.A.); <email>matteo.deluca@unina.it</email> (M.D.L.); <email>lucrezia.dimarino2@unina.it</email> (L.D.M.); <email>rachele.robbio@unina.it</email> (R.R.)</aff><aff id="af2-sensors-25-05770"><label>2</label>Epileptology Unit, Magnetoencephalography Laboratory, Fondazione IRCCS Istituto Neurologico Carlo Besta, 20133 Milan, Italy; <email>dunja.duran@istituto-besta.it</email> (D.D.); <email>elisa.visani@istituto-besta.it</email> (E.V.)</aff><aff id="af3-sensors-25-05770"><label>3</label>Institute of Industrial Technologies and Automation, National Council of Research (STIIMA-CNR), 20133 Milan, Italy; <email>ludovica.gargiulo@stiima.cnr.it</email></aff><aff id="af4-sensors-25-05770"><label>4</label>Neurophysiology Unit, Fondazione IRCCS Istituto Neurologico Carlo Besta, 20133 Milan, Italy; <email>paola.lanteri@istituto-besta.it</email></aff><aff id="af5-sensors-25-05770"><label>5</label>ab medica S.p.A., 20023 Cerro Maggiore, Italy; <email>nalin.marco@abmedica.it</email> (M.N.); <email>picciafuoco.mauro@abmedica.it</email> (M.P.)</aff><author-notes><corresp id="c1-sensors-25-05770"><label>*</label>Correspondence: <email>nicola.moccaldi@unina.it</email></corresp></author-notes><pub-date pub-type="epub"><day>16</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5770</elocation-id><history><date date-type="received"><day>04</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>05</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>16</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05770.pdf"/><abstract><p>Wearable electroencephalography (EEG) enables brain monitoring in real-world environments beyond clinical settings; however, the relaxed constraints of the acquisition setup often compromise signal quality. This review examines methods for artifact detection and for the identification of artifact categories (e.g., ocular) and specific sources (e.g., eye blink) in wearable EEG. A systematic search was conducted across six databases using the query: (&#8220;electroencephalographic&#8221; OR &#8220;electroencephalography&#8221; OR &#8220;EEG&#8221;) AND (&#8220;Artifact detection&#8221; OR &#8220;Artifact identification&#8221; OR &#8220;Artifact removal&#8221; OR &#8220;Artifact rejection&#8221;) AND &#8220;wearable&#8221;. Following PRISMA guidelines, 58 studies were included. Artifacts in wearable EEG exhibit specific features due to dry electrodes, reduced scalp coverage, and subject mobility, yet only a few studies explicitly address these peculiarities. Most pipelines integrate detection and removal phases but rarely separate their impact on performance metrics, mainly accuracy (71%) when the clean signal is the reference and selectivity (63%), assessed with respect to physiological signal. Wavelet transforms and ICA, often using thresholding as a decision rule, are among the most frequently used techniques for managing ocular and muscular artifacts. ASR-based pipelines are widely applied for ocular, movement, and instrumental artifacts. Deep learning approaches are emerging, especially for muscular and motion artifacts, with promising applications in real-time settings. Auxiliary sensors (e.g., IMUs) are still underutilized despite their potential in enhancing artifact detection under ecological conditions. Only two studies addressed artifact category identification. A mapping of validated pipelines per artifact type and a survey of public datasets are provided to support benchmarking and reproducibility.</p></abstract><kwd-group><kwd>EEG</kwd><kwd>artifact detection</kwd><kwd>artifact identification</kwd><kwd>artifact removal</kwd><kwd>wearable</kwd></kwd-group><funding-group><award-group><funding-source>Ministry of Enterprise and Made in Italy (MIMI)</funding-source></award-group><award-group><funding-source>Italian Ministry of Universities and Research (MUR)</funding-source><award-id>PNC0000007</award-id></award-group><funding-statement>This work was conducted within the &#8220;INTENSE&#8221; project, with financial support from the Ministry of Enterprise and Made in Italy (MIMI). It was also developed as part of the &#8220;ICT for Health&#8221; initiative, partially funded by the Italian Ministry of Universities and Research (MUR) through the FIT4MEDROB grant (PNC0000007).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05770"><title>1. Introduction</title><p>The application of EEG has evolved progressively over time, expanding across distinct domains [<xref rid="B1-sensors-25-05770" ref-type="bibr">1</xref>]. Initially, EEG was predominantly employed in clinical settings [<xref rid="B2-sensors-25-05770" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05770" ref-type="bibr">3</xref>], especially for the diagnosis of neurological disorders such as epilepsy [<xref rid="B4-sensors-25-05770" ref-type="bibr">4</xref>], Alzheimer&#8217;s disease [<xref rid="B5-sensors-25-05770" ref-type="bibr">5</xref>], Parkinson&#8217;s disease [<xref rid="B6-sensors-25-05770" ref-type="bibr">6</xref>], brain tumors [<xref rid="B7-sensors-25-05770" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05770" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05770" ref-type="bibr">9</xref>], and stroke [<xref rid="B10-sensors-25-05770" ref-type="bibr">10</xref>]. In parallel, EEG was widely used in neuroscience research to investigate nervous system functionality and brain dynamics, as well as to identify correlations between neural rhythms and specific cognitive or sensory states [<xref rid="B11-sensors-25-05770" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05770" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05770" ref-type="bibr">13</xref>]. In subsequent years, EEG gained traction in psychological contexts, particularly through the development of neurofeedback interventions. In these applications, real-time visual or auditory feedback of neural activity is used to promote self-regulation skills [<xref rid="B14-sensors-25-05770" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05770" ref-type="bibr">15</xref>]. Neurofeedback has been applied to support the treatment of attention and learning disorders, substance use disorders, traumatic brain injuries, and Autism Spectrum Disorder (ASD) [<xref rid="B15-sensors-25-05770" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05770" ref-type="bibr">16</xref>] as well as to enhance cognitive and motor performance in healthy individuals such as surgeons [<xref rid="B17-sensors-25-05770" ref-type="bibr">17</xref>] and athletes [<xref rid="B18-sensors-25-05770" ref-type="bibr">18</xref>]. In recent decades, EEG-based Brain&#8211;Computer Interfaces (BCIs) have been developed, including the P300 speller [<xref rid="B19-sensors-25-05770" ref-type="bibr">19</xref>] and motor-imagery-based systems [<xref rid="B20-sensors-25-05770" ref-type="bibr">20</xref>], with the aim of facilitating communication and control of external devices.</p><p>Nowadays, in addition to well-established domains such as (i) neuroscience research and (ii) healthcare, EEG is increasingly being applied across a wide range of emerging fields, including (iii) well-being and mental health [<xref rid="B21-sensors-25-05770" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05770" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05770" ref-type="bibr">23</xref>], (iv) entertainment, such as the development of medical-oriented games, attention-monitoring systems, and interfaces for controlling drones or humanoid robots [<xref rid="B24-sensors-25-05770" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05770" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05770" ref-type="bibr">26</xref>], (v) industrial settings, where EEG-based BCIs support safety, efficiency, and decision-making by tracking cognitive states, e.g., fatigue, stress, or alertness, particularly in repetitive or high-risk tasks using collaborative robots and machines [<xref rid="B21-sensors-25-05770" ref-type="bibr">21</xref>,<xref rid="B27-sensors-25-05770" ref-type="bibr">27</xref>], and (vi) professional and competitive sports, for the assessment and enhancement of motor performance [<xref rid="B28-sensors-25-05770" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05770" ref-type="bibr">29</xref>]. The expansion of EEG applications into a broader range of non-clinical domains has been made possible by the development of portable and wearable systems [<xref rid="B22-sensors-25-05770" ref-type="bibr">22</xref>,<xref rid="B30-sensors-25-05770" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05770" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05770" ref-type="bibr">32</xref>]. This trend has driven a strong acceleration in the wearable BCI market, with projected compound annual growth rates ranging from 8% to 17% over the next decade [<xref rid="B33-sensors-25-05770" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05770" ref-type="bibr">34</xref>].</p><p>In the healthcare sector, the emergence of the <italic toggle="yes">community medicine</italic> paradigm has fostered interest in accessible technologies offering diagnosis and therapy at the patient&#8217;s home. In this context, wearable EEG devices are regarded as a promising solution for expanding access to medical interventions in rehabilitation, as well as for enabling early and cost-effective screening across large segments of the population, thus representing a significant opportunity for public health [<xref rid="B35-sensors-25-05770" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05770" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05770" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05770" ref-type="bibr">38</xref>]. In addition, within the context of personalized medicine, wearable EEG devices enable real-time monitoring of therapeutic protocols, allowing dynamic adjustments to enhance treatment effectiveness [<xref rid="B39-sensors-25-05770" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05770" ref-type="bibr">40</xref>]. Furthermore, the ecological relevance of the measurement based on wearable technologies is improved, and psychologists consider EEG as a promising new psychometric tool for capturing cognitive and emotional states in real-world environments [<xref rid="B41-sensors-25-05770" ref-type="bibr">41</xref>,<xref rid="B42-sensors-25-05770" ref-type="bibr">42</xref>].</p><p>Although wearable EEG offers significant opportunities, caution persists within the clinical and neuroscientific communities, primarily because of concerns regarding its lower signal quality relative to conventional EEG systems [<xref rid="B43-sensors-25-05770" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05770" ref-type="bibr">44</xref>]. The main factors contributing to signal degradation in wearable EEG systems are (i) uncontrolled environments, (ii) in-motion conditions, and (iii) the adoption of dry or semi-wet electrodes for rapid setup. Operation in everyday environments limits the experimenter&#8217;s ability to mitigate environmental noise, such as electromagnetic interference [<xref rid="B45-sensors-25-05770" ref-type="bibr">45</xref>], exposes the system to the effects of natural movements, including high-intensity motion permitted to the user, and results in reduced electrode stability due to the absence of conductive gel [<xref rid="B46-sensors-25-05770" ref-type="bibr">46</xref>]. Additionally, the reduced number of channels, typically below sixteen [<xref rid="B47-sensors-25-05770" ref-type="bibr">47</xref>], limits spatial resolution and impairs the effectiveness of standard artifact rejection techniques based on source separation methods, such as Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [<xref rid="B48-sensors-25-05770" ref-type="bibr">48</xref>].</p><p>Most existing reviews on artifact detection target high-density EEG and physiological sources, in particular ocular, muscular, and cardiac [<xref rid="B49-sensors-25-05770" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-05770" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-05770" ref-type="bibr">51</xref>,<xref rid="B52-sensors-25-05770" ref-type="bibr">52</xref>]. Only a few surveys extend this analysis to non-physiological artifacts [<xref rid="B53-sensors-25-05770" ref-type="bibr">53</xref>,<xref rid="B54-sensors-25-05770" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-05770" ref-type="bibr">55</xref>]. Limited attention has been dedicated to evaluating algorithms designed for the specific artifact properties of wearable EEG systems. A notable exception is the study by Seok et al. (2021) [<xref rid="B56-sensors-25-05770" ref-type="bibr">56</xref>], presenting a structured overview of techniques for wearable EEG and photoplethysmography signal processing; however, its scope remains restricted to motion-related artifacts. The present review provides a systematic survey of methods designed to detect artifacts and to identify the specific artifact categories within EEG signals acquired through wearable devices. With respect to the previous literature, this review expands the scope by including a wider range of artifacts observed in wearable devices rather than focusing only on specific types. Moreover, category artifact identification assumes a novel relevance. Accurate identification of artifact categories is a critical step in enhancing signal quality in wearable EEG systems. Each artifact type exhibits distinct spatial, temporal, and spectral characteristics that require tailored detection and removal strategies [<xref rid="B54-sensors-25-05770" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-05770" ref-type="bibr">55</xref>,<xref rid="B57-sensors-25-05770" ref-type="bibr">57</xref>]. Without clear classification, processing pipelines risk applying overly generic solutions, which can be ineffective and may even compromise the integrity of neurophysiological components of interest [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>]. Artifact categorization also enables the design of modular and adaptive pipelines capable of addressing the complex and dynamic conditions typical of real-world recordings, including low-density or single-channel configurations [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>]. Such advancements are crucial to improving the reliability of wearable EEG analyses in both clinical and applied settings.</p><p>This review specifically focuses on wearable EEG devices and addresses the following research questions (RQs):<list list-type="bullet"><list-item><p>RQ-I: have any studies addressed the specific challenges of artifact management in wearable EEG systems?</p></list-item><list-item><p>RQ-II: which algorithms are available in the literature for artifact detection and artifact category identification in EEG signals acquired by wearable devices?</p></list-item><list-item><p>RQ-III: which parameters are used to assess the performance of artifact detection and artifact category identification algorithms?</p></list-item><list-item><p>RQ-IV: which assessment metrics are employed, and which reference signals are used to assess the performance of artifact detection and artifact category identification algorithms?</p></list-item></list></p></sec><sec id="sec2-sensors-25-05770"><title>2. Materials and&#160;Methods</title><sec><title>Search&#160;Strategy</title><p>The literature search was conducted across the databases Google Scholar, Scopus, PubMed, IEEE Xplore, Science Direct, and Web of Science by applying the following combined query in all searchable fields: (&#8220;electroencephalographic&#8221; OR &#8220;electroencephalography&#8221; OR &#8220;EEG&#8221;) AND (&#8220;Artifact detection&#8221; OR &#8220;Artifact identification&#8221; OR &#8220;Artifact removal&#8221; OR &#8220;Artifact rejection&#8221;) AND &#8220;wearable&#8221;. No restrictions were imposed on the year of publication. The most recent search was conducted on 24 April 2025. The article selection process was carried out systematically and transparently, following the PRISMA guidelines [<xref rid="B62-sensors-25-05770" ref-type="bibr">62</xref>]. During the pre-screening phase, only peer-reviewed articles published in journals or conference proceedings, written in English, and with full-text availability were considered, then the duplicates were excluded. In the screening phase, titles and abstracts were reviewed to eliminate studies considered irrelevant or inconsistent with the search query. Finally, during the eligibility phase, the full texts of the remaining studies were assessed based on the following exclusion criteria: (i) studies not focusing on wearable EEG systems (defined by having sixteen or fewer channels, or using dry electrodes, or assessing performance considering hardware efficiency metrics (power consumption, silicon area, and computational burden)); (ii) studies not referring to EEG artifact management; (iii) animal studies. An additional screening was conducted on the reference lists of the screened studies to further enhance the sample size and ensure comprehensive coverage of the literature. Only articles meeting all criteria were included in the final review to address the research questions. Data extracted from the included studies were organized using a standardized Excel template specifically designed by the research team. Four reviewers independently charted the data. The template was initially pilot-tested on a subset of four studies (i.e., one study per reviewer) to verify clarity and ensure consistency in data collection. Discrepancies were resolved through discussion and consensus, with the involvement of a third reviewer when consensus could not be reached. The PRISMA checklist is provided in the <xref rid="app1-sensors-25-05770" ref-type="app">Supplementary Materials</xref> to ensure transparency and adherence to reporting guidelines.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05770"><title>3. Results</title><p>The articles selection procedure is presented in <xref rid="sec3dot1-sensors-25-05770" ref-type="sec">Section 3.1</xref>. The temporal publication trend of the studies is discussed in <xref rid="sec3dot2-sensors-25-05770" ref-type="sec">Section 3.2</xref>, where it is compared with publication trend in generic wearable EEG device studies. <xref rid="sec3dot3-sensors-25-05770" ref-type="sec">Section 3.3</xref> offers a detailed overview about the acquisition setup and performance assessment methods of the algorithms proposed by the selected studies.The <xref rid="sec3dot4-sensors-25-05770" ref-type="sec">Section 3.4</xref> maps the artifact detection pipelines proposed in the literature for wearable EEG. Moreover, <xref rid="sec3dot5-sensors-25-05770" ref-type="sec">Section 3.5</xref> defines the artifact detection and artifact category identification strategies among the studies.</p><sec id="sec3dot1-sensors-25-05770"><title>3.1. PRISMA</title><p>A flow diagram detailing the results of identification, screening, eligibility, and inclusion phases is provided in <xref rid="sensors-25-05770-f001" ref-type="fig">Figure 1</xref>. The query applied to the six databases allowed the identification of 10,060 articles. After duplicate removal and the application of pre-screening criteria, 8512 records were excluded. The remaining 1548 articles underwent title- and abstract-based screening, leading to the exclusion of an additional 1492 studies. During the eligibility phase, full-text evaluation was conducted on 56 potentially relevant articles, with 2 further exclusions based on predefined exclusion criteria. To ensure comprehensive coverage, reference lists of the included studies were reviewed, resulting in the addition of four articles not captured in the initial search. In total, 58 studies met all criteria and were included in the qualitative analysis. To overcome the limited number of recent studies employing deep learning approaches identified in the initial search (only 2 papers after 2023), a modified search strategy was implemented [<xref rid="B63-sensors-25-05770" ref-type="bibr">63</xref>]. The keyword &#8216;wearable&#8217; was removed to relax the inclusion criteria and the keyword &#8216;deep learning&#8217; was added with the operator &#8216;AND&#8217; to increase the likelihood of identifying new and innovative deep learning solutions published after 2023. Consistency with the scope of the review was preserved by ensuring that all devices in the retrieved studies could be qualified as wearable systems, defined as employing fewer than sixteen channels or using dry electrodes. This modified search resulted in the inclusion of four additional studies.</p></sec><sec id="sec3dot2-sensors-25-05770"><title>3.2. Temporal Trends in Wearable EEG and Artifact&#160;Management</title><p>The analysis of the publication year for the collected articles reveals an almost stagnant publication trend. As an illustrative case, an advanced search on Scopus using the query (&#8216;EEG&#8217; AND &#8216;wearable&#8217;) was applied to titles, keywords, and abstracts. The analysis prioritized databases yielding a higher number of results. Among them, Scopus was selected over Google Scholar for further investigation, as it allows for field-specific searches and provides results with greater relevance to the research query. <xref rid="sensors-25-05770-f002" ref-type="fig">Figure 2</xref> shows a comparison of publication trends between articles on wearable EEG (blue) extracted from Scopus and studies collected in this review focusing on artifact detection and removal in wearable EEG (red).</p><p>The overall number of publications related to wearable EEG devices has steadily increased over the last decade, reaching approximately 440 articles in 2024. This trend confirms the expanding interest in wearable EEG across clinical, research, and applied contexts while simultaneously highlighting the limited attention devoted to artifact management. The observed discrepancy points to a significant gap in the current literature: the growing use of wearable EEG contrasts with the limited focus on artifact contamination, hindering the development of effective removal methods and the acquisition of reliable, high-quality signals.</p></sec><sec id="sec3dot3-sensors-25-05770"><title>3.3. Acquisition Setup and Performance Assessment Methods of the Algorithms Across the&#160;Studies</title><p>The current Section focuses on the main contents provided by the reviewed articles. In particular, <xref rid="sec3dot3dot1-sensors-25-05770" ref-type="sec">Section 3.3.1</xref> reports the description of the grid adopted to extract specific parameters from articles. <xref rid="sec3dot3dot2-sensors-25-05770" ref-type="sec">Section 3.3.2</xref> presents the results related to the acquisition setup parameters, whereas <xref rid="sec3dot3dot3-sensors-25-05770" ref-type="sec">Section 3.3.3</xref> details the results concerning the methods used to assess the algorithm performance.</p><sec id="sec3dot3dot1-sensors-25-05770"><title>3.3.1. Grid for Collecting Acquisition Setup Parameters and Performance Assessment&#160;Methods</title><p>The configuration of acquisition systems and methods for algorithms performance assessment, reported in the included studies, are provided in <xref rid="sensors-25-05770-t001" ref-type="table">Table 1</xref>. For each study, the targeted artifacts are specified, including their category and specific source, to outline the application domain of the algorithms. Experimental data are categorized as real (R) recordings, fully simulated (S) signals, or semi-simulated (SS) signals, the latter obtained by combining simulated or real artifacts with clean real recordings. For R and SS data, the table reports the number of subjects, number and duration of trials, and the specific task or experimental condition. For S data, only the trial structure is provided, as subject-specific details are not applicable. Channel configurations, including the number, type, and electrode placement, are detailed for R and SS data, while for S signals, only the number of channels processed by the algorithm is indicated. Subsequent columns report the reference signal used for validation, alongside performance parameters and metrics. The assessment parameters and corresponding metrics reported in the studies refer to the overall performance of the algorithms, generally including both artifact detection and removal. Consequently, although the metrics are directly associated with the output of the removal phase, they can also provide an indirect assessment of the detection phase, as the latter represents an essential preliminary step for the removal process. The final column <italic toggle="yes">Algorithm</italic> retains the original nomenclature used in each study for the proposed methods, typically including both detection and removal stages.</p></sec><sec sec-type="results" id="sec3dot3dot2-sensors-25-05770"><title>3.3.2. Results of the Acquisition Setup Parameter&#160;Collection</title><p>The artifacts examined in the studies are reported according to their categories and sources (see <italic toggle="yes">Focused Artifact</italic> in <xref rid="sensors-25-05770-t001" ref-type="table">Table 1</xref>). The artifact categories include ocular, muscular, cardiac, instrumental, Electromagnetic Interference (EMI), movement, and an unspecified category labeled as <italic toggle="yes">other</italic>.</p><p>Concerning the sources, for ocular artifacts, a distinction is made between eye blinks and eye movements. Eye blinks are transient events caused by rapid eyelid closure, generating large, short-lived deflections in the EEG signal [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>]. In contrast, eye movements involve voluntary or involuntary changes in gaze direction, characterized by longer durations and lower frequencies [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>,<xref rid="B128-sensors-25-05770" ref-type="bibr">128</xref>]. Muscular artifacts originate from Muscle Contraction (MC) contaminating the EEG signal, often associated with specific actions, usually involving multiple muscle groups simultaneously. For example, jaw motion engages the orbicularis oris, masseter, and temporalis, while chewing primarily activates the masseter and temporalis. Accordingly, sources of muscular artifacts include MC of Masseter (Ma), Temporalis (Te), Corrugator Supercilii (CS), Zygomaticus (Z), Orbicularis Oris (OOr), Orbicularis Oculi (OOc), Limb (Li), Nasalis (N), Submentalis (Subm), Shoulder (Sh), Pharyngeal (P), and Tongue (To). EMI artifacts mainly refer to Power-Line Noise (PLN) (50 or 60 Hz, depending on the geographical location of the recording source) [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>,<xref rid="B129-sensors-25-05770" ref-type="bibr">129</xref>]. Instrumental artifacts sources are (i) electrode pop, due to sudden changes in impedance at the electrode&#8211;scalp interface or unstable contacts [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>,<xref rid="B129-sensors-25-05770" ref-type="bibr">129</xref>]; (ii) electrode displacement, caused by the electrode moving relative to the scalp [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>]; (iii) Electrode Impedance Imbalance (EII), linked to variations in impedance or potential at the electrode&#8211;skin interface, frequently due to poor contacts, damaged cables, or insufficient use of conductive gel [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>,<xref rid="B128-sensors-25-05770" ref-type="bibr">128</xref>]; (iv) Thermal Electronics Noise (TEN), generated by the resistance of electronic components, characterized by a flat frequency spectrum [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>]; (v) clipping, occurring when the output voltage exceeds the amplifier&#8217;s handling capacity, resulting in a truncated signal [<xref rid="B130-sensors-25-05770" ref-type="bibr">130</xref>]. Instead, movement artifact sources correspond to (i) body movements [<xref rid="B129-sensors-25-05770" ref-type="bibr">129</xref>,<xref rid="B131-sensors-25-05770" ref-type="bibr">131</xref>] (e.g., during walking or running); (ii) head movements [<xref rid="B128-sensors-25-05770" ref-type="bibr">128</xref>] (e.g., head shaking); (iii) limb movements during daily activities [<xref rid="B127-sensors-25-05770" ref-type="bibr">127</xref>,<xref rid="B129-sensors-25-05770" ref-type="bibr">129</xref>] (e.g., typing or grasping objects); (iv) tremor [<xref rid="B128-sensors-25-05770" ref-type="bibr">128</xref>], either physiological or pathological (e.g., Parkinson&#8217;s disease [<xref rid="B131-sensors-25-05770" ref-type="bibr">131</xref>]). In cases where only the artifact category is provided, the related source information is denoted as <italic toggle="yes">Source Not Specified</italic> (<italic toggle="yes">S.N.S.</italic>).</p><p>In particular, <xref rid="sensors-25-05770-f003" ref-type="fig">Figure 3</xref> illustrates the percentage distribution of artifact categories and the corresponding sources explored. Ocular artifact represents the most frequently studied category (37%), confirming the priority given to removing eye blinks and eye movements, addressed in 29 and 26 studies, respectively. Muscular artifacts represent the second most addressed category (23%), although in many cases (10) the specific muscle location remains unreported. When specified, the most frequent sources include MaMC (14 cases), TeMC (9 cases), and CSMC (7 cases), followed by ZMC (5 cases), OOrMC (5 cases), and OOcMC (5 cases), with isolated cases for LiMC, NMC, SubMC, ShMC, PMC, and ToMC. Movement artifacts represent 16% of cases (10 studies evaluated body movements, 5 studies head movements, 6 studies limb movements, and 4 studies tremor). Instrumental artifacts represent 15% of cases, primarily related to cable movements (4 studies), electrode dislocations (5 studies), electrical impedance imbalances (3 studies), and electronic thermal noise (1 study). Instead, EMI, specifically PLN, represents 4% of cases. Finally, cardiac artifacts account for approximately 4% of cases (6 studies). The <italic toggle="yes">other</italic> category appears in a single study, Casadei et al. (2020) [<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>], analyzing artifacts characterized by large discontinuities in alpha activity. This study proposes a preliminary method applied to clinical EEG signals, highlighting its potential for long-term monitoring with wearable EEG, where such artifacts are more frequent. Additionally, the label <italic toggle="yes">S.N.S.</italic> appears in approximately 20 cases, most frequently for muscular artifacts, affecting the reproducibility of the proposed solution.</p><p>Regarding sample size (see <italic toggle="yes">Experimental Sample</italic> in <xref rid="sensors-25-05770-t001" ref-type="table">Table 1</xref>), the studies range from single-subject designs [<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>] to cohorts with over 100 participants [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B74-sensors-25-05770" ref-type="bibr">74</xref>,<xref rid="B99-sensors-25-05770" ref-type="bibr">99</xref>,<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>,<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>,<xref rid="B121-sensors-25-05770" ref-type="bibr">121</xref>,<xref rid="B125-sensors-25-05770" ref-type="bibr">125</xref>,<xref rid="B126-sensors-25-05770" ref-type="bibr">126</xref>], while the majority (66%) include between 5 and 30 subjects. Approximately 61% of the recordings are collected in resting-state or sleeping conditions, while the remaining 39% involve cognitive and/or motor task executions. In terms of data type, 59% of the studies employ R, 11% use fully S, and 30% rely on SS. Some studies validate the proposed algorithm on several datasets of the same type, as in Grosselin et al. (2019) [<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>] and Kumaravel et al. (2023) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>]. Others assess performance across different data types, testing the same algorithm on both R and S signals [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>,<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>,<xref rid="B71-sensors-25-05770" ref-type="bibr">71</xref>,<xref rid="B76-sensors-25-05770" ref-type="bibr">76</xref>,<xref rid="B95-sensors-25-05770" ref-type="bibr">95</xref>,<xref rid="B108-sensors-25-05770" ref-type="bibr">108</xref>,<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>] or on R and SS data [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>,<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>,<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>,<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>]. As regards the number of electrodes, in R and SS datasets, four studies use single-channel devices [<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>,<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>,<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>,<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>] and seven focus on a single channel extracted from multi-channel recordings [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>,<xref rid="B96-sensors-25-05770" ref-type="bibr">96</xref>,<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>,<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>,<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>,<xref rid="B120-sensors-25-05770" ref-type="bibr">120</xref>]. Three additional studies apply their methods to a limited number of channels compatible with wearable systems [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>,<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>,<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>]. A total of 36 studies employ acquisition systems with 2 to 16 channels. Some studies include devices with more than 16 electrodes (19 [<xref rid="B92-sensors-25-05770" ref-type="bibr">92</xref>,<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>], 21 [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>], and 22 [<xref rid="B106-sensors-25-05770" ref-type="bibr">106</xref>]), taken into account in the analysis due to their explicit focus on wearable EEG, either by using dry electrodes [<xref rid="B53-sensors-25-05770" ref-type="bibr">53</xref>] or by addressing hardware efficiency metrics such as power consumption, silicon area, and computational load [<xref rid="B92-sensors-25-05770" ref-type="bibr">92</xref>,<xref rid="B106-sensors-25-05770" ref-type="bibr">106</xref>,<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>]. Finally, in seven studies, the number of electrodes is not specified. Concerning electrode type, wet configurations remain predominant, covering approximately 61% of the sample, while the use of dry sensors reaches 24%. In the remaining 15% of cases, details about the electrode type are not reported. In S datasets, no physical acquisition setup can be involved, with all reviewed studies processing a single virtual channel in parallel.</p></sec><sec sec-type="results" id="sec3dot3dot3-sensors-25-05770"><title>3.3.3. Results of the Performance Assessment Methods&#160;Collection</title><p>Methods performance is assessed using a reference signal, specific parameters, and quantitative metrics. Some studies rely on self-produced data, while others make use of publicly available datasets reported in <xref rid="sensors-25-05770-t002" ref-type="table">Table 2</xref>.</p><p>The public datasets are categorized by data type, signal category, experimental protocol, participants, hardware setup, and signal processing phase. Despite the availability of most public datasets, a few were not accessible. Specifically, the EEGlab dataset [<xref rid="B143-sensors-25-05770" ref-type="bibr">143</xref>] could not be retrieved, while the dataset by [<xref rid="B142-sensors-25-05770" ref-type="bibr">142</xref>] is available only upon request. Among all the datasets examined, only one was explicitly developed to model movement-related artifacts typical of wearable devices [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]. The majority of the public datasets include R data, whereas only one [<xref rid="B114-sensors-25-05770" ref-type="bibr">114</xref>] also provides SS data. In terms of signal category, there is a clear predominance of physiological EEG data, often recorded during cognitive or motor tasks, or clinical EEG data, containing recordings from patients with neurological conditions (e.g., epilepsy or stroke). Several datasets also provide physiological (Electrooculogram (EOG), Electromyogram (EMG), Electrocardiogram (ECG)) or instrumental (cable movement) artifacts. In some cases, the artifact signals are recorded in a controlled manner, using protocols designed to induce eye blinks, eye movements, or cable movement. Whether using self-produced or public data, three main categories of reference signals can be identified across studies: (i) clean signal (S or R), (ii) artifact (S or R), and (iii) physiological signal (R). The clean signal can be simulated or real. In the case of simulated clean signals, Peng et al. (2013) [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>] and Zhao et al. (2014) [<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>] refer to the simulation method proposed by [<xref rid="B144-sensors-25-05770" ref-type="bibr">144</xref>], who generated clean EEG signals containing 1280 samples (5 s duration, 256 Hz sampling frequency) by filtering white noise through a seventh-order autoregressive model, established based on segments of real EEG recordings not containing visible artifacts. Rahman et al. (2015) [<xref rid="B71-sensors-25-05770" ref-type="bibr">71</xref>] adopted a simulation approach originally introduced by Yeung et al. (2004) [<xref rid="B145-sensors-25-05770" ref-type="bibr">145</xref>], although with a different purpose. In the former, the simulated signal was used as a reference to assess the performance of the eye-blink artifact removal method proposed, whereas in the latter, the obtained signal was employed to assess the validity of analytical methods previously employed to demonstrate that event-related potential (ERP) peaks originate from the synchronization of ongoing EEG oscillations. In both cases, EEG epochs (from &#8722;400 to 400 ms, 250 Hz sampling rate) were simulated by adding two phasic ERP peaks to background EEG noise constructed by summing 50 sinusoids with random frequencies (0.1&#8211;125 Hz) and phases (0&#8211;2&#960;) and a maximum amplitude of 20 &#956;V at 0.1 Hz. In both cases, the clean signal serves as reference for artifact removal algorithm performance assessment, mainly in terms of accuracy. In the case of real clean signal, reference selection occurs either before (ex ante) or after (ex post) signal acquisition, enabling performance assessment in terms of accuracy on both R and SS data. In ex ante cases, 30 or 60 s segments recorded before artifact onset from the same participant are used. During the acquisition, the subject remains still and relaxed to reduce movement-related artifacts [<xref rid="B67-sensors-25-05770" ref-type="bibr">67</xref>,<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>,<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>,<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>,<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>]. In other cases, to reduce ocular artifacts, reference data are commonly acquired during cross fixation tasks [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>,<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>] or under resting-state conditions with eyes open [<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>] or closed [<xref rid="B96-sensors-25-05770" ref-type="bibr">96</xref>]. An alternative reference is obtained involving controlled experimental settings: Sweeney et al. (2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>] and Mahmud et al. (2023) [<xref rid="B112-sensors-25-05770" ref-type="bibr">112</xref>] used an uncontaminated channel as reference by voluntarily generating the artifact on one channel (experimenter moves the target channel cable) and retaining the other for comparison. In ex post cases, clean signal segments are selected after acquisition. Some studies rely on visual inspection of the signal [<xref rid="B88-sensors-25-05770" ref-type="bibr">88</xref>,<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>], occasionally supported by synchronized video [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>]. In several cases, clean signal is obtained by using traditional artifact removal methods, particularly ICA, eventually combined with automated classification tools such as ICLabel [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>,<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>,<xref rid="B111-sensors-25-05770" ref-type="bibr">111</xref>,<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>,<xref rid="B125-sensors-25-05770" ref-type="bibr">125</xref>,<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>]. In one case [<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>], root mean square (RMS)-based criteria are applied to identify artifact-free segments. Similarly, the artifact signal can also be either simulated or real. Simulated artifacts are presented in only one study [<xref rid="B76-sensors-25-05770" ref-type="bibr">76</xref>], aimed at evaluating the algorithm&#8217;s capability to remove artifacts by comparing original and residual artifact amplitudes. In this case, the artifact is simulated using the Markov Process Amplitude (MPA) EEG model, where EEG oscillations (delta, theta, alpha, beta) are modeled as sinusoids with amplitudes governed by a first-order Gaussian&#8211;Markov stochastic process, parameterized to replicate the power spectrum of real EEG. In the case of real artifacts, contaminated signals are mainly used to evaluate classification metrics in order to assess detection performance. In Jayas et al. (2023) [<xref rid="B110-sensors-25-05770" ref-type="bibr">110</xref>], artifact annotation is automatically performed using built-in functions of the MNE-Python package, which applies predefined thresholds to detect EOG and EMG signals. In other studies [<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>,<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>], labels are manually assigned after artificially adding artifacts to previously clean signals. In Peh et al. (2022) [<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>] and Zhang et al. (2022) [<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>], pre-labeled datasets are used. In D&#8217;Rozario et al. (2015) [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>], labeling is conducted through visual inspection of the EEG signal. The physiological signal, generally labeled as raw data in the articles, is used as a reference for the assessment of removal algorithm selectivity, assuming a signal not completely artifact-free. In most cases, the reference is obtained through controlled experimental protocols, where artifact generation is guided while leaving portions of physiological signal intact. Majmudar et al. (2015) [<xref rid="B69-sensors-25-05770" ref-type="bibr">69</xref>] include an initial resting-state phase with eyes closed (10 s), followed by a phase involving blinking every 5 s. Chen et al. (2022) [<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>] provide vocal instructions to induce specific artifacts (blinking, chewing, frowning, eye/head movements) during six runs, each separated by 60 s of rest. Occhipinti et al. (2022) [<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>] define two distinct recording stages: a 4 min resting phase (2 with eyes open and 2 with eyes closed) and an active phase including artifact-inducing activities (speaking, chewing, walking in place at 120 bpm), each lasting 2 min. On the other hand, in some cases [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>,<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>,<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>,<xref rid="B108-sensors-25-05770" ref-type="bibr">108</xref>,<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>], no instructions during acquisition are explicitly described. Additionally, in studies incorporating auxiliary sensors, EOG [<xref rid="B71-sensors-25-05770" ref-type="bibr">71</xref>,<xref rid="B73-sensors-25-05770" ref-type="bibr">73</xref>] or EMG [<xref rid="B93-sensors-25-05770" ref-type="bibr">93</xref>] channels serve as references.</p><p>In terms of assessment parameters, accuracy (defined as the distance between the measured value and the true value) emerges as the most commonly used, being reported in 71% of the studies. In particular, accuracy is related to metrics such as RMS, Root Mean Squared Error (RMSE), Relative Root Mean Squared Error (RRMSE), Mean Square Error (MSE), or Mean Absolute Error (MAE) values (47% of the studies), followed by power or Signal-to-Noise Ratios (SNRs), including Difference in Signal-to-Noise Ratio (DSNR) and Peak Signal-to-Noise Ratio (PSNR), present in 29% of studies. Metrics with less than 10% frequency include Dynamic Time Warping (DTW), artifact reduction rate, spectral indices (Spectral Score, Distribution Score, Power Spectral Density (PSD)-based, Amplitude Modulation Rate of Change (AMRC), accuracy gain, amplitude or phase coherence, Zero Crossing Rate (ZCR), and Max-gradient). On the other hand, the parameter selectivity is reported in 63% of studies, based 45% on correlation metrics (Pearson&#8217;s correlation, Correlation Coefficient (CC), or coherence), followed by power or spectral metrics (23% of the studies), Mutual Information (MI) (10% of the studies), and Magnitude Square Coherence (MSC) plot (3% of the studies). Finally, distribution statistics such as skewness, kurtosis, or Autocorrelation Function (ACF) appear in 5% of the studies. Instead, classification performance metrics are evaluated in 29% of the studies. The most commonly reported metrics include classification accuracy (19% of the studies), True Positive Rate (TPR) (15% of the studies), and False Positive Rate (FPR) (12% of the studies). Precision and recall are reported in 3% of the studies, while False Positives per Minute (FPM) and False Positives per Hour in 3% of the studies. Metrics, such as Cohen&#8217;s k and Positive Predictive Value (PPV), are each employed in only one study, conducted by D&#8217;Rozario et al. (2015) [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>] and Zhang et al. (2022) [<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>], respectively. Another parameter is operational speed, based on latency as metrics, assessed in 13% of the studies. Moreover, the hardware efficiency parameter appears in 7% of studies. In particular, the related metric power consumption occurs in 4% of the studies, while the metrics computational burden and silicon area are each considered in only one study, conducted by Xing et al. (2024) [<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>] and Bahadur et al. (2024) [<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>], respectively. Only one study [<xref rid="B74-sensors-25-05770" ref-type="bibr">74</xref>] includes a clinical efficacy parameter, represented by a self-rating score for depression risk.</p><p>Finally, the algorithms (including both artifact detection and removal stages) are reported according to the classification proposed in [<xref rid="B57-sensors-25-05770" ref-type="bibr">57</xref>]. Specifically, the classification relies on the main method adopted, disregarding the decision rule method, also in order to achieve a more synthetic classification. The classes of methods considered in the reviewed studies are filtering methods, blind source separation methods, source decomposition methods, and combinations of different algorithms. In particular, filtering methods consist of conventional or adaptive filters, including Kalman filter, Adaptive Noise Canceler (ANC), Least Mean Square (LMS), Recursive Least-Square (RLS), or multi-channel adaptive filter (MCAF). Blind source separation methods refer to techniques employing ICA, Canonical Correlation Analysis (CCA), and Singular Spectrum Analysis (SSA). Instead, source decomposition methods include algorithms based on wavelet transform, Empirical Mode Decomposition (EMD), Ensemble Empirical Mode Decomposition (EEMD), Multivariate Empirical Mode Decomposition (MEMD), Fast Multivariate Empirical Mode Decomposition (FMEMD), Variational Mode Extraction (VME), or Morphological Component Analysis (MCA). Finally, the combinations of different algorithms may involve Wavelet-ICA (wICA), Discrete Wavelet Transform (DWT), and ANC or methods involving machine learning algorithms (e.g., Support Vector Machine (SVM), K-means, decision trees, random and gradient boosting) based on supervised/unsupervised learning applied to temporal or frequency-domain features. In addition to the aforementioned classes, a group of approaches not addressed in these classifications can be identified, particularly, deep learning methods, which include a variety of neural network models for artifact detection and removal (e.g., Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU), Multilayer Perceptron (MLP), autoencoders, Generative Adversarial Network (GAN), EEGIFNet); ASR-based methods (e.g., ASR, rASR, MEMD-ASR). Furthermore, several studies propose individual algorithms that do not fit neatly into the proposed classes, using specific mathematical or statistical strategies. Examples include a model-based amplitude estimation [<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>] extracting components of a modeled alpha sinusoidal wave, an algorithm based on statistical features for artifact detection [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>] identifying segments using predefined thresholds on standard deviation, a strategy by evaluating several morphological parameters (e.g., amplitude, symmetry, slope) [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>], a Multiscale modified-distribution entropy (M-mDistEn) algorithm [<xref rid="B97-sensors-25-05770" ref-type="bibr">97</xref>] based on entropy measure quantifying signal irregularity across multiple time scales, an approach combining features such as trend slope, standard deviation, peaky distribution, and spectral power to detect EEG segments with artifacts (using <italic toggle="yes">pop_rejtrend, pop_jointprob, pop_rejkurt, pop_rejcont, EEGLAB</italic> functions) [<xref rid="B75-sensors-25-05770" ref-type="bibr">75</xref>].</p><p><xref rid="sensors-25-05770-f004" ref-type="fig">Figure 4</xref> illustrates the distribution of algorithm classes across the reviewed studies, with combinations of different algorithms emerging as the most prevalent, and source decomposition methods being the least employed.</p><p>Focusing on the performance results, large variability emerges across studies due to differences in artifact considered, recording setup, and algorithmic strategy. Filtering and wavelet-based methods typically report SNR improvements between 5 and 15 dB for ocular artifacts, while others (e.g., Kim et al. (2015) [<xref rid="B70-sensors-25-05770" ref-type="bibr">70</xref>] using ANC on motor-task data) observed marginal or even negative improvements depending on task complexity. RMSE and RRMSE values usually fall within 0.1&#8211;0.6. Lower errors are reported for wavelet&#8211;thresholding on ocular artifacts (Shahbakhti et al. (2021) [<xref rid="B95-sensors-25-05770" ref-type="bibr">95</xref>], Zhang et al. (2022) [<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>]), whereas higher values are observed in semi-simulated datasets containing several or muscular artifacts (e.g., Cheng et al. (2019) [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>], Butkevi&#269;i&#363;t&#279; et al. (2019) [<xref rid="B86-sensors-25-05770" ref-type="bibr">86</xref>], Islam et al. (2020) [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>]). Classification-based performance reach accuracies above 90% in several cases (e.g., D&#8217;Rozario et al. (2015) [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>], Ingolfsson et al. (2024) [<xref rid="B124-sensors-25-05770" ref-type="bibr">124</xref>], Chen et al. (2022) [<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>]). Latency values, when provided, range from tens of milliseconds in lightweight filtering (e.g., Matiko et al. (2013) [<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>]) with Adaptive Predictive Filtering (APF) + (DWT) to several seconds in more complex frameworks such as ASR or Deep Learning-based methods (Blum et al. (2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>], Zhang et al. (2022) [<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>]).</p></sec></sec><sec id="sec3dot4-sensors-25-05770"><title>3.4. Parameters of Artifact Detection Pipelines Across the&#160;Studies</title><p>In this Section, a focus on the detection algorithms is made. In particular, the parameters used at each stage of the detection pipeline are presented (<xref rid="sensors-25-05770-t003" ref-type="table">Table 3</xref>). The table outlines the pre-processing, the epoching, the feature extraction, the feature selection, and the classification methods or decision rules used to differentiate artifact-contaminated segments from clean data or identify a specific artifact source. Finally, it reports the spatial specificity of the detection process in terms of the channels involved: (i) single-channel, operating on individual channels independently; (ii) multi-channels, using information from several channels without processing the entire montage; or (iii) all-channels, simultaneously analyzing all available channels. A focus on ASR algorithm, considered relevant for wearable EEG devices, is presented in <xref rid="sec3dot4dot1-sensors-25-05770" ref-type="sec">Section 3.4.1</xref>.</p><p>Pre-processing procedures across studies commonly involve the application of band-pass, high-pass, and low-pass filters. Band-pass filters are frequently set within 0.5&#8211;40 Hz to preserve EEG bands of interest and reduce drift and power line noise [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>,<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>,<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>], with upper limits extending to 60&#8211;64 Hz [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>,<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>] and in some workflows up to 70 Hz [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>,<xref rid="B88-sensors-25-05770" ref-type="bibr">88</xref>,<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>,<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>]. High-pass filters are commonly configured with cut-off frequencies at 0.1 Hz [<xref rid="B73-sensors-25-05770" ref-type="bibr">73</xref>], 0.25 Hz [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>], 0.3 Hz [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>], 0.5 Hz [<xref rid="B96-sensors-25-05770" ref-type="bibr">96</xref>,<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>,<xref rid="B110-sensors-25-05770" ref-type="bibr">110</xref>], or 1 Hz [<xref rid="B75-sensors-25-05770" ref-type="bibr">75</xref>,<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>,<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>,<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>,<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>]. Low-pass filters are generally applied with cut-off values between 35 and 45 Hz [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>,<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>,<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>,<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>], although some studies extend this range up to 60&#8211;64 Hz for the analysis of lower gamma band activity [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>]. A notch filter at 50 or 60 Hz is also commonly employed (17 studies) to suppress power line interference. In some cases, it is applied alongside a band-pass filter with an upper cut-off below 50 Hz, aiming to eliminate residual noise persisting after preliminary filtering [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>,<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>,<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>]. In other cases, a broader band-pass filter extending up to 65 Hz is used, with the notch filter selectively removing the power line component within this frequency range [<xref rid="B53-sensors-25-05770" ref-type="bibr">53</xref>,<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B82-sensors-25-05770" ref-type="bibr">82</xref>,<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>].</p><p>Additional steps, including normalization and detrending, are incorporated in certain pipelines to optimize signals for feature extraction or neural network input [<xref rid="B125-sensors-25-05770" ref-type="bibr">125</xref>,<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>]. Pre-processing is sometimes applied uniformly across signals, either clean or with artifact [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>,<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>,<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>], while in other workflows, each type of signal is pre-processed differently [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>,<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>,<xref rid="B88-sensors-25-05770" ref-type="bibr">88</xref>,<xref rid="B93-sensors-25-05770" ref-type="bibr">93</xref>,<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>,<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>,<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>].</p><p>In terms of epoching, approximately 56% of studies employ windows with durations of 2 s or less. Excluding the 20% of cases without reported details, the remaining workflows typically use windows up to 10 s, with the exception of Peng et al. (2013) [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>] and Dey et al. (2020) [<xref rid="B92-sensors-25-05770" ref-type="bibr">92</xref>], analyzing epochs of 40 and 128 s, respectively. Several studies [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>,<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>,<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>,<xref rid="B105-sensors-25-05770" ref-type="bibr">105</xref>,<xref rid="B110-sensors-25-05770" ref-type="bibr">110</xref>,<xref rid="B112-sensors-25-05770" ref-type="bibr">112</xref>,<xref rid="B119-sensors-25-05770" ref-type="bibr">119</xref>,<xref rid="B124-sensors-25-05770" ref-type="bibr">124</xref>] investigate the influence of window length on algorithm performance by testing multiple lengths. Instead, Kumaravel et al. (2021) [<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>] examine the role of overlapping in artifact detection, using 10 s windows with overlap percentages ranging from 50% to 75%.</p><p>With regard to feature extraction and classification, one of the most frequent approaches uses wavelet transform alongside thresholding, primarily for ocular artifact removal [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>,<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>,<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>] and instrumental artifacts [<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>], applied in both single-channel [<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>,<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>] and all-channel configurations [<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>]. In particular, for ocular artifacts, wavelets enable transient localization in the time&#8211;frequency domain by decomposing signals into frequency bands, allowing thresholding to be applied directly on the wavelet coefficients of low-frequency components for detection (for instance, Bahadur et al. (2024) [<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>] and Peng et al. (2013) [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>]). Furthermore, Kaongoen et al. (2023) [<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>] employ stationary wavelet transform combined with ASR to decompose single-channel EEG signals into components of equal sample length, attempting to address challenges associated with the limited number of channels in low-density systems. The combination of ICA with thresholding is widely applied for ocular artifact removal [<xref rid="B82-sensors-25-05770" ref-type="bibr">82</xref>] as well, and often also for muscular artifacts and PLN [<xref rid="B75-sensors-25-05770" ref-type="bibr">75</xref>], cardiac [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>], or movement artifacts [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>], in single-channel [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>], multi-channel [<xref rid="B75-sensors-25-05770" ref-type="bibr">75</xref>], and all-channel configurations [<xref rid="B82-sensors-25-05770" ref-type="bibr">82</xref>,<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>]. ICA separates statistically independent components, while thresholding identifies and removes those containing artifacts. Studies differ in the features used for thresholding and in the ICA variants adopted to enhance component separation, beyond channel specificity and targeted artifacts. For instance, Val Calvo et al. (2019) [<xref rid="B82-sensors-25-05770" ref-type="bibr">82</xref>] employ wICA and Enhanced Automatic wICA (EAWICA), analyzing kurtosis and multi-scale sample entropy in the first case, and kurtosis together with Renyi entropy in the second one. Cheng et al. (2019) [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>] applies the classic ICA approach, evaluating the autocorrelation of the extracted sources. In contrast, some workflows apply ICA without thresholding, as seen in Sweeney et al. (2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>] and Islam et al. (2020) [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>], both addressing cable movement artifact detection, with the latter also targeting body and limb movements. Sweeney et al. (2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>] employ EEMD-ICA, adaptive filter and Kalman filter, according to the artifact, using manual detection based on signal shape, frequency, and amplitude. Islam et al. (2020) [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>] use Infomax ICA, assessing independent components through topographic maps, spectral analysis, and autocorrelation in an all-channel framework. The FFT combined with thresholding is used in pipelines targeting muscular artifacts alongside ocular and movement artifacts [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>] and also PLN [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>]. In this context, FFT computes the spectral power distribution, while thresholding is applied to parameters such as average power [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>] or high-frequency energy for calculating indices like kurtosis and Median Absolute Deviation (MAD) [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>], with implementations in single-channel [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>] and multi-channel [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>] settings. In another case, Matiko et al. (2013) [<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>] apply Short-Time Fourier Transform (STFT). Five studies implement ASR-based pipelines for the ocular artifact [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>], in combination with muscular artifacts [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>], movement [<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>,<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>], or also with cardiac and instrumental artifacts [<xref rid="B106-sensors-25-05770" ref-type="bibr">106</xref>]. In these methods, the decision rule typically involves thresholding based on the mean and standard deviation of the calibration signal, with differences arising from the features used. Kumaravel et al. (2021) [<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>] and Xiao et al. (2022) [<xref rid="B106-sensors-25-05770" ref-type="bibr">106</xref>] apply traditional ASR, using PCA on sliding windows to compare component energy against calibration thresholds. Arpaia et al. (2022) [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>] and Blum et al. (2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>] additionally test the Riemannian ASR variant, replacing PCA with PGA, and projecting data onto geodesic sub-manifolds. Finally, Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>] propose MEMD as a preliminary step, allowing ASR to operate on the extracted IMFs. A unique case is presented in the study of Rosanne et al. (2019) [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>], characterized by multiple algorithms for ocular and movement artifact removal using thresholding in an all-channel configuration, each paired with specific feature extraction methods. The evaluated pipelines include (i) ASR with short-window variance derived through PCA; (ii) Automatic Artifact Removal (AAR) based on Second Order Blind Identification (SOBI), assessing cross-correlation matrices and fractal dimensions to identify ocular components; (iii) SOBI-based wICA; (iv) wICA preceded by a wavelet decomposition step for IC extraction; and (v) Infomax ICA, using kurtosis, spatial average, variance difference, spatial eye difference, and discontinuity analysis. Studies using EMD and variants show different approaches for the removal of EEG artifacts. Occhipinti et al. (2022) [<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>] apply Noise-Assisted Multivariate EMD (NA-MEMD) to decompose multi-channel signals into IMFs synchronized across channels. The signals are considered free of muscular and movement artifacts when their power spectral density remains below a defined threshold. Chen et al. (2022) [<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>] integrate EMD with wavelet analysis through Empirical Wavelet Transform (EWT), subsequently applying DWT to the IMFs and comparing the pipeline with Maximal Overlap Discrete Wavelet Transform (MODWT)-Multiresolution Analysis (MRA) and CCA. In this case, six features are computed on each component and used within a one-class SVM to detect ocular, muscular, movement, and EMI artifacts. Finally, Narmada et al. (2023) [<xref rid="B111-sensors-25-05770" ref-type="bibr">111</xref>] propose an EMD-DWT combination optimized using the Opposition Searched&#8211;Elephant Herding Optimization (OS-EHO) algorithm in order to enhance the efficiency of ocular, cardiac, and muscular artifact removal while preserving physiological EEG components. Thresholding is also applied in CCA-based algorithms for muscular artifact removal, as presented by Liu et al. (2019) [<xref rid="B88-sensors-25-05770" ref-type="bibr">88</xref>] and Liu et al. (2021) [<xref rid="B93-sensors-25-05770" ref-type="bibr">93</xref>]. In Liu et al. (2019) [<xref rid="B88-sensors-25-05770" ref-type="bibr">88</xref>], CCA is combined with Fast Multivariate EMD (FMEMD) to extract IMFs and compute signal autocorrelation. A feature selection phase is then performed, randomly selecting between three and eight channels from the original trace for subsequent artifact removal stages. Liu et al. (2021) [<xref rid="B93-sensors-25-05770" ref-type="bibr">93</xref>], instead, test three different variants: (i) CCA with autocorrelation coefficients evaluated across all channels; (ii) EEMD-CCA applied to IMFs in a single-channel setup; and (iii) MEMD-CCA for analyzing multivariate IMFs in a multi-channel configuration. Additionally, an adaptive RLS filter is tested on single-channel data with EMG reference, allowing evaluation of coefficients associated with EMG components by minimizing a weighted least squares cost function. Singular cases include Noorbasha et al. (2020) [<xref rid="B91-sensors-25-05770" ref-type="bibr">91</xref>] and Jiang et al. (2023) [<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>], both addressing ocular artifact detection. Noorbasha et al. (2020) [<xref rid="B91-sensors-25-05770" ref-type="bibr">91</xref>] apply Singular Value Decomposition (SVD) with MDL-based subspace recognition to identify and extract ocular components from single-channel EEG. Jiang (2023) employ VME to isolate mode functions, subsequently evaluated through thresholding for artifact detection. Additionally, some workflows integrate feature extraction methods as wavelet, FFT, and ICA, with supervised classifiers as SVM, k-Nearest Neighbors (kNN), and Random Forest for detecting movement artifacts [<xref rid="B70-sensors-25-05770" ref-type="bibr">70</xref>], ocular and muscular artifacts [<xref rid="B74-sensors-25-05770" ref-type="bibr">74</xref>], eye blinks [<xref rid="B77-sensors-25-05770" ref-type="bibr">77</xref>], and, in some cases, also instrumental artifacts [<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>]. Studies vary in classifier selection, chosen according to dataset characteristics, and in the potential inclusion of a feature selection step before classification. For instance, Ingolfsson et al. (2022) [<xref rid="B99-sensors-25-05770" ref-type="bibr">99</xref>] employ TPOT to select energy features derived from DWT and FFT, subsequently applying binary, multi-class, and multi-class multi-output classification. In a later study, Ingolfsson et al. (2024) [<xref rid="B124-sensors-25-05770" ref-type="bibr">124</xref>] use TPOT during training for feature selection of similar energy features, combined with Minimal Cost-Complexity Pruning (MCCP) for embedded model construction. Moreover, Dey et al. (2020) [<xref rid="B92-sensors-25-05770" ref-type="bibr">92</xref>] apply correlation analysis and a <italic toggle="yes">t</italic>-test for feature selection on time series extracted after windowing, followed by MLP-based classification of the analyzed windows. Finally, deep learning-based models commonly combine convolutional layers with fully connected layers using softmax or sigmoid activation functions for classification. This approach represents about 15% of the analyzed cases, targeting ocular and muscular artifacts [<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>,<xref rid="B118-sensors-25-05770" ref-type="bibr">118</xref>,<xref rid="B120-sensors-25-05770" ref-type="bibr">120</xref>,<xref rid="B126-sensors-25-05770" ref-type="bibr">126</xref>], movement [<xref rid="B119-sensors-25-05770" ref-type="bibr">119</xref>,<xref rid="B121-sensors-25-05770" ref-type="bibr">121</xref>,<xref rid="B125-sensors-25-05770" ref-type="bibr">125</xref>,<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>], or power line interferences [<xref rid="B105-sensors-25-05770" ref-type="bibr">105</xref>]. Feature extraction relies on convolutional feature maps automatically generated by the convolutional layers, encoding complex patterns associated with both artifacts and physiological EEG activity, and enabling it to distinguish artifactual components automatically. Classification typically involves fully connected layers with softmax or sigmoid activations, without explicit thresholding using the argmax or the intrinsic sigmoid threshold for decision-making. Recent approaches employing autoencoder and UNet architectures, as proposed by Mahmud et al. and Saleh et al., implement clean signal reconstruction without a separate classification phase, integrating detection and removal within the reconstruction process. The majority of methods are applied in single-channel configurations, with Nair et al. (2025) [<xref rid="B126-sensors-25-05770" ref-type="bibr">126</xref>] representing an exception by extending the algorithm to two channels simultaneously.</p><p>The following paragraph focuses on the hyperparameters associated with each of the methods under investigation. In wavelet-based methods, the mother wavelet type is most frequently Daubechies (approximately 60% of the wavelet studies), followed by Haar (about 30%), with decomposition levels most often between 5 and 7 (around 70% of the studies). ICA-based pipelines differ in the number of extracted components: in about 55% of cases, the number of ICs was set equal to the number of channels. ASR approaches reported different best cutoff values, most commonly in the range 4&#8211;9 (70% of ASR-based studies), but occasionally up to 25 (20%), with window lengths of 0.3&#8211;0.5 s in approximately half of the cases. For FFT and STFT methods, window functions such as Hanning in 60% of the cases, with FFT lengths most often set to 1024 points in the reported implementations, are specified. For CCA and EMD variants, the number of channels used ranges from 3 to 8 in about 40% of the cases, while the remaining studies employed all available channels; noise channels and the number of directions in E(M)EMD were reported in about 50% of these studies. In CNN models, kernel size is typically mono dimensional and small (&#8804;3 in over 65% of cases), input segment length is most frequently 0.5&#8211;2 s (60%), with Adam or AdamW optimizers applied in over 80% of cases.</p><sec id="sec3dot4dot1-sensors-25-05770"><title>3.4.1. ASR
and Its Relevance for Wearable EEG Artifact&#160;Management</title><p>ASR is an algorithm used for removing high-amplitude artifacts, such as eye blinks, muscle activity, and movement-related artifacts, from EEG data [<xref rid="B146-sensors-25-05770" ref-type="bibr">146</xref>,<xref rid="B147-sensors-25-05770" ref-type="bibr">147</xref>]. Therefore, ASR appears promising for application with EEG wearable devices, allowing recordings to be performed during motor tasks. In fact, wearable EEG devices experience more frequent artifacts, largely because they operate in dynamic, everyday conditions such as locomotion [<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>,<xref rid="B97-sensors-25-05770" ref-type="bibr">97</xref>]. In contrast to common considerations, the essence of ASR is not only based on the application of PCA itself but rather encompasses subspace reconstruction accounting for statistical deviations from a clean signal model. In this regard, Kim et al. (2025) [<xref rid="B146-sensors-25-05770" ref-type="bibr">146</xref>] expect promising results from replacing PCA with ICA or other methods, although they have not tested them yet. In the first step, ASR identifies clean reference segments. The signal is divided into epochs and, for each epoch, a wise-channel root mean square (RMS) is computed. A z-score-based criterion is then applied across the RMS values of all channels, and an epoch is rejected only if its z-score falls outside the acceptance range in up to 7.5% of the channels. From the remaining clean epochs, covariance matrices are computed, and their median is used to define the reference matrix. The eigenvalue decomposition of the reference matrix provides the basis of the principal components for subsequent analysis. The data are then projected into the component space. Within the principal component space, ASR estimates rejection thresholds for each component. In particular, the RMS of each principal component is computed, and an acceptance interval, depending on its mean and variance scaled by a sensitivity parameter &#8220;k&#8221;, is defined. When applied to raw EEG data, each new epoch is projected into the same principal component space. For each epoch, only those principal components whose RMS falls within the predefined acceptance interval are retained. The inverse transformation of the retained components reconstructs the artifact-cleaned signal.</p><p>Modified implementations of the ASR algorithm have been adopted in several studies. Blum et al. (2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>] and Arpaia et al. (2022) [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>] employ the Riemannian Artifact Subspace Reconstruction (rASR) approach. rASR uses Riemannian geometry to process covariance matrices, which are symmetric positive-definite (SPD) and reside in a curved manifold. rASR computes the Karcher mean (Riemannian equivalent of the arithmetic mean) to average covariance matrices. It also replaces PCA with Principal Geodesic Analysis (PGA) to generalize dimensionality reduction to the curved manifold of SPD matrices. Another variant of ASR, proposed by Xiao et al. (2022), applies a Fast Fourier Transform (FFT) and uses signal energy within specific frequency bands for calibration instead of RMS. This ensures that the reference data are relevant to the subsequent analysis by focusing on frequency bands of interest. Conversely, Kumaravel et al. (2023) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>] develop a method leveraging accelerometer data from Inertial Measurement Units (IMUs) to replace the conventional calibration step of the ASR algorithm. This approach is based on the principle that the magnitude of accelerometer data directly correlates with motion artifacts in the EEG signal. By identifying outlier segments in the accelerometer data, the method can effectively detect contaminated EEG data, providing a practical solution for real-time analysis in mobile EEG systems. In addition, several studies have proposed combined approaches integrating ASR with other methods, aiming to enhance its performance beyond the standard implementation. Rosanne et al. (2019) [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>] explored two such strategies: (i) ASR combined with Automatic EEG artifact detector based on the joint use of spatial and temporal features (ADJUST), where the components are automatically identified as artifactual and subsequently removed after applying ICA, and (ii) ASR combined with Wavelet-ICA, which applies wavelet-based thresholding to ICA components to effectively suppress high-amplitude artifacts such as eye blinks. Kaongoen et al. (2023) [<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>] further introduced other method combinations. By first decomposing single-channel EEG signals using singular spectrum analysis (SSA), ensemble empirical mode decomposition (EEMD), or wavelet transform (WT), they generated multiple subcomponents that were then processed with ASR. This design adapted ASR to a single-channel context, highlighting how signal decomposition can expand ASR applicability to scenarios with limited spatial information. Finally, Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>] proposed multivariate empirical mode decomposition (MEMD)-ASR, where MEMD provides a channel-consistent decomposition into scale-aligned intrinsic mode functions (IMFs) before ASR is applied. Unlike approaches that decompose channels individually, MEMD preserves inter-channel relationships and avoids the loss of information introduced by transformations, thus supplying ASR with input that is both higher in dimensional integrity and better aligned across channels.</p><p>Focusing on the stand-alone use of ASR, its effectiveness is highly sensitive to the values of user-defined parameters. Two main parameters are crucial for ASR [<xref rid="B146-sensors-25-05770" ref-type="bibr">146</xref>]: the window length and the cutoff threshold <italic toggle="yes">k</italic>. A typical window length is 0.5 s; however, optimal performance is typically found with <italic toggle="yes">k</italic> values between 10 and 30. An excessively long window may fail to capture transient artifacts when using RMS, whereas an overly low <italic toggle="yes">k</italic> value can lead to the removal of valid neural signals, and an overly high <italic toggle="yes">k</italic> value may result in insufficient artifact suppression. The parameter and configurations used in the collected studies are <italic toggle="yes">k</italic> = 7 and window length = 0.5 s (Rosanne et al. (2019) [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>]); <italic toggle="yes">k</italic> = 1 and window length = 0.3 s (Blum et al. (2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>]); <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (Kumaravel et al. (2021) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>]); <italic toggle="yes">k</italic> = 15, 25 for ASR, and <italic toggle="yes">k</italic> = 2, 5 for rASR (Arpaia et al. (2022) [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>]); <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and window length = 0.5 s (Kaongoen et al. (2023) [<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>]); <italic toggle="yes">k</italic> = 7&#8211;12 and window length = 0.5 s (Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>]). In particular, Rosanne et al.&#8217;s (2019) [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>] results indicate that ASR-ADJUST was the most effective under medium-movement conditions, while ASR-wICA performed best during high movement. Blum et al. (2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>] demonstrate ASR produces overcorrection with <italic toggle="yes">k</italic> = 5), which required less aggressive settings. The alternative rASR, using a lower <italic toggle="yes">k</italic>, preserved Evoked Potential morphology while being computationally more efficient (82% reduction in runtime compared to ASR), highlighting that lower <italic toggle="yes">k</italic> values can be advantageous in low-amplitude, visually evoked paradigms. Kumaravel et al. (2023) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>] explore <italic toggle="yes">k</italic> across conditions of ocular and movement contamination in Steady-State Visual Evoked Potentials (SSVEP) paradigms, performing ASR by (i) removing artifacted segments (Removal mode) and (ii) reconstructing the signal (Correction mode). Optimal <italic toggle="yes">k</italic> values were frequency- and mode-dependent: <italic toggle="yes">k</italic> = 10&#8211;20 for 2&#8211;4 Hz and <italic toggle="yes">k</italic> = 15&#8211;40 for 8 Hz. Moreover, the Removal mode of the ASR required slightly higher <italic toggle="yes">k</italic> (&#8804;8) to avoid excessive rejection of data segments, while Correction mode allowed lower thresholds. Importantly, IMU-ASR achieved comparable performance to standard ASR while reducing computation time by &#8764;94%. In low-channel configurations, Arpaia et al. (2022) [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>] compare ASR (with <italic toggle="yes">k</italic> = 15 and 25) and rASR (with <italic toggle="yes">k</italic> = 2 and 5) against ICA and PCA. Results show that ASR preserved baseline EEG activity more effectively than ICA or PCA, which either distorted clean signals or failed in few-channel settings. While rASR was computationally efficient, its low <italic toggle="yes">k</italic> values tended to excessively modify clean EEG. In the study by Kaongoen et al. (2023) [<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>], performance peaks at very low cutoff thresholds (<italic toggle="yes">k</italic> = 5 for a movement dataset; <italic toggle="yes">k</italic> = 4 for an ocular dataset) and deteriorates with higher values. Finally, Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>] report optimal performance at <italic toggle="yes">k</italic> = 9 with 4 channels. Compared to ASR alone, MEMD-ASR avoids overcorrection.</p></sec></sec><sec id="sec3dot5-sensors-25-05770"><title>3.5. Artifact Detection vs. Artifact Category Identification&#160;Strategies</title><p>The artifact category discrimination capability of the reviewed methods was assessed according to the level of detail in artifact source differentiation. As illustrated in <xref rid="sensors-25-05770-f005" ref-type="fig">Figure 5</xref>, the subsequent strategies were identified: (i) approaches addressing multiple artifact categories in a source-unspecific manner (33 studies); (ii) methods detecting multiple sources within the same artifact category (21 studies); (iii) methods targeting a single artifact source (17 studies); (iv) methods attempting to identify distinct artifact sources (2 studies). Most studies in the first category adopt a binary approach, distinguishing clean and contaminated signals without specifying the artifact category present in the affected epochs. For instance, Ingolfsson et al. (2022) [<xref rid="B99-sensors-25-05770" ref-type="bibr">99</xref>] propose a Multi-class Multi-output Classification (MMC) system capable of detecting twelve artifact sources, including ocular, instrumental, muscular, and movement-related artifacts. The system labels each time window and channel as either clean or contaminated, without differentiating between artifact categories. Moreover, Li et al. (2023) [<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>] demonstrate the generalizability of their detection method across ocular, muscular, and movement artifacts and show that segmenting the signal into clean and contaminated portions enhances artifact removal performance in terms of accuracy and selectivity. Grosselin et al. (2019) [<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>] propose an algorithm representing an early step toward artifact category identification. Specifically, the study evaluates EEG signal quality from single-channel wearable devices by distinguishing among clean signals, broadly contaminated signals (ocular and movement-related), and muscular artifacts (masseter and temporalis), thereby enabling the identification of specific muscular artifact sources. Zhang et al. (2021) [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>] and Inoue et al. (2019) [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>] stand out as the only studies fully meeting the criteria of the fourth category. Inoue et al. (2019) [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>] develop an approach relying on amplitude thresholds and spatio-temporal symmetry to identify blinks, eye movements, and muscular artifacts. Zhang et al. (2021) [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>] propose a two-stage pipeline for single-channel EEG, separately identifying and removing ocular, muscular artifacts, and PLN. In particular, the first stage detects the artifact category, and the second applies targeted removal, preserving clean signal segments.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05770"><title>4. Discussion</title><p>This review investigated four main aspects in the literature: the specific challenges of artifact management in wearable EEG systems (RQ-I), the algorithms used for artifact detection and classification (RQ-II), the parameters (RQ-III), and the metrics and reference signals (RQ-IV) used to assess algorithm performance. In the following sections, the discussion is organized into three paragraphs for RQ-I, RQ-II, and RQ-III, together with RQ-IV. Finally, in <xref rid="sec4dot4-sensors-25-05770" ref-type="sec">Section 4.4</xref>, emerging perspectives and limitations of the present review are reported.</p><sec id="sec4dot1-sensors-25-05770"><title>4.1. Specific Challenges
of Artifact Management in Wearable EEG&#160;Systems</title><p>The literature shows a notable delay in addressing the unique challenges introduced by wearable EEG technologies. Only a limited number of studies explicitly consider the heightened complexity posed by these systems, particularly in managing movement-related artifacts. Such artifacts are more frequent and severe in wearable EEG due to their operation in dynamic, real-life settings (e.g., walking, running) [<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>,<xref rid="B97-sensors-25-05770" ref-type="bibr">97</xref>]. They are characterized by high amplitude, non-stationarity, and broad spectral content that overlaps with neural signals, severely limiting the efficacy of traditional filtering techniques. Artifacts from cable motion are especially problematic due to their transient, irregular patterns and lack of time-locking with physical movements [<xref rid="B148-sensors-25-05770" ref-type="bibr">148</xref>]. Additionally, the literature underrepresents the impact of electrode type. Dry electrode systems, increasingly used in wearable EEG, are more prone to artifacts due to the absence of conductive gel, which in wet systems helps stabilize the electrode&#8211;skin interface and maintain low impedance [<xref rid="B56-sensors-25-05770" ref-type="bibr">56</xref>,<xref rid="B67-sensors-25-05770" ref-type="bibr">67</xref>]. A further gap concerns the limited use of auxiliary signals for artifact detection and removal in wearable EEG. In traditional EEG systems, additional channels such as EMG and EOG are commonly employed to monitor and subtract specific sources of contamination. However, these solutions are less compatible with wearable applications. In the context of wearable EEG, more suitable alternatives include sensors such as accelerometers, microphones, and other miniaturized devices. Although some studies have begun to integrate these wearable-compatible sensors (e.g., IMUs for motion detection [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>], ETI for impedance monitoring [<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>], microphones for vibration localization [<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>]), their adoption remains limited. A notable exception within the broader scientific consensus is the work of Sweeney et al. (2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>], who employed gold-standard, non-wearable equipment to preserve a physiological reference signal recorded near an artifact-contaminated site. The artifact was induced through deliberate cable movement. His study stands out for its explicit recognition of the unique challenges posed by wearable EEG systems, particularly the difficulty in distinguishing signal from artifact under the novel, low-SNR conditions that characterize real-world, mobile recordings.</p><p>In addition, several artifact sources specific to wearable EEG remain largely unaddressed in the reviewed literature. For example, fluctuations in the reference channel-to-ground potential [<xref rid="B149-sensors-25-05770" ref-type="bibr">149</xref>], such as those caused by foot movement, can introduce noise at the electrode level. While this issue is typically minimized in wired systems sharing a common ground with the ADC, it may be exacerbated in wireless configurations. Similarly, although wireless systems offer increased immunity to conducted electrical noise, they remain vulnerable to radiated interference, including electric arcs or transient surges from nearby electronic devices. Exposure to common-mode noise, such as environmental EMI, is also more critical in wearable contexts [<xref rid="B150-sensors-25-05770" ref-type="bibr">150</xref>,<xref rid="B151-sensors-25-05770" ref-type="bibr">151</xref>]. While powerline noise is occasionally considered, other radiated or device-induced interferences are rarely explored. This is particularly problematic given the uneven distribution of noise across electrodes, especially with dry sensors, which undermines the effectiveness of conventional solutions like the Driven Right Leg (DRL) circuit [<xref rid="B152-sensors-25-05770" ref-type="bibr">152</xref>]. In such conditions, generating a stable counter-phase signal becomes unreliable and may even amplify noise. Another neglected issue concerns impedance fluctuations due to sweating, especially in dry electrode systems [<xref rid="B153-sensors-25-05770" ref-type="bibr">153</xref>]. These variations can cause slow signal drifts that standard preprocessing pipelines struggle to correct. Finally, the application of conventional artifact removal strategies for low-density and single-channel configurations is not easily applicable also due to the reduced scalp coverage [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>,<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>,<xref rid="B76-sensors-25-05770" ref-type="bibr">76</xref>,<xref rid="B91-sensors-25-05770" ref-type="bibr">91</xref>,<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>].</p></sec><sec id="sec4dot2-sensors-25-05770"><title>4.2. Algorithms for Artifact Detection and&#160;Classification</title><p>In general, the algorithmic pipelines described in the literature include two main stages: artifact detection/identification and artifact removal. Most detection modules perform a binary identification of artifact presence, without distinguishing between different categories or sources. Some algorithms are validated on multiple artifact types, which shows a certain level of generalization capability. However, this generalization applies only to the detection phase and does not extend to the identification of artifact types. For this reason, their usefulness remains limited when more detailed information is needed for further processing. Only two studies attempt to implement a classification of artifact category or source [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>]. In the removal phase, several algorithms are designed and tested assuming that the artifact type is already known. This allows for good performance in controlled scenarios. However, it limits their applicability in real-world conditions, where artifacts are often mixed and not labeled in advance. These algorithms are rarely tested on signals that reflect the variability of real physiological recordings, which reduces their practical value. At the same time, the high specificity of these algorithms in removing well-defined artifact types makes them promising for modular pipelines. A two-step architecture, with (i) source identification followed by (ii) targeted artifact removal, could be an effective approach for wearable EEG applications [<xref rid="B54-sensors-25-05770" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-05770" ref-type="bibr">55</xref>,<xref rid="B57-sensors-25-05770" ref-type="bibr">57</xref>]. Among the most robust approaches for binary artifact detection, Ingolfsson et al. (2022) [<xref rid="B99-sensors-25-05770" ref-type="bibr">99</xref>] propose an MMC system capable of detecting twelve artifact sources, including ocular, instrumental, muscular, and movement-related artifacts. The system labels each time window and channel as either clean or contaminated, without differentiating between artifact categories. Furthermore, Li et al. (2023) [<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>] not only demonstrate the generalizability of their detection method across EOG, EMG, and movement artifacts but also show that segmenting the signal into clean and contaminated portions improved artifact removal performance in terms of both accuracy and selectivity. Some studies represent early steps toward effective artifact category identification. For example, Grosselin et al. (2019) [<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>] assessed EEG quality from single-channel wearable devices, distinguishing clean, generically contaminated (ocular and movement), and muscle-contaminated signals (masseter and temporalis), thus providing the basis for specific muscular artifact sources identification.</p><p>Zhang et al. (2021) [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>] and Inoue et al. (2019) [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>] increase the number of artifact categories targeted for identification. In particular, Inoue et al. (2019) [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>] introduce a method for distinguishing blinks, eye movements, and muscular artifacts using amplitude thresholds and spatio-temporal symmetry criteria. However, the procedure is not clearly described, and the algorithm has been tested on only one subject, limiting the assessment of the method&#8217;s actual discrimination capability. On the other hand, Zhang et al. (2021) [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>] propose a two-stage pipeline for the detection and removal of ocular, muscular artifacts, and powerline noise in a single-channel EEG system. The identification stage relies on a cascade of three dedicated detection blocks, each targeting specific artifact categories and sources. When a block successfully detects artifacts within its scope, the identification process terminates; otherwise, the signal is forwarded to the next block. This work provides the most promising results for artifact category identification, although it does not perform source-level analysis for ocular and muscular artifacts. Many proposed detection methods are developed and validated on signals containing specific artifact categories or sources. Although these approaches may not generalize well when processing real-world signals, they provide a valuable foundation. After assessing their behavior against artifact categories beyond those used for validation, such methods could serve as building blocks for designing cascade-based identification systems composed of specialized detection modules. Wavelet transforms and ICA, both using thresholding as the artifact decision rule, are commonly employed to manage ocular and muscular artifacts. ASR-based pipelines, often integrating PCA and thresholding, are widely applied to address ocular, movement, and instrumental artifacts. Supervised classification approaches (e.g., SVM, kNN, Random Forest) are typically coupled with feature extraction to detect muscle and motion artifacts.</p><p>The gap in artifact categorization within the selected studies likely stems from the cultural background of the authors of the papers. Predominantly associated with the disciplinary fields of computer science and engineering, they tend to maintain an approach focused on implementing automated removal solutions, favoring a black-box approach that prioritizes algorithmic efficacy over an in-depth understanding of the underlying neurophysiologic phenomena. This is evidenced by the scarcity of comprehensive databases mapping the full spectrum of artifact types, contrasting with existing datasets that are typically limited to single artifact classes (see <xref rid="sensors-25-05770-t002" ref-type="table">Table 2</xref>).</p><p>Regarding the epoching for the algorithm application, most studies do not provide a clear rationale for the choice of a specific length. However, Jayas et al. (2023) [<xref rid="B110-sensors-25-05770" ref-type="bibr">110</xref>], using machine learning techniques, explore the optimal segment length by testing durations of 1 s, 2 s, and 5 s, with 5 s epoch yielding the best performance.</p><p>In addition, several studies involve adapting techniques originally developed for high-density EEG systems to low-channel configurations [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>,<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>,<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>,<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>,<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>,<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>,<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>,<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>]. However, authors do not always offer a critical consideration of using these algorithms in low-channel settings. For instance, ICA is often applied in low-density configurations without discussing the limitations introduced by reduced spatial coverage [<xref rid="B82-sensors-25-05770" ref-type="bibr">82</xref>,<xref rid="B87-sensors-25-05770" ref-type="bibr">87</xref>]. Indeed, source separation techniques, as ICA, are particularly effective when applied with 32 or more electrodes [<xref rid="B48-sensors-25-05770" ref-type="bibr">48</xref>]. Nonetheless, Arpaia et al. (2022) [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>] experimentally demonstrated the decline in performance of ICA using fewer than eight channels, suggesting that methods like ASR may be better suited for low-cost and wearable EEG systems. Instead, Cheng et al. (2019) [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>] proposed a strategy to overcome the low-channel constraint by using SSA to create artificial multi-channel datasets from single-channel signals, enabling ICA application even in the absence of real multi-channel data.</p></sec><sec id="sec4dot3-sensors-25-05770"><title>4.3. Performance Assessment Parameters, Metrics, and Reference&#160;Signals</title><p>The results regarding the performance assessment of the algorithms are not exclusively associated with either the detection or the removal phase but rather refer to the combined sequence of both phases. Three main categories of reference signals are used: (i) clean signals, (ii) artifact signals, and (iii) physiological signals. Clean signals can be simulated or real. In the case of real clean signals, these are typically obtained through two approaches. The first is an ex ante strategy, where the subject is instructed to remain still or relaxed during data acquisition in order to minimize artifact generation. However, this does not ensure the complete absence of contamination. The second is an ex post strategy, where clean segments are selected after acquisition. This can be performed using traditional artifact removal algorithms or through visual inspection, which introduces subjectivity and depends on the user&#8217;s level of expertise. In the case of simulated clean signals, most approaches rely on the random generation of signals with partial attempts to model neurophysiological patterns, although they only partially capture the dynamic and nonlinear complexity of typical physiological EEG activity [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>,<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>,<xref rid="B71-sensors-25-05770" ref-type="bibr">71</xref>]. Clean signals are mainly used to assess the accuracy. In this review, accuracy is defined according to the International Vocabulary of Metrology (VIM3) [<xref rid="B154-sensors-25-05770" ref-type="bibr">154</xref>] as the closeness of agreement between a measured value and a true value of a measurand. As a result, in this context, the reference signal cannot be considered a true value in a narrow metrological sense.</p><p>Some studies adopt artifact signals as a reference for detection performance assessment. Typically real and not simulated, reference artifact signals allow for the computation of standard classification metrics such as classification accuracy (the percentage of correct predictions out of the total predictions). Physiological signals, representing partially contaminated real recordings, are also used as references. These signals are particularly useful for evaluating selectivity, i.e., the algorithm&#8217;s ability to preserve brain activity while removing artifacts [<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>].</p><p>Further parameters are assessed independently of the EEG signal. Hardware efficiency is rarely evaluated, despite its relevance for real-time and embedded implementations of the algorithms in ecological conditions. For instance, the related metric power consumption occurs in only three studies [<xref rid="B92-sensors-25-05770" ref-type="bibr">92</xref>,<xref rid="B106-sensors-25-05770" ref-type="bibr">106</xref>,<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>]. Remarkably, the metrics computational burden [<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>] and silicon area [<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>] are each considered in only one study.</p><p>An analysis providing the main findings, with a focus on robustness, metrics, and hyperparameter sensitivity of the proposed methods is reported below in order to deeply assess the challenges of dry electrodes or movement-related artifacts in wearable EEG scenarios. The analysis groups the studies according to the main artifact category focused on, namely ocular, muscular, and movement artifacts.</p><p>With respect to ocular artifacts recorded with dry electrodes, most studies rely on wavelet-based or multicomponent decomposition methods. Early approaches (Matiko et al. (2013) [<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>], Peng et al. (2013) [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>], Zhao et al. (2014) [<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>]) demonstrate moderate success but are highly sensitive to the choice of mother wavelet, number of decomposition levels, and windowing parameters, limiting reproducibility across datasets. More recent work, such as the MEMD-ASR approach (Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>]), tests different cutoff thresholds (k between 7 and 12; k = 9 resulted as the best value) and multivariate empirical mode decomposition, showing improved robustness, especially with few-channel dry systems. Despite these advances, the lack of standardized performance metrics (e.g., SNR vs. CC vs. RRMSE) prevents direct numerical comparisons, though qualitative evidence suggests that ASR-based pipelines offer superior preservation of neural signal integrity.</p><p>For muscular artifacts recorded using dry electrodes, Grosselin et al. (2019) [<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>] and Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>] adopt different approaches, though the two studies remain only partially comparable due to methodological and metric differences. Grosselin et al. (2019) employ a classification-based strategy, reporting high accuracy (92.2 &#177; 2.2%) and SNR-dependent performance (e.g., 99.8% accuracy at &lt;0 dB, dropping to 43.1% at 10 dB). This highlights the potential of supervised learning but also its dependence on SNR levels and training data quality. By contrast, Arpaia et al. (2024) introduce the MEMD-ASR pipeline, tailored for non-stationary artifacts and robust even with very few channels.</p><p>Regarding movement-related artifacts using dry electrodes, classical adaptive filtering approaches (Mihajlovi&#263; et al. (2014) [<xref rid="B67-sensors-25-05770" ref-type="bibr">67</xref>]) or ICA-based pipelines (Islam et al. (2020) [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>]) show improvements in SNR and coherence but require either additional reference channels or careful manual selection of independent components to avoid removing neural information. Sweeney et al. (2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>] demonstrate that integrating accelerometer references into Kalman or EEMD-ICA pipelines yields substantial SNR gains, though at the cost of complexity and processing time. Mihajlovi&#263; et al. (2014) [<xref rid="B67-sensors-25-05770" ref-type="bibr">67</xref>] introduce impedance-based filtering (ETI-MCAF), efficient in real time but less reliable at low frequencies (&lt;2 Hz). Also, M-mDistEn (Aung et al. (2021) [<xref rid="B97-sensors-25-05770" ref-type="bibr">97</xref>]) shows high performance but requires parameter tuning. More recently, IMU-ASR (Kumaravel et al. (2023) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>]) and MEMD-ASR (Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>]) achieved robust results with simpler configurations and improved computational efficiency. The IMU-ASR framework (Kumaravel et al. (2023) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>]) represents a significant step forward, replacing computationally expensive calibration with inertial measurements, thus reducing processing time (&#8764;94%) while maintaining performance. Optimal k thresholds (10&#8211;40, depending on frequency and removal vs. correction mode) allow fine-tuning of artifact suppression without excessive signal loss, making this method particularly suitable for real-time or wearable applications. In Rosanne et al. (2019) [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>], combinations of ASR and other methods achieved better results than ADJUST, Wavelet-ICA, Harvard automated processing pipeline for electroencephalography (HAPPE), AAR, or AAR+wICA: ASR-ADJUST performed best under medium movement and ASR-wICA under high movement during mental workload conditions. Similarly, MEMD-ASR pipelines (Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>]) demonstrated resilience even in minimal-channel settings (2&#8211;4 electrodes).</p><p>Instead, several studies evaluate artifact suppression in the context of movement artifacts and cable-induced instrumental artifacts across wet and dry electrode setups. Methods such as Kalman filtering or EEMD-ICA (Sweeney et al. (2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]) deliver strong SNR improvements (&#8764;9&#8211;10 dB), albeit with high computational cost and dependency on reference accelerometers. ASR-based hybrid pipelines (SSA-ASR and WT-ASR) (Kaongoen et al. (2023) [<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>]) reach the highest <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> SNR (&#8764;15 dB) against ICA, SSA, WT, EEMD, with low k thresholds (4&#8211;5), underscoring the sensitivity of ASR tuning. Similarly, MEMD-ASR (Arpaia et al. (2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>]) and rASR (Blum et al. (2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>]), against traditional ASR, prove highly effective, particularly in preserving morphological features and operating efficiently in low-channel systems. These findings indicate that ASR, especially when integrated with complementary decomposition methods (such as SSA, WT, MEMD), offers a flexible and computationally efficient solution for artifact suppression.</p><p>Across all scenarios, ASR consistently outperforms or matches alternative methods (ICA, PCA, HAPPE), particularly in conditions with movement artifacts, limited channels, or real-time requirements. Parameter tuning, especially the cutoff threshold k, is critical for balancing artifact suppression and signal preservation. Empirically, moderate k values (4&#8211;9) perform best for ocular or moderate movement artifacts, while higher thresholds (&#8764;10&#8211;25 or up to 40 for high-frequency SSVEP tasks) are preferable in removal modes or high-noise conditions. In fact, the selection of ASR parameters could depend on the specific task being performed. For instance, in [<xref rid="B155-sensors-25-05770" ref-type="bibr">155</xref>], it is highlighted how the choice of ASR cut-off parameter should be adapted to the level of movement-related artifacts in the EEG data. Specifically, tasks that induce greater movement contamination tend to require lower cut-off values to achieve effective reconstruction, comparable to the amount of data typically removed during manual cleaning. For example, in tasks such as the single-leg stance and n-back, the optimal cut-off ranges align with values previously reported in the literature, such as the 5&#8211;7 range suggested by Mullen et al. (2015) [<xref rid="B156-sensors-25-05770" ref-type="bibr">156</xref>] for BCI applications with dry electrodes and the 20&#8211;30 range adjusted by Chang et al. (2019) [<xref rid="B157-sensors-25-05770" ref-type="bibr">157</xref>] for EEG data recorded during simulated driving. However, for the walking task, lower cut-off parameters are necessary, likely due to the higher level of movement artifacts compared to less dynamic conditions such as simulated driving. This evidence underscores the importance of task-specific calibration of ASR parameters to ensure optimal artifact removal while preserving neural signal quality. Hybrid approaches, such as MEMD-ASR or IMU-ASR, enhance robustness and reduce computational load, making ASR-based pipelines suitable for wearable and mobile EEG applications. Moreover, ASR maintains stability in minimal-channel dry systems, where ICA often distorts clean signals. The main limitation across the literature is the lack of standardized metrics, which hinders quantitative comparisons across methods and datasets. Moreover, many algorithms exhibit significant hyperparameter sensitivity, making their performance highly dependent on dataset characteristics and requiring careful tuning.</p></sec><sec id="sec4dot4-sensors-25-05770"><title>4.4. Emerging Directions and&#160;Limitations</title><p>In relation to the management of movement artifacts, a study published after the closing date of the present literature search [<xref rid="B146-sensors-25-05770" ref-type="bibr">146</xref>] proposed two new approaches based on ASR: (i) ASR-Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and (ii) ASR-Generalized Extreme Value (GEV). These new approaches prevent short, burst-like movement artifacts (typical of intense real-world motor tasks) from being included in the reference data. They are based on redefining the calibration phase by sampling each individual data point instead of the RMS value over a window, resulting in less skewed calibration distributions with smaller related threshold values (k values between 3 and 5). Although these approaches have not been tested on wearable EEG, they could represent an alternative approach, for example, in cases where IMU is not available.</p><p>Moreover, recent deep learning&#8211;based approaches for EEG artifact handling reveal promising advances. Generative adversarial networks with recurrent generators and convolutional discriminators [<xref rid="B105-sensors-25-05770" ref-type="bibr">105</xref>], as well as transformer-based models, have been applied to both multi-channel [<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>] and single-channel [<xref rid="B118-sensors-25-05770" ref-type="bibr">118</xref>,<xref rid="B120-sensors-25-05770" ref-type="bibr">120</xref>] configurations, demonstrating effective suppression of ocular and muscular artifacts and achieving satisfactory performance even when the number of available channels is reduced. For multiple artifact types, different strategies have emerged, encompassing sequential autoencoders [<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>], dual-branch fusion networks [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>], and segmentation&#8211;denoising frameworks [<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>]. Wavelet-enhanced generative adversarial networks have further improved adaptability, though with higher computational cost [<xref rid="B111-sensors-25-05770" ref-type="bibr">111</xref>]. In wearable contexts, compact autoencoders have enabled real-time denoising on embedded hardware [<xref rid="B125-sensors-25-05770" ref-type="bibr">125</xref>], while hybrid systems combining convolutional networks with least-mean-square filtering have proven effective on two-channel EEG [<xref rid="B126-sensors-25-05770" ref-type="bibr">126</xref>]. Clinical studies in neonatal EEG also confirm that semi-supervised convolutional networks can capture diverse contamination sources with reduced channels [<xref rid="B119-sensors-25-05770" ref-type="bibr">119</xref>,<xref rid="B121-sensors-25-05770" ref-type="bibr">121</xref>]. The gradual transition toward deep learning has yielded consistent performance gains, as highlighted in the comparative study by O&#8217;Sullivan et al. (2023) [<xref rid="B119-sensors-25-05770" ref-type="bibr">119</xref>], where convolutional neural networks outperformed traditional thresholding and machine learning methods. In this work, the authors compared three strategies for artifact detection in neonatal EEG recorded with a nine-channel device: a threshold-based digital signal processing method combined with a compact neural network, a machine learning classifier based on random convolutional kernel transformations, and a fully convolutional deep learning architecture. When assessing all artifact types jointly, the convolutional neural network achieved higher results in typical classification performance metrics, such as Area Under the Curve (AUC) (+20%), Matthews Correlation Coefficient (MCC) (+36%), and sensitivity (+100%) compared to the threshold-based approach, and modest gains over the machine learning classifier (AUC +4%, MCC +6%, sensitivity +9%). For individual artifact categories, the machine learning and threshold-based methods occasionally outperformed the convolutional model, particularly for movement and muscular artifacts, where the machine learning classifier achieved up to 7% higher AUC and the threshold-based approach obtained more than 150% higher MCC. These results highlight that while complex deep networks generalize better across heterogeneous contaminations, simpler and more interpretable algorithms may be more effective for specific artifact categories that exhibit clear and easily detectable signatures. In general, deep learning approaches have achieved very high performance in controlled settings; however, they remain black-box systems, offering limited explainability and uncertain generalization to real-world conditions, especially when encountering artifact types not represented in the training data.</p><p>Some limitations of this review have to be acknowledged. Firstly, a quantitative meta-analysis was not possible due to the heterogeneity of methodologies, electrode configurations, experimental conditions, and outcome measures across the included studies. Moreover, the inclusion criteria considered only studies involving EEG devices with no more than 16 channels, dry or semi-wet electrodes, or hardware efficiency evaluation [<xref rid="B47-sensors-25-05770" ref-type="bibr">47</xref>]. This may have excluded systems that do not fully align with these criteria, especially as far as the number of electrodes is concerned, but share key characteristics of portable EEG devices. Finally, the reported performance results were not evaluated in relation to the quality of the employed datasets. In particular, the use of simulated data in some studies and the limited sample sizes in others may limit the results&#8217; generalizability.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05770"><title>5. Conclusions</title><p>This review systematically investigated methods for artifact detection and artifact category identification for EEG signals acquired using wearable devices, addressing artifact sources, artifact detection and removal algorithms, assessment parameters and metrics, and the corresponding reference signals. Artifacts in wearable EEG exhibit specific characteristics due to operational conditions such as the use of dry electrodes and the allowance of free movement. Additionally, their management poses unique challenges owing to the reduced scalp coverage. Although interest in wearable EEG has grown significantly in recent years, this trend has not been matched by a corresponding emphasis on artifact management in such setups. The majority of the papers propose a pipeline including both artifact detection and artifact removal phases. Only two studies address artifact category identification, possibly enabling the application of ad hoc strategies. Nonetheless, a trend of association between specific artifact and specific algorithms employed was observed, possibly useful to develop combinations of algorithms to perform identification. Wavelet Transforms and ICA, often using thresholding as a decision rule, are among the most frequently used techniques for managing ocular and muscular artifacts. ASR-based pipelines are widely applied for ocular, movement, and instrumental artifacts. Deep learning architectures are emerging as powerful alternatives, particularly for muscular and motion artifacts, and show promise for real-time implementation despite current limitations in online deployment. In addition, signals provided by auxiliary sensors (e.g., IMUs) remain underutilized, although they represent a key factor in improving artifact detection and removal in wearable EEG systems, particularly under ecological conditions. As far as the algorithm performance assessment is concerned, three main categories of reference signals are identified: (i) <italic toggle="yes">clean</italic>, (ii) <italic toggle="yes">artifact</italic>, and (iii) <italic toggle="yes">physiological</italic> (partially contaminated). Accuracy resulted as the most implemented assessment parameter, reflecting similarity to the <italic toggle="yes">clean</italic> reference. Instead, selectivity is assessed by adopting <italic toggle="yes">physiological</italic> signal as reference.</p><p>This review highlights that artifacts remain an overlooked topic in the wearable EEG literature, particularly regarding artifact category identification. This limitation affects artifact removal algorithms, which are developed and validated on specific artifact types with unknown performance on other artifacts or signals with heterogeneous artifact patterns. This review emphasizes existing contributions targeting artifact category identification and provides a mapping of removal algorithms validated on specific artifact types. A clear association emerges between artifact type and removal method, enabling the selection of appropriate techniques based on previously identified categories. Finally, a comprehensive survey of public datasets is also included to support standardization in artifact management for wearable EEG.</p></sec></body><back><ack><title>Acknowledgments</title><p>During the preparation of this manuscript, the author(s) used ChatGPT-4o for the purposes of improving the English language. The authors have reviewed and edited the output and take full responsibility for the content of this publication.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-05770"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mdpi.com/article/10.3390/s25185770/s1">https://www.mdpi.com/article/10.3390/s25185770/s1</uri>, Table S1: PRISMA-S Checklist.</p><supplementary-material id="sensors-25-05770-s001" position="float" content-type="local-data" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05770-s001.zip" position="float" orientation="portrait"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization, D.D., P.L., E.V., N.M. and P.A.; methodology, L.D.M., R.R., M.N., M.P., D.D., P.L., E.V. and L.G.; investigation, L.D.M., R.R., L.G. and M.D.L.; data curation, L.D.M. and R.R.; writing&#8212;original draft preparation, L.D.M. and R.R.; writing&#8212;review and editing, L.D.M., R.R., D.D., P.L., E.V., N.M., P.A., M.N., M.P., L.G. and M.D.L.; visualization, L.D.M., R.R. and M.D.L.; supervision, P.A., L.G. and N.M.; project administration, P.A.; funding acquisition, P.A. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available in the research engines presented in <xref rid="sec2-sensors-25-05770" ref-type="sec">Section 2</xref>.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Marco Nalin and Mauro Picciafuoco were employed by ab medica S.p.A. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:</p><array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ACF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Autocorrelation Function</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ADJUST</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Artefact Detector based on the Joint Use of Spatial and Temporal features</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>AMRC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Amplitude Modulation Rate of Change</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ANC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Adaptive Noise Canceler</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ANFIS</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Adaptive Noise Cancellation System</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ANOVA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Analysis of Variance</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>APF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Adaptive Predictor Filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ASR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Artifact Subspace Reconstruction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>AUC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Area Under the Curve</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>BC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Binary Classification</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>BiGRU</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Bidirectional Gated Recurrent Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>BM</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Belief Matching</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>BPF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Band-Pass Filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>CC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Correlation of Coefficient</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>CCA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Canonical Correlation Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>CCR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Correct Classification Rate</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>CNN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>CSED</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Cumulative Sum of Squared Error Difference</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>CSMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Corrugator Supercilii Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>DWT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Discrete Wavelet Transform</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>DSNR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Difference in Signal-to-Noise Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>DTW</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Dynamic Time Warping</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>EAWICA</bold>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;</td><td align="left" valign="middle" rowspan="1" colspan="1">Enhanced Automatic Wavelet ICA</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Eyes Closed</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ECG</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Electrocardiogram</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EEG</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Electroencephalogram</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EEMD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Ensemble Empirical Mode Decomposition</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EII</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Electric Impedance Imbalance</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EM</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Expectation&#8211;Maximization</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EMD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Empirical Mode Decomposition</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EMG</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Electromyogram</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EMI</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Electromagnetic Interference</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EO</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Eyes Open</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>EOG</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Electrooculogram</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ERP</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Event-Related Potential</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ETI</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Electrode-Tissue Impedance</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Fully Connected</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FCBF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Fast Correlation-Based Filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Fractal Dimension</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FDR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">False Discovery Rate</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FIR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Finite Impulse Response</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FMEMD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Fast Multivariate Empirical Mode Decomposition</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FFT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Fast Fourier Transform</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FORCe</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Wavelet + ICA (SOBI) + Thresholding</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FP-h</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">False Positive rate per hour</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FPM</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">False Positive rate per Minute</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FPR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">False Positive Rate</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>FTR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Frequency-Tagging Response</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>GAN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Generative Adversarial Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>GRU</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Gated Recurrent Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>GRU-MARSC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Gated Recurrent Unit-based Multi-type<break/>Artifact Removal algorithm for Single-Channel</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>GSTV</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Group Sparsity Total Variation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>GWO</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Gray Wolf Optimization</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>HPF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">High-Pass Filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>HPO</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">HyperParameter Optimization</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ICA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Independent Component Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ICA-W</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Independent Component Analysis&#8211;Wavelet</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ICs</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Independent Components</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>I-CycleGAN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Improved CycleGAN</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>IMDL</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Integrated Minimum Description Length</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>IMFs</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Intrinsic Mode Functions</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>IMU</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Inertial Measurement Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ISD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Index of Spectral Deformation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>KNN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">k-Nearest Neighbors</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LDA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Linear Discriminant Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LiMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Limb Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LaMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Laryngeal Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LLMS</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Leaky Least Mean Squares</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LMM</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Local Maximal and Minimal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LMS</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Least Mean Square</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LogP</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Log Power</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LPF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Low-Pass Filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>LZC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Lempel&#8211;Ziv Complexity</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MAD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Median Absolute Deviation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MAE</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Absolute Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MCA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Morphological Component Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MCAF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-Channel Adaptive Filtering</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MARSC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-type Artifact Removal algorithm for Single-Channel</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-class Classification</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MCC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Matthews Correlation Coefficient</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MCCP</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Minimal Cost-Complexity Pruning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MEMD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multivariate Empirical Mode Decomposition</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MFE</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Morphological Feature Extraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MI</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Mutual Information</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-class Multi-output Classification</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MaMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Masseter Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MeMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Mentalis Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MODWT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Maximal Overlap Discrete Wavelet Transform</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MRA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">MultiResolution Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MSC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Magnitude Square Coherence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MSDW</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-window Summation of Derivatives within a Window</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MSE</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Square Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MV-EMD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multivariate EMD with CCA</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>M-mDistEn</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multiscale Modified-Distribution Entropy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>MLP</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Multilayer Perceptron</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>mRMR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Minimum Redundancy Maximum Relevance</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>NA-MEMD</bold>&#160;&#160;&#160;&#160;&#160;</td><td align="left" valign="middle" rowspan="1" colspan="1">Noise-Assisted Multivariate Empirical Mode Decomposition</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>NMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Nasalis Muscle Contration</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>NLMS</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Normalized Least Mean Square</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>NSR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Noise-to-Signal Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>OD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Outlier Detection</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>OOcMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Orbicularis Oculi Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>OOrMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Orbicularis Oris Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>OS-EHO</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Opposition Searched&#8211;Elephant Herding Optimization</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PCA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Principal Component Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PCMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Posterior Cervical Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PLN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Power-Line Noise</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Pharyngeal Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PGA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Principal Geodesic Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PSD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Power Spectral Density</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PSNR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Peak Signal-to-Noise Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>PPV</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Positive Predictive Value</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>QDA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Quadratic Discriminant Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RLS</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Recursive Least-Squares Adaptive</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RMS</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Root Mean Square</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RMSE</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Root Mean Squared Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RCs</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Reconstructed Components</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ResCNN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Residual Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ResUnet1D</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">1D Residual U-Net Semantic Segmentation Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RNN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Recurrent Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RP</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Relative Power</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RRMSE</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Relative Root Mean Squared Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RRMSEf</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Relative Root Mean Squared Error in time domain</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RRMSEt</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Relative Root Mean Squared Error in frequency domain</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>RSD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Relative Spectral Difference</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SAR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Signal-to-Artifact Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SBF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Stop-Band Filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Standard Deviation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SDW</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Summation of Derivatives within a Window</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ShMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Shoulder Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SSA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Singular Spectrum Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SubMc</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Submentalis Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SE</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Shannon Entropy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SEF</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Spectral Edge Frequency</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SG</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Savitzky&#8211;Golay Filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SOBI</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Second Order Blind Identification</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SNR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Signal-to-Noise Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SVM</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Support Vector Machine</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SSVEP</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Steady-State Visual Evoked Potentials</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SWT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Stationary Wavelet Transform</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>STFT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Short-Time Fourier Transform</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>SVD</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Singular Value Decomposition</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>TEN</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Thermal Electronics Noise</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>TeMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Temporalis Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ToMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Tongue Muscle Contraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>TFA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Time&#8211;Frequency Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>TPR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">True Positive Rate</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>TPOT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Tree-based Pipeline Optimization Tool</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>vEOG</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">vertical Electrooculogram</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>VME</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Variational Mode Extraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>VME-DWT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Variational Mode Extraction with Discrete Wavelet Transform</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>WICs</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Wavelet Independent Components</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>wICA</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Wavelet-enhanced Independent Component Analysis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>WT</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Wavelet Transform</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ZCR</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Zero Crossing Rate</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>ZMC</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Zygomaticus Muscle Contraction</td></tr></tbody></array></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05770"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>V&#228;rbu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Muhammad</surname><given-names>N.</given-names></name><name name-style="western"><surname>Muhammad</surname><given-names>Y.</given-names></name></person-group><article-title>Past, present, and future of EEG-based BCI applications</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>3331</elocation-id><pub-id pub-id-type="doi">10.3390/s22093331</pub-id><pub-id pub-id-type="pmid">35591021</pub-id><pub-id pub-id-type="pmcid">PMC9101004</pub-id></element-citation></ref><ref id="B2-sensors-25-05770"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blinowska</surname><given-names>K.</given-names></name><name name-style="western"><surname>Durka</surname><given-names>P.</given-names></name></person-group><article-title>Electroencephalography (eeg)</article-title><source>Wiley Encycl. Biomed. Eng.</source><year>2006</year><volume>10</volume><elocation-id>9780471740360</elocation-id></element-citation></ref><ref id="B3-sensors-25-05770"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amer</surname><given-names>N.S.</given-names></name><name name-style="western"><surname>Belhaouari</surname><given-names>S.B.</given-names></name></person-group><article-title>Eeg signal processing for medical diagnosis, healthcare, and monitoring: A comprehensive review</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>143116</fpage><lpage>143142</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3341419</pub-id></element-citation></ref><ref id="B4-sensors-25-05770"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharmila</surname><given-names>A.</given-names></name></person-group><article-title>Epilepsy detection from EEG signals: A review</article-title><source>J. Med. Eng. Technol.</source><year>2018</year><volume>42</volume><fpage>368</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1080/03091902.2018.1513576</pub-id><pub-id pub-id-type="pmid">30465700</pub-id></element-citation></ref><ref id="B5-sensors-25-05770"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kowalski</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Gawel</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pfeffer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Barcikowska</surname><given-names>M.</given-names></name></person-group><article-title>The diagnostic value of EEG in Alzheimer disease: Correlation with the severity of mental impairment</article-title><source>J. Clin. Neurophysiol.</source><year>2001</year><volume>18</volume><fpage>570</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1097/00004691-200111000-00008</pub-id><pub-id pub-id-type="pmid">11779971</pub-id></element-citation></ref><ref id="B6-sensors-25-05770"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Geraedts</surname><given-names>V.J.</given-names></name><name name-style="western"><surname>Boon</surname><given-names>L.I.</given-names></name><name name-style="western"><surname>Marinus</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gouw</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>van Hilten</surname><given-names>J.J.</given-names></name><name name-style="western"><surname>Stam</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Tannemaat</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Contarino</surname><given-names>M.F.</given-names></name></person-group><article-title>Clinical correlates of quantitative EEG in Parkinson disease: A systematic review</article-title><source>Neurology</source><year>2018</year><volume>91</volume><fpage>871</fpage><lpage>883</lpage><pub-id pub-id-type="doi">10.1212/WNL.0000000000006473</pub-id><pub-id pub-id-type="pmid">30291182</pub-id></element-citation></ref><ref id="B7-sensors-25-05770"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Karameh</surname><given-names>F.N.</given-names></name><name name-style="western"><surname>Dahleh</surname><given-names>M.A.</given-names></name></person-group><article-title>Automated classification of EEG signals in brain tumor diagnostics</article-title><source>Proceedings of the 2000 American Control Conference ACC</source><comment>(IEEE cat. No. 00CH36334)</comment><conf-loc>Chicago, IL, USA</conf-loc><conf-date>28&#8211;30 June 2000</conf-date><volume>Volume 6</volume><fpage>4169</fpage><lpage>4173</lpage></element-citation></ref><ref id="B8-sensors-25-05770"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Murugesan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sukanesh</surname><given-names>R.</given-names></name></person-group><article-title>Automated detection of brain tumor in EEG signals using artificial neural networks</article-title><source>Proceedings of the 2009 International Conference on Advances in Computing, Control, and Telecommunication Technologies</source><conf-loc>Trivandrum, Kerala, India</conf-loc><conf-date>28&#8211;29 December 2009</conf-date><fpage>284</fpage><lpage>288</lpage></element-citation></ref><ref id="B9-sensors-25-05770"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Preu&#223;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Preiss</surname><given-names>S.</given-names></name><name name-style="western"><surname>Syrbe</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nestler</surname><given-names>U.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Merkenschlager</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bertsche</surname><given-names>A.</given-names></name><name name-style="western"><surname>Christiansen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bernhard</surname><given-names>M.K.</given-names></name></person-group><article-title>Signs and symptoms of pediatric brain tumors and diagnostic value of preoperative EEG</article-title><source>Child&#8217;s Nerv. Syst.</source><year>2015</year><volume>31</volume><fpage>2051</fpage><lpage>2054</lpage><pub-id pub-id-type="doi">10.1007/s00381-015-2842-z</pub-id><pub-id pub-id-type="pmid">26248670</pub-id></element-citation></ref><ref id="B10-sensors-25-05770"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Finnigan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Van Putten</surname><given-names>M.J.</given-names></name></person-group><article-title>EEG in ischaemic stroke: Quantitative EEG can uniquely inform (sub-) acute prognoses and clinical management</article-title><source>Clin. Neurophysiol.</source><year>2013</year><volume>124</volume><fpage>10</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2012.07.003</pub-id><pub-id pub-id-type="pmid">22858178</pub-id></element-citation></ref><ref id="B11-sensors-25-05770"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Soufineyestani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dowling</surname><given-names>D.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>A.</given-names></name></person-group><article-title>Electroencephalography (EEG) technology applications and available devices</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>7453</elocation-id><pub-id pub-id-type="doi">10.3390/app10217453</pub-id></element-citation></ref><ref id="B12-sensors-25-05770"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Biasiucci</surname><given-names>A.</given-names></name><name name-style="western"><surname>Franceschiello</surname><given-names>B.</given-names></name><name name-style="western"><surname>Murray</surname><given-names>M.M.</given-names></name></person-group><article-title>Electroencephalography</article-title><source>Curr. Biol.</source><year>2019</year><volume>29</volume><fpage>R80</fpage><lpage>R85</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.11.052</pub-id><pub-id pub-id-type="pmid">30721678</pub-id></element-citation></ref><ref id="B13-sensors-25-05770"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nidal</surname><given-names>K.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>A.S.</given-names></name></person-group><source>EEG/ERP Analysis: Methods and Applications</source><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2014</year></element-citation></ref><ref id="B14-sensors-25-05770"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hammond</surname><given-names>D.C.</given-names></name></person-group><article-title>What is neurofeedback: An update</article-title><source>J. Neurother.</source><year>2011</year><volume>15</volume><fpage>305</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1080/10874208.2011.623090</pub-id></element-citation></ref><ref id="B15-sensors-25-05770"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hammond</surname><given-names>D.C.</given-names></name></person-group><article-title>Neurofeedback treatment of depression and anxiety</article-title><source>J. Adult Dev.</source><year>2005</year><volume>12</volume><fpage>131</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1007/s10804-005-7029-5</pub-id></element-citation></ref><ref id="B16-sensors-25-05770"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marzbani</surname><given-names>H.</given-names></name><name name-style="western"><surname>Marateb</surname><given-names>H.R.</given-names></name><name name-style="western"><surname>Mansourian</surname><given-names>M.</given-names></name></person-group><article-title>Neurofeedback: A comprehensive review on system design, methodology and clinical applications</article-title><source>Basic Clin. Neurosci.</source><year>2016</year><volume>7</volume><fpage>143</fpage><pub-id pub-id-type="pmid">27303609</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.15412/J.BCN.03070208</pub-id><pub-id pub-id-type="pmcid">PMC4892319</pub-id></element-citation></ref><ref id="B17-sensors-25-05770"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ros</surname><given-names>T.</given-names></name><name name-style="western"><surname>Moseley</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Bloom</surname><given-names>P.A.</given-names></name><name name-style="western"><surname>Benjamin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Parkinson</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Gruzelier</surname><given-names>J.H.</given-names></name></person-group><article-title>Optimizing microsurgical skills with EEG neurofeedback</article-title><source>BMC Neurosci.</source><year>2009</year><volume>10</volume><elocation-id>87</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-10-87</pub-id><pub-id pub-id-type="pmid">19630948</pub-id><pub-id pub-id-type="pmcid">PMC2723116</pub-id></element-citation></ref><ref id="B18-sensors-25-05770"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiang</surname><given-names>M.Q.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>X.H.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>B.G.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>M.</given-names></name></person-group><article-title>The effect of neurofeedback training for sport performance in athletes: A meta-analysis</article-title><source>Psychol. Sport Exerc.</source><year>2018</year><volume>36</volume><fpage>114</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1016/j.psychsport.2018.02.004</pub-id></element-citation></ref><ref id="B19-sensors-25-05770"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Farwell</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Donchin</surname><given-names>E.</given-names></name></person-group><article-title>Talking off the top of your head: Toward a mental prosthesis utilizing event-related brain potentials</article-title><source>Electroencephalogr. Clin. Neurophysiol.</source><year>1988</year><volume>70</volume><fpage>510</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(88)90149-6</pub-id><pub-id pub-id-type="pmid">2461285</pub-id></element-citation></ref><ref id="B20-sensors-25-05770"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name name-style="western"><surname>Neuper</surname><given-names>C.</given-names></name></person-group><article-title>Motor imagery and direct brain-computer communication</article-title><source>Proc. IEEE</source><year>2001</year><volume>89</volume><fpage>1123</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1109/5.939829</pub-id></element-citation></ref><ref id="B21-sensors-25-05770"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nijholt</surname><given-names>A.</given-names></name><name name-style="western"><surname>Contreras-Vidal</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Jeunet</surname><given-names>C.</given-names></name><name name-style="western"><surname>V&#228;ljam&#228;e</surname><given-names>A.</given-names></name></person-group><article-title>Brain-Computer Interfaces for Non-Clinical (Home, Sports, Art, Entertainment, Education, Well-Being) Applications</article-title><source>Front. Comput. Sci.</source><year>2022</year><volume>4</volume><elocation-id>860619</elocation-id><pub-id pub-id-type="doi">10.3389/fcomp.2022.860619</pub-id></element-citation></ref><ref id="B22-sensors-25-05770"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cannard</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wahbeh</surname><given-names>H.</given-names></name><name name-style="western"><surname>Delorme</surname><given-names>A.</given-names></name></person-group><article-title>Electroencephalography correlates of well-being using a low-cost wearable system</article-title><source>Front. Hum. Neurosci.</source><year>2021</year><volume>15</volume><elocation-id>745135</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2021.745135</pub-id><pub-id pub-id-type="pmid">35002651</pub-id><pub-id pub-id-type="pmcid">PMC8740323</pub-id></element-citation></ref><ref id="B23-sensors-25-05770"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Flanagan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Saikia</surname><given-names>M.J.</given-names></name></person-group><article-title>Consumer-grade electroencephalogram and functional near-infrared spectroscopy neurofeedback technologies for mental health and wellbeing</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>8482</elocation-id><pub-id pub-id-type="doi">10.3390/s23208482</pub-id><pub-id pub-id-type="pmid">37896575</pub-id><pub-id pub-id-type="pmcid">PMC10610697</pub-id></element-citation></ref><ref id="B24-sensors-25-05770"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Sourina</surname><given-names>O.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>M.K.</given-names></name></person-group><article-title>Eeg-based &#8220;serious&#8221; games design for medical applications</article-title><source>Proceedings of the 2010 International Conference on Cyberworlds</source><conf-loc>Singapore</conf-loc><conf-date>20&#8211;22 October 2010</conf-date><fpage>270</fpage><lpage>276</lpage></element-citation></ref><ref id="B25-sensors-25-05770"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>de Queiroz Cavalcanti</surname><given-names>D.</given-names></name><name name-style="western"><surname>Melo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Silva</surname><given-names>T.</given-names></name><name name-style="western"><surname>Falc&#227;o</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cavalcanti</surname><given-names>M.</given-names></name><name name-style="western"><surname>Becker</surname><given-names>V.</given-names></name></person-group><article-title>Research on brain-computer interfaces in the entertainment field</article-title><source>Proceedings of the International Conference on Human-Computer Interaction</source><conf-loc>Copenhagen, Denmark</conf-loc><conf-date>23&#8211;28 July 2023</conf-date><fpage>404</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-35596-7_26</pub-id></element-citation></ref><ref id="B26-sensors-25-05770"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xian</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name></person-group><article-title>Progress in EEG-Based Brain Robot Interaction Systems</article-title><source>Comput. Intell. Neurosci.</source><year>2017</year><volume>2017</volume><fpage>1742862</fpage><pub-id pub-id-type="doi">10.1155/2017/1742862</pub-id><pub-id pub-id-type="pmid">28484488</pub-id><pub-id pub-id-type="pmcid">PMC5397651</pub-id></element-citation></ref><ref id="B27-sensors-25-05770"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Douibi</surname><given-names>K.</given-names></name><name name-style="western"><surname>Le Bars</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lemontey</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nag</surname><given-names>L.</given-names></name><name name-style="western"><surname>Balp</surname><given-names>R.</given-names></name><name name-style="western"><surname>Breda</surname><given-names>G.</given-names></name></person-group><article-title>Toward EEG-based BCI applications for industry 4.0: Challenges and possible applications</article-title><source>Front. Hum. Neurosci.</source><year>2021</year><volume>15</volume><elocation-id>705064</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2021.705064</pub-id><pub-id pub-id-type="pmid">34483868</pub-id><pub-id pub-id-type="pmcid">PMC8414547</pub-id></element-citation></ref><ref id="B28-sensors-25-05770"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeunet</surname><given-names>C.</given-names></name><name name-style="western"><surname>Glize</surname><given-names>B.</given-names></name><name name-style="western"><surname>McGonigal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Batail</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Micoulaud-Franchi</surname><given-names>J.A.</given-names></name></person-group><article-title>Using EEG-based brain computer interface and neurofeedback targeting sensorimotor rhythms to improve motor skills: Theoretical background, applications and prospects</article-title><source>Neurophysiol. Clin.</source><year>2019</year><volume>49</volume><fpage>125</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/j.neucli.2018.10.068</pub-id><pub-id pub-id-type="pmid">30414824</pub-id></element-citation></ref><ref id="B29-sensors-25-05770"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheron</surname><given-names>G.</given-names></name><name name-style="western"><surname>Petit</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cheron</surname><given-names>J.</given-names></name><name name-style="western"><surname>Leroy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cebolla</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cevallos</surname><given-names>C.</given-names></name><name name-style="western"><surname>Petieau</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hoellinger</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zarka</surname><given-names>D.</given-names></name><name name-style="western"><surname>Clarinval</surname><given-names>A.M.</given-names></name><etal/></person-group><article-title>Brain oscillations in sport: Toward EEG biomarkers of performance</article-title><source>Front. Psychol.</source><year>2016</year><volume>7</volume><elocation-id>246</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2016.00246</pub-id><pub-id pub-id-type="pmid">26955362</pub-id><pub-id pub-id-type="pmcid">PMC4768321</pub-id></element-citation></ref><ref id="B30-sensors-25-05770"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sugden</surname><given-names>R.J.</given-names></name><name name-style="western"><surname>Pham-Kim-Nghiem-Phu</surname><given-names>V.L.L.</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>I.</given-names></name><name name-style="western"><surname>Leon</surname><given-names>A.</given-names></name><name name-style="western"><surname>Diamandis</surname><given-names>P.</given-names></name></person-group><article-title>Remote collection of electrophysiological data with brain wearables: Opportunities and challenges</article-title><source>Bioelectron. Med.</source><year>2023</year><volume>9</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1186/s42234-023-00114-5</pub-id><pub-id pub-id-type="pmid">37340487</pub-id><pub-id pub-id-type="pmcid">PMC10283168</pub-id></element-citation></ref><ref id="B31-sensors-25-05770"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Casson</surname><given-names>A.J.</given-names></name></person-group><article-title>Wearable EEG and beyond</article-title><source>Biomed. Eng. Lett.</source><year>2019</year><volume>9</volume><fpage>53</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1007/s13534-018-00093-6</pub-id><pub-id pub-id-type="pmid">30956880</pub-id><pub-id pub-id-type="pmcid">PMC6431319</pub-id></element-citation></ref><ref id="B32-sensors-25-05770"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mihajlovi&#263;</surname><given-names>V.</given-names></name><name name-style="western"><surname>Grundlehner</surname><given-names>B.</given-names></name><name name-style="western"><surname>Vullers</surname><given-names>R.</given-names></name><name name-style="western"><surname>Penders</surname><given-names>J.</given-names></name></person-group><article-title>Wearable, wireless EEG solutions in daily life applications: What are we missing?</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2014</year><volume>19</volume><fpage>6</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">25486653</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/JBHI.2014.2328317</pub-id></element-citation></ref><ref id="B33-sensors-25-05770"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Skyrme</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dale</surname><given-names>S.</given-names></name></person-group><source>Brain-Computer Interfaces 2025&#8211;2045: Technologies, Players, Forecasts</source><publisher-name>IDTechEx</publisher-name><publisher-loc>Cambridge, UK</publisher-loc><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.idtechex.com/en/research-report/brain-computer-interfaces/1024" ext-link-type="uri">https://www.idtechex.com/en/research-report/brain-computer-interfaces/1024</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-05-20">(accessed on 20 May 2025)</date-in-citation></element-citation></ref><ref id="B34-sensors-25-05770"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gokhale</surname><given-names>S.</given-names></name></person-group><source>Brain Computer Interface Market Size, Share, Trends, Report 2024&#8211;2034</source><publisher-name>Precedence Research</publisher-name><publisher-loc>Ottawa, ON, Canada</publisher-loc><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.precedenceresearch.com/brain-computer-interface-market" ext-link-type="uri">https://www.precedenceresearch.com/brain-computer-interface-market</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-05-20">(accessed on 20 May 2025)</date-in-citation></element-citation></ref><ref id="B35-sensors-25-05770"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Senkler</surname><given-names>B.</given-names></name><name name-style="western"><surname>Schellack</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Glatz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Freymueller</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hornberg</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mc Call</surname><given-names>T.</given-names></name></person-group><article-title>Exploring urban mental health using mobile EEG&#8212;A systematic review</article-title><source>PLoS Ment. Health</source><year>2025</year><volume>2</volume><elocation-id>e0000203</elocation-id></element-citation></ref><ref id="B36-sensors-25-05770"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rossini</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>Di Iorio</surname><given-names>R.</given-names></name><name name-style="western"><surname>Vecchio</surname><given-names>F.</given-names></name><name name-style="western"><surname>Anfossi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Babiloni</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bozzali</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bruni</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Cappa</surname><given-names>S.F.</given-names></name><name name-style="western"><surname>Escudero</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fraga</surname><given-names>F.J.</given-names></name><etal/></person-group><article-title>Early diagnosis of Alzheimer&#8217;s disease: The role of biomarkers including advanced EEG signal analysis. Report from the IFCN-sponsored panel of experts</article-title><source>Clin. Neurophysiol.</source><year>2020</year><volume>131</volume><fpage>1287</fpage><lpage>1310</lpage><pub-id pub-id-type="pmid">32302946</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.clinph.2020.03.003</pub-id></element-citation></ref><ref id="B37-sensors-25-05770"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chin</surname><given-names>T.I.</given-names></name><name name-style="western"><surname>An</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yibeltal</surname><given-names>K.</given-names></name><name name-style="western"><surname>Workneh</surname><given-names>F.</given-names></name><name name-style="western"><surname>Pihl</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jensen</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Asmamaw</surname><given-names>G.</given-names></name><name name-style="western"><surname>Fasil</surname><given-names>N.</given-names></name><name name-style="western"><surname>Teklehaimanot</surname><given-names>A.</given-names></name><name name-style="western"><surname>North</surname><given-names>K.</given-names></name><etal/></person-group><article-title>Implementation of a mobile EEG system in the acquisition of resting EEG and visual evoked potentials among young children in rural Ethiopia</article-title><source>Front. Hum. Neurosci.</source><year>2025</year><volume>19</volume><elocation-id>1552410</elocation-id><pub-id pub-id-type="pmid">40556683</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fnhum.2025.1552410</pub-id><pub-id pub-id-type="pmcid">PMC12186157</pub-id></element-citation></ref><ref id="B38-sensors-25-05770"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Galv&#225;n</surname><given-names>P.</given-names></name><name name-style="western"><surname>Vel&#225;zquez</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rivas</surname><given-names>R.</given-names></name><name name-style="western"><surname>Benitez</surname><given-names>G.</given-names></name><name name-style="western"><surname>Barrios</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hilario</surname><given-names>E.</given-names></name></person-group><article-title>Health diagnosis improvement in remote community health centers through telemedicine</article-title><source>Med. Access@ Point Care</source><year>2018</year><volume>2</volume><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B39-sensors-25-05770"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shivaraja</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chellappan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kamal</surname><given-names>N.</given-names></name><name name-style="western"><surname>Remli</surname><given-names>R.</given-names></name></person-group><article-title>Personalization of a mobile eeg for remote monitoring</article-title><source>Proceedings of the 2022 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)</source><conf-loc>Kuala Lumpur, Malaysia</conf-loc><conf-date>7&#8211;9 December 2022</conf-date><fpage>328</fpage><lpage>333</lpage></element-citation></ref><ref id="B40-sensors-25-05770"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>D&#8217;Angiulli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lockman-Dufour</surname><given-names>G.</given-names></name><name name-style="western"><surname>Buchanan</surname><given-names>D.M.</given-names></name></person-group><article-title>Promise for personalized diagnosis? Assessing the precision of wireless consumer-grade electroencephalography across mental states</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>6430</elocation-id><pub-id pub-id-type="doi">10.3390/app12136430</pub-id></element-citation></ref><ref id="B41-sensors-25-05770"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lopez</surname><given-names>K.L.</given-names></name><name name-style="western"><surname>Monachino</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Vincent</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Peck</surname><given-names>F.C.</given-names></name><name name-style="western"><surname>Gabard-Durnam</surname><given-names>L.J.</given-names></name></person-group><article-title>Stability, change, and reliable individual differences in electroencephalography measures: A lifespan perspective on progress and opportunities</article-title><source>NeuroImage</source><year>2023</year><volume>275</volume><fpage>120116</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120116</pub-id><pub-id pub-id-type="pmid">37169118</pub-id><pub-id pub-id-type="pmcid">PMC10262067</pub-id></element-citation></ref><ref id="B42-sensors-25-05770"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amaro</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ramusga</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bonifacio</surname><given-names>A.</given-names></name><name name-style="western"><surname>Frazao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Almeida</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lopes</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chokhachian</surname><given-names>A.</given-names></name><name name-style="western"><surname>Santucci</surname><given-names>D.</given-names></name><name name-style="western"><surname>Morgado</surname><given-names>P.</given-names></name><name name-style="western"><surname>Miranda</surname><given-names>B.</given-names></name></person-group><article-title>Advancing Mobile Neuroscience: A Novel Wearable Backpack for Multi-Sensor Research in Urban Environments</article-title><source>bioRxiv</source><year>2025</year><pub-id pub-id-type="doi">10.1101/2025.04.13.648607</pub-id></element-citation></ref><ref id="B43-sensors-25-05770"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>H&#246;ller</surname><given-names>Y.</given-names></name></person-group><article-title>Quantitative EEG in cognitive neuroscience</article-title><source>Brain Sci.</source><year>2021</year><volume>11</volume><elocation-id>517</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci11040517</pub-id><pub-id pub-id-type="pmid">33921596</pub-id><pub-id pub-id-type="pmcid">PMC8073768</pub-id></element-citation></ref><ref id="B44-sensors-25-05770"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.Y.</given-names></name><name name-style="western"><surname>Phang</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Stevenson</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>I.P.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>T.P.</given-names></name><name name-style="western"><surname>Ko</surname><given-names>L.W.</given-names></name></person-group><article-title>Diversity and Suitability of the State-of-the-Art Wearable and Wireless EEG Systems Review</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2023</year><volume>27</volume><fpage>3830</fpage><lpage>3843</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2023.3239053</pub-id><pub-id pub-id-type="pmid">37022001</pub-id></element-citation></ref><ref id="B45-sensors-25-05770"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Recent progress in wearable brain&#8211;computer interface (BCI) devices based on electroencephalogram (EEG) for medical applications: A review</article-title><source>Health Data Sci.</source><year>2023</year><volume>3</volume><fpage>0096</fpage><pub-id pub-id-type="pmid">38487198</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.34133/hds.0096</pub-id><pub-id pub-id-type="pmcid">PMC10880169</pub-id></element-citation></ref><ref id="B46-sensors-25-05770"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lopez-Gordo</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Sanchez-Morillo</surname><given-names>D.</given-names></name><name name-style="western"><surname>Valle</surname><given-names>F.P.</given-names></name></person-group><article-title>Dry EEG electrodes</article-title><source>Sensors</source><year>2014</year><volume>14</volume><fpage>12847</fpage><lpage>12870</lpage><pub-id pub-id-type="doi">10.3390/s140712847</pub-id><pub-id pub-id-type="pmid">25046013</pub-id><pub-id pub-id-type="pmcid">PMC4168519</pub-id></element-citation></ref><ref id="B47-sensors-25-05770"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Apicella</surname><given-names>A.</given-names></name><name name-style="western"><surname>Arpaia</surname><given-names>P.</given-names></name><name name-style="western"><surname>Isgro</surname><given-names>F.</given-names></name><name name-style="western"><surname>Mastrati</surname><given-names>G.</given-names></name><name name-style="western"><surname>Moccaldi</surname><given-names>N.</given-names></name></person-group><article-title>A survey on EEG-based solutions for emotion recognition with a low number of channels</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>117411</fpage><lpage>117428</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3219844</pub-id></element-citation></ref><ref id="B48-sensors-25-05770"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Klug</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gramann</surname><given-names>K.</given-names></name></person-group><article-title>Identifying key factors for improving ICA-based decomposition of EEG data in mobile and stationary experiments</article-title><source>Eur. J. Neurosci.</source><year>2021</year><volume>54</volume><fpage>8406</fpage><lpage>8420</lpage><pub-id pub-id-type="doi">10.1111/ejn.14992</pub-id><pub-id pub-id-type="pmid">33012055</pub-id></element-citation></ref><ref id="B49-sensors-25-05770"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gudikandula</surname><given-names>N.</given-names></name><name name-style="western"><surname>Janapati</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sengupta</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chintala</surname><given-names>S.</given-names></name></person-group><article-title>Recent Advancements in Online Ocular Artifacts Removal in EEG based BCI: A Review</article-title><source>Proceedings of the 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)</source><conf-loc>Mandi, India</conf-loc><conf-date>6&#8211;11 July 2024</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICCCNT61001.2024.10724228</pub-id></element-citation></ref><ref id="B50-sensors-25-05770"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mannan</surname><given-names>M.M.N.</given-names></name><name name-style="western"><surname>Kamran</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>M.Y.</given-names></name></person-group><article-title>Identification and removal of physiological artifacts from electroencephalogram signals: A review</article-title><source>IEEE Access</source><year>2018</year><volume>6</volume><fpage>30630</fpage><lpage>30652</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2842082</pub-id></element-citation></ref><ref id="B51-sensors-25-05770"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sadiya</surname><given-names>S.</given-names></name><name name-style="western"><surname>Alhanai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ghassemi</surname><given-names>M.M.</given-names></name></person-group><article-title>Artifact detection and correction in eeg data: A review</article-title><source>Proceedings of the 2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)</source><conf-loc>Virtual Conference</conf-loc><conf-date>4&#8211;6 May 2021</conf-date><fpage>495</fpage><lpage>498</lpage></element-citation></ref><ref id="B52-sensors-25-05770"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jung</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Saikiran</surname><given-names>S.S.</given-names></name></person-group><article-title>A review on EEG artifacts and its different removal technique</article-title><source>Asia-Pac. J. Converg. Res. Interchange</source><year>2016</year><volume>2</volume><fpage>43</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.21742/apjcri.2016.12.06</pub-id></element-citation></ref><ref id="B53-sensors-25-05770"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Rastegarnia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name></person-group><article-title>Methods for artifact detection and removal from scalp EEG: A review</article-title><source>Neurophysiol. Clin. Neurophysiol.</source><year>2016</year><volume>46</volume><fpage>287</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.neucli.2016.07.002</pub-id><pub-id pub-id-type="pmid">27751622</pub-id></element-citation></ref><ref id="B54-sensors-25-05770"><label>54.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Prakash</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>D.</given-names></name></person-group><article-title>Artifact Detection and Removal in EEG: A Review of Methods and Contemporary Usage</article-title><source>Proceedings of the International Conference on Artificial-Business Analytics, Quantum and Machine Learning</source><conf-loc>Bengaluru, India</conf-loc><conf-date>14&#8211;15 July 2023</conf-date><fpage>263</fpage><lpage>274</lpage></element-citation></ref><ref id="B55-sensors-25-05770"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Agounad</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tarahi</surname><given-names>O.</given-names></name><name name-style="western"><surname>Moufassih</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hamou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mazid</surname><given-names>A.</given-names></name></person-group><article-title>Advanced Signal Processing and Machine/Deep Learning Approaches on a Preprocessing Block for EEG Artifact Removal: A Comprehensive Review</article-title><source>Circuits Syst. Signal Process.</source><year>2024</year><volume>44</volume><fpage>3112</fpage><lpage>3160</lpage><pub-id pub-id-type="doi">10.1007/s00034-024-02936-3</pub-id></element-citation></ref><ref id="B56-sensors-25-05770"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Seok</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>C.</given-names></name></person-group><article-title>Motion artifact removal techniques for wearable EEG and PPG sensor systems</article-title><source>Front. Electron.</source><year>2021</year><volume>2</volume><elocation-id>685513</elocation-id><pub-id pub-id-type="doi">10.3389/felec.2021.685513</pub-id></element-citation></ref><ref id="B57-sensors-25-05770"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Urig&#252;en</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Garcia-Zapirain</surname><given-names>B.</given-names></name></person-group><article-title>EEG artifact removal&#8212;State-of-the-art and guidelines</article-title><source>J. Neural Eng.</source><year>2015</year><volume>12</volume><fpage>031001</fpage><pub-id pub-id-type="doi">10.1088/1741-2560/12/3/031001</pub-id><pub-id pub-id-type="pmid">25834104</pub-id></element-citation></ref><ref id="B58-sensors-25-05770"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sabor</surname><given-names>N.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lian</surname><given-names>Y.</given-names></name></person-group><article-title>Automatic removal of multiple artifacts for single-channel EEG</article-title><source>J. Shanghai Jiaotong Univ. (Sci.)</source><year>2021</year><volume>27</volume><fpage>437</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1007/s12204-021-2374-5</pub-id></element-citation></ref><ref id="B59-sensors-25-05770"><label>59.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Inoue</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sugi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Matsuda</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Goto</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nohira</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mase</surname><given-names>R.</given-names></name></person-group><article-title>Recording and characterization of EEGS by using wearable EEG device</article-title><source>Proceedings of the 2019 19th International Conference on Control, Automation and Systems (ICCAS)</source><conf-loc>Jeju, Republic of Korea</conf-loc><conf-date>15&#8211;18 October 2019</conf-date><fpage>194</fpage><lpage>197</lpage></element-citation></ref><ref id="B60-sensors-25-05770"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cui</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>A dual-branch interactive fusion network to remove artifacts from single-channel EEG</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>73</volume><fpage>4001912</fpage><pub-id pub-id-type="doi">10.1109/TIM.2023.3342863</pub-id></element-citation></ref><ref id="B61-sensors-25-05770"><label>61.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kaongoen</surname><given-names>N.</given-names></name><name name-style="western"><surname>Jo</surname><given-names>S.</given-names></name></person-group><article-title>Adapting Artifact Subspace Reconstruction Method for SingleChannel EEG using Signal Decomposition Techniques</article-title><source>Proceedings of the 2023 45th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>24&#8211;27 July 2023</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/EMBC40787.2023.10340077</pub-id><pub-id pub-id-type="pmid">38083141</pub-id></element-citation></ref><ref id="B62-sensors-25-05770"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Page</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>McKenzie</surname><given-names>J.E.</given-names></name><name name-style="western"><surname>Bossuyt</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>Boutron</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hoffmann</surname><given-names>T.C.</given-names></name><name name-style="western"><surname>Mulrow</surname><given-names>C.D.</given-names></name><name name-style="western"><surname>Shamseer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tetzlaff</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Akl</surname><given-names>E.A.</given-names></name><name name-style="western"><surname>Brennan</surname><given-names>S.E.</given-names></name><etal/></person-group><article-title>The PRISMA 2020 statement: An updated guideline for reporting systematic reviews</article-title><source>BMJ</source><year>2021</year><volume>372</volume><fpage>n71</fpage><pub-id pub-id-type="doi">10.1136/bmj.n71</pub-id><pub-id pub-id-type="pmid">33782057</pub-id><pub-id pub-id-type="pmcid">PMC8005924</pub-id></element-citation></ref><ref id="B63-sensors-25-05770"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rethlefsen</surname><given-names>M.L.</given-names></name><name name-style="western"><surname>Kirtley</surname><given-names>S.</given-names></name><name name-style="western"><surname>Waffenschmidt</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ayala</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Moher</surname><given-names>D.</given-names></name><name name-style="western"><surname>Page</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Koffel</surname><given-names>J.B.</given-names></name></person-group><article-title>PRISMA-S: An extension to the PRISMA statement for reporting literature searches in systematic reviews</article-title><source>Syst. Rev.</source><year>2021</year><volume>10</volume><fpage>39</fpage><pub-id pub-id-type="doi">10.1186/s13643-020-01542-z</pub-id><pub-id pub-id-type="pmid">33499930</pub-id><pub-id pub-id-type="pmcid">PMC7839230</pub-id></element-citation></ref><ref id="B64-sensors-25-05770"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sweeney</surname><given-names>K.T.</given-names></name><name name-style="western"><surname>Ayaz</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ward</surname><given-names>T.E.</given-names></name><name name-style="western"><surname>Izzetoglu</surname><given-names>M.</given-names></name><name name-style="western"><surname>McLoone</surname><given-names>S.F.</given-names></name><name name-style="western"><surname>Onaral</surname><given-names>B.</given-names></name></person-group><article-title>A methodology for validating artifact removal techniques for physiological signals</article-title><source>IEEE Trans. Inf. Technol. Biomed.</source><year>2012</year><volume>16</volume><fpage>918</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1109/TITB.2012.2207400</pub-id><pub-id pub-id-type="pmid">22801522</pub-id></element-citation></ref><ref id="B65-sensors-25-05770"><label>65.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Matiko</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Beeby</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tudor</surname><given-names>J.</given-names></name></person-group><article-title>Real time eye blink noise removal from EEG signals using morphological component analysis</article-title><source>Proceedings of the 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source><conf-loc>Osaka, Japan</conf-loc><conf-date>3&#8211;7 July 2013</conf-date><fpage>13</fpage><lpage>16</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/EMBC.2013.6609425</pub-id><pub-id pub-id-type="pmid">24109612</pub-id></element-citation></ref><ref id="B66-sensors-25-05770"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Ratcliffe</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>G.</given-names></name></person-group><article-title>Removal of ocular artifacts in EEG&#8212;An improved approach combining DWT and ANC for portable applications</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2013</year><volume>17</volume><fpage>600</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2013.2253614</pub-id><pub-id pub-id-type="pmid">24592462</pub-id></element-citation></ref><ref id="B67-sensors-25-05770"><label>67.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mihajlovi&#263;</surname><given-names>V.</given-names></name><name name-style="western"><surname>Patki</surname><given-names>S.</given-names></name><name name-style="western"><surname>Grundlehner</surname><given-names>B.</given-names></name></person-group><article-title>The impact of head movements on EEG and contact impedance: An adaptive filtering solution for motion artifact reduction</article-title><source>Proceedings of the 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source><conf-loc>Chicago, IL, USA</conf-loc><conf-date>26&#8211;30 August 2014</conf-date><fpage>5064</fpage><lpage>5067</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/EMBC.2014.6944763</pub-id><pub-id pub-id-type="pmid">25571131</pub-id></element-citation></ref><ref id="B68-sensors-25-05770"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Moore</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name></person-group><article-title>Automatic identification and removal of ocular artifacts in EEG&#8212;Improved adaptive predictor filtering for portable applications</article-title><source>IEEE Trans. Nanobioscience</source><year>2014</year><volume>13</volume><fpage>109</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1109/TNB.2014.2316811</pub-id><pub-id pub-id-type="pmid">24802943</pub-id></element-citation></ref><ref id="B69-sensors-25-05770"><label>69.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Majmudar</surname><given-names>C.A.</given-names></name><name name-style="western"><surname>Mahajan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Morshed</surname><given-names>B.I.</given-names></name></person-group><article-title>Real-time hybrid ocular artifact detection and removal for single channel EEG</article-title><source>Proceedings of the 2015 IEEE International Conference on Electro/Information Technology (EIT)</source><conf-loc>Dekalb, IL, USA</conf-loc><conf-date>21&#8211;23 May 2015</conf-date><fpage>330</fpage><lpage>334</lpage></element-citation></ref><ref id="B70-sensors-25-05770"><label>70.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>B.H.</given-names></name><name name-style="western"><surname>Jo</surname><given-names>S.</given-names></name></person-group><article-title>Real-time motion artifact detection and removal for ambulatory BCI</article-title><source>Proceedings of the 3rd International Winter Conference on Brain-Computer Interface (BCI 2015)</source><conf-loc>Gangwon-do, Republic of Korea</conf-loc><conf-date>12&#8211;14 January 2015</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B71-sensors-25-05770"><label>71.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Abd Rahman</surname><given-names>F.</given-names></name><name name-style="western"><surname>Othman</surname><given-names>M.</given-names></name></person-group><article-title>Real time eye blink artifacts removal in electroencephalogram using savitzky-golay referenced adaptive filtering</article-title><source>Proceedings of the International Conference for Innovation in Biomedical Engineering and Life Sciences (ICIBEL 2015)</source><conf-loc>Putrajaya, Malaysia</conf-loc><conf-date>6&#8211;8 December 2015</conf-date><fpage>68</fpage><lpage>71</lpage></element-citation></ref><ref id="B72-sensors-25-05770"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>D&#8217;Rozario</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Dungan</surname><given-names>G.C.</given-names></name><name name-style="western"><surname>Banks</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>P.Y.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>K.K.</given-names></name><name name-style="western"><surname>Killick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Grunstein</surname><given-names>R.R.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.W.</given-names></name></person-group><article-title>An automated algorithm to identify and reject artefacts for quantitative EEG analysis during sleep in patients with sleep-disordered breathing</article-title><source>Sleep Breath.</source><year>2015</year><volume>19</volume><fpage>607</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1007/s11325-014-1056-z</pub-id><pub-id pub-id-type="pmid">25225154</pub-id></element-citation></ref><ref id="B73-sensors-25-05770"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>W.D.</given-names></name><name name-style="western"><surname>Cha</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>K.</given-names></name><name name-style="western"><surname>Im</surname><given-names>C.H.</given-names></name></person-group><article-title>Detection of eye blink artifacts from single prefrontal channel electroencephalogram</article-title><source>Comput. Methods Programs Biomed.</source><year>2016</year><volume>124</volume><fpage>19</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2015.10.011</pub-id><pub-id pub-id-type="pmid">26560852</pub-id></element-citation></ref><ref id="B74-sensors-25-05770"><label>74.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name></person-group><article-title>Wearable EEG-based real-time system for depression monitoring</article-title><source>Proceedings of the Brain Informatics: International Conference, BI 2017</source><conf-loc>Beijing, China</conf-loc><conf-date>16&#8211;18 November 2017</conf-date><fpage>190</fpage><lpage>201</lpage></element-citation></ref><ref id="B75-sensors-25-05770"><label>75.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Thammasan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hagad</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Fukui</surname><given-names>K.i.</given-names></name><name name-style="western"><surname>Numao</surname><given-names>M.</given-names></name></person-group><article-title>Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals</article-title><source>Proceedings of the 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</source><conf-loc>San Antonio, TX, USA</conf-loc><conf-date>23&#8211;26 October 2017</conf-date><fpage>44</fpage><lpage>49</lpage></element-citation></ref><ref id="B76-sensors-25-05770"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name></person-group><article-title>An adaptive singular spectrum analysis method for extracting brain rhythms of electroencephalography</article-title><source>PeerJ</source><year>2017</year><volume>5</volume><fpage>e3474</fpage><pub-id pub-id-type="doi">10.7717/peerj.3474</pub-id><pub-id pub-id-type="pmid">28674650</pub-id><pub-id pub-id-type="pmcid">PMC5493032</pub-id></element-citation></ref><ref id="B77-sensors-25-05770"><label>77.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dehzangi</surname><given-names>O.</given-names></name><name name-style="western"><surname>Melville</surname><given-names>A.</given-names></name><name name-style="western"><surname>Taherisadr</surname><given-names>M.</given-names></name></person-group><article-title>Automatic eeg blink detection using dynamic time warping score clustering</article-title><source>Proceedings of the Advances in Body Area Networks I: Post-Conference Proceedings of BodyNets 2017</source><conf-loc>Dalian, China</conf-loc><conf-date>12&#8211;13 September 2017</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>49</fpage><lpage>60</lpage></element-citation></ref><ref id="B78-sensors-25-05770"><label>78.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Remove diverse artifacts simultaneously from a single-channel EEG based on SSA and ICA: A semi-simulated study</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>60276</fpage><lpage>60289</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2915564</pub-id></element-citation></ref><ref id="B79-sensors-25-05770"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goldberger</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Amaral</surname><given-names>L.</given-names></name><name name-style="western"><surname>Glass</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hausdorff</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ivanov</surname><given-names>P.C.</given-names></name><name name-style="western"><surname>Mark</surname><given-names>R.</given-names></name><name name-style="western"><surname>Mietus</surname><given-names>J.</given-names></name><name name-style="western"><surname>Moody</surname><given-names>G.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Stanley</surname><given-names>H.</given-names></name></person-group><article-title>PhysioBank, PhysioToolkit, and Physionet: Components of a new research resource for complex physiologic signals</article-title><source>Circulation</source><year>2000</year><volume>101</volume><fpage>E215</fpage><lpage>E220</lpage><pub-id pub-id-type="doi">10.1161/01.CIR.101.23.e215</pub-id><pub-id pub-id-type="pmid">10851218</pub-id></element-citation></ref><ref id="B80-sensors-25-05770"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leeb</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>F.</given-names></name><name name-style="western"><surname>Keinrath</surname><given-names>C.</given-names></name><name name-style="western"><surname>Scherer</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bischof</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pfurtscheller</surname><given-names>G.</given-names></name></person-group><article-title>Brain&#8211;computer communication: Motivation, aim, and impact of exploring a virtual apartment</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2007</year><volume>15</volume><fpage>473</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2007.906956</pub-id><pub-id pub-id-type="pmid">18198704</pub-id></element-citation></ref><ref id="B81-sensors-25-05770"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brunner</surname><given-names>C.</given-names></name><name name-style="western"><surname>Leeb</surname><given-names>R.</given-names></name><name name-style="western"><surname>M&#252;ller-Putz</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schl&#246;gl</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pfurtscheller</surname><given-names>G.</given-names></name></person-group><article-title>BCI Competition 2008&#8211;Graz data set A</article-title><source>IEEE Dataport</source><year>2008</year><volume>16</volume><fpage>34</fpage></element-citation></ref><ref id="B82-sensors-25-05770"><label>82.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Val-Calvo</surname><given-names>M.</given-names></name><name name-style="western"><surname>&#193;lvarez-S&#225;nchez</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Ferr&#225;ndez-Vicente</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Fern&#225;ndez</surname><given-names>E.</given-names></name></person-group><article-title>Optimization of real-time EEG artifact removal and emotion estimation for human-robot interaction applications</article-title><source>Front. Comput. Neurosci.</source><year>2019</year><volume>13</volume><elocation-id>80</elocation-id><pub-id pub-id-type="pmid">31849630</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fncom.2019.00080</pub-id><pub-id pub-id-type="pmcid">PMC6889828</pub-id></element-citation></ref><ref id="B83-sensors-25-05770"><label>83.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grosselin</surname><given-names>F.</given-names></name><name name-style="western"><surname>Navarro-Sune</surname><given-names>X.</given-names></name><name name-style="western"><surname>Vozzi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pandremmenou</surname><given-names>K.</given-names></name><name name-style="western"><surname>de Vico Fallani</surname><given-names>F.</given-names></name><name name-style="western"><surname>Attal</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chavez</surname><given-names>M.</given-names></name></person-group><article-title>Quality assessment of single-channel EEG for wearable devices</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>601</elocation-id><pub-id pub-id-type="doi">10.3390/s19030601</pub-id><pub-id pub-id-type="pmid">30709004</pub-id><pub-id pub-id-type="pmcid">PMC6387437</pub-id></element-citation></ref><ref id="B84-sensors-25-05770"><label>84.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rosanne</surname><given-names>O.</given-names></name><name name-style="western"><surname>Albuquerque</surname><given-names>I.</given-names></name><name name-style="western"><surname>Gagnon</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Tremblay</surname><given-names>S.</given-names></name><name name-style="western"><surname>Falk</surname><given-names>T.H.</given-names></name></person-group><article-title>Performance comparison of automated EEG enhancement algorithms for mental workload assessment of ambulant users</article-title><source>Proceedings of the 2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>20&#8211;23 March 2019</conf-date><fpage>61</fpage><lpage>64</lpage></element-citation></ref><ref id="B85-sensors-25-05770"><label>85.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blum</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jacobsen</surname><given-names>N.S.</given-names></name><name name-style="western"><surname>Bleichner</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>Debener</surname><given-names>S.</given-names></name></person-group><article-title>A Riemannian modification of artifact subspace reconstruction for EEG artifact handling</article-title><source>Front. Hum. Neurosci.</source><year>2019</year><volume>13</volume><elocation-id>141</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2019.00141</pub-id><pub-id pub-id-type="pmid">31105543</pub-id><pub-id pub-id-type="pmcid">PMC6499032</pub-id></element-citation></ref><ref id="B86-sensors-25-05770"><label>86.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Butkevi&#269;i&#363;t&#279;</surname><given-names>E.</given-names></name><name name-style="western"><surname>Bikul&#269;ien&#279;</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sidekerskien&#279;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bla&#382;auskas</surname><given-names>T.</given-names></name><name name-style="western"><surname>Maskeli&#363;nas</surname><given-names>R.</given-names></name><name name-style="western"><surname>Dama&#353;evi&#269;ius</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>W.</given-names></name></person-group><article-title>Removal of movement artefact for mobile EEG analysis in sports exercises</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>7206</fpage><lpage>7217</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2890335</pub-id></element-citation></ref><ref id="B87-sensors-25-05770"><label>87.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Albuquerque</surname><given-names>I.</given-names></name><name name-style="western"><surname>Rosanne</surname><given-names>O.</given-names></name><name name-style="western"><surname>Gagnon</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Tremblay</surname><given-names>S.</given-names></name><name name-style="western"><surname>Falk</surname><given-names>T.H.</given-names></name></person-group><article-title>Fusion of spectral and spectro-temporal EEG features for mental workload assessment under different levels of physical activity</article-title><source>Proceedings of the 2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>20&#8211;23 March 2019</conf-date><fpage>311</fpage><lpage>314</lpage></element-citation></ref><ref id="B88-sensors-25-05770"><label>88.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>Y.</given-names></name></person-group><article-title>An efficient and robust muscle artifact removal method for few-channel EEG</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>176036</fpage><lpage>176050</lpage><pub-id pub-id-type="doi">10.1109/access.2019.2957401</pub-id></element-citation></ref><ref id="B89-sensors-25-05770"><label>89.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Casadei</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ferrero</surname><given-names>R.</given-names></name><name name-style="western"><surname>Brown</surname><given-names>C.</given-names></name></person-group><article-title>Model-based filtering of EEG alpha waves for enhanced accuracy in dynamic conditions and artifact detection</article-title><source>Proceedings of the 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)</source><conf-loc>Dubrovnik, Croatia</conf-loc><conf-date>25&#8211;28 May 2020</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B90-sensors-25-05770"><label>90.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>El-Hajj</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Alawieh</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dawy</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Abbas</surname><given-names>N.</given-names></name><name name-style="western"><surname>El-Imad</surname><given-names>J.</given-names></name></person-group><article-title>EEG mobility artifact removal for ambulatory epileptic seizure prediction applications</article-title><source>Biomed. Signal Process. Control</source><year>2020</year><volume>55</volume><elocation-id>101638</elocation-id></element-citation></ref><ref id="B91-sensors-25-05770"><label>91.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Noorbasha</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Sudha</surname><given-names>G.F.</given-names></name></person-group><article-title>Removal of EOG artifacts from single channel EEG&#8211;an efficient model combining overlap segmented ASSA and ANC</article-title><source>Biomed. Signal Process. Control</source><year>2020</year><volume>60</volume><elocation-id>101987</elocation-id></element-citation></ref><ref id="B92-sensors-25-05770"><label>92.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dey</surname><given-names>E.</given-names></name><name name-style="western"><surname>Roy</surname><given-names>N.</given-names></name></person-group><article-title>Omad: On-device mental anomaly detection for substance and non-substance users</article-title><source>Proceedings of the 2020 IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE)</source><conf-loc>Cincinnati, OH, USA</conf-loc><conf-date>26&#8211;28 October 2020</conf-date><fpage>466</fpage><lpage>471</lpage></element-citation></ref><ref id="B93-sensors-25-05770"><label>93.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Muscle artifact removal toward mobile SSVEP-based BCI: A comparative study</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2021</year><volume>70</volume><fpage>4005512</fpage><pub-id pub-id-type="doi">10.1109/tim.2021.3085944</pub-id></element-citation></ref><ref id="B94-sensors-25-05770"><label>94.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kumaravel</surname><given-names>V.P.</given-names></name><name name-style="western"><surname>Kartsch</surname><given-names>V.</given-names></name><name name-style="western"><surname>Benatti</surname><given-names>S.</given-names></name><name name-style="western"><surname>Vallortigara</surname><given-names>G.</given-names></name><name name-style="western"><surname>Farella</surname><given-names>E.</given-names></name><name name-style="western"><surname>Buiatti</surname><given-names>M.</given-names></name></person-group><article-title>Efficient artifact removal from low-density wearable EEG using artifacts subspace reconstruction</article-title><source>Proceedings of the 2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</source><conf-loc>Guadalajara, Mexico</conf-loc><conf-date>26&#8211;30 July 2021</conf-date><fpage>333</fpage><lpage>336</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/EMBC46164.2021.9629771</pub-id><pub-id pub-id-type="pmid">34891303</pub-id></element-citation></ref><ref id="B95-sensors-25-05770"><label>95.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shahbakhti</surname><given-names>M.</given-names></name><name name-style="western"><surname>Beiramvand</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nazari</surname><given-names>M.</given-names></name><name name-style="western"><surname>Broniec-W&#243;jcik</surname><given-names>A.</given-names></name><name name-style="western"><surname>Augustyniak</surname><given-names>P.</given-names></name><name name-style="western"><surname>Rodrigues</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Wierzchon</surname><given-names>M.</given-names></name><name name-style="western"><surname>Marozas</surname><given-names>V.</given-names></name></person-group><article-title>VME-DWT: An efficient algorithm for detection and elimination of eye blink from short segments of single EEG channel</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2021</year><volume>29</volume><fpage>408</fpage><lpage>417</lpage><pub-id pub-id-type="pmid">33497337</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TNSRE.2021.3054733</pub-id></element-citation></ref><ref id="B96-sensors-25-05770"><label>96.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sha&#8217;abania</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fuadb</surname><given-names>N.</given-names></name><name name-style="western"><surname>Jamalb</surname><given-names>N.</given-names></name></person-group><article-title>Eye Blink Artefact Removal of Single Frontal EEG Channel Algorithm using Ensemble Empirical Mode Decomposition and Outlier Detection</article-title><source>Signal</source><year>2021</year><volume>22</volume><fpage>23</fpage></element-citation></ref><ref id="B97-sensors-25-05770"><label>97.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aung</surname><given-names>S.T.</given-names></name><name name-style="western"><surname>Wongsawat</surname><given-names>Y.</given-names></name></person-group><article-title>Analysis of EEG signals contaminated with motion artifacts using multiscale modified-distribution entropy</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>33911</fpage><lpage>33921</lpage><pub-id pub-id-type="doi">10.1109/access.2021.3061692</pub-id></element-citation></ref><ref id="B98-sensors-25-05770"><label>98.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Noorbasha</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Sudha</surname><given-names>G.F.</given-names></name></person-group><article-title>Removal of motion artifacts from EEG records by overlap segmentation SSA with modified grouping criteria for portable or wearable applications</article-title><source>Proceedings of the Soft Computing and Signal Processing: Proceedings of 3rd ICSCSP 2020</source><conf-loc>Hyderabad, India</conf-loc><conf-date>22&#8211;23 February 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2021</year><volume>Volume 1</volume><fpage>397</fpage><lpage>409</lpage></element-citation></ref><ref id="B99-sensors-25-05770"><label>99.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ingolfsson</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Cossettini</surname><given-names>A.</given-names></name><name name-style="western"><surname>Benatti</surname><given-names>S.</given-names></name><name name-style="western"><surname>Benini</surname><given-names>L.</given-names></name></person-group><article-title>Energy-efficient tree-based EEG artifact detection</article-title><source>Proceedings of the 2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</source><conf-loc>Glasgow, Scotland, UK</conf-loc><conf-date>11&#8211;15 July 2022</conf-date><fpage>3723</fpage><lpage>3728</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/EMBC48229.2022.9871413</pub-id><pub-id pub-id-type="pmid">36086434</pub-id></element-citation></ref><ref id="B100-sensors-25-05770"><label>100.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>An outlier detection-based method for artifact removal of few-channel EEGs</article-title><source>J. Neural Eng.</source><year>2022</year><volume>19</volume><fpage>056028</fpage><pub-id pub-id-type="doi">10.1088/1741-2552/ac954d</pub-id><pub-id pub-id-type="pmid">36167058</pub-id></element-citation></ref><ref id="B101-sensors-25-05770"><label>101.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Occhipinti</surname><given-names>E.</given-names></name><name name-style="western"><surname>Davies</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Hammour</surname><given-names>G.</given-names></name><name name-style="western"><surname>Mandic</surname><given-names>D.P.</given-names></name></person-group><article-title>Hearables: Artefact removal in Ear-EEG for continuous 24/7 monitoring</article-title><source>Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN)</source><conf-loc>Padua, Italy</conf-loc><conf-date>18&#8211;23 July 2022</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B102-sensors-25-05770"><label>102.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Paissan</surname><given-names>F.</given-names></name><name name-style="western"><surname>Kumaravel</surname><given-names>V.P.</given-names></name><name name-style="western"><surname>Farella</surname><given-names>E.</given-names></name></person-group><article-title>Interpretable CNN for single-channel artifacts detection in raw EEG signals</article-title><source>Proceedings of the 2022 IEEE Sensors Applications Symposium (SAS)</source><conf-loc>Sundsvall, Sweden</conf-loc><conf-date>1&#8211;3 August 2022</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B103-sensors-25-05770"><label>103.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mantini</surname><given-names>D.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name></person-group><article-title>EEGdenoiseNet: A benchmark dataset for end-to-end deep learning solutions of EEG denoising</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2009.11662</pub-id><pub-id pub-id-type="doi">10.1088/1741-2552/ac2bf8</pub-id><pub-id pub-id-type="pmid">34596046</pub-id></element-citation></ref><ref id="B104-sensors-25-05770"><label>104.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Peh</surname><given-names>W.Y.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dauwels</surname><given-names>J.</given-names></name></person-group><article-title>Transformer convolutional neural networks for automated artifact detection in scalp EEG</article-title><source>Proceedings of the 2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</source><conf-loc>Glasgow, UK</conf-loc><conf-date>11&#8211;15 July 2022</conf-date><fpage>3599</fpage><lpage>3602</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/EMBC48229.2022.9871916</pub-id><pub-id pub-id-type="pmid">36086402</pub-id></element-citation></ref><ref id="B105-sensors-25-05770"><label>105.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brophy</surname><given-names>E.</given-names></name><name name-style="western"><surname>Redmond</surname><given-names>P.</given-names></name><name name-style="western"><surname>Fleury</surname><given-names>A.</given-names></name><name name-style="western"><surname>De Vos</surname><given-names>M.</given-names></name><name name-style="western"><surname>Boylan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ward</surname><given-names>T.</given-names></name></person-group><article-title>Denoising EEG signals for real-world BCI applications using GANs</article-title><source>Front. Neuroergonomics</source><year>2022</year><volume>2</volume><elocation-id>805573</elocation-id><pub-id pub-id-type="doi">10.3389/fnrgo.2021.805573</pub-id><pub-id pub-id-type="pmid">38235245</pub-id><pub-id pub-id-type="pmcid">PMC10790876</pub-id></element-citation></ref><ref id="B106-sensors-25-05770"><label>106.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name></person-group><article-title>A modified artifact subspace rejection algorithm based on frequency properties for meditation detection application</article-title><source>Proceedings of the 2022 12th International Conference on Information Technology in Medicine and Education (ITME)</source><conf-loc>Xiamen, China</conf-loc><conf-date>18&#8211;20 November 2022</conf-date><fpage>429</fpage><lpage>433</lpage></element-citation></ref><ref id="B107-sensors-25-05770"><label>107.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Arpaia</surname><given-names>P.</given-names></name><name name-style="western"><surname>De Bendetto</surname><given-names>E.</given-names></name><name name-style="western"><surname>Esposito</surname><given-names>A.</given-names></name><name name-style="western"><surname>Natalizio</surname><given-names>A.</given-names></name><name name-style="western"><surname>Parvis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pesola</surname><given-names>M.</given-names></name></person-group><article-title>Comparing artifact removal techniques for daily-life electroencephalography with few channels</article-title><source>Proceedings of the 2022 IEEE International Symposium on Medical Measurements and Applications (MeMeA)</source><conf-loc>Taormina, Messina, Italy</conf-loc><conf-date>22&#8211;24 June 2022</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B108-sensors-25-05770"><label>108.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Noorbasha</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Sudha</surname><given-names>G.F.</given-names></name></person-group><article-title>Electrical Shift and Linear Trend Artifacts Removal from Single Channel EEG Using SWT-GSTV Model</article-title><source>Proceedings of the International Conference on Soft Computing and Signal Processing</source><conf-loc>Hyderabad, India</conf-loc><conf-date>18&#8211;19 June 2021</conf-date><fpage>469</fpage><lpage>478</lpage></element-citation></ref><ref id="B109-sensors-25-05770"><label>109.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Du</surname><given-names>J.</given-names></name></person-group><article-title>Two-stage intelligent multi-type artifact removal for single-channel EEG settings: A GRU autoencoder based approach</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2022</year><volume>69</volume><fpage>3142</fpage><lpage>3154</lpage><pub-id pub-id-type="doi">10.1109/TBME.2022.3161994</pub-id><pub-id pub-id-type="pmid">35324430</pub-id></element-citation></ref><ref id="B110-sensors-25-05770"><label>110.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jayas</surname><given-names>T.</given-names></name><name name-style="western"><surname>Adarsh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Muralidharan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gubbi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pal</surname><given-names>A.</given-names></name></person-group><article-title>Computer Aided Detection of Dominant Artifacts in Ear-EEG Signal</article-title><source>Proceedings of the 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</source><conf-loc>Honolulu, Oahu, HI, USA</conf-loc><conf-date>1&#8211;4 October 2023</conf-date><fpage>4423</fpage><lpage>4428</lpage><pub-id pub-id-type="doi">10.1109/SMC53992.2023.10394058</pub-id></element-citation></ref><ref id="B111-sensors-25-05770"><label>111.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Narmada</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shukla</surname><given-names>M.</given-names></name></person-group><article-title>A novel adaptive artifacts wavelet Denoising for EEG artifacts removal using deep learning with Meta-heuristic approach</article-title><source>Multimed. Tools Appl.</source><year>2023</year><volume>82</volume><fpage>40403</fpage><lpage>40441</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-14949-2</pub-id></element-citation></ref><ref id="B112-sensors-25-05770"><label>112.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mahmud</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hossain</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Chowdhury</surname><given-names>M.E.</given-names></name><name name-style="western"><surname>Reaz</surname><given-names>M.B.I.</given-names></name></person-group><article-title>MLMRS-Net: Electroencephalography (EEG) motion artifacts removal using a multi-layer multi-resolution spatially pooled 1D signal reconstruction network</article-title><source>Neural Comput. Appl.</source><year>2023</year><volume>35</volume><fpage>8371</fpage><lpage>8388</lpage></element-citation></ref><ref id="B113-sensors-25-05770"><label>113.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name></person-group><article-title>Eyeblink detection algorithm based on joint optimization of VME and morphological feature extraction</article-title><source>IEEE Sens. J.</source><year>2023</year><volume>23</volume><fpage>21374</fpage><lpage>21384</lpage></element-citation></ref><ref id="B114-sensors-25-05770"><label>114.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Klados</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Bamidis</surname><given-names>P.D.</given-names></name></person-group><article-title>A semi-simulated EEG/EOG dataset for the comparison of EOG artifact rejection techniques</article-title><source>Data Brief</source><year>2016</year><volume>8</volume><fpage>1004</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1016/j.dib.2016.06.032</pub-id><pub-id pub-id-type="pmid">27508255</pub-id><pub-id pub-id-type="pmcid">PMC4969208</pub-id></element-citation></ref><ref id="B115-sensors-25-05770"><label>115.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kumaravel</surname><given-names>V.P.</given-names></name><name name-style="western"><surname>Farella</surname><given-names>E.</given-names></name></person-group><article-title>IMU-integrated Artifact Subspace Reconstruction for Wearable EEG Devices</article-title><source>Proceedings of the 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source><conf-loc>Istanbul, Turkey</conf-loc><conf-date>5&#8211;8 December 2023</conf-date><fpage>2508</fpage><lpage>2514</lpage></element-citation></ref><ref id="B116-sensors-25-05770"><label>116.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>A segmentation-denoising network for artifact removal from single-channel EEG</article-title><source>IEEE Sens. J.</source><year>2023</year><volume>23</volume><fpage>15115</fpage><lpage>15127</lpage></element-citation></ref><ref id="B117-sensors-25-05770"><label>117.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Shoeb</surname><given-names>A.H.</given-names></name></person-group><article-title>Application of Machine Learning to Epileptic Seizure Onset Detection and Treatment</article-title><source>Ph.D. Thesis</source><publisher-name>Massachusetts Institute of Technology</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2009</year></element-citation></ref><ref id="B118-sensors-25-05770"><label>118.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>A GAN guided parallel CNN and transformer network for EEG denoising</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2023</year><volume>70</volume><elocation-id>4005512</elocation-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/JBHI.2023.3277596</pub-id><pub-id pub-id-type="pmid">37220036</pub-id></element-citation></ref><ref id="B119-sensors-25-05770"><label>119.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>O&#8217;Sullivan</surname><given-names>M.E.</given-names></name><name name-style="western"><surname>Lightbody</surname><given-names>G.</given-names></name><name name-style="western"><surname>Mathieson</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Marnane</surname><given-names>W.P.</given-names></name><name name-style="western"><surname>Boylan</surname><given-names>G.B.</given-names></name><name name-style="western"><surname>O&#8217;Toole</surname><given-names>J.M.</given-names></name></person-group><article-title>Development of an EEG artefact detection algorithm and its application in grading neonatal hypoxic-ischemic encephalopathy</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>213</volume><fpage>118917</fpage></element-citation></ref><ref id="B120-sensors-25-05770"><label>120.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pi</surname><given-names>D.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Denosieformer: A transformer-based approach for single-channel EEG artifact removal</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>73</volume><fpage>2501116</fpage></element-citation></ref><ref id="B121-sensors-25-05770"><label>121.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hermans</surname><given-names>T.</given-names></name><name name-style="western"><surname>Smets</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lemmens</surname><given-names>K.</given-names></name><name name-style="western"><surname>Dereymaeker</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jansen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Naulaers</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zappasodi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Van Huffel</surname><given-names>S.</given-names></name><name name-style="western"><surname>Comani</surname><given-names>S.</given-names></name><name name-style="western"><surname>De Vos</surname><given-names>M.</given-names></name></person-group><article-title>A multi-task and multi-channel convolutional neural network for semi-supervised neonatal artefact detection</article-title><source>J. Neural Eng.</source><year>2023</year><volume>20</volume><fpage>026013</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1088/1741-2552/acbc4b</pub-id><pub-id pub-id-type="pmid">36791462</pub-id></element-citation></ref><ref id="B122-sensors-25-05770"><label>122.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bahadur</surname><given-names>I.N.</given-names></name><name name-style="western"><surname>Boppana</surname><given-names>L.</given-names></name></person-group><article-title>Efficient architecture for ocular artifacts removal from EEG: A Novel approach based on DWT-LMM</article-title><source>Microelectron. J.</source><year>2024</year><volume>150</volume><fpage>106284</fpage><pub-id pub-id-type="doi">10.1016/j.mejo.2024.106284</pub-id></element-citation></ref><ref id="B123-sensors-25-05770"><label>123.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Arpaia</surname><given-names>P.</given-names></name><name name-style="western"><surname>De Benedetto</surname><given-names>E.</given-names></name><name name-style="western"><surname>Esposito</surname><given-names>A.</given-names></name><name name-style="western"><surname>Natalizic</surname><given-names>A.</given-names></name><name name-style="western"><surname>Parvis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pesola</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sansone</surname><given-names>M.</given-names></name></person-group><article-title>Artifacts Removal from Low-Density EEG Measured with Dry Electrodes</article-title><source>Proceedings of the 2024 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)</source><conf-loc>St Albans, UK</conf-loc><conf-date>21&#8211;23 October 2024</conf-date><fpage>195</fpage><lpage>200</lpage></element-citation></ref><ref id="B124-sensors-25-05770"><label>124.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ingolfsson</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Benatti</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bernini</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ducouret</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ryvlin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Beniczky</surname><given-names>S.</given-names></name><name name-style="western"><surname>Benini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cossettini</surname><given-names>A.</given-names></name></person-group><article-title>Minimizing artifact-induced false-alarms for seizure detection in wearable EEG devices with gradient-boosted tree classifiers</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>2980</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-52551-0</pub-id><pub-id pub-id-type="pmid">38316856</pub-id><pub-id pub-id-type="pmcid">PMC10844293</pub-id></element-citation></ref><ref id="B125-sensors-25-05770"><label>125.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saleh</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>L.</given-names></name><name name-style="western"><surname>Casson</surname><given-names>A.J.</given-names></name></person-group><article-title>EEG artifact removal at the edge using AI hardware</article-title><source>IEEE Sens. Lett.</source><year>2024</year><volume>9</volume><fpage>7003004</fpage></element-citation></ref><ref id="B126-sensors-25-05770"><label>126.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nair</surname><given-names>S.</given-names></name><name name-style="western"><surname>James</surname><given-names>B.P.</given-names></name><name name-style="western"><surname>Leung</surname><given-names>M.F.</given-names></name></person-group><article-title>An optimized hybrid approach to denoising of EEG signals using CNN and LMS filtering</article-title><source>Electronics</source><year>2025</year><volume>14</volume><elocation-id>1193</elocation-id><pub-id pub-id-type="doi">10.3390/electronics14061193</pub-id></element-citation></ref><ref id="B127-sensors-25-05770"><label>127.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Rastegarnia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sanei</surname><given-names>S.</given-names></name></person-group><article-title>Signal artifacts and techniques for artifacts and noise removal</article-title><source>Signal Processing Techniques for Computational Health Informatics</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>23</fpage><lpage>79</lpage></element-citation></ref><ref id="B128-sensors-25-05770"><label>128.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kaya</surname><given-names>I.</given-names></name></person-group><article-title>A brief summary of EEG artifact handling</article-title><source>Brain-Computer Interface</source><publisher-name>IntechOpen</publisher-name><publisher-loc>London, UK</publisher-loc><year>2019</year><pub-id pub-id-type="doi">10.5772/intechopen.99127</pub-id></element-citation></ref><ref id="B129-sensors-25-05770"><label>129.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Villasana</surname><given-names>F.C.</given-names></name></person-group><article-title>Getting to Know EEG Artifacts and How to Handle Them in BrainVision Analyzer 2. Brain Products</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pressrelease.brainproducts.com/eeg-artifacts-handling-in-analyzer/" ext-link-type="uri">https://pressrelease.brainproducts.com/eeg-artifacts-handling-in-analyzer/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2023-10-03">(accessed on 3 October 2023)</date-in-citation></element-citation></ref><ref id="B130-sensors-25-05770"><label>130.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kane</surname><given-names>N.</given-names></name><name name-style="western"><surname>Acharya</surname><given-names>J.</given-names></name><name name-style="western"><surname>Beniczky</surname><given-names>S.</given-names></name><name name-style="western"><surname>Caboclo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Finnigan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kaplan</surname><given-names>P.W.</given-names></name><name name-style="western"><surname>Shibasaki</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pressler</surname><given-names>R.</given-names></name><name name-style="western"><surname>Van Putten</surname><given-names>M.J.</given-names></name></person-group><article-title>A revised glossary of terms most commonly used by clinical electroencephalographers and updated proposal for the report format of the EEG findings. Revision 2017</article-title><source>Clin. Neurophysiol. Pract.</source><year>2017</year><volume>2</volume><fpage>170</fpage><lpage>185</lpage><pub-id pub-id-type="pmid">30214992</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cnp.2017.07.002</pub-id><pub-id pub-id-type="pmcid">PMC6123891</pub-id></element-citation></ref><ref id="B131-sensors-25-05770"><label>131.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amin</surname><given-names>U.</given-names></name><name name-style="western"><surname>Nascimento</surname><given-names>F.A.</given-names></name><name name-style="western"><surname>Karakis</surname><given-names>I.</given-names></name><name name-style="western"><surname>Schomer</surname><given-names>D.</given-names></name><name name-style="western"><surname>Benbadis</surname><given-names>S.R.</given-names></name></person-group><article-title>Normal variants and artifacts: Importance in EEG interpretation</article-title><source>Epileptic Disord.</source><year>2023</year><volume>25</volume><fpage>591</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1002/epd2.20040</pub-id><pub-id pub-id-type="pmid">36938895</pub-id></element-citation></ref><ref id="B132-sensors-25-05770"><label>132.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xing</surname><given-names>L.</given-names></name><name name-style="western"><surname>Casson</surname><given-names>A.J.</given-names></name></person-group><article-title>Deep autoencoder for real-time single-channel EEG cleaning and its smartphone implementation using tensorflow lite with hardware/software acceleration</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2024</year><volume>71</volume><fpage>3111</fpage><lpage>3122</lpage><pub-id pub-id-type="doi">10.1109/TBME.2024.3408331</pub-id><pub-id pub-id-type="pmid">38829759</pub-id></element-citation></ref><ref id="B133-sensors-25-05770"><label>133.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cho</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ahn</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ahn</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kwon</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jun</surname><given-names>S.C.</given-names></name></person-group><article-title>EEG datasets for motor imagery brain&#8211;computer interface</article-title><source>GigaScience</source><year>2017</year><volume>6</volume><fpage>gix034</fpage><pub-id pub-id-type="doi">10.1093/gigascience/gix034</pub-id><pub-id pub-id-type="pmcid">PMC5493744</pub-id><pub-id pub-id-type="pmid">28472337</pub-id></element-citation></ref><ref id="B134-sensors-25-05770"><label>134.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cheung</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>R.</given-names></name></person-group><source>Ear-EEG Recording for Brain Computer Interface of Motor Task</source><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year><pub-id pub-id-type="doi">10.21227/j7rq-2p11</pub-id></element-citation></ref><ref id="B135-sensors-25-05770"><label>135.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Obeid</surname><given-names>I.</given-names></name><name name-style="western"><surname>Picone</surname><given-names>J.</given-names></name></person-group><article-title>The temple university hospital EEG data corpus</article-title><source>Front. Neurosci.</source><year>2016</year><volume>10</volume><elocation-id>196</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2016.00196</pub-id><pub-id pub-id-type="pmid">27242402</pub-id><pub-id pub-id-type="pmcid">PMC4865520</pub-id></element-citation></ref><ref id="B136-sensors-25-05770"><label>136.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>W.L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>B.L.</given-names></name></person-group><article-title>Investigating Critical Frequency Bands and Channels for EEG-based Emotion Recognition with Deep Neural Networks</article-title><source>IEEE Trans. Auton. Ment. Dev.</source><year>2015</year><volume>7</volume><fpage>162</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2015.2431497</pub-id></element-citation></ref><ref id="B137-sensors-25-05770"><label>137.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schelter</surname><given-names>B.</given-names></name><name name-style="western"><surname>Winterhalder</surname><given-names>M.</given-names></name><name name-style="western"><surname>Maiwald</surname><given-names>T.</given-names></name><name name-style="western"><surname>Brandt</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schad</surname><given-names>A.</given-names></name><name name-style="western"><surname>Timmer</surname><given-names>J.</given-names></name><name name-style="western"><surname>Schulze-Bonhage</surname><given-names>A.</given-names></name></person-group><article-title>Do false predictions of seizures depend on the state of vigilance? A report from two seizure-prediction methods and proposed remedies</article-title><source>Epilepsia</source><year>2006</year><volume>47</volume><fpage>2058</fpage><lpage>2070</lpage><pub-id pub-id-type="doi">10.1111/j.1528-1167.2006.00848.x</pub-id><pub-id pub-id-type="pmid">17201704</pub-id></element-citation></ref><ref id="B138-sensors-25-05770"><label>138.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaya</surname><given-names>M.</given-names></name><name name-style="western"><surname>Binli</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Ozbay</surname><given-names>E.</given-names></name><name name-style="western"><surname>Yanar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mishchenko</surname><given-names>Y.</given-names></name></person-group><article-title>A large electroencephalographic motor imagery dataset for electroencephalographic brain computer interfaces</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>180211</fpage><pub-id pub-id-type="doi">10.1038/sdata.2018.211</pub-id><pub-id pub-id-type="pmid">30325349</pub-id><pub-id pub-id-type="pmcid">PMC6190745</pub-id></element-citation></ref><ref id="B139-sensors-25-05770"><label>139.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Torkamani-Azar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kanik</surname><given-names>S.D.</given-names></name><name name-style="western"><surname>Aydin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cetin</surname><given-names>M.</given-names></name></person-group><article-title>Prediction of reaction time and vigilance variability from spatio-spectral features of resting-state EEG in a long sustained attention task</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2020</year><volume>24</volume><fpage>2550</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2020.2980056</pub-id><pub-id pub-id-type="pmid">32167917</pub-id></element-citation></ref><ref id="B140-sensors-25-05770"><label>140.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reichert</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tellez Ceja</surname><given-names>I.F.</given-names></name><name name-style="western"><surname>Sweeney-Reed</surname><given-names>C.M.</given-names></name><name name-style="western"><surname>Heinze</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Hinrichs</surname><given-names>H.</given-names></name><name name-style="western"><surname>D&#252;rschmid</surname><given-names>S.</given-names></name></person-group><article-title>Impact of stimulus features on the performance of a gaze-independent brain-computer interface based on covert spatial attention shifts</article-title><source>Front. Neurosci.</source><year>2020</year><volume>14</volume><elocation-id>591777</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2020.591777</pub-id><pub-id pub-id-type="pmid">33335470</pub-id><pub-id pub-id-type="pmcid">PMC7736242</pub-id></element-citation></ref><ref id="B141-sensors-25-05770"><label>141.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rantanen</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ilves</surname><given-names>M.</given-names></name><name name-style="western"><surname>Vehkaoja</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kontunen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lylykangas</surname><given-names>J.</given-names></name><name name-style="western"><surname>M&#228;kel&#228;</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rautiainen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Surakka</surname><given-names>V.</given-names></name><name name-style="western"><surname>Lekkala</surname><given-names>J.</given-names></name></person-group><article-title>A survey on the feasibility of surface EMG in facial pacing</article-title><source>Proceedings of the 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>6&#8211;20 August 2016</conf-date><fpage>1688</fpage><lpage>1691</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/EMBC.2016.7591040</pub-id><pub-id pub-id-type="pmid">28268652</pub-id></element-citation></ref><ref id="B142-sensors-25-05770"><label>142.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kanoga</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nakanishi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mitsukura</surname><given-names>Y.</given-names></name></person-group><article-title>Assessing the effects of voluntary and involuntary eyeblinks in independent components of electroencephalogram</article-title><source>Neurocomputing</source><year>2016</year><volume>193</volume><fpage>20</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2016.01.057</pub-id></element-citation></ref><ref id="B143-sensors-25-05770"><label>143.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Delorme</surname><given-names>A.</given-names></name><name name-style="western"><surname>Makeig</surname><given-names>S.</given-names></name></person-group><article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>J. Neurosci. Methods</source><year>2004</year><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="B144-sensors-25-05770"><label>144.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kahle</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wilson</surname><given-names>G.</given-names></name><name name-style="western"><surname>Russell</surname><given-names>C.</given-names></name></person-group><article-title>Removal of ocular artifacts from EEG: A comparison of adaptive filtering method and regression method using simulated data</article-title><source>Proceedings of the 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference (EMBC 2005)</source><conf-loc>Shanghai, China</conf-loc><conf-date>1&#8211;4 September 2005</conf-date><fpage>1110</fpage><lpage>1113</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/IEMBS.2005.1616614</pub-id><pub-id pub-id-type="pmid">17282383</pub-id></element-citation></ref><ref id="B145-sensors-25-05770"><label>145.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yeung</surname><given-names>N.</given-names></name><name name-style="western"><surname>Bogacz</surname><given-names>R.</given-names></name><name name-style="western"><surname>Holroyd</surname><given-names>C.B.</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>J.D.</given-names></name></person-group><article-title>Detection of synchronized oscillations in the electroencephalogram: An evaluation of methods</article-title><source>Psychophysiology</source><year>2004</year><volume>41</volume><fpage>822</fpage><lpage>832</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2004.00239.x</pub-id><pub-id pub-id-type="pmid">15563335</pub-id></element-citation></ref><ref id="B146-sensors-25-05770"><label>146.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Kothe</surname><given-names>C.</given-names></name><name name-style="western"><surname>Iversen</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Miyakoshi</surname><given-names>M.</given-names></name></person-group><article-title>Juggler&#8217;s ASR: Unpacking the principles of artifact subspace reconstruction for revision toward extreme MoBI</article-title><source>J. Neurosci. Methods</source><year>2025</year><volume>420</volume><fpage>110465</fpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2025.110465</pub-id><pub-id pub-id-type="pmid">40324599</pub-id></element-citation></ref><ref id="B147-sensors-25-05770"><label>147.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gorjan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gramann</surname><given-names>K.</given-names></name><name name-style="western"><surname>De Pauw</surname><given-names>K.</given-names></name><name name-style="western"><surname>Marusic</surname><given-names>U.</given-names></name></person-group><article-title>Removal of movement-induced EEG artifacts: Current state of the art and guidelines</article-title><source>J. Neural Eng.</source><year>2022</year><volume>19</volume><fpage>011004</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1088/1741-2552/ac542c</pub-id><pub-id pub-id-type="pmid">35147512</pub-id></element-citation></ref><ref id="B148-sensors-25-05770"><label>148.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Giangrande</surname><given-names>A.</given-names></name><name name-style="western"><surname>Botter</surname><given-names>A.</given-names></name><name name-style="western"><surname>Piitulainen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cerone</surname><given-names>G.L.</given-names></name></person-group><article-title>Motion artifacts in dynamic EEG recordings: Experimental observations, electrical modelling, and design considerations</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6363</elocation-id><pub-id pub-id-type="doi">10.3390/s24196363</pub-id><pub-id pub-id-type="pmid">39409399</pub-id><pub-id pub-id-type="pmcid">PMC11479364</pub-id></element-citation></ref><ref id="B149-sensors-25-05770"><label>149.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chi</surname><given-names>Y.M.</given-names></name><name name-style="western"><surname>Cauwenberghs</surname><given-names>G.</given-names></name></person-group><article-title>Wireless non-contact EEG/ECG electrodes for body sensor networks</article-title><source>Proceedings of the 2010 International Conference on Body Sensor Networks (BSN 2010)</source><conf-loc>Singapore</conf-loc><conf-date>7&#8211;9 June 2010</conf-date><fpage>297</fpage><lpage>301</lpage></element-citation></ref><ref id="B150-sensors-25-05770"><label>150.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Benatti</surname><given-names>S.</given-names></name><name name-style="western"><surname>Milosevic</surname><given-names>B.</given-names></name><name name-style="western"><surname>Tomasini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Farella</surname><given-names>E.</given-names></name><name name-style="western"><surname>Schoenle</surname><given-names>P.</given-names></name><name name-style="western"><surname>Bunjaku</surname><given-names>P.</given-names></name><name name-style="western"><surname>Rovere</surname><given-names>G.</given-names></name><name name-style="western"><surname>Fateh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Benini</surname><given-names>L.</given-names></name></person-group><article-title>Multiple biopotentials acquisition system for wearable applications</article-title><source>Proceedings of the Special Session on Smart Medical Devices-From Lab to Clinical Practice (DATE 2015)</source><conf-loc>Lisbon, Portugal</conf-loc><conf-date>12&#8211;15 January 2015</conf-date><volume>Volume 2</volume><fpage>260</fpage><lpage>268</lpage></element-citation></ref><ref id="B151-sensors-25-05770"><label>151.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tomasini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Benatti</surname><given-names>S.</given-names></name><name name-style="western"><surname>Milosevic</surname><given-names>B.</given-names></name><name name-style="western"><surname>Farella</surname><given-names>E.</given-names></name><name name-style="western"><surname>Benini</surname><given-names>L.</given-names></name></person-group><article-title>Power line interference removal for high-quality continuous biosignal monitoring with low-power wearable devices</article-title><source>IEEE Sens. J.</source><year>2016</year><volume>16</volume><fpage>3887</fpage><lpage>3895</lpage></element-citation></ref><ref id="B152-sensors-25-05770"><label>152.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mitra</surname><given-names>S.</given-names></name><name name-style="western"><surname>Van Hoof</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yazicioglu</surname><given-names>R.F.</given-names></name><name name-style="western"><surname>Makinwa</surname><given-names>K.A.</given-names></name></person-group><article-title>Active electrodes for wearable EEG acquisition: Review and electronics design methodology</article-title><source>IEEE Rev. Biomed. Eng.</source><year>2017</year><volume>10</volume><fpage>187</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1109/rbme.2017.2656388</pub-id><pub-id pub-id-type="pmid">28113349</pub-id></element-citation></ref><ref id="B153-sensors-25-05770"><label>153.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalevo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Miettinen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Leino</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kainulainen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Korkalainen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Myllymaa</surname><given-names>K.</given-names></name><name name-style="western"><surname>T&#246;yr&#228;s</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lepp&#228;nen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Laitinen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Myllymaa</surname><given-names>S.</given-names></name></person-group><article-title>Effect of sweating on electrode-skin contact impedances and artifacts in EEG recordings with various screen-printed Ag/Agcl electrodes</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>50934</fpage><lpage>50943</lpage></element-citation></ref><ref id="B154-sensors-25-05770"><label>154.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>JCGM</collab></person-group><source>International Vocabulary of Metrology&#8212;Basic and General Concepts and Associated Terms (VIM)</source><edition>3rd ed.</edition><publisher-name>Bureau International des Poids et Mesures (BIPM)</publisher-name><publisher-loc>S&#232;vres, France</publisher-loc><year>2012</year></element-citation></ref><ref id="B155-sensors-25-05770"><label>155.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Anders</surname><given-names>P.</given-names></name><name name-style="western"><surname>M&#252;ller</surname><given-names>H.</given-names></name><name name-style="western"><surname>Skj&#230;ret-Maroni</surname><given-names>N.</given-names></name><name name-style="western"><surname>Vereijken</surname><given-names>B.</given-names></name><name name-style="western"><surname>Baumeister</surname><given-names>J.</given-names></name></person-group><article-title>The influence of motor tasks and cut-off parameter selection on artifact subspace reconstruction in EEG recordings</article-title><source>Med. Biol. Eng. Comput.</source><year>2020</year><volume>58</volume><fpage>2673</fpage><lpage>2683</lpage><pub-id pub-id-type="pmid">32860085</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s11517-020-02252-3</pub-id><pub-id pub-id-type="pmcid">PMC7560919</pub-id></element-citation></ref><ref id="B156-sensors-25-05770"><label>156.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mullen</surname><given-names>T.R.</given-names></name><name name-style="western"><surname>Kothe</surname><given-names>C.A.</given-names></name><name name-style="western"><surname>Chi</surname><given-names>Y.M.</given-names></name><name name-style="western"><surname>Ojeda</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kerth</surname><given-names>T.</given-names></name><name name-style="western"><surname>Makeig</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>T.P.</given-names></name><name name-style="western"><surname>Cauwenberghs</surname><given-names>G.</given-names></name></person-group><article-title>Real-time neuroimaging and cognitive monitoring using wearable dry EEG</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2015</year><volume>62</volume><fpage>2553</fpage><lpage>2567</lpage><pub-id pub-id-type="doi">10.1109/tbme.2015.2481482</pub-id><pub-id pub-id-type="pmid">26415149</pub-id><pub-id pub-id-type="pmcid">PMC4710679</pub-id></element-citation></ref><ref id="B157-sensors-25-05770"><label>157.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Pion-Tonachini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>T.P.</given-names></name></person-group><article-title>Evaluation of artifact subspace reconstruction for automatic artifact components removal in multi-channel EEG recordings</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2019</year><volume>67</volume><fpage>1114</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1109/tbme.2019.2930186</pub-id><pub-id pub-id-type="pmid">31329105</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05770-f001" orientation="portrait"><label>Figure 1</label><caption><p>PRISMA&#8212;flow of articles selection process.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05770-g001.jpg"/></fig><fig position="float" id="sensors-25-05770-f002" orientation="portrait"><label>Figure 2</label><caption><p>Comparison of publication trends between articles on wearable EEG (blue) extracted from Scopus and studies collected by this review focusing on artifact detection and removal in wearable EEG (red). Artifact-related issues in wearable EEG remain underexplored, with a stagnating publication trend.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05770-g002.jpg"/></fig><fig position="float" id="sensors-25-05770-f003" orientation="portrait"><label>Figure 3</label><caption><p>Pie chart of the percentage distribution of artifact categories addressed in the reviewed articles. For each category, a corresponding bar chart indicates its corresponding sources. &#8220;Source Not Specified&#8221; (S.N.S.) is used when the artifact category is indicated without explicit information on its source. An explanatory table for other technical acronyms is recommended at the end of the document.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05770-g003.jpg"/></fig><fig position="float" id="sensors-25-05770-f004" orientation="portrait"><label>Figure 4</label><caption><p>Number of studies addressing a specific algorithm class, according to the classification proposed in [<xref rid="B57-sensors-25-05770" ref-type="bibr">57</xref>]. The classes <italic toggle="yes">Deep Learning methods</italic>, <italic toggle="yes">ASR-based methods</italic> and <italic toggle="yes">Other</italic> are included to account for algorithms that can not be mapped to the classes in [<xref rid="B57-sensors-25-05770" ref-type="bibr">57</xref>] (e.g., [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>,<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>,<xref rid="B75-sensors-25-05770" ref-type="bibr">75</xref>,<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>,<xref rid="B97-sensors-25-05770" ref-type="bibr">97</xref>]).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05770-g004.jpg"/></fig><fig position="float" id="sensors-25-05770-f005" orientation="portrait"><label>Figure 5</label><caption><p>Number of articles focusing on artifact detection strategies (green) and artifact category identification strategies (orange) across the reviewed studies. Detection strategies are further classified based on their robustness, defined as the extent of artifact sources used for validation. Only two studies attempt to identify the specific category or source of artifacts.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05770-g005.jpg"/></fig><table-wrap position="float" id="sensors-25-05770-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05770-t001_Table 1</object-id><label>Table 1</label><caption><p>Acquisition setup and performance assessment methods of the algorithms proposed by the collected studies. The acquisition setup includes the experimental sample and task and the EEG channel setup. The signal processing methods refer to the algorithm employed, the specific artifact addressed, the reference signal, as well as the performance parameters and metrics with their corresponding results (reported in parentheses). When a study compared multiple algorithms, only the nomenclature and results of the best-performing one are reported. Regarding focused artifacts, a citation is provided only when the artifact is taken from a public dataset. The reported algorithms cover both the artifact detection and removal phases. S.N.S. = Source Not Specified, used when the artifact category is indicated without explicit information on its source. R = real recordings; S = simulated signals; SS = semi-simulated signals. &#8220;Not Applicable&#8221; (n.a.) indicates parameters not relevant in the context (e.g., acquisition setup for simulated data). &#8220;Not Reported&#8221; (n.r.) refers to parameters relevant but unspecified by the authors (e.g., participant sex). &#8220;Not Considered&#8221; (n.c.) refers to parameters relevant but not used in the study (e.g., absence of pre-processing). An explanatory table for other technical acronyms is recommended at the end of the document.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Article</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Focused Artifact Category (Source)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Experimental Sample</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Task Description</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Channel Setup No. &amp; Type (Location)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reference Signal</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Assessment Parameters</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Assessment Metrics (Results)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Algorithm</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sweeney et al. (2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Instrumental<break/>(Cable<break/>movements) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 6 subjects<break/>4 trials &#215; 540 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 n.r.<break/>(Fpz, Fp1)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>channel</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR (<inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> SNR:<break/>(a) 5.1 dB; (b) 9.7 dB;<break/>(c) 8.9 dB),<break/>Correlation<break/>(improvement rate:<break/>(a) 37.66%; (b) 83.13%;<break/>(c) 76.5%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(a) Adaptive Filter;<break/>(b) Kalman Filter;<break/>(c) EEMD-ICA</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Matiko et al.<break/>(2013) [<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: n.r.<break/>60 trials &#215; 1 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 dry (Fp1)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC<break/>(improvement rate: 30.56%)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">MCA based<break/>on STFT</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operational speed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency (26.90 ms)</td></tr><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Peng&#160;et&#160;al.<break/>(2013) [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>]</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye movements,<break/>blinks)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>S</bold>: 50 trials &#215; 20 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 n.a. (n.a.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Initial EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE (0.00531),<break/>MAE (frequency:<break/>
<inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.02233, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.01436,<break/>
<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.00382, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.00055;<break/> time: 0.00531)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">DWT + ANC</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 25 subjects<break/>1 trial &#215; 120 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">3 dry<break/>(Fp1, Fp2, Fpz)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency domain<break/>correlation<break/>(numerical values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operational speed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency (numerical values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 22 subjects<break/>1 trial &#215; 40 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency domain correlation (numerical values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mihajlovic et al.<break/>(2014) [<xref rid="B67-sensors-25-05770" ref-type="bibr">67</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Movement<break/>(Head)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 6 subjects<break/>3 trials &#215; 60 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Motor tasks</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4 dry<break/>(C3, C4, Cz and Pz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG baseline</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral Score<break/>(reduction rate: &#8764;60&#8211;70%),
Distribution Score<break/>(reduction rate: &#8764;70&#8211;80%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BPF + leaky least-mean square MCAF</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Zhao&#160;et&#160;al.<break/>(2014) [<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>S</bold>: 50 trials &#215; 30 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 n.a. (n.a.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Initial EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE (0.6443),<break/>MAE (<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula>: 0.2501;<break/>
<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula>: 0.1545;<break/><inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>: 0.0975; <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>: 0.0174)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">DWT + APF</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 20 subjects<break/>1 trial &#215; 120 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">3 dry<break/>(Fp1, Fp2, Fpz)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EEG baseline</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency domain correlation<break/>(numerical values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operational speed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency (5000 points, 1 s)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Majmudar<break/>et al. (2015)<break/>[<xref rid="B69-sensors-25-05770" ref-type="bibr">69</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 3 subjects<break/>1 trial &#215; 45 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">2 wet<break/>(Fp1, Fp2)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TFA (numerical values n.r.),<break/>MSC plot (f &gt; 16&#160;Hz: &#8764;1;<break/>f &lt; 16&#160;Hz: &lt;1),<break/>CC (0.39&#160;&#177;&#160;0.25), MI (0.91&#160;&#177;&#160;0.12)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Algebraic<break/>approach + DWT</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operational speed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency<break/>(improvement rate: &#8764;25%)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kim&#160;et&#160;al.<break/>(2015) [<xref rid="B70-sensors-25-05770" ref-type="bibr">70</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Movement<break/>(Body, limb)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 5 subjects<break/>1 trial &#215; 300 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state;<break/>dual-task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14 wet<break/>(AF3, F7, F3,<break/>FC5, T7, P7, O1,<break/>O2, P8, T8, FC6,<break/>F4, F8, AF4)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSD at SSVEP<break/>and P300<break/>frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR (<inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> SNR:<break/> 0.26 &#177; 0.11 (SSVEP);<break/>0.07 &#177; 0.10 (P300))</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fast ICA + Kalman<break/>filter + SVM</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Rahman&#160;et&#160;al.<break/>(2015) [<xref rid="B71-sensors-25-05770" ref-type="bibr">71</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>S</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 n.a. (n.a.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Initial EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR (20.23 dB),<break/>MSE (<inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4.60</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">SG filter + ANFIS</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: n.r.<break/>1 trial &#215; 55 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">14 n.r.<break/>(only FP1<break/>is reported)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EOG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR (16.98 dB),<break/>MSE (<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.39</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (measured Fp1/<break/>estimated Fp1: 0.1478;<break/>measured EOG/<break/>estimated eye blink: 0.9899)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D&#8217;Rozario et<break/>al. (2015) [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye<break/>movements,<break/>blinks); Muscular<break/>(S.N.S.);<break/>Movement (S.N.S.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 24 subjects<break/>2&#8211;4 trial &#215; 28,800 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sleep</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(a) 6 wet (C3,<break/>C4, Fz, Cz,<break/>Pz, and&#160;Oz),<break/>(b) 5 wet (C3,<break/>Fz, Cz, Pz and O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifacts<break/>identified by<break/>visual inspection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cohen&#8217;s kappa (0.53&#160;&#177;&#160;0.16),<break/>Classification<break/>Accuracy (93.5&#160;&#177;&#160;3.0%),<break/>TPR (68.7&#160;&#177;&#160;7.6%),<break/>FPR (4.3&#160;&#177;&#160;1.8%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SD-based automated<break/>artifact detection<break/>and removal</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chang&#160;et&#160;al.<break/>(2016) [<xref rid="B73-sensors-25-05770" ref-type="bibr">73</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular<break/>(Eye blinks)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 24 subjects<break/>10 trial &#215; 15 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cognitive task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 wet<break/>(Fp1, Fp2, vEOG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">vEOG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TPR (&#8764;99%),<break/>FPR (&#8764;10%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSDW</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zhao&#160;et&#160;al.<break/>(2017) [<xref rid="B74-sensors-25-05770" ref-type="bibr">74</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (S.N.S.);<break/>Muscular (S.N.S.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 170 subjects<break/>1 trial &#215; 72/90 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state;<break/>audio stimulation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 n.r.<break/>(Fp1, Fp2, Fpz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Temporal trend comparison<break/>(numerical values n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet transform<break/>+ Kalman filter</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thammasan et<break/>al. (2017) [<xref rid="B75-sensors-25-05770" ref-type="bibr">75</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular<break/>(Eye movements);<break/>Muscolar (S.N.S.);<break/>EMI (PLN)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 9 subjects<break/>24 trials &#215;<break/>67/112 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state;<break/>audio stimulation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 soft dry<break/>(Fp1, Fp2, F3, F4,<break/>F7, F8, T7, T8)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifacts<break/>identified by<break/>visual inspection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics on<break/>other topics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Enhancement on<break/>Classification<break/>Accuracy (n.r.),<break/>Enhancement on<break/>MCC (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automatic rejection<break/>based on Regression<break/>(pop_rejtrend), Joint<break/>Probability<break/>(pop_jointprob),<break/>Kurtosis<break/>(pop_rejkurt),<break/>FFT (pop_rejcont)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Hu&#160;et&#160;al.<break/>(2017) [<xref rid="B76-sensors-25-05770" ref-type="bibr">76</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements,<break/>blinks);<break/>Instrumental<break/>(EII, TEN)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>S</bold>: n.r. &#215; 8 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 n.a. (n.a.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Initial EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics<break/></td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification accuracy<break/>(95.8%)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Adaptive SSA</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 3 subjects<break/>1 trial &#215; 120 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 wet<break/>(frontal electrodes)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Power spectrum differences (numerical values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dehzangi<break/>et al. (2018) [<xref rid="B77-sensors-25-05770" ref-type="bibr">77</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular<break/>(Eye blinks)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 5 subjects<break/>4 trials &#215; 240/360 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cognitive task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7 wet<break/>(F7, Fz, F8,<break/>T7, T8, Pz, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifact labels</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DTW distances<break/>(multi-score detection<break/>performance: 87.4&#160;&#177;&#160;8.1%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DTW score +<break/>K-means clustering<break/>+ SVM</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Cheng et al.<break/>(2019) [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Cardiac&#160;[<xref rid="B79-sensors-25-05770" ref-type="bibr">79</xref>];<break/>Ocular<break/>(Eye movements,<break/>blinks) [<xref rid="B80-sensors-25-05770" ref-type="bibr">80</xref>,<xref rid="B81-sensors-25-05770" ref-type="bibr">81</xref>];<break/>Muscular<break/>(LiMC) [<xref rid="B80-sensors-25-05770" ref-type="bibr">80</xref>,<xref rid="B81-sensors-25-05770" ref-type="bibr">81</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 11 subjects<break/>n.r. &#215; 10 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state;<break/>motor-imagery</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r. wet (n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE<break/>(triple contamination:<break/>SNR&#160;=&#160;0.5: 0.23&#160;&#177;&#160;0.06;<break/>SNR&#160;=&#160;1.0: 0.18&#160;&#177;&#160;0.04;<break/>SNR&#160;=&#160;1.5: 0.15&#160;&#177;&#160;0.03)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">SSA + ICA</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC<break/>(triple contamination:<break/>SNR&#160;=&#160;0.5: 0.78&#160;&#177;&#160;0.06;<break/>SNR&#160;=&#160;1.0: 0.82&#160;&#177;&#160;0.04;<break/>SNR&#160;=&#160;1.5: 0.85&#160;&#177;&#160;0.03)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Val-Calvo<break/>et al. (2019) [<xref rid="B82-sensors-25-05770" ref-type="bibr">82</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>:<break/>15 subjects<break/>15 trials &#215; n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Video<break/>stimulation</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">8 n.r.<break/>(AF3, T7, TP7, P7,<break/>AF4, T8, TP8, P8)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CORR<break/>(0.87 (all bands), 0.86 (<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula>)),<break/>MI (0.66 (all bands), 0.64 (<inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula>))</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EAWICA</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE<break/>(0.27 (all bands), 0.29 (<inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula>))</td></tr><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Grosselin&#160;et&#160;al.<break/>(2019) [<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>]</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements,<break/>blinks); Muscular<break/>(MaMC, TeMC);<break/>Instrumental<break/>(Clipping,<break/>electrode pop);<break/>Movement<break/>(Body)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R1</bold>:<break/>3 subjects n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32 wet (n.r.)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EEG baseline</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Accuracy</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">SNR-based accuracy<break/>(SNR &lt; 0 dB: 99.8%;<break/>0 &#8804; SNR &lt; 10 dB: 82.5%;<break/>SNR &#8805; 10 dB: 43.13%)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Classification-based<break/>approach</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R2</bold>: 21 subjects<break/>n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 dry (P3, P4)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R3</bold>: 10 subjects<break/>n.r. &#215; 60 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state<break/>(altert condition)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 wet (P3, P4)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Classification<break/>Performance<break/>Metrics</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Classification Accuracy<break/>(92.2 &#177; 2.2%)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R4</bold>: 10 subjects<break/>n.r. &#215; 60 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state<break/>(altert condition)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 dry (P3, P4)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rosanne et al.<break/>(2019) [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular<break/>(Eye blinks);<break/>Movement<break/>(Body, limb)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 48 subjects<break/>6 trials &#215; 1200 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dual-task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 n.r. (FP1,<break/>FP2, AF7, AF8,<break/>T9, T10, P3, P4)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification Performance Metrics on other topics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Enhancement on<break/>Classification<break/>Accuracy (no movement:<break/>(b) 10%; medium<break/>physical activity:<break/>(b) 4%; high physical<break/>activity: (a) 4%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(a) ASR+wICA+<break/>Random Forest,<break/>(b) ASR+ADJUST+<break/>Random Forest</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inoue et al.<break/>(2019) [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye<break/>movements,<break/>blinks); Musco-<break/>lar (OOcMC);<break/>Movement (Body)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 10 subjects<break/>1 trial &#215; n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state;<break/>Motor task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 n.r. (F3, C3,<break/>T3, O1, F4,<break/>C4, T4, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recorded video</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automatic detection<break/>algorithm based on<break/>frequency analysis</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Blum et al.<break/>(2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 27 subjects<break/>1 trial &#215; n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state;<break/>dual-task</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">24 wet (n.r.)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EEG baseline</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR (numerical<break/> values n.r.)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Riemannian ASR</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Blink amplitude<break/>(similarity value: 0.15)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operational speed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency (5.6&#160;&#177;&#160;0.7 s)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Butkevi&#269;i&#363;t&#279;<break/>et al. (2019)<break/>[<xref rid="B86-sensors-25-05770" ref-type="bibr">86</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Movement<break/>(Body, limb)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: n.r<break/>10 trials &#215; 60 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Motor tasks</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pearson&#8217;s correlation<break/>coefficient (0.055&#160;&#177;&#160;0.058)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEADS + EMD</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Albuquerque<break/>et al. (2019)<break/>[<xref rid="B87-sensors-25-05770" ref-type="bibr">87</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (S.N.S.)<break/>Movement<break/>(Body, limb)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 47 subjects<break/>2 trials &#215; n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Motor task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 dry<break/>(T9, AF7, FP1,<break/>FP2, AF8, T10)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSD (ANOVA:<break/> 0.8715&#160;&#177;&#160;0.0699;<break/> mRMR: 0.8706&#160;&#177;&#160;0.0701),<break/>AMRC (ANOVA:<break/>0.8815&#160;&#177;&#160;0.0521;<break/>mRMR: 0.8440&#160;&#177;&#160;0.0608)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">wICA</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Liu et<break/>al. (2019) [<xref rid="B88-sensors-25-05770" ref-type="bibr">88</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Muscular (S.N.S.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 31 subjects<break/>n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">6 wet<break/>(n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE (numerical<break/>values n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">FMEMD-CCA</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (numerical<break/>values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Casadei et al.<break/>(2020) [<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Other (generic<break/>large artifacts);<break/>Instrumental<break/>(Electrode pop)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 1 subject<break/>n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 n.r. (O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Band-pass<break/>filtered EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Amplitude and<break/>phase consistency<break/>(numerical<break/> values n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Model-based<break/>amplitude estimation</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Islam&#160;et&#160;al.<break/>(2020) [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Instrumental<break/>(Cable<break/>movements);<break/>Movement<break/>(Body, limb)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 6 subjetcs,<break/>9 trials &#215; 240 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state;<break/>motor tasks</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">21 dry (Fp1, Fp2,<break/>F7, F3, Fz, F4,<break/>F8, A1, T3, C3,<break/>Cz, C4, T4, A2,<break/>T5, P3, Pz, P4,<break/>T6, O1, O2)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EEG baseline</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifact reduction<break/>rate (6.96&#160;&#177;&#160;2.96%),<break/>SNR (<inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> SNR:<break/> 10.74&#160;&#177;&#160;4.24 dB),<break/>RMSE (<inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> RMSE:<break/> 48.71&#160;&#177;&#160;36.14 mV)</td><td rowspan="2" align="left" valign="middle" colspan="1">Infomax ICA</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSD distortion (improvement:<break/> 51.00&#160;&#177;&#160;21.36%),<break/>correlation (improvement:<break/> 77.31&#160;&#177;&#160;12.57%),<break/>coherence (improvement:<break/> 94.82&#160;&#177;&#160;5.54%)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>:<break/>5 subjects<break/>n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Accuracy (90.8&#160;&#177;&#160;4.7%),<break/>TPR (84.4&#160;&#177;&#160;22.8%),<break/>FPR (45.1&#160;&#177;&#160;59.7%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Noorbasha et<break/>al. (2020) [<xref rid="B91-sensors-25-05770" ref-type="bibr">91</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (S.N.S.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>:<break/>3 subjects<break/>1 trial &#215; 120 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 wet<break/>(frontal<break/>electrodes)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR-based RRMSE<break/>(SNR&#160;=&#160;8 dB,<break/> RRMSE&#160;=&#160;98%),<break/>MAE (<inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> MAE:<break/> &#8722;17.43&#160;&#177;&#160;1.11 dB)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ov-ASSA + ANC</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Dey&#160;et&#160;al.<break/>(2020) [<xref rid="B92-sensors-25-05770" ref-type="bibr">92</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye blinks);<break/>Muscolar (CSMC)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 20 subjects<break/>10 trials &#215; 10 s</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">19 wet (n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification Performance Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification Accuracy<break/>(82.1&#160;&#177;&#160;2.9%),<break/>F1-score (0.800 &#177; 0.023)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">MLP-based model</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inference time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hardware<break/>efficiency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Power consumption<break/>(over 70% reduction<break/> of model size with<break/> &gt;3% loss in accuracy)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Liu et al.<break/>(2021) [<xref rid="B93-sensors-25-05770" ref-type="bibr">93</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Muscular<break/>(MaMC, TeMC)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 10 subjects<break/>24 trials &#215; 7 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSVEP</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 wet (POz, PO3,<break/>PO4, PO5, PO6,<break/>Oz, O1, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EMG reference</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Accuracy<break/>improvement<break/>(1-channel 24.42%,<break/> 3-channels 15.72%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RLS Adaptive<break/>Filter</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kumaravel<break/>et al. (2021)<break/>[<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye<break/>blinks); Move-<break/>ment (Head,<break/>body, limb)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 6 subjects<break/>3 trials &#215; 25 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSVEP</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 wet (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG baseline</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSVEP analysis<break/>(FTR improvement:<break/> 2&#160;Hz&#8211;Correction<break/> 18.7%,<break/>4&#160;Hz&#8211;Removal 67.5%,<break/> 8&#160;Hz&#8211;Removal 49.5%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR</td></tr><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Shahbakhti<break/>et al. (2021)<break/>[<xref rid="B95-sensors-25-05770" ref-type="bibr">95</xref>]</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 1368 trials<break/>&#215; 4104 s</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.a.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 n.a. (n.a.)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Initial EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification Performance Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TPR (95.77&#160;&#177;&#160;4.14%),<break/>FPR (0.0057&#160;&#177;&#160;0.007)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">VME + DWT</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE (0.135 &#177; 0.031)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (0.955 &#177; 0.024),<break/>PSD difference<break/>(<inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.90</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,<break/>
<inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>6.02</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>2.73</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,<break/><inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.39</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.81</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,<break/>
<inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.98</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.71</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,<break/><inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4.58</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.93</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 32 subjects<break/>3000 trials &#215;<break/>9000 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Motor-imagery;<break/>attention task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 wet<break/>(frontal electrode)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TPR (95.3&#160;&#177;&#160;2.3%),<break/>FPR (0.0074&#160;&#177;&#160;0.0024)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Sha&#8217;bani et al.<break/>(2021) [<xref rid="B96-sensors-25-05770" ref-type="bibr">96</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 36 subjects<break/>n.r. &#215; 1280 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r. (focus on AF3)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE (7.62 &#177; 2.51)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EEMD + OD +<break/>cubic spline<break/>interpolation</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pearson&#8217;s correlation<break/>(0.802 &#177; 0.102),<break/>PDS differences<break/>(<inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula>: 7.11 &#177; 2.90;<break/>
<inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula>: 1.68 &#177; 0.79;<break/><inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>: 1.99 &#177; 1.41;<break/>
<inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>: 10.09 &#177; 13.29;<break/>
<inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula>: 7.80 &#177; 9.77),<break/>SAR (&#8764;12)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Aung et al.<break/>(2021) [<xref rid="B97-sensors-25-05770" ref-type="bibr">97</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Instrumental<break/>(Cable<break/>movements)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 6 subjects<break/>24 trials &#215; 540 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 n.r. (Fpz e Fp1)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>channel</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Accuracy (86.2&#160;&#177;&#160;5.9%),<break/>TPR (84.8&#160;&#177;&#160;6.3%),<break/>FPR (2.0&#160;&#177;&#160;4.5%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M-mDistEn</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Zhang et al.<break/>(2021) [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>blinks); Muscular<break/>(MaMC, TeMC,<break/>PMC, LaMC,<break/>ToMc, NMC);<break/>Movement<break/>(Tremor); EMI<break/>(PLN, S.N.S.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: n.r.<break/>n.r. &#215; 10 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.a.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 n.a. (n.a.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Initial EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE (Non-Blink<break/> zones: 0.59 &#177; 0.07;<break/>Blink zones: 2.81 &#177; 0.38)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">DWT + CCA</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (Non-Blink<break/> zones: 0.947&#160;&#177;&#160;0.003;<break/>Blink zones: 0.167&#160;&#177;&#160;0.027)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 23 subjects<break/>1 trial &#215; n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sleep</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23 n.r. (focus on<break/>C4, P7, FT9, FP1)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (0.923 &#177; 0.048),<break/>MI (1.00 &#177; 0.33),<break/>MSC plot<break/>(numerical values n.r.)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Noorbasha<break/>et al. (2021)<break/>[<xref rid="B98-sensors-25-05770" ref-type="bibr">98</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Instrumental<break/>(Cable<break/>movements) [<xref rid="B79-sensors-25-05770" ref-type="bibr">79</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 6 subjects<break/>4 trials &#215; 540 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">2 n.r.<break/>(P2, P1)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR (<inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> SNR: 1.6 dB<break/> (0.79% overlap)),<break/>RRMSE (improvement<break/>rate: 15.62%<break/>(0.79% overlap))</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">SSA with<break/>modified grouping</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operational speed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency (0.84 s<break/>((0.79% overlap))</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ingolfsson<break/>et al. (2022)<break/>[<xref rid="B99-sensors-25-05770" ref-type="bibr">99</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye movements), Instru-<break/>mental (Electrode<break/>pop, displace-<break/>ment); Muscular<break/>(MaMC, TeMC);<break/>Movement<break/>(Tremor)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>:<break/>213 subjects<break/>n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22 n.r. (focus<break/>on F7, T3, T3, T5,<break/>F8, T4, T4, T6)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifact labels</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Accuracy (87.8 &#177; 1.5%),<break/>F1-score<break/>(0.850 &#177; 0.019)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT + MMC</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Chen&#160;et&#160;al.<break/>(2022) [<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements,<break/>blinks); Musco-<break/>lar (MaMC,<break/>TeMC, CSMC);<break/>Movement<break/>(Head); EMI (PLN)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 32 subjects<break/>6 trials &#215; 720 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Audio and video<break/>stimulation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 wet (F3, F4,<break/>C3 C4, T3, T4,<break/>O1, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSD differences<break/>(numerical values n.r.)</td><td rowspan="3" align="left" valign="middle" colspan="1">MRA + CCA<break/>+ SVM OD</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: 32 subjects<break/>n.r. &#215; 1200 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 wet (F3, F4,<break/>C3 C4, T3, T4,<break/>O1, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NSR-based RRMSE<break/>(NSR = 6 dB,
Ocular: 12.7&#160;&#177;&#160;2.2;<break/>Muscular and Movement:<break/>14.2&#160;&#177;&#160;2.5;<break/>PLN continous 10.3&#160;&#177;&#160;1.6;<break/>PLN intermittent 12.0&#160;&#177;&#160;1.9)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 12 subjects<break/>440 trials &#215; 880 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Video stimulation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 wet (Cz, Pz, Oz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifact-related ICs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ERP peak amplitudes<break/>(numerical values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Occhipinti et al.<break/>(2022) [<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Muscular (MaMC,<break/>TeMC, PMC,<break/>LaMC, ToMC)<break/>Movement (Body)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 12 subjects<break/>1 trial &#215; 120 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state;<break/>cognitive tasks</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 wet<break/>(into the ear canal)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Amplitude and mean<break/>power reduction<break/>rate (numerical (value n.r.))</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA-MEMD</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Paissan et al.<break/>(2022) [<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye<break/>movements,<break/>blinks) [<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>];<break/>Muscular<break/>(CSMC, ZMC,<break/>OOrMC, OOcMC,<break/>MaMC) [<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>:<break/>105 subjects<break/>1 trial &#215; n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state;<break/>motor tasks</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>channel</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR-based<break/>classification<break/>accuracy<break/>(SNR = 3 dB)<break/>(classification<break/>accuracy = 75%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1D-CNN with<break/>HPO</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Peh et al.<break/>(2022) [<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye<break/>movements);<break/>Muscular<break/>(MaMC, TeMC,<break/>S.N.S.); Instrumen-<break/>tal (Electrode pop);<break/>Movement (Tremor)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 310 subjects<break/>1 trial &#215; n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state;<break/>dual-task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19 n.r. (Fp1, F3,<break/>C3, P3, F7, T3,<break/>T5, O1, Fz, Cz,<break/>Pz, Fp2, F4, C4,<break/>P4, F8, T4, T6, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifact labels</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><break/>Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Balanced Accuracy<break/>(muscolar: 0.95);<break/>(intrumental: 0.73);<break/>(ocular: 0.83);<break/>(movement: 0.86),<break/>TPR (49.2&#160;&#177;&#160;10.3%),<break/>FPR (3.0&#160;&#177;&#160;1.6%)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN with<break/>BM loss</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Brophy et al.<break/>(2022) [<xref rid="B105-sensors-25-05770" ref-type="bibr">105</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (S.N.S.);<break/>Muscular<break/>(S.N.S.);<break/>EMI (PLN)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Motor-imagery</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE<break/>(numerical values n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">GAN</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC<break/>(numerical values n.r.),<break/>PSD differences<break/>(numerical values n.r.)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Xiao et al.<break/>(2022) [<xref rid="B106-sensors-25-05770" ref-type="bibr">106</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Cardiac; Ocular<break/>(Eye movements);<break/>Muscular (s.n.s);<break/>Instrumental (EII);<break/>EMI (PLN)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 28 subjects<break/>40 trials &#215; 20&#8211;60 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">22 wet (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectrum differences<break/>numerical values n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Modified ASR<break/>method based on<break/>spectral properties</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hardware<break/>efficiency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Power consumption<break/>(numerical values n.r.)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Arpaia et al.<break/>(2022) [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye mo-<break/>vements, blinks);<break/>Muscular (S.N.S.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 13 subjects<break/>1 trial &#215;<break/>900&#8211;2700 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">27 n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EEG baseline</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE<break/>(numerical values n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">ASR</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SD differences<break/>(numerical values n.r.)</td></tr><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Noorbasha<break/>et al. (2022) [<xref rid="B108-sensors-25-05770" ref-type="bibr">108</xref>]</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Instrumental<break/>(Electrode pop,<break/>EII)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: n.r.<break/>5 trials &#215; 5 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">18 n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MAE (0.0282 &#177; 0.0211)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">SWT + GSTV</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSD differences<break/>(numerical values n.r.)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>:<break/>22 trials &#215; 5 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.a.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 n.a. (n.a.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Initial EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE (SNR&#160;=&#160;6 dB,<break/> 0.45 &#177; 0.05)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (SNR&#160;=&#160;6 dB, 0.86 &#177; 0.03)</td></tr><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Zhang&#160;et&#160;al.<break/>(2022) [<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>]</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Cardiac; Ocular<break/>(Eye movements,<break/>Blinks)<break/>Muscular (S.N.S.)<break/>Instrumental<break/>(Electrode<break/>displacement)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 27 subjects<break/>2 trials &#215; 30 s</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">19 n.r. (Fp1, Fp2,<break/>F3, F4, C3, C4,<break/>P3, P4, 01, 02,<break/>F7, F8, T3, T4,<break/>T5, T6, Fz, Cz, Pz)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE (mixed artifacts: 0.60)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">GRU-MARSC</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (mixed artifacts: 0.81),<break/>PSD differences (numerical<break/> values n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification Performance Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">classification accuracy (98.52%),<break/>PPV (98.22%),<break/>TPR (98.81%)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operational speed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency<break/>(10,250 samples, 11.05 s)</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Jayas&#160;et&#160;al.<break/>(2023) [<xref rid="B110-sensors-25-05770" ref-type="bibr">110</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye movements,<break/>blinks)<break/>Muscular (S.N.S.)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 6 subjects<break/>n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Motor task</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">8 wet<break/>(4 in each ear,<break/>2 in front and<break/>back of the ear,<break/>2 in upper<break/>and bottom)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Co-registered<break/>scalp-EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMS (n.r.), SNR (n.r.),<break/>ZCR (n.r.),
Max Gradient (n.r.)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Classification model<break/>based on<break/>Random Forest</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Skewness (n.r.), Kurtosis (n.r.),<break/>Spectral Entropy (n.r.), ACF (n.r.)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification Performance Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification accuracy<break/>(76.70%)<break/>F1-score (0.85%)</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Narmada&#160;et&#160;al.<break/>(2023) [<xref rid="B111-sensors-25-05770" ref-type="bibr">111</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Cardiac; Ocular<break/>(Eye<break/>movements);<break/>Muscular<break/>(LiMC)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>:<break/>(a) 22 subjects<break/>n.r. &#215; 8 s;<break/>(b) 9 subjects<break/>576 trials &#215; 8 s</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">(a) n.r.;<break/>(b) Motor-imagery</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">(a) n.r.;<break/>(b) 22 n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MAE (Cardiac: 1.13&#160;&#177;&#160;0.64;<break/>Muscular: 1.22&#160;&#177;&#160;1.18;<break/> Ocular: 0.81&#160;&#177;&#160;0.19),<break/>PSNR (Cardiac:<break/> 44.76&#160;&#177;&#160;2.13;<break/> Muscular: 44.74&#160;&#177;&#160;3.88;<break/>Ocular: 46.05&#160;&#177;&#160;0.95),<break/>RMS (Cardiac: 1.17&#160;&#177;&#160;0.64;<break/> Muscular: 1.29&#160;&#177;&#160;1.20;<break/> Ocular: 0.85&#160;&#177;&#160;0.19)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Deep learning +<break/>adaptive wavelet</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (Cardiac: 1.174&#160;&#177;&#160;0.006;<break/> Muscular: 1.137&#160;&#177;&#160;0.036;<break/> Ocular: 1.177&#160;&#177;&#160;0.0003)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Efficiency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CSED (Cardiac:<break/> 1373.9&#160;&#177;&#160;0.65;<break/> Muscular: 1374.8&#160;&#177;&#160;5.16;<break/> Ocular: 1378.8&#160;&#177;&#160;2.68)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Mahmud&#160;et&#160;al.<break/>(2023) [<xref rid="B112-sensors-25-05770" ref-type="bibr">112</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Instrumental<break/>(Cable<break/>Movement) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 6 subjects<break/>4 trials &#215; 540 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">2 n.r. (Fpz, Fp1h)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>channel</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DSNR (26.641 dB),<break/>MAE (0.056&#160;&#177;&#160;0.025),<break/>artifact reduction<break/>rate (90.52%)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Deep learning +<break/>adaptive wavelet</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSD comparison<break/>(numerical values n.r.)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Jiang&#160;et&#160;al.<break/>(2023) [<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular<break/>(Eye blinks)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: 27 subjects<break/>1 trial &#215; n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 dry (Fp1)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TPR (92.86%),<break/>FPM (0.85)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">VME+ MFE<break/>+ GWO</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 9 subjects<break/>n.r. &#215; 480&#8211;900 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 n.r. (Fp1/Fp2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Expert-annotated<break/>blinks</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CCR (97.63%),<break/>TPR (92.64%),<break/>FPM (0.02),<break/>FDR (2.37%)</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Cui&#160;et&#160;al.<break/>(2023) [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>] &#160;&#160;</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Cardiac&#160;[<xref rid="B79-sensors-25-05770" ref-type="bibr">79</xref>];<break/>Ocular (Eye<break/>movements,<break/>blinks) [<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>];<break/>Muscular (CSMC,<break/>ZMC, OOrMC,<break/>OOcMC,<break/>MaMC) [<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>];<break/>Movement<break/>(Body) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]
</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 158 subjects<break/>1 trial &#215; n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 n.a. (n.a.)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE<break/>(Muscular: 0.356;<break/> Ocular: 0.210;<break/> Cardiac: 0.273;<break/> Movement: 0.262),<break/>SNR<break/>(Muscular: 9.463;<break/> Ocular: 14.653;<break/> Cardiac: 10.275;<break/> Movement: 11.951)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">EEGIFNet</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC<break/>(Muscular: 0.926;<break/> Ocular: 0.974;<break/> Cardiac: 0.951;<break/> Movement: 0.945)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Efficiency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CSED (flop of 100.784 M)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Kaongoen<break/>et al. (2023)<break/>[<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements, blinks);<break/>Instrumental<break/>(Cable movements)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: (i) 24 subjects<break/>1 trial &#215; 540 s&#160;[<xref rid="B79-sensors-25-05770" ref-type="bibr">79</xref>]<break/>(ii) 33 subjects n.r.<break/>[<xref rid="B114-sensors-25-05770" ref-type="bibr">114</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 n.a. (n.a.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE (<inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> MSE: 9.39&#160;&#177;&#160;1.45),<break/>SNR (<inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mo>&#916;</mml:mo></mml:mrow></mml:math></inline-formula> SNR: 15.24&#160;&#177;&#160;0.52)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">WT + ASR</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC<break/>(0.210&#160;&#177;&#160;0.095)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Kumaravel et<break/>al. (2023) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R1</bold>: 6 subjects<break/>3 trials &#215; 25 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSVEP</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 dry (n.r)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FTR (2&#160;Hz: 1.3&#160;&#177;&#160;0.3;<break/> 4&#160;Hz: 3.0&#160;&#177;&#160;0.8;<break/> 8&#160;Hz: 5.5&#160;&#177;&#160;1.5)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">IMU-ASR</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R2</bold>: 37 subjects<break/>4 trials &#215; 600 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Motor task</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">120 wet (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brain ICs (No.: 6 &#177; 3),<break/>muscle ICs (No.: 12 &#177; 9)</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Li&#160;et&#160;al.<break/>(2023) [<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements,<break/>blinks); Muscular<break/>(CSMC, ZMC,<break/>OOrMC, OOcMC,<break/>MaMC);<break/>Movement<break/>(S.N.S.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: (i) 27 subjects,<break/>2 trials &#215; n.r.&#160;[<xref rid="B114-sensors-25-05770" ref-type="bibr">114</xref>]<break/>(ii) 105 subjects,<break/>1 trial &#215; n.r.&#160;[<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">(i) Resting-state<break/>(ii) Resting-state,<break/>motor tasks</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">(i) 19 n.r. (FP1, FP2,<break/>F3, F4, C3, C4,
P3,<break/>P4, O1, O2, F7,<break/>F8, T3, T4, T5,<break/>T6, Fz, Cz, Pz);<break/>(ii) n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE<break/>(0.4129&#160;&#177;&#160;0.0979),<break/>SNR<break/>(6.0321&#160;&#177;&#160;1.8962)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">ResUnet1D-RNN</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC<break/>(90.75%&#160;&#177;&#160;4.27)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R&#160;[<xref rid="B117-sensors-25-05770" ref-type="bibr">117</xref>]</bold>: 23 subjects,<break/>23&#8211;26 trials &#215; n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Waveform and PSD<break/>qualitative analysis<break/>(numerical values n.r.)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Yin&#160;et&#160;al.<break/>(2023) [<xref rid="B118-sensors-25-05770" ref-type="bibr">118</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements,<break/>blinks)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 27 subjects<break/>from&#160;[<xref rid="B114-sensors-25-05770" ref-type="bibr">114</xref>]<break/>n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">8 n.r.<break/>(FP1, FP2, F3, F4,<break/>F7, F8, T3, T4)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNR (11.123&#160;&#177;&#160;1.306),<break/>RRMSE (0.340&#160;&#177;&#160;0.044)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">GCTNet Generator<break/>(CNN and<break/>Transformer Blocks)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (0.929&#160;&#177;&#160;0.015)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">O&#8217;Sullivan<break/>et al. (2023)<break/>[<xref rid="B119-sensors-25-05770" ref-type="bibr">119</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Instrumental<break/>(Poor electrode<break/>contact);<break/>Muscular<break/>(S.N.S.):<break/>Movement<break/>(S.N.S.);<break/>Cardiac</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 51<break/>subjects<break/>n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Daily activities</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9 disposable<break/>(F3, F4, C3,<break/>C4, Cz, T3,<break/>T4, O1, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><break/>Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC (0.844),<break/>MCC (0.649),<break/>Classification<break/>Sensitivity (0.794),<break/>Classification<break/>Specificity (0.894)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN deep<break/>learning architecture</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Chen et al.<break/>(2023) [<xref rid="B120-sensors-25-05770" ref-type="bibr">120</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements,<break/>blinks);<break/>Muscular (Head<break/>movement)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 52<break/>subjects<break/>from&#160;[<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>]<break/>n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Motor-imagery</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 n.r.<break/>(n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE (0.444)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Denoiseformer<break/>Tranfomer-based<break/>Encoder and<break/>Self-Attentional<break/>Mechanism</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><break/>Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><break/>CC (0.859)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hermans&#160;et&#160;al.<break/>(2023) [<xref rid="B121-sensors-25-05770" ref-type="bibr">121</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Instrumental<break/>(Device<break/>interference,<break/>electrodes);<break/>Muscular<break/>(S.N.S.):<break/>Movement<break/>(S.N.S.);<break/>Cardiac</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 133<break/>subjects<break/>n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Daily Activities</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 n.r.<break/>(Fp1, Fp2, C3, C4,<break/>T3, T4, O1, O2)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><break/>Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Accuracy (96.6%),<break/>F1-score (86.2),<break/>Miss Rate (11.7)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Semi-supervised<break/>multi-task<break/>CNN (encoder<break/>+ decoder)</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Bahadur et<break/>al. (2024) [<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements,<break/>blinks)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>: 27 subjects<break/>n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">19 n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG<break/></td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE (2.22&#160;&#177;&#160;0.27)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">DWT + LMM</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (0.93&#160;&#177;&#160;0.02)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hardware<break/>efficiency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Silicon area<break/>(area: 5181.73 &#956;m<sup>2</sup>,<break/> power: 446.06 <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>W)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Arpaia&#160;et&#160;al.<break/>(2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye mo-<break/>vements, blinks);<break/>Muscular<break/>(TeMC, MaMC);<break/>Movement (Head)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 2 subjects<break/>50 trials &#215; 40 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 dry<break/>(Fz, C3, Cz, C4,<break/>Pz, PO7, PO8, Oz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG baseline</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSE<break/>(4-channels (k&#160;=&#160;9):<break/> contaminated 5.0&#160;&#177;&#160;0.0,<break/>clean 4.5&#160;&#177;&#160;2.0;<break/>3-channels (k&#160;=&#160;8):<break/>contaminated 6.0&#160;&#177;&#160;4.0,<break/> clean 5.5&#160;&#177;&#160;3.5;<break/>2-channels (k&#160;=&#160;9):<break/>contaminated 3.0&#160;&#177;&#160;0.0,<break/>clean 3.0&#160;&#177;&#160;0.0)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MEMD + ASR</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ingolfsson et<break/>al. (2024) [<xref rid="B124-sensors-25-05770" ref-type="bibr">124</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ocular (Eye mo-<break/>vements); Musco-<break/>lar (MaMC,<break/>TeMC, S.N.S.)<break/>Movement<break/>(Tremor)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 22 subjects<break/>n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting-state</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4 n.r.<break/>(F7-T7, T7-P7,<break/>F8-T8, T8-P8)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifact labels</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><break/>Classification<break/>Performance<break/>Metrics</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification<break/>Accuracy (93.95%),<break/>Sensitivity<break/>(61.27&#160;&#177;&#160;5.66%),<break/>FPR (FP-h) (&lt;0.58)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">XGBoost-based<break/>model</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Saleh&#160;et&#160;al.<break/>(2024) [<xref rid="B125-sensors-25-05770" ref-type="bibr">125</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye mo-<break/>vements) [<xref rid="B114-sensors-25-05770" ref-type="bibr">114</xref>];<break/>Muscular<break/>(CSMC, ZMC,<break/>OOrMC, OOcMC,<break/>MaMC) [<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>];<break/>Instrumental<break/>(Electrode<break/>displacement) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>:<break/>138 subjects<break/>1 trial &#215; n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state;<break/>motor tasks</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">64 n.r.<break/>(n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">No contaminated<break/>EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRMSEt (Ocular:<break/> 0.52&#160;&#177;&#160;0.10; Movement:<break/> 0.70&#160;&#177;&#160;0.10; Muscular:<break/> 0.58&#160;&#177;&#160;0.16; Clean<break/>EEG: 0.30&#160;&#177;&#160;0.07)<break/>RRMSE<sub><italic toggle="yes">f</italic></sub> (Ocular:<break/>0.53&#160;&#177;&#160;0.20; Movement:<break/> 0.72&#160;&#177;&#160;0.17; Muscular:<break/> 0.59&#160;&#177;&#160;0.18; Clean<break/> EEG: 0.42&#160;&#177;&#160;0.11)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Deep<break/>Convolutional<break/>Autoencoder</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC (Ocular:<break/>0.86&#160;&#177;&#160;0.06; Movement:<break/>0.71&#160;&#177;&#160;0.11; Muscular:<break/> 0.80&#160;&#177;&#160;0.12; Clean<break/> EEG: 0.95&#160;&#177;&#160;0.02)</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Nair&#160;et&#160;al.<break/>(2025) [<xref rid="B126-sensors-25-05770" ref-type="bibr">126</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Ocular (Eye<break/>movements);<break/>Muscular<break/>(MeMC, SubMc)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: 197 subjects<break/>2 trials &#215; 72,000 s</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Resting-state</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">2 n.r.<break/>(n.r.)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Raw EEG</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE (3.0&#160;&#177;&#160;0.5 <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>V),<break/>SNR (22.5&#160;&#177;&#160;1.2 dB)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">CNN + LMS<break/>(OBC Radix-4 DA)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Selectivity</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CC<break/>(0.93&#160;&#177;&#160;0.02)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05770-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05770-t002_Table 2</object-id><label>Table 2</label><caption><p>Details of public datasets considered in the included studies. The datasets are defined by data type (R = real, S = simulated, SS = semi-simulated), signal category (physiological or artifact), and experimental protocol, including environmental conditions, subject position, and stimulus/task. For each dataset, the table reports participants&#8217; information (number and type, age (mean &#177; SD), sex (Male (M), Female (F))), hardware setup (device, number and type of electrodes), and signal processing stage. &#8220;Not Applicable&#8221; (n.a.) indicates parameters not relevant in the context (e.g., acquisition setup for simulated data). &#8220;Not Reported&#8221; (n.r.) refers to parameters relevant but unspecified by the authors (e.g., participant sex). &#8220;Not Considered&#8221; (n.c.) refers to parameters relevant but not used in the study (e.g., absence of pre-processing). An explanatory table for other technical acronyms is recommended at the end of the document.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Public Dataset</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data Type</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Signal Category</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Experimental Protocol</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Participants</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Hardware</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Signal Processing</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG datasets for<break/>motor-imagery<break/>brain&#8211;computer<break/>interface&#160;[<xref rid="B133-sensors-25-05770" ref-type="bibr">133</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and Ocular<break/>and EMG Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: laboratory setting;<break/>background noise level was held<break/>between 37 and 39 decibels<break/><bold>Subject Position</bold>: sitting in a chair with<break/>armrests in front of a monitor<break/><bold>Stimulus/Task</bold>: (i) motor-imagery movement<break/>of left and right hands (ii) resting state EO<break/>and (iii) artifact recordings (eye blinking,<break/>eyeball movement up/down and left/right,<break/>head movement, and jaw clenching)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>52 healthy subjects<break/><bold>Age</bold>: 24.8 &#177; 3.9 years<break/><bold>Sex</bold>: 19 F, 33 M</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: Biosemi<break/>ActiveTwo system<break/><bold>Electrodes No.</bold>: 64<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Common average<break/>reference, fourth<break/>order Butterworth<break/>filter [8&#8211;30&#160;Hz]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BCI Competition<break/>2008 Graz<break/>data set B&#160;[<xref rid="B80-sensors-25-05770" ref-type="bibr">80</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and<break/>EOG Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: sitting in an armchair in<break/>front of an LCD computer monitor placed<break/>approximately 1 m in front at eye level<break/><bold>Stimulus/Task</bold>: (i) motor-imagery movement<break/>of the left and right hands, (ii) rest EO<break/>(while looking at a fixation cross on the<break/>screen) (iii), rest EC (iv), eye movements<break/>(eye blinking, rolling, up&#8211;down, left&#8211;right<break/>movements)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>10 healthy subjects<break/><bold>Age</bold>: 24.7 &#177; 3.3 years<break/><bold>Sex</bold>: 6 M and 4 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: Easycap<break/><bold>Electrodes No.</bold>: 3<break/>bipolar recordings<break/>extracted from the 22<break/>total electrodes. EOG<break/>was recorded with 3<break/>monopolar electrodes.<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BPF<break/>[0.5&#8211;100&#160;Hz]<break/>and notch filter<break/>at 50&#160;Hz</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BCI Competition<break/>2008 Graz<break/>data set A&#160;[<xref rid="B81-sensors-25-05770" ref-type="bibr">81</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and<break/>EOG Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: sitting in a comfortable<break/>armchair in front of a computer screen<break/><bold>Stimulus/Task</bold>: (i) four motor-imagery<break/>tasks (left and right hand, both feet, and&#160;<break/>tongue) (ii) EO (looking at a fixation cross<break/>on the screen), (iii) EC, (iv) eye movements</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>9 healthy subjects<break/><bold>Age</bold>: n.r.<break/><bold>Sex</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>: n.r.<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ear-EEG Recording for<break/>Brain Computer<break/>Interface of<break/>Motor Task&#160;[<xref rid="B134-sensors-25-05770" ref-type="bibr">134</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: sitting in front of a<break/>computer monitor<break/><bold>Stimulus/Task</bold>: subjects were asked to<break/>imagine and grasp the left or right hand<break/>according to an arrow direction present<break/>on the computer monitor</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>6 healthy subjects<break/><bold>Age</bold>: 22&#8211;28 years<break/><bold>Sex</bold>: 2 M, 4 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: Neuroscan<break/>Quick Cap<break/>(Model C190)<break/><bold>Electrodes No.</bold>: n.r.<break/>for the cap-EEG.<break/>Ear-EEG were recorded<break/>with 8 ear electrodes<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BPF<break/>[0.5&#8211;100&#160;Hz]<break/>together with<break/>a notch filter</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A Methodology<break/>for Validating<break/>Artifact Removal<break/>Techniques for<break/>Physiological<break/>Signals&#160;[<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and<break/>Cable Motion<break/>Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: subjects were asked to keep<break/>their eyes closed and maintain a<break/>stationary head position throughout the<break/>experiment. An&#160;artifact motion was then<break/>induced to one of the electrodes by<break/>pulling on the connecting lead.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>6 healthy subjects<break/><bold>Age</bold>: 27.0 &#177; 4.3 years<break/><bold>Sex</bold>: 3 M, 3 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: Electro-cap<break/>International<break/><bold>Electrodes No.</bold>: 2<break/>electrodes were<break/>considered based<break/>on the 256 electrodes<break/>composing the EEG device<break/><bold>Electrodes Type</bold>: wet</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The impact of<break/>the MIT-BIH<break/>Arrhythmia<break/>Database&#160;[<xref rid="B79-sensors-25-05770" ref-type="bibr">79</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>ECG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: clinical<break/>setting inside the Boston&#8217;s Beth Israel<break/>Hospital (BIH; now the Beth Israel Deaconess<break/>Medical Center)<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>23 healthy subjects<break/>and 24 participants with<break/>uncommon but clinically<break/>important arrhythmia<break/>(47 subjects in total)<break/><bold>Age</bold>: 23 to 89 years<break/><bold>Sex</bold>: 25 M, 22 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>: n.r.<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TUH EEG<break/>Artifact<break/>Corpus dataset<break/>[<xref rid="B135-sensors-25-05770" ref-type="bibr">135</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: data are composed of<break/>archival records acquired in clinical settings<break/>in the Temple University Hospital (TUH)<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>archival recordings of<break/>10.874 healthy and<break/>clinical subjects<break/><bold>Age</bold>: 1 to 90 years<break/><bold>Sex</bold>: 51% F, 49% M</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>: n.r.<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SEED<break/>database&#160;[<xref rid="B136-sensors-25-05770" ref-type="bibr">136</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and EOG<break/>Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: subjects were asked to watch<break/>15 film clips designed to elicit positive,<break/>neutral, and negative emotions</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>15 healthy subjects<break/><bold>Age</bold>: 23.3 &#177; 2.4 years<break/><bold>Sex</bold>: 7 M, 8 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: ESI<break/>NeuroScan System<break/>for EEG signals<break/>acquisition and SMI<break/>eye-tracking glasses<break/>for eye movements<break/><bold>Electrodes No.</bold>: n.r.<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BPF<break/>[0&#8211;75&#160;Hz]<break/>was applied</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CHB-MIT<break/>Scalp EEG<break/>Database&#160;[<xref rid="B117-sensors-25-05770" ref-type="bibr">117</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: EEG recordings were<break/>acquired during and after seizures attacks</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>22 pediatric subjects<break/>with intractable seizures<break/><bold>Age</bold>: 1.5 to 22 years<break/><bold>Sex</bold>: 5 M, 17 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>: n.r.<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Freiburg EEG<break/>dataset&#160;[<xref rid="B137-sensors-25-05770" ref-type="bibr">137</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: clinical settings<break/>in the Epilepsy Center of the University<break/>Hospital of Freiburg, Germany<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: EEG recordings were made<break/>during an invasive pre-surgical<break/>epilepsy monitoring</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>21 patients suffering<break/>from medically<break/>intractable<break/>focal epilepsy<break/><bold>Age</bold>: n.r.<break/><bold>Sex</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: Neurofile NT<break/>digital video<break/>EEG system<break/><bold>Electrodes No.</bold>: 128<break/>depth-electrodes<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No notch or BPF<break/>have been applied</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A large EEG<break/>motor-imagery<break/>dataset for EEG<break/>brain computer<break/>interfaces&#160;[<xref rid="B138-sensors-25-05770" ref-type="bibr">138</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: comfortably sitting in a<break/>recliner chair in front of a computer<break/>screen positioned approximately 200 cm<break/>in front at slightly above the eye level<break/><bold>Stimulus/Task</bold>: (i) three motor imageries of<break/>left and right-hand movements and one<break/>passive mental imagery in which participants<break/>remained neutral and engaged in no motor<break/>imagery, (ii) imagery of left and right<break/>leg movement and tongue movements,<break/>(iii) imaginary movements of the<break/>fingers on one hand, (iv) to remain passive<break/>throughout the experiment.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>13 healthy subjects<break/><bold>Age</bold>: 20 to 35 years<break/><bold>Sex</bold>: 8 M, 5 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: EEG-1200<break/>JE-921A EEG system<break/>(NihonKohden, Japan)<break/>with the help of<break/>an Electro-Cap<break/>International<break/><bold>No. of Electrodes</bold>: 19<break/><bold>Electrodes Type</bold>: wet</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BPF<break/>[0.53&#8211;70&#160;Hz] and a<break/>50&#160;Hz notch filter</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prediction of<break/>Reaction Time<break/>and Vigilance<break/>Variability From<break/>Spatio-Spectral<break/>Features of<break/>Resting-State<break/>EEG in a Long<break/>Sustained Attention<break/>Task&#160;[<xref rid="B139-sensors-25-05770" ref-type="bibr">139</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: in a dimly lit EEG<break/>room within a Faraday cage, in&#160;<break/>the early afternoon<break/><bold>Subject Position</bold>: comfortably sitting in a<break/>chair 20 cm away from a 17-inch LCD monitor<break/><bold>Stimulus/Task</bold>: (i) resting session with EO,<break/>(ii) resting-state with EC, (iii) SART sessions:<break/>pressing the left mouse button when any digit<break/>appeared on the screen except for the digit 3,<break/>in which case responses should be withheld</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>10 healthy subjects<break/><bold>Age</bold>: 30.3 &#177; 6.9 years<break/><bold>Sex</bold>: 6 F, 4 M</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>: 64<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Impact of Stimulus<break/>Features on the<break/>Performance of<break/>a Gaze-Independent<break/>Brain-Computer<break/>Interface Based on<break/>Covert Spatial<break/>Attention<break/>Shifts&#160;[<xref rid="B140-sensors-25-05770" ref-type="bibr">140</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and<break/>EOG Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: in an acoustically<break/>shielded and dimly lit cabin<break/><bold>Subject Position</bold>: sitting in front of a 24&#8221;<break/>display from a distance of 70 cm<break/><bold>Stimulus/Task</bold>: (i) horizontally and vertically<break/>track with their gaze a cross presented on the<break/>display or blink when the cross was<break/>replaced by a circle, (ii) respond to yes/no<break/>questions or statements by shifting their<break/>attention to a green-cross to respond with<break/>&#8220;yes&#8221; or to a red-cross to respond with &#8220;no&#8221;.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>18 healthy participants<break/><bold>Age</bold>: 19 to 38 years<break/><bold>Sex</bold>: 10 F, 8 M</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>: 29.<break/>Furthermore,<break/>vertical and<break/>horizontal EOG were<break/>simultaneously<break/>recorded.<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A semi-simulated<break/>EEG/EOG<break/>dataset for the<break/>comparison of<break/>EOG artifact<break/>rejection<break/>techniques&#160;[<xref rid="B114-sensors-25-05770" ref-type="bibr">114</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: (i) EC condition,<break/>(ii) EO condition</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>27 healthy subjects<break/><bold>Age</bold>: 28.2 &#177; 7.5 years<break/>for M participants,<break/>27.1 &#177; 5.2 years<break/>for F participants<break/><bold>Sex</bold>: 14 M, 13 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>:<break/>19 for cap EEG;<break/>4 for EOG<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BPF [0.5&#8211;40&#160;Hz]<break/>for cap EEG and<break/>[0.5&#8211;5&#160;Hz] for EOG<break/>with a notch filter<break/>at 50&#160;Hz.<break/>Obtained data were then<break/>inspected to ensure no<break/>significant contamination<break/>by external artifacts.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A Survey on<break/>the Feasibility<break/>of Surface<break/>EMG in Facial<break/>Pacing&#160;[<xref rid="B141-sensors-25-05770" ref-type="bibr">141</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(EMG<break/>Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: n.r.<break/><bold>Subject Position</bold>: n.r.<break/><bold>Stimulus/Task</bold>: (i) voluntary smile, lip<break/>pucker, and&#160;frown movement tasks,<break/>(ii) smile and pucker movement while<break/>chewing gum</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>12 healthy subjects<break/><bold>Age</bold>: 31 to 55 years<break/><bold>Sex</bold>: 6 F, 6 M</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: NeXus-10<break/>physiological<break/>monitoring device<break/>(Mind Media BV)<break/><bold>Electrodes No.</bold>: n.r.<break/><bold>Electrodes Type</bold>:<break/>pre-gelled</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8th order<break/>Butterworth filters<break/>implemented as<break/>zero-phase forward<break/>and reverse ones<break/>used to remove 50 Hz<break/>power line noise<break/>and to limit the signal<break/>frequencies to the range<break/>of 20&#8211;500&#160;Hz.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Assessing the<break/>effects of voluntary<break/>and involuntary<break/>eyeblinks in<break/>independent<break/>components of<break/>EEG&#160;[<xref rid="B142-sensors-25-05770" ref-type="bibr">142</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and<break/>EOG Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: in a dim<break/>room (mean illuminance:<break/>188.95 &#177; 24.50 lx)<break/><bold>Subject Position</bold>: the distance between<break/>subject and display was roughly 60 cm<break/><bold>Stimulus/Task</bold>: (i) focus on a black<break/>cross-fixation in the center of the display<break/>and to blink with both eyes after a<break/>sound stimulus, (ii) press the key<break/>corresponding to the associated sound<break/>after a sound stimulus</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>20 healthy subjects<break/><bold>Age</bold>: 22.8 &#177; 1.5 years<break/><bold>Sex</bold>: 14 M, 6 F</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: n.r.<break/><bold>Electrodes No.</bold>:<break/>14 for cap EEG<break/>and 2 for EOG<break/><bold>Electrodes Type</bold>: wet</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Butterworth<break/>BPF [0.5&#8211;60&#160;Hz]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEGdenoiseNet<break/>[<xref rid="B103-sensors-25-05770" ref-type="bibr">103</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference<break/>(Physiological<break/>EEG and<break/>Ocular and<break/>EMG Artifact)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Ambiental Conditions</bold>: laboratory setting;<break/>background noise level was held<break/>between 37 and 39 decibels<break/><bold>Subject Position</bold>: sitting in a chair with<break/>armrests in front of a monitor<break/><bold>Stimulus/Task</bold>: (i) motor-imagery movement<break/>of left and right hands, (ii) resting state EO,<break/>and (iii) artifact recordings (eye blinking,<break/>eyeball movement up/down and left/right,<break/>head movement, and jaw clenching)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>No. and Type</bold>:<break/>52 healthy subjects<break/><bold>Age</bold>: 24.8 &#177; 3.9 years<break/><bold>Sex</bold>: 19 F, 33 M</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Device</bold>: Biosemi<break/>ActiveTwo system<break/><bold>Electrodes No.</bold>: 64<break/><bold>Electrodes Type</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Physiological data<break/>were preprocessed<break/>with BPF [1&#8211;80&#160;Hz]<break/>and a 50&#160;Hz notch<break/>filter. Then, the&#160;<break/>not-brain components<break/>were removed after ICA.<break/>Finally, a&#160;visual<break/>inspection by an<break/>expert validated the<break/>clean procedure.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05770-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05770-t003_Table 3</object-id><label>Table 3</label><caption><p>Parameters of artifact detection pipelines proposed in the included studies. R = real recordings; S = simulated signals; SS = semi-simulated signals. &#8220;Not Applicable&#8221; (n.a.) indicates parameters not relevant in the context (e.g., acquisition setup for simulated data). &#8220;Not Reported&#8221; (n.r.) refers to parameters relevant but unspecified by the authors (e.g., participant sex). &#8220;Not Considered&#8221; (n.c.) refers to parameters relevant but not used in the study (e.g., absence of pre-processing). An explanatory table for other technical acronyms is recommended at the end of the document.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Article</th><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Pre-Processing</th><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Epoching</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Feature Extraction</th><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Feature<break/>Selection</th><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Classification/<break/>Decision Rule</th><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Channel<break/>Specificity</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Domain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameters</th></tr></thead><tbody><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Sweeney&#160;et&#160;al.<break/>(2012) [<xref rid="B64-sensors-25-05770" ref-type="bibr">64</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adaptive Filter</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artifact estimate<break/>time series</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">optimization method = NLMS, reference signal = triaxial accelerometer, length = n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Manual detection<break/>based on<break/>shape, frequency,<break/>and amplitude</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kalman Filter</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Predictor estimate<break/>time series</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEMD-ICA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ICs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEMD: ensemble = 100,<break/>noise = n.r., contrast<break/>function = n.r.;<break/>ICA: n.r.;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Matiko&#160;et&#160;al.<break/>(2013) [<xref rid="B65-sensors-25-05770" ref-type="bibr">65</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 s; overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MCA (using STFT)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Basis coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MCA: number of components = 2; STFT: window length = 500&#160;ms<break/>(for eye blinks), 2 s (for EEG);</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c. (clean signal<break/>reconstruction)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Peng&#160;et&#160;al.<break/>(2013) [<xref rid="B66-sensors-25-05770" ref-type="bibr">66</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>S/R</bold>: BPF (0.5&#8211;40&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mother wavelet = Daubechies 4, decomposition layers = 7</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mihajlovic<break/>et al. (2014) [<xref rid="B67-sensors-25-05770" ref-type="bibr">67</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 3rd order<break/>Butterworth SBF<break/>(49&#8211;51&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 s; 75% overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Welch method</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSD and coherence</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">optimization method = LLMS, forgetting factor = 0.1, step size = 0.5, reference signal = ETI</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zhao&#160;et&#160;al.<break/>(2014) [<xref rid="B68-sensors-25-05770" ref-type="bibr">68</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: BPF<break/>(0.5&#8211;45&#160;Hz)<break/><bold>S</bold>: n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet coefficients<break/>(edge)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mother wavelet = Daubechies 7, decomposition layers = 5;
mother wavelet = Haar, decompo- sition layers = 5</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Majmudar<break/>et al. (2015) [<xref rid="B69-sensors-25-05770" ref-type="bibr">69</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5 s; &#8764;31%<break/>overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Algebraic approach<break/>FIR-based</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D&#8217;Rozario et al.<break/>(2015) [<xref rid="B72-sensors-25-05770" ref-type="bibr">72</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: HPF (0.3&#160;Hz),<break/>LPF (35&#160;Hz),<break/>Low BPF (100&#160;Hz),<break/>Notch (50&#160;Hz);<break/><bold>S</bold>: n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 s; no<break/>overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SD-based<break/>automatic<break/>artifact detection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kim&#160;et&#160;al.<break/>(2015) [<xref rid="B70-sensors-25-05770" ref-type="bibr">70</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FastICA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ICs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No. of ICs = 10</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rahman&#160;et&#160;al.<break/>(2016) [<xref rid="B71-sensors-25-05770" ref-type="bibr">71</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SG filter</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chang&#160;et&#160;al.<break/>(2016) [<xref rid="B73-sensors-25-05770" ref-type="bibr">73</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 1st order Butterworth HPF (0.1&#160;Hz), Downsampling (64&#160;Hz), Median filter</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SDW, MSDW</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LMM</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">window size = same as the
half-width of the artifact, determined by an empirical procedure</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Zhao&#160;et&#160;al.<break/>(2017) [<xref rid="B74-sensors-25-05770" ref-type="bibr">74</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: Mid-filter, FIR filter<break/>(1&#8211;40&#160;Hz)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">2 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Welch method</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Thresholding</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Thammasan<break/>et al. (2017)<break/>[<xref rid="B75-sensors-25-05770" ref-type="bibr">75</xref>]</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: HPF (1&#160;Hz),<break/>Notch (60&#160;Hz &#177; 1&#160;Hz<break/>bandwidth)</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">4 s, 3.5 s;<break/>overlap n.r.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Regression</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trend Slope</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">threshold cutoff = 0.3</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Thresholding</td><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Joint Probability</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">threshold cutoff = 3*SD</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kurtosis</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FFT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">multi-taper PSD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">taper bandwidth = 5&#160;Hz,
sliding window = 1 s, number of tapers = 9</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hu&#160;et&#160;al. (2017) [<xref rid="B76-sensors-25-05770" ref-type="bibr">76</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adaptive SS</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RCs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">window length = 40</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dehzangi et al.<break/>(2018) [<xref rid="B77-sensors-25-05770" ref-type="bibr">77</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5 s;<break/>90% overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-means clustering</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DTW multi-score<break/>space positioning</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K clusters = 2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-means clustering<break/>+ SVM</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cheng&#160;et&#160;al.<break/>(2019) [<xref rid="B78-sensors-25-05770" ref-type="bibr">78</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: BPF (1&#8211;70&#160;Hz) (for<break/>EEG); BPF (0.5&#8211;10&#160;Hz)<break/>(for EOG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSA-ICA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Autocorrelation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSA: window<break/>lenght = 130;<break/>ICA: No. of ICs = 13;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Val-Calvo&#160;et&#160;al.<break/>(2019) [<xref rid="B82-sensors-25-05770" ref-type="bibr">82</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: IIR Notch (50&#160;Hz),<break/>BPF (1&#8211;50&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time +<break/>Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EAWICA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WICs + Kurtosis and Renyi entropy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WAVELET: n.r.;<break/>ICA: No. of ICs = 8;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chi-squared<break/>statistic-based<break/>feature selection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Grosselin et al.<break/>(2019) [<xref rid="B83-sensors-25-05770" ref-type="bibr">83</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R1</bold>: DC offset<break/>removal,<break/>Notch (50&#160;Hz)<break/><bold>R2/R3</bold>: n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R1</bold>: 1 s;<break/>overlap n.c.<break/><bold>R2/R3</bold>: n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.a.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Statistical features, EEG bands max value, SD, kurtosis, skewness</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">FCBF</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Weighted k-NN</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FFT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SEF (80%, 90%,95%),<break/>Spectral Moments<break/>(0, 1st, 2nd), Power<break/>Spectrum Centre<break/>Frequency, Spectral<break/>RMS, ISD, SNR,<break/>Modified Median/<break/>Mean Freq., Ratio<break/>Spectrum Area,<break/>Non-normalized<break/>Power, LogP, RP,<break/>Wavelet energy;<break/>Cospectral Coefficients,<break/>Frequency-<break/>filtered energies,<break/>RSD; Shannon entropy,<break/>spectral entropy,<break/>SVD entropy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Rosanne&#160;et&#160;al.<break/>(2019) [<xref rid="B84-sensors-25-05770" ref-type="bibr">84</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: FIR BPF<break/>(1&#8211;45&#160;Hz)</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">8 s, 7 s;<break/>overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR (PCA)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short-window variance</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">cutoff k <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, window <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> s</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">One-way<break/>ANOVA</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Thresholding</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">wICA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time/Space</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ICA with infomax (ADJUST)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kurtosis, Spatial<break/>Average/Variance<break/>Difference,<break/>max Epoch Variance,<break/>Spatial Eye<break/>Difference,<break/>Generic Discontinuities<break/>Spatial Feature</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inoue&#160;et&#160;al.<break/>(2019) [<xref rid="B59-sensors-25-05770" ref-type="bibr">59</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: BPF (0.5&#8211;60&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.12 s, 2.56 s;<break/>overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FFT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean power spectrum</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FFT length = 1024 points, window function = Hanning, frequency resolution = 0.2&#160;Hz</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Blum&#160;et&#160;al.<break/>(2019) [<xref rid="B85-sensors-25-05770" ref-type="bibr">85</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: FIR LPF (40&#160;Hz),<break/>FIR HPF (0.25&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">rASR (PGA)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean and standard<break/>deviation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">flatline&#160;=&#160;1, hp&#160;=&#160;(0.25, 0.95)&#160;Hz, channel&#160;=&#160;0.9, noisy&#160;=&#160;3, burst&#160;=&#160;2, window&#160;=&#160;0.3 s, cutoff k&#160;=&#160;1, stepsize&#160;=&#160;16, maxdims&#160;=&#160;1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Butkevi&#269;i&#363;t&#279;<break/>et al. (2019) [<xref rid="B86-sensors-25-05770" ref-type="bibr">86</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EMD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMFs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Albuquerque<break/>et&#160;al. (2019) [<xref rid="B87-sensors-25-05770" ref-type="bibr">87</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: Downsampling (250&#160;Hz), BPF (0.5&#8211;45&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4 s; overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">wICA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">One-way ANOVA; mRMR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Liu&#160;et&#160;al.<break/>(2019) [<xref rid="B88-sensors-25-05770" ref-type="bibr">88</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: BPF (20&#8211;100&#160;Hz)<break/>(EMG); n.r. (EEG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FMEMD and CCA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Autocorrelation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N-channel (N&#160;=&#160;3&#8211;8) random selection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Casadei&#160;et&#160;al. (2020) [<xref rid="B89-sensors-25-05770" ref-type="bibr">89</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: digital BPF (8&#8211;14&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.09 s, 0.1 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Band-pass filter</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Amplitude and phase</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">filter order = 150, time window = 0.3 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Islam&#160;et&#160;al.<break/>(2020) [<xref rid="B90-sensors-25-05770" ref-type="bibr">90</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R/SS</bold>: BPF (0.5&#8211;64&#160;Hz),<break/>Notch (50&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infomax ICA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ICs (Max MI)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No. of ICs = No. of channels</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Topographic maps, spectrum, autocorrelation analysis</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Noorbasha et al.<break/>(2020) [<xref rid="B98-sensors-25-05770" ref-type="bibr">98</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 s; overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Covariance matrix eigenvalues</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMDL</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dey&#160;et&#160;al.<break/>(2020) [<xref rid="B92-sensors-25-05770" ref-type="bibr">92</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: Notch (60&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128 s; 80% overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windowing</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time series</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128 s, 80% overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Correlation; <italic toggle="yes">t</italic>-test</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Liu&#160;et&#160;al.<break/>(2021) [<xref rid="B93-sensors-25-05770" ref-type="bibr">93</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: BPF (5-65&#160;Hz),<break/>Notch (50&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RLS Adaptive<break/>Filtering</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EMG-related coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">optimization method = RLS, reference signal = EMG, length = n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weighted least squares cost function minimization</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel +<break/>EMG reference</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kumaravel&#160;et&#160;al. (2021) [<xref rid="B94-sensors-25-05770" ref-type="bibr">94</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: LPF (40&#160;Hz), HPF (0.15&#8211;0.3&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10 s; overlap 50&#8211;75%</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR (PCA)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Energy of components</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">cutoff k &gt; 8,
(other parameters n.r.)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shahbakhti et al.<break/>(2021) [<xref rid="B95-sensors-25-05770" ref-type="bibr">95</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VME-DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT skewness</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">modes&#160;=&#160;12, compactness coefficient&#160;=&#160;3000, threshold value = 0.1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sha&#8217;abani&#160;et&#160;al.<break/>(2021) [<xref rid="B96-sensors-25-05770" ref-type="bibr">96</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: HPF (0.5&#160;Hz),<break/>Normalization</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEMD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMFs<break/>correlation/amplitude</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ensembles&#160;=&#160;200,<break/>noise amplitude&#160;=&#160;0.4, decomposition level&#160;=&#160;3</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Aung&#160;et&#160;al.<break/>(2021) [<xref rid="B97-sensors-25-05770" ref-type="bibr">97</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M-mDistEn</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Entropy measures</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">time delay = 1, dimension for the reconstruction of the phase space = 3, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> SD, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, bins&#160;=&#160;64, fuzzy width&#160;=&#160;0.3, fuzzy step&#160;=&#160;2, scales&#160;=&#160;1&#8211;15</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">QDA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Zhang&#160;et&#160;al.<break/>(2021) [<xref rid="B58-sensors-25-05770" ref-type="bibr">58</xref>]</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.a.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet coefficients (edge)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mother wavelet = Haar, decomposition levels = 6</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Thresholding</td><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Single-channel</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Frequency</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">FFT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kurtosis</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">step size = 1</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MAD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Noorbasha et al. (2021) [<xref rid="B98-sensors-25-05770" ref-type="bibr">98</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: BPF (0.5&#8211;30&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Eigenvectors local mobility</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"> Ingolfsson et<break/>al. (2022) [<xref rid="B99-sensors-25-05770" ref-type="bibr">99</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 s;<break/>overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FFT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Energy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">TPOT</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">MMC</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Energy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mother wavelet = Haar, decomposition levels = n.r.</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Chen&#160;et&#160;al.<break/>(2022) [<xref rid="B100-sensors-25-05770" ref-type="bibr">100</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R1/SS</bold>: HPF (0.5&#160;Hz);<break/><bold>R2</bold>: HPF (0.5&#160;Hz),<break/>Re-sampling (250&#160;Hz),<break/>Re-ref., z-score</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">2 s;<break/>overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">MEMD +<break/>CCA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Autocorrelation, FD,<break/>skewness, kurtosis<break/></td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">MEMD: directions<break/>number = 16,<break/>noise channels 3;<break/>CCA: n.r.;</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">OD<break/>based on<break/>one-class SVM</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R1/SS, R2</bold> (ii):<break/>All channels;<break/><bold>R2</bold> (i): multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total power,<break/>peak frequency<break/></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Occhipinti&#160;et&#160;al.<break/>(2022) [<xref rid="B101-sensors-25-05770" ref-type="bibr">101</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: BPF (1&#8211;40&#160;Hz),<break/>Notch (50&#160;Hz),<break/>detrending,<break/>normalization</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA-MEMD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMFs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No. of IMFs = n.r., noise channels = 6, reference signal = 2 microphones and 1 accelerometer</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Paissan&#160;et&#160;al.<break/>(2022) [<xref rid="B102-sensors-25-05770" ref-type="bibr">102</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: BPF (1&#8211;80&#160;Hz), Notch (50&#160;Hz) (EEG); BPF (0.3&#8211;10&#160;Hz) (EOG); BPF (1&#8211;120&#160;Hz), Notch (50&#160;Hz) (EMG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1-D Convolutional<break/>layers</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>feature maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">epochs = 100,<break/>optimizer = Adam,<break/>kernel length = 3</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Linear layers +<break/>softmax function<break/>(argmax)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Peh&#160;et&#160;al.<break/>(2022) [<xref rid="B104-sensors-25-05770" ref-type="bibr">104</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: 4th order Butterworth,<break/>Notch (60&#160;Hz),<break/>4th order HPF (1 Hz),<break/>Downsampling (128 Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5 s; 25% overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Transformer-<break/>enhanced CNN with<break/>BM loss</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Statistical + correlation<break/>features</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">input length = 0.5 s,<break/>overlap = 25%, kernel length&#160;=&#160;3,
optimizer: Adam,<break/>learning rate&#160;=&#160;<inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>,<break/>batch size&#160;=&#160;1000</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CatBoost classifier</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-/<break/>multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brophy&#160;et&#160;al.<break/>(2022) [<xref rid="B105-sensors-25-05770" ref-type="bibr">105</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4 s, 2 s;<break/>overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>feature maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">input size = 640 (PLN), 512&#160;(Ocular), 1024&#160;(Muscular), other parameters = n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully connected layer<break/>+ sigmoid activation<break/>(thresholding)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Xiao&#160;et&#160;al. (2022) [<xref rid="B106-sensors-25-05770" ref-type="bibr">106</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: BPF (1&#8211;40&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR (PCA)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Energy of frequency components</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Arpaia&#160;et&#160;al.<break/>(2022) [<xref rid="B107-sensors-25-05770" ref-type="bibr">107</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: Baseline-based<break/>normalization, BPF<break/>(1&#8211;40&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR (PCA)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time series component</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">cutoff k = 25<break/>(non-aggressive)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10 times channels random selection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Noorbasha&#160;et&#160;al. (2022) [<xref rid="B108-sensors-25-05770" ref-type="bibr">108</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 s; overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multiple frequency bands coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">decomposition levels = 6</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GSTV filter</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zhang&#160;et&#160;al.<break/>(2022) [<xref rid="B109-sensors-25-05770" ref-type="bibr">109</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: Resampling<break/>(250&#160;Hz), Notch,<break/>Standardization</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.1 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Encoded pattern<break/>feature domain</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MARSC</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Encoded feature map</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">input size = 1025, signal slice length = 20, overlapped samples = 5, signal segments = 5, batch size = 5, optimizer = Adam, learning rate = 0.001</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GRU (discriminator<break/>and encoder)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Jayas&#160;et&#160;al.<break/>(2023) [<xref rid="B110-sensors-25-05770" ref-type="bibr">110</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>R</bold>: HPF (0.5&#160;Hz),<break/>Notch (50/100&#160;Hz),<break/>FIR BPF bands,<break/>z-score normalization</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">1 s, 2 s, 5 s;<break/>overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Amplitude RMS; amplitude Kurtosis; amplitude skewness; SNR; ACF; ZCR; Maximum Gradient</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Random<break/>Forest</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Power mean; Spectral entropy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Narmada&#160;et&#160;al. (2023) [<xref rid="B111-sensors-25-05770" ref-type="bibr">111</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EMD + DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I-CycleGAN + OS-EHO thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mahmud&#160;et&#160;al.<break/>(2023) [<xref rid="B112-sensors-25-05770" ref-type="bibr">112</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: Resampling, Baseline<break/>drift correction, z-score<break/>and range<break/>normalization,<break/>Notch (50&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4 s, 2 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-resolution<break/>spatial pooling</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>feature maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">weight regulator<break/>value = 0.5,<break/>kernel size = 3,<break/>other parameters n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UNet-style<break/>encoder-decoder<break/>structure + deep<break/>supervision</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Jiang&#160;et&#160;al.<break/>(2023) [<xref rid="B113-sensors-25-05770" ref-type="bibr">113</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R/SS</bold>: Real time BPF (3&#8211;100&#160;Hz), SSA LPF on&#160;eyeblink</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VME</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mode function</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cui&#160;et&#160;al.<break/>(2023) [<xref rid="B60-sensors-25-05770" ref-type="bibr">60</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: BPF (1&#8211;80&#160;Hz), Notch (powerline), Resampling (256&#160;Hz) (EEG); BPF (0.3&#8211;10&#160;Hz) (EOG); BPF 1&#8211;120&#160;Hz (EMG), etc.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>layers + BiGRU</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>feature maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">kernel size = 9,<break/>convolutional layers<break/>channels = 32,<break/>optimizer = Adam,<break/>learning rate = 5 &#215; <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FC layer + sigmoid<break/>activation<break/>(thresholding)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Kaongoen<break/>&#160;et&#160;al. (2023)<break/>[<xref rid="B61-sensors-25-05770" ref-type="bibr">61</xref>]</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><bold>SS</bold>:<break/>Downsampling<break/>(256&#160;Hz),<break/>Notch (50&#160;Hz),<break/>BPF (1&#8211;50&#160;Hz)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">3 s; no overlap</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean, SD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR: cutoff k = 4 and 5,<break/>window length = 500 ms</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Thresholding</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">decomposition<break/>levels = 4</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Kumaravel et<break/>al. (2023) [<xref rid="B115-sensors-25-05770" ref-type="bibr">115</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R1</bold>: LPF (40&#160;Hz), HPF (0.15&#8211;0.3&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 s, 10 s;<break/>overlap n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Time</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">ASR (PCA)</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Mean, SD</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">IMU-based calibration,<break/>other parameters n.r.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">n.c.</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Thresholding</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R2</bold>: HPF (1&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 s; overlap n.c.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Li&#160;et&#160;al.<break/>(2023) [<xref rid="B116-sensors-25-05770" ref-type="bibr">116</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: BPF (0.5&#8211;40&#160;Hz), Notch (50&#160;Hz) (EEG/EOG); <bold>R</bold>: BPF (1&#8211;80&#160;Hz) + Notch (50&#160;Hz) (EEG/EMG)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResUnet1D</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional feature<break/>maps (1D-CNN)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">kernel size = 1&#160;&#215;&#160;3, 1&#160;&#215;&#160;5,<break/>1&#160;&#215;&#160;7, optimizer = Adam,<break/>learning rate = 5 &#215; 10<sup>&#8722;5</sup>,<break/>batch size = 256</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RNN</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yin&#160;et&#160;al.<break/>(2023) [<xref rid="B118-sensors-25-05770" ref-type="bibr">118</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time/Space</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>feature maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning rate = 1 &#215; 10<sup>&#8722;4</sup>, batch size = 128, optimizer = Adam, kernel size = 3, activation function = Softmax</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GCTNet<break/>discriminator</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chen&#160;et&#160;al.<break/>(2023) [<xref rid="B120-sensors-25-05770" ref-type="bibr">120</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 s.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tranformer</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Attentional maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning rate = 3 &#215; 10<sup>&#8722;4</sup>,<break/>batch-size = 50,<break/>optimizer = AdamW,<break/>kernel size = 1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Denoiseformer<break/>Transformer-<break/>based decoder</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hermans&#160;et&#160;al.<break/>(2023) [<xref rid="B121-sensors-25-05770" ref-type="bibr">121</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Band-pass (0.27&#8211;30&#160;Hz),<break/>Notch (50&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 s.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN autoencoder</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning rate = 1 &#215; 10<sup>&#8722;3</sup>, batch size = 87, optimizer = Adam, kernel size = 1, activation function = ReLU</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN classifier</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">O&#8217;Sullivan&#160;et&#160;al. (2023) [<xref rid="B119-sensors-25-05770" ref-type="bibr">119</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional Kernel Transform, CNN</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Layers No. = 3,<break/>Filters No. = 32,<break/>Kernel Size = 3,<break/>stride length = 1,<break/>Pooling Size = 8,<break/>pooling stride<break/>length = 4,<break/>activation function = ReLU,<break/>learning rate = 0.0009,<break/>optimiser = Adam</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN binary<break/>probability<break/>outputs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-Channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Xing&#160;et&#160;al.<break/>(2024) [<xref rid="B132-sensors-25-05770" ref-type="bibr">132</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: Detrend, downsampling (200&#160;Hz), BPF (1&#8211;50&#160;Hz), HPF (1&#160;Hz), Normalization. <bold>R</bold>: n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 s; overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1-D convolution<break/>layers</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>feature maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">optimizer = Adam,<break/>learning rate = 0.001,<break/>epochs = 1000,<break/>input segments = 4</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c. (clean signal<break/>reconstruction)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bahadur&#160;et&#160;al.<break/>(2024) [<xref rid="B122-sensors-25-05770" ref-type="bibr">122</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>SS</bold>: BPF (0.5&#8211;40&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet coefficients</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mother wavelet = Haar, decomposition levels chosen from <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to cover 0.1&#8211;16&#160;Hz ocular band</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Arpaia&#160;et&#160;al.<break/>(2024) [<xref rid="B123-sensors-25-05770" ref-type="bibr">123</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>R</bold>: Notch (50&#160;Hz),<break/>HPF (1&#160;Hz)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5 s; overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MEMD + ASR (PCA)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMFs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASR: window<break/>length = 0.5 s,<break/>cutoff k = 9;<break/>MEMD: n.r.;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr><tr><td rowspan="2" align="left" valign="middle" colspan="1">Ingolfsson et al.<break/>(2024) [<xref rid="B124-sensors-25-05770" ref-type="bibr">124</xref>]</td><td rowspan="2" align="left" valign="middle" colspan="1">n.c.</td><td rowspan="2" align="left" valign="middle" colspan="1">1 s, 2 s, 4 s,<break/>8 s; overlap n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time&#8211;frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Energy features</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mother wavelet = Haar; decomposition levels = 4</td><td rowspan="2" align="left" valign="middle" colspan="1">TPOT</td><td rowspan="2" align="left" valign="middle" colspan="1">TPOT (train) +<break/>MCCP<break/>(embedded)</td><td rowspan="2" align="left" valign="middle" colspan="1">All channels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FFT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High-frequency energy</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Saleh&#160;et&#160;al. (2024) [<xref rid="B125-sensors-25-05770" ref-type="bibr">125</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4 s; overlap n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1-D convolution layers</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional feature&#160;maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c. (clean signal reconstruction)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-channel</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nair&#160;et&#160;al.<break/>(2025) [<xref rid="B126-sensors-25-05770" ref-type="bibr">126</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time/Space</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional<break/>feature maps</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.r.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n.c.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FC layer +<break/>sigmoid<break/>activation<break/>(thresholding)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-channel</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>