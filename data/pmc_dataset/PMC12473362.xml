<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473362</article-id><article-id pub-id-type="pmcid-ver">PMC12473362.1</article-id><article-id pub-id-type="pmcaid">12473362</article-id><article-id pub-id-type="pmcaiid">12473362</article-id><article-id pub-id-type="pmid">41012938</article-id><article-id pub-id-type="doi">10.3390/s25185701</article-id><article-id pub-id-type="publisher-id">sensors-25-05701</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Robust Pose Estimation and Size Classification for Unknown Dump Truck Using Normal Distribution Transform</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Imai</surname><given-names initials="K">Kai</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05701" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Watanabe</surname><given-names initials="K">Kota</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-05701" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Okabe</surname><given-names initials="H">Hiroaki</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05701" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Matsuyama</surname><given-names initials="T">Takafumi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05701" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shirao</surname><given-names initials="A">Atsushi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af2-sensors-25-05701" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6971-5280</contrib-id><name name-style="western"><surname>Ito</surname><given-names initials="T">Takuma</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05701" ref-type="aff">1</xref><xref rid="c1-sensors-25-05701" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Wang</surname><given-names initials="M">Miaohui</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05701"><label>1</label>Graduate School of Engineering, The University of Tokyo, 7-3-1 Hongo, Tokyo 113-8656, Japan</aff><aff id="af2-sensors-25-05701"><label>2</label>Komatsu Ltd., Tsu 23 Futsu-machi, Komatsu-shi 923-0392, Ishikawa, Japan</aff><author-notes><corresp id="c1-sensors-25-05701"><label>*</label>Correspondence: <email>takumaito@g.ecc.u-tokyo.ac.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5701</elocation-id><history><date date-type="received"><day>28</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>08</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>09</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>12</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05701.pdf"/><abstract><p>Labor shortage has been a severe problem in the Japanese construction industry, and the automation of construction work has been in high demand. One of the needs is the automation of soil loading onto dump trucks. This task requires pose estimation and size classification of the dump trucks to determine the appropriate loading position and volume. At actual construction sites, specifications of dump trucks are not always known in advance. However, most of the existing methods cannot robustly estimate the pose and the size of such unknown dump trucks. To address this issue, we propose a two-stage method that estimates the pose of dump trucks and then classifies their size categories. We use Normal Distribution Transform (NDT) for pose estimation of dump trucks. Specifically, we utilize NDT templates of dump trucks which distinguish global differences among size categories and simultaneously absorb local shape variations within each category. The proposed method is evaluated by data in a real-world environment. The proposed method appropriately estimates the pose of dump trucks under various settings of positions and orientations. In addition, the method correctly classifies the observed dump truck with all three predefined size categories. Furthermore, the computation time is approximately 0.13 s, which is sufficiently short for practical operation. These results indicate that the method will contribute to the automation of soil loading onto dump trucks with unknown specifications.</p></abstract><kwd-group><kwd>dump truck</kwd><kwd>pose estimation</kwd><kwd>size classification</kwd><kwd>normal distribution transform</kwd><kwd>LiDAR</kwd><kwd>point cloud</kwd><kwd>construction automation</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05701"><title>1. Introduction</title><p>In Japan, the construction industry has faced a labor shortage due to an aging population and declining number of skilled workers. Specifically, the number of construction technicians has been basically decreasing over the past ten years [<xref rid="B1-sensors-25-05701" ref-type="bibr">1</xref>]. According to the statistical survey about the working population of the construction industry in 2024 [<xref rid="B1-sensors-25-05701" ref-type="bibr">1</xref>], those aged 60 and above account for over 25% of the total, while those under 30 account for about 12%. For this social background, the automation of construction tasks has become necessary. One of the major unsolved issues is to automate soil loading onto dump trucks with wheel loaders. <xref rid="sensors-25-05701-f001" ref-type="fig">Figure 1</xref>a illustrates the component technologies and the pipeline for soil loading. In this case, the automation consists of some processes such as bucket filling [<xref rid="B2-sensors-25-05701" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05701" ref-type="bibr">3</xref>], autonomous locomotion [<xref rid="B4-sensors-25-05701" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05701" ref-type="bibr">5</xref>], and soil loading onto dump trucks [<xref rid="B6-sensors-25-05701" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05701" ref-type="bibr">7</xref>]. Among these processes, we focus on the recognition of dump trucks for soil loading based on point cloud data. To be more precise, the wheel loaders require pose estimation and size classification of the dump trucks to determine the appropriate loading position and capacity. <xref rid="sensors-25-05701-f001" ref-type="fig">Figure 1</xref>b illustrates the assumed layout of the wheel loader, dump truck, and soil. In this setup, the automatic loading system needs to estimate the pose and classify the size of the dump truck located in front of the wheel loader for automatic soil loading.</p><p>One of the key challenges in actual construction sites is that the specifications of dump trucks are not always known in advance. Because dump trucks from various contractors enter and leave actual construction sites, it is difficult for the automatic loading system to fully manage their specifications. This situation raises two issues. First, classification of size categories is required to estimate the loading capacity of each truck. Second, pose estimation becomes more difficult due to the increased variability in truck shape within each size category. As for the second issue, <xref rid="sensors-25-05701-f002" ref-type="fig">Figure 2</xref> illustrates the local shape variations in the unknown dump trucks due to the design of the vessel. Because various contractors retrofit base vehicles with specific vessels, local shape variations occur even within the same size category depending on the vessel design. Therefore, it is necessary to develop a method that performs both pose estimation and size classification for dump trucks with unknown specifications. To address this challenge, the method must distinguish global differences among size categories in size classification and also absorb local shape variations within each category in pose estimation. In addition, for practical implementation on construction machinery, the method should perform fast under limited computational resources.</p><p>Based on the above considerations, this study proposes a two-stage method based on Normal Distribution Transform (NDT) [<xref rid="B8-sensors-25-05701" ref-type="bibr">8</xref>], which is a method of point cloud registration. In existing research for recognition of dump trucks using point clouds, truck poses have often been estimated through point cloud registration between an observed point cloud and a reference point cloud. Among such approaches, Iterative Closest Point (ICP) [<xref rid="B9-sensors-25-05701" ref-type="bibr">9</xref>] has been widely used [<xref rid="B10-sensors-25-05701" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05701" ref-type="bibr">11</xref>]. ICP matches a pair of point clouds by minimizing the distance between each point in the observed point cloud and its nearest neighbor in the reference point cloud. This approach works well when the specifications of the observed dump truck are known in advance and a corresponding reference point cloud can be prepared. However, as discussed above, such assumptions are not always met in the actual construction sites. In addition, although deep-learning-based methods have also been investigated for dump trucks [<xref rid="B12-sensors-25-05701" ref-type="bibr">12</xref>], they still have challenges in terms of the cost of collecting sufficient training data. On the other hand, NDT represents a reference point cloud by a set of normal distributions and minimizes the distance between the observed point cloud and these distributions. Because NDT approximates local shape variations with probability distributions, it is expected to be more robust against shape variations between the observed dump truck and the reference point cloud in the first step. For size classification, NDT alone does not provide a direct solution. Accordingly, we extend it to a parallel comparison based on the fact that NDT optimizes the matching score to determine the transformation. Specifically, we prepare multiple reference point clouds that represent different size categories and conduct NDT-based pose estimations in parallel. At least in Japan, dump truck sizes can be roughly categorized into a limited number of predefined categories, and it is practical to prepare the corresponding reference point clouds. Size classification is then performed by selecting the size category that provides the highest optimized score in the second step.</p><p><xref rid="sensors-25-05701-f003" ref-type="fig">Figure 3</xref> shows the conceptual diagram of the proposed method. First, an NDT template is constructed for each size category of dump trucks. These templates are constructed to distinguish global differences across size categories while absorbing local shape variations within each size category. Then, the pose estimation is performed by matching the input point cloud with the template of each size category by NDT. After that, the size classification is achieved by comparing the optimized scores across size categories and selecting the one with the highest score. In this way, the proposed method achieves robust pose estimation and size classification for unknown dump trucks.</p><p>The main contributions of this study are as follows:<list list-type="bullet"><list-item><p>Proposal of a two-stage method for pose estimation and size classification of dump trucks with unknown specifications using NDT templates;</p></list-item><list-item><p>Experimental validation of the proposed method using real-world data under various settings of different positions and size categories.</p></list-item></list></p><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-05701" ref-type="sec">Section 2</xref> reviews the existing research on the recognition of dump trucks using point cloud data. <xref rid="sec3-sensors-25-05701" ref-type="sec">Section 3</xref> explains the details of the proposed method. In <xref rid="sec4-sensors-25-05701" ref-type="sec">Section 4</xref> and <xref rid="sec5-sensors-25-05701" ref-type="sec">Section 5</xref>, the proposed method is evaluated by real-world data for pose estimation and size classification, respectively. <xref rid="sec6-sensors-25-05701" ref-type="sec">Section 6</xref> organizes discussion and limitations in this study. Finally, <xref rid="sec7-sensors-25-05701" ref-type="sec">Section 7</xref> presents the conclusions.</p></sec><sec id="sec2-sensors-25-05701"><title>2. Related Work</title><p>This study focuses on technologies of pose estimation and size classification of dump trucks using point cloud data. Although we use point cloud registration in the proposed framework, <xref rid="sec2dot1-sensors-25-05701" ref-type="sec">Section 2.1</xref> reviews other approaches to pose and shape estimation and explains motivation for using point cloud registration. Then, <xref rid="sec2dot2-sensors-25-05701" ref-type="sec">Section 2.2</xref> summarizes existing registration methods and describes the advantage for using NDT. Finally, <xref rid="sec2dot3-sensors-25-05701" ref-type="sec">Section 2.3</xref> reviews recognition methods of dump trucks in construction automation and emphasizes that pose and size estimation for dump trucks with unknown specifications has not been sufficiently investigated.</p><sec id="sec2dot1-sensors-25-05701"><title>2.1. Pose and Shape Estimation Using Point Cloud</title><p>In the broader field of automobile recognition, several methods have been proposed to estimate the pose of automobiles by finding a 2D boundary rectangle that fits best to a point cloud. These methods aim to extract pose information from partially observed point clouds of surrounding vehicles captured by onboard LiDAR, using simplified rectangular representations and iterative optimization. Zhang et al. [<xref rid="B13-sensors-25-05701" ref-type="bibr">13</xref>] proposed a fitting method by searching for the optimal orientation angle. Liu et al. [<xref rid="B14-sensors-25-05701" ref-type="bibr">14</xref>] proposed a method for generating candidate attitude angles using a convex hull and fitting a rectangle to the point cloud. Baeg et al. [<xref rid="B15-sensors-25-05701" ref-type="bibr">15</xref>] developed a method for fitting rectangles that encodes the point cloud into 2D grids and uses the number of points in the cell and the center coordinates. Although such rectangle fitting is fast and robust for partially observed point clouds, it is too simplified and insufficient for accurate pose estimation required in soil loading. In addition to such 2D rectangle fitting approaches, deep learning-based methods for detecting 3D bounding boxes have also advanced. While the former iteratively fits rectangles to the observed point cloud through optimization, the latter is trained on labeled datasets and directly outputs bounding boxes for the observed point cloud. For example, Yan et al. [<xref rid="B16-sensors-25-05701" ref-type="bibr">16</xref>] proposed SECOND, which applies sparse convolutions on voxel features. Lang et al. [<xref rid="B17-sensors-25-05701" ref-type="bibr">17</xref>] proposed PointPillars, which encodes point clouds into vertical pillars and then processes them with 2D convolutions. Shi et al. [<xref rid="B18-sensors-25-05701" ref-type="bibr">18</xref>] proposed PV-RCNN, which integrates voxel-based features and keypoint-based features in a two-stage framework. While these methods have high detection accuracy, 3D box representations remain a simplified approximation that does not fully capture the detailed geometry of automobiles and are insufficient for soil loading. To be more precise, automation tasks such as soil loading require recognizing not only the whole dump truck but also more detailed parts, such as the vessel. In contrast, because point cloud registration matches the observed point cloud with a reference point cloud or template, it helps to recognize more detailed parts of the dump truck.</p><p>Apart from the above simplified rectangle or box detection approaches, some studies have aimed to recognize more detailed automobile size and geometry. Zhang et al. [<xref rid="B19-sensors-25-05701" ref-type="bibr">19</xref>] classified automobiles using aerial LiDAR data based on differences in height and overall shape. Kraemer et al. [<xref rid="B20-sensors-25-05701" ref-type="bibr">20</xref>] developed a method that estimates vehicle shape using polylines constrained by free-space information. In addition, Kraemer et al. [<xref rid="B21-sensors-25-05701" ref-type="bibr">21</xref>] employed multi-layer laser scanners and used ICP-based registration to estimate both motion and shape through point cloud accumulation. Monica et al. [<xref rid="B22-sensors-25-05701" ref-type="bibr">22</xref>] introduced a recurrent neural network-based approach that estimates shape and pose from sequential LiDAR data. Other approaches, such as that of Ding et al. [<xref rid="B23-sensors-25-05701" ref-type="bibr">23</xref>], use multiple monocular cameras and convolutional neural networks to detect vehicle key points and estimate 3D geometry. Monica et al. [<xref rid="B24-sensors-25-05701" ref-type="bibr">24</xref>] further extended shape estimation by converting stereo depth maps into point clouds using Pseudo-LiDAR++. While they achieve detailed shape reconstruction using various representations, such as polylines, keypoints, and accumulated point clouds, they typically assume dense observations of the vehicle. In contrast, in the assumed setup in this study, only a partial point cloud of a dump truck is observed by LiDAR. Moreover, computational efficiency is critical for practical operation.</p><p>In summary, methods that detect vehicles as 2D rectangles or 3D bounding boxes are too simplified, while those that aim at more detailed shape estimation have high computational cost. In contrast, because this study focuses on dump trucks representative templates can be prepared in advance. Therefore, the proposed method employs point cloud registration to match the observed point cloud with such templates. To be more precise, the proposed method performs NDT-based pose estimation with multiple normal distribution templates that represent different dump truck size categories in parallel. By comparing the NDT-based scores, our method classifies the size of the dump truck in a computationally efficient way.</p></sec><sec id="sec2dot2-sensors-25-05701"><title>2.2. Methods of Point Cloud Registration</title><p>As described in <xref rid="sec2dot1-sensors-25-05701" ref-type="sec">Section 2.1</xref>, point cloud registration is used for pose estimation of dump trucks in this study. Point cloud registration methods estimate the pose by searching for the coordinate transformation that matches the observed point cloud with a reference point cloud. Among them, one of the most representative and traditional approaches is ICP. ICP matches an observed point cloud with a reference point cloud by iteratively searching for the nearest neighbor points [<xref rid="B9-sensors-25-05701" ref-type="bibr">9</xref>]. On the other hand, in NDT, the reference point cloud is divided into voxels at uniform intervals, and the distribution of the reference point cloud within each voxel is modeled as a normal distribution. Matching is then performed by minimizing the Mahalanobis&#8217; distance between each observed point cloud after transformation and the corresponding normal distribution [<xref rid="B8-sensors-25-05701" ref-type="bibr">8</xref>]. Magnusson et al. [<xref rid="B25-sensors-25-05701" ref-type="bibr">25</xref>] compared the performance of ICP and NDT in terms of robustness to initial pose and computation time. In their experiments, NDT converges from a larger range of initial poses and performs faster than ICP. In addition, an extension of ICP called Generalized-ICP (GICP) [<xref rid="B26-sensors-25-05701" ref-type="bibr">26</xref>] has been proposed. In GICP, each point in the reference and observed point cloud is assumed to be generated from a different normal distribution. Although GICP achieved higher accuracy than both ICP and NDT, it requires more computation time than NDT.</p><p>In the assumed setup in this study, the specifications of the observed dump trucks are unknown, and it is impossible to prepare a reference point cloud of the same vehicle. Therefore, local shape variations within the same size category become a challenge in the case of ICP, which minimizes the distance between individual points. In contrast, GICP and NDT can handle this issue by modeling the reference point cloud not as discrete points but as spatial distributions. Due to its computational efficiency, this study employs NDT and utilizes it for both pose estimation and size classification.</p><p>In addition to the above traditional methods, several learning-based registration methods have also been proposed with the development of deep learning. Aoki et al. [<xref rid="B27-sensors-25-05701" ref-type="bibr">27</xref>] proposed PointNetLK, which integrates PointNet [<xref rid="B28-sensors-25-05701" ref-type="bibr">28</xref>] with a modified Lucas&#8211;Kanade algorithm for point cloud registration. Wang and Solomon [<xref rid="B29-sensors-25-05701" ref-type="bibr">29</xref>] proposed Deep Closest Point, which employs PointNet or DGCNN [<xref rid="B30-sensors-25-05701" ref-type="bibr">30</xref>] to extract point features and applies attention-based soft matching to estimate rigid transformations. Although these deep-learning-based methods achieve higher accuracy compared with traditional methods, they also have a challenge in training. Collecting sufficient labeled data requires considerable cost, and their generalization for out-of-domain data, such as point clouds captured by different LiDAR sensors, is limited. In contrast, NDT template can be constructed if one representative dump truck point cloud is available for each size category. This makes the implementation more practical.</p></sec><sec id="sec2dot3-sensors-25-05701"><title>2.3. Recognition Methods of Dump Trucks in Construction Automation</title><p>To determine the appropriate loading position, the automatic loading system requires pose estimation of the dump truck. One possible approach is to equip the dump truck with sensors or markers in advance and then utilize this information for pose estimation [<xref rid="B31-sensors-25-05701" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05701" ref-type="bibr">32</xref>]. However, as mentioned earlier, because dump trucks from various contractors operate in actual construction sites, such information is not always available. Apart from this approach, various studies have investigated methods based on point cloud data. Stentz et al. [<xref rid="B33-sensors-25-05701" ref-type="bibr">33</xref>] estimated the truck pose by fitting planar regions extracted from point cloud to a six-plane dump truck model. Phillips et al. [<xref rid="B34-sensors-25-05701" ref-type="bibr">34</xref>] generated multiple pose hypotheses of a dump truck in advance and then performed pose estimation by evaluating their likelihood against the acquired point cloud data. Lee et al. [<xref rid="B10-sensors-25-05701" ref-type="bibr">10</xref>] proposed a two-stage pose estimation method which consists of an initial transformation using a hexahedral truck model and a refinement using ICP. Sugasawa et al. [<xref rid="B11-sensors-25-05701" ref-type="bibr">11</xref>] estimated the poses of dump trucks with LiDAR mounted on an excavator using ICP. An et al. [<xref rid="B12-sensors-25-05701" ref-type="bibr">12</xref>] developed a deep point cloud registration network aimed at fusing local and global features and applied it to pose estimation of a dump truck. All of these methods estimate poses by preparing template model or reference point cloud in advance and matching the observed point cloud to it. As a result, the vessel area of the dump truck can be identified and utilized for following soil loading.</p><p>However, the above methods have assumed that the dump truck&#8217;s specifications are known in advance. Therefore, these methods are not robust to the local shape variation in actual construction sites, where the specifications of the dump trucks are not always known in advance. Although learning-based registration approaches may be able to address the issue of shape variation, their applicability is limited by the cost of data collection and the computational resources. To overcome these limitations, we utilize an NDT-based pose estimation method. The important point is that NDT represents reference dump trucks as normal distribution templates, which absorb local shape variations in the dump trucks and perform sufficiently fast for practical operation.</p></sec></sec><sec id="sec3-sensors-25-05701"><title>3. Method</title><sec id="sec3dot1-sensors-25-05701"><title>3.1. Overview</title><p><xref rid="sensors-25-05701-f004" ref-type="fig">Figure 4</xref> shows the pipeline of the proposed method. In advance, a set of templates is constructed from reference point clouds for each size category (<xref rid="sec3dot2-sensors-25-05701" ref-type="sec">Section 3.2</xref>). During online operation, first, the point cloud of the dump truck is observed by the LiDAR sensors mounted on the wheel loader. The observed point cloud is then preprocessed with a rectangle fitting method to obtain an initial transformation for the following NDT process (<xref rid="sec3dot3-sensors-25-05701" ref-type="sec">Section 3.3</xref>). Next, NDT iteratively updates transformation parameters by matching the observed point cloud with pre-constructed templates (<xref rid="sec3dot4-sensors-25-05701" ref-type="sec">Section 3.4</xref>). These preprocess and NDT-based pose estimation are performed in parallel for multiple templates. After the pose estimation for each size category, the method compares the scores which are designed for size classification and then selects the one with the highest value (<xref rid="sec3dot5-sensors-25-05701" ref-type="sec">Section 3.5</xref>). To be more precise, we introduce negative point clouds into the conventional NDT score to correctly distinguish between different size categories.</p><p>In this study, pose estimation is formulated as the problem of estimating a coordinate transformation that matches the observed point cloud with a template whose pose is already known. Because both the wheel loader and the dump truck are positioned horizontally on flat ground at the construction site, we consider the 2-D transformation parameter <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> defined as follows in this study:<disp-formula id="FD1-sensors-25-05701"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent translational parameters along the <italic toggle="yes">x</italic>-axis and <italic toggle="yes">y</italic>-axis, respectively, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the yaw angle. When a point (<inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) in the observed point cloud is transformed by this transformation parameter <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the position of the point after the transformation (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8242;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8242;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8242;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) is expressed as follows:<disp-formula id="FD2-sensors-25-05701"><label>(2)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8242;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8242;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8242;</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#8722;</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the center of the template. In other words, the transformation is defined as a translation in the xy-plane and a rotation around the <italic toggle="yes">z</italic>-axis that passes through the center of the template.</p></sec><sec id="sec3dot2-sensors-25-05701"><title>3.2. Construction of Normal Distribution Template</title><p>Before the online operation, normal distribution templates are constructed for different size categories using reference point clouds. These reference point clouds represent the typical shape of dump trucks in each category, and they are prepared in advance. Specifically, they are created by merging point clouds of a dump truck captured from all directions. We emphasize that the dump trucks which the reference point clouds represent are not always identical to those which will be observed during actual operation. Local shape variations may exist between the reference and observed dump trucks, even within the same size category.</p><p>The following part describes the process of constructing a single template from a reference point cloud. First, the reference point cloud is divided into a grid of voxels. Then, the mean vector <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and covariance matrix <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#931;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are computed for a subset of points within the voxel <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD3-sensors-25-05701"><label>(3)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05701"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#931;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of points within voxel <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) represents the <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th point within the voxel <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><xref rid="sensors-25-05701-f005" ref-type="fig">Figure 5</xref>a illustrates the reference point cloud and constructed template. As shown in <xref rid="sensors-25-05701-f005" ref-type="fig">Figure 5</xref>a, an ellipsoid represents the 95% confidence region of the normal distribution in each voxel. In addition, <xref rid="sensors-25-05701-f005" ref-type="fig">Figure 5</xref>b illustrates how the template is divided into a voxel grid. The voxel grid is defined by 6 parameters: The voxel sizes (<inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and the offsets (<inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). As an example, the process of the voxel placement along the <italic toggle="yes">x</italic>-axis is described as follows:</p><list list-type="order"><list-item><p>The point with the smallest x-coordinate is extracted from the reference point cloud.</p></list-item><list-item><p>The starting line of the grid is then determined by subtracting the offset <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> from the x-coordinate of the above extracted point.</p></list-item><list-item><p>From this starting point, the space is divided along the <italic toggle="yes">x</italic>-axis at intervals <italic toggle="yes">l</italic><sub>x</sub></p></list-item></list><fig position="anchor" id="sensors-25-05701-f005" orientation="portrait"><label>Figure 5</label><caption><p>Visualization of Template. In this case, (<inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) is (0.4, 0.8, 0.4) [m] and (<inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) is (0.2, 0.2, 0.0) [m]. (<bold>a</bold>) Reference point cloud and constructed template. (<bold>b</bold>) Top view and side view of the template with associated parameters.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g005.jpg"/></fig><p>The same process is applied to the <italic toggle="yes">y</italic>-axis and <italic toggle="yes">z</italic>-axis using voxel sizes <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and offsets <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p></sec><sec id="sec3dot3-sensors-25-05701"><title>3.3. Preprocess</title><p>To avoid convergence to an unfavorable local optimum in the NDT-based pose estimation, a preprocess is conducted to roughly estimate the truck&#8217;s pose before the NDT. Although applications that deal with large-scale point clouds often require downsampling or compression [<xref rid="B35-sensors-25-05701" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05701" ref-type="bibr">36</xref>] to improve data transfer speed or execution efficiency, this study uses only a single frame and does not include such processing in the preprocess. This preprocess consists of the following two steps:<list list-type="order"><list-item><p>Filtering of the observed point cloud based on the predefined parking area and height threshold.</p></list-item><list-item><p>Rectangle fitting to approximate the truck&#8217;s shape with a rectangle for estimation of an initial transformation.</p></list-item></list></p><p>The details of each step are explained in the following parts.</p><sec id="sec3dot3dot1-sensors-25-05701"><title>3.3.1. Filtering Based on Parking Area and Height</title><p>First, the observed point cloud is filtered based on the positions. Specifically, points outside the predetermined parking area are removed. In addition, to remove noise and ground points, only points above a certain height threshold are retained. This process ensures that the filtered point cloud primarily represents the dump truck.</p></sec><sec id="sec3dot3dot2-sensors-25-05701"><title>3.3.2. Initial Transformation Using Rectangle Fitting</title><p>After the filtering, the rectangle fitting method [<xref rid="B13-sensors-25-05701" ref-type="bibr">13</xref>] is applied to the filtered point cloud to obtain a 2-D bounding rectangle. This rectangle approximates the horizontal projection of the filtered points. <xref rid="sensors-25-05701-f006" ref-type="fig">Figure 6</xref> illustrates the process for deriving the initial transformation. Here, the coordinate system is defined such that the long side of the template is aligned with the <italic toggle="yes">x</italic>-axis. First, the center of the bounding rectangle is computed from its four vertices. A translational transformation is then applied to align this center with the center of the template, as shown in <xref rid="sensors-25-05701-f006" ref-type="fig">Figure 6</xref>a. Next, a rotational transformation is performed to align the orientation of the bounding rectangle with that of the template. However, because this initial transformation is based only on the geometric shape of the rectangle, it cannot distinguish between forward and backward orientations of the dump truck, as shown in <xref rid="sensors-25-05701-f006" ref-type="fig">Figure 6</xref>b. To address this ambiguity, two types of initial transformations are used in the following NDT-based pose estimation. One transformation is a 180-degree rotation of the other. In other words, one corresponds to the correct orientation and the other to the reversed orientation.</p></sec></sec><sec id="sec3dot4-sensors-25-05701"><title>3.4. NDT-Based Pose Estimation</title><p>In this study, we assume that the yaw angle is estimated with sufficiently high accuracy in the preprocess, and only the translational transformation parameters are updated in the NDT-based pose estimation for computational cost. First, the observed point cloud is transformed by a parameter <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> according to Equation (2). The probability density function is evaluated for all transformed points <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and all voxels in the template. Here, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the total number of the points in the point cloud. For each point <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, let <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the nearest voxel in the template. Then, the likelihood of <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is expressed as follows:<disp-formula id="FD5-sensors-25-05701"><label>(5)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi></mml:msqrt></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msqrt><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#931;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#931;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#931;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the mean vector and covariance matrix of a voxel <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the dimension of <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which is 3 in this study.</p><p>This study employs the NDT formulation proposed by Biber et al. [<xref rid="B37-sensors-25-05701" ref-type="bibr">37</xref>], who introduced a mixture model of a normal distribution and a uniform distribution. By incorporating this mixture model and applying further approximations, their method improves both robustness and computational efficiency. Following their formulation, the evaluation function for parameter <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> becomes computable as a sum of exponential functions [<xref rid="B38-sensors-25-05701" ref-type="bibr">38</xref>] as follows:<disp-formula id="FD6-sensors-25-05701"><label>(6)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#931;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are constraints.</p><p>To optimize <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, the Newton method has been generally used in existing research. However, in this study, we employ the Euler method instead. This decision is based on two reasons. First, we found that the Newton method exhibits unstable behavior near inflection points, which can lead to divergence in the update of transformation parameters. Second, the Newton method requires the computation of the Hessian matrix, resulting in a high computational cost. Based on these considerations, we employed the Euler method to improve stability and efficiency in NDT-based pose estimation for dump trucks.</p><p>In the Euler method, the transformation parameter&#8217;s increment <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is given as follows:<disp-formula id="FD7-sensors-25-05701"><label>(7)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the step size and <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the gradient vector of <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Because the preprocess provides a rough initial transformation and the NDT aims to refine this transformation, we limit the maximum value of <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> by setting <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD8-sensors-25-05701"><label>(8)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This ensures a maximum movement of 0.01 m within one iteration.</p><p>As described in <xref rid="sec3dot1-sensors-25-05701" ref-type="sec">Section 3.1</xref>, the preprocess and the NDT-based pose estimation are performed with multiple templates corresponding to different size categories. In addition, as described in <xref rid="sec3dot3-sensors-25-05701" ref-type="sec">Section 3.3</xref>, to determine the correct forward-backward orientation, two different initial transformations are inputted to the NDT-based pose estimation. Therefore, for the observed point cloud, the system conducts NDT-based pose estimation in parallel for all combinations of size categories and forward-backward orientations. In other words, if <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of size categories, the system performs <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> NDT-based evaluations in parallel, considering both possible orientations for each category.</p></sec><sec id="sec3dot5-sensors-25-05701"><title>3.5. Size Classification with Negative Point Cloud</title><p>For the <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> NDT-based pose estimations which are conducted in parallel, the system performs a two-step score selection. First, for each size category, it compares the two NDT scores with different initial orientations and selects the higher score. This first step determines the correct forward-backward orientation within each size category. Next, among the <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> NDT results that correspond to different size categories, the system selects the highest score. This second step determines the correct size category. Through this two-step process, the system determines both the forward-backward orientation and the size category of the observed dump truck.</p><p>However, in the second step of the score selection, the size classification cannot be reliably performed only by comparing the conventional NDT scores. Because NDT maximizes the overlap between the template and the transformed point cloud, it may result in an invalid higher score when the template corresponds to a larger dump truck than the observed point cloud. <xref rid="sensors-25-05701-f007" ref-type="fig">Figure 7</xref>a,b illustrate valid and invalid overlap between the point cloud and the template, respectively. In the figures, the green points represent the observed point cloud, and a set of red ellipsoids represents the template. To mitigate the issue of this invalid overlap, we propose a method to reduce the score when the template size is larger than the actual dump truck. Specifically, in the second step of the score selection, we introduce a virtual &#8220;negative&#8221; point cloud around the observed point cloud, which overlaps with the larger templates as shown in <xref rid="sensors-25-05701-f007" ref-type="fig">Figure 7</xref>c. In the figure, the blue points represent the negative point cloud. This negative point cloud contributes as a negative term when calculating the NDT score. The NDT score incorporating the negative point cloud is expressed as<disp-formula id="FD9-sensors-25-05701"><label>(9)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#931;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent the number of points of the actual and negative point cloud, respectively. Here, the negative term is incorporated into the score by setting <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD10-sensors-25-05701"><label>(10)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><xref rid="sensors-25-05701-f008" ref-type="fig">Figure 8</xref> illustrates the positioning of the negative point cloud. In the figure, the green points represent the observed point cloud, while the blue points represent the negative point cloud. The negative point cloud is assigned to the areas in front of and behind the observed point cloud, as well as above the vessel. The spatial range of the negative point cloud in front of and behind the dump truck is defined by the fitted rectangle obtained in the preprocess and parameters <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref rid="sensors-25-05701-f008" ref-type="fig">Figure 8</xref>a. Here, <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the gap distance between the negative point cloud and the fitted rectangle. Moreover, <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the length of the negative point cloud range. In addition, the range in the y-direction is defined by the minimum and maximum y-coordinates of the fitted rectangle. Similarly, the range of the negative point cloud above the vessel is defined by <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref rid="sensors-25-05701-f008" ref-type="fig">Figure 8</xref>b. In addition, the range in the x-direction is defined by the x-coordinate of the center of the fitted rectangle and the maximum x-coordinate. Furthermore, the interval between the negative points <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is also a parameter in the assignment process. These five parameters <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are designed based on the requirements for size classification, and their numerical values are described in <xref rid="sec5dot1-sensors-25-05701" ref-type="sec">Section 5.1</xref>.</p></sec></sec><sec id="sec4-sensors-25-05701"><title>4. Evaluation of Pose Estimation</title><sec id="sec4dot1-sensors-25-05701"><title>4.1. Experimental Setup</title><p>This section evaluates the performance of the NDT-based pose estimation. To isolate the evaluation of pose estimation from that of size classification, the experiments used a single template that belongs to the same size class as the observed dump truck. It should be noted that the template and the observed dump truck were not identical, and local shape variations existed between them. For this evaluation, point cloud data of a dump truck were collected under 12 different settings. Specifically, the dump truck was parked at four different positions for each of the three orientations. These settings are labeled 1-A to 3-D, as shown in <xref rid="sensors-25-05701-f009" ref-type="fig">Figure 9</xref>. The purpose of this experiment is to evaluate the method&#8217;s ability to estimate poses under two types of variation: (1) local shape differences between the template and the observed dump truck within the same size category, and (2) variations in the truck&#8217;s position and orientation within the parking area.</p><p>The whole process was conducted on a laptop computer with an AMD (Santa Clara, CA, USA)<sup>&#174;</sup> Ryzen 7 7730U [<xref rid="B39-sensors-25-05701" ref-type="bibr">39</xref>] processor on C++ nodes implemented in ROS Noetic. In addition, the data were acquired using two Livox HAP LiDARs mounted on the front of the wheel loader. The specifications of the LiDAR sensor are summarized in <xref rid="sensors-25-05701-t001" ref-type="table">Table 1</xref> [<xref rid="B40-sensors-25-05701" ref-type="bibr">40</xref>]. In addition, the maximum number of NDT update iterations was set to 20.</p></sec><sec id="sec4dot2-sensors-25-05701"><title>4.2. Evaluation Method</title><p>The evaluation is conducted from three aspects: (1) correctness of forward-backward orientation, (2) error in the estimated yaw angle <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and (3) errors in the estimated translational parameters <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Each aspect is described below.</p><sec id="sec4dot2dot1-sensors-25-05701"><title>4.2.1. Forward-Backward Orientation</title><p>As described in <xref rid="sec3dot3-sensors-25-05701" ref-type="sec">Section 3.3</xref>, the proposed method performs pose estimation for both forward and backward orientations of the dump truck and selects the one with the higher NDT score. The orientation is considered correct if the selected result matches the actual orientation of the observed dump truck.</p></sec><sec id="sec4dot2dot2-sensors-25-05701"><title>4.2.2. Error in Yaw Angle <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></title><p>To evaluate the rotational accuracy, two points are manually selected in advance, and the line connecting these two points is used as a landmark line. Specifically, the landmark lines are selected on the side surface of the vessel for both the reference and observed point clouds. After applying the estimated transformation, the yaw angle error is calculated as the difference between the slopes of the reference line and the transformed line.</p></sec><sec id="sec4dot2dot3-sensors-25-05701"><title>4.2.3. Error in Translational Parameters <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
and <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></title><p>To evaluate translational accuracy, a feature point is manually selected as a landmark in advance. In this study, the left mirror of the dump truck is selected as the landmark point. To cancel out the rotational error, the observed point cloud is first rotated by the error in <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> after pose estimation. Then, the corresponding point in the observed point cloud after transformation is compared with the position in the reference point cloud. The errors in <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are computed as the differences between these two positions.</p></sec></sec><sec id="sec4dot3-sensors-25-05701"><title>4.3. Results of Single Setting</title><p>First, we summarize the results of case 1-A, which is shown in <xref rid="sensors-25-05701-f009" ref-type="fig">Figure 9</xref>, for detailed analysis of one setting. <xref rid="sensors-25-05701-f010" ref-type="fig">Figure 10</xref> shows the observed point cloud after transformation by the NDT-based pose estimation. Specifically, <xref rid="sensors-25-05701-f010" ref-type="fig">Figure 10</xref>a,b show the template and the observed point cloud after transformation in the top and side views, respectively. In the figures, a set of red ellipsoids represents the template, and green points represent the observed point cloud. In this case, the forward-backward orientation is correctly estimated, and the errors in <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are 0.01 m, 0.10 m, and &#8722;0.012 rad, respectively.</p></sec><sec id="sec4dot4-sensors-25-05701"><title>4.4. Results of All Settings</title><p>Next, we analyze the results of all settings for comprehensive performance. <xref rid="sensors-25-05701-t002" ref-type="table">Table 2</xref> summarizes the pose estimation results for the 12 settings. In the table, the &#8220;Orientation&#8221; column indicates whether the forward-backward orientation was correctly estimated. Here, &#8220;&#10003;&#8221; denotes a correct estimation, while &#8220;&#215;&#8221; denotes an incorrect estimation. The &#8220;Score&#8221; column indicates the maximum NDT score. In addition, &#8220;Error <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#8221;, &#8220;Error <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#8221;, and &#8220;Error <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#8221; columns indicate the errors in <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. These estimation errors are evaluated for all rows except for the rows of cases 2-D and 3-C, where the orientation was incorrectly estimated.</p><p><xref rid="sensors-25-05701-f011" ref-type="fig">Figure 11</xref> shows box plots of absolute errors for the ten cases where the orientation was correctly estimated. In these cases, the maximum absolute errors were 0.10 m in <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, 0.16 m in <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and 0.019 rad (approximately 1.089&#176;) in <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. These results indicate that the proposed method can estimate the pose with reasonable accuracy when the correct orientation is estimated.</p><p>As an example case of incorrect orientation estimation, <xref rid="sensors-25-05701-f012" ref-type="fig">Figure 12</xref> shows the result for case 3-C. In this case, the method failed to estimate the correct forward-backward orientation of the dump truck. <xref rid="sensors-25-05701-f013" ref-type="fig">Figure 13</xref> illustrates a side view of the transformed point cloud overlaid with the reference point cloud. It shows that the top part of the dump truck was outside the LiDAR&#8217;s observable area due to the positional relationship between the sensor and the truck. This limitation in observation might contribute to the incorrect pose estimation. Such an incorrect case is primarily due to limitations in sensor coverage rather than the method.</p><p>In summary, although the results show certain limitations in cases where the LiDAR observation is insufficient, the proposed method can estimate the pose with reasonable accuracy even under variations in the truck&#8217;s position and orientation within the parking area.</p></sec></sec><sec id="sec5-sensors-25-05701"><title>5. Evaluation of Size Classification</title><sec id="sec5dot1-sensors-25-05701"><title>5.1. Size Categories of Dump Trucks</title><p>In this section, we evaluate the size classification performance using multiple templates representing different dump truck size categories. <xref rid="sensors-25-05701-t003" ref-type="table">Table 3</xref> summarizes the four dump trucks used as observed point clouds, which are categorized into small, medium, and large. For each size category, one template was constructed using a representative truck; Small A truck was used for small template, Medium for medium, and Large for large. <xref rid="sensors-25-05701-f014" ref-type="fig">Figure 14</xref> shows the reference point clouds used to construct these templates. To evaluate classification performance, we examined all combinations of the four observed point clouds and the three templates. This setting allows us to evaluate the method&#8217;s ability to distinguish between size categories, as well as its robustness to local shape variations within the same category.</p><p>As described in <xref rid="sec3dot5-sensors-25-05701" ref-type="sec">Section 3.5</xref>, the parameters for placing negative point clouds are determined according to the size specifications of dump trucks. In this evaluation, the parameters were set for classification based on both truck length and vessel height. Specifically, <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were both set to 0.3 m to classify by dump truck length. Also, <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were set to 0.4 m and 0.5 m to classify by vessel height, respectively. In addition, the interval between points <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was set to 0.1 m.</p></sec><sec id="sec5dot2-sensors-25-05701"><title>5.2. Results</title><p><xref rid="sensors-25-05701-t004" ref-type="table">Table 4</xref> shows the results of the size classification before incorporating negative point clouds. The table displays the score computed for each combination of template and observed point clouds. The highest score for each observed point cloud is highlighted in bold and marked with an asterisk. The rightmost column indicates whether the classification was correct. These results show that when a small dump truck was observed, it was frequently misclassified as a larger size. This misclassification occurred because the computed score was incorrectly high when the size category of the template was larger than the observed point cloud. Without negative point clouds, the computed score lacked sufficient penalization for incorrectly overlapped regions between the observed point cloud and the template.</p><p>In contrast, <xref rid="sensors-25-05701-t005" ref-type="table">Table 5</xref> shows the results after incorporating negative point clouds. The results demonstrate that appropriate size classification was achieved, even for observed dump trucks belonging to the small category. <xref rid="sensors-25-05701-f015" ref-type="fig">Figure 15</xref> illustrates the spatial relationship between the template and the observed point cloud after incorporating a negative point cloud. Specifically, the figure shows the combination of the large-size template and the point cloud of Small A truck. In the figure, a set of red ellipsoids indicates the template, the green points indicate the observed point cloud after transformation, and the blue points indicate the negative point cloud. In this case, the score decreases where the template overlaps with the negative point cloud. As a result, the score was reduced from 1.02 to 0.52 in this case. On the other hand, <xref rid="sensors-25-05701-f016" ref-type="fig">Figure 16</xref> shows the combination of the medium-size template and the observed point cloud of Medium truck. In this case, the score decreased slightly from 1.34 to 1.33 because the template and negative point cloud are far away, as shown in the figure.</p><p>These results demonstrate that incorporating negative point clouds into the conventional NDT score enables the system to distinguish global differences between size categories. Consequently, the proposed method can achieve size classification even with the presence of local shape variations within the same category.</p><p>Finally, we discuss the computational time of the proposed method. As described in <xref rid="sec3-sensors-25-05701" ref-type="sec">Section 3</xref>, when an observed point cloud of a dump truck is input, a total of <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> pipelines are performed in parallel. In this study, the number of size categories <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is 3. <xref rid="sensors-25-05701-t006" ref-type="table">Table 6</xref> shows the computation time of each process when the observed point cloud corresponds to the Small A dump truck. The table shows the computation time for each process: Preprocess (<xref rid="sec3dot3-sensors-25-05701" ref-type="sec">Section 3.3</xref>), NDT-based pose estimation (<xref rid="sec3dot4-sensors-25-05701" ref-type="sec">Section 3.4</xref>), Size classification (<xref rid="sec3dot5-sensors-25-05701" ref-type="sec">Section 3.5</xref>), and Total. In this case, even the slowest pipeline, which is the bottleneck of the whole method, required 0.132 s in total. This result demonstrates that the proposed method is suitable for practical operation.</p></sec></sec><sec sec-type="discussion" id="sec6-sensors-25-05701"><title>6. Discussion and Limitations</title><p>This study proposed an NDT-based two-stage framework that addresses pose estimation and size classification of dump trucks with unknown specifications. The method utilizes the probabilistic representation of point clouds to handle local shape variations and extends NDT to parallel comparisons across multiple templates for size classification. The experimental results demonstrated that the method could estimate truck poses with sufficient accuracy while correctly classifying trucks into three predefined size categories. The computational time is approximately 0.13 s per estimation, which is suitable for practical operation under limited computational resources. These results support the feasibility of applying the proposed framework to construction machinery in practice.</p><p>Compared with the design approaches in existing studies, two main approaches have been explored for pose estimation of dump truck using point cloud registration: ICP-based methods and deep-learning-based methods. The former generally assumes that the observed dump truck and the reference point cloud correspond to the same vehicle. In contrast, this study verified that pose estimation can be achieved even when the observed dump truck and the template are from different vehicles. On the other hand, although deep-learning-based approaches have the potential to achieve high accuracy and robustness, they require large training data, which have a considerable cost in terms of data collection, and they also have challenges for practical operation on construction machinery with limited computational resources. In contrast, the proposed method only requires representative reference point clouds as templates, which is more practical for construction machinery. Furthermore, by extending NDT to parallel comparisons, the proposed method also achieves size classification of dump trucks. To the best of our knowledge, this function has not been addressed in existing studies.</p><p>Despite these contributions, several limitations remain. First, the experiments were limited to three size categories, which does not fully reflect the diversity of actual size categories of dump truck. Although the number of size categories for dump trucks is finite and few, the three categories considered in the experiments are somewhat fewer than those in practice. Therefore, further validation with additional size categories is necessary. Second, the robustness of the method under adverse environmental conditions, such as rain, fog, or dust, was not evaluated. Accordingly, further experiments under real or simulated adverse environments are needed. Third, the proposed method depends on the coverage of LiDAR sensors, and pose errors may occur if parts of the truck lie outside the coverage. Although this issue could be mitigated by carefully selecting the mounting position of LiDAR sensors, in actual application, the sensor placement is determined not only for dump truck recognition but also in consideration of other functions, such as recognition of piled soil or autonomous locomotion. Specifically, one possible way to reduce the problem in this study is to mount the LiDAR upward so that the top part of the dump truck can be observed. However, this also creates a trade-off because the blind area on the ground becomes larger and makes it harder to detect surrounding objects. Therefore, a comprehensive design discussion is required regarding sensor placement. Finally, parameters related to negative point clouds were tuned for specific scenarios and their adaptability to other cases was not validated. For future work, we intend to develop an automatic parameter searching approach to improve adaptability across datasets.</p></sec><sec sec-type="conclusions" id="sec7-sensors-25-05701"><title>7. Conclusions</title><p>In this study, we proposed a two-stage method for pose estimation and size classification of dump trucks with unknown specifications. The proposed method performs pose estimation and size classification by NDT in parallel with multiple templates that represent different size categories. For appropriate size classification, we incorporate negative point clouds into the conventional NDT score. The performance of the proposed method was evaluated using data acquired in a real-world environment. The results demonstrated that the proposed method could estimate the pose of dump trucks robustly and classify their size by incorporating negative point clouds. Based on these results, the proposed method will contribute to not only wheel loader operation but also the overall automation of soil loading onto dump trucks at construction sites.</p><p>However, as described in <xref rid="sec6-sensors-25-05701" ref-type="sec">Section 6</xref>, several limitations, such as the limited number of size categories, the evaluation under adverse environments, the investigation on LiDAR placement, and the requirement for more adaptable parameter settings, remain to be addressed. It is necessary for future work to address these issues to enhance the robustness and applicability of the proposed method in real construction environments.</p></sec></body><back><ack><title>Acknowledgments</title><p>This work was supported by KOMATSU Ltd.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, K.I., K.W., A.S. and T.I.; Methodology, K.I., K.W. and T.I.; Software, K.I. and K.W.; Validation, K.I. and K.W.; Formal analysis, K.I., K.W. and T.I.; Investigation, K.I., K.W., H.O. and T.M.; Writing&#8212;original draft, K.I. and K.W.; Writing&#8212;review &amp; editing, H.O., T.M., A.S. and T.I.; Visualization, K.I.; Supervision, A.S. and T.I.; Project administration, A.S. and T.I. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>K.I., K.W., H.O., T.M. and T.I. are inventors of pending patent on this work. H.O., T.M. and A.S. are employees of Komatsu Ltd. T.I. is an employee of the University of Tokyo. The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">ICP</td><td align="left" valign="middle" rowspan="1" colspan="1">Iterative Closest Point</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NDT</td><td align="left" valign="middle" rowspan="1" colspan="1">Normal Distribution Transform</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05701"><label>1.</label><element-citation publication-type="webpage"><article-title>Labour Force Survey; e-Stat Portal Site of Official Statics of Japan</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.e-stat.go.jp/en/stat-search/files?page=1&amp;layout=datalist&amp;toukei=00200531&amp;tstat=000001226583&amp;cycle=7&amp;tclass1=000001226584&amp;tclass2=000001226585&amp;tclass3val=0" ext-link-type="uri">https://www.e-stat.go.jp/en/stat-search/files?page=1&amp;layout=datalist&amp;toukei=00200531&amp;tstat=000001226583&amp;cycle=7&amp;tclass1=000001226584&amp;tclass2=000001226585&amp;tclass3val=0</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-20">(accessed on 20 March 2025)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-05701"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dadhich</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sandin</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bodin</surname><given-names>U.</given-names></name><name name-style="western"><surname>Andersson</surname><given-names>U.</given-names></name><name name-style="western"><surname>Martinsson</surname><given-names>T.</given-names></name></person-group><article-title>Field test of neural-network based automatic bucket-filling algorithm for wheel-loaders</article-title><source>Autom. Constr.</source><year>2019</year><volume>97</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2018.10.013</pub-id></element-citation></ref><ref id="B3-sensors-25-05701"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eriksson</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ghabcheloo</surname><given-names>R.</given-names></name><name name-style="western"><surname>Geimer</surname><given-names>M.</given-names></name></person-group><article-title>Optimizing bucket-filling strategies for wheel loaders inside a dream environment</article-title><source>Autom. Constr.</source><year>2024</year><volume>168</volume><fpage>105804</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2024.105804</pub-id></element-citation></ref><ref id="B4-sensors-25-05701"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name></person-group><article-title>Synchronized path planning and tracking for front and rear axles in articulated wheel loaders</article-title><source>Autom. Constr.</source><year>2024</year><volume>165</volume><fpage>105538</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2024.105538</pub-id></element-citation></ref><ref id="B5-sensors-25-05701"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Scene-Adaptive Loader Trajectory Planning and Tracking Control</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>1135</elocation-id><pub-id pub-id-type="doi">10.3390/s25041135</pub-id><pub-id pub-id-type="pmid">40006364</pub-id><pub-id pub-id-type="pmcid">PMC11858886</pub-id></element-citation></ref><ref id="B6-sensors-25-05701"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Intelligentization of wheel loader shoveling system based on multi-source data acquisition</article-title><source>Autom. Constr.</source><year>2023</year><volume>147</volume><fpage>104733</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2022.104733</pub-id></element-citation></ref><ref id="B7-sensors-25-05701"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ekevid</surname><given-names>T.</given-names></name><name name-style="western"><surname>L&#246;we</surname><given-names>W.</given-names></name></person-group><article-title>Operator model for wheel loader short-cycle loading handling</article-title><source>Autom. Constr.</source><year>2024</year><volume>167</volume><fpage>105691</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2024.105691</pub-id></element-citation></ref><ref id="B8-sensors-25-05701"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Biber</surname><given-names>P.</given-names></name><name name-style="western"><surname>Strasser</surname><given-names>W.</given-names></name></person-group><article-title>The normal distributions transform: A new approach to laser scan matching</article-title><source>Proceedings of the 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;31 October 2003</conf-date><fpage>2743</fpage><lpage>2748</lpage></element-citation></ref><ref id="B9-sensors-25-05701"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Besl</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>McKay</surname><given-names>N.D.</given-names></name></person-group><article-title>A method for registration of 3-D shapes</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1992</year><volume>14</volume><fpage>239</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1109/34.121791</pub-id></element-citation></ref><ref id="B10-sensors-25-05701"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>J.-H.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Park</surname><given-names>S.-Y.</given-names></name></person-group><article-title>3D pose recognition system of dump truck for autonomous excavator</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>3471</elocation-id><pub-id pub-id-type="doi">10.3390/app12073471</pub-id></element-citation></ref><ref id="B11-sensors-25-05701"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sugasawa</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chikushi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Komatsu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Louhi Kasahara</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Pathak</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yajima</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hamasaki</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nagatani</surname><given-names>K.</given-names></name><name name-style="western"><surname>Chiba</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chayama</surname><given-names>K.</given-names></name><etal/></person-group><article-title>Visualization of Dump Truck and Excavator in Bird&#8217;s-eye View by Fisheye Cameras and 3D Range Sensor</article-title><source>Intelligent Autonomous Systems 16</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><pub-id pub-id-type="doi">10.1007/978-3-030-95892-3_47</pub-id></element-citation></ref><ref id="B12-sensors-25-05701"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>An</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>L.</given-names></name></person-group><article-title>Point cloud registration network based on dual-attention mechanism for truck pose estimation</article-title><source>Proceedings of the 2024 39th Youth Academic Annual Conference of Chinese Association of Automation (YAC)</source><conf-loc>Dalian, China</conf-loc><conf-date>7&#8211;9 June 2024</conf-date><fpage>1364</fpage><lpage>1370</lpage></element-citation></ref><ref id="B13-sensors-25-05701"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dolan</surname><given-names>J.M.</given-names></name></person-group><article-title>Efficient L-shape fitting for vehicle detection using laser scanners</article-title><source>Proceedings of the IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Los Angeles, CA, USA</conf-loc><conf-date>11&#8211;14 June 2017</conf-date><fpage>54</fpage><lpage>59</lpage></element-citation></ref><ref id="B14-sensors-25-05701"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Estimation of 2D bounding box orientation with convex-hull points&#8212;A quantitative evaluation on accuracy and efficiency</article-title><source>Proceedings of the 2020 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>19 October&#8211;13 November 2020</conf-date><fpage>945</fpage><lpage>950</lpage></element-citation></ref><ref id="B15-sensors-25-05701"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Baeg</surname><given-names>J.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name></person-group><article-title>Oriented bounding box detection robust to vehicle shape on road under real-time constraints</article-title><source>Proceedings of the 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Bilbao, Spain</conf-loc><conf-date>24&#8211;28 September 2023</conf-date><fpage>3383</fpage><lpage>3389</lpage></element-citation></ref><ref id="B16-sensors-25-05701"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>SECOND: Sparsely Embedded Convolutional Detection</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3337</elocation-id><pub-id pub-id-type="doi">10.3390/s18103337</pub-id><pub-id pub-id-type="pmid">30301196</pub-id><pub-id pub-id-type="pmcid">PMC6210968</pub-id></element-citation></ref><ref id="B17-sensors-25-05701"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Caesar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>PointPillars: Fast Encoders for Object Detection From Point Clouds</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>12689</fpage><lpage>12697</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.01298</pub-id></element-citation></ref><ref id="B18-sensors-25-05701"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>10526</fpage><lpage>10535</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.01054</pub-id></element-citation></ref><ref id="B19-sensors-25-05701"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Vosselman</surname><given-names>G.</given-names></name><name name-style="western"><surname>Elberink</surname><given-names>S.J.O.</given-names></name></person-group><article-title>Vehicle recognition in aerial lidar point cloud based on dynamic time warping</article-title><source>IISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2017</year><volume>IV-2/W4</volume><fpage>193</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.5194/isprs-annals-IV-2-W4-193-2017</pub-id></element-citation></ref><ref id="B20-sensors-25-05701"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kraemer</surname><given-names>S.</given-names></name><name name-style="western"><surname>Stiller</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bouzouraa</surname><given-names>M.E.</given-names></name></person-group><article-title>Lidar-based object tracking and shape estimation using polylines and free-space information</article-title><source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>1&#8211;5 October 2018</conf-date><fpage>4515</fpage><lpage>4520</lpage></element-citation></ref><ref id="B21-sensors-25-05701"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kraemer</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bouzouraa</surname><given-names>M.E.</given-names></name><name name-style="western"><surname>Stiller</surname><given-names>C.</given-names></name></person-group><article-title>Simultaneous tracking and shape estimation using a multi-layer laserscanner</article-title><source>Proceedings of the 2017 IEEE 20th Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>16&#8211;19 October 2017</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B22-sensors-25-05701"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Monica</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>W.-L.</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>M.</given-names></name></person-group><article-title>Sequential joint shape and pose estimation of vehicles with application to automatic amodal segmentation labeling</article-title><source>Proceedings of the 2022 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>23&#8211;27 May 2022</conf-date><fpage>2678</fpage><lpage>2685</lpage></element-citation></ref><ref id="B23-sensors-25-05701"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>H.</given-names></name></person-group><article-title>Vehicle pose and shape estimation through multiple monocular vision</article-title><source>Proceedings of the 2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)</source><conf-loc>Kuala Lumpur, Malaysia</conf-loc><conf-date>12&#8211;15 December 2018</conf-date><fpage>709</fpage><lpage>715</lpage></element-citation></ref><ref id="B24-sensors-25-05701"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Monica</surname><given-names>J.</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>M.</given-names></name></person-group><article-title>Vision only 3-D shape estimation for autonomous driving</article-title><source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>24 October&#8211;24 January 2020</conf-date><fpage>1676</fpage><lpage>1683</lpage></element-citation></ref><ref id="B25-sensors-25-05701"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Magnusson</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nuchter</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lorken</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lilienthal</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Hertzberg</surname><given-names>J.</given-names></name></person-group><article-title>Evaluation of 3D registration reliability and speed&#8212;A comparison of ICP and NDT</article-title><source>Proceedings of the 2009 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Kobe, Japan</conf-loc><conf-date>12&#8211;17 May 2009</conf-date><fpage>3907</fpage><lpage>3912</lpage></element-citation></ref><ref id="B26-sensors-25-05701"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Segal</surname><given-names>A.V.</given-names></name><name name-style="western"><surname>Haehnel</surname><given-names>D.</given-names></name><name name-style="western"><surname>Thrun</surname><given-names>S.</given-names></name></person-group><article-title>Generalized-ICP</article-title><source>Proceedings of the Robotics: Science and Systems (RSS)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>28 June&#8211;1 July 2009</conf-date></element-citation></ref><ref id="B27-sensors-25-05701"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Aoki</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Goforth</surname><given-names>H.</given-names></name><name name-style="western"><surname>Srivatsan</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Lucey</surname><given-names>S.</given-names></name></person-group><article-title>PointNetLK: Robust &amp; Efficient Point Cloud Registration Using PointNet</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>7156</fpage><lpage>7165</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.00733</pub-id></element-citation></ref><ref id="B28-sensors-25-05701"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Charles</surname><given-names>R.Q.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kaichun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>77</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.16</pub-id></element-citation></ref><ref id="B29-sensors-25-05701"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Solomon</surname><given-names>J.</given-names></name></person-group><article-title>Deep Closest Point: Learning Representations for Point Cloud Registration</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;20 June 2019</conf-date><fpage>3522</fpage><lpage>3531</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2019.00362</pub-id></element-citation></ref><ref id="B30-sensors-25-05701"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sarma</surname><given-names>S.E.</given-names></name><name name-style="western"><surname>Bronstein</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Solomon</surname><given-names>J.M.</given-names></name></person-group><article-title>Dynamic Graph CNN for Learning on Point Clouds</article-title><source>ACM Trans. Graph.</source><year>2019</year><volume>38</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3326362</pub-id></element-citation></ref><ref id="B31-sensors-25-05701"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name></person-group><article-title>Machine vision based autonomous loading perception for super-huge mining excavator</article-title><source>Proceedings of the 2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)</source><conf-loc>Chengdu, China</conf-loc><conf-date>1&#8211;4 August 2021</conf-date><fpage>1250</fpage><lpage>1255</lpage></element-citation></ref><ref id="B32-sensors-25-05701"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suzuki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ohno</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kojima</surname><given-names>S.</given-names></name><name name-style="western"><surname>Miyamoto</surname><given-names>N.</given-names></name><name name-style="western"><surname>Suzuki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Komatsu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nagatani</surname><given-names>K.</given-names></name></person-group><article-title>Estimation of articulated angle in six-wheeled dump trucks using multiple GNSS receivers for autonomous driving</article-title><source>Adv. Robot.</source><year>2021</year><volume>35</volume><fpage>1376</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1080/01691864.2021.1974942</pub-id></element-citation></ref><ref id="B33-sensors-25-05701"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stentz</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bares</surname><given-names>J.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rowe</surname><given-names>P.</given-names></name></person-group><article-title>A Robotic Excavator for Autonomous Truck Loading</article-title><source>Auton. Robot.</source><year>1999</year><volume>7</volume><fpage>175</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1023/A:1008914201877</pub-id></element-citation></ref><ref id="B34-sensors-25-05701"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Phillips</surname><given-names>T.G.</given-names></name><name name-style="western"><surname>McAree</surname><given-names>P.R.</given-names></name></person-group><article-title>An evidence-based approach to object pose estimation from LiDAR measurements in challenging environments</article-title><source>J. Field Robot.</source><year>2018</year><volume>35</volume><fpage>921</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1002/rob.21788</pub-id></element-citation></ref><ref id="B35-sensors-25-05701"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name></person-group><article-title>Compression Approaches for LiDAR Point Clouds and Beyond: A Survey</article-title><source>ACM Trans. Multimed. Comput. Commun.</source><year>2025</year><volume>188</volume><fpage>31</fpage><pub-id pub-id-type="doi">10.1145/3715916</pub-id></element-citation></ref><ref id="B36-sensors-25-05701"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>W.</given-names></name></person-group><article-title>suLPCC: A Novel LiDAR Point Cloud Compression Framework for Scene Understanding Tasks</article-title><source>IEEE Trans. Ind. Inform.</source><year>2025</year><volume>21</volume><fpage>3816</fpage><lpage>3827</lpage><pub-id pub-id-type="doi">10.1109/TII.2025.3534400</pub-id></element-citation></ref><ref id="B37-sensors-25-05701"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Biber</surname><given-names>P.</given-names></name><name name-style="western"><surname>Fleck</surname><given-names>S.</given-names></name><name name-style="western"><surname>Strasser</surname><given-names>W.</given-names></name></person-group><article-title>A probabilistic framework for robust and accurate matching of point clouds</article-title><source>Lect. Notes Comput. Sci.</source><year>2004</year><volume>3175</volume><fpage>480</fpage><lpage>487</lpage></element-citation></ref><ref id="B38-sensors-25-05701"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Magnusson</surname><given-names>M.</given-names></name></person-group><article-title>The three-dimensional normal-distributions transform: An efficient representation for registration, surface analysis, and loop detection</article-title><source>&#214;rebro Stud. Technol.</source><year>2009</year><volume>36</volume><fpage>201</fpage></element-citation></ref><ref id="B39-sensors-25-05701"><label>39.</label><element-citation publication-type="webpage"><article-title>AMD Ryzen&#8482; 7 7730U, AMD</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.amd.com/en/products/processors/laptop/ryzen/7000-series/amd-ryzen-7-7730u.html" ext-link-type="uri">https://www.amd.com/en/products/processors/laptop/ryzen/7000-series/amd-ryzen-7-7730u.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-08">(accessed on 8 September 2025)</date-in-citation></element-citation></ref><ref id="B40-sensors-25-05701"><label>40.</label><element-citation publication-type="webpage"><article-title>Livox HAP (T1) User Manual, Livox</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dl.djicdn.com/downloads/Livox/HAP/HAP(T1)_User_Manual_V1.2_EN.pdf" ext-link-type="uri">https://dl.djicdn.com/downloads/Livox/HAP/HAP(T1)_User_Manual_V1.2_EN.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-28">(accessed on 28 August 2025)</date-in-citation></element-citation></ref><ref id="B41-sensors-25-05701"><label>41.</label><element-citation publication-type="book"><std>IEC 60825-1:2014</std><source>Safety of Laser Products-Part 1: Equipment Classification and Requirements</source><publisher-name>International Electrotechnical Commission</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2014</year></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05701-f001" orientation="portrait"><label>Figure 1</label><caption><p>Assumed situation in automated soil loading. (<bold>a</bold>) Component technologies and the pipeline. (<bold>b</bold>) Layout of the wheel loader, dump truck, and soil.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g001.jpg"/></fig><fig position="float" id="sensors-25-05701-f002" orientation="portrait"><label>Figure 2</label><caption><p>Local shape variations in dump trucks due to vessel design.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g002.jpg"/></fig><fig position="float" id="sensors-25-05701-f003" orientation="portrait"><label>Figure 3</label><caption><p>Conceptual diagram of the proposed method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g003.jpg"/></fig><fig position="float" id="sensors-25-05701-f004" orientation="portrait"><label>Figure 4</label><caption><p>Pipeline of proposed method. Before online operation, template construction is performed in advance (<xref rid="sec3dot2-sensors-25-05701" ref-type="sec">Section 3.2</xref>). Online operation consists of preprocess (<xref rid="sec3dot3-sensors-25-05701" ref-type="sec">Section 3.3</xref>), NDT-based pose estimation (<xref rid="sec3dot4-sensors-25-05701" ref-type="sec">Section 3.4</xref>), and score calculation (<xref rid="sec3dot5-sensors-25-05701" ref-type="sec">Section 3.5</xref>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g004.jpg"/></fig><fig position="float" id="sensors-25-05701-f006" orientation="portrait"><label>Figure 6</label><caption><p>Process of pre-transformation using rectangle fitting. Green dots indicate filtered point cloud, and red dotted lines represent diagonals of fitted rectangle. (<bold>a</bold>) Translational transformation. (<bold>b</bold>) Ambiguity of forward-backward orientation estimation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g006.jpg"/></fig><fig position="float" id="sensors-25-05701-f007" orientation="portrait"><label>Figure 7</label><caption><p>Template and actual point cloud when an invalid higher score situation. (<bold>a</bold>) valid overlap between observed point cloud and template. (<bold>b</bold>) invalid overlap between observed point cloud and template. (<bold>c</bold>) Negative point cloud incorporated into (<bold>b</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g007a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g007b.jpg"/></fig><fig position="float" id="sensors-25-05701-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visualization of transformed point cloud and negative point cloud. White lines around green points represent fitted rectangle, and white lines around blue points represent assignment area for negative points. (<bold>a</bold>) Top view. (<bold>b</bold>) Side view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g008.jpg"/></fig><fig position="float" id="sensors-25-05701-f009" orientation="portrait"><label>Figure 9</label><caption><p>Position and angle conditions of observed data. A&#8211;D represent four different dump truck positions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g009.jpg"/></fig><fig position="float" id="sensors-25-05701-f010" orientation="portrait"><label>Figure 10</label><caption><p>Template and observed point cloud after transformation in case 1-A. (<bold>a</bold>) Top view. (<bold>b</bold>) Side view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g010.jpg"/></fig><fig position="float" id="sensors-25-05701-f011" orientation="portrait"><label>Figure 11</label><caption><p>Box plot of absolute errors in (<bold>a</bold>) <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, (<bold>b</bold>) <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and (<bold>c</bold>) <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Blue area indicates interquartile range.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g011.jpg"/></fig><fig position="float" id="sensors-25-05701-f012" orientation="portrait"><label>Figure 12</label><caption><p>Template and transformed point cloud in case 3-C. (<bold>a</bold>) Top view. (<bold>b</bold>) Side view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g012.jpg"/></fig><fig position="float" id="sensors-25-05701-f013" orientation="portrait"><label>Figure 13</label><caption><p>Observed point cloud and reference point cloud in case 3-C. Green points represent observed point cloud, and red points represent reference point cloud.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g013.jpg"/></fig><fig position="float" id="sensors-25-05701-f014" orientation="portrait"><label>Figure 14</label><caption><p>The reference point clouds of small, medium, and large categories. Red points represent reference point clouds.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g014.jpg"/></fig><fig position="float" id="sensors-25-05701-f015" orientation="portrait"><label>Figure 15</label><caption><p>Template, actual point cloud, and negative point cloud in case where dump truck is Small A while the template is Large. (<bold>a</bold>) Top view. (<bold>b</bold>) Side view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g015.jpg"/></fig><fig position="float" id="sensors-25-05701-f016" orientation="portrait"><label>Figure 16</label><caption><p>Template, actual point cloud, and negative point cloud in case where both the dump truck and template are Medium. (<bold>a</bold>) Top view. (<bold>b</bold>) Side view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05701-g016.jpg"/></fig><table-wrap position="float" id="sensors-25-05701-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05701-t001_Table 1</object-id><label>Table 1</label><caption><p>Specifications of the Livox HAP LiDAR used in this evaluation [<xref rid="B40-sensors-25-05701" ref-type="bibr">40</xref>].</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Laser Wavelength</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">905 nm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Laser Safety</td><td align="center" valign="middle" rowspan="1" colspan="1">Class 1 (IEC 60825-1:2014 [<xref rid="B41-sensors-25-05701" ref-type="bibr">41</xref>])</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Detection Range (100 klx)</td><td align="center" valign="middle" rowspan="1" colspan="1">150 m @ 10% reflectivity</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FOV</td><td align="center" valign="middle" rowspan="1" colspan="1">120&#176; (Horizontal) &#215; 25&#176; (Vertical)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Distance Random Error (1&#963; @ 20 m)</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;2 cm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Angular Random Error (1&#963;)</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.1&#176;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Beam Divergence</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28&#176; (Vertical) &#215; 0.03&#176; (Horizontal)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Angular Resolution @ ROI</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23&#176; (Vertical) &#215; 0.18&#176; (Horizontal)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Point Rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">452,000 points/s (first or strongest return)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05701-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05701-t002_Table 2</object-id><label>Table 2</label><caption><p>Evaluation score and error results for 12 settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Case</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Orientation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Score</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold">Error</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> [m]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold">Error</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> [m]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold">Error</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> [rad]</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1-A</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.012</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1-B</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.017</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1-C</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">1.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.006</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1-D</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.003</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2-A</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">1.06</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.011</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2-B</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.004</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2-C</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">1.11</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.019</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2-D</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3-A</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">1.03</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.015</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3-B</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.006</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3-C</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3-D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;0.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.006</td></tr></tbody></table><table-wrap-foot><fn><p><inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent translational parameters along the <italic toggle="yes">x</italic>-axis and <italic toggle="yes">y</italic>-axis, respectively, and <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the yaw angle. In &#8220;Orientation&#8221; column, &#8220;&#10003;&#8221; denotes a correct estimation, while &#8220;&#215;&#8221; denotes an incorrect estimation.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05701-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05701-t003_Table 3</object-id><label>Table 3</label><caption><p>Size categories of dump truck and their length and vessel height.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Size Category</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Length [m]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Vessel Height [m]</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" colspan="1">small</td><td align="center" valign="middle" rowspan="1" colspan="1">Small A</td><td align="center" valign="middle" rowspan="1" colspan="1">7.80</td><td align="center" valign="middle" rowspan="1" colspan="1">2.52</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Small B</td><td align="center" valign="middle" rowspan="1" colspan="1">7.86</td><td align="center" valign="middle" rowspan="1" colspan="1">2.21</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">7.86</td><td align="center" valign="middle" rowspan="1" colspan="1">3.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">large</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.10</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05701-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05701-t004_Table 4</object-id><label>Table 4</label><caption><p>Confusion matrix of evaluation score before incorporating negative point cloud.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Template</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Size Classification</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small A</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Input</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">small</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small A</td><td align="center" valign="middle" rowspan="1" colspan="1">1.18</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.21</bold> *</td><td align="center" valign="middle" rowspan="1" colspan="1">1.02</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small B</td><td align="center" valign="middle" rowspan="1" colspan="1">1.06</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.33</bold> *</td><td align="center" valign="middle" rowspan="1" colspan="1">1.15</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">medium</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">1.03</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.34</bold> *</td><td align="center" valign="middle" rowspan="1" colspan="1">1.04</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">large</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>1.18</bold> *</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td></tr></tbody></table><table-wrap-foot><fn><p>Highest score for each observed point cloud is highlighted in bold and marked with an asterisk. In &#8220;Size Classification&#8221; column, &#8220;&#10003;&#8221; denotes a correct estimation, while &#8220;&#215;&#8221; denotes an incorrect estimation.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05701-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05701-t005_Table 5</object-id><label>Table 5</label><caption><p>Confusion matrix of evaluation score after incorporating negative point cloud.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Template</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Size Classification</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small A</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Input</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">small</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small A</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.17 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">0.52</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small B</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.00 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79</td><td align="center" valign="middle" rowspan="1" colspan="1">0.52</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">medium</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">1.02</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.33 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">large</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.14 *</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td></tr></tbody></table><table-wrap-foot><fn><p>Highest score for each observed point cloud is highlighted in bold and marked with an asterisk. In &#8220;Size Classification&#8221; column, &#8220;&#10003;&#8221; denotes a correct estimation.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05701-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05701-t006_Table 6</object-id><label>Table 6</label><caption><p>Computation time of proposed method when Small A dump truck is observed.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Template</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small A</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Process</td><td align="center" valign="middle" rowspan="1" colspan="1">Preprocess</td><td align="center" valign="middle" rowspan="1" colspan="1">0.009 [s]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.010 [s]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.009 [s]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">NDT-based pose estimation</td><td align="center" valign="middle" rowspan="1" colspan="1">0.088 [s]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.090 [s]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.088 [s]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Size classification <break/> with negative point clouds</td><td align="center" valign="middle" rowspan="1" colspan="1">0.034 [s]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.032 [s]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.026 [s]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.131 [s]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.132 [s]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.123 [s]</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>