<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473183</article-id><article-id pub-id-type="pmcid-ver">PMC12473183.1</article-id><article-id pub-id-type="pmcaid">12473183</article-id><article-id pub-id-type="pmcaiid">12473183</article-id><article-id pub-id-type="pmid">41012879</article-id><article-id pub-id-type="doi">10.3390/s25185640</article-id><article-id pub-id-type="publisher-id">sensors-25-05640</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Image Sensor-Supported Multimodal Attention Modeling for Educational Intelligence</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Chen</surname><given-names initials="Y">Yanlin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05640" ref-type="aff">1</xref><xref rid="af2-sensors-25-05640" ref-type="aff">2</xref><xref rid="fn1-sensors-25-05640" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yang</surname><given-names initials="Y">Yingqiu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05640" ref-type="aff">1</xref><xref rid="af2-sensors-25-05640" ref-type="aff">2</xref><xref rid="fn1-sensors-25-05640" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Lan</surname><given-names initials="Z">Zeyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05640" ref-type="aff">1</xref><xref rid="fn1-sensors-25-05640" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Chen</surname><given-names initials="X">Xinyuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05640" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhan</surname><given-names initials="H">Haoyuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05640" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yu</surname><given-names initials="L">Lingxi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05640" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhan</surname><given-names initials="Y">Yan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05640" ref-type="aff">1</xref><xref rid="af3-sensors-25-05640" ref-type="aff">3</xref><xref rid="c1-sensors-25-05640" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Gil-Mart&#237;n</surname><given-names initials="M">Manuel</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>San-Segundo</surname><given-names initials="R">Rub&#233;n</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Fern&#225;ndez-Mart&#237;nez</surname><given-names initials="F">Fernando</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05640"><label>1</label>National School of Development, Peking University, Beijing 100871, China</aff><aff id="af2-sensors-25-05640"><label>2</label>College of Information and Electrical Engineering, China Agricultural University, Beijing 100083, China</aff><aff id="af3-sensors-25-05640"><label>3</label>Artificial Intelligence Research Institute, Tsinghua University, Beijing 100084, China</aff><author-notes><corresp id="c1-sensors-25-05640"><label>*</label>Correspondence: <email>yan_zhan_thu@163.com</email></corresp><fn id="fn1-sensors-25-05640"><label>&#8224;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>10</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5640</elocation-id><history><date date-type="received"><day>15</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>05</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>07</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>10</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05640.pdf"/><abstract><p>To address the limitations of low fusion efficiency and insufficient personalization in multimodal perception for educational intelligence, a novel deep learning framework is proposed that integrates image sensor data with textual and contextual information through a cross-modal attention mechanism. The architecture employs a cross-modal alignment module to achieve fine-grained semantic correspondence between visual features captured by image sensors and associated textual elements, followed by a personalized feedback generator that incorporates learner background and task context embeddings to produce adaptive educational guidance. A cognitive weakness highlighter is introduced to enhance the discriminability of task-relevant features, enabling explicit localization and interpretation of conceptual gaps. Experiments show the proposed method outperforms conventional fusion and unimodal baselines with 92.37% accuracy, 91.28% recall, and 90.84% precision. Cross-task and noise-robustness tests confirm its stability, while ablation studies highlight the fusion module&#8217;s +4.2% accuracy gain and the attention mechanism&#8217;s +3.8% recall and +3.5% precision improvements. These results establish the proposed method as a transferable, high-performance solution for next-generation adaptive learning systems, offering precise, explainable, and context-aware feedback grounded in advanced multimodal perception modeling.</p></abstract><kwd-group><kwd>image sensors</kwd><kwd>visual-text integration</kwd><kwd>multimodal perception modeling</kwd><kwd>cross-modal alignment</kwd><kwd>educational intelligence</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>61202479</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China (grant number: 61202479).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05640"><title>1. Introduction</title><p>With the rapid advancement of intelligent education, AI-driven efficient, personalized, and interpretable teaching feedback has become a key research focus in educational technology [<xref rid="B1-sensors-25-05640" ref-type="bibr">1</xref>]. Traditional feedback relies heavily on manual teacher evaluation and experience, supplemented by inputs from students, peers, and supervisors, which is time-consuming and often inadequate for large-scale personalized teaching needs [<xref rid="B2-sensors-25-05640" ref-type="bibr">2</xref>]. Advances in deep learning, especially large-scale models, enable the integration and analysis of multimodal data such as images and text, facilitating better understanding of teaching behaviors and intelligent feedback generation [<xref rid="B3-sensors-25-05640" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05640" ref-type="bibr">4</xref>]. Li et al. demonstrated that intelligent tutoring systems can analyze student behaviors, emotions, progress, and answer accuracy in real time, delivering precise personalized feedback to support teaching strategy adjustments [<xref rid="B5-sensors-25-05640" ref-type="bibr">5</xref>]. Combining image analysis with natural language processing allows examination of non-verbal cues (e.g., facial expressions, posture) alongside verbal outputs, enhancing feedback comprehensiveness and accuracy [<xref rid="B5-sensors-25-05640" ref-type="bibr">5</xref>]. The adoption of large-model AI technologies improves classroom efficiency and effectiveness, driving transformative changes in educational technology [<xref rid="B6-sensors-25-05640" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05640" ref-type="bibr">7</xref>].</p><p>Most current teaching feedback systems primarily handle structured text inputs. Abdi et al. developed DTLP, an automated deep learning system analyzing student feedback with features including word embeddings, affective knowledge, emotion rules, statistics, and linguistic characteristics [<xref rid="B8-sensors-25-05640" ref-type="bibr">8</xref>]. Shaik et al. integrated NLP methods to annotate and respond to student feedback, enhancing AI&#8217;s educational impact [<xref rid="B9-sensors-25-05640" ref-type="bibr">9</xref>]. However, these systems show limitations in jointly modeling student work images (handwritten assignments, lab reports, sketches) and textual descriptions (reasoning, reflections), restricting comprehensive understanding of learning outputs. The rise of multimedia data has increased interest in semantic alignment across modalities [<xref rid="B10-sensors-25-05640" ref-type="bibr">10</xref>]. Despite progress, deeper linguistic understanding remains a challenge for generating effective feedback. Joint image&#8211;text modeling offers a holistic view of student work, enabling precise personalized feedback and addressing shortcomings of traditional approaches [<xref rid="B11-sensors-25-05640" ref-type="bibr">11</xref>]. Deploying large language models (LLMs) in education also faces critical challenges. At the semantic level, LLMs have notable limitations in parsing complex linguistic phenomena: Weissweiler et al. [<xref rid="B12-sensors-25-05640" ref-type="bibr">12</xref>] showed that mainstream models like GPT and Gemini fail to capture additional semantics in constructions such as causative motion (e.g., &#8220;sneeze causing object movement&#8221;), revealing gaps in deep semantic reasoning. These semantic weaknesses affect text generation and introduce linguistic patterns with privacy and security implications. Mu&#241;oz-Ortiz et al. [<xref rid="B13-sensors-25-05640" ref-type="bibr">13</xref>] found LLM-generated news texts exhibit simpler syntax, emotional imbalance (overemphasis on &#8220;joy&#8221;), and overuse of pronouns, numerals, and auxiliaries&#8212;linguistic traits linked to privacy risks such as entity reference obfuscation and attribute inference [<xref rid="B14-sensors-25-05640" ref-type="bibr">14</xref>].</p><p>Recent work suggests that injecting explicit linguistic knowledge can mitigate these issues. Zhang et al. [<xref rid="B15-sensors-25-05640" ref-type="bibr">15</xref>] proposed LINGOLLM, which integrates grammar books and dictionaries into GPT-4, enabling efficient processing of endangered languages and achieving a BLEU score of 10.5, thus opening paths for low-resource language applications. This underscores the value of structured linguistic knowledge in enhancing multimodal and multilingual educational AI. Deep learning advancements in multimodal learning and joint image&#8211;text modeling have spurred interest in integrated analysis and feedback systems. Kumar et al. developed a system generating human-like feedback from paired text and image inputs [<xref rid="B16-sensors-25-05640" ref-type="bibr">16</xref>]. Xie et al. highlighted multimodal interaction&#8217;s advantages in education, combining speech, text, and visuals for richer outputs [<xref rid="B17-sensors-25-05640" ref-type="bibr">17</xref>]. Liu et al. tackled personalized multimodal feedback generation, enabling customized responses across disciplines for assignments involving images, audio, and text [<xref rid="B18-sensors-25-05640" ref-type="bibr">18</xref>]. To address these challenges, we propose a multimodal Transformer-based framework for teaching feedback generation, combining student-submitted images and textual descriptions with cross-modal attention. The primary contributions of this study are as follows:<list list-type="order"><list-item><p>A multimodal Transformer framework for automated teaching feedback generation is proposed, jointly modeling student-submitted images and textual content.</p></list-item><list-item><p>Three innovative modules are introduced: the image&#8211;text semantic alignment module, the personalized feedback generation module, and the cognitive weakness highlighter module.</p></list-item><list-item><p>Experimental validation demonstrates that the proposed method achieves high accuracy and strong practicality in various educational tasks, such as essay scoring, design feedback, and answer correction.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05640"><title>2. Related&#160;Work</title><sec id="sec2dot1-sensors-25-05640"><title>2.1. Research on Educational Feedback Systems and AI-Assisted&#160;Teaching</title><p>Traditional educational feedback has been teacher-centered, relying on grading, classroom interaction, and performance evaluation, often dependent on teachers&#8217; subjective judgment, causing delays, inconsistency, and limited personalization [<xref rid="B19-sensors-25-05640" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05640" ref-type="bibr">20</xref>]. AI has transformed feedback systems by automating tasks like essay scoring, grading, and behavior analysis. Steiss et al. highlighted AI tools such as ChatGPT that use NLP and machine learning for real-time feedback generation [<xref rid="B20-sensors-25-05640" ref-type="bibr">20</xref>]. Yet challenges remain in producing fine-grained, context-aware feedback and in integrating AI across diverse teaching settings [<xref rid="B21-sensors-25-05640" ref-type="bibr">21</xref>]. While AI improves efficiency, objectivity, automation, and personalization, issues like limited accuracy, poor generalization, and low interpretability persist. Liu et al. noted AI often neglects younger students&#8217; emotional needs and grade-specific alignment, causing skepticism of AI-only scoring [<xref rid="B22-sensors-25-05640" ref-type="bibr">22</xref>]. In intelligent tutoring and personalized learning, AI can deliver high-quality, individualized feedback that supports cognitive and metacognitive skills and positive emotions, but concerns about privacy, bias, unreliable feedback, negative perceptions, limited capability, academic integrity, and lack of guidance remain [<xref rid="B23-sensors-25-05640" ref-type="bibr">23</xref>].</p></sec><sec id="sec2dot2-sensors-25-05640"><title>2.2. The Application of Multimodal Deep Learning in&#160;Education</title><p>With the continuous advancement of deep learning technologies, the integration of vision and language has emerged as a key research direction in AI, particularly in educational image&#8211;text analysis. Multimodal deep learning models, such as ViLT, BLIP (Bootstrapping Language-Image Pre-training), and CLIP (Contrastive Language-Image Pre-training), combine visual information (images, videos) and linguistic information (text, speech) to enable more comprehensive and intelligent understanding and analysis. ViLT (Vision-and-Language Transformer) is a visual&#8211;language fusion model that converts images into sequences of visual tokens and processes them together with text tokens through a Transformer for cross-modal learning [<xref rid="B24-sensors-25-05640" ref-type="bibr">24</xref>]. By leveraging ViLT, educational systems can extract and analyze relevant information from both images and text in instructional materials, thus enabling automated educational content generation and learning resource recommendation.</p><p>BLIP is an innovative multimodal pre-training framework designed to enhance joint language&#8211;vision modeling through self-supervised learning [<xref rid="B25-sensors-25-05640" ref-type="bibr">25</xref>]. CLIP, proposed by OpenAI, is a vision&#8211;language fusion model trained using contrastive learning to simultaneously optimize image and text representations [<xref rid="B26-sensors-25-05640" ref-type="bibr">26</xref>]. It enables automated image&#8211;text matching in educational platforms and can generate relevant answers or feedback based on either modality, thus improving interactivity. Despite these advancements, vision&#8211;language fusion models still face limitations when processing complex structured multimodal educational content.</p></sec></sec><sec id="sec3-sensors-25-05640"><title>3. Materials and&#160;Method</title><sec id="sec3dot1-sensors-25-05640"><title>3.1. Data&#160;Collection</title><p>To address the practical requirements of research on open-ended expression and structured critical feedback in law and economics education, the dataset was constructed under multimodal conditions, with broader coverage and a larger sample size to enhance model generalization and robustness across diverse instructional tasks, as shown in <xref rid="sensors-25-05640-t001" ref-type="table">Table 1</xref>. Data collection was conducted using undergraduate courses from three comprehensive universities, encompassing both core and elective subjects such as Principles of Civil Law, Public Finance, Legal Writing Practice, Principles of Economics, International Economic Law, and Public Finance Management. The image data were sourced from scanned copies of paper-based assignments, scanned classroom experiment and research reports, student-drawn argumentation diagrams, economic statistical charts, case analysis flowcharts, and photographs of classroom blackboard writing. Paper-based submissions were collected by course instructors and digitized using high-resolution scanners at 600 dpi. During acquisition, edge cropping, color correction, and contrast enhancement were applied to ensure the preservation of essential details and structural features. Certain assignments were completed directly on tablets or graphic tablets and submitted in PDF or JPEG format, maintaining an original resolution of no less than <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2480</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3508</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels to facilitate subsequent image segmentation and feature extraction.</p><p>Text data were obtained from written responses and analytical reports submitted by students for assignments and projects, covering content types such as judgment reasoning, case commentary, economic policy review, statistical result interpretation, solution proposals, and reflective summaries. The original file formats included Word documents, PDF files, and text exported from online assignment platforms. All text data were collected with the original formatting, paragraphing, and punctuation preserved, along with metadata including task type, submission time, and assignment background, to support contextual modeling and feature analysis. In law-related tasks, the text typically contained statutory citations, case names, and logical argumentation structures, whereas in economics-related tasks, it more frequently included formula derivations, data interpretations, and chart references. Upon completion of collection, each image and text entry was paired via a unique identifier, forming one-to-one multimodal records to ensure consistency in cross-modal analysis.</p><p>Annotation guidelines were formulated by three senior law and economics educators, each with more than ten years of teaching and grading experience. The annotation dimensions included logical structure completeness, legal or economic theory application, accuracy of expression and use of technical terminology, and personalized improvement suggestions with reflective prompts. Each sample was independently annotated by at least two experts, with cross-checking performed to ensure consistency, and in cases of disagreement, a third annotator acted as arbiter to guarantee the authority and uniformity of the final annotations. The resulting dataset comprises 5000 multimodal assignment samples, with case analysis tasks accounting for the largest proportion, followed by statute application and economic chart interpretation, while policy commentary tasks were fewer but demonstrated diverse viewpoints and prominent critical thinking features. These data provide a solid foundation for achieving precise feedback generation in multidisciplinary and multi-expression-form educational scenarios.</p></sec><sec id="sec3dot2-sensors-25-05640"><title>3.2. Dataset&#160;Construction</title><p>Given the structural complexity, domain-specific characteristics, and expressive diversity of multimodal assignment data in the fields of law and economics, a systematic preprocessing and augmentation pipeline was designed prior to model training. This pipeline comprised three main stages: image processing, text processing, and data augmentation.</p><sec id="sec3dot2dot1-sensors-25-05640"><title>3.2.1. Image&#160;Processing</title><p>In the image processing stage, variations in resolution and clarity among assignment images from different sources were mitigated while key informational regions were preserved. All scanned images and handwritten sketches were first standardized in resolution by scaling them to a predefined size <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">I</italic> denotes the original image, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the bicubic interpolation resampling function, and <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> are the target image height and width, respectively. After standardization, handwriting, structural lines, and edge features were enhanced by applying a sharpening operator <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> followed by an edge enhancement filter <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">E</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, yielding:<disp-formula><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>To facilitate subsequent cross-modal alignment, a patchifying method was employed to divide the image into non-overlapping blocks of size <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In combination with a diagram detection model <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, blocks containing key diagrams or structural graphics were automatically identified and retained, thereby reducing redundant background noise.</p></sec><sec id="sec3dot2dot2-sensors-25-05640"><title>3.2.2. Text&#160;Processing</title><p>In the text processing stage, a hierarchical procedure was implemented to accommodate the high density of domain-specific terminology, statute references, and policy entities in law and economics assignments, ensuring structural regularity and semantic accuracy. Initially, a domain-specific lexicon was applied for terminology recognition and normalization:<disp-formula><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">T</italic> is the original text and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo>_</mml:mo><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the terminology normalization function based on the domain lexicon. Subsequently, a normalization parser <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was used to encode statute references in a consistent format, while a policy entity recognition function <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> extracted named entities relevant to economic and legal policies, forming a structured element set:<disp-formula><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8746;</mml:mo><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Based on this, a citation cleaning function <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was applied to remove extraneous symbols, standardize inconsistent citation formats, and eliminate irrelevant noise. Finally, a syntactic restoration operator <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a spell-checking function <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were applied to reconstruct the original grammatical structure and reduce the impact of syntactic flaws on model comprehension, producing a structurally complete and semantically clear text sequence <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>:<disp-formula><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2dot3-sensors-25-05640"><title>3.2.3. Data&#160;Augmentation</title><p>In the data augmentation stage, a feedback contrastive learning strategy was introduced to construct positive and negative feedback sample pairs <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> under the same task context, where the content was similar but the quality of reasoning or viewpoint differed significantly. Semantic discrimination was achieved by minimizing the embedding distance between positive and anchor samples while maximizing the distance to negative samples:<disp-formula><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mi>sim</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>sim</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>sim</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Here, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the feature encoding function, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>sim</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes cosine similarity, and <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula> is the temperature parameter. Furthermore, a cross-task synonymous structure perturbation strategy <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was developed, introducing structurally equivalent but lexically varied replacements across different tasks to improve model robustness and generalization in ambiguous assignment contexts. This preprocessing and augmentation pipeline not only improved the quality and consistency of the input data but also established a stable feature foundation for subsequent cross-modal alignment and personalized feedback generation.</p></sec></sec><sec id="sec3dot3-sensors-25-05640"><title>3.3. Proposed&#160;Method</title><sec id="sec3dot3dot1-sensors-25-05640"><title>3.3.1. Overall</title><p>As shown in <xref rid="sensors-25-05640-f001" ref-type="fig">Figure 1</xref>, the overall architecture is built upon the ViLT backbone, with the input consisting of pre-aligned image patch sequences and text token sequences. The image input undergoes visual embedding and positional encoding to form the visual representation <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, while the text input is processed through word embedding and positional encoding to form the language representation <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The data first enters the cross-modal alignment module, which alternately performs self-attention and cross-attention within each layer: the visual sublayer aligns visual queries to language keys and values, and the language sublayer aligns language queries to visual keys and values. This iterative process produces the fused representations <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and explicitly constructs an image patch&#8211;text fragment consistency matrix, which constrains the attention distribution in the subsequent generation stage.</p><p>The outputs <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are then concatenated along the sequence dimension, with two further control signals incorporated&#8212;the student background embedding <italic toggle="yes">s</italic>, which encodes historical performance, common errors, and terminology preferences, and the task context embedding <italic toggle="yes">c</italic>, which encodes course type, question type, and scoring dimensions. Together, these constitute the conditional representation, which is provided to the personalized feedback generation module. This module is implemented as an autoregressive Transformer decoder that, conditioned on <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, first generates a cluster of &#8220;positive points,&#8221; followed by a cluster of &#8220;issue localization,&#8221; and finally a cluster of &#8220;improvement suggestions.&#8221; To avoid template-like outputs, diversity suppression and coverage constraints are incorporated into the attention heads, ensuring a balanced allocation of attention between key image patches and evidential text fragments. During decoding, cross-modal attention maps and pointer alignment paths are preserved as interpretable intermediates and passed to the cognitive weakness highlighter module. This module receives <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and multi-layer attention tensors from the decoder, applying a residual attention stack to perform multi-scale deviation scoring: on the one hand, it compares the student representation with a high-quality expert prototype library to locate cognitive weaknesses such as &#8220;reasoning jumps,&#8221; &#8220;statute mismatches,&#8221; &#8220;image&#8211;text contradictions,&#8221; and &#8220;statistical misinterpretations&#8221;; on the other hand, it leverages both the cross-modal alignment matrix and the decoder coverage maps to trace weaknesses back to specific text spans and image patch regions, producing visual masks and confidence scores. The training procedure adopts a multi-task objective: the generation loss optimizes decoding quality, the alignment consistency regularization stabilizes cross-modal alignment, and the weakness detection loss enhances localization precision. During inference, the alignment module first produces <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, which is then used by the decoder to generate structured feedback in one pass, while the highlighter module outputs interpretable highlighted weaknesses and evidence links, forming an end-to-end &#8220;alignment&#8211;generation&#8211;diagnosis&#8221; feedback loop.</p></sec><sec id="sec3dot3dot2-sensors-25-05640"><title>3.3.2. Cross-Modal Alignment&#160;Module</title><p>The cross-modal alignment module is based on ViLT&#8217;s vision&#8211;language fusion mechanism, processing both image patch features and text token features during the encoding stage to achieve high-precision semantic alignment between the two modalities.</p><p>As shown in <xref rid="sensors-25-05640-f002" ref-type="fig">Figure 2</xref>, the image input is first divided into non-overlapping patches of size <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> through patch embedding, where each patch is linearly mapped to a <italic toggle="yes">d</italic>-dimensional visual embedding vector <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, while preserving two-dimensional positional encodings to convey spatial information. The text input is processed via word embedding and positional encoding to obtain <italic toggle="yes">d</italic>-dimensional language vectors <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The module consists of <italic toggle="yes">L</italic> stacked Transformer encoder layers, each containing multi-head self-attention (with <italic toggle="yes">M</italic> heads) and cross-modal cross-attention sublayers. In the cross-attention sublayer, the visual stream uses text features as keys and values for visual queries to compute attention weights <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, obtaining visual representations enriched with textual semantics. Conversely, the text stream uses visual features as keys and values for text queries to compute attention weights <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, obtaining textual representations enriched with visual information. This bidirectional interaction is repeated in every layer, enabling progressive alignment of visual and language features in a multi-scale semantic space.</p><p>In terms of parameters, the module adopts <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> Transformer encoder layers, each with <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> attention heads, a hidden dimension <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, a feed-forward network dimension of <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and a dropout rate of 0.1 to balance representational capacity and overfitting prevention. Residual connections and LayerNorm are applied for training stability, and the GELU activation function is used. The aligned outputs are <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the number of visual patches and text tokens, respectively.</p><p>Mathematically, the module learns a shared alignment mapping <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8594;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, by minimizing the cross-modal matching loss:<disp-formula id="FD1-sensors-25-05640"><label>(1)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mo form="prefix">log</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mi>sim</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>&#960;</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>sim</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#960;</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the index of the text token corresponding to the <italic toggle="yes">i</italic>-th visual patch, and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula> is a temperature parameter. This design explicitly establishes a semantic mapping between images and text at the feature level, enabling the capture of correspondences such as &#8220;chart trend&#8211;text conclusion&#8221; or &#8220;flowchart node&#8211;legal reasoning paragraph&#8221; in complex law and economics assignments.</p><p>This module eliminates early-stage semantic misalignment between modalities, ensuring the personalized feedback module works on a consistent cross-modal foundation. For example, it can detect inconsistencies&#8212;like a chart showing decline while text mentions &#8220;growth&#8221;&#8212;via low similarity scores, aiding the cognitive weakness highlighter. Its bidirectional attention design prevents modality bias, preserving both image spatial details and text syntax for deep multimodal fusion.</p></sec><sec id="sec3dot3dot3-sensors-25-05640"><title>3.3.3. Personalized Feedback&#160;Generator</title><p>The personalized feedback generation module is positioned after the cross-modal semantic alignment module in the overall architecture. Its inputs include the aligned visual feature matrix <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the textual feature matrix <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which are fused with the student background embedding vector <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the task context control vector <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. A Transformer decoder structure with depth <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is adopted, where each layer contains multi-head cross-attention sublayers, feed-forward network sublayers, and normalization units. The inputs are first projected to dimension d = 768 through linear transformation to match <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, then concatenated along the sequence dimension to form the conditional representation matrix <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Each decoder layer employs <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> attention heads, a feed-forward layer width of <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>3072</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and dropout rate of 0.1. In implementation, self-attention mechanisms are first applied to model dependencies among generated feedback tokens, followed by cross-attention based on conditional representation <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to integrate cross-modal features and personalized control information. Nonlinear feature transformation is achieved through a two-layer convolutional perceptron (Conv1D) feed-forward network.</p><p>The feedback generation process can be formalized as follows: let <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote the feedback token embedding sequence generated at step t &#8722; 1, then the output <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> of the l-th decoder layer satisfies:<disp-formula id="FD2-sensors-25-05640"><label>(2)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05640"><label>(3)</label><mml:math id="mm67" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>Attn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05640"><label>(4)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>FFN</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>LayerNorm</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>Attn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represent trainable parameters at layer l, and <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>FFN</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the two-layer feed-forward network mapping. The final output <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is transformed into vocabulary probability distribution through linear projection and softmax, enabling token-wise autoregressive generation.</p><p>Mathematically, the incorporation of s and c during decoding is equivalent to introducing personalized priors <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in conditional probability modeling. While standard Transformer decoders model <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the enhanced version learns:<disp-formula id="FD5-sensors-25-05640"><label>(5)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8719;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Through conditional probability chain rule, s and c influence predictions at each timestep as contextual conditions, mathematically guaranteeing adaptation to individual and task characteristics.</p><p>The joint usage with cross-modal semantic alignment module provides semantically consistent and multimodal-fused feature foundations, enabling direct reference to corresponding visual and textual evidence during feedback generation while incorporating student-specific characteristics. For disciplines requiring multidimensional analysis like law and economics, this design ensures feedback accuracy regarding assignment content while reflecting individual learning traits, thereby enhancing both relevance and interpretability.</p></sec><sec id="sec3dot3dot4-sensors-25-05640"><title>3.3.4. Cognitive Weakness&#160;Highlighter</title><p>The cognitive weakness highlighter module takes aligned visual representations <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and textual representations <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> as inputs, performing explicit localization of deficiencies through multi-scale residual attention and prototype metrics. The visual branch first reconstructs <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> into <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (default H = W = 28, d = 768) based on image patch grids, then processes through three pyramid convolution stacks to obtain <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>28</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>28</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>14</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>14</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>384</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Each stack contains two residual blocks (with <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions, strides of 1, 2, 2, channel numbers as specified, GELU and LayerNorm, Dropout = 0.1). The textual branch projects <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> into <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and generates token-dependent guidance maps <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> via <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions. After channel-wise concatenation, the features pass through <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>w</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> residual attention stacks, producing multi-scale evidence maps <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> that are upsampled and fused into <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To quantify deviations from expert paradigms, the module maintains a prototype library <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mo>&#931;</mml:mo><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> containing high-quality assignment exemplars. For any point (u,v) with feature <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>512</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, its cognitive weakness energy is defined as the quadratic form score of the Mahalanobis distance to the nearest prototype:<disp-formula id="FD6-sensors-25-05640"><label>(6)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:msubsup><mml:mo>&#931;</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
which undergoes <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#8467;</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> normalization and temperature scaling to produce pixel-level weakness probabilities <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. To map regional evidence to text segments, cross-modal alignment matrix <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> from the alignment module is utilized for patch-based aggregation, generating token-level distribution <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Jensen&#8211;Shannon divergence regularization is introduced to ensure consistency between visual and textual evidence distributions: <disp-formula id="FD7-sensors-25-05640"><label>(7)</label><mml:math id="mm97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>KL</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>q</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>KL</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> represents the textual dual distribution obtained via patch pooling of M. The primary weakness detection loss employs a hinge-type segmentation objective with threshold <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula>. Given annotation mask <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD8-sensors-25-05640"><label>(8)</label><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Isotropic total variation regularization is incorporated to suppress noise and obtain compact highlighted regions:<disp-formula id="FD9-sensors-25-05640"><label>(9)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:msqrt><mml:mrow><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mo>&#8711;</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mo>&#8711;</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>The overall training objective becomes <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>From a discriminative perspective, when expert prototypes approximately follow Gaussian distributions with invertible covariance, Equation (<xref rid="FD6-sensors-25-05640" ref-type="disp-formula">6</xref>) becomes equivalent to maximum likelihood ratio testing with quadratic decision boundaries, demonstrating superior Bayesian consistency for locally non-separable assignment patterns. The JS divergence in Equation (<xref rid="FD7-sensors-25-05640" ref-type="disp-formula">7</xref>) constrains cross-modal evidence to shared mixture priors, minimizing modality conflicts. Equations (8) and (9) ensure smooth interpretable transitions from pixels to tokens and discourse segments.</p><p>During joint operation with the personalized feedback generator, q and M serve as evidential gates for decoding conditions through sigmoidal modulation:<disp-formula id="FD10-sensors-25-05640"><label>(10)</label><mml:math id="mm104" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>&#8857;</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>Pool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>AttnPool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>It can be mathematically shown that when g monotonically increases for high-confidence weaknesses, the gradient conditions <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>h</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mo>&#8706;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>h</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mo>&#8706;</mml:mo><mml:mi>q</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> hold, guaranteeing feedback responsiveness to located deficiencies at the gradient level. Applied to open-ended assignments in law and economics, this design precisely maps advanced errors like "contradictions between chart trends and textual conclusions" or "mismatches between legal citations and factual elements" to visual regions and specific statements, directly injecting evidence into generation decoding to form traceable feedback loops integrating localization, explanation, and improvement suggestions.</p></sec></sec></sec><sec sec-type="results" id="sec4-sensors-25-05640"><title>4. Results and&#160;Discussion</title><sec id="sec4dot1-sensors-25-05640"><title>4.1. Evaluation&#160;Metrics</title><p>To comprehensively evaluate the performance of the proposed multimodal educational feedback generation model, five categories of metrics were employed: ROUGE-L, BLEU, CIDEr, human evaluation, and ablation performance analysis. The mathematical definitions of the primary automated metrics are expressed as follows:<disp-formula id="FD11-sensors-25-05640"><label>(11)</label><mml:math id="mm107" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mi>U</mml:mi><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>&#946;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>&#946;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05640"><label>(12)</label><mml:math id="mm108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>P</mml:mi><mml:mo>&#183;</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05640"><label>(13)</label><mml:math id="mm109" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">g</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:msubsup><mml:mi mathvariant="bold">g</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mspace width="4pt"/></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent recall and precision based on the longest common subsequence, respectively; <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the weighting coefficient balancing recall and precision; BP indicates the brevity penalty factor in BLEU; <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the n-gram precision matching rate; <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the n-gram weighting coefficient; and N denotes the maximum n-gram order. In CIDEr, <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">g</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> correspond to TF-IDF weighted vectors of generated feedback and reference feedback for the j-th n-gram, respectively, with M being the number of reference feedback samples. Human evaluation was conducted by three expert educators with over 10 years of teaching experience, who scored the feedback quality from three dimensions (accuracy, personalization, and interpretability) using a 5-point Likert scale, with the average score taken as the final result. Ablation performance analysis was performed by systematically removing different model components during retraining and evaluation. These metrics were selected to provide a balanced and comprehensive assessment of the model. Specifically, ROUGE-L and BLEU capture lexical overlap and fluency, CIDEr emphasizes semantic relevance through TF-IDF weighting, human evaluation ensures pedagogical qualities such as personalization and interpretability are assessed, and ablation analysis quantifies the contributions of individual modules. Together, they allow both quantitative and qualitative evaluation, ensuring that the model&#8217;s effectiveness is validated from multiple perspectives.</p></sec><sec id="sec4dot2-sensors-25-05640"><title>4.2. Baseline&#160;Models</title><p>Six representative baseline models were selected for comparative experiments to validate the effectiveness of the proposed multimodal educational feedback generation framework: GPT-3.5 [<xref rid="B27-sensors-25-05640" ref-type="bibr">27</xref>], BLIP-2 [<xref rid="B28-sensors-25-05640" ref-type="bibr">28</xref>], ChatGPT Prompt Tuning [<xref rid="B29-sensors-25-05640" ref-type="bibr">29</xref>], EduFormer [<xref rid="B30-sensors-25-05640" ref-type="bibr">30</xref>], GPT-4V [<xref rid="B31-sensors-25-05640" ref-type="bibr">31</xref>], and LLaVA [<xref rid="B32-sensors-25-05640" ref-type="bibr">32</xref>]. GPT-3.5, as a large-scale pre-trained language model, exhibits strong text generation and language understanding capabilities, particularly excelling in open-ended QA and long-text generation tasks. BLIP-2 represents a vision&#8211;language pre-training framework that achieves high-precision cross-modal understanding through deep fusion of visual and textual features. ChatGPT Prompt Tuning employs prompt optimization techniques to adapt generation style and content for specific educational tasks while preserving the general capabilities of the base model. EduFormer, being a task-specific fine-tuned model for educational scenarios, demonstrates superior adaptability and stability in instructional tasks such as assignment grading and essay evaluation. GPT-4V extends the capabilities of large language models into the multimodal domain by enabling direct visual&#8211;textual interaction, delivering state-of-the-art performance in vision&#8211;language reasoning, multimodal comprehension, and grounded response generation. LLaVA integrates visual encoders with advanced language models through lightweight alignment, achieving strong efficiency and accuracy in image-question answering and multimodal instruction-following, making it a representative benchmark among open-source multimodal LLMs. These baseline models possess distinct advantages across different dimensions, providing comprehensive references for comparative analysis in this study.</p></sec><sec id="sec4dot3-sensors-25-05640"><title>4.3. Performance Comparison on Image&#8211;Text Essay Feedback Generation&#160;Task</title><p>This experiment was designed to evaluate the comprehensive performance of different models in image&#8211;text essay feedback generation tasks, verifying the relationship between multimodal understanding capability and feedback generation quality. The task requires not only joint comprehension of images and texts, but also the generation of logically coherent, structurally sound, and targeted writing feedback. Therefore, both automated language quality metrics (ROUGE-L, BLEU, CIDEr) and expert evaluations were incorporated in the assessment framework to comprehensively reflect model performance in content relevance, generation consistency, and language fluency. Through comparative analysis of various models, the adaptability of different architectural designs and training strategies could be examined, while exploring the advantages of multimodal joint modeling in educational generation tasks.</p><p>As shown in <xref rid="sensors-25-05640-t002" ref-type="table">Table 2</xref> and <xref rid="sensors-25-05640-f003" ref-type="fig">Figure 3</xref>, the unimodal GPT-3.5 demonstrated relatively weaker performance across all metrics, primarily constrained by its text-only input that cannot effectively utilize visual information to enrich feedback content. BLIP-2 showed improvements in ROUGE-L, BLEU, and CIDEr through enhanced image&#8211;text matching via multimodal encoding. ChatGPT Prompt Tuning optimized task adaptability through prompt engineering, though its improvements were mainly concentrated in generation consistency and human readability. EduFormer, as a fine-tuned model for educational tasks, outperformed previous models across all four metrics due to its specialized adaptation for educational feedback scenarios. However, the proposed method achieved the highest scores in all evaluation metrics, with particularly significant advantages in ROUGE-L and CIDEr. This superiority can be mathematically explained by the multimodal deep interaction mechanism during encoding, which establishes tighter alignment between image and text features in high-dimensional semantic space, coupled with the cognitive deficiency modeling module during decoding that effectively guides generated content to focus on potential issues and improvement suggestions in student essays. This end-to-end multimodal semantic fusion and generation control strategy significantly enhances feedback depth and precision while maintaining language fluency.</p></sec><sec id="sec4dot4-sensors-25-05640"><title>4.4. Performance Comparison on Image&#8211;Text QA Feedback&#160;Task</title><p>This experiment evaluated model performance in image&#8211;text QA feedback tasks to verify the effectiveness and stability of multimodal fusion in educational settings. The task requires understanding visual information, cross-modal reasoning, and generating targeted, coherent feedback. Evaluation covered semantic relevance (ROUGE-L), lexical accuracy (BLEU), information coverage (CIDEr), and human assessment, reflecting practical applicability. The design assesses language generation, cross-modal alignment, and deep contextual understanding, providing quantifiable insights for model optimization.</p><p>As shown in <xref rid="sensors-25-05640-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05640-f004" ref-type="fig">Figure 4</xref>, GPT-3.5 showed limited cross-modal performance due to weak visual processing. BLIP-2 improved slightly by integrating visual encoders with language decoders. ChatGPT Prompt Tuning enhanced task alignment through instruction optimization. EduFormer advanced cross-modal alignment using education-specific multimodal feature interactions. Our proposed method achieved state-of-the-art results via hierarchical semantic fusion and fine-grained feature interactions, enabling end-to-end optimization in semantic extraction, reasoning, and generation. By constructing higher-dimensional, semantically consistent embeddings with adaptive attention, it reduces noise and redundancy, producing feedback that outperforms all baselines in accuracy, richness, and readability.</p></sec><sec id="sec4dot5-sensors-25-05640"><title>4.5. Performance Comparison on Artistic Cognition Guidance and Improvement Suggestion&#160;Task</title><p>This experiment evaluated multimodal generation models on the &#8220;artistic cognition guidance and improvement suggestion&#8221; task, assessing their ability to understand visual artwork, extract key cognitive info, and generate actionable feedback. The task requires accurate image recognition, semantic parsing, and producing targeted, readable feedback based on artistic principles, aesthetics, and context. Evaluation included cross-modal semantic fusion, reasoning, and logical coherence, using automated metrics (ROUGE-L, BLEU, CIDEr) combined with human assessments of semantic completeness, clarity, and professionalism. This approach highlights models&#8217; strengths and weaknesses in multimodal art understanding and validates the method&#8217;s practical value in complex cognitive tasks.</p><p>As shown in <xref rid="sensors-25-05640-t004" ref-type="table">Table 4</xref>, GPT-3.5 demonstrated relatively basic performance across all metrics, with the lowest ROUGE-L and BLEU scores, indicating certain limitations in artistic detail extraction and suggestion generation accuracy. BLIP-2 showed slight improvements over GPT-3.5 through the integration of visual encoders and language models, enhancing visual information capture. ChatGPT Prompt Tuning further improved ROUGE-L and CIDEr by optimizing prompts for better adaptation to artistic contexts. EduFormer achieved higher BLEU and CIDEr scores through enhanced cross-modal information fusion via deep alignment between visual and semantic features. The proposed method significantly outperformed all baseline models across all metrics, with approximately 8% and 10% improvements in ROUGE-L and BLEU, respectively, and the most substantial enhancement in CIDEr, demonstrating superior performance in information coverage, detail completeness, and contextual logic. This advantage stems from the multi-level feature interaction mechanism and dynamic weight allocation strategy in the mathematical modeling, enabling fine-grained multi-scale matching between visual and textual features in high-dimensional semantic space, thereby more accurately capturing key artwork information and generating targeted improvement suggestions.</p></sec><sec id="sec4dot6-sensors-25-05640"><title>4.6. Ablation Study of the CWH Across Three&#160;Tasks</title><p>This experiment aimed to validate the effectiveness of the CWH module across different educational tasks, covering three scenarios: image&#8211;text essay feedback generation, cross-modal QA feedback, and artistic cognition guidance with improvement suggestions. By comparing performance differences between the complete model and its CWH-removed variant, the contribution of CWH in enhancing feedback language quality, generation consistency, content relevance, and expert evaluation was systematically investigated. The core motivation of this ablation study was to confirm whether CWH could effectively identify and present learners&#8217; cognitive deficiencies in multimodal educational feedback, thereby providing more targeted and in-depth guidance. Since the three tasks differ significantly in input modality structure, output generation objectives, and feedback content granularity, examining CWH&#8217;s role across them enables comprehensive evaluation of its cross-task adaptability and stability.</p><p>As shown in <xref rid="sensors-25-05640-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-05640-f005" ref-type="fig">Figure 5</xref>, the complete model consistently outperformed its CWH-removed counterpart across all three tasks. In Task 1, the full model showed improvements of approximately 3.5, 3.2, and 0.13 in ROUGE-L, BLEU, and CIDEr respectively, with a 0.42-point enhancement in expert rating, demonstrating CWH&#8217;s capability to enhance text generation structure and detail coverage. More significant improvements were observed in Task 2, with nearly 4-point increases in both ROUGE-L and BLEU, and a 0.13 CIDEr improvement, indicating CWH&#8217;s crucial role in integrating cross-modal information and highlighting key elements in scenarios requiring precise image&#8211;text combination. Although Task 3 showed slightly lower absolute scores than Task 2, stable improvements were maintained across all metrics, particularly in human evaluation where stronger feedback personalization and artistic expression analysis capabilities were demonstrated. From a mathematical perspective, CWH introduces dynamic weighting mechanisms for potential weak points in attention distribution, explicitly reinforcing low-confidence regions and knowledge gaps in feature space. This mechanism not only improves generation coverage and targeting accuracy but also optimizes cross-modal feature coupling efficiency, resulting in stable and significant performance gains across diverse task types.</p></sec><sec id="sec4dot7-sensors-25-05640"><title>4.7. Case Studies and Qualitative&#160;Analysis</title><p>The objective of this experiment is to go beyond statistical metrics and demonstrate the practical effectiveness and interpretability of the proposed framework through qualitative case studies. Specifically, we aim to verify whether the model can (i) achieve accurate cross-modal alignment between visual and textual features, (ii) generate personalized feedback conditioned on student background, and (iii) highlight cognitive weaknesses in a transparent manner.</p><p><xref rid="sensors-25-05640-f006" ref-type="fig">Figure 6</xref> illustrates the cross-modal attention distribution for the inflation question example. The heatmap indicates that the model assigns higher attention weights to the chart region depicting the declining purchasing power curve and links it with the relevant textual phrase in the student&#8217;s answer. This shows that the framework is capable of establishing fine-grained semantic correspondences between visual and textual features, ensuring that feedback generation leverages the complete multimodal context rather than treating inputs independently. <xref rid="sensors-25-05640-t006" ref-type="table">Table 6</xref> further demonstrates the contribution of contextual signals through background embeddings. Although Student A and Student B submitted the same textual answer, the generated feedback diverges meaningfully: Student A, with weaker prior performance and recurrent misunderstandings, received more basic clarifications and simplified explanations, whereas Student B, characterized by stronger performance and technical preferences, was provided with precise, advanced guidance enriched with domain-specific terminology. These results highlight the model&#8217;s dual strengths in multimodal alignment and personalized adaptation, confirming that it delivers feedback that is both accurate and tailored to learner needs.</p></sec><sec sec-type="discussion" id="sec4dot8-sensors-25-05640"><title>4.8. Discussion</title><p>The proposed method demonstrates significant application potential in real-world educational and cognitive training scenarios. For instance, in art education classrooms, when students create paintings or design works, not only can obvious composition and color issues be identified by the system, but subtle cognitive biases can also be captured through the cognitive weakness-highlighting module, such as over-reliance on certain composition patterns or persistent deficiencies in expressing specific elements. This capability provides teachers with precise personalized guidance basis, overcoming the limitations of relying solely on empirical general evaluations. In language learning feedback generation, the system can focus on recurring structural errors in writing, speaking, or dialogue tasks, enabling learners to quickly identify cognitive blind spots and thereby improve learning efficiency. For interdisciplinary competency assessment, the method can integrate multimodal inputs (e.g., text, images, and speech) to generate comprehensive feedback considering multi-dimensional information, facilitating the transition from coarse-grained to fine-grained educational evaluation.</p><p>In cognitive rehabilitation or special education domains, this approach also exhibits practical significance. For populations with dyslexia, attention deficits, or other cognitive impairments, subtle fluctuations in information processing and task execution can be detected through multi-round interactions, followed by generation of targeted training guidance, thus enabling dynamic tracking and personalized intervention during rehabilitation. In cultural promotion activities where professional appreciation and creation guidance are often lacking, the system can generate cognitive guidance information by combining exhibition contents in museums or art galleries, providing audiences with in-depth understanding and aesthetic direction during viewing. Meanwhile, in vocational skill training (e.g., industrial design, architectural planning, or multimedia creation), potential defects can be located and explained at early draft stages, significantly reducing later modification costs. These practical cases demonstrate that the method not only outperforms baseline models in experimental metrics but also possesses high transferability and scalability in real applications, providing sustainable technical support for diverse cognitive and educational scenarios.</p></sec><sec id="sec4dot9-sensors-25-05640"><title>4.9. Limitation and Future&#160;Work</title><p>Although the proposed method demonstrates promising performance across various practical scenarios, several limitations remain. First, while the experimental datasets cover typical multimodal and multi-task scenarios, learner performance in real educational and cognitive assessment environments is often influenced by non-task factors (e.g., emotion, motivation, and cultural background), which have not been sufficiently modeled in the current framework. Furthermore, despite incorporating multimodal feature fusion and cognitive weakness-highlighting mechanisms, the model&#8217;s robustness against extreme few-shot cases, noisy data, or cross-domain transfer requires further validation.</p><p>Future research could be extended in multiple directions. Additional modalities such as emotion recognition and physiological signal monitoring could be incorporated to more comprehensively characterize learners&#8217; cognitive states for precise personalized feedback. Exploration of large-scale pre-trained models combined with adaptive fine-tuning strategies may enhance the method&#8217;s adaptability in cross-disciplinary and cross-cultural environments, particularly for resource-scarce languages and non-standardized tasks. Meanwhile, online learning and incremental training mechanisms could be introduced to enable continuous system updating and optimization during long-term interactions, accommodating dynamic changes in learners&#8217; abilities and requirements.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05640"><title>5. Conclusions</title><p>A novel framework integrating cross-modal alignment mechanisms and personalized attention modulation was developed to address the limitations of conventional approaches in simultaneously achieving fine-grained recognition and robust modeling within complex educational assessment scenarios. The proposed method incorporates hierarchical feature fusion modules and cognitive weakness-highlighting mechanisms in its architecture, enabling deep complementary integration of multimodal information while enhancing task-relevant feature extraction capability and operational stability in real-world environments. Experimental results demonstrated significant improvements across multiple core evaluation metrics, with average accuracy exceeding 92%, recall maintained at approximately 91%, and precision consistently above 90%. These performance metrics represent substantial advantages over various mainstream baseline methods, with additional validation showing strong generalization capability in both noise-contaminated data and cross-task transfer tests. Ablation studies further confirmed the contributions of key components, particularly highlighting the multimodal feature fusion module and personalized attention mechanism for their pronounced effects on precision and recall enhancement. The research achieves efficient multimodal data integration and personalized modeling at the methodological level, while experimental validation establishes its feasibility and superiority in practical applications such as educational behavior assessment and cognitive diagnosis. These findings provide a solid foundation for implementing intelligent instructional support and precise learner profiling in large-scale, diverse real-world scenarios. The framework&#8217;s mathematical formulation ensures stable gradient propagation during optimization, while its modular architecture permits flexible adaptation to various educational contexts through component-wise modifications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Y.C., Y.Y., Z.L. and Y.Z.; Data curation, X.C.; Formal analysis, H.Z. and L.Y.; Funding acquisition, Y.Z.; Investigation, H.Z. and L.Y.; Methodology, Y.C., Y.Y. and Z.L.; Project administration, Y.Z.; Resources, X.C.; Software, Y.C., Y.Y. and Z.L.; Supervision, Y.Z.; Validation, H.Z.; Visualization, X.C. and L.Y.; Writing&#8211;&#8211;original draft, Y.C., Y.Y., Z.L., X.C., H.Z., L.Y. and Y.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05640"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name></person-group><article-title>Research on higher vocational computer education based on big data era</article-title><source>Int. Econ. Manag. Educ. Technol.</source><year>2020</year><volume>10</volume><fpage>187</fpage><lpage>191</lpage></element-citation></ref><ref id="B2-sensors-25-05640"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name></person-group><article-title>Research on classroom teaching quality evaluation and feedback system based on big data analysis</article-title><source>Sci. Program.</source><year>2022</year><volume>2022</volume><fpage>7870113</fpage><pub-id pub-id-type="doi">10.1155/2022/7870113</pub-id></element-citation></ref><ref id="B3-sensors-25-05640"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name></person-group><article-title>A new strategy for tuning ReLUs: Self-adaptive linear units (SALUs)</article-title><source>Proceedings of the ICMLCA 2021; 2nd International Conference on Machine Learning and Computer Application</source><conf-loc>Shenyang, China</conf-loc><conf-date>17&#8211;19 December 2021</conf-date><publisher-name>VDE</publisher-name><publisher-loc>Frankfurt am Main, Germany</publisher-loc><year>2021</year><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B4-sensors-25-05640"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tsai</surname><given-names>Y.S.</given-names></name><name name-style="western"><surname>Ga&#353;evi&#263;</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name></person-group><article-title>Can large language models provide feedback to students? A case study on ChatGPT</article-title><source>Proceedings of the 2023 IEEE international conference on advanced learning technologies (ICALT)</source><conf-loc>Orem, UT, USA</conf-loc><conf-date>10&#8211;13 July 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>323</fpage><lpage>325</lpage></element-citation></ref><ref id="B5-sensors-25-05640"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name></person-group><article-title>Artificial intelligence in education: Opportunities, current status, and prospects</article-title><source>Geogr. Res. Bull.</source><year>2024</year><volume>3</volume><fpage>146</fpage><lpage>174</lpage></element-citation></ref><ref id="B6-sensors-25-05640"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ru</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Generative AI-supported teacher comments: An empirical study based on junior high school mathematics classrooms</article-title><source>E-Educ. Res.</source><year>2024</year><volume>45</volume><fpage>58</fpage><lpage>66</lpage></element-citation></ref><ref id="B7-sensors-25-05640"><label>7.</label><element-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Le</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Learning by teaching with ChatGPT: The effect of teachable ChatGPT agent on programming education</article-title><source>Br. J. Educ. Technol.</source><year>2024</year><comment>early view</comment></element-citation></ref><ref id="B8-sensors-25-05640"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sedrakyan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Veldkamp</surname><given-names>B.</given-names></name><name name-style="western"><surname>van Hillegersberg</surname><given-names>J.</given-names></name><name name-style="western"><surname>van den Berg</surname><given-names>S.M.</given-names></name></person-group><article-title>Students feedback analysis model using deep learning-based method and linguistic knowledge for intelligent educational systems</article-title><source>Soft Comput.</source><year>2023</year><volume>27</volume><fpage>14073</fpage><lpage>14094</lpage><pub-id pub-id-type="doi">10.1007/s00500-023-07926-2</pub-id></element-citation></ref><ref id="B9-sensors-25-05640"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shaik</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dann</surname><given-names>C.</given-names></name><name name-style="western"><surname>McDonald</surname><given-names>J.</given-names></name><name name-style="western"><surname>Redmond</surname><given-names>P.</given-names></name><name name-style="western"><surname>Galligan</surname><given-names>L.</given-names></name></person-group><article-title>A review of the trends and challenges in adopting natural language processing methods for education feedback analysis</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>56720</fpage><lpage>56739</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3177752</pub-id></element-citation></ref><ref id="B10-sensors-25-05640"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cang</surname><given-names>Y.</given-names></name></person-group><article-title>Advanced multimodal deep learning architecture for image-text matching</article-title><source>Proceedings of the 2024 IEEE 4th International Conference on Electronic Technology, Communication and Information (ICETCI)</source><conf-loc>Changchun, China</conf-loc><conf-date>24&#8211;26 May 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1185</fpage><lpage>1191</lpage></element-citation></ref><ref id="B11-sensors-25-05640"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chua</surname><given-names>T.S.</given-names></name></person-group><article-title>Next-gpt: Any-to-any multimodal llm</article-title><source>Proceedings of the Forty-first International Conference on Machine Learning</source><conf-loc>Vienna, Austria</conf-loc><conf-date>21&#8211;27 July 2024</conf-date></element-citation></ref><ref id="B12-sensors-25-05640"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weissweiler</surname><given-names>L.</given-names></name><name name-style="western"><surname>K&#246;ksal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sch&#252;tze</surname><given-names>H.</given-names></name></person-group><article-title>Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2403.06965</pub-id><pub-id pub-id-type="arxiv">2403.06965</pub-id></element-citation></ref><ref id="B13-sensors-25-05640"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mu&#241;oz-Ortiz</surname><given-names>A.</given-names></name><name name-style="western"><surname>G&#243;mez-Rodr&#237;guez</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vilares</surname><given-names>D.</given-names></name></person-group><article-title>Contrasting Linguistic Patterns in Human and LLM-Generated News Text</article-title><source>Artif. Intell. Rev.</source><year>2024</year><volume>57</volume><fpage>265</fpage><pub-id pub-id-type="doi">10.1007/s10462-024-10903-2</pub-id><pub-id pub-id-type="pmid">39328400</pub-id><pub-id pub-id-type="pmcid">PMC11422446</pub-id></element-citation></ref><ref id="B14-sensors-25-05640"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Backes</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</article-title><source>High-Confid. Comput.</source><year>2024</year><volume>4</volume><fpage>100211</fpage><pub-id pub-id-type="doi">10.1016/j.hcc.2024.100211</pub-id></element-citation></ref><ref id="B15-sensors-25-05640"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Garrette</surname><given-names>D.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>P.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Artetxe</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yvon</surname><given-names>F.</given-names></name><name name-style="western"><surname>Kirov</surname><given-names>C.</given-names></name><etal/></person-group><article-title>Hire a Linguist!: Learning Endangered Languages in LLMs with In-Context Linguistic Descriptions</article-title><source>Proceedings of the Findings of the Association for Computational Linguistics: ACL 2024</source><conf-loc>Bangkok, Thailand</conf-loc><conf-date>11&#8211;16 August 2024</conf-date><publisher-name>Association for Computational Linguistics</publisher-name><publisher-loc>Stroudsburg, PA, USA</publisher-loc><year>2024</year><fpage>15654</fpage><lpage>15669</lpage><pub-id pub-id-type="doi">10.18653/v1/2024.findings-acl.936</pub-id></element-citation></ref><ref id="B16-sensors-25-05640"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>P.</given-names></name><name name-style="western"><surname>Bhatt</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ingle</surname><given-names>O.</given-names></name><name name-style="western"><surname>Goyal</surname><given-names>D.</given-names></name><name name-style="western"><surname>Raman</surname><given-names>B.</given-names></name></person-group><article-title>Affective Feedback Synthesis Towards Multimodal Text and Image Data</article-title><source>ACM Trans. Multimed. Comput. Commun. Appl.</source><year>2023</year><volume>19</volume><fpage>190</fpage><pub-id pub-id-type="doi">10.1145/3589186</pub-id></element-citation></ref><ref id="B17-sensors-25-05640"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>A Review of Multimodal Interaction in Remote Education: Technologies, Applications, and Challenges</article-title><source>Appl. Sci.</source><year>2025</year><volume>15</volume><elocation-id>3937</elocation-id><pub-id pub-id-type="doi">10.3390/app15073937</pub-id></element-citation></ref><ref id="B18-sensors-25-05640"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name></person-group><article-title>Personalized multimodal feedback generation in education</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2011.00192</pub-id><pub-id pub-id-type="arxiv">2011.00192</pub-id></element-citation></ref><ref id="B19-sensors-25-05640"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharif</surname><given-names>H.</given-names></name><name name-style="western"><surname>Atif</surname><given-names>A.</given-names></name></person-group><article-title>The evolving classroom: How learning analytics is shaping the future of education and feedback mechanisms</article-title><source>Educ. Sci.</source><year>2024</year><volume>14</volume><elocation-id>176</elocation-id><pub-id pub-id-type="doi">10.3390/educsci14020176</pub-id></element-citation></ref><ref id="B20-sensors-25-05640"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Steiss</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tate</surname><given-names>T.</given-names></name><name name-style="western"><surname>Graham</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cruz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hebert</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Moon</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tseng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Warschauer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Olson</surname><given-names>C.B.</given-names></name></person-group><article-title>Comparing the quality of human and ChatGPT feedback of students&#8217; writing</article-title><source>Learn. Instr.</source><year>2024</year><volume>91</volume><fpage>101894</fpage><pub-id pub-id-type="doi">10.1016/j.learninstruc.2024.101894</pub-id></element-citation></ref><ref id="B21-sensors-25-05640"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Youn</surname><given-names>C.H.</given-names></name><name name-style="western"><surname>Salam</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>A.A.</given-names></name></person-group><article-title>AI-Driven Tools in Providing Feedback on Students&#226;&#8364;&#8482; Writing</article-title><source>Int. J. Res. Innov. Soc. Sci.</source><year>2025</year><volume>9</volume><fpage>58</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.47772/IJRISS.2025.903SEDU0006</pub-id></element-citation></ref><ref id="B22-sensors-25-05640"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Esbenshade</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sarkar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name></person-group><article-title>Implementation Considerations for Automated AI Grading of Student Work</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="doi">10.48550/arXiv.2506.07955</pub-id><pub-id pub-id-type="arxiv">2506.07955</pub-id></element-citation></ref><ref id="B23-sensors-25-05640"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name></person-group><article-title>AI-assisted assessment in higher education: A systematic review</article-title><source>J. Educ. Technol. Innov.</source><year>2024</year><volume>6</volume><fpage>137</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.61414/jeti.v6i4.209</pub-id></element-citation></ref><ref id="B24-sensors-25-05640"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>W.</given-names></name><name name-style="western"><surname>Son</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>I.</given-names></name></person-group><article-title>Vilt: Vision-and-language transformer without convolution or region supervision</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Virtual</conf-loc><conf-date>18&#8211;24 July 2024</conf-date><publisher-name>PMLR</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2021</year><fpage>5583</fpage><lpage>5594</lpage></element-citation></ref><ref id="B25-sensors-25-05640"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hoi</surname><given-names>S.</given-names></name></person-group><article-title>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Baltimore, MD, USA</conf-loc><conf-date>17&#8211;23 July 2022</conf-date><publisher-name>PMLR</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2022</year><fpage>12888</fpage><lpage>12900</lpage></element-citation></ref><ref id="B26-sensors-25-05640"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Hallacy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ramesh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Goh</surname><given-names>G.</given-names></name><name name-style="western"><surname>Agarwal</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sastry</surname><given-names>G.</given-names></name><name name-style="western"><surname>Askell</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mishkin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Clark</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Learning transferable visual models from natural language supervision</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Virtual</conf-loc><conf-date>18&#8211;24 July 2021</conf-date><publisher-name>PMLR</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2021</year><fpage>8748</fpage><lpage>8763</lpage></element-citation></ref><ref id="B27-sensors-25-05640"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>A comprehensive capability analysis of GPT-3 and GPT-3.5 series models</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2303.10420</pub-id><pub-id pub-id-type="arxiv">2303.10420</pub-id></element-citation></ref><ref id="B28-sensors-25-05640"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Savarese</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hoi</surname><given-names>S.</given-names></name></person-group><article-title>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>23&#8211;29 July 2023</conf-date><publisher-name>PMLR</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2023</year><fpage>19730</fpage><lpage>19742</lpage></element-citation></ref><ref id="B29-sensors-25-05640"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tyson</surname><given-names>G.</given-names></name><name name-style="western"><surname>Haq</surname><given-names>E.U.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>L.H.</given-names></name><name name-style="western"><surname>Hui</surname><given-names>P.</given-names></name></person-group><article-title>Apt-pipe: A prompt-tuning tool for social data annotation using chatgpt</article-title><source>Proceedings of the ACM Web Conference 2024</source><conf-loc>Singapore</conf-loc><conf-date>13&#8211;17 May 2024</conf-date><fpage>245</fpage><lpage>255</lpage></element-citation></ref><ref id="B30-sensors-25-05640"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tee</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.</given-names></name><name name-style="western"><surname>Huili</surname><given-names>S.</given-names></name></person-group><article-title>ZJ-EduFormer: Predicting Supply Chain Student Stress Using Transformer</article-title><source>Proceedings of the 2024 5th International Conference on Information Science and Education (ICISE-IE)</source><conf-loc>Zhanjiang, China</conf-loc><conf-date>20&#8211;22 December 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>307</fpage><lpage>311</lpage></element-citation></ref><ref id="B31-sensors-25-05640"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>The dawn of lmms: Preliminary explorations with gpt-4v (ision)</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2309.17421</pub-id></element-citation></ref><ref id="B32-sensors-25-05640"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ning</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>L.</given-names></name></person-group><article-title>Video-llava: Learning united visual representation by alignment before projection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2311.10122</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05640-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall framework diagram illustrates the multimodal input processing and encoding-decoding workflow.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05640-g001.jpg"/></fig><fig position="float" id="sensors-25-05640-f002" orientation="portrait"><label>Figure 2</label><caption><p>Cross-modal alignment module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05640-g002.jpg"/></fig><fig position="float" id="sensors-25-05640-f003" orientation="portrait"><label>Figure 3</label><caption><p>Visualization of the ablation study results for the CWH across three tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05640-g003.jpg"/></fig><fig position="float" id="sensors-25-05640-f004" orientation="portrait"><label>Figure 4</label><caption><p>Visualization of the ablation study results of the CWH across three tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05640-g004.jpg"/></fig><fig position="float" id="sensors-25-05640-f005" orientation="portrait"><label>Figure 5</label><caption><p>Ablation results of the CWH across three tasks. Each subplot compares the performance of the full model and w/o CWH on ROUGE-L, BLEU, CIDEr, and human scores, highlighting the consistent effectiveness of CWH in enhancing model performance across tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05640-g005.jpg"/></fig><fig position="float" id="sensors-25-05640-f006" orientation="portrait"><label>Figure 6</label><caption><p>In this heatmap, the red color family (such as red, orange) represents relatively high heat (such as attention, numerical intensity, etc.) in the corresponding area, while the green color family (such as green, blue) represents relatively low heat.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05640-g006.jpg"/></fig><table-wrap position="float" id="sensors-25-05640-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05640-t001_Table 1</object-id><label>Table 1</label><caption><p>Sample distribution of dataset used in this paper.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Task Type</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sample Count</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Image Count</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Text Count</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Case analysis</td><td align="center" valign="middle" rowspan="1" colspan="1">1927</td><td align="center" valign="middle" rowspan="1" colspan="1">1298</td><td align="center" valign="middle" rowspan="1" colspan="1">3103</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Statute application</td><td align="center" valign="middle" rowspan="1" colspan="1">1311</td><td align="center" valign="middle" rowspan="1" colspan="1">325</td><td align="center" valign="middle" rowspan="1" colspan="1">2037</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Economic chart interpretation</td><td align="center" valign="middle" rowspan="1" colspan="1">1193</td><td align="center" valign="middle" rowspan="1" colspan="1">1905</td><td align="center" valign="middle" rowspan="1" colspan="1">1592</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Policy commentary</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">781</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">379</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1017</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05640-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05640-t002_Table 2</object-id><label>Table 2</label><caption><p>Task 1: Performance comparison on image&#8211;text essay feedback generation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ROUGE-L</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">BLEU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CIDEr</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Human Eval. (/5)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPT-3.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.542</td><td align="center" valign="middle" rowspan="1" colspan="1">0.398</td><td align="center" valign="middle" rowspan="1" colspan="1">1.210</td><td align="center" valign="middle" rowspan="1" colspan="1">3.82</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BLIP-2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.563</td><td align="center" valign="middle" rowspan="1" colspan="1">0.415</td><td align="center" valign="middle" rowspan="1" colspan="1">1.284</td><td align="center" valign="middle" rowspan="1" colspan="1">3.95</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ChatGPT Prompt Tuning</td><td align="center" valign="middle" rowspan="1" colspan="1">0.587</td><td align="center" valign="middle" rowspan="1" colspan="1">0.437</td><td align="center" valign="middle" rowspan="1" colspan="1">1.325</td><td align="center" valign="middle" rowspan="1" colspan="1">4.10</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EduFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">0.604</td><td align="center" valign="middle" rowspan="1" colspan="1">0.452</td><td align="center" valign="middle" rowspan="1" colspan="1">1.372</td><td align="center" valign="middle" rowspan="1" colspan="1">4.18</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPT-4V</td><td align="center" valign="middle" rowspan="1" colspan="1">0.636</td><td align="center" valign="middle" rowspan="1" colspan="1">0.478</td><td align="center" valign="middle" rowspan="1" colspan="1">1.445</td><td align="center" valign="middle" rowspan="1" colspan="1">4.38</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LLaVA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.624</td><td align="center" valign="middle" rowspan="1" colspan="1">0.469</td><td align="center" valign="middle" rowspan="1" colspan="1">1.412</td><td align="center" valign="middle" rowspan="1" colspan="1">4.31</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.653</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.496</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.498</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.46</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05640-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05640-t003_Table 3</object-id><label>Table 3</label><caption><p>Task 2: Performance comparison on image&#8211;text QA feedback.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ROUGE-L</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">BLEU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CIDEr</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Human Eval. (/5)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPT-3.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.528</td><td align="center" valign="middle" rowspan="1" colspan="1">0.385</td><td align="center" valign="middle" rowspan="1" colspan="1">1.142</td><td align="center" valign="middle" rowspan="1" colspan="1">3.76</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BLIP-2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.547</td><td align="center" valign="middle" rowspan="1" colspan="1">0.401</td><td align="center" valign="middle" rowspan="1" colspan="1">1.208</td><td align="center" valign="middle" rowspan="1" colspan="1">3.89</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ChatGPT Prompt Tuning</td><td align="center" valign="middle" rowspan="1" colspan="1">0.569</td><td align="center" valign="middle" rowspan="1" colspan="1">0.422</td><td align="center" valign="middle" rowspan="1" colspan="1">1.264</td><td align="center" valign="middle" rowspan="1" colspan="1">4.02</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EduFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">0.593</td><td align="center" valign="middle" rowspan="1" colspan="1">0.439</td><td align="center" valign="middle" rowspan="1" colspan="1">1.316</td><td align="center" valign="middle" rowspan="1" colspan="1">4.11</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPT-4V</td><td align="center" valign="middle" rowspan="1" colspan="1">0.629</td><td align="center" valign="middle" rowspan="1" colspan="1">0.472</td><td align="center" valign="middle" rowspan="1" colspan="1">1.402</td><td align="center" valign="middle" rowspan="1" colspan="1">4.33</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LLaVA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.615</td><td align="center" valign="middle" rowspan="1" colspan="1">0.461</td><td align="center" valign="middle" rowspan="1" colspan="1">1.367</td><td align="center" valign="middle" rowspan="1" colspan="1">4.26</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.642</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.481</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.437</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.39</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05640-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05640-t004_Table 4</object-id><label>Table 4</label><caption><p>Task 3: Performance comparison on artistic cognition guidance and improvement suggestion.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ROUGE-L</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">BLEU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CIDEr</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Human Eval. (/5)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPT-3.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.556</td><td align="center" valign="middle" rowspan="1" colspan="1">0.406</td><td align="center" valign="middle" rowspan="1" colspan="1">1.196</td><td align="center" valign="middle" rowspan="1" colspan="1">3.84</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BLIP-2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.574</td><td align="center" valign="middle" rowspan="1" colspan="1">0.423</td><td align="center" valign="middle" rowspan="1" colspan="1">1.255</td><td align="center" valign="middle" rowspan="1" colspan="1">3.96</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ChatGPT Prompt Tuning</td><td align="center" valign="middle" rowspan="1" colspan="1">0.598</td><td align="center" valign="middle" rowspan="1" colspan="1">0.441</td><td align="center" valign="middle" rowspan="1" colspan="1">1.309</td><td align="center" valign="middle" rowspan="1" colspan="1">4.09</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EduFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">0.615</td><td align="center" valign="middle" rowspan="1" colspan="1">0.455</td><td align="center" valign="middle" rowspan="1" colspan="1">1.346</td><td align="center" valign="middle" rowspan="1" colspan="1">4.17</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPT-4V</td><td align="center" valign="middle" rowspan="1" colspan="1">0.650</td><td align="center" valign="middle" rowspan="1" colspan="1">0.490</td><td align="center" valign="middle" rowspan="1" colspan="1">1.432</td><td align="center" valign="middle" rowspan="1" colspan="1">4.36</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LLaVA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.636</td><td align="center" valign="middle" rowspan="1" colspan="1">0.478</td><td align="center" valign="middle" rowspan="1" colspan="1">1.389</td><td align="center" valign="middle" rowspan="1" colspan="1">4.29</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.664</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.502</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.476</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.43</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05640-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05640-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation study of the CWH across three tasks.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<bold>Task 1: Feedback Generation</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ROUGE-L</td><td align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">BLEU</td><td align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CIDEr</td><td align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Human</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Full Model</td><td align="center" valign="middle" rowspan="1" colspan="1">45.2</td><td align="center" valign="middle" rowspan="1" colspan="1">38.6</td><td align="center" valign="middle" rowspan="1" colspan="1">1.25</td><td align="center" valign="middle" rowspan="1" colspan="1">4.72</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/o CWH</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.30</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Task 2: QA Feedback</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ROUGE-L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BLEU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CIDEr</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Human</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Full Model</td><td align="center" valign="middle" rowspan="1" colspan="1">47.5</td><td align="center" valign="middle" rowspan="1" colspan="1">40.1</td><td align="center" valign="middle" rowspan="1" colspan="1">1.31</td><td align="center" valign="middle" rowspan="1" colspan="1">4.80</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/o CWH</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.35</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Task 3: Art Cognition Guidance</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ROUGE-L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BLEU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CIDEr</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Human</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Full Model</td><td align="center" valign="middle" rowspan="1" colspan="1">44.0</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5</td><td align="center" valign="middle" rowspan="1" colspan="1">1.22</td><td align="center" valign="middle" rowspan="1" colspan="1">4.65</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/o CWH</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.25</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05640-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05640-t006_Table 6</object-id><label>Table 6</label><caption><p>Case study illustrating personalized feedback generation for different students with the same answer.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Input</th><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Output (Feedback)</th></tr></thead><tbody><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Question:</bold> Explain the impact of inflation on purchasing power.<break/><bold>Student Answer (text):</bold> &#8220;Inflation makes goods more expensive but does not change income.&#8221;<break/><bold>Student Answer (image):</bold> Handwritten short essay.<break/><bold>Student Background A:</bold> Low prior performance; common error = confusion between nominal and real value.</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Feedback A:</bold> &#8220;Your answer identifies the price increase, but&#160;remember to also mention income and real purchasing power. Inflation reduces the <italic toggle="yes">real</italic> value of money, meaning you can buy less with the same income.&#8221;</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Question:</bold> Explain the impact of inflation on purchasing power.<break/><bold>Student Answer (text):</bold> &#8220;Inflation makes goods more expensive but does not change income.&#8221;<break/><bold>Student Answer (image):</bold> Handwritten short essay.<break/><bold>Student Background B:</bold> High prior performance; prefers technical terminology.</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Feedback B:</bold> &#8220;Good point on price increase. For&#160;precision, note that inflation decreases <italic toggle="yes">real purchasing power</italic> by eroding the constant-value basis of nominal income, which is crucial in macroeconomic analysis.&#8221;</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>