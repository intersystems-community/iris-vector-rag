<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473299</article-id><article-id pub-id-type="pmcid-ver">PMC12473299.1</article-id><article-id pub-id-type="pmcaid">12473299</article-id><article-id pub-id-type="pmcaiid">12473299</article-id><article-id pub-id-type="pmid">41013117</article-id><article-id pub-id-type="doi">10.3390/s25185879</article-id><article-id pub-id-type="publisher-id">sensors-25-05879</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>TFP-YOLO: Obstacle and Traffic Sign Detection for Assisting Visually Impaired Pedestrians</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-4739-1504</contrib-id><name name-style="western"><surname>Zheng</surname><given-names initials="Z">Zhiwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6328-7876</contrib-id><name name-style="western"><surname>Cheng</surname><given-names initials="J">Jin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="c1-sensors-25-05879" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-1559-0887</contrib-id><name name-style="western"><surname>Jin</surname><given-names initials="F">Fanghua</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Leone</surname><given-names initials="A">Alessandro</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05879">School of Science, Beijing Information Science and Technology University, Beijing 100192, China; <email>2023021114@bistu.edu.cn</email> (Z.Z.); <email>2023020471@bistu.edu.cn</email> (F.J.)</aff><author-notes><corresp id="c1-sensors-25-05879"><label>*</label>Correspondence: <email>chengjin@bistu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5879</elocation-id><history><date date-type="received"><day>02</day><month>9</month><year>2025</year></date><date date-type="rev-recd"><day>16</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>19</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05879.pdf"/><abstract><p>With the increasing demand for intelligent mobility assistance among the visually impaired, machine guide dogs based on computer vision have emerged as an effective alternative to traditional guide dogs, owing to their flexible deployment and scalability. To enhance their visual perception capabilities in complex urban environments, this paper proposes an improved YOLOv8-based detection algorithm, termed TFP-YOLO, designed to recognize traffic signs such as traffic lights and crosswalks, as well as small obstacle objects including pedestrians and bicycles, thereby improving the target detection performance of machine guide dogs in complex road scenarios. The proposed algorithm incorporates a Triplet Attention mechanism into the backbone network to strengthen the perception of key regions, and integrates a Triple Feature Encoding (TFE) module to achieve collaborative extraction of both local and global features. Additionally, a P2 detection head is introduced to improve the accuracy of small object detection, particularly for traffic lights. Furthermore, the WIoU loss function is adopted to enhance training stability and the model&#8217;s generalization capability. Experimental results demonstrate that the proposed algorithm achieves a detection accuracy of 93.9% and a precision of 90.2%, while reducing the number of parameters by 17.2%. These improvements significantly enhance the perception performance of machine guide dogs in identifying traffic information and obstacles, providing strong technical support for subsequent path planning and embedded deployment, and demonstrating considerable practical application value.</p></abstract><kwd-group><kwd>computer vision</kwd><kwd>machine guide dog</kwd><kwd>object detection</kwd><kwd>YOLOv8</kwd></kwd-group><funding-group><award-group><funding-source>Natural Science Foundation of Beijing</funding-source><award-id>4212036</award-id></award-group><award-group><funding-source>Beijing Municipal Universities</funding-source><award-id>bistu71E2510933</award-id><award-id>bistu71E2510935</award-id><award-id>bistu71E2510936</award-id></award-group><funding-statement>This work was supported by the Natural Science Foundation of Beijing (Grant number 4212036), Fundamental Research Funds for the Beijing Municipal Universities (Grant number bistu71E2510933, bistu71E2510935, bistu71E2510936).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05879"><title>1. Introduction</title><p>According to global statistics, over 2.2 billion people worldwide suffer from some form of visual impairment [<xref rid="B1-sensors-25-05879" ref-type="bibr">1</xref>]. In China alone, the visually impaired population is approximately 17.31 million, making it the country with the largest number of blind individuals. Notably, 23.5% of this group are adolescents or young to middle-aged adults. This substantial demographic faces an urgent demand for safe and efficient mobility assistance tools. At present, most navigation methods rely on smartphones for global route planning. While such systems can offer general directional guidance, they fall short in providing timely and detailed local navigation during movement, particularly when encountering dynamic obstacles such as pedestrians, bicycles, and vehicles, or static traffic elements such as crosswalks and traffic lights. This limitation poses serious challenges to the efficiency and safety of travel for the visually impaired.</p><p>With the rapid advancement of robotic assistance technologies and intelligent perception algorithms, machine guide dogs are increasingly regarded as an ideal alternative to traditional guide dogs. These systems are becoming a crucial component in the mobility solutions available to visually impaired individuals. Compared to real guide dogs, which are costly to train and difficult to scale, quadruped robotic guide dogs based on intelligent navigation and environmental perception offer advantages such as high replicability and flexible deployment. In recent years, Xiao et al. [<xref rid="B2-sensors-25-05879" ref-type="bibr">2</xref>] proposed a robot guide dog system based on the hybrid physical interaction of the Mini Cheetah quadruped robot and a traction rope. The robot guide dog system can guide blind people to move safely in narrow environments through a rope with variable tension and relaxation, demonstrating the feasibility and effectiveness of robots in assisting blind people to navigate in real scenarios, as shown in <xref rid="sensors-25-05879-f001" ref-type="fig">Figure 1</xref>. However, ensuring the reliable operation of such systems in complex environments hinges on the ability to accurately and efficiently perceive typical obstacles and traffic cues in real time.</p><p>Therefore, constructing a visual perception system capable of road target detection is essential for the effective operation of guide robots. On the one hand, such a system enables accurate detection of common obstacles and traffic signs in road environments, thereby preventing visually impaired users from deviating from the intended path due to occlusions or ambiguous route information. On the other hand, it provides a reliable environmental awareness foundation for subsequent local path planning and dynamic obstacle avoidance. Vision-based object detection modules have thus become a core component in achieving safe, stable, and intelligent guidance for machine guide dogs.</p><p>The detection of typical road obstacles and traffic signs has long been a research focus in the field of computer vision. Currently, mainstream approaches in object detection utilize a combination of radar, ultrasonic sensors, and vision-based systems. Among these, visual detection offers richer scene information, lower cost, and easier deployment compared to alternative sensing technologies. Visual detection typically involves capturing scenes using cameras and applying algorithms to identify objects of interest within the images.</p><p>At present, there are two detection methods used in target detection tasks: traditional detection methods and deep learning detection methods. Traditional target detection algorithms use sliding windows and manually extracted features [<xref rid="B3-sensors-25-05879" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05879" ref-type="bibr">4</xref>]. The Regionlets [<xref rid="B5-sensors-25-05879" ref-type="bibr">5</xref>] detection model generates multiple local regions by modeling each region of the object, and trains these regions through a support vector machine (SVM). This model can better cope with the morphological changes and complex backgrounds of the object. The Deformable Part Model (DPM) [<xref rid="B6-sensors-25-05879" ref-type="bibr">6</xref>] detection model extracts local features from the image to represent each part, and then applies these features to the part model through convolution operations. The core idea of DPM was later further developed in deep learning models. Traditional detection methods have a lot of redundant calculations, a slow running speed, poor robustness in complex environments, and other problems, making it difficult for them to achieve satisfactory detection results. In contrast, Ross Girshick et al. [<xref rid="B7-sensors-25-05879" ref-type="bibr">7</xref>] proposed the application of deep convolutional networks to the field of target detection and designed a new network architecture R-CNN to improve the accuracy of object detection and compared it with the traditional target detection algorithm on the VOC 2010 dataset. The detection accuracies of multiple categories such as bike, car, and person are higher than those of traditional detection methods, and the average accuracy is also higher than that of traditional methods. Deep learning algorithms do not require manual feature extraction and have strong anti-interference ability, so they are widely used in the field of target detection.</p><p>Traditional methods usually rely on manually designed feature extractors and classifiers, facing problems such as complex backgrounds, occlusion, and scale changes. By introducing target detection technology based on deep learning, breakthrough progress has been made. Detection algorithms based on deep learning are mainly divided into two-stage and single-stage detection algorithms. The two-stage detection algorithm obtains the detection result by extracting candidate boxes and performing secondary correction. Representative algorithms include R-CNN [<xref rid="B8-sensors-25-05879" ref-type="bibr">8</xref>], Fast R-CNN [<xref rid="B9-sensors-25-05879" ref-type="bibr">9</xref>], Faster-RCNN [<xref rid="B10-sensors-25-05879" ref-type="bibr">10</xref>], etc. Single-stage algorithms can complete positioning and classification at one time. Representative algorithms include the SSD series of algorithms [<xref rid="B11-sensors-25-05879" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05879" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05879" ref-type="bibr">13</xref>] and the YOLO series of algorithms [<xref rid="B14-sensors-25-05879" ref-type="bibr">14</xref>]. In the past few years, researchers have developed the target detection framework from a two-stage to a one-stage framework.</p><p>The YOLO [<xref rid="B15-sensors-25-05879" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05879" ref-type="bibr">16</xref>] algorithm adopts a single-stage target detection method, which divides the entire image into grid cells and predicts multiple bounding boxes and category probabilities in each cell to achieve rapid detection of traffic signs. Compared with traditional methods, the YOLO [<xref rid="B17-sensors-25-05879" ref-type="bibr">17</xref>] algorithm has higher detection speed and accuracy.</p><p>The core idea of the algorithm is to learn feature representation from the original image through a deep convolutional neural network (CNN) and then use the predictor to generate bounding boxes and category probabilities. YOLOv8 adopts the deep neural network structure of Darknet, combined with feature extraction at different levels, to effectively capture the shape, texture, and contextual information of traffic signs. In addition, the YOLOv8 algorithm also introduces a series of optimization strategies, such as multi-scale training, data enhancement, and loss function optimization, to further improve the performance of obstacle and traffic sign detection.</p><p>Existing object detection methods, particularly those based on the YOLO series, have shown remarkable performance in real-time applications. YOLOv8, with its efficient Darknet backbone and multi-scale feature extraction, achieves a strong balance of speed and accuracy. However, it faces limitations in the context of machine guide dog applications. First, YOLOv8 struggles with small target detection (e.g., traffic lights smaller than <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels), as deeper network layers lose fine-grained details due to downsampling. Second, blurred-edge features, such as crosswalks under occlusion or varying lighting, are often misdetected due to weak edge representations. Third, the Complete IoU (CIoU) loss function in YOLOv8 overemphasizes low-quality samples, reducing robustness in complex scenes with scale variations and boundary ambiguities. These shortcomings hinder reliable navigation assistance in diverse urban environments, necessitating targeted improvements for visually impaired applications.</p><p>To address these challenges, this paper designs a new feature fusion network architecture that combines local and global feature information to obtain more accurate feature maps. Compared with the original network structure, it has fewer parameters and higher average accuracy.</p><p>The contributions of this paper are summarized as follows, and are quantitatively verified on both public and custom datasets:<list list-type="simple"><list-item><label>1</label><p>A lightweight Triplet Attention module is introduced into the backbone network. It captures cross-dimensional interactions to enhance the correlation among local regions and the interactions between feature channels. This mechanism significantly improves the network&#8217;s focus on indistinct features with blurred edges, such as crosswalks, resulting in an approximately 4.6% improvement in the recall rate for this category of objects.</p></list-item><list-item><label>2</label><p>We design a multi-scale feature enhancement module, termed Triple Feature Encoding (TFE). It fuses spatial information from three different feature map resolutions (large, medium, and small). This structure facilitates the extraction of fine-grained details from small objects and reduces background noise interference. Working in concert with the P2 detection head, it achieves a 5.2% increase in average precision (AP) for small objects such as traffic lights.</p></list-item><list-item><label>3</label><p>A P2 detection head is employed to construct a four-head multi-scale detection architecture. It extracts lower-level features from higher-resolution feature maps, which aids in identifying small-scale targets. This design lowers the model&#8217;s effective detection size limit from 8 &#215; 8 pixels to 4 &#215; 4 pixels, significantly enhancing the perception of very small objects. It collaborates with other detection heads to effectively handle objects of varying scales.</p></list-item><list-item><label>4</label><p>The Complete IoU (CIoU) loss function is replaced with Wise-IoU v3 (WIoU). Its dynamic focusing mechanism addresses the challenges of blurred boundaries and scale variations by enhancing the focus on hard samples (e.g., small traffic lights) while reducing the emphasis on low-quality samples. This replacement ultimately improves the mean average precision (mAP@0.5) by 4.1% while reducing the number of parameters by 17.2%, achieving a better trade-off between accuracy and efficiency.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05879"><title>2. Related Work</title><p>Recent advancements in object detection, particularly within the YOLO series, have focused on improving accuracy for small targets and complex urban scenes, which are critical for applications like machine guide dogs. These systems require real-time detection of obstacles and traffic signs to ensure safe navigation for visually impaired users.</p><p>Early improvements to YOLO models emphasized using attention mechanisms to enhance feature representation. For instance, Yan et al. [<xref rid="B18-sensors-25-05879" ref-type="bibr">18</xref>] integrated the Squeeze-and-Excitation (SE) Block into YOLOv5, achieving a 1.44% mAP improvement by prioritizing salient features. Similarly, Li et al. [<xref rid="B19-sensors-25-05879" ref-type="bibr">19</xref>] incorporated SE Net and CBAM into YOLOv3&#8217;s backbone, boosting mAP by up to 8.50% through channel-wise importance learning. Ma et al. [<xref rid="B20-sensors-25-05879" ref-type="bibr">20</xref>] introduced the Feature Select Module (FSM) in the neck layer of YOLOv3, YOLOv4, and YOLOv5-L, reducing noise in feature fusion and improving performance by 0.60% to 1.50%. Ju et al. [<xref rid="B21-sensors-25-05879" ref-type="bibr">21</xref>] proposed AFFAM for YOLOv3, combining global and spatial attention for multi-scale feature fusion, yielding mAP gains of 5.08% to 7.41% on datasets like KITTI.</p><p>More recent works have targeted small object detection in traffic scenarios, directly relevant to urban navigation challenges. For example, the ETSR-YOLO model [<xref rid="B22-sensors-25-05879" ref-type="bibr">22</xref>] enhanced YOLO for multi-scale traffic sign detection, improving robustness in complex environments. TSD-YOLO [<xref rid="B23-sensors-25-05879" ref-type="bibr">23</xref>] introduced a Space-to-Depth module to handle scale variations in traffic signs, addressing missed detections. DP-YOLO [<xref rid="B24-sensors-25-05879" ref-type="bibr">24</xref>] optimized YOLOv8s for small traffic signs by reducing parameters while boosting accuracy. SOD-YOLOv8 [<xref rid="B25-sensors-25-05879" ref-type="bibr">25</xref>] specifically improved YOLOv8 for small objects in traffic scenes, incorporating optimizations for urban drone imagery. CAS-YOLOv8 [<xref rid="B26-sensors-25-05879" ref-type="bibr">26</xref>] enhanced remote sensing object detection with contextual attention, showing promise for urban small targets.</p><p>Despite these advances, existing models often increase parameter complexity or compromise real-time performance, limiting their deployment in resource-constrained guide dog systems. Moreover, few address the unique needs of visually impaired navigation, such as detecting blurred-edge features (e.g., crosswalks) alongside small targets (e.g., traffic lights) in varied urban conditions. This study builds on YOLOv8&#8217;s efficiency, introducing targeted improvements to achieve a balance of accuracy, speed, and lightweight design for machine guide dog applications.</p><p>While robust perception is fundamental, a complete machine guide dog system also requires advanced path planning and navigation algorithms to ensure safe and efficient guidance. Traditional global planners such as A* [<xref rid="B27-sensors-25-05879" ref-type="bibr">27</xref>] and Dijkstra&#8217;s algorithm [<xref rid="B28-sensors-25-05879" ref-type="bibr">28</xref>] perform well in static environments but lack real-time reactivity to unknown obstacles. In contrast, local planners like the Dynamic Window Approach (DWA) [<xref rid="B29-sensors-25-05879" ref-type="bibr">29</xref>] offer high reactivity but may suffer from local minima and suboptimal global performance.</p><p>To address these limitations, hybrid approaches that integrate global and local planning have emerged. A notable example is the BRRT*-DWA framework with Adaptive Monte Carlo Localization (AMCL) proposed by Ayalew et al. [<xref rid="B30-sensors-25-05879" ref-type="bibr">30</xref>], which combines bidirectional rapidly exploring random tree star (BRRT*) for global path generation with DWA for real-time obstacle avoidance in dynamic environments.</p><p>Crucially, the performance of such navigation systems highly depends on the accuracy of perceptual inputs. Our work enhances this pipeline by providing a highly accurate visual perception module that reliably detects obstacles (e.g., pedestrians, vehicles) and traffic elements (e.g., crosswalks, traffic lights). These outputs enable robust downstream path planning and localization, ultimately improving the safety and effectiveness of the machine guide dog system.</p></sec><sec id="sec3-sensors-25-05879"><title>3. TFP-YOLO Method</title><sec id="sec3dot1-sensors-25-05879"><title>3.1. TFP-YOLO Model</title><p>To enable effective detection of obstacles and traffic signs encountered during the navigation of machine guide dogs, the network model must achieve a balance between low parameter complexity and high mean average precision (mAP). In this work, we propose improvements to the original YOLOv8 architecture by introducing modifications to its backbone, neck, and head components. These enhancements are designed to increase detection accuracy while reducing the overall model size. The structure of the improved network is illustrated in <xref rid="sensors-25-05879-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec3dot2-sensors-25-05879"><title>3.2. Triplet Attention Mechanism</title><p>To improve detection accuracy for challenging targets such as crosswalks, which often exhibit unclear or fragmented edge features, the Triplet Attention mechanism is embedded at the end of the backbone network. This attention module enhances the network&#8217;s sensitivity to fine-grained spatial patterns, particularly in cases where edge information is weak. Triplet Attention is a three-branch module that receives an input tensor and outputs a feature tensor of the same shape. By applying directional transformations across its branches, the module allows edge features&#8212;such as those of crosswalk lines&#8212;to be captured from multiple orientations, thereby improving the network&#8217;s robustness in such detection tasks. The structural diagram of the Triplet Attention mechanism is shown in <xref rid="sensors-25-05879-f003" ref-type="fig">Figure 3</xref>.</p><p>Given an input feature tensor <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the Triplet Attention module processes it through three independent branches.</p><sec id="sec3dot2dot1-sensors-25-05879"><title>3.2.1. Branch 1 (Height&#8211;Channel Interaction)</title><p>The input <italic toggle="yes">x</italic> is first permuted to shape <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then rotated <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msup><mml:mn>90</mml:mn><mml:mo>&#8728;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> counterclockwise along the height <italic toggle="yes">H</italic> axis, producing <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. This transformation promotes interaction between the height and channel dimensions. The rotated tensor is then passed through a Z-Pool layer, which reduces the height dimension to 2. The resulting tensor <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is fed into a standard convolutional layer with kernel size <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, generating a feature map of shape <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A Batch Normalization layer follows, and the output is passed through a sigmoid activation function to produce attention weights. These weights are then applied to the rotated feature map, which is subsequently rotated <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msup><mml:mn>90</mml:mn><mml:mo>&#8728;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> clockwise to restore the original spatial configuration.</p></sec><sec id="sec3dot2dot2-sensors-25-05879"><title>3.2.2. Branch 2 (Width&#8211;Channel Interaction)</title><p>Similarly, the input <italic toggle="yes">x</italic> is rotated <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msup><mml:mn>90</mml:mn><mml:mo>&#8728;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> counterclockwise along the width <italic toggle="yes">W</italic> axis, resulting in a tensor <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with shape <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Z-Pool reduces the height dimension, generating <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is then processed by a convolutional layer (kernel size <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) to produce an attention map of shape <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This output is normalized and activated using BatchNorm and a sigmoid function, respectively, and the attention weights are applied to <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. Finally, the tensor is rotated <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msup><mml:mn>90</mml:mn><mml:mo>&#8728;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> clockwise to match the original input orientation.</p></sec><sec id="sec3dot2dot3-sensors-25-05879"><title>3.2.3. Branch 3 (Spatial Attention)</title><p>In the third branch, the channel dimension of the input <italic toggle="yes">x</italic> is pooled via Z-Pool, resulting in <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This is passed through a <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional layer followed by Batch Normalization and a sigmoid activation, producing a spatial attention map that is applied directly to the input <italic toggle="yes">x</italic>.</p><p>Each branch produces an attention-weighted feature map of shape <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and these three outputs are fused via element-wise averaging to obtain the final attention-enhanced feature map. The overall structure of this mechanism is illustrated in <xref rid="sensors-25-05879-f004" ref-type="fig">Figure 4</xref>.</p><p>The Z-Pool operation is a critical component in all three branches. It applies max pooling and average pooling along the channel dimension of the input tensor, then concatenates the results. This operation not only reduces feature depth, thereby lowering computational complexity, but also retains rich semantic information from the feature maps. The process is formally defined as:<disp-formula id="FD1-sensors-25-05879"><label>(1)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Z-Pool</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mi>MaxPool</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>AvgPool</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The formula expression for the Triplet Attention mechanism is:<disp-formula id="FD2-sensors-25-05879"><label>(2)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mover><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#968;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#968;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#968;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the sigmoid activation function and <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#968;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#968;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#968;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> represent the standard 2D convolutional layers of the three branches in Triplet Attention, with a kernel size of <italic toggle="yes">k</italic>. After simplifying (<xref rid="FD2-sensors-25-05879" ref-type="disp-formula">2</xref>), <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></inline-formula> becomes:<disp-formula id="FD3-sensors-25-05879"><label>(3)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mover><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mover><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are the three attention weights computed in the Triplet Attention mechanism. In (<xref rid="FD3-sensors-25-05879" ref-type="disp-formula">3</xref>), <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mover><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mover><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> indicate that the original features have been rotated <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msup><mml:mn>90</mml:mn><mml:mo>&#8728;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> clockwise.</p></sec></sec><sec id="sec3dot3-sensors-25-05879"><title>3.3. Triple Feature Encoding Module</title><p>The feature map sizes and resolutions of each layer in the backbone network vary. Traditional fusion mechanisms, such as the standard FPN, typically only upsample small-sized feature maps and then fuse them with the features of the previous layer, often neglecting the rich detailed information contained in large-sized feature maps. In contrast, the proposed TFE module explicitly splits and processes feature maps from three different scales (large, medium, and small). Specifically, it incorporates large-scale feature maps and performs downsampling on them, while upsampling the small-scale feature maps. This process unifies their spatial dimensions to match the medium-scale feature map. By doing so, the TFE module effectively integrates both high-resolution details from shallow layers and high-level semantic information from deep layers, thereby significantly enhancing the extraction of feature information for small targets such as traffic lights. The structure of the TFE module is shown in <xref rid="sensors-25-05879-f005" ref-type="fig">Figure 5</xref>.</p><p>The formula is as follows: <disp-formula id="FD4-sensors-25-05879"><label>(4)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>TFE</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>TFE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the feature map output by the TFE module. <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the large-, medium-, and small-sized feature maps, respectively. <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are combined to form <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>TFE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which has the same resolution as <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> but three times the number of channels.</p></sec><sec id="sec3dot4-sensors-25-05879"><title>3.4. P2 Detection Head</title><p>After the <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> input image passes through the backbone and neck, the head of YOLOv8 sets three detection heads P3, P4, and P5 by default, corresponding to feature maps of <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>80</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>40</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, for detecting large, medium, and small targets. However, the original YOLOv8n can only detect targets larger than <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels, and it struggles to effectively identify key obstacles such as traffic lights and traffic signs with smaller sizes.</p><p>To improve the detection ability of small targets, this paper adds a <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>160</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>160</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> detection head to the P2 layer. As shown in <xref rid="sensors-25-05879-f006" ref-type="fig">Figure 6</xref>, the model introduces an upsampling operation based on the <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>80</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> feature map, expands it to <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>160</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>160</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and fuses it with the corresponding scale features in the backbone, while adding a downsampling path to match the structure. Finally, four detection heads are constructed to jointly complete multi-scale target detection, which significantly enhances the model&#8217;s recognition ability for small targets. The specific detection scales are shown in <xref rid="sensors-25-05879-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec3dot5-sensors-25-05879"><title>3.5. Wisev3-IoU Loss Function</title><p>The original CIoU loss function used in YOLOv8n tends to overemphasize low-quality samples in the training set due to factors such as distance and aspect ratio, which can impair the model&#8217;s generalization performance. To enhance detection performance, this paper introduces the WIoU loss function with a dynamic focusing mechanism for bounding box regression. This method mitigates the excessive punishment of geometric factors on the model by reducing the competitiveness of high-quality boxes and weakening the harmful gradients generated by low-quality samples, thereby improving the model&#8217;s generalization and localization capabilities.</p><p>WIoU is defined as: <disp-formula id="FD5-sensors-25-05879"><label>(5)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>IoU</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">L</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>IoU</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mo>&#8734;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05879"><label>(6)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>IoU</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>IoU</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05879"><label>(7)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mi>WIoU</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#956;</mml:mi><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mi>WIoU</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>IoU</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05879"><label>(8)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mi>WIoU</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>gt</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>gt</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05879"><label>(9)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#946;</mml:mi><mml:mrow><mml:mi>&#963;</mml:mi><mml:msup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the outlier degree, IoU is the intersection-over-union between the predicted and ground truth boxes, <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>IoU</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the loss, and <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">L</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>IoU</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is its moving average normalization factor. <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mi>WIoU</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the WIoU loss, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> is the gradient gain, <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> are hyperparameters, and <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mi>WIoU</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the normalized center distance between the predicted and ground truth boxes. Coordinates <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>gt</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>gt</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> correspond to the predicted and true box centers, while <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the width and height of their minimum enclosing box. The asterisk (*) indicates that <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are excluded from gradient computation to avoid adverse training effects.</p><p>Smaller <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> indicates higher-quality anchor boxes; assigning them lower gradient gains suppresses harmful gradients from outliers. When <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, granting maximum gradient gain. To preserve this strategy early in training, <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">L</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>IoU</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is initialized to 1, and a momentum term <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mroot><mml:mrow><mml:mn>0.05</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mroot></mml:mrow></mml:mrow></mml:math></inline-formula> (with <italic toggle="yes">t</italic> as epoch and <italic toggle="yes">n</italic> as batch size) is used to delay its convergence. In later stages, WIoU reduces gradients for low-quality anchors and shifts attention toward mid-quality ones to enhance localization. Hyperparameters <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> are set to 1.9 and 3, respectively. The parameter dynamics are illustrated in <xref rid="sensors-25-05879-f007" ref-type="fig">Figure 7</xref>.</p></sec><sec id="sec3dot6-sensors-25-05879"><title>3.6. Module Synergy Analysis</title><p>The four proposed components&#8212;Triplet Attention (TA), Triple Feature Encoding (TFE), P2 detection head, and Wise-IoU (WIoU) loss&#8212;work synergistically to address the core challenges of small and blurred object detection.</p><sec id="sec3dot6dot1-sensors-25-05879"><title>3.6.1. TA and TFE: Enhanced Feature Representation</title><p>The TA mechanism amplifies crucial spatial and channel-wise features, enhancing fine-grained patterns essential for identifying indistinct targets like crosswalks. The TFE module directly benefits from this refined input. By concatenating multi-scale features, it effectively fuses high-resolution details with deep semantic context. This ensures the detection heads receive feature maps rich in salient information for accurate detection.</p></sec><sec id="sec3dot6dot2-sensors-25-05879"><title>3.6.2. TFE and P2 Head: Precision for Small Targets</title><p>The TFE module&#8217;s emphasis on large-scale feature maps is perfectly complemented by the high-resolution (<inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>160</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>160</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) P2 detection head. This combination creates a dedicated pathway for small objects. TFE provides a robust, multi-scale feature set, mitigating information loss, while the P2 head leverages this to achieve precise localization and classification of targets as small as <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels, a task challenging for standard heads.</p></sec><sec id="sec3dot6dot3-sensors-25-05879"><title>3.6.3. WIoU Loss: Robust Learning</title><p>The high-quality features produced by TA and TFE provide clearer signals for hard samples (small, occluded, or blurred objects). The WIoU loss capitalizes on this by dynamically assigning higher gradient gains to lower-quality samples (higher <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>), focusing the learning process in these challenging cases. This synergy between superior feature encoding and a focused loss function significantly enhances model robustness and generalization.</p></sec><sec id="sec3dot6dot4-sensors-25-05879"><title>3.6.4. Holistic Integration</title><p>This cohesive design ensures TFP-YOLO operates as an integrated system, not merely a collection of parts. Each component amplifies the others&#8217; effectiveness, resulting in superior detection performance critical for machine guide dog navigation.</p></sec></sec></sec><sec id="sec4-sensors-25-05879"><title>4. Experiment and Result Analysis</title><sec id="sec4dot1-sensors-25-05879"><title>4.1. Dataset</title><p>This study utilizes both publicly available datasets and a custom-made dataset. The publicly available dataset consists of a subset of the RoadSign dataset that meets the requirements for small object detection, totaling 877 images, including 701 training images and 176 validation images. These images cover four common types of traffic signs: crosswalk, traffic light, stop, and speed-limit.</p><p>The TSDD dataset [<xref rid="B31-sensors-25-05879" ref-type="bibr">31</xref>], a Chinese traffic sign dataset released by the National Natural Science Foundation of China, comprises 10,000 street scene images containing nearly 30,000 traffic signs of various types. The images were collected under diverse conditions including different times, weather, lighting, and motion blur, and are all original images. From these, 2000 images were selected as the test set.</p><p>The GTSDB dataset [<xref rid="B32-sensors-25-05879" ref-type="bibr">32</xref>] consists of street scene images captured under various conditions in Germany, with a total of 900 images and over 2000 annotated objects. The signs are categorized into mandatory signs, prohibitory signs, danger signs, and others. From this dataset, 600 images were chosen as the test set.</p><p>The custom dataset was collected by a camera on sidewalks in Beijing. After recording videos, frames were extracted using a Python script, with an image resolution of 640 &#215; 640, covering various weather conditions such as sunny, rainy, and snowy days. The dataset includes eight types of typical obstacles and traffic signs, including pedestrians, cars, bicycles, crosswalks, traffic lights, poles, and traffic cones. The data is divided into a training set (8929 images), a validation set (1078 images), and a test set in a 0.8:0.1:0.1 ratio, with the latter two sets being mutually independent. Some samples and annotations are shown in <xref rid="sensors-25-05879-f008" ref-type="fig">Figure 8</xref>.</p></sec><sec id="sec4dot2-sensors-25-05879"><title>4.2. Experimental Environment and Evaluation Indicators</title><sec id="sec4dot2dot1-sensors-25-05879"><title>4.2.1. Experimental Environment Setting</title><p>This experiment was conducted on an Ubuntu 22.04 platform using an NVIDIA RTX 4090 device with 24 GB of video memory. The training environment consisted of Python 3.10, PyTorch 2.1.0, and CUDA 12.1.</p></sec><sec id="sec4dot2dot2-sensors-25-05879"><title>4.2.2. Experiment Parameter Settings</title><p>During training for urban street scene recognition, the input sample size of the model was converted to 640 &#215; 640, the batch size was set to 16, the SGD optimizer was used, the epoch was set to 200, the learning rate was 0.01, the momentum was 0.937, and the weight decay coefficient was 0.0005. Data augmentation techniques included random horizontal flipping, random scaling, mosaic augmentation, and mixup. The specific parameter settings are shown in <xref rid="sensors-25-05879-t002" ref-type="table">Table 2</xref>.</p><p>This paper evaluates the model using precision (<italic toggle="yes">P</italic>), recall (<italic toggle="yes">R</italic>), mean average precision (mAP), mAP50, parameter count, and FPS, where higher values indicate better performance. The metrics are calculated as:<disp-formula id="FD10-sensors-25-05879"><label>(10)</label><mml:math id="mm82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05879"><label>(11)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05879"><label>(12)</label><mml:math id="mm84" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05879"><label>(13)</label><mml:math id="mm85" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent true positives, false positives, and false negatives, respectively. mAP50 computes the average <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> at IoU = 0.5, while FPS measures processing speed in frames per second.</p></sec></sec></sec><sec id="sec5-sensors-25-05879"><title>5. Result and Discussion</title><sec id="sec5dot1-sensors-25-05879"><title>5.1. Comparison Experiment of Different Weight Versions of YOLOv8</title><p>Pre-trained weights provide valuable prior knowledge for object detection models through transfer learning, accelerating convergence and improving performance. Their effectiveness varies across different weighting schemes and datasets. <xref rid="sensors-25-05879-t003" ref-type="table">Table 3</xref> shows the detection performance of YOLOv8 with different weights for obstacles and traffic signs.</p><p><xref rid="sensors-25-05879-t003" ref-type="table">Table 3</xref> shows that the weighting model affects detection accuracy, speed, and parameter count. YOLOv8n offers the best balance with minimal parameters (3.01M) and the fastest speed, making it ideal for guide robot perception. We therefore select YOLOv8n as our base model.</p></sec><sec id="sec5dot2-sensors-25-05879"><title>5.2. Attention Mechanism Comparison Experiment</title><p>To validate the superiority of our Triplet Attention module, we compared it against CBAM, SE, CA, and EMA mechanisms in YOLOv8. As shown in <xref rid="sensors-25-05879-t004" ref-type="table">Table 4</xref>, Triplet Attention outperforms others by capturing cross-dimensional interactions more efficiently, enhancing local feature relevance and channel relationships while maintaining lightweight design.</p><p><xref rid="sensors-25-05879-t004" ref-type="table">Table 4</xref> compares different attention mechanisms in YOLOv8n. While EMA shows good performance, Triplet Attention achieves superior results with 91.4% mAP0.5, 91.3% precision, and 87.2% recall.</p></sec><sec id="sec5dot3-sensors-25-05879"><title>5.3. Loss Function Comparison Experiment</title><p>To validate the improved YOLOv8n&#8217;s detection performance, we compared various IoU metrics (Wise-IoUv3, CIoU, DIoU, EIoU, SIoU) as shown in <xref rid="sensors-25-05879-t005" ref-type="table">Table 5</xref>. For vehicle detection tasks with imbalanced categories (e.g., pedestrians) and small targets (e.g., traffic lights), Wise-IoUv3 addresses these challenges through its dynamic weighting mechanism. It assigns larger weights to small categories and neglected regions, preventing model bias toward dominant categories while improving small object detection.</p><p><xref rid="sensors-25-05879-t005" ref-type="table">Table 5</xref> shows that the enhanced YOLOv8n (with Triplet Attention, TFE, and P2) achieves 92.1% mAP0.5 (90.8% P, 86.3% R) using CIoU. Among tested WIoUv3 variants, the configuration with <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> yields optimal performance, 93.9% mAP0.5 (90.2% P, 89.8% R), surpassing other loss functions.</p></sec><sec id="sec5dot4-sensors-25-05879"><title>5.4. Analysis of Object Detection Results</title><p>To validate the detection capability of the proposed algorithm model in complex road scenarios, experimental verification was conducted on two public datasets. <xref rid="sensors-25-05879-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-05879-t007" ref-type="table">Table 7</xref> present the comparative detection results between the YOLOv8 model and the proposed model on the validation sets of these two public datasets. Experimental results demonstrate that on the TSDD dataset, the proposed model achieves improvements of 5.3 percentage points in mAP50 and 7.6 percentage points in mAP50:95. Similarly, on the GTSDB dataset, it shows improvements of 5.7 percentage points in mAP50 and 3.4 percentage points in mAP50:95.</p></sec><sec id="sec5dot5-sensors-25-05879"><title>5.5. Ablation Experiment</title><p>To validate the effectiveness of each module in enhancing the Yolov8n network, ablation experiments were conducted using Yolov8n as the baseline. Metrics included average precision, accuracy, recall, and parameter count, as shown in <xref rid="sensors-25-05879-t008" ref-type="table">Table 8</xref> (where &#10003; indicates inclusion of a module). Results show that each added module improved performance to varying degrees. The Wise-IoUv3 module achieved the highest AP gain of 1.7% without increasing parameters, demonstrating its effectiveness in improving bounding box localization for varied object shapes and scales. The P2 module enhanced AP by 1.1%, particularly benefiting small target detection such as traffic lights. Although combining P2 and Wise-IoUv3 yielded slightly lower AP than Wise-IoUv3 alone, it reduced parameters to 2.74 M. The full integration of Triplet Attention, TFE, P2, and Wise-IoUv3 boosted Yolov8n&#8217;s accuracy to 93.9%, with 2.49 M parameters and 182 FPS.</p><p>The results presented in <xref rid="sensors-25-05879-t006" ref-type="table">Table 6</xref> not only demonstrate the performance improvements achieved by each proposed module but also reveal their distinct effect sizes and underlying mechanisms.</p><p>The Triplet Attention (TA) module boosts performance (+1.6% mAP) by enhancing cross-dimensional spatial&#8211;channel interactions, which is critical for recognizing objects with weak textual cues (e.g., crosswalks), as reflected in the increased recall.</p><p>The TFE module&#8217;s primary role is to aggregate and preserve multi-scale features, supplying richer representations for the detection heads. Its effect is most evident when combined with the P2 head.</p><p>The P2 detection head provides a substantial gain (+1.1% mAP) by leveraging high-resolution (160 &#215; 160) features. This is decisive for small objects (e.g., traffic lights), as it drastically improves localization precision for targets below 8 &#215; 8 pixels.</p><p>The Wise-IoU v3 (WIoU) loss brings the largest individual improvement (+1.7% mAP) by introducing a dynamic focusing mechanism that suppresses gradients from low-quality examples, thereby improving generalization and robustness.</p><p>The full model&#8217;s performance (93.9% mAP) demonstrates clear synergy: TFE provides multi-scale features, TA refines their representation, P2 detects small objects precisely, and WIoU ensures stable training. This integration achieves an optimal accuracy&#8211;efficiency balance.</p></sec><sec id="sec5dot6-sensors-25-05879"><title>5.6. Comparison of Different Models</title><p>We compared mainstream models including YOLOv5n, YOLOv6n, and YOLOv10n in terms of parameter count, average precision, accuracy, and FPS to evaluate the performance of the proposed model, as summarized in <xref rid="sensors-25-05879-t009" ref-type="table">Table 9</xref>.</p><p>As shown in <xref rid="sensors-25-05879-t009" ref-type="table">Table 9</xref>, although YOLOv6n slightly surpasses YOLOv8n in average accuracy, it suffers from significantly lower FPS. Models such as YOLOv5n, YOLOv10n, and YOLOv11n achieve comparable accuracy but with reduced speed. YOLOv7 and Faster R-CNN not only have much larger parameter counts but also perform worse in both accuracy and FPS.</p><p>Overall, YOLOv8n offers a better balance of accuracy, speed, and model size than other unmodified models. The proposed improved model further reduces parameters by 0.52M, boosts accuracy by 4.1%, and achieves higher precision and FPS, validating the effectiveness of our approach.</p></sec><sec id="sec5dot7-sensors-25-05879"><title>5.7. Algorithm Verification</title><p>The improved algorithm was compared with YOLOv8n, YOLOv10n, and YOLOv11n&#8212;top performers in average accuracy&#8212;to evaluate detection on typical obstacles for visually impaired users, including crosswalks and traffic signs under various scenarios and weather conditions. Results are shown in <xref rid="sensors-25-05879-f009" ref-type="fig">Figure 9</xref>.</p><p>The improved algorithm reduces false negatives for small targets like traffic lights and lowers false positives for blurred-edge targets compared to the other three algorithms. This enhances obstacle and traffic sign detection for guide dogs in complex environments, improving local path planning accuracy and reliability and laying a solid foundation for future research.</p></sec><sec id="sec5dot8-sensors-25-05879"><title>5.8. Implementation of YOLO Framework on NVIDIA Jetson Orin Nano Super</title><p>Today, single-board computers like the Nvidia Jetson Orin Nano Super are gaining popularity for edge computing applications, including artificial intelligence and deep learning. The Jetson Orin Nano Super, featuring a 6-core Arm<sup>&#174;</sup> Cortex<sup>&#174;</sup>-A78AE v8.2 64-bit CPU, a 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores, an 8 GB 128-bit LPDDR5 RAM offering 102 GB/s bandwidth, and comprehensive high-speed I/O support, delivers outstanding AI computational performance up to 67 TOPS with remarkable power efficiency. In this study, we leverage the benefits of this embedded edge computing device specifically for model reasoning, coupled with an Intel RealSense D435i camera to form a comprehensive perception system. Given that model training demands more powerful computing resources than the testing process, we perform dedicated model training tasks on a GPU-equipped workstation in the cloud. The generated weight files are deployed on the edge device, where the Jetson Orin Nano Super processes real-time visual data from the D435i camera to perform efficient obstacle and traffic sign detection in various environmental conditions.</p><p>To validate the practical deployment of the proposed system, we established a complete experimental setup utilizing the Jetson Orin Nano Super as the central processing unit and the Intel RealSense D435i camera as the visual perception module. In this configuration, the D435i camera serves as the &#8220;eyes&#8221; of the machine guide dog, continuously capturing RGB video streams at 1280 &#215; 720 resolution with a frame rate of 30 FPS. The Jetson Orin Nano Super functions as the main controller, executing real-time inference using the optimized YOLO model. The physical implementation of this hardware system is illustrated in <xref rid="sensors-25-05879-f010" ref-type="fig">Figure 10</xref>.</p><p>The experimental results demonstrate that the integrated system achieves outstanding performance metrics: a detection precision of 94.2% and a recall rate of 91.6%, indicating high accuracy and reliability in obstacle and traffic sign recognition. The system maintains an average processing throughput of 28.7 FPS, ensuring smooth real-time operation. The per-frame inference latency remains below 50 ms, providing responsive feedback for navigation assistance. Regarding resource utilization, the GPU occupancy is approximately 42%, with memory consumption of 3.1 GB, indicating efficient resource management. The total power consumption is controlled at 8.7 W, demonstrating the energy efficiency of the edge deployment. All performance indicators meet the expected targets for real-world machine guide dog applications, validating the effectiveness of the proposed hardware&#8211;software co-design approach. The detailed performance metrics are systematically summarized in <xref rid="sensors-25-05879-t010" ref-type="table">Table 10</xref>.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05879"><title>6. Conclusions</title><p>To address the challenges of detecting small targets (e.g., traffic lights) and blurred-edge features (e.g., crosswalks) commonly encountered by guide dogs in complex environments, this paper proposes an improved YOLOv8-based detection model. Triplet Attention is integrated into the backbone to enhance local feature and channel modeling. The TFE module is introduced to fuse multi-scale and spatial information, boosting small target extraction. A P2 detection head further improves detection of small objects, while the WIoU loss optimizes bounding box regression, improving robustness against low-quality samples. Experiments demonstrate that the improved model achieves 93.9% mAP and 90.2% precision with a 17.2% reduction in parameters. Future work will focus on lightweight strategies like pruning and quantization to enable efficient deployment on guide dog systems in real-world scenarios.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Z.Z.; software, Z.Z. and F.J.; data curation, Z.Z.; funding acquisition, J.C.; writing&#8212;original draft preparation, Z.Z. and F.J.; writing&#8212;review and editing, J.C. and F.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset of this article can be downloaded at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/zzw0709/TFP-YOLO">https://github.com/zzw0709/TFP-YOLO</uri> (accessed on 16 August 2025). The source code of TFP-YOLO can be obtained from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05879"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhuo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>Trends and projections of the burden of visual impairment in Asia: Findings from the Global Burden of Disease Study 2021</article-title><source>Asia-Pac. J. Ophthalmol.</source><year>2025</year><volume>14</volume><fpage>100196</fpage><pub-id pub-id-type="doi">10.1016/j.apjo.2025.100196</pub-id><pub-id pub-id-type="pmid">40187497</pub-id></element-citation></ref><ref id="B2-sensors-25-05879"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sreenath</surname><given-names>K.</given-names></name></person-group><article-title>Robotic Guide Dog: Leading a Human with Leash-Guided Hybrid Physical Interaction</article-title><source>Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>30 May&#8211;5 June 2021</conf-date><fpage>11470</fpage><lpage>11476</lpage></element-citation></ref><ref id="B3-sensors-25-05879"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Viola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>M.J.</given-names></name></person-group><article-title>Robust real-time face detection</article-title><source>Int. J. Comput. Vis.</source><year>2004</year><volume>57</volume><fpage>137</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000013087.49260.fb</pub-id></element-citation></ref><ref id="B4-sensors-25-05879"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Viola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>M.</given-names></name></person-group><article-title>Rapid object detection using a boosted cascade of simple features</article-title><source>Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</source><conf-loc>Kauai, HI, USA</conf-loc><conf-date>8&#8211;14 December 2001</conf-date><volume>Volume 1</volume><fpage>I</fpage></element-citation></ref><ref id="B5-sensors-25-05879"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name></person-group><article-title>Regionlets for generic object detection</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Sydney, Australia</conf-loc><conf-date>1&#8211;8 December 2013</conf-date><fpage>17</fpage><lpage>24</lpage></element-citation></ref><ref id="B6-sensors-25-05879"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Felzenszwalb</surname><given-names>P.</given-names></name><name name-style="western"><surname>McAllester</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ramanan</surname><given-names>D.</given-names></name></person-group><article-title>A discriminatively trained, multiscale, deformable part model</article-title><source>Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>24&#8211;26 June 2008</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B7-sensors-25-05879"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Donahue</surname><given-names>J.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>J.</given-names></name></person-group><article-title>Rich feature hierarchies for accurate object detection and semantic segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Columbus, OH, USA</conf-loc><conf-date>23&#8211;28 June 2014</conf-date><fpage>580</fpage><lpage>587</lpage></element-citation></ref><ref id="B8-sensors-25-05879"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Antun</surname><given-names>V.</given-names></name><name name-style="western"><surname>Renna</surname><given-names>F.</given-names></name><name name-style="western"><surname>Poon</surname><given-names>C.</given-names></name><name name-style="western"><surname>Adcock</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hansen</surname><given-names>A.C.</given-names></name></person-group><article-title>On instabilities of deep learning in image reconstruction and the potential costs of AI</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2020</year><volume>117</volume><fpage>30088</fpage><lpage>30095</lpage><pub-id pub-id-type="doi">10.1073/pnas.1907377117</pub-id><pub-id pub-id-type="pmid">32393633</pub-id><pub-id pub-id-type="pmcid">PMC7720232</pub-id></element-citation></ref><ref id="B9-sensors-25-05879"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Fast R-CNN</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Santiago, Chile</conf-loc><conf-date>7&#8211;13 December 2015</conf-date><fpage>1440</fpage><lpage>1448</lpage></element-citation></ref><ref id="B10-sensors-25-05879"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>39</volume><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="B11-sensors-25-05879"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name></person-group><article-title>SSD: Single shot multibox detector</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#8211;14 October 2016</conf-date><fpage>21</fpage><lpage>37</lpage></element-citation></ref><ref id="B12-sensors-25-05879"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified, real-time object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="B13-sensors-25-05879"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>YOLO9000: Better, faster, stronger</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>6517</fpage><lpage>6525</lpage></element-citation></ref><ref id="B14-sensors-25-05879"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>YOLOv3: An incremental improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B15-sensors-25-05879"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><article-title>Yolov4: Optimal speed and accuracy of object detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id><pub-id pub-id-type="arxiv">2004.10934</pub-id></element-citation></ref><ref id="B16-sensors-25-05879"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Weng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ke</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>W.</given-names></name><etal/></person-group><article-title>YOLOv6: A single-stage object detection framework for industrial applications</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2209.02976</pub-id><pub-id pub-id-type="arxiv">2209.02976</pub-id></element-citation></ref><ref id="B17-sensors-25-05879"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><article-title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real time object detectors</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>7464</fpage><lpage>7475</lpage></element-citation></ref><ref id="B18-sensors-25-05879"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>F.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name></person-group><article-title>Improved target detection algorithm based on YOLO</article-title><source>Proceedings of the 2021 4th International Conference on Robotics, Control and Automation Engineering (RCAE)</source><conf-loc>Wuhan, China</conf-loc><conf-date>4&#8211;6 November 2021</conf-date><fpage>21</fpage><lpage>25</lpage></element-citation></ref><ref id="B19-sensors-25-05879"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name></person-group><article-title>Road object detection of YOLO algorithm with attention mechanism</article-title><source>Front. Signal Process.</source><year>2021</year><volume>5</volume><fpage>9</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.22606/fsp.2021.51002</pub-id></element-citation></ref><ref id="B20-sensors-25-05879"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name></person-group><article-title>Feature selection module for CNN based object detector</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>69456</fpage><lpage>69466</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3073565</pub-id></element-citation></ref><ref id="B21-sensors-25-05879"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ju</surname><given-names>M.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>H.</given-names></name></person-group><article-title>Adaptive feature fusion with attention mechanism for multi-scale target detection</article-title><source>Neural Comput. Appl.</source><year>2021</year><volume>33</volume><fpage>2769</fpage><lpage>2781</lpage><pub-id pub-id-type="doi">10.1007/s00521-020-05150-9</pub-id></element-citation></ref><ref id="B22-sensors-25-05879"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>ETSR-YOLO: An improved multi-scale traffic sign detection algorithm based on YOLOv5</article-title><source>PLoS ONE</source><year>2023</year><volume>18</volume><elocation-id>e0295807</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0295807</pub-id><pub-id pub-id-type="pmid">38096147</pub-id><pub-id pub-id-type="pmcid">PMC10721062</pub-id></element-citation></ref><ref id="B23-sensors-25-05879"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Du</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>N.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>TSD-YOLO: Small traffic sign detection based on improved YOLO v8</article-title><source>IET Image Process.</source><year>2024</year><volume>18</volume><fpage>2884</fpage><lpage>2898</lpage><pub-id pub-id-type="doi">10.1049/ipr2.13141</pub-id></element-citation></ref><ref id="B24-sensors-25-05879"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ji</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name></person-group><article-title>DP-YOLO: A lightweight traffic sign detection model for small object detection</article-title><source>Digit. Signal Process.</source><year>2025</year><volume>165</volume><fpage>105311</fpage><pub-id pub-id-type="doi">10.1016/j.dsp.2025.105311</pub-id></element-citation></ref><ref id="B25-sensors-25-05879"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khalili</surname><given-names>B.</given-names></name><name name-style="western"><surname>Smyth</surname><given-names>A.W.</given-names></name></person-group><article-title>SOD-YOLOv8&#8212;Enhancing YOLOv8 for small object detection in aerial imagery and traffic scenes</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6209</elocation-id><pub-id pub-id-type="doi">10.3390/s24196209</pub-id><pub-id pub-id-type="pmid">39409249</pub-id><pub-id pub-id-type="pmcid">PMC11478522</pub-id></element-citation></ref><ref id="B26-sensors-25-05879"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Su</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>CAS-YOLOv8: Improving urban remote sensing object detection with context-aware mechanisms</article-title><source>Proceedings of the International Conference on Smart Transportation and City Engineering (STCE 2024)</source><conf-loc>Chongqing, China</conf-loc><conf-date>6&#8211;8 December 2024</conf-date><volume>Volume 13575</volume><fpage>135754X</fpage><pub-id pub-id-type="doi">10.1117/12.3062014</pub-id></element-citation></ref><ref id="B27-sensors-25-05879"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hart</surname><given-names>P.E.</given-names></name><name name-style="western"><surname>Nilsson</surname><given-names>N.J.</given-names></name><name name-style="western"><surname>Raphael</surname><given-names>B.</given-names></name></person-group><article-title>A Formal Basis for the Heuristic Determination of Minimum Cost Paths</article-title><source>IEEE Trans. Syst. Sci. Cybern.</source><year>1968</year><volume>4</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1109/TSSC.1968.300136</pub-id></element-citation></ref><ref id="B28-sensors-25-05879"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dijkstra</surname><given-names>E.W.</given-names></name></person-group><article-title>A note on two problems in connexion with graphs</article-title><source>Numer. Math.</source><year>1959</year><volume>1</volume><fpage>269</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1007/BF01386390</pub-id></element-citation></ref><ref id="B29-sensors-25-05879"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fox</surname><given-names>D.</given-names></name><name name-style="western"><surname>Burgard</surname><given-names>W.</given-names></name><name name-style="western"><surname>Thrun</surname><given-names>S.</given-names></name></person-group><article-title>The dynamic window approach to collision avoidance</article-title><source>IEEE Robot. Autom. Mag.</source><year>1997</year><volume>4</volume><fpage>23</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1109/100.580977</pub-id></element-citation></ref><ref id="B30-sensors-25-05879"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wubshet</surname><given-names>A.</given-names></name><name name-style="western"><surname>Menebo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Negash</surname><given-names>L.</given-names></name><name name-style="western"><surname>Abdissa</surname><given-names>C.M.</given-names></name></person-group><article-title>Solving Optimal Path Planning Problem of an Intelligent Mobile Robot in Dynamic Environment Using Bidirectional Rapidly-exploring Random Tree Star-Dynamic Window Approach (BRRT*-DWA) with Adaptive Monte Carlo Localization (AMCL)</article-title><source>TechRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.36227/techrxiv.24623784.v1</pub-id></element-citation></ref><ref id="B31-sensors-25-05879"><label>31.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>L.L.</given-names></name><name name-style="western"><surname>Prof</surname><given-names>P.D.</given-names></name></person-group><article-title>Traffic Sign Detection Database. National Nature Science Foundation of China (NSFC). [EB/OL]</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://nlpr.ia.ac.cn/pal/trafficdata/detection.html" ext-link-type="uri">https://nlpr.ia.ac.cn/pal/trafficdata/detection.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-16">(accessed on 16 September 2025)</date-in-citation></element-citation></ref><ref id="B32-sensors-25-05879"><label>32.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>German traffic sign detection benchmark (GTSDB)</collab></person-group><article-title>In Proceedings of the IEEE International Joint Conference on Neural Networks, Brisbane, Australia, 10&#8211;15 June 2012; [EB/OL]</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://benchmark.ini.rub.de/gtsdb_dataset.html" ext-link-type="uri">https://benchmark.ini.rub.de/gtsdb_dataset.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-16">(accessed on 16 September 2025)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05879-f001" orientation="portrait"><label>Figure 1</label><caption><p>Schematic diagram of robot guide dog.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g001.jpg"/></fig><fig position="float" id="sensors-25-05879-f002" orientation="portrait"><label>Figure 2</label><caption><p>Architecture of proposed TFP-YOLO. The backbone network integrates a series of Conv and C2F modules to extract initial features, enhanced by a Triplet Attention mechanism. The neck module employs C2F and SPPF structures, incorporating Bottleneck and Concat operations to fuse multi-scale features, with upsampling and shortcut connections for improved information flow. The detection head utilizes multiple detect layers (P2&#8211;P5) to predict bounding boxes and class probabilities, optimized by Bbox Loss and Cls Loss.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g002.jpg"/></fig><fig position="float" id="sensors-25-05879-f003" orientation="portrait"><label>Figure 3</label><caption><p>Diagram of triplet attention.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g003.jpg"/></fig><fig position="float" id="sensors-25-05879-f004" orientation="portrait"><label>Figure 4</label><caption><p>Network topology diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g004.jpg"/></fig><fig position="float" id="sensors-25-05879-f005" orientation="portrait"><label>Figure 5</label><caption><p>The structure of TFE module. C represents the number of channels and S represents the feature map size. Each Triple Feature Encoder module uses three feature maps of different sizes as input.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g005.jpg"/></fig><fig position="float" id="sensors-25-05879-f006" orientation="portrait"><label>Figure 6</label><caption><p>Add P2 detection head.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g006.jpg"/></fig><fig position="float" id="sensors-25-05879-f007" orientation="portrait"><label>Figure 7</label><caption><p>Description of WIoU.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g007.jpg"/></fig><fig position="float" id="sensors-25-05879-f008" orientation="portrait"><label>Figure 8</label><caption><p>Dataset partial sample.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g008.jpg"/></fig><fig position="float" id="sensors-25-05879-f009" orientation="portrait"><label>Figure 9</label><caption><p>Comparison of different methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g009.jpg"/></fig><fig position="float" id="sensors-25-05879-f010" orientation="portrait"><label>Figure 10</label><caption><p>Implementation of the obstacle detection system on NVIDIA Jetson platform: (<bold>a</bold>) Jetson Orin Nano Super developer kit and Intel RealSense D435i depth camera; (<bold>b</bold>) real-time system performance monitoring.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05879-g010.jpg"/></fig><table-wrap position="float" id="sensors-25-05879-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t001_Table 1</object-id><label>Table 1</label><caption><p>Detection head dimension table.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Detection Head</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P5</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P4</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P3</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P2 (New)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Feature map size</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>40</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>80</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>160</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>160</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Target size</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8805;<inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>32</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8805;<inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>16</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8805;<inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8805;<inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t002_Table 2</object-id><label>Table 2</label><caption><p>Training parameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter Name</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter Value</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Input size</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Initial learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Minimum learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0001</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Epochs</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">SGD</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Momentum</td><td align="center" valign="middle" rowspan="1" colspan="1">0.937</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Weight decay</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0005</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Number of threads</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ratio</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.75</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t003_Table 3</object-id><label>Table 3</label><caption><p>Detection results of different weight models of Yolov8.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP0.5/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8n</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">87.7</td><td align="center" valign="middle" rowspan="1" colspan="1">85.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.01</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4</td><td align="center" valign="middle" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" rowspan="1" colspan="1">89.1</td><td align="center" valign="middle" rowspan="1" colspan="1">11.21</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8m</td><td align="center" valign="middle" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" rowspan="1" colspan="1">92.8</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td><td align="center" valign="middle" rowspan="1" colspan="1">23.64</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8l</td><td align="center" valign="middle" rowspan="1" colspan="1">92.9</td><td align="center" valign="middle" rowspan="1" colspan="1">92.6</td><td align="center" valign="middle" rowspan="1" colspan="1">87.9</td><td align="center" valign="middle" rowspan="1" colspan="1">42.13</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv8x</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.46</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of different attention mechanisms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model (YOLOv8n)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP0.5/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R/%</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CBAM</td><td align="center" valign="middle" rowspan="1" colspan="1">90.7</td><td align="center" valign="middle" rowspan="1" colspan="1">89.6</td><td align="center" valign="middle" rowspan="1" colspan="1">86.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SE</td><td align="center" valign="middle" rowspan="1" colspan="1">90.6</td><td align="center" valign="middle" rowspan="1" colspan="1">89.4</td><td align="center" valign="middle" rowspan="1" colspan="1">85.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CA</td><td align="center" valign="middle" rowspan="1" colspan="1">90.7</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">86.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EMA</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">90.3</td><td align="center" valign="middle" rowspan="1" colspan="1">84.8</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Triplet Attention</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of different loss functions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model (YOLOv8n + TA + T + P)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP0.5/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R/%</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CIoU</td><td align="center" valign="middle" rowspan="1" colspan="1">92.1</td><td align="center" valign="middle" rowspan="1" colspan="1">90.8</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DIoU</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7</td><td align="center" valign="middle" rowspan="1" colspan="1">90.0</td><td align="center" valign="middle" rowspan="1" colspan="1">86.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EIoU</td><td align="center" valign="middle" rowspan="1" colspan="1">92.0</td><td align="center" valign="middle" rowspan="1" colspan="1">90.5</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SIoU</td><td align="center" valign="middle" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" rowspan="1" colspan="1">90.6</td><td align="center" valign="middle" rowspan="1" colspan="1">86.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WIoUv3 (<inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">93.8</td><td align="center" valign="middle" rowspan="1" colspan="1">89.9</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WIoUv3 (<inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">93.4</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td><td align="center" valign="middle" rowspan="1" colspan="1">87.4</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WIoUv3 (<inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of model performance before and after improvement on the TSDD dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Category</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">YOLOv8n</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Proposed Model</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50/%</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50:95/%</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50/%</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50:95/%</bold>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Traffic sign</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td><td align="center" valign="middle" rowspan="1" colspan="1">45.2</td><td align="center" valign="middle" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" rowspan="1" colspan="1">52.8</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of model performance before and after improvement on the GTSDB dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Category</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">YOLOv8n</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Proposed Model</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50/%</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50:95/%</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50/%</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP50:95/%</bold>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Mandatory</td><td align="center" valign="middle" rowspan="1" colspan="1">82.8</td><td align="center" valign="middle" rowspan="1" colspan="1">61.9</td><td align="center" valign="middle" rowspan="1" colspan="1">88.5</td><td align="center" valign="middle" rowspan="1" colspan="1">62.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Prohibitory</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" rowspan="1" colspan="1">73.0</td><td align="center" valign="middle" rowspan="1" colspan="1">96.8</td><td align="center" valign="middle" rowspan="1" colspan="1">75.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Danger</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">63.9</td><td align="center" valign="middle" rowspan="1" colspan="1">97.1</td><td align="center" valign="middle" rowspan="1" colspan="1">72.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Others</td><td align="center" valign="middle" rowspan="1" colspan="1">80.4</td><td align="center" valign="middle" rowspan="1" colspan="1">59.0</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td><td align="center" valign="middle" rowspan="1" colspan="1">61.6</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.9</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t008_Table 8</object-id><label>Table 8</label><caption><p>Ablation experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">TA</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">TFE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">WIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">v8n</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">87.7</td><td align="center" valign="middle" rowspan="1" colspan="1">85.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.01</td><td align="center" valign="middle" rowspan="1" colspan="1">176</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" rowspan="1" colspan="1">91.3</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.05</td><td align="center" valign="middle" rowspan="1" colspan="1">170</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1"/><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">90.6</td><td align="center" valign="middle" rowspan="1" colspan="1">87.8</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">2.91</td><td align="center" valign="middle" rowspan="1" colspan="1">179</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">90.9</td><td align="center" valign="middle" rowspan="1" colspan="1">88.1</td><td align="center" valign="middle" rowspan="1" colspan="1">85.4</td><td align="center" valign="middle" rowspan="1" colspan="1">3.01</td><td align="center" valign="middle" rowspan="1" colspan="1">184</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">87.8</td><td align="center" valign="middle" rowspan="1" colspan="1">3.01</td><td align="center" valign="middle" rowspan="1" colspan="1">176</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" rowspan="1" colspan="1">86.4</td><td align="center" valign="middle" rowspan="1" colspan="1">2.49</td><td align="center" valign="middle" rowspan="1" colspan="1">173</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">85.8</td><td align="center" valign="middle" rowspan="1" colspan="1">2.74</td><td align="center" valign="middle" rowspan="1" colspan="1">190</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.8</td><td align="center" valign="middle" rowspan="1" colspan="1">90.1</td><td align="center" valign="middle" rowspan="1" colspan="1">88.0</td><td align="center" valign="middle" rowspan="1" colspan="1">2.49</td><td align="center" valign="middle" rowspan="1" colspan="1">188</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">182</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t009_Table 9</object-id><label>Table 9</label><caption><p>Comparison of different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP@0.5/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P/%</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv5n</td><td align="center" valign="middle" rowspan="1" colspan="1">89.4</td><td align="center" valign="middle" rowspan="1" colspan="1">88.6</td><td align="center" valign="middle" rowspan="1" colspan="1">2.51</td><td align="center" valign="middle" rowspan="1" colspan="1">7.4</td><td align="center" valign="middle" rowspan="1" colspan="1">162</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv6n</td><td align="center" valign="middle" rowspan="1" colspan="1">90.0</td><td align="center" valign="middle" rowspan="1" colspan="1">87.6</td><td align="center" valign="middle" rowspan="1" colspan="1">4.23</td><td align="center" valign="middle" rowspan="1" colspan="1">48.9</td><td align="center" valign="middle" rowspan="1" colspan="1">74</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv7</td><td align="center" valign="middle" rowspan="1" colspan="1">89.6</td><td align="center" valign="middle" rowspan="1" colspan="1">89.9</td><td align="center" valign="middle" rowspan="1" colspan="1">37.2</td><td align="center" valign="middle" rowspan="1" colspan="1">19.2</td><td align="center" valign="middle" rowspan="1" colspan="1">142</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8n</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">87.7</td><td align="center" valign="middle" rowspan="1" colspan="1">3.01</td><td align="center" valign="middle" rowspan="1" colspan="1">18.0</td><td align="center" valign="middle" rowspan="1" colspan="1">176</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10n</td><td align="center" valign="middle" rowspan="1" colspan="1">88.6</td><td align="center" valign="middle" rowspan="1" colspan="1">86.8</td><td align="center" valign="middle" rowspan="1" colspan="1">2.71</td><td align="center" valign="middle" rowspan="1" colspan="1">22.7</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv11n</td><td align="center" valign="middle" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" rowspan="1" colspan="1">85.0</td><td align="center" valign="middle" rowspan="1" colspan="1">2.58</td><td align="center" valign="middle" rowspan="1" colspan="1">23.5</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Faster R-CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">79.8</td><td align="center" valign="middle" rowspan="1" colspan="1">83.7</td><td align="center" valign="middle" rowspan="1" colspan="1">41.2</td><td align="center" valign="middle" rowspan="1" colspan="1">38.2</td><td align="center" valign="middle" rowspan="1" colspan="1">58</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MobileNet-SSD</td><td align="center" valign="middle" rowspan="1" colspan="1">78.6</td><td align="center" valign="middle" rowspan="1" colspan="1">81.2</td><td align="center" valign="middle" rowspan="1" colspan="1">12.13</td><td align="center" valign="middle" rowspan="1" colspan="1">24.7</td><td align="center" valign="middle" rowspan="1" colspan="1">86</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NanoDet</td><td align="center" valign="middle" rowspan="1" colspan="1">78.6</td><td align="center" valign="middle" rowspan="1" colspan="1">80.2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92</td><td align="center" valign="middle" rowspan="1" colspan="1">1.9</td><td align="center" valign="middle" rowspan="1" colspan="1">231</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">182</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05879-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05879-t010_Table 10</object-id><label>Table 10</label><caption><p>Performance metrics of the deployed system on Jetson Orin Nano Super.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Performance Metric</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Unit</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Target</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Detection Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">94.2</td><td align="center" valign="middle" rowspan="1" colspan="1">%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8805;90.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Recall Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" rowspan="1" colspan="1">%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8805;90.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Average FPS</td><td align="center" valign="middle" rowspan="1" colspan="1">28.7</td><td align="center" valign="middle" rowspan="1" colspan="1">FPS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8805;25.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Per-frame Latency</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;50</td><td align="center" valign="middle" rowspan="1" colspan="1">ms</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8804;50</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU Utilization</td><td align="center" valign="middle" rowspan="1" colspan="1">42.0</td><td align="center" valign="middle" rowspan="1" colspan="1">%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8804;60</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Memory Consumption</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1</td><td align="center" valign="middle" rowspan="1" colspan="1">GB</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8804;4.0</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Power Consumption</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">W</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8804;10.0</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>