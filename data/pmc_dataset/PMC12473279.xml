<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473279</article-id><article-id pub-id-type="pmcid-ver">PMC12473279.1</article-id><article-id pub-id-type="pmcaid">12473279</article-id><article-id pub-id-type="pmcaiid">12473279</article-id><article-id pub-id-type="pmid">41012872</article-id><article-id pub-id-type="doi">10.3390/s25185633</article-id><article-id pub-id-type="publisher-id">sensors-25-05633</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Lightweight Federated Learning Approach for Resource-Constrained Internet of Things</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3803-820X</contrib-id><name name-style="western"><surname>Baqer</surname><given-names initials="M">M.</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Ordieres Mer&#233;</surname><given-names initials="J">Joaquin</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05633">Department of Computer Engineering, College of Information Technology, University of Bahrain, Sakhair P.O. Box 32038, Bahrain; <email>mbaqer@uob.edu.bh</email></aff><pub-date pub-type="epub"><day>10</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5633</elocation-id><history><date date-type="received"><day>22</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>30</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>08</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>10</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the author.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05633.pdf"/><abstract><p>Federated learning is increasingly recognized as a viable solution for deploying distributed intelligence across resource-constrained platforms, including smartphones, wireless sensor networks, and smart home devices within the broader Internet of Things ecosystem. However, traditional federated learning approaches face serious challenges in resource-constrained settings due to high processing demands, substantial memory requirements, and high communication overhead, rendering them impractical for battery-powered IoT environments. These factors increase battery consumption and, consequently, decrease the operational longevity of the network. This study proposes a streamlined, single-shot federated learning approach that minimizes communication overhead, enhances energy efficiency, and thereby extends network lifetime. The proposed approach leverages the <italic toggle="yes">k</italic>-nearest neighbors (<italic toggle="yes">k</italic>-NN) algorithm for edge-level pattern recognition and utilizes majority voting at the server/base station to reach global pattern recognition consensus, thereby eliminating the need for data transmissions across multiple communication rounds to achieve classification accuracy. The results indicate that the proposed approach maintains competitive classification accuracy performance while significantly reducing the required number of communication rounds.</p></abstract><kwd-group><kwd>federated learning</kwd><kwd>internet of things</kwd><kwd>wireless sensor networks</kwd><kwd>artificial intelligence</kwd><kwd>machine learning</kwd><kwd>energy-efficient learning</kwd><kwd>nearest neighbor</kwd><kwd>pattern recognition</kwd><kwd>in-network processing</kwd><kwd>communication efficiency</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05633"><title>1. Introduction</title><p>Over the past few years, federated learning has become a widely adopted paradigm for distributed, collaborative machine learning, particularly suitable for privacy-sensitive applications [<xref rid="B1-sensors-25-05633" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05633" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05633" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05633" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05633" ref-type="bibr">5</xref>]. Originally introduced by Google, federated learning shifts the focus from centralized data processing to on-device computation, enabling devices to exchange only model updates rather than raw sensor data [<xref rid="B6-sensors-25-05633" ref-type="bibr">6</xref>]. In federated learning, devices collaboratively construct a global model by communicating locally computed model updates instead of exchanging raw data, thereby preserving user privacy. This process iteratively improves the global model while keeping raw data local, reducing the need to transmit large amounts of sensor data across the network to the server or base station [<xref rid="B1-sensors-25-05633" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05633" ref-type="bibr">2</xref>,<xref rid="B7-sensors-25-05633" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05633" ref-type="bibr">8</xref>]. The key characteristics of federated learning include scalability, device and statistical heterogeneity, and data privacy [<xref rid="B1-sensors-25-05633" ref-type="bibr">1</xref>,<xref rid="B9-sensors-25-05633" ref-type="bibr">9</xref>].</p><p>The performance of federated learning is often comparable to&#8212;and in some cases surpasses&#8212;that of traditional centralized machine learning approaches [<xref rid="B7-sensors-25-05633" ref-type="bibr">7</xref>]. Comparable principles have also been observed in graph neural network research [<xref rid="B10-sensors-25-05633" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05633" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05633" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05633" ref-type="bibr">13</xref>], where distributed learning is employed to optimize communication efficiency and maintain data locality. Traditional machine learning approaches require substantial computational power, memory storage, and large training datasets [<xref rid="B2-sensors-25-05633" ref-type="bibr">2</xref>,<xref rid="B14-sensors-25-05633" ref-type="bibr">14</xref>]. Moreover, conventional CPUs or GPUs are generally unavailable in IoT devices and are unsuitable for untethered and battery-powered deployments [<xref rid="B2-sensors-25-05633" ref-type="bibr">2</xref>]. Crucially, most machine learning methods rely on centralized processing, necessitating that both data and models reside in a single location [<xref rid="B1-sensors-25-05633" ref-type="bibr">1</xref>,<xref rid="B14-sensors-25-05633" ref-type="bibr">14</xref>]. Federated learning algorithms enable multiple nodes to collaboratively train a global model in a fully distributed manner without communicating raw data to a central location for processing [<xref rid="B2-sensors-25-05633" ref-type="bibr">2</xref>,<xref rid="B15-sensors-25-05633" ref-type="bibr">15</xref>].</p><p>Federated learning has been proposed as an effective solution for distributed information processing in the Internet of Things (IoT). The IoT represents a transformative, ubiquitous paradigm that has been seamlessly integrated into the fabric of everyday life [<xref rid="B16-sensors-25-05633" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05633" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05633" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05633" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05633" ref-type="bibr">20</xref>]. Initially, the term IoT was associated with radio frequency identification (RFID) tags [<xref rid="B21-sensors-25-05633" ref-type="bibr">21</xref>]. However, the current IoT paradigm encompasses a diverse range of interconnected devices, including near-field communication (NFC) devices, sensors, actuators, wireless sensor networks (WSNs), and smartphones. IoT devices are now pervasive in daily life and influence decision-making, behavior, and lifestyle [<xref rid="B21-sensors-25-05633" ref-type="bibr">21</xref>]. The application domains of the IoT are vast and continually expanding across various domains and sectors [<xref rid="B21-sensors-25-05633" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05633" ref-type="bibr">22</xref>].</p><p>This study focuses on WSNs and wireless IoT (WIoT) devices, both of which form important subsets of the broader IoT domain. These devices act as bridges between the physical and digital environments by acquiring sensor data and transmitting it to other IoT nodes or end-users [<xref rid="B16-sensors-25-05633" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05633" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05633" ref-type="bibr">18</xref>,<xref rid="B23-sensors-25-05633" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05633" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05633" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05633" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05633" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05633" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05633" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05633" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05633" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05633" ref-type="bibr">32</xref>]. Typically constrained by limited processing power, memory capacity, and battery life [<xref rid="B21-sensors-25-05633" ref-type="bibr">21</xref>], WSNs and WIoT devices are often deployed for remote sensing in locations where battery replacement or physical maintenance is impractical or infeasible. In such scenarios, energy efficiency becomes a critical design objective, directly affecting network lifespan and operational reliability [<xref rid="B33-sensors-25-05633" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05633" ref-type="bibr">34</xref>].</p><p>To ensure reliable operation, IoT applications must optimize processing power, memory storage, and communication overhead to conserve the limited battery energy of the IoT devices [<xref rid="B16-sensors-25-05633" ref-type="bibr">16</xref>]. Sensor data collected from IoT environments require local storage, processing, and communication; however, these processes are constrained by limited device resources [<xref rid="B24-sensors-25-05633" ref-type="bibr">24</xref>]. Furthermore, IoT network communication is often highly dynamic, less reliable, and slower than that of traditional infrastructure, making the assumption of continuous node availability unrealistic in many deployment scenarios [<xref rid="B1-sensors-25-05633" ref-type="bibr">1</xref>]. Beyond device-level challenges, large-scale IoT deployments exert heavy demands on communication networks and end-systems such as base stations and servers.</p><p>Technological advances have driven the rise of federated learning-enabled IoT edge computing, offering efficient solutions and innovative applications [<xref rid="B35-sensors-25-05633" ref-type="bibr">35</xref>]. This synergy enables diverse applications: in healthcare, federated learning&#8211;IoT integration accelerates patient diagnosis and treatment while preserving data privacy [<xref rid="B36-sensors-25-05633" ref-type="bibr">36</xref>]. Specifically for medical IoT, federated learning enhances both privacy and classification accuracy in applications such as chronic disease prediction [<xref rid="B37-sensors-25-05633" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05633" ref-type="bibr">38</xref>]. Furthermore, federated learning is increasingly applied in smart city domains such as traffic management and smart vehicles, where federated learning enables collaborative solutions including an improved recognition of unlabeled traffic signs among vehicles and a more efficient identification of charging stations [<xref rid="B39-sensors-25-05633" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05633" ref-type="bibr">40</xref>].</p><p>Recent research has explored lightweight and secure distributed learning paradigms that align operational constraints with the approach proposed in this study. For example, the combinatorial optimization graph neural network (CO-GNN) employs graph neural networks (GNNs) to jointly optimize beamforming, power allocation, and intelligent reflecting surface (IRS) phase shifts in non-orthogonal multiple access (NOMA) systems, achieving secure, resource-efficient transmission without explicit channel state information (CSI) estimation [<xref rid="B41-sensors-25-05633" ref-type="bibr">41</xref>]. Similarly, the work in [<xref rid="B42-sensors-25-05633" ref-type="bibr">42</xref>] introduced a safety-optimal, fault-tolerant control method for unknown nonlinear systems with actuator faults and asymmetric input constraints, ensuring both safety and optimality. This approach integrates control barrier functions, neural network-based system identification, and an adaptive critic scheme to guarantee stability and efficiency. Furthermore, the authors in [<xref rid="B43-sensors-25-05633" ref-type="bibr">43</xref>] proposed a collaboration-based node selection (CNS) approach that enhances wireless security by combining relay selection with friendly interference, thereby improving secrecy and reliability through selection combining (SC) and maximal ratio combining (MRC) strategies, with performance validated through both theoretical analysis and simulation.</p><p>Despite extensive research on federated learning and the IoT, achieving result model convergence and acceptable accuracy often requires a large number of communication rounds, during which local model updates from active network nodes are communicated throughout the network and aggregated at a central server or base stations. These iterative communication rounds introduce substantial communication overhead and impose significant energy demands on participating nodes. These limitations underscore the need for energy-efficient strategies to improve federated learning performance and the lifetime of the network. Consequently, the implications of communication overhead and energy consumption remain underexplored in the context of resource-constrained IoT environments. The main contributions of this paper are as follows:
<list list-type="bullet"><list-item><p>Novel Federated Learning Approach: A federated learning approach is proposed, namely the tiny federated nearest neighbors (TFNN) approach, tailored for IoT devices and networks that significantly reduces communication overhead, computational load, and energy conservation&#8212;critical considerations in resource-constrained environments.</p></list-item><list-item><p>Single-shot Communication: This study leverages the <italic toggle="yes">k</italic>-nearest neighbors (<italic toggle="yes">k</italic>-NN) algorithm to perform collaborative pattern recognition in a single communication round, thereby minimizing communication overhead.</p></list-item><list-item><p>Performance Benchmarking: TFNN is benchmarked with the classical <italic toggle="yes">k</italic>-NN and federated learning baseline algorithms, demonstrating that TFNN achieves competitive accuracy while requiring only a single communication round, in contrast to the multiple rounds typically needed by conventional federated learning approaches.</p></list-item></list></p><p>The rest of the paper is organized as follows: <xref rid="sec2-sensors-25-05633" ref-type="sec">Section 2</xref> provides an overview of related work on federated learning and <italic toggle="yes">k</italic>-NN. <xref rid="sec3-sensors-25-05633" ref-type="sec">Section 3</xref> describes the proposed TFNN approach in detail. <xref rid="sec4-sensors-25-05633" ref-type="sec">Section 4</xref> outlines the evaluation methodology, and <xref rid="sec5-sensors-25-05633" ref-type="sec">Section 5</xref> presents the simulation results and performance analysis. Finally, <xref rid="sec6-sensors-25-05633" ref-type="sec">Section 6</xref> concludes the paper by summarizing the key findings, discussing limitations, and proposing future research directions.</p></sec><sec id="sec2-sensors-25-05633"><title>2. Related Work</title><p>This study explores the synergy between the <italic toggle="yes">k</italic>-NN algorithm and federated learning to enable distributed, collaborative pattern recognition in IoT systems. Before presenting the TFNN approach, we review the principles of <italic toggle="yes">k</italic>-NN and federated learning and examine how these algorithms interact in the context of distributed sensing.</p><sec id="sec2dot1-sensors-25-05633"><title>2.1. k-Nearest Neighbors</title><p>The <italic toggle="yes">k</italic>-NN algorithm, originally introduced by Fix and Hodges [<xref rid="B44-sensors-25-05633" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05633" ref-type="bibr">45</xref>], is a supervised non-parametric method widely employed in diverse classification problems. Its appeal stems from the absence of a dedicated training stage, its straightforward implementation, and its ability to handle multi-classes. Instead of iterative parameter updates, the <italic toggle="yes">k</italic>-NN algorithm essentially memorizes the training dataset and applies distance metrics to classify new instances.</p><p>The <italic toggle="yes">k</italic>-NN algorithm is described by the pseudocode in Algorithm 1. The <italic toggle="yes">k</italic>-NN algorithm employs a training set <italic toggle="yes">D</italic> to perform pattern recognition. Each instance in <italic toggle="yes">D</italic> consists of a pair (<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) that describes a pattern and its label. For a given target sample <italic toggle="yes">x</italic> (i.e., the event pattern whose label is to be predicted), the algorithm computes the distance <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between <italic toggle="yes">x</italic> and each training pattern <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">D</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>The distance metric can be defined using various methods, such as Euclidean distance, Manhattan distance, or other suitable distance measures. After computing the distance, the algorithm selects the <italic toggle="yes">k</italic> samples with the smallest distances. The parameter <italic toggle="yes">k</italic> specifies the number of nearest neighbors used to determine the label of the test sample.</p><p>Finally, the algorithm determines the majority label&#8212;the label that occurs most frequently&#8212;among the samples in <italic toggle="yes">N</italic> by counting the occurrence of each label <italic toggle="yes">y</italic> in <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, as described in the following formula:
<disp-formula id="FD1-sensors-25-05633"><label>(1)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Count</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mi mathvariant="double-struck">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow></mml:math></inline-formula> denotes the indicator function. The majority label is the label that appears most frequently among the <italic toggle="yes">k<sup>th</sup></italic> nearest neighbors. The classified label <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is defined as follows:
<disp-formula id="FD2-sensors-25-05633"><label>(2)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>y</mml:mi></mml:munder><mml:mi>Count</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Formula (<xref rid="FD2-sensors-25-05633" ref-type="disp-formula">2</xref>) states that the predicted label for the test pattern is the label that occurs most frequently among its <italic toggle="yes">k</italic>-nearest neighbors. In other words, it selects the label with the highest count, i.e., the majority winning label. The <italic toggle="yes">k</italic>-NN algorithm is outlined in the pseudocode shown in Algorithm&#160;1.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold>&#160;k-NN algorithm.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>1:</label><p><bold>Input</bold></p></list-item><list-item><label>2:</label><p><italic toggle="yes">D: a set of training samples</italic> {<italic toggle="yes">(<inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>),&#8230;,((<inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>)</italic>}</p></list-item><list-item><label>3:</label><p><italic toggle="yes">k: the number of nearest neighbors</italic></p></list-item><list-item><label>4:</label><p><bold>for</bold> each training sample (<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>)&#8712; <italic toggle="yes">D</italic>&#160;<bold>do</bold></p></list-item><list-item><label>5:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;compute <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>6:</label><p><bold>end for</bold></p></list-item><list-item><label>7:</label><p>let <italic toggle="yes">N</italic> &#8838; <italic toggle="yes">D</italic> be the set of training samples with the <italic toggle="yes">k</italic> smallest distance</p></list-item><list-item><label>8:</label><p><bold>return</bold> the majority label of the samples in <italic toggle="yes">N</italic></p></list-item></list></td></tr></tbody></array></p></sec><sec id="sec2dot2-sensors-25-05633"><title>2.2. Federated Learning</title><p>The federated averaging (FedAvg) algorithm [<xref rid="B8-sensors-25-05633" ref-type="bibr">8</xref>] is a widely used aggregation algorithm in federated learning, where in each communication round, a subset of devices independently trains local models on private data. Instead of sharing raw data, these devices upload their local models to a central server, which produces a new global model by averaging the updates. This process iterates until the model converges to a satisfactory performance level.</p><p>A federated learning scenario for an IoT network consists of <italic toggle="yes">N</italic> nodes (IoT devices) and a centralized location for final processing&#8212;a server or base stations. Federated learning can be mathematically formulated as a distributed optimization problem for the global loss function <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>FL</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD3-sensors-25-05633"><label>(3)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>FL</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#969;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#969;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow></mml:math></inline-formula> represents the global model weights, <italic toggle="yes">K</italic> is the total number of data points across all devices, and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the number of data points stored on device <italic toggle="yes">n</italic>.</p><p>As described earlier, FedAvg is based on averaging the results of local stochastic gradient descent (SGD) updates, as summarized in Algorithm&#160;2 [<xref rid="B8-sensors-25-05633" ref-type="bibr">8</xref>]. The server selects a subset of nodes to participate in each round and distributes the current global model to all selected nodes. Upon receiving the global model, each node updates its own local model by performing the SGD procedure. The nodes then transmit their updated local models to the server, which computes a weighted average of the received updates to produce the new global model [<xref rid="B46-sensors-25-05633" ref-type="bibr">46</xref>].</p><p>Initially, the global model <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is randomly initialized. In each round <italic toggle="yes">t</italic>, the server randomly selects a subset <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of clients (determined by a fixed participation fraction) and distributes the current global model <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to all nodes in <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. All selected nodes operate in parallel, updating their local models according to <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#969;</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>&#8592;</mml:mo><mml:msub><mml:mi>&#969;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Each node then computes its local gradient descent step and sends its updated local model to the server. The server aggregates these updates to generate the new global model <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2</bold>&#160;FedAvg&#160;algorithm.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>&#160;&#160;1:</label><p><bold>Aggregation Server</bold></p></list-item><list-item><label>&#160;&#160;2:</label><p>Initialize <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#160;&#160;3:</label><p><bold>for</bold> each round <italic toggle="yes">t</italic> = 1,2,&#8230; <bold>do</bold></p></list-item><list-item><label>&#160;&#160;4:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;Select a subset of clients <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#160;&#160;5:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>for</bold> each client <italic toggle="yes">k</italic> &#8712;<inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>&#160;<bold>in parallel do</bold></p></list-item><list-item><label>&#160;&#160;6:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>&#8592;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> ClientUpdate(<italic toggle="yes">k</italic>, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>)</p></list-item><list-item><label>&#160;&#160;7:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>&#160;&#160;8:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8592;</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:msubsup><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#9657; Averaging model updates from all selected clients</p></list-item><list-item><label>&#160;&#160;9:</label><p><bold>end for</bold></p></list-item><list-item><label>10:</label><p>ClientUpdate(<italic toggle="yes">k</italic>, <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow></mml:math></inline-formula>): // Run on client <italic toggle="yes">k</italic></p></list-item><list-item><label>11:</label><p><bold>for</bold> each local epoch <italic toggle="yes">e</italic> from 0 to <italic toggle="yes">E</italic>&#160;<bold>do</bold></p></list-item><list-item><label>12:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;<bold>for</bold> each minibatch <italic toggle="yes">b</italic> of size <italic toggle="yes">B</italic>&#160;<bold>do</bold></p></list-item><list-item><label>13:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#969;</mml:mi><mml:mo>&#8592;</mml:mo><mml:mi>&#969;</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#951;</mml:mi><mml:mo>&#8711;</mml:mo><mml:mo>&#8467;</mml:mo><mml:mo>(</mml:mo><mml:mi>&#969;</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="italic">b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#9657; Gradient descent update</p></list-item><list-item><label>14:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>15:</label><p><bold>end for</bold></p></list-item><list-item><label>16:</label><p>return <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow></mml:math></inline-formula> to the server</p></list-item></list></td></tr></tbody></array></p><p>Nonetheless, FedAvg incurs substantial communication costs, since a large number of communication rounds are often required to achieve convergence. Moreover, local updates can sometimes cause FedAvg to diverge from the optimal global solution [<xref rid="B47-sensors-25-05633" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-05633" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05633" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-05633" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-05633" ref-type="bibr">51</xref>]. FedAvg also faces scalability challenges and may struggle to adapt to the dynamic conditions of IoT networks. In addition, its reliance on the arithmetic mean for model aggregation makes it vulnerable to data corruption and outliers [<xref rid="B46-sensors-25-05633" ref-type="bibr">46</xref>,<xref rid="B52-sensors-25-05633" ref-type="bibr">52</xref>].</p><p>FedProx, proposed in [<xref rid="B53-sensors-25-05633" ref-type="bibr">53</xref>], extends the standard federated learning paradigm by introducing a proximal term into the local optimization objective function. This additional term discourages excessive deviation from the global model, thereby helping stabilize training in scenarios where participating IoT devices differ in computational capacity, data volume, or distribution. While this approach enhances robustness in non-IID conditions, it comes with notable trade-offs. The need to compute the proximal term increases computational workload on each device, and model behavior can vary significantly depending on the chosen regularization coefficient. Moreover, FedProx still requires multiple client&#8211;server communication rounds before convergence and remains dependent on the accuracy and stability of the global model used as a reference.</p><p>SCAFFOLD is a widely used federated learning algorithm designed to mitigate client drift caused by non-IID data [<xref rid="B47-sensors-25-05633" ref-type="bibr">47</xref>]. It employs control variates&#8212;auxiliary vectors exchanged between the server and clients&#8212;to guide local updates toward the global optimum. SCAFFOLD offers provable convergence guarantees and performs effectively in heterogeneous data settings with significantly fewer communication rounds. Nonetheless, SCAFFOLD requires tens of communication rounds of model updates and the exchange of gradients and therefore still suffers from communication overhead and computational cost.</p><p>In general, it is important to note that achieving target accuracy in federated learning entails substantial communication overhead. <xref rid="sensors-25-05633-t001" ref-type="table">Table 1</xref> compares federated learning algorithms, namely FedAvg, FedProx, SCAFFOLD, and AdaptiveFL.</p></sec></sec><sec id="sec3-sensors-25-05633"><title>3. Tiny Federated Nearest Neighbors</title><p>This study proposes a federated learning nearest neighbor&#8211;based approach, referred to as TFNN, building upon the work presented in [<xref rid="B55-sensors-25-05633" ref-type="bibr">55</xref>]. TFNN is designed to provide an energy-efficient federated learning solution for the IoT, explicitly addressing the memory, processing, and communication constraints of IoT nodes.</p><p>Pseudocode Algorithm&#160;3 outlines the TFNN approach, in which IoT devices collaborate to transform their local inferences into a global event recognition outcome through decision fusion at the server or base station. In this approach, each IoT device stores training patterns locally and applies the <italic toggle="yes">k</italic>-NN algorithm to classify input test (event) patterns, as described in Algorithm&#160;1.</p><p>In TFNN, the server first initializes the network by identifying the participating nodes, denoted as <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each node independently executes a local inference process in parallel by invoking the <monospace>Node_Update</monospace> function, where <italic toggle="yes">k</italic> represents the number of nearest neighbors used in the local <italic toggle="yes">k</italic>-NN classifier. The parameter <italic toggle="yes">P</italic> is not transmitted; instead, each node is assumed to access the event pattern directly through local observation. The local prediction <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from each node is then returned to the server.</p><p>Finally, the base station performs global decision fusion by applying majority voting over the class labels received from IoT nodes to produce the final predicted label <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>f</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the event pattern <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denote the class label of the <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>th</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> nearest neighbors of <italic toggle="yes">x</italic>. The indicator function <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> outputs 1 if <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and 0 otherwise.<disp-formula id="FD4-sensors-25-05633"><label>(4)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>&#948;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of neighbors labeled as class <italic toggle="yes">c</italic>.<disp-formula id="FD5-sensors-25-05633"><label>(5)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>c</mml:mi></mml:munder><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> represents the majority class label selected as the final prediction. The winning class is defined as the class receiving the majority of votes (<inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>%</mml:mo><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), as defined in Formulas (<xref rid="FD4-sensors-25-05633" ref-type="disp-formula">4</xref>) and (<xref rid="FD5-sensors-25-05633" ref-type="disp-formula">5</xref>).
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 3</bold>&#160;Tiny federated nearest neighbors (TFNN) approach.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>&#160;&#160;1:</label><p>Define set of active IoT nodes <bold>Network Initialization:</bold>&#160;<inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo>&#8592;</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#160;&#160;2:</label><p><bold>Server/Base Station Executes</bold></p></list-item><list-item><label>&#160;&#160;3:</label><p><bold>for</bold> each node <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in parallel <bold>do</bold></p></list-item><list-item><label>&#160;&#160;4:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:mrow><mml:mtext>Node_Update</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#9657; Local classification via k-NN</p></list-item><list-item><label>&#160;&#160;5:</label><p>//communication step: each node sends only <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mover><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (a class label) to server</p></list-item><list-item><label>&#160;&#160;6:</label><p><bold>end for</bold></p></list-item><list-item><label>&#160;&#160;7:</label><p><inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>f</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:mi mathvariant="bold">argmax</mml:mi><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#9657; Server aggregates classifications via majority voting</p></list-item><list-item><label>&#160;&#160;8:</label><p>function <bold>Node_Update</bold>(<italic toggle="yes">k</italic>)</p></list-item><list-item><label>&#160;&#160;9:</label><p><inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:mrow><mml:mtext>k-NN</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#9657; Perform location classification only</p></list-item><list-item><label>10:</label><p>return <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to server</p></list-item></list></td></tr></tbody></array></p><p>This federated learning design ensures that raw data is never exchanged, thereby preserving privacy and enhancing efficiency. Each node transmits only its winning class label, which the server aggregates through majority voting. This approach provides a lightweight, communication-efficient, and privacy-preserving federated learning mechanism well suited for low-power and resource-constrained IoT environments.</p></sec><sec id="sec4-sensors-25-05633"><title>4. Evaluation</title><p>Assessing federated learning algorithms for IoT deployments requires evaluating multiple performance factors. In this study, we employ analytical models and simulations to quantify these metrics for TFNN and baseline methods, focusing on their implications for energy consumption, communication overhead, and recognition performance.</p><sec id="sec4dot1-sensors-25-05633"><title>4.1. Computational Complexity</title><p>In federated learning scenarios&#8212;particularly within IoT networks constrained by limited computational and power resources&#8212;local computational efficiency is critical. The TFNN approach addresses this challenge by eliminating iterative model training, thereby reducing processing demands. Instead, each IoT node performs lightweight local inference using a <italic toggle="yes">k</italic>-NN algorithm on a fixed-size pattern dataset.</p><p>The local computational cost at each node in TFNN is primarily determined by the <italic toggle="yes">k</italic>-NN classification step. For a local prototype set <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of size <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and input feature vectors of dimension <italic toggle="yes">d</italic>, each node locally computes the distance between the input event pattern and stored patterns, resulting in a per-pattern complexity of<disp-formula id="FD6-sensors-25-05633"><label>(6)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>&#183;</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Compared to machine learning algorithms that require forward passes through large neural networks, this cost is both low and predictable, making TFNN well suited for microcontrollers and IoT-grade processors. Furthermore, TFNN avoids backpropagation, weight storage, and floating-point intensive operations common in many federated learning algorithms. This significantly reduces the memory footprint and energy consumption at each node&#8212;an essential advantage for battery-powered and intermittently connected devices. Overall, TFNN provides a favorable trade-off by enabling decentralized intelligence with minimal local computation, making it highly practical for resource-constrained federated edge environments.</p></sec><sec id="sec4dot2-sensors-25-05633"><title>4.2. Accuracy Analysis</title><p>The classification accuracy of the TFNN approach depends on several interdependent factors, including the number of participating nodes (<italic toggle="yes">m</italic>) and the number of nearest neighbors (<italic toggle="yes">k</italic>). At the local level, each node executes a <italic toggle="yes">k</italic>-NN classifier, with accuracy determined by the quality of its local dataset and the choice of <italic toggle="yes">k</italic>. While smaller values of <italic toggle="yes">k</italic> may lead to overfitting or increased sensitivity to noise, larger values generally improve stability but may reduce specificity.</p><p>The size and quality of local data <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math></inline-formula> directly affect prediction accuracy, with larger local datasets generally improving <italic toggle="yes">k</italic>-NN performance due to better neighborhood coverage. When using <italic toggle="yes">k</italic>-nearest samples for classification, the risk of underfitting or overfitting becomes more pronounced. In extreme cases, <italic toggle="yes">k</italic>-NN may yield unreliable results if <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">k</mml:mi><mml:mo>&#8805;</mml:mo><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> or become overly sensitive if <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Formula (<xref rid="FD7-sensors-25-05633" ref-type="disp-formula">7</xref>) describes the accuracy trend of <italic toggle="yes">k</italic>-NN accordingly:<disp-formula id="FD7-sensors-25-05633"><label>(7)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Acc</mml:mi><mml:mi>kNN</mml:mi></mml:msub><mml:mo>&#8733;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="script">O</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mspace width="1.em"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>for</mml:mi></mml:mrow><mml:mspace width="4.pt"/><mml:mi>large</mml:mi><mml:mspace width="4.pt"/><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Consequently, a small <italic toggle="yes">k</italic> results in low bias and high variance, making the classifier sensitive to noise and prone to misclassifications due to outliers or label noise. In contrast, a large <italic toggle="yes">k</italic> provides better generalization with high bias and low variance; however, it may overlook minority classes. In general, an effective choice is <italic toggle="yes">k</italic> &#8776;<inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of samples at node <italic toggle="yes">i</italic>.</p><p>Moreover, let the local accuracy of the <italic toggle="yes">k</italic>-NN classifier at node <italic toggle="yes">i</italic> be denoted by <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>NN</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, the local accuracy of the <italic toggle="yes">k</italic>-NN classifier at node <italic toggle="yes">i</italic> is given by<disp-formula id="FD8-sensors-25-05633"><label>(8)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>Acc</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:mi mathvariant="double-struck">I</mml:mi><mml:mspace width="-0.166667em"/><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>NN</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Under regularity conditions&#8212;such as independent and identically distributed samples and the Lipschitz continuity of the conditional distribution&#8212;as <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8594;</mml:mo><mml:mo>&#8734;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the local accuracy converges to<disp-formula id="FD9-sensors-25-05633"><label>(9)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8594;</mml:mo><mml:mo>&#8734;</mml:mo></mml:mrow></mml:munder><mml:msup><mml:mi>Acc</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#8805;</mml:mo><mml:msub><mml:mi>Acc</mml:mi><mml:mi>Bayes</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Acc</mml:mi><mml:mi>Bayes</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the Bayes-optimal accuracy for the classification problem, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is a constant that depends on the underlying data distribution, <italic toggle="yes">k</italic> is the number of nearest neighbors, and <italic toggle="yes">d</italic> is the dimensionality of the feature space.</p><p>For practical finite datasets, a tight upper bound described in [<xref rid="B44-sensors-25-05633" ref-type="bibr">44</xref>] is given by<disp-formula id="FD10-sensors-25-05633"><label>(10)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>Acc</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#10886;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mfrac></mml:mstyle></mml:msqrt></mml:mfenced><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>Acc</mml:mi><mml:mi>Bayes</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Acc</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the expected classification accuracy at node <italic toggle="yes">i</italic>, <italic toggle="yes">k</italic> is the number of nearest neighbors, <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is a constant that depends on the underlying data distribution and class overlap, and <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Acc</mml:mi><mml:mi>Bayes</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the accuracy of the Bayes-optimal classifier.</p><p>In the TFNN approach, the final classification is obtained by aggregating local predictions from <italic toggle="yes">m</italic> IoT nodes using unweighted majority voting, as described in Formula (<xref rid="FD5-sensors-25-05633" ref-type="disp-formula">5</xref>). Each node <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> independently predicts a class label <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> using a local <italic toggle="yes">k</italic>-NN classifier, and the server or base stations then computes the global output using the same majority voting rule. The resulting global accuracy depends on the statistical properties of the ensemble decision-making process.</p><p>Assuming that each node makes an independent prediction with a probability <italic toggle="yes">p</italic> of being correct, the probability that the majority of nodes produce the correct label is given by the binomial cumulative distribution function (CDF): <disp-formula id="FD11-sensors-25-05633"><label>(11)</label><mml:math id="mm80" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Acc</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>&#8968;</mml:mo><mml:mi>m</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#8969;</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac linethickness="0pt"><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced><mml:mspace width="0.166667em"/><mml:msup><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac linethickness="0pt"><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:math></inline-formula> denotes the binomial coefficient, <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> represent the probability that exactly <italic toggle="yes">i</italic> nodes predict correctly, and <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represents the probability that the remaining <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> nodes predict incorrectly.</p><p>This reflects the classic ensemble effect: as long as individual nodes perform better than random guessing (<inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), increasing the number of participating IoT nodes improves the global accuracy, enabling the ensemble to outperform any individual node. In the asymptotic case, with IID data and a sufficiently large number of nodes, the performance of TFNN can approach that of a globally trained <italic toggle="yes">k</italic>-NN model.</p></sec><sec id="sec4dot3-sensors-25-05633"><title>4.3. Communication Overhead</title><p>Minimizing communication overhead is a critical concern for IoT devices operating on limited battery reserves. Generally, federated learning strategies require multiple exchanges of large iterative model parameters between distributed clients and a coordinating server. TFNN, by contrast, streamlines this process through a single-shot classification approach. Each participating node independently executes a lightweight <italic toggle="yes">k</italic>-NN classifier on its locally stored data and transmits only the resulting class labels to the central aggregator at the server or base station.</p><p>Let <italic toggle="yes">m</italic> denote the number of participating nodes and <italic toggle="yes">C</italic> the number of classes. Assuming each prediction is encoded as a one-hot vector of length <italic toggle="yes">C</italic> or as an integer requiring <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8968;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi><mml:mo>&#8969;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> bits, TFNN incurs negligible downlink cost since the datasets are fixed and locally stored. Unlike conventional federated learning, the server does not broadcast global model updates back to the nodes.</p><p>In addition to abstract communication complexity, practical physical costs such as radio transmission energy, propagation distance, and receiver wake-up durations must also be considered. Each IoT node transmits a scalar index <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, requiring <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> bits, or a one-hot vector of length <italic toggle="yes">C</italic>, requiring <italic toggle="yes">C</italic> bits. Thus, the total uplink communication costs for <italic toggle="yes">m</italic> nodes is given by<disp-formula id="FD12-sensors-25-05633"><label>(12)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mspace width="-0.166667em"/><mml:mfenced separators="" open="(" close=")"><mml:mi>m</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mfenced><mml:mspace width="1.em"/><mml:mi>bits</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the distance from node <italic toggle="yes">i</italic> to the server or base station and <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is the path loss exponent, the per-node transmission energy scales as<disp-formula id="FD13-sensors-25-05633"><label>(13)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>comm</mml:mi></mml:mrow><mml:mrow><mml:mi>tx</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>tx</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>&#945;</mml:mi></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Aggregating over all nodes yields the total uplink transmission energy: <disp-formula id="FD14-sensors-25-05633"><label>(14)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>comm</mml:mi></mml:mrow><mml:mrow><mml:mi>tx</mml:mi><mml:mo>,</mml:mo><mml:mi>total</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>E</mml:mi><mml:mi>tx</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>&#945;</mml:mi></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In duty-cycled networks, transceivers consume a wake-up energy cost (<inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>wake</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) in addition to transmission or reception energy. If <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>rx</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the per-bit reception energy, the server&#8217;s total reception energy is given by<disp-formula id="FD15-sensors-25-05633"><label>(15)</label><mml:math id="mm96" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>comm</mml:mi></mml:mrow><mml:mi>rx</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>rx</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Including wake-up costs, the per-classification communication energy is expressed as<disp-formula id="FD16-sensors-25-05633"><label>(16)</label><mml:math id="mm97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>total</mml:mi></mml:mrow><mml:mrow><mml:mi>node</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>wake</mml:mi></mml:mrow><mml:mi>tx</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>tx</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>&#945;</mml:mi></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-05633"><label>(17)</label><mml:math id="mm98" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>total</mml:mi></mml:mrow><mml:mi>BS</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>&#183;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>wake</mml:mi></mml:mrow><mml:mi>rx</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>rx</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>By communicating only a single scalar class label per IoT node in one communication round, TFNN achieves a substantially lower communication overhead than standard federated learning, making it well suited for low-power, bandwidth-constrained IoT deployments.</p></sec><sec id="sec4dot4-sensors-25-05633"><title>4.4. Privacy Considerations</title><p>Similar to conventional federated learning methods that exchange model updates, TFNN transmits only a scalar class label from each node for each inference. This substantially reduces the information available to potential eavesdroppers and mitigates risks associated with model inversion or data reconstruction attacks. Raw sensor data or detailed model parameters are never shared across the network. However, in scenarios where the class label space itself may reveal sensitive information (e.g., rare disease diagnosis), even label-only transmission could expose limited contextual details to a malicious observer. In such cases, additional privacy-enhancing techniques&#8212;such as label perturbation or differential privacy&#8212;can be integrated without significantly affecting TFNN efficiency.</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-05633"><title>5. Results</title><p>The on-board battery energy of IoT devices is predominantly consumed during communication. Therefore, this study focuses on evaluating the communication efficiency and classification accuracy of the TFNN approach by comparing its performance with traditional federated learning and pattern recognition methods, specifically AdaptiveFL, FedProx, SCAFFOLD, FedAvg, and the <italic toggle="yes">k</italic>-NN algorithm, respectively. The evaluation of TFNN was conducted using a custom simulator developed in MATLAB 2024b and Python 3.13.3 [<xref rid="B56-sensors-25-05633" ref-type="bibr">56</xref>,<xref rid="B57-sensors-25-05633" ref-type="bibr">57</xref>]. The experiments employed three widely used benchmark datasets&#8212;MNIST [<xref rid="B58-sensors-25-05633" ref-type="bibr">58</xref>], CIFAR-10, and CIFAR-100 [<xref rid="B59-sensors-25-05633" ref-type="bibr">59</xref>]&#8212;across various experimental setups. The CIFAR-10 dataset contains 60,000 color images of size <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>32</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels across 10 classes, while the CIFAR-100 dataset comprises 100 classes with 600 images per class. MNIST consists of handwritten digits (0&#8211;9, 10 classes), where each image comprises <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>28</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>28</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> grayscale patterns.</p><p><xref rid="sensors-25-05633-f001" ref-type="fig">Figure 1</xref> illustrates the classification accuracy of the <italic toggle="yes">k</italic>-NN algorithm with different values of <italic toggle="yes">k</italic> compared with the proposed TFNN approach, using 100 IoT nodes, and varying the number of IID stored training patterns from 10 to 500 from the MNIST dataset. As the number of stored patterns increases, the accuracy of both methods improves; however, TFNN consistently achieves higher accuracy than <italic toggle="yes">k</italic>-NN across all configurations. While <italic toggle="yes">k</italic>-NN serves as a simple and interpretable baseline, TFNN leverages lightweight collaborative inference in a federated setting, resulting in substantially improved performance. Notably, TFNN achieves approximately 90% accuracy when the number of stored patterns reaches 500.</p><p><xref rid="sensors-25-05633-f002" ref-type="fig">Figure 2</xref> compares the single-shot classification accuracy of the proposed TFNN (<inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) approach with AdaptiveFL, FedProx, SCAFFOLD, and FedAvg in IoT networks ranging from 10 to 100 nodes, with each node storing 500 IID MNIST patterns. The results show that TFNN consistently outperforms all federated learning baselines, achieving approximately 90% accuracy across all network sizes and thereby demonstrating strong communication efficiency&#8212;an essential feature for resource-constrained IoT environments.</p><p><xref rid="sensors-25-05633-f003" ref-type="fig">Figure 3</xref> presents the accuracy of the federated baselines after 200 communication rounds compared with the single-shot TFNN result. While the baselines achieve approximately 92%, this requires nearly 200 additional client&#8211;server exchanges beyond TFNN&#8217;s one-round operation and yields only about a 2% improvement in accuracy. These results underscore TFNN&#8217;s ability to achieve competitive accuracy with orders-of-magnitude lower communication overhead&#8212;and, by implication, lower communication energy&#8212;in practical IoT deployments.</p><p><xref rid="sensors-25-05633-t002" ref-type="table">Table 2</xref>, <xref rid="sensors-25-05633-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05633-t004" ref-type="table">Table 4</xref> present the classification accuracy of AdaptiveFL, FedProx, SCAFFOLD, FedAvg, and the proposed TFNN (<inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) under both IID and Non-IID partitions of the MNIST, CIFAR-10, and CIFAR-100 datasets. The simulations were conducted over 200 communication rounds with network sizes ranging from 10 to 100 IoT nodes, where each node stored 500 patterns.</p><p>Whilst using the MNIST dataset, all baseline algorithms achieved accuracies in the range of 92&#8211;93% under IID conditions, confirming strong convergence on relatively simple data. TFNN attained accuracy of approximately 80&#8211;90%, representing a modest trade-off in accuracy in exchange for an approximately 200-fold reduction in communication energy, as TFNN operates in a single-shot manner.</p><p>On the CIFAR-10 dataset, baseline federated learning methods achieved accuracies clustered around 36&#8211;39% under IID conditions, whereas TFNN remained consistently lower (&#8776;23&#8211;25%). Under non-IID partitions, accuracy remained low for both the baseline federated learning algorithm and TFNN. The CIFAR-100 dataset is, in general, challenging, with all algorithms&#8212;including TFNN&#8212;achieving only about 10% accuracy or less. The reduced performance of TFNN on CIFAR-10 and CIFAR-100 can be attributed to the higher feature dimensionality and greater class complexity of these datasets.</p><p>Overall, the results reveal a clear energy-efficiency and accuracy trade-off. TFNN achieves competitive accuracy while requiring only a single communication round, thereby drastically reducing communication costs and, consequently, energy consumption compared with classical federated learning algorithms. These findings position TFNN as a viable lightweight alternative for resource-constrained IoT and WSN deployments, where communication efficiency is critical and absolute accuracy may be moderately sacrificed. However, the utilization of TFNN needs to take into account the complexity of the event patterns in order to maintain acceptable accuracy.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-05633"><title>6. Conclusions</title><p>This study proposed TFNN, a lightweight and communication-efficient approach to federated learning tailored for resource-constrained IoT and WSN environments. Unlike conventional federated learning algorithms that require tens to hundreds of communication rounds and the exchange of large model parameter updates, TFNN performs single-shot pattern recognition by transmitting only local class labels from IoT nodes to the server or base station. Consequently, TFNN significantly reduces the energy consumed in communication compared with traditional federated learning while enabling federated pattern recognition in a single communication round.</p><p>The simulation results demonstrated that TFNN achieves competitive classification accuracy while reducing communication cost by up to two folds compared with baseline federated learning methods. This makes TFNN particularly promising for battery-powered and bandwidth-constrained IoT deployments, where extending network lifetime is more critical than achieving absolute accuracy.</p><p>Future studies will focus on refining and optimizing TFNN, specifically addressing its sensitivity to complex datasets by integrating lightweight feature extraction or prototype-sharing mechanisms. This is expected to enhance TFNN&#8217;s ability to capture discriminative representations in high-dimensional data. Furthermore, deploying TFNN in practical IoT environments will allow for an exploration of its full potential across diverse real-world scenarios. Such extensions will further establish TFNN as a scalable and energy-efficient solution for next-generation IoT systems.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The author declares no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05633"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>W.Y.B.</given-names></name><name name-style="western"><surname>Luong</surname><given-names>N.C.</given-names></name><name name-style="western"><surname>Hoang</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Jiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Y.C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name></person-group><article-title>Federated Learning in Mobile Edge Networks: A Comprehensive Survey</article-title><source>IEEE Commun. Surv. Tutor.</source><year>2020</year><volume>22</volume><fpage>2031</fpage><lpage>2063</lpage><pub-id pub-id-type="doi">10.1109/COMST.2020.2986024</pub-id></element-citation></ref><ref id="B2-sensors-25-05633"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>L.</given-names></name><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Krishnamachari</surname><given-names>B.</given-names></name><name name-style="western"><surname>Avestimehr</surname><given-names>A.S.</given-names></name></person-group><article-title>Federated Learning for the Internet of Things: Applications, Challenges, and Opportunities</article-title><source>IEEE Internet Things Mag.</source><year>2022</year><volume>5</volume><fpage>24</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1109/IOTM.004.2100182</pub-id></element-citation></ref><ref id="B3-sensors-25-05633"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Annavaram</surname><given-names>M.</given-names></name><name name-style="western"><surname>Avestimehr</surname><given-names>S.</given-names></name></person-group><article-title>Group knowledge transfer: Federated learning of large CNNs at the edge</article-title><source>Proceedings of the Advances in Neural Information Processing Systems 34</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>6&#8211;12 December 2020</conf-date><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2020</year><comment>Number 1180 in NIPS &#8217;20</comment><fpage>13</fpage></element-citation></ref><ref id="B4-sensors-25-05633"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Niknam</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dhillon</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>J.H.</given-names></name></person-group><article-title>Federated Learning for Wireless Communications: Motivation, Opportunities, and Challenges</article-title><source>IEEE Commun. Mag.</source><year>2020</year><volume>58</volume><fpage>46</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1109/MCOM.001.1900461</pub-id></element-citation></ref><ref id="B5-sensors-25-05633"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tahir</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>M.I.</given-names></name></person-group><article-title>On the Performance of Federated Learning Algorithms for IoT</article-title><source>Internet Things</source><year>2022</year><volume>3</volume><fpage>273</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.3390/iot3020016</pub-id></element-citation></ref><ref id="B6-sensors-25-05633"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shaheen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Farooq</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Umer</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>B.S.</given-names></name></person-group><article-title>Applications of Federated Learning; Taxonomy, Challenges, and Research Trends</article-title><source>Electronics</source><year>2022</year><volume>11</volume><elocation-id>670</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11040670</pub-id></element-citation></ref><ref id="B7-sensors-25-05633"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Weng</surname><given-names>L.</given-names></name></person-group><article-title>Federated Learning: A Distributed Shared Machine Learning Method</article-title><source>Complexity</source><year>2021</year><volume>2021</volume><fpage>8261663</fpage><pub-id pub-id-type="doi">10.1155/2021/8261663</pub-id></element-citation></ref><ref id="B8-sensors-25-05633"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>McMahan</surname><given-names>H.B.</given-names></name><name name-style="western"><surname>Moore</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ramage</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hampson</surname><given-names>S.</given-names></name><name name-style="western"><surname>y Arcas</surname><given-names>B.A.</given-names></name></person-group><article-title>Communication-Efficient Learning of Deep Networks from Decentralized Data</article-title><source>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)</source><conf-loc>Fort Lauderdale, FL, USA</conf-loc><conf-date>20&#8211;22 April 2017</conf-date><fpage>1273</fpage><lpage>1282</lpage></element-citation></ref><ref id="B9-sensors-25-05633"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Sahu</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Talwalkar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Smith</surname><given-names>V.</given-names></name></person-group><article-title>Federated Learning: Challenges, Methods, and Future Directions</article-title><source>IEEE Signal Process. Mag.</source><year>2020</year><volume>37</volume><fpage>50</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1109/MSP.2020.2975749</pub-id></element-citation></ref><ref id="B10-sensors-25-05633"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Baqer</surname><given-names>M.</given-names></name></person-group><article-title>Energy-Efficient Pattern Recognition for Wireless Sensor Networks</article-title><source>Mobile Intelligence</source><publisher-name>Wiley-IEEE Press</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2010</year></element-citation></ref><ref id="B11-sensors-25-05633"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Baqer</surname><given-names>M.</given-names></name></person-group><article-title>VRNS: A new energy efficient event recognition approach via Voting with Random Node Selection</article-title><source>Proceedings of the 2010 6th IEEE International Conference on Distributed Computing in Sensor Systems Workshops (DCOSSW)</source><conf-loc>Santa Barbara, CA, USA</conf-loc><conf-date>21&#8211;23 June 2010</conf-date><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="B12-sensors-25-05633"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Baqer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>A.I.</given-names></name><name name-style="western"><surname>Baig</surname><given-names>Z.A.</given-names></name></person-group><article-title>Implementing a Graph Neuron Array for Pattern Recognition Within Unstructured Wireless Sensor Networks</article-title><source>Proceedings of the International Conference on Embedded and Ubiquitous Computing: EUC 2005 Workshops</source><conf-loc>Nagasaki, Japan</conf-loc><conf-date>6&#8211;9 December 2005</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2005</year><fpage>208</fpage><lpage>217</lpage></element-citation></ref><ref id="B13-sensors-25-05633"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>A.I.</given-names></name><name name-style="western"><surname>Ramachandran</surname><given-names>V.</given-names></name></person-group><article-title>A Peer-to-Peer Associative Memory Network for Intelligent Information Systems</article-title><source>Proceedings of the Thirteenth Australasian Conference on Information Systems</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>3&#8211;6 December 2002</conf-date></element-citation></ref><ref id="B14-sensors-25-05633"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lazzarini</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tianfield</surname><given-names>H.</given-names></name><name name-style="western"><surname>Charissis</surname><given-names>V.</given-names></name></person-group><article-title>Federated Learning for IoT Intrusion Detection</article-title><source>AI</source><year>2023</year><volume>4</volume><fpage>509</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.3390/ai4030028</pub-id></element-citation></ref><ref id="B15-sensors-25-05633"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Song</surname><given-names>S.</given-names></name><name name-style="western"><surname>Letaief</surname><given-names>K.B.</given-names></name></person-group><article-title>Client-Edge-Cloud Hierarchical Federated Learning</article-title><source>Proceedings of the IEEE International Conference on Communications (ICC)</source><conf-loc>Dublin, Ireland</conf-loc><conf-date>7&#8211;11 June 2020</conf-date></element-citation></ref><ref id="B16-sensors-25-05633"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Atzori</surname><given-names>L.</given-names></name><name name-style="western"><surname>Iera</surname><given-names>A.</given-names></name><name name-style="western"><surname>Morabito</surname><given-names>G.</given-names></name></person-group><article-title>The Internet of Things: A survey</article-title><source>Comput. Netw.</source><year>2010</year><volume>54</volume><fpage>2787</fpage><lpage>2805</lpage><pub-id pub-id-type="doi">10.1016/j.comnet.2010.05.010</pub-id></element-citation></ref><ref id="B17-sensors-25-05633"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Baqer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kamal</surname><given-names>A.</given-names></name></person-group><article-title>S-Sensors: Integrating physical world inputs with social networks using wireless sensor networks</article-title><source>Proceedings of the 2009 International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>7&#8211;10 December 2009</conf-date></element-citation></ref><ref id="B18-sensors-25-05633"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Estrin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Culler</surname><given-names>D.</given-names></name><name name-style="western"><surname>Pister</surname><given-names>K.</given-names></name><name name-style="western"><surname>Sukhatme</surname><given-names>G.</given-names></name></person-group><article-title>Connecting the physical world with pervasive networks</article-title><source>IEEE Pervasive Comput.</source><year>2002</year><volume>1</volume><fpage>59</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1109/MPRV.2002.993145</pub-id></element-citation></ref><ref id="B19-sensors-25-05633"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baqer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Balbis</surname><given-names>L.</given-names></name></person-group><article-title>A Sampling Sensor Nodes to Extend the Longevity of Wireless Sensor Networks</article-title><source>Int. J. Sens. Wirel. Commun. Control</source><year>2019</year><volume>9</volume><fpage>53</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.2174/2210327908666180727125534</pub-id></element-citation></ref><ref id="B20-sensors-25-05633"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Baqer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Al Mutawah</surname><given-names>K.</given-names></name></person-group><article-title>Random node sampling for energy efficient data collection in wireless sensor networks</article-title><source>Proceedings of the 2013 IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>2&#8211;5 April 2013</conf-date><fpage>467</fpage><lpage>472</lpage></element-citation></ref><ref id="B21-sensors-25-05633"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ashton</surname><given-names>K.</given-names></name></person-group><article-title>That &#8217;Internet of Things&#8217; Thing</article-title><source>RFID J.</source><year>2009</year><volume>22</volume><fpage>97</fpage><lpage>114</lpage><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.rfidjournal.com/expert-views/that-internet-of-things-thing/73881/" ext-link-type="uri">https://www.rfidjournal.com/expert-views/that-internet-of-things-thing/73881/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-11-15">(accessed on 15 November 2024)</date-in-citation></element-citation></ref><ref id="B22-sensors-25-05633"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Elijah</surname><given-names>O.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Orikumhi</surname><given-names>I.</given-names></name><name name-style="western"><surname>Leow</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Hindia</surname><given-names>M.N.</given-names></name></person-group><article-title>An Overview of Internet of Things (IoT) and Data Analytics in Agriculture: Benefits and Challenges</article-title><source>IEEE Internet Things J.</source><year>2018</year><volume>5</volume><fpage>3758</fpage><lpage>3773</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2018.2844296</pub-id></element-citation></ref><ref id="B23-sensors-25-05633"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vermesan</surname><given-names>O.</given-names></name><name name-style="western"><surname>Friess</surname><given-names>P.</given-names></name><name name-style="western"><surname>Guillemin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Gusmeroli</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sundmaeker</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bassi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jubert</surname><given-names>I.</given-names></name><name name-style="western"><surname>Mazura</surname><given-names>M.</given-names></name><name name-style="western"><surname>Harrison</surname><given-names>M.</given-names></name><name name-style="western"><surname>Eisenhauer</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Internet of Things Strategic Research Roadmap</article-title><source>Internet of Things&#8212;Global Technological and Societal Trends from Smart Environments and Spaces to Green ICT</source><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2022</year><fpage>9</fpage><lpage>52</lpage></element-citation></ref><ref id="B24-sensors-25-05633"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sethi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sarangi</surname><given-names>S.R.</given-names></name></person-group><article-title>Internet of Things: Architectures, Protocols, and Applications</article-title><source>J. Electr. Comput. Eng.</source><year>2017</year><volume>2017</volume><fpage>9324035</fpage><pub-id pub-id-type="doi">10.1155/2017/9324035</pub-id></element-citation></ref><ref id="B25-sensors-25-05633"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Akyildiz</surname><given-names>I.</given-names></name><name name-style="western"><surname>Su</surname><given-names>W.S.</given-names></name><name name-style="western"><surname>Sankarasubramaniam</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cayirci</surname><given-names>E.</given-names></name></person-group><article-title>A Survey on Sensor Networks</article-title><source>IEEE Commun. Mag.</source><year>2002</year><volume>19</volume><fpage>102</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1109/MCOM.2002.1024422</pub-id></element-citation></ref><ref id="B26-sensors-25-05633"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mainwaring</surname><given-names>A.</given-names></name><name name-style="western"><surname>Culler</surname><given-names>D.</given-names></name><name name-style="western"><surname>Polastre</surname><given-names>J.</given-names></name><name name-style="western"><surname>Szewczyk</surname><given-names>R.</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>J.</given-names></name></person-group><article-title>Wireless Sensor Networks for Habitat Monitoring</article-title><source>Proceedings of the 1st ACM International Workshop on Wireless Sensor Networks and Applications (WSNA &#8217;02)</source><conf-loc>Atlanta, GA, USA</conf-loc><conf-date>28 September 2002</conf-date></element-citation></ref><ref id="B27-sensors-25-05633"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pakzad</surname><given-names>S.</given-names></name><name name-style="western"><surname>Culler</surname><given-names>D.</given-names></name><name name-style="western"><surname>Demmel</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fenves</surname><given-names>G.</given-names></name><name name-style="western"><surname>Glaser</surname><given-names>S.</given-names></name><name name-style="western"><surname>Turon</surname><given-names>M.</given-names></name></person-group><article-title>Wireless Sensor Networks for Structural Health Monitoring</article-title><source>Proceedings of the 4th International Conference on Embedded Networked Sensor Systems (SenSys&#8217;06)</source><conf-loc>Boulder, CO, USA</conf-loc><conf-date>31 October&#8211;3 November 2006</conf-date></element-citation></ref><ref id="B28-sensors-25-05633"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bonnet</surname><given-names>P.</given-names></name><name name-style="western"><surname>Gehrke</surname><given-names>J.</given-names></name><name name-style="western"><surname>Seshadri</surname><given-names>P.</given-names></name></person-group><article-title>Querying the Physical World</article-title><source>IEEE Pers. Commun.</source><year>2000</year><volume>7</volume><fpage>10</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/98.878531</pub-id></element-citation></ref><ref id="B29-sensors-25-05633"><label>29.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kramp</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kranenburg</surname><given-names>R.V.</given-names></name><name name-style="western"><surname>Lange</surname><given-names>S.</given-names></name></person-group><article-title>Introduction to the Internet of Things</article-title><source>Enabling Things to Talk: Designing IoT Solutions with the IoT Architectural Reference Model</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2013</year><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="B30-sensors-25-05633"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdul-Qawy</surname><given-names>P.P.J.A.S.</given-names></name><name name-style="western"><surname>Magesh</surname><given-names>E.</given-names></name><name name-style="western"><surname>Srinivasulu</surname><given-names>T.</given-names></name></person-group><article-title>The Internet of Things (IoT): An Overview</article-title><source>Int. J. Eng. Res. Appl.</source><year>2015</year><volume>5</volume><fpage>71</fpage><lpage>82</lpage></element-citation></ref><ref id="B31-sensors-25-05633"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mashal</surname><given-names>I.</given-names></name><name name-style="western"><surname>Alsaryrah</surname><given-names>O.</given-names></name><name name-style="western"><surname>Chung</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.Z.</given-names></name><name name-style="western"><surname>Kuo</surname><given-names>W.H.</given-names></name><name name-style="western"><surname>Agrawal</surname><given-names>D.P.</given-names></name></person-group><article-title>Choices for interaction with Things on Internet and Underlying Issues</article-title><source>Ad Hoc Netw.</source><year>2015</year><volume>28</volume><fpage>68</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.adhoc.2014.12.006</pub-id></element-citation></ref><ref id="B32-sensors-25-05633"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>S.U.</given-names></name><name name-style="western"><surname>Zaheer</surname><given-names>R.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>S.</given-names></name></person-group><article-title>Future Internet: The Internet of Things Architecture, Possible Applications and Key Challenges</article-title><source>Proceedings of the 10th International Conference on Frontiers of Information Technology</source><conf-loc>Islamabad, Pakistan</conf-loc><conf-date>17&#8211;19 December 2012</conf-date></element-citation></ref><ref id="B33-sensors-25-05633"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Patil</surname><given-names>K.</given-names></name><name name-style="western"><surname>Banerjee</surname><given-names>S.</given-names></name></person-group><article-title>Brief Overview on Wireless Sensor Network Technologies in the Internet of Things (IoT)</article-title><source>Int. J. Eng. Manag. Res.</source><year>2023</year><volume>13</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.31033/ijemr.13.6.1</pub-id></element-citation></ref><ref id="B34-sensors-25-05633"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Arampatzis</surname><given-names>T.</given-names></name><name name-style="western"><surname>Lygeros</surname><given-names>J.</given-names></name><name name-style="western"><surname>Manesis</surname><given-names>S.</given-names></name></person-group><article-title>A Survey of Applications of Wireless Sensors and Wireless Sensor Networks</article-title><source>Proceedings of the 2005 IEEE International Symposium on Mediterranean Conference on Control and Automation</source><conf-loc>Limassol, Cyprus</conf-loc><conf-date>27&#8211;29 June 2005</conf-date></element-citation></ref><ref id="B35-sensors-25-05633"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lopez</surname><given-names>P.G.</given-names></name><name name-style="western"><surname>Montresor</surname><given-names>A.</given-names></name><name name-style="western"><surname>Epema</surname><given-names>D.</given-names></name><name name-style="western"><surname>Datta</surname><given-names>A.</given-names></name><name name-style="western"><surname>Higashino</surname><given-names>T.</given-names></name><name name-style="western"><surname>Lamnitchi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Barcellos</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Felber</surname><given-names>P.</given-names></name><name name-style="western"><surname>Riviere</surname><given-names>E.</given-names></name></person-group><article-title>Edge-centric Computing: Vision and Challenges</article-title><source>ACM SIGCOMM Comput. Commun. Rev.</source><year>2015</year><volume>45</volume><fpage>37</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1145/2831347.2831354</pub-id></element-citation></ref><ref id="B36-sensors-25-05633"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.C.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pham</surname><given-names>Q.V.</given-names></name><name name-style="western"><surname>Pathirana</surname><given-names>P.N.</given-names></name><name name-style="western"><surname>Le</surname><given-names>L.B.</given-names></name><name name-style="western"><surname>Seneviratne</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Niyato</surname><given-names>D.</given-names></name><name name-style="western"><surname>Poor</surname><given-names>H.V.</given-names></name></person-group><article-title>Federated Learning Meets Blockchain in Edge Computing: Opportunities and Challenges</article-title><source>IEEE Internet Things J.</source><year>2021</year><volume>8</volume><fpage>12806</fpage><lpage>12825</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2021.3072611</pub-id></element-citation></ref><ref id="B37-sensors-25-05633"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alsamhi</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Shvetsov</surname><given-names>A.V.</given-names></name><name name-style="western"><surname>Hawbani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shvetsova</surname><given-names>S.V.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name></person-group><article-title>Survey on Federated Learning enabling indoor navigation for industry 4.0 in B5G</article-title><source>Future Gener. Comput. Syst.</source><year>2023</year><volume>148</volume><fpage>250</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1016/j.future.2023.06.001</pub-id></element-citation></ref><ref id="B38-sensors-25-05633"><label>38.</label><element-citation publication-type="book"><person-group person-group-type="editor"><name name-style="western"><surname>Bhattacharya</surname><given-names>P.</given-names></name><name name-style="western"><surname>Verma</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tanwar</surname><given-names>S.</given-names></name></person-group><source>Federated Learning for Internet of Medical Things: Concepts, Paradigms, and Solutions</source><edition>1st ed.</edition><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B39-sensors-25-05633"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Albaseer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ciftler</surname><given-names>B.S.</given-names></name><name name-style="western"><surname>Abdallah</surname><given-names>M.</given-names></name><name name-style="western"><surname>Al-Fuqaha</surname><given-names>A.</given-names></name></person-group><article-title>Exploiting Unlabeled Data in Smart Cities using Federated Edge Learning</article-title><source>Proceedings of the 2020 International Wireless Communications and Mobile Computing (IWCMC)</source><conf-loc>Limassol, Cyprus</conf-loc><conf-date>15&#8211;19 June 2020</conf-date><fpage>1666</fpage><lpage>1671</lpage></element-citation></ref><ref id="B40-sensors-25-05633"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Braud</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tarkoma</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hui</surname><given-names>P.</given-names></name></person-group><article-title>Trustworthy AI in the Age of Pervasive Computing and Big Data</article-title><source>Proceedings of the 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)</source><conf-loc>Austin, TX, USA</conf-loc><conf-date>23&#8211;27 March 2020</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B41-sensors-25-05633"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>W.</given-names></name></person-group><article-title>Heterogeneous Secure Transmissions in IRS-Assisted NOMA Communications: CO-GNN Approach</article-title><source>IEEE Internet Things J.</source><year>2025</year><volume>12</volume><fpage>34113</fpage><lpage>34125</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2025.3577332</pub-id></element-citation></ref><ref id="B42-sensors-25-05633"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>C.</given-names></name></person-group><article-title>Adaptive critic design for safety-optimal FTC of unknown nonlinear systems with asymmetric constrained-input</article-title><source>ISA Trans.</source><year>2024</year><volume>155</volume><fpage>309</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1016/j.isatra.2024.09.018</pub-id><pub-id pub-id-type="pmid">39306561</pub-id></element-citation></ref><ref id="B43-sensors-25-05633"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>Securing Multidestination Transmissions with Relay and Friendly Interference Collaboration</article-title><source>IEEE Internet Things J.</source><year>2024</year><volume>11</volume><fpage>18782</fpage><lpage>18795</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2024.3367001</pub-id></element-citation></ref><ref id="B44-sensors-25-05633"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hart</surname><given-names>P.</given-names></name></person-group><article-title>Nearest neighbor pattern classification</article-title><source>IEEE Trans. Inf. Theory</source><year>1967</year><volume>13</volume><fpage>21</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1109/TIT.1967.1053964</pub-id></element-citation></ref><ref id="B45-sensors-25-05633"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fix</surname><given-names>E.</given-names></name><name name-style="western"><surname>Hodges</surname><given-names>J.L.</given-names></name></person-group><article-title>An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation</article-title><source>Int. Stat. Rev.</source><year>1951</year><volume>57</volume><fpage>233</fpage><lpage>247</lpage></element-citation></ref><ref id="B46-sensors-25-05633"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aledhari</surname><given-names>M.</given-names></name><name name-style="western"><surname>Razzak</surname><given-names>R.</given-names></name><name name-style="western"><surname>Parizi</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Saeed</surname><given-names>F.</given-names></name></person-group><article-title>Federated Learning: A Survey on Enabling Technologies, Protocols, and Applications</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>140699</fpage><lpage>140725</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3013541</pub-id><pub-id pub-id-type="pmid">32999795</pub-id><pub-id pub-id-type="pmcid">PMC7523633</pub-id></element-citation></ref><ref id="B47-sensors-25-05633"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Karimireddy</surname><given-names>S.P.</given-names></name><name name-style="western"><surname>Kale</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mohri</surname><given-names>S.R.M.</given-names></name><name name-style="western"><surname>Stich</surname><given-names>S.</given-names></name><name name-style="western"><surname>Suresh</surname><given-names>A.T.</given-names></name></person-group><article-title>SCAFFOLD: Stochastic Controlled Averaging for Federated Learning</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Online</conf-loc><conf-date>13&#8211;18 July 2020</conf-date></element-citation></ref><ref id="B48-sensors-25-05633"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Malinovskiy</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kovalev</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gasanov</surname><given-names>E.</given-names></name><name name-style="western"><surname>Condat</surname><given-names>L.</given-names></name><name name-style="western"><surname>Richtarik</surname><given-names>P.</given-names></name></person-group><article-title>From Local SGD to Local Fixed-Point Methods for Federated Learning</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Online</conf-loc><conf-date>13&#8211;18 July 2020</conf-date></element-citation></ref><ref id="B49-sensors-25-05633"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pathak</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wainwright</surname><given-names>M.J.</given-names></name></person-group><article-title>Fedsplit: An algorithmic framework for fast federated optimization</article-title><source>Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS&#8217;20)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>6&#8211;12 December 2020</conf-date></element-citation></ref><ref id="B50-sensors-25-05633"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Charles</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Kone&#269;n&#7923;</surname><given-names>J.</given-names></name></person-group><article-title>Convergence and Accuracy Trade-offs in Federated Learning and Meta-learning</article-title><source>Proceedings of the International Conference on Artificial Intelligence and Statistics</source><conf-loc>Virtual</conf-loc><conf-date>13&#8211;15 April 2021</conf-date></element-citation></ref><ref id="B51-sensors-25-05633"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Joshi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Poor</surname><given-names>H.V.</given-names></name></person-group><article-title>Tackling the objective inconsistency problem in heterogeneous federated optimization</article-title><source>Proceedings of the 34th International Conference on Neural Information Processing Systems</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>6&#8211;12 December 2020</conf-date></element-citation></ref><ref id="B52-sensors-25-05633"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Collins</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hassani</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mokhtari</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shakkottai</surname><given-names>S.</given-names></name></person-group><article-title>FedAvg with Fine Tuning: Local Updates Lead to Representation Learning</article-title><source>Proceedings of the 36th International Conference on Neural Information Processing Systems</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>28 November&#8211;9 December 2022</conf-date></element-citation></ref><ref id="B53-sensors-25-05633"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Sahu</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Zaheer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sanjabi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Talwalkar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Smith</surname><given-names>V.</given-names></name></person-group><article-title>Federated Optimization in Heterogeneous Networks</article-title><source>Proceedings of the 3rd Machine Learning and Systems Conference</source><conf-loc>Austin, TX, USA</conf-loc><conf-date>2&#8211;4 March 2020</conf-date><volume>Volume 2</volume><fpage>429</fpage><lpage>450</lpage></element-citation></ref><ref id="B54-sensors-25-05633"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>AdaptiveFL: Adaptive Heterogeneous Federated Learning for Resource-Constrained AIoT Systems</article-title><source>ACM Trans. Des. Autom. Electron. Syst.</source><year>2025</year><volume>30</volume><fpage>40</fpage></element-citation></ref><ref id="B55-sensors-25-05633"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baqer</surname><given-names>M.</given-names></name></person-group><article-title>Energy-Efficient Federated Learning for Internet of Things: Leveraging In-Network Processing and Hierarchical Clustering</article-title><source>Future Internet</source><year>2025</year><volume>17</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3390/fi17010004</pub-id></element-citation></ref><ref id="B56-sensors-25-05633"><label>56.</label><element-citation publication-type="book"><source>MATLAB</source><comment>Version 24.2.0.2923080 (R2024b)</comment><publisher-name>The MathWorks Inc.</publisher-name><publisher-loc>Natick, MA, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B57-sensors-25-05633"><label>57.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>Python Software Foundation</collab></person-group><source>Python</source><comment>Version 3.13.3</comment><publisher-name>Python Software Foundation</publisher-name><publisher-loc>Beaverton, OR, USA</publisher-loc><year>2025</year></element-citation></ref><ref id="B58-sensors-25-05633"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>L.</given-names></name></person-group><article-title>The MNIST Database of Handwritten Digit Images for Machine Learning Research</article-title><source>IEEE Signal Process. Mag.</source><year>2012</year><volume>29</volume><fpage>141</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1109/MSP.2012.2211477</pub-id></element-citation></ref><ref id="B59-sensors-25-05633"><label>59.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name></person-group><article-title>Learning Multiple Layers of Features from Tiny Images. Technical Report</article-title><year>2009</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf" ext-link-type="uri">https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-15">(accessed on 15 August 2025)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05633-f001" orientation="portrait"><label>Figure 1</label><caption><p>Accuracy comparison between traditional <italic toggle="yes">k</italic>-NN and TFNN using 100 IoT nodes, evaluated at <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> while increasing the number of stored patterns from 10 to 500 patterns.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05633-g001.jpg"/></fig><fig position="float" id="sensors-25-05633-f002" orientation="portrait"><label>Figure 2</label><caption><p>Single-shot (one communication round) classification accuracy comparing TFNN (<inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) with AdaptiveFL, FedProx, SCAFFOLD, and FedAvg.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05633-g002.jpg"/></fig><fig position="float" id="sensors-25-05633-f003" orientation="portrait"><label>Figure 3</label><caption><p>Single-shot classification accuracy of TFNN (<inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) compared with AdaptiveFL, FedProx, SCAFFOLD, and FedAvg after 200 communication rounds.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05633-g003.jpg"/></fig><table-wrap position="float" id="sensors-25-05633-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05633-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of federated learning algorithms FedAvg, FedProx, SCAFFOLD, and AdaptiveFL.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FedAvg</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FedProx</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SCAFFOLD</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AdaptiveFL</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Proposed by</td><td align="center" valign="middle" rowspan="1" colspan="1">McMahan&#160;et&#160;al., 2017 [<xref rid="B8-sensors-25-05633" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Li&#160;et&#160;al., 2020 [<xref rid="B53-sensors-25-05633" ref-type="bibr">53</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Karimireddy&#160;et&#160;al., 2020 [<xref rid="B47-sensors-25-05633" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Jia&#160;et&#160;al., 2024 [<xref rid="B54-sensors-25-05633" ref-type="bibr">54</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Convergence rate</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate (suitable for IID)</td><td align="center" valign="middle" rowspan="1" colspan="1">More stable than FedAvg on non-IID; moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Faster on non-IID (drift corrected)</td><td align="center" valign="middle" rowspan="1" colspan="1">Faster than FedAvg under heterogeneous settings</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Computation cost (per round)</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Low&#8211;medium (proximal term)</td><td align="center" valign="middle" rowspan="1" colspan="1">medium (control updates)</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium (slice/merge + subnet training)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Communication rounds</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">High&#8211;moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Fewer than FedAvg on non-IID</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate; fewer than FedAvg when device heterogeneity is well matched</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Robustness to system/data heterogeneity</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">Partial (regularizes objective)</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes (corrects drift)</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes (adapts model to device)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High on IID; drops on non-IID</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">More stable than FedAvg on non-IID</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High; provable convergence on non-IID</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Competitive; benefits from adaptive modeling</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05633-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05633-t002_Table 2</object-id><label>Table 2</label><caption><p>Accuracy after 200 communication rounds using MNIST dataset under IID vs. non-IID partitions across varying numbers of participating nodes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Nodes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AdaptiveFL</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">FedProx</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">SCAFFOLD</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">FedAvg</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">TFNN (k = 3)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">91.33%</td><td align="center" valign="middle" rowspan="1" colspan="1">31.63%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.09%</td><td align="center" valign="middle" rowspan="1" colspan="1">51.30%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.13%</td><td align="center" valign="middle" rowspan="1" colspan="1">52.14%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.18%</td><td align="center" valign="middle" rowspan="1" colspan="1">48.91%</td><td align="center" valign="middle" rowspan="1" colspan="1">89.05%</td><td align="center" valign="middle" rowspan="1" colspan="1">32.93%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">92.44%</td><td align="center" valign="middle" rowspan="1" colspan="1">37.55%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.26%</td><td align="center" valign="middle" rowspan="1" colspan="1">80.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.59%</td><td align="center" valign="middle" rowspan="1" colspan="1">78.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.96%</td><td align="center" valign="middle" rowspan="1" colspan="1">80.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">90.08%</td><td align="center" valign="middle" rowspan="1" colspan="1">42.30%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">50</td><td align="center" valign="middle" rowspan="1" colspan="1">92.63%</td><td align="center" valign="middle" rowspan="1" colspan="1">52.55%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.38%</td><td align="center" valign="middle" rowspan="1" colspan="1">80.60%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.44%</td><td align="center" valign="middle" rowspan="1" colspan="1">77.39%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.26%</td><td align="center" valign="middle" rowspan="1" colspan="1">75.98%</td><td align="center" valign="middle" rowspan="1" colspan="1">90.30%</td><td align="center" valign="middle" rowspan="1" colspan="1">39.41%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">75</td><td align="center" valign="middle" rowspan="1" colspan="1">92.61%</td><td align="center" valign="middle" rowspan="1" colspan="1">86.66%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.05%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.73%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.67%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.84%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.26%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.71%</td><td align="center" valign="middle" rowspan="1" colspan="1">90.43%</td><td align="center" valign="middle" rowspan="1" colspan="1">47.72%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.35%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.00%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.21%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.61%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.65%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.94%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.71%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.98%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.48%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.84%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05633-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05633-t003_Table 3</object-id><label>Table 3</label><caption><p>Accuracy after 200 communication rounds using CIFAR-10 dataset under IID vs. non-IID partitions across varying numbers of participating nodes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Nodes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AdaptiveFL</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">FedProx</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">SCAFFOLD</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">FedAvg</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">TFNN (k = 3)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">35.73%</td><td align="center" valign="middle" rowspan="1" colspan="1">13.80%</td><td align="center" valign="middle" rowspan="1" colspan="1">36.83%</td><td align="center" valign="middle" rowspan="1" colspan="1">19.68%</td><td align="center" valign="middle" rowspan="1" colspan="1">36.64%</td><td align="center" valign="middle" rowspan="1" colspan="1">10.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">36.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">17.93%</td><td align="center" valign="middle" rowspan="1" colspan="1">23.62%</td><td align="center" valign="middle" rowspan="1" colspan="1">18.87%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">35.41%</td><td align="center" valign="middle" rowspan="1" colspan="1">13.53%</td><td align="center" valign="middle" rowspan="1" colspan="1">37.06%</td><td align="center" valign="middle" rowspan="1" colspan="1">14.76%</td><td align="center" valign="middle" rowspan="1" colspan="1">38.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">9.94%</td><td align="center" valign="middle" rowspan="1" colspan="1">39.20%</td><td align="center" valign="middle" rowspan="1" colspan="1">14.91%</td><td align="center" valign="middle" rowspan="1" colspan="1">24.28%</td><td align="center" valign="middle" rowspan="1" colspan="1">18.59%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">50</td><td align="center" valign="middle" rowspan="1" colspan="1">35.78%</td><td align="center" valign="middle" rowspan="1" colspan="1">12.98%</td><td align="center" valign="middle" rowspan="1" colspan="1">38.60%</td><td align="center" valign="middle" rowspan="1" colspan="1">17.58%</td><td align="center" valign="middle" rowspan="1" colspan="1">36.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">10.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">36.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">15.39%</td><td align="center" valign="middle" rowspan="1" colspan="1">24.75%</td><td align="center" valign="middle" rowspan="1" colspan="1">23.19%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">75</td><td align="center" valign="middle" rowspan="1" colspan="1">36.67%</td><td align="center" valign="middle" rowspan="1" colspan="1">25.53%</td><td align="center" valign="middle" rowspan="1" colspan="1">37.09%</td><td align="center" valign="middle" rowspan="1" colspan="1">31.44%</td><td align="center" valign="middle" rowspan="1" colspan="1">36.57%</td><td align="center" valign="middle" rowspan="1" colspan="1">19.74%</td><td align="center" valign="middle" rowspan="1" colspan="1">37.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">32.44%</td><td align="center" valign="middle" rowspan="1" colspan="1">25.37%</td><td align="center" valign="middle" rowspan="1" colspan="1">27.61%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.46%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.29%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.85%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.64%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.60%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.34%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.82%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.70%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.23%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.61%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05633-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05633-t004_Table 4</object-id><label>Table 4</label><caption><p>Accuracy after 200 communication rounds using CIFAR-100 dataset under IID vs. non-IID partitions across varying numbers of participating nodes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Nodes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AdaptiveFL</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">FedProx</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">SCAFFOLD</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">FedAvg</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">TFNN (k = 3)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IID</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-IID</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">4.95%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.29%</td><td align="center" valign="middle" rowspan="1" colspan="1">7.09%</td><td align="center" valign="middle" rowspan="1" colspan="1">2.34%</td><td align="center" valign="middle" rowspan="1" colspan="1">8.34%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">7.26%</td><td align="center" valign="middle" rowspan="1" colspan="1">2.47%</td><td align="center" valign="middle" rowspan="1" colspan="1">7.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.69%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">4.84%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.69%</td><td align="center" valign="middle" rowspan="1" colspan="1">6.51%</td><td align="center" valign="middle" rowspan="1" colspan="1">3.14%</td><td align="center" valign="middle" rowspan="1" colspan="1">6.68%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.22%</td><td align="center" valign="middle" rowspan="1" colspan="1">8.73%</td><td align="center" valign="middle" rowspan="1" colspan="1">3.13%</td><td align="center" valign="middle" rowspan="1" colspan="1">8.87%</td><td align="center" valign="middle" rowspan="1" colspan="1">1.60%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">50</td><td align="center" valign="middle" rowspan="1" colspan="1">6.69%</td><td align="center" valign="middle" rowspan="1" colspan="1">3.36%</td><td align="center" valign="middle" rowspan="1" colspan="1">7.57%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">8.17%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.99%</td><td align="center" valign="middle" rowspan="1" colspan="1">7.87%</td><td align="center" valign="middle" rowspan="1" colspan="1">3.84%</td><td align="center" valign="middle" rowspan="1" colspan="1">9.70%</td><td align="center" valign="middle" rowspan="1" colspan="1">5.36%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">75</td><td align="center" valign="middle" rowspan="1" colspan="1">6.54%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.15%</td><td align="center" valign="middle" rowspan="1" colspan="1">8.80%</td><td align="center" valign="middle" rowspan="1" colspan="1">5.52%</td><td align="center" valign="middle" rowspan="1" colspan="1">8.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.97%</td><td align="center" valign="middle" rowspan="1" colspan="1">5.07%</td><td align="center" valign="middle" rowspan="1" colspan="1">4.63%</td><td align="center" valign="middle" rowspan="1" colspan="1">10.37%</td><td align="center" valign="middle" rowspan="1" colspan="1">6.90%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.11%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.30%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.26%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.32%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.35%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.00%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.27%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.57%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.15%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.28%</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>