<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473549</article-id><article-id pub-id-type="pmcid-ver">PMC12473549.1</article-id><article-id pub-id-type="pmcaid">12473549</article-id><article-id pub-id-type="pmcaiid">12473549</article-id><article-id pub-id-type="pmid">41013001</article-id><article-id pub-id-type="doi">10.3390/s25185765</article-id><article-id pub-id-type="publisher-id">sensors-25-05765</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Human Activity Recognition via Attention-Augmented TCN-BiGRU Fusion</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>He</surname><given-names initials="JL">Ji-Long</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05765" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7396-7042</contrib-id><name name-style="western"><surname>Wang</surname><given-names initials="JH">Jian-Hong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05765" ref-type="aff">1</xref><xref rid="c1-sensors-25-05765" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5536-2071</contrib-id><name name-style="western"><surname>Lo</surname><given-names initials="CM">Chih-Min</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05765" ref-type="aff">2</xref><xref rid="c1-sensors-25-05765" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jiang</surname><given-names initials="Z">Zhaodi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05765" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Loh</surname><given-names initials="K">Kenneth</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05765"><label>1</label>School of Computer Science and Technology, Shandong University of Technology, Zibo 255000, China; <email>23505020692@stumail.sdut.edu.cn</email> (J.-L.H.); <email>zdjiang1011@163.com</email> (Z.J.)</aff><aff id="af2-sensors-25-05765"><label>2</label>Department of Digital Multimedia Design, School of Innovative Design and Management, National Taipei University of Business, No. 321, Sec. 1, Jinan Rd., Zhongzheng District, Taipei City 100025, Taiwan</aff><author-notes><corresp id="c1-sensors-25-05765"><label>*</label>Correspondence: <email>jhwang@sdut.edu.cn</email> (J.-H.W.); <email>cmlotw@ntub.edu.tw</email> (C.-M.L.)</corresp></author-notes><pub-date pub-type="epub"><day>16</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5765</elocation-id><history><date date-type="received"><day>04</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>31</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>12</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>16</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05765.pdf"/><abstract><p>With the widespread application of wearable sensors in health monitoring and human&#8211;computer interaction, deep learning-based human activity recognition (HAR) research faces challenges such as the effective extraction of multi-scale temporal features and the enhancement of robustness against noise in multi-source data. This study proposes the TGA-HAR (TCN-GRU-Attention-HAR) model. The TGA-HAR model integrates Temporal Convolutional Neural Networks and Recurrent Neural Networks by constructing a hierarchical feature abstraction architecture through cascading Temporal Convolutional Network (TCN) and Bidirectional Gated Recurrent Unit (BiGRU) layers for complex activity recognition. This study utilizes TCN layers with dilated convolution kernels to extract multi-order temporal features. This study utilizes BiGRU layers to capture bidirectional temporal contextual correlation information. To further optimize feature representation, the TGA-HAR model introduces residual connections to enhance the stability of gradient propagation and employs an adaptive weighted attention mechanism to strengthen feature representation. The experimental results of this study demonstrate that the model achieved test accuracies of 99.37% on the WISDM dataset, 95.36% on the USC-HAD dataset, and 96.96% on the PAMAP2 dataset. Furthermore, we conducted tests on datasets collected in real-world scenarios. This method provides a highly robust solution for complex human activity recognition tasks.</p></abstract><kwd-group><kwd>human activity recognition</kwd><kwd>temporal convolutional network</kwd><kwd>bidirectional gated recurrent unit</kwd><kwd>attention mechanism</kwd><kwd>sensor data sequences</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05765"><title>1. Introduction</title><p>With the rapid development of Internet of Things (IoT) technology and the proliferation of wearable devices, human activity recognition (HAR) using inertial data has become an important research direction in the field of artificial intelligence due to the low cost and portability of wearable devices [<xref rid="B1-sensors-25-05765" ref-type="bibr">1</xref>]. Human activity recognition systems mainly include two types of methods, video-based and sensor-based, with the latter utilizing multi-inertial measurement unit (IMU) data fusion to handle the complexity of sensor data. Sensor-based methods demonstrate broad application prospects in fields such as healthcare monitoring (e.g., fall detection for elderly individuals, chronic disease management) [<xref rid="B2-sensors-25-05765" ref-type="bibr">2</xref>], smart home control (automatic adjustment based on user behavior) [<xref rid="B3-sensors-25-05765" ref-type="bibr">3</xref>], and sports science analysis (athlete posture optimization) [<xref rid="B4-sensors-25-05765" ref-type="bibr">4</xref>]. However, constructing efficient and robust recognition models remains a major challenge in current research due to the characteristics of sensor data such as high dimensionality (multiple data channels), strong temporal dependency (rapidly changing data with strong sequential correlations), and susceptibility to noise interference.</p><p>Traditional HAR methods primarily rely on manual feature engineering combined with shallow machine learning models (e.g., Random Forests, Support Vector Machines). This approach requires domain experts to invest significant time in designing time-domain features (e.g., mean, variance), frequency domain features (e.g., Fourier coefficients), and time&#8211;frequency domain features (e.g., wavelet transforms). The performance of these methods is constrained by the comprehensiveness of feature design [<xref rid="B5-sensors-25-05765" ref-type="bibr">5</xref>].</p><p>In recent years, deep learning techniques have achieved significant breakthroughs in HAR tasks through their end-to-end feature learning mechanism. Convolutional Neural Network (CNN) can effectively extract local spatial patterns from sensor signals via its local receptive fields, but struggles to capture long-term temporal dependency features [<xref rid="B6-sensors-25-05765" ref-type="bibr">6</xref>]. Recurrent Neural Network (RNN) and its variants (LSTM/GRU) can capture temporal dynamics. However, traditional LSTM suffers from parameter redundancy due to its complex gating mechanisms. In contrast, the Gated Recurrent Unit (GRU) significantly reduces computational complexity by streamlining the gating structure (merging update and reset gates) while maintaining temporal feature extraction capabilities, making it more suitable for resource-constrained mobile scenarios [<xref rid="B7-sensors-25-05765" ref-type="bibr">7</xref>].</p><p>The rise of Temporal Convolutional Networks (TCN) has introduced new research pathways in the field of time-series signal analysis. Based on the causal dilated convolution structure of TCN, the model can efficiently capture long-term dependencies while strictly maintaining temporal causality by progressively expanding the receptive field through exponentially increasing dilation rates [<xref rid="B8-sensors-25-05765" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05765" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05765" ref-type="bibr">10</xref>]. However, as demands evolve toward modeling more complex long-term dynamic patterns, the single TCN architecture still has room for optimization in terms of feature interaction depth and time-varying state modeling.</p><p>Existing research often overlooks the varying importance of key time periods in sensor signals, failing to effectively integrate the local feature extraction strengths of CNN with the global temporal feature extraction advantages of Recurrent Neural Networks. Simultaneously, some studies directly use the output of RNN as final features for classification, lacking sufficient capability to suppress noisy signals and failing to effectively eliminate redundant components in the features [<xref rid="B11-sensors-25-05765" ref-type="bibr">11</xref>].</p><p>Therefore, this study proposes a multi-level fusion model named TCN-GRU-Attention-HAR (TGA-HAR). The TGA-HAR model integrates Temporal Convolutional Network (TCN), Bidirectional GRU, and Attention mechanism. First, the dilated causal convolution structure of TCN expands the receptive field. By stacking residual connections, the model achieves multi-scale temporal feature extraction, overcoming the locality constraints of traditional CNN. Second, a Bidirectional GRU (BiGRU) layer is introduced to capture bidirectional contextual dependencies in sequential data. Its streamlined gating mechanism reduces the number of parameters while improving training efficiency. Finally, the Attention mechanism dynamically allocates weights to different time steps, amplifying key motion features and suppressing noise interference. This hierarchical architecture enables the model to simultaneously capture local morphological features and long-term temporal patterns in sensor data, forming complementary and enhanced feature representations.</p><p>This study adopts an end-to-end learning framework, directly inputting preprocessed time-domain data into the deep learning model. Through its multi-layer network structure, the model automatically extracts spatiotemporal features of activities and outputs classification results, eliminating the manual statistical feature design process in traditional methods. This approach fully leverages the representation learning capability of deep learning, thereby enhancing the model&#8217;s adaptability to complex motion patterns.</p><p>To validate the effectiveness and generalization capability of the model, this study systematically evaluates the proposed method on three public datasets (PAMAP2, USC-HAD, and WISDM). These datasets exhibit significant heterogeneity in terms of sensor types (Inertial Measurement Unit (IMU)/accelerometer/gyroscope), activity categories (basic daily activities/complex motions), and data collection environments (laboratory-controlled/daily-life settings). We used the Real-World IDLab Dataset to test the performance of the TGA-HAR model in real-world conditions. Through cross-dataset comparative experiments, this study not only verifies the model&#8217;s superior performance in single-scenario evaluations but also rigorously examines its robustness under different sensor configurations and varying behavioral complexity levels.</p><p>The main contributions of this study are summarized as follows:<list list-type="bullet"><list-item><p>This study proposes a TCN-GRU-Attention multidimensional feature fusion framework, which realizes complex human activity recognition by hierarchically integrating local features extracted from sensor data with global temporal relational features.</p></list-item><list-item><p>This study designs a collaborative optimization strategy that combines TCN-BiGRU and the Attention mechanism: TCN extracts multi-scale local features, BiGRU captures bidirectional global temporal dependencies, and the Attention mechanism dynamically assigns weights to key time steps, thereby enhancing the discriminative power of feature representations.</p></list-item><list-item><p>This study systematically verifies the effectiveness of fusing multidimensional inertial data. Experiments on the WISDM, USC-HAD, PAMAP2, and Real-World IDLab Dataset, which cover activity categories from basic daily behaviors to high-intensity exercises, confirm the model&#8217;s superior performance in single scenarios and robust generalization across diverse sensor configurations and activity complexities.</p></list-item></list></p><p>The subsequent sections of this paper are organized as follows: <xref rid="sec2-sensors-25-05765" ref-type="sec">Section 2</xref> reviews the research progress of deep learning in the field of HAR; <xref rid="sec3-sensors-25-05765" ref-type="sec">Section 3</xref> elaborates on the model architecture and design of TCN-GRU-Attention; <xref rid="sec4-sensors-25-05765" ref-type="sec">Section 4</xref> introduces the experimental datasets, experimental design, and experimental results; <xref rid="sec5-sensors-25-05765" ref-type="sec">Section 5</xref> summarizes this paper.</p></sec><sec id="sec2-sensors-25-05765"><title>2. Related Work</title><p><xref rid="sensors-25-05765-f001" ref-type="fig">Figure 1</xref> presents a comparison of research on human activity recognition based on data sources and feature extraction methods. Human activity recognition primarily includes two approaches: vision-based methods and sensor-based methods. Vision-based methods utilize data sources such as RGB cameras [<xref rid="B12-sensors-25-05765" ref-type="bibr">12</xref>], depth cameras [<xref rid="B13-sensors-25-05765" ref-type="bibr">13</xref>], and infrared cameras [<xref rid="B14-sensors-25-05765" ref-type="bibr">14</xref>], extracting features through optical flow [<xref rid="B15-sensors-25-05765" ref-type="bibr">15</xref>], 3D skeletons [<xref rid="B16-sensors-25-05765" ref-type="bibr">16</xref>], and pose estimation [<xref rid="B17-sensors-25-05765" ref-type="bibr">17</xref>] to analyze human motions. Sensor-based methods rely on inertial measurement units (IMUs) [<xref rid="B18-sensors-25-05765" ref-type="bibr">18</xref>], accelerometers [<xref rid="B19-sensors-25-05765" ref-type="bibr">19</xref>], gyroscopes [<xref rid="B20-sensors-25-05765" ref-type="bibr">20</xref>], and magnetometers [<xref rid="B21-sensors-25-05765" ref-type="bibr">21</xref>] for data acquisition, employing feature extraction techniques such as time-domain analysis [<xref rid="B22-sensors-25-05765" ref-type="bibr">22</xref>], frequency domain analysis [<xref rid="B23-sensors-25-05765" ref-type="bibr">23</xref>], and sensor fusion [<xref rid="B24-sensors-25-05765" ref-type="bibr">24</xref>] to achieve efficient human activity recognition. In recent years, activity recognition technology based on WiFi channel state information (CSI) has also attracted attention due to its non-intrusive nature and environmental universality. It achieves detection by analyzing changes in the multipath propagation of wireless signals under the interference of human movement [<xref rid="B25-sensors-25-05765" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05765" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05765" ref-type="bibr">27</xref>].</p><p>However, vision-based methods suffer from limitations including high hardware costs (expensive high-definition cameras), poor privacy protection, and strong environmental dependencies (e.g., sensitivity to background lighting and susceptibility to occlusion). Activity recognition technology based on WiFi CSI has limitations of sensitivity to environmental noise, strong hardware dependence, and high computational complexity. Therefore, this study focuses on sensor-based methods for human activity recognition, specifically utilizing IMUs and multi-sensor fusion to accomplish complex human activity recognition.</p><p><xref rid="sensors-25-05765-t001" ref-type="table">Table 1</xref> presents several existing research approaches in human activity recognition (HAR), their used datasets, data sources, and advantages. The Multi-branch CNN-BiLSTM achieved significant progress in multidimensional data fusion strategies for HAR based on inertial sensor data. This hybrid architecture combining multi-branch convolutional neural networks (CNN) with bidirectional long short-term memory networks (BiLSTM) could extract multi-scale temporal features using convolutional kernels of different sizes [<xref rid="B28-sensors-25-05765" ref-type="bibr">28</xref>]. The Attention-based Multi-head CNN incorporated a squeeze-and-excitation attention module to dynamically adjust sensor channel weights, effectively suppressing noise interference [<xref rid="B29-sensors-25-05765" ref-type="bibr">29</xref>]. The CNN-LSTM-Self-Attention model extracted joint temporal features from triaxial acceleration, gyroscope, and linear acceleration signals. This hybrid model achieved favorable recognition performance on the self-collected H-Activity dataset, demonstrating the effectiveness of multimodal data fusion for complex action classification [<xref rid="B30-sensors-25-05765" ref-type="bibr">30</xref>].</p><p>To address feature space optimization, the spatial attention CNN + genetic algorithm employed a spatiotemporal encoding method based on continuous wavelet transform to convert sensor signals into 2D images, combined with a spatial attention mechanism. This approach integrated genetic algorithms for optimal feature subset selection [<xref rid="B31-sensors-25-05765" ref-type="bibr">31</xref>]. CNN-BiGRU utilized a cascaded model combining Bidirectional Gated Recurrent Units (BiGRU) with convolutional neural networks (CNN). It enhanced long-range feature memory through bidirectional temporal dependency extraction [<xref rid="B32-sensors-25-05765" ref-type="bibr">32</xref>]. TCN-Attention-HAR integrated a multi-scale Temporal Convolutional Network with a self-attention mechanism, employing knowledge distillation to compress model parameters. This demonstrated synergistic advantages in temporal feature extraction and feature compression [<xref rid="B33-sensors-25-05765" ref-type="bibr">33</xref>].</p><p>For global dependencies, the Temporal-Channel Dual-Branch Self-Attention Network independently processed temporal dimensions and cross-sensor channel interactions using dual-branch convolutional self-attention networks. This architecture achieved higher average recognition accuracy than traditional LSTM and Transformer models on seven public datasets including MHEALTH [<xref rid="B34-sensors-25-05765" ref-type="bibr">34</xref>]. Transformer applied self-attention mechanisms to analyze accelerometer and gyroscope signals, successfully capturing global cross-timestep dependencies. It achieved high recognition accuracy on the KU-HAR dataset, providing a novel paradigm for end-to-end multi-sensor fusion [<xref rid="B35-sensors-25-05765" ref-type="bibr">35</xref>].</p></sec><sec sec-type="methods" id="sec3-sensors-25-05765"><title>3. Methods</title><p>This study integrates TCN, BiGRU, and the attention mechanism to propose the TGA-HAR model. Through hierarchical fusion of TCN, BiGRU, and the attention mechanism, a three-stage feature extraction and dynamic weighting framework is constructed to achieve recognition of complex human activities. The overall architecture is shown in <xref rid="sensors-25-05765-f002" ref-type="fig">Figure 2</xref>.</p><p>In the proposed TGA-HAR model, raw data acquired from sensors are segmented via time windows to ensure each data segment contains complete time-series information. The colored lines in the figure correspond to data collected from different channels, respectively. Subsequently, the time-window-segmented data is fed into the TCN module. Leveraging its characteristic dilated causal convolution, the TCN module extracts multi-scale temporal features. After feature extraction, the processed data is input into the residual BiGRU module to further capture long-term dependencies and forward-backward temporal relationships within the data. The Res-BiGRU output is then delivered to the attention layer. Through learnable fully connected operations, the attention layer achieves adaptive dynamic weight allocation, enhancing the model&#8217;s focus on critical features. Finally, the weighted data undergoes dimensionality reduction via a fully connected layer to accomplish effective human activity recognition.</p><p><xref rid="sensors-25-05765-t002" ref-type="table">Table 2</xref> presents symbols used in <xref rid="sec3-sensors-25-05765" ref-type="sec">Section 3</xref> and their corresponding descriptions.</p><sec id="sec3dot1-sensors-25-05765"><title>3.1. Temporal Convolutional Network</title><p>Temporal Convolutional Network (TCN) addresses the fundamental limitations of traditional convolutional neural networks in extracting features from time-series data, particularly their inability to capture long-term dependencies [<xref rid="B36-sensors-25-05765" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05765" ref-type="bibr">37</xref>]. The fixed kernel size of traditional CNNs restricts their capacity to learn long-range temporal patterns, while TCN overcomes this issue by integrating causal dilated convolution and residual connections. TCN constructs a convolutional architecture specifically designed for sequence data. The TCN framework is illustrated in <xref rid="sensors-25-05765-f003" ref-type="fig">Figure 3</xref> [<xref rid="B36-sensors-25-05765" ref-type="bibr">36</xref>]. It ensures identical lengths for input and output sequences while strictly adhering to causality&#8212;where outputs depend solely on current and historical inputs&#8212;thereby preventing future information leakage. Here, <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the convolution kernel size, and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the dilation factor.</p><sec id="sec3dot1dot1-sensors-25-05765"><title>3.1.1. Causal Dilated Convolution</title><p>Causal convolution is the core design that distinguishes Temporal Convolutional Networks (TCN) from traditional convolutional methods, ensuring strict causality in temporal feature extraction. This mechanism constrains the orientation of the convolutional kernel&#8217;s receptive field, enabling the model to utilize only current and historical information for prediction, thereby preventing future data leakage from affecting time series.</p><p>Dilated convolution (also known as atrous convolution) constitutes an essential component of TCN. By introducing a dilation factor d, it expands the receptive field of the convolutional kernel to capture long-range temporal dependencies without increasing the number of parameters, as illustrated in <xref rid="sensors-25-05765-f004" ref-type="fig">Figure 4</xref> [<xref rid="B37-sensors-25-05765" ref-type="bibr">37</xref>].</p><p>The causal dilated convolution operation is expressed by Equation (1) [<xref rid="B36-sensors-25-05765" ref-type="bibr">36</xref>].<disp-formula id="FD1-sensors-25-05765"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the dilation factor, set to <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>4</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in this study to control the spacing between convolutional kernel elements. The setting of this dilation factor was selected with reference to [<xref rid="B36-sensors-25-05765" ref-type="bibr">36</xref>]. Dilation factors configured as 1, 2, and 4 can effectively facilitate TCN in extracting multi-level local information, thereby aiding the TGA-HAR model in achieving improved performance. <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the input data at time point <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th convolutional layer. When <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, a zero-padding strategy is employed to complete the computation. <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the convolutional kernel weight at position <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The output of causal dilated convolution, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> depends entirely on inputs at time step <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and earlier, strictly preventing any introduction of future temporal data. Simultaneously, by exponentially increasing the dilation factor between layers, the receptive field of TCN expands exponentially with network depth. This significantly enhances the model&#8217;s capability to capture long-term temporal patterns.</p></sec><sec id="sec3dot1dot2-sensors-25-05765"><title>3.1.2. Activation Function</title><p>Within the TCN architecture, we employ a modified Swish activation function, whose mathematical definition is given by Equation (2) [<xref rid="B38-sensors-25-05765" ref-type="bibr">38</xref>].<disp-formula id="FD2-sensors-25-05765"><label>(2)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
Here, we set <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This equation represents a specific instance of the Swish function when <italic toggle="yes">&#946;</italic> = 1, achieving dynamic nonlinear feature transformation through element-wise multiplication of input values with their Sigmoid-gated weights. Compared to conventional activation functions like ReLU, it retains the core advantages of Swish while simplifying parameter configuration.</p></sec><sec id="sec3dot1dot3-sensors-25-05765"><title>3.1.3. Residual Connections</title><p>Residual connections address the issues of vanishing gradients and network performance degradation by propagating shallow-layer features directly to deeper layers via skip connections, whose mathematical formulation is defined by Equation (3) [<xref rid="B39-sensors-25-05765" ref-type="bibr">39</xref>].<disp-formula id="FD3-sensors-25-05765"><label>(3)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
Here, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the input data to the residual block. <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the output data of the residual block, which serves as the input to the next residual block or the subsequent Res-BiGRU module. <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> signifies the residual function composed of dilated causal convolution, batch normalization, and an activation function. <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi>&#969;</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to the convolutional kernel weight matrices, while <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a linear projection matrix activated for dimensionality matching when input and output dimensions differ. In standard TCN implementations, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is typically simplified to an identity matrix, as shown in Equation (4).<disp-formula id="FD4-sensors-25-05765"><label>(4)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the TCN architecture, the structure of the residual block is illustrated in <xref rid="sensors-25-05765-f005" ref-type="fig">Figure 5</xref> [<xref rid="B36-sensors-25-05765" ref-type="bibr">36</xref>]. The main path sequentially processes data through causal dilated convolution, normalization, activation function, and dropout, while the skip connection performs dimensionality transformation when necessary. Here, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the convolution kernel size, and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the dilation factor. Here, we use a 1 &#215; 1 convolution kernel to match the dimensions. The data processed by the causal dilated convolution is summed with the data processed by the 1 &#215; 1 convolution kernel to realize the residual connection.</p></sec></sec><sec id="sec3dot2-sensors-25-05765"><title>3.2. Bidirectional Gated Recurrent Unit</title><p>Considering the inherent bidirectional temporal relationships in human activities, after extracting temporal features via the TCN module, the TGA-HAR model employs a Bidirectional Gated Recurrent Unit (BiGRU) to capture bidirectional contextual dependencies in sequential data [<xref rid="B40-sensors-25-05765" ref-type="bibr">40</xref>]. The BiGRU module strikes a balance between feature fusion efficiency and gradient stability.</p><sec id="sec3dot2dot1-sensors-25-05765"><title>3.2.1. Gated Recurrent Unit</title><p>The Gated Recurrent Unit (GRU), proposed by Cho et al., is a variant of RNN. GRU was designed to address the vanishing gradient problem in traditional RNNs while simplifying the structure of LSTM networks [<xref rid="B40-sensors-25-05765" ref-type="bibr">40</xref>]. Its core design utilizes two gating mechanisms&#8212;the update gate and reset gate&#8212;to dynamically regulate information flow, thereby effectively capturing long-term dependencies in sequential data. Compared with LSTM, GRU reduces the number of parameters by merging gating units, improves training efficiency, and achieves comparable performance in most tasks. The computational procedure of GRU is formalized in Equations (5)&#8211;(8) [<xref rid="B41-sensors-25-05765" ref-type="bibr">41</xref>].<disp-formula id="FD5-sensors-25-05765"><label>(5)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05765"><label>(6)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05765"><label>(7)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>&#8901;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05765"><label>(8)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the current input. <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the update gate state. <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the reset gate state. <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the candidate hidden state. <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Sigmoid activation function. <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the trainable weight matrices for the update gate, reset gate, and candidate hidden state, respectively. <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the hidden state at the previous time step <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#8857;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes elementwise multiplication, and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the final state update.</p></sec><sec id="sec3dot2dot2-sensors-25-05765"><title>3.2.2. Residual Bidirectional Gated Recurrent Unit</title><p>For long-term feature extraction tasks, the Residual Bidirectional Gated Recurrent Unit (Res-BiGRU) architecture constructs an efficient feature learning system through hierarchically stacked bidirectional gating structures and layer-normalized residual connections, as illustrated in <xref rid="sensors-25-05765-f006" ref-type="fig">Figure 6</xref> [<xref rid="B40-sensors-25-05765" ref-type="bibr">40</xref>]. In this paper, the input dimension of Res-BiGRU is set to the number of channels of the convolution kernels in the TCN module.</p><p>The model employs Bidirectional Gated Recurrent Units (BiGRU) to perform parallel forward and reverse temporal scans. Leveraging gating mechanisms, it dynamically filters critical temporal features while deeply integrating bidirectional contextual dependencies of sequential data. To address gradient decay in deep networks, cross-layer residual paths embedded with normalization are designed. Low-level features, after undergoing the normalization operation, are summed and fused with the normalized output of the high-level network. This not only enhances the stability of gradient propagation but also facilitates the compatible interaction of cross-multiscale features. The stacked BiGRU modules achieve progressive abstraction of temporal representations. Gating units maintain efficiency in extracting long-range dependency features with streamlined parameters, while normalized residual connections enable nonlinear synergistic enhancement between local fine-grained patterns and global semantic structures through feature calibration.</p><p>This architecture combines bidirectional context awareness with normalized residual learning, forming a robust and efficient solution for temporal feature extraction.</p></sec></sec><sec id="sec3dot3-sensors-25-05765"><title>3.3. Adaptively Weighted Attention Layer</title><p>To enhance the model&#8217;s focus on critical temporal features, we employ a dynamically weighted attention mechanism. This mechanism recalibrates the temporal characteristics of Res-BiGRU output features through learnable weighting parameters. Models utilizing attention mechanisms can simultaneously capture positional relationships between information elements and quantify the importance of distinct features. This study adopts a feedforward-structured dynamic attention mechanism, whose core concept leverages a learnable function to generate probabilistic weights for historical states, thereby achieving adaptive aggregation of contextual information.</p><p>The formulae for this mechanism are defined by Equations (9)&#8211;(11) [<xref rid="B42-sensors-25-05765" ref-type="bibr">42</xref>].<disp-formula id="FD9-sensors-25-05765"><label>(9)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05765"><label>(10)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05765"><label>(11)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, the function <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is implemented using a learnable fully connected layer, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the hidden state at time t output by Res-BiGRU. <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of hidden units in Res-BiGRU, set to 128 in this study. <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is learned through backpropagation.</p><p>In this paper, the feature <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> with a dimension of [batch_size, seq_len, 128] is mapped to 1-dimensional attention scores <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> via the fully connected layer <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The weight matrix of this fully connected layer has a dimension of [128, 1], and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to a dimension of [batch_size, seq_len, 1]. Subsequently, weight normalization is performed, and a softmax operation is applied to <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> along the &#8220;seq_len&#8221; dimension to obtain attention weights <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (dimension: [batch_size, seq_len, 1]), which ensures the sum of the weights equals 1. Finally, weighted aggregation is implemented: after element-wise multiplication of <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, summation is conducted along the seq_len dimension, yielding the final attention feature <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (dimension: [batch_size, 128]). This feature <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is then used for subsequent fully connected classification.</p><p>The attention weights <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> dynamically evolve during training. As the number of training epochs increases, the feature information <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> gradually becomes more discriminative and representative.</p></sec><sec id="sec3dot4-sensors-25-05765"><title>3.4. Classifier</title><p>In the model architecture of this study, the fully connected (FC) layer serves as the core component of the classifier, undertaking dual functions of feature mapping and classification decision-making. This module consists of three linear transformation layers that progressively compress the dimensionality of input features, ultimately mapping them to an output dimension matching the number of target activity categories. Each linear layer is followed by a LeakyReLU activation function to enhance nonlinear representational capacity. The mathematical formulation of LeakyReLU is defined by Equation (12) [<xref rid="B43-sensors-25-05765" ref-type="bibr">43</xref>].<disp-formula id="FD12-sensors-25-05765"><label>(12)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>LeakyReLU</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#950;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>&#950;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#950;</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>&#955;</mml:mi><mml:mi>&#950;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#950;</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo></mml:mrow><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#955;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
Here, the negative slope coefficient <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 0.01. Compared to standard ReLU, LeakyReLU preserves minimal gradients for negative inputs <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#955;</mml:mi><mml:mo>&#8810;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, effectively mitigating the &#8220;neuron death&#8221; problem while maintaining sparse activation properties to suppress overfitting. This design significantly enhances the stability of gradient backpropagation in deep networks, particularly suitable for asymmetrically distributed data prevalent in temporal features.</p><p>During feature processing, the TCN module extracts local temporal patterns through dilated causal convolution. The Res-BiGRU module captures bidirectional long-term dependencies. The attention layer dynamically focuses on critical time steps.</p><p>The fully connected layer systematically integrates these high-level features. Its first two layers achieve dimensionality reduction and implement nonlinear transformations via LeakyReLU activation functions. This layered design reduces model parameters while preserving discriminative feature information, thereby enabling end-to-end classification.</p></sec></sec><sec sec-type="results" id="sec4-sensors-25-05765"><title>4. Results</title><sec id="sec4dot1-sensors-25-05765"><title>4.1. Dataset Description</title><p>The selection of public datasets for Human Activity Recognition (HAR) research requires balancing activity diversity, sensor modalities, and experimental reproducibility. This study employs three public datasets&#8212;WISDM, USC-HAD, and PAMAP2&#8212;representing low-cost single-sensor scenarios, laboratory-grade multimodal scenarios, and complex multimodal activity scenarios, respectively. Their complementary characteristics cover recognition requirements from basic actions to high-complexity activities, providing benchmark support for research on algorithm robustness and multimodal fusion.</p><p>The WISDM dataset was collected in a laboratory-controlled environment to ensure data quality. Acceleration data for six basic daily activities (walking, jogging, ascending stairs, descending stairs, sitting, standing) were gathered via smartphone accelerometers placed in the front pants leg pockets of 36 participants. With a sampling frequency of 20 Hz, the dataset comprises 1,098,209 samples (approximately 15.25 h of acceleration data). Data collection utilized only triaxial accelerometers, reflecting the typical configuration of early mobile devices [<xref rid="B44-sensors-25-05765" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05765" ref-type="bibr">45</xref>]. Dataset specifications are summarized in <xref rid="sensors-25-05765-t003" ref-type="table">Table 3</xref>.</p><p>USC-HAD (University of Southern California Human Activity Dataset) is a multimodal sensor dataset specifically designed for HAR, providing high-precision, diverse behavioral benchmarks for smart device interaction, healthcare monitoring, and robotic motion analysis. The dataset comprises data from 14 participants performing daily activities in a laboratory setting. Sensor data was captured by a 6-axis Inertial Measurement Unit (IMU)&#8212;including a triaxial accelerometer and gyroscope&#8212;fixed on the right front hip at a sampling frequency of 100 Hz. Each activity was repeated five times to enhance data diversity [<xref rid="B46-sensors-25-05765" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05765" ref-type="bibr">47</xref>]. The average data collection duration per participant was approximately 6 h. Dataset specifications are detailed in <xref rid="sensors-25-05765-t004" ref-type="table">Table 4</xref>, while the executed activities are listed in <xref rid="sensors-25-05765-t005" ref-type="table">Table 5</xref>.</p><p>The PAMAP2 dataset targets complex activity recognition, capturing 18 activities in daily life environments using three Inertial Measurement Units (IMUs) (sampling frequency: 100 Hz) positioned on the wrist, chest, and ankle, and a heart rate sensor (sampling frequency: 9 Hz) worn by nine participants. The data collection duration per participant exceeded 10 h. Its multimodal signals&#8212;including acceleration, gyroscope, magnetometer, and heart rate data&#8212;support sensor fusion research but require handling high-frequency data and partial missing values. Consequently, it is exceptionally suitable for complex-scenario activity recognition and cross-modal analysis [<xref rid="B48-sensors-25-05765" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05765" ref-type="bibr">49</xref>]. The dataset comprises 3,850,491 raw samples, with 2,724,817 valid activity data points (after preprocessing to remove invalid segments). Specifications are detailed in <xref rid="sensors-25-05765-t006" ref-type="table">Table 6</xref>, and executed activities are listed in <xref rid="sensors-25-05765-t007" ref-type="table">Table 7</xref>.</p></sec><sec id="sec4dot2-sensors-25-05765"><title>4.2. Data Preprocessing</title><p>To achieve human activity recognition, time-series signals collected from diverse sensor modalities are segmented into continuous samples via the sliding window technique. These samples are then normalized to [&#8722;1, 1]. The dataset preprocessing workflow is illustrated in <xref rid="sensors-25-05765-f007" ref-type="fig">Figure 7</xref>.</p><p>For the WISDM dataset, considering its low sampling frequency, a longer 4 s window (window length = 80) with 50% overlap rate is adopted during time window segmentation. This dataset approximates real-life scenarios, but its activity categories primarily focus on fundamental motion patterns. We split the training set and test set in an 8:2 ratio based on users. Specifically, the finally selected users for the test set are {1, 9, 11, 16, 19, 23, 28, 31}, and the data of the remaining users is used as the training set.</p><p>For the USC-HAD dataset, linear interpolation is first applied to repair missing values in sensor data, followed by downsampling to 50 Hz to balance computational efficiency and motion detail preservation. Time window segmentation employs a sliding window with a temporal stride of 128 (2.56 s) and 50% overlap rate, generating three-dimensional time-series data (samples &#215; 128 &#215; 6). We split the training set and test set in an 8:2 ratio based on users. Specifically, the finally selected users for the test set are {1, 10, 12}, and the data of the remaining users is used as the training set.</p><p>For the PAMAP2 dataset, we select 12 types of actions for classification, where the action indices correspond to those in <xref rid="sensors-25-05765-t007" ref-type="table">Table 7</xref>: {1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24}. We utilize three IMUs to extract raw triaxial acceleration (&#177;16 G range) and angular velocity. Missing values are filled via linear interpolation, and valid activity segments are selected. We extract 18-dimensional time-series data from hand-, waist-, and ankle-mounted IMUs, using activity labels as supervision signals. To reduce computational complexity, the data is downsampled to 33 Hz and subsequently normalized to the range of [&#8722;1, 1]. During time window segmentation, a window length of 169 samples (corresponding to 5.12 s) is adopted, with a stride of 84 samples (resulting in a 50% overlap rate). We select the data of Subject 105 as the test set, while the data of the remaining subjects is used as the training set.</p><p>This preprocessing preserves the temporal dynamics of sensor time-series while enhancing data regularity through downsampling, normalization, and windowing. The preprocessed data provides high-quality input for subsequent end-to-end model learning. The sample counts of training and test sets for each preprocessed dataset are summarized in <xref rid="sensors-25-05765-t008" ref-type="table">Table 8</xref>.</p><p>This study aims to further address issues commonly encountered in sensor time-series data in Human Activity Recognition (HAR) tasks, such as insufficient sample diversity and limited model generalization ability. To this end, we only perform data augmentation on the preprocessed training set data of three datasets: WISDM, USC-HAD, and PAMAP2. Specifically, random Gaussian noise is added, with a mean of 0 and standard deviation of 0.01. When combined with the downsampling, normalization, and sliding window preprocessing mentioned earlier, this data augmentation strategy can further improve the quality of input data, help the model better accommodate data variations in real-world scenarios, and provide more reliable support for subsequent end-to-end model learning.</p></sec><sec id="sec4dot3-sensors-25-05765"><title>4.3. Experimental Environment</title><p>The experiments in this study were conducted on a computational platform equipped with an NVIDIA GeForce RTX 4050 Laptop GPU and an Intel Core i7-13700H CPU. This computational unit accelerates matrix operations through the CUDA 11.8 parallel computing architecture. At the software level, the model was implemented using the PyTorch 2.2.0 deep learning framework, which is specifically optimized for mixed-precision training and dynamic computation graphs. PyTorch 2.2.0 is fully compatible with Python 3.11. The experimental environment configuration is detailed in <xref rid="sensors-25-05765-t009" ref-type="table">Table 9</xref>.</p><p>The hyperparameter configuration for this experiment is detailed in <xref rid="sensors-25-05765-t010" ref-type="table">Table 10</xref>. Parameter updates are performed using the Adam optimizer, with 400 training epochs and a fixed batch size of 300. The learning rate is set to 0.001, and the cross-entropy loss function is selected to optimize probability distribution discrepancies in classification tasks. This configuration balances training efficiency and convergence stability, empirically ensuring optimal model performance within reasonable computational overhead.</p><p><xref rid="sensors-25-05765-t011" ref-type="table">Table 11</xref> details the holistic architectural configuration of the model. The backbone comprises a hierarchical feature processor integrating TCN, Res-BiGRU, and attention mechanism.</p><p>Input data first traverses a dilated convolutional stack with three residual TCN blocks. Each block employs 1D causal convolutions with 64 channels, kernel size = 5, and dilation factors sequentially set to 1, 2, and 4. The modified Swish activation function replaces conventional ReLU to enhance nonlinear representational capacity, supplemented with inter-layer batch normalization and regularization strategies.</p><p>TCN output features are fed into a skip-connected bidirectional GRU network. Feature data passes through the first BiGRU layer, whose outputs serve as inputs to the second BiGRU layer. The outputs of both layers undergo residual summation, forming a gradient-stable context-aware temporal feature extraction module.</p><p>In the attention layer, adaptively weighted attention normalizes and computes importance weights per timestep, enabling dynamic feature-weighted fusion. The classifier adopts a three-stage dimensionality reduction framework, with each fully connected layer followed by LeakyReLU activation (negative slope: 0.01) to balance information flow.</p></sec><sec id="sec4dot4-sensors-25-05765"><title>4.4. Evaluation Metrics</title><p>For classification tasks, the metrics used to evaluate model performance include accuracy, precision, recall, and F1 score.</p><p>Accuracy measures the proportion of correctly classified instances relative to the total population, with higher values indicating superior overall model performance. Precision quantifies the ratio of true positives to all predicted positives for a given class, reflecting its classification exactness. Recall represents the proportion of true positives correctly identified relative to all actual positives in that class, indicating coverage completeness. F1 score, defined as the harmonic mean of precision and recall, evaluates balanced classification performance per target class.</p><p>The mathematical definitions of these metrics are given in Equations (13)&#8211;(16) [<xref rid="B50-sensors-25-05765" ref-type="bibr">50</xref>].<disp-formula id="FD14-sensors-25-05765"><label>(13)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD15-sensors-25-05765"><label>(14)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-05765"><label>(15)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-05765"><label>(16)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
Here, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the total number of classes, <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of samples correctly predicted as class <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the total sample count, <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the true sample count of class <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes samples misclassified as class <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> but belonging to other classes, and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote samples truly belonging to class <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> but mispredicted as other classes. <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the precision and recall metrics for class <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p></sec><sec id="sec4dot5-sensors-25-05765"><title>4.5. Experimental Result</title><p>The training accuracy curves of the TGA-HAR model across three human activity datasets (WISDM, USC-HAD, and PAMAP2) are presented in <xref rid="sensors-25-05765-f008" ref-type="fig">Figure 8</xref>. During initial training, the model exhibits rapid accuracy improvement on all datasets. For the WISDM dataset, training accuracy reaches 99% at Epoch 9. The USC-HAD configuration achieves 99% accuracy at Epoch 109, while the PAMAP2 variant attains 99% at Epoch 56. After Epoch 150, both WISDM and USC-HAD configurations stabilize above 99% accuracy. The PAMAP2 implementation exhibits greater volatility.</p><sec id="sec4dot5dot1-sensors-25-05765"><title>4.5.1. Overall Performance Evaluation of TGA-HAR on Three Datasets</title><p><xref rid="sensors-25-05765-f009" ref-type="fig">Figure 9</xref> illustrates the model&#8217;s holistic performance across three datasets.</p><p>On the WISDM dataset, the model demonstrates exceptional generalization capability, achieving 99.37% accuracy, 99.38% precision, 99.37% recall, and 99.37% F1 score. This performance may be attributed to standardized activity types and simplified accelerometer-based data collection for basic motions.</p><p>For the USC-HAD dataset, experimental evaluation reveals robust recognition performance with 95.36% accuracy, 95.31% precision, 95.36% recall, and a corresponding 95.30% F1-score.</p><p>On the PAMAP2 dataset, the model excels in complex scenarios involving multi-sensor fusion, attaining 96.96% accuracy, 96.96% F1 score, 97.03% precision, and 96.96% recall. These metrics confirm that the TGA-HAR model effectively addresses the challenges of fusing heterogeneous sensors (chest-, ankle-, and wrist-mounted monitors) in intricate activity recognition tasks.</p></sec><sec id="sec4dot5dot2-sensors-25-05765"><title>4.5.2. Per-Activity Performance Evaluation of TGA-HAR on Three Datasets</title><p>The per-activity classification metrics for the WISDM dataset are presented in <xref rid="sensors-25-05765-f010" ref-type="fig">Figure 10</xref>. Experiments on six human activities demonstrate outstanding performance across all three core evaluation metrics.</p><p>For all six activity classes in WISDM, precision, recall, and F1 score exceed 95%, confirming the model&#8217;s stable generalization capability for basic daily activities using triaxial acceleration data. The confusion matrix heatmap is shown in <xref rid="sensors-25-05765-f011" ref-type="fig">Figure 11</xref>.</p><p>The heatmap reveals that misclassified cases are minimal across all categories, with an average of 6 misclassifications per activity class. Both the per-activity metrics and heatmap demonstrate the model&#8217;s strong discriminative capability in recognizing fundamental motions.</p><p>The per-activity classification metrics for the USC-HAD dataset are presented in <xref rid="sensors-25-05765-f012" ref-type="fig">Figure 12</xref>. The TGA-HAR model demonstrates strong classification performance on USC-HAD, indicating high accuracy and robustness in recognizing most activities. However, metrics for elevator-related operations (ascending/descending) are significantly lower, while standing recognition metrics are slightly below other activities. This suggests potential feature similarity challenges between static postures.</p><p>The confusion matrix heatmap for activity classification is shown in <xref rid="sensors-25-05765-f013" ref-type="fig">Figure 13</xref>. The heatmap reveals high diagonal accuracy but exhibits localized inter-class confusion. Minor misclassifications in walking-related activities may stem from temporal feature similarities caused by subtle gait variations. Notably, there is pronounced mutual misclassification between elevator ascending/descending and standing categories. Multiple elevator instances are misclassified as standing, and vice versa. This phenomenon occurs because elevator motions exhibit minimal variations in accelerometer and gyroscope signals during steady-state motion (except initial acceleration/final deceleration phases), where sensor readings resemble static standing. For other activities, low off-diagonal values confirm effective class discrimination. Additionally, feature extraction strategies require optimization for elevator and standing scenarios to enhance precision.</p><p>The per-activity classification metrics for the PAMAP2 dataset are shown in <xref rid="sensors-25-05765-f014" ref-type="fig">Figure 14</xref>, while the confusion matrix heatmap for activity classification is displayed in <xref rid="sensors-25-05765-f015" ref-type="fig">Figure 15</xref>. Comprehensive analysis of precision, recall, F1 score, and confusion matrix demonstrates that the TGA-HAR model achieves exceptional classification performance for most activity categories.</p><p>Specifically, 12 activity classes exhibit precision, recall, and F1 score consistently maintained above 92%. These values reflect high classification reliability. Confusion matrix analysis further validates the model&#8217;s robustness&#8212;diagonal elements (correctly classified samples) show significant dominance, with off-diagonal misclassification regions maintaining low values. This confirms the model&#8217;s strong discriminative power and generalization capability in multi-class recognition scenarios.</p></sec></sec><sec id="sec4dot6-sensors-25-05765"><title>4.6. Comparative Experiment</title><p>To compare the performance of different models on three datasets, this study designed comparative experiments. The baseline CNN model consists of 6 one-dimensional convolutional layers, each using convolutional kernels of size 5 and 64 channels. After each convolution operation, batch normalization, activation function application, and Dropout regularization are performed sequentially. The RES-CNN model was developed by modifying the aforementioned CNN architecture, where residual connections are introduced after every two convolutional layers to alleviate the vanishing gradient problem in deep networks. For the CNN-LSTM model, a hybrid architecture is adopted. The CNN-LSTM model cascades two layers of unidirectional LSTM networks after the CNN feature extraction module. Each LSTM layer is configured with 64 hidden units to further extract sequential features. The results of the comparative experiments are presented in <xref rid="sensors-25-05765-t012" ref-type="table">Table 12</xref>.</p><p>On the WISDM dataset, the CNN model achieves an accuracy of 96.80% and an F1 score of 96.97%. The Res-CNN model improves these metrics to 96.98% (accuracy) and 97.04% (F1 score). The CNN-LSTM model reaches 98.66% for accuracy and 98.67% for F1 score. The TGA-HAR model (This Study) performs the best, with both accuracy and F1 score at 99.37%. On the USC-HAD dataset, the CNN model yields an accuracy of 88.95% and an F1 score of 89.02%. The Res-CNN model enhances these to 90.73% (accuracy) and 90.66% (F1 score), and the CNN-LSTM model achieves an accuracy of 92.31% with an F1 score of 92.36%. The TGA-HAR model (This Study) leads with an accuracy of 95.36% and an F1 score of 95.30%. For the PAMAP2 dataset, the CNN model obtains an accuracy of 92.21% and an F1 score of 92.24%. The Res-CNN model improves these values to 92.78% (accuracy) and 92.81% (F1 score), respectively, and the CNN-LSTM model further increases them to 94.11% (accuracy) and 94.08% (F1 score). The TGA-HAR model (This Study) again demonstrates the best performance, with both accuracy and F1 score at 96.96%.</p><p>In this paper, TGA-HAR adopts a LOSO-like partitioning strategy (i.e., test users are excluded from the training set), while TCN-Attention-HAR uses random 8:2 partitioning of the entire sample set. For cross-dataset performance comparison, TGA-HAR outperforms TCN-Attention-HAR by a slight margin on the WISDM dataset; on the USC-HAD and PAMAP2 datasets, TGA-HAR is slightly inferior to TCN-Attention-HAR, but the performance gap is within 1.5%. This discrepancy stems from the distinct evaluation scenarios&#8212;and crucially, TGA-HAR&#8217;s architectural design further shapes its adaptability to these scenarios.</p><p>Specifically, compared with TCN-Attention-HAR, TGA-HAR incorporates Res-BiGRU, which endows it with advantages in global temporal modeling. The integration of Res-BiGRU enhances TGA-HAR&#8217;s capability to capture long-range and bidirectional temporal dependencies, a feature that helps extract common activity patterns across users (rather than relying on individual-specific traits).</p><p>In random partitioning, overlap between training and test users allows models to leverage known users&#8217; individual characteristics for recognition&#8212;an advantage that TCN-Attention-HAR may exploit more in this scenario. In contrast, LOSO-like partitioning enforces strict user isolation: TGA-HAR is required to extract shared activity features from 80% of users to adapt to unknown users, directly addressing feature shifts caused by individual differences in human activities. This setup evaluates &#8220;inter-user generalization ability,&#8221; which aligns more with real-world application scenarios but poses higher task difficulty. While TGA-HAR&#8217;s Res-BiGRU supports its generalization, the stricter LOSO-like constraint still leads to marginal performance differences on some datasets&#8212;a result that reflects the scenario&#8217;s inherent complexity rather than a flaw in TGA-HAR&#8217;s design. Overall, the TGA-HAR model proposed in this study outperforms the CNN, Res-CNN, and CNN-LSTM models in terms of both accuracy and F1 score across all three datasets. Additionally, the CNN-LSTM model performs better than the Res-CNN and CNN models, while the Res-CNN model only achieves a marginal improvement over the CNN model.</p></sec><sec id="sec4dot7-sensors-25-05765"><title>4.7. Ablation Experiment</title><sec id="sec4dot7dot1-sensors-25-05765"><title>4.7.1. Module Verification</title><p>Ablation studies constitute a core methodology for validating the effectiveness of deep learning models. This research employs a hierarchical decoupling strategy to investigate the functional contributions of individual components in temporal pattern recognition.</p><p>The TGA-HAR model adopts a multi-stage feature processing architecture:<list list-type="bullet"><list-item><p>Dilated convolutions extract multi-scale local features</p></list-item><list-item><p>Bidirectional recurrent structures capture temporal dependencies</p></list-item><list-item><p>Adaptive weighting mechanisms refine the feature space</p></list-item></list></p><p>To verify inter-module synergies, three controlled models are designed:<list list-type="bullet"><list-item><p>GRU-Only: This model retains solely the bidirectional GRU module.</p></list-item><list-item><p>TCN-Only: This model preserves only the Temporal Convolutional Network (TCN) module.</p></list-item><list-item><p>TCN-GRU-No-Attention: This model combines TCN and GRU without attention-based feature enhancement.</p></list-item></list></p><p>These controlled models quantify component contributions across feature extraction, temporal propagation, and feature refinement stages by progressively reducing modules. Experiments are conducted on WISDM, USC-HAD, and PAMAP2 datasets under identical hyperparameters and environmental conditions as the full TGA-HAR model, with results detailed in <xref rid="sensors-25-05765-t013" ref-type="table">Table 13</xref>.</p><p>In the relatively simple WISDM scenario, single-module models (TCN-Only or GRU-Only) show certain capabilities in capturing temporal patterns. The TCN-Only model achieves an accuracy of 96.70% and an F1 score of 96.77%. The GRU-Only model attains an accuracy of 96.43% and an F1 score of 96.54%. The TCN-GRU-No-Attention combination, leveraging the complementary nature of local feature extraction by TCN and global temporal feature extraction by GRU, reaches an accuracy of 99.02% and an F1 score of 99.02%. Given the characteristics of the WISDM dataset, the attention mechanism in the subsequent TGA-HAR model further optimizes performance. The TGA-HAR model (This Study) achieves an accuracy of 99.37% and an F1 score of 99.37%, showing a step-by-step improvement compared to the non-attention variant.</p><p>In more complex USC-HAD scenarios with intricate activities, standalone models have limitations. The TCN-Only model, when extracting local features, achieves an accuracy of 91.22% and an F1 score of 91.26%. The GRU-Only model, focusing on global temporal features, attains an accuracy of 91.47% and an F1 score of 91.50%. The TCN-GRU-No-Attention combination partially alleviates these problems through cross-scale feature fusion, improving the accuracy by 1.67% (compared to GRU-Only) and the F1 score by 1.62%. Importantly, the attention mechanism in the TGA-HAR model significantly enhances fine&#8212;grained activity recognition through weighted feature allocation. It increases the accuracy by 2.22% (compared to TCN-GRU-No-Attention) and the F1 score by 2.18%, finally achieving an accuracy of 95.36% and an F1 score of 95.30%.</p><p>For the multimodal complex scenario of PAMAP2, the causal convolutions of the TCN-Only model effectively extract short-term temporal correlations from multi-sensor data, achieving an accuracy of 93.45% and an F1 score of 93.41%. The GRU-Only model captures long-term temporal dependencies from multi-sensor data, attaining an accuracy of 92.88% and an F1 score of 92.89%. The TCN-GRU-No-Attention model, through the joint extraction of features by TCN and GRU, can capture both short-term correlations and long-term dependencies, improving the accuracy by 1.23% (compared to TCN-Only) and the F1 score by 1.28%. The attention layer in the TGA-HAR model dynamically weights the contributions across timesteps, enhancing the classification ability for complex activities. It achieves an accuracy of 96.96% and an F1 score of 96.96%, representing a 2.28% improvement in accuracy compared to the TCN-GRU-No-Attention model.</p><p>Performance disparities reveal intrinsic links between component functions and data characteristics. TCN-GRU synergy addresses local-global feature integration challenges, while the attention mechanism enables adaptive optimization for complex scenarios. These elements collectively construct a hierarchical feature processing pipeline for robust activity classification.</p></sec><sec id="sec4dot7dot2-sensors-25-05765"><title>4.7.2. The Impact of the Number of GRU Layers on Performance</title><p>To investigate the impact of the number of GRU layers on the performance of the TGA-HAR model, this experiment uses the PAMAP2 dataset. The model keeps the convolutional structure of the TCN module, the computational logic of the attention mechanism, and the training hyperparameters exactly consistent. During the experiment, the number of GRU layers in the Res-BiGRU module is sequentially set to 2, 4, and 6. Through this setup, the effect of the layer count on the model is verified. After training is completed, the accuracy and F1 score are evaluated on the test set. The experimental results are shown in <xref rid="sensors-25-05765-t014" ref-type="table">Table 14</xref>.</p><p>When the number of GRU layers is 2, both the model&#8217;s accuracy and F1 score reach 96.96%. When the number of layers increases to 4, both metrics rise synchronously to 97.34%, which is an increase of 0.38 percentage points compared to the 2-layer structure. When the number of layers is further increased to 6, the accuracy reaches 97.53%, and this represents an increase of 0.19 percentage points compared to the 4-layer structure. The F1 score in this case is 97.52%, which corresponds to an increase of 0.18 percentage points compared to the 4-layer structure.</p><p>From the overall trend, increasing the number of GRU layers gradually enhances the model&#8217;s ability to model temporal features. Deeper networks can capture more complex dependency relationships, so the model performance shows a continuous upward trend. However, the magnitude of performance improvement exhibits the characteristic of &#8220;diminishing marginal returns&#8221;: the performance gain is more significant when increasing from 2 layers to 4 layers, while the magnitude of improvement has narrowed when increasing from 4 layers to 6 layers. Although deeper networks can explore more complex patterns, they are accompanied by a series of issues, such as a sharp increase in the number of parameters, extended computational latency, and greater difficulty in training convergence.</p><p>The experimental hardware platform in this study is an NVIDIA GeForce RTX 4050 Laptop GPU with 6 GB of VRAM (Video Random Access Memory). Experimental verification shows that although this platform can support the training and operation of 4-layer and 6-layer GRU (Gated Recurrent Unit) models, it leads to a significant degradation in the model&#8217;s recognition speed. Notably, human activity recognition tasks typically require real-time response capabilities (e.g., instant judgment in scenarios involving wearable devices or surveillance systems), and excessively slow recognition speed would undermine the practical application value of the model. After weighing the trade-off between performance and real-time requirements, this study provisionally designates the 2-layer GRU structure as the implementation scheme for the human activity recognition task. If subsequent research can rely on more high-performance hardware platforms (such as high-VRAM GPUs like the RTX 3090/4090, or the construction of multi-GPU parallel architectures), it will be feasible to verify the actual performance of GRU structures with more layers, thereby further tapping the model&#8217;s potential in temporal feature modeling.</p></sec></sec><sec id="sec4dot8-sensors-25-05765"><title>4.8. Evaluation of the TGA-HAR Model in Real-World Environments</title><p>Datasets collected in the aforementioned laboratory-controlled environments fail to reproduce issues such as sensor noise, natural interruptions of activities, and variations in device wearing that occur in real-world scenarios. This results in a potential disconnect between the evaluation of model performance and its practical applications.</p><p>To address this issue, this subsection adopts the Real-World IDLab Dataset constructed by Stojchevska [<xref rid="B51-sensors-25-05765" ref-type="bibr">51</xref>] as the benchmark for evaluation in real-world environments. Characterized by uncontrolled longitudinal data collection, this dataset fully reflects the natural state of human daily activities and thus provides reliable support for the evaluation of model generalization ability.</p><p>The Real-World IDLab Dataset employs the Empatica E4 wristband as its data collection device, with a primary focus on utilizing data from the device&#8217;s triaxial accelerometer (sampling rate: 32 Hz). Formal data collection involved 18 participants (aged 22&#8211;45 years, including 5 females and 13 males), and no researchers were present for observation or monitoring during the collection process. Participants were fully allowed to independently determine activity scenarios (e.g., home, office, outdoor environments) and implementation methods in accordance with their daily routines. This setup naturally captures realistic characteristics such as activity interruptions (e.g., waiting for a red light while walking) and speed variations (e.g., slowing down to avoid pedestrians while cycling)&#8212;features that stand in sharp contrast to standardized activity scenarios.</p><p>The Real-World IDLab Dataset focuses on 5 categories of core daily activities (Computer Table, Cycling, Running, Standing Still, and Walking). To enhance label quality, it adopts two strategies: model-assisted annotation (pushing activity predictions every 5 min for participants to confirm or correct) and location backtracking (allowing verification of the start and end positions of activity periods). Additionally, targeted cleaning is performed on overlapping labels&#8212;for instance, when two mutually exclusive activities are labeled for the same time period, the time boundaries are adjusted or redundant labels are removed. In the data preprocessing stage, the accelerometer signals are scaled to the range of &#8722;2 g to +2 g.</p><p>Among the dataset we downloaded [<xref rid="B52-sensors-25-05765" ref-type="bibr">52</xref>], there is data from 15 participants. We selected the same time window as that in [<xref rid="B51-sensors-25-05765" ref-type="bibr">51</xref>], and segmented the data using a 12 s sliding window with a 50% overlap rate, resulting in 3 &#215; 384 matrix samples (3 axial directions &#215; 384 time points). After completing window segmentation, the number of samples for each of the 5 activities per participant is shown in <xref rid="sensors-25-05765-t015" ref-type="table">Table 15</xref>.</p><p>We selected data from a total of 3 participants, which included participant_7, participant_9, and participant_16, as the test set, while the data from the remaining participants was used as the training set. This ensures that the 5 activities have a sufficient number of samples in both the training and test sets. After the division was completed, the samples of the training and test sets are shown in <xref rid="sensors-25-05765-t016" ref-type="table">Table 16</xref>.</p><p>We thus selected the same evaluation metrics as those in [<xref rid="B51-sensors-25-05765" ref-type="bibr">51</xref>], namely F1-micro, F1-macro, and Balanced Accuracy. F1-micro, a metric for measuring overall classification accuracy, calculates precision and recall globally by treating all classes as a single whole; F1-macro, which measures the average of classification accuracies across classes, works by first computing precision and recall independently for each class and then averaging the F1 scores of all classes, whereas Balanced Accuracy, a metric that gauges the balance of a classification model&#8217;s recall across classes, is defined as the average of the recall rates of all classes. Their calculation formulas are presented in Equations (17)&#8211;(19).<disp-formula id="FD18-sensors-25-05765"><label>(17)</label><mml:math id="mm80" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD19-sensors-25-05765"><label>(18)</label><mml:math id="mm81" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD20-sensors-25-05765"><label>(19)</label><mml:math id="mm82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
Here, <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the total number of classes. <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote precision and recall at the global level, respectively. <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote precision and recall for the <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th class, respectively.</p><p>Using the experimental environment and hyperparameter settings consistent with those mentioned above, we completed the model training and then conducted testing using the data from the 4 participants. The results are presented in <xref rid="sensors-25-05765-t017" ref-type="table">Table 17</xref>.</p><p>The heatmap of the confusion matrix for the TGA-HAR model on the test set of the Real-World IDLab Dataset is shown in <xref rid="sensors-25-05765-f016" ref-type="fig">Figure 16</xref>. As shown in the confusion matrix heatmap, the TGA-HAR model exhibits distinct classification performance across different activity categories. For Computer Table, Cycling, Running, and Walking, the model correctly identifies most samples (66,202 for Computer Table, 1080 for Cycling, 4095 for Running, and 7140 for Walking). However, for Standing Still, only 19 samples are correctly classified, indicating poor recognition performance.</p><p>This performance discrepancy can be attributed to the interplay between the TGA-HAR architecture (TCN-ResBiGRU-Attention) and the inherent characteristics of each activity. Categories like Computer Table, Cycling, Running, and Walking possess distinct and rich temporal features (e.g., stable static patterns for Computer Table, periodic motion rhythms for Cycling and Running). The TCN with dilated convolutions effectively extracts multi-scale temporal features, the Res-BiGRU captures bidirectional temporal dependencies, and the attention mechanism enhances the weights of these discriminative features&#8212;thus enabling accurate classification. In contrast, Standing Still has extremely weak temporal dynamics, making its feature distinction from other static categories (e.g., Computer Table) subtle. TCN struggles to extract meaningful patterns from near-static data, Res-BiGRU&#8217;s temporal modeling capability is limited in the absence of sequential variation, and the attention mechanism fails to identify prominent feature weights due to insufficient discriminative signals. Additionally, Standing Still has a smaller sample size compared to other categories, leading to inadequate generalization of the model for this class.</p><p>While the TGA-HAR model exhibits limited performance in recognizing the &#8220;Standing Still&#8221; category, it still demonstrates significant advantages based on its TCN-ResBiGRU-Attention architecture&#8212;specifically, the dilated convolutions in TCN enable effective extraction of multi-scale temporal features, ResBiGRU robustly captures bidirectional temporal dependencies, and the attention mechanism adaptively enhances the weights of discriminative features. This design allows the model to achieve high accuracy in recognizing dynamic activities (e.g., Cycling, Running, Walking) with rich temporal patterns, as well as static activities like &#8220;Computer Table&#8221; with distinct motionless characteristics, fully validating its robustness and adaptability in complex real-world HAR tasks.</p><p>However, the model&#8217;s weakness in recognizing &#8220;Standing Still&#8221; requires targeted improvements. Data augmentation can be applied to &#8220;Standing Still&#8221; category data (e.g., adding noise-infused samples or resampling) to expand the training dataset and improve generalization capability; alternatively, the model architecture can be adjusted&#8212;such as adding a lightweight static feature modeling branch or optimizing the attention mechanism to prioritize static feature patterns. These measures can improve the recognition performance of &#8220;Standing Still&#8221; while preserving the model&#8217;s advantages in recognizing other categories.</p></sec><sec id="sec4dot9-sensors-25-05765"><title>4.9. Activity Recognition System</title><p>This study aims to achieve complex activity recognition using three wireless IMUs worn by test subjects. The activities correspond to those defined in the PAMAP2 dataset (detailed in <xref rid="sensors-25-05765-t007" ref-type="table">Table 7</xref>). Sensors are positioned at the dominant wrist, ankle, and chest, with a sampling frequency of 100 Hz.</p><p>The recognition system processes 12-dimensional raw data comprising triaxial acceleration and angular velocity from three IMUs. We implement a recognition system to demonstrate experimental outcomes. The system employs the TGA-HAR model, where raw data undergoes preprocessing (<xref rid="sec4dot2-sensors-25-05765" ref-type="sec">Section 4.2</xref>) before being fed into the model.</p><p>Upon completing activity recognition, the system outputs result as shown in <xref rid="sensors-25-05765-f017" ref-type="fig">Figure 17</xref>. <xref rid="sensors-25-05765-f017" ref-type="fig">Figure 17</xref>a shows standby state awaiting input. <xref rid="sensors-25-05765-f017" ref-type="fig">Figure 17</xref>b show recognized activities during operation. Recognition results for all 18 activities are comprehensively presented in <xref rid="app1-sensors-25-05765" ref-type="app">Figure A1</xref>.</p><p>To scientifically quantify the computational efficiency of the TGA-HAR model, this study utilizes the test set of the PAMAP2 dataset as experimental samples. During experiments, a millisecond-level timer precisely captures the recognition time per sample to evaluate the model&#8217;s real-time inference performance. Concurrently, CPU resource utilization is continuously monitored at 1 s intervals throughout the recognition process.</p><p><xref rid="sensors-25-05765-f018" ref-type="fig">Figure 18</xref> illustrates the recognition time of each human activity of the TGA-HAR model on the PAMAP2 test set. The <italic toggle="yes">x</italic>-axis represents 1053 human activity test samples, while the <italic toggle="yes">y</italic>-axis denotes the inference latency per sample in milliseconds. A dashed horizontal line marks the average inference latency of 34.8 ms, reflecting overall performance. The solid line traces real-time dynamic fluctuations in per-sample latency, demonstrating the model&#8217;s behavior during continuous activity recognition. Data distribution reveals that most samples exhibit stable latency fluctuations around the mean, indicating consistent computational efficiency. However, six samples (Nos. 177, 251, 404, 546, 631, 860) show higher latencies (50.51 ms, 45.02 ms, 50.52 ms, 45.00 ms, 53.99 ms, 48.99 ms), attributable to transient resource contention during inference.</p><p><xref rid="sensors-25-05765-f019" ref-type="fig">Figure 19</xref> displays the CPU utilization percentage per second during TGA-HAR model inference across 1053 human activity samples, recorded from 0 to 36.0 s. The <italic toggle="yes">x</italic>-axis represents elapsed time in seconds, while the <italic toggle="yes">y</italic>-axis indicates the CPU usage percentage. A dashed horizontal line marks the average CPU utilization of 13.4% throughout the test. The solid line traces actual second-by-second CPU usage during single-sample inference on the PAMAP2 test set. During the initial 0&#8211;2 s, CPU usage peaks at 27.6% due to model initialization and first-frame processing. Subsequent utilization exhibits sawtooth-like fluctuations. As documented in [<xref rid="B53-sensors-25-05765" ref-type="bibr">53</xref>], PyTorch employs a decoupled architecture separating control flow (Python) from data flow (C++ operators). Monitoring real-time CPU usage captures switching overhead between these layers, resulting in observed fluctuations&#8212;a phenomenon consistent with PyTorch training observations in [<xref rid="B54-sensors-25-05765" ref-type="bibr">54</xref>].</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05765"><title>5. Conclusions</title><p>This study proposes TGA-HAR&#8212;a three-tier architecture integrating TCN, Res-BiGRU, and an attention mechanism&#8212;which achieves significant performance improvements in complex human activity recognition tasks. Experimental results on the WISDM, USC-HAD, and PAMAP2 datasets demonstrate that the model attains accuracies of 99.37%, 95.36%, and 96.96%, with corresponding F1 scores of 99.37%, 95.30%, and 96.96%, respectively. The model employs dilated causal convolutions in the TCN layer for multi-scale local feature extraction, utilizes bidirectional GRU structures to capture long-range temporal dependencies, and enhances classification capability through attention-guided dynamic feature weighting. Tests on real-world datasets show that the TGA-HAR model exhibits good overall recognition capability in HAR tasks. Specifically, it achieves excellent recognition performance for four activity categories, including the dynamic activities Cycling, Running, Walking and the static activity Computer Table, but has limited capability in recognizing Standing Still. Subsequent optimizations can focus on data augmentation for Standing Still, such as adding noise-infused samples or resampling, and adjustments to the model architecture like adding lightweight static feature branches. These efforts aim to improve Standing Still recognition while ensuring the model maintains its strong performance on the four aforementioned categories. During single-sample inference, the TGA-HAR model requires an average latency of 34.80 ms with mean CPU utilization of 13.4%. Ablation studies further confirm synergistic interactions among these modules.</p><p>While the TGA-HAR model has shown favorable performance in tests on real-world datasets, further enhancements are still needed to better address practical deployment requirements in the field of HAR. Architecturally, integrating Transformer-based self-attention mechanisms could enhance cross-modal temporal relationship extraction. Furthermore, developing lightweight variants via neural architecture search is crucial for meeting wearable device deployment constraints. While linear interpolation effectively handles short-term sensor signal loss, its adaptability to non-uniform distributions or prolonged missing sequences remains limited. Future work should explore machine learning-based imputation methods to establish more reliable data foundations. This study focuses on the structural design and basic performance verification of the TGA-HAR model, providing a new solution with low-latency characteristics for the field of HAR. However, there is still room for expansion in the systematic demonstration of the &#8220;performance&#8211;efficiency&#8221; trade-off. Future research will supplement horizontal tests on the computational efficiency of mainstream HAR models. By constructing a two-dimensional comparison matrix of &#8220;accuracy&#8211;inference speed&#8221; and &#8220;accuracy&#8211;resource consumption&#8221;, it will examine the TGA-HAR model&#8217;s performance in terms of &#8220;performance&#8211;efficiency&#8221; and provide more sufficient data support for the model&#8217;s application in practical deployment scenarios such as low-latency embedded devices and lightweight health monitoring terminals. The multi-stage fusion framework proposed in this study offers a novel methodological reference for complex temporal pattern recognition.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.-L.H. and J.-H.W.; methodology, J.-L.H. and J.-H.W.; formal analysis, J.-L.H., J.-H.W., C.-M.L. and Z.J.; investigation, J.-L.H.; data curation, J.-L.H.; writing&#8212;original draft preparation, J.-L.H.; writing&#8212;review and editing, J.-L.H., J.-H.W., C.-M.L. and Z.J.; visualization, J.-L.H.; supervision, J.-H.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets analyzed during the current study are openly available in the following repositories: the WISDM Activity Recognition Dataset in the WISDM Lab repository at [<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.cis.fordham.edu/wisdm/dataset.php">http://www.cis.fordham.edu/wisdm/dataset.php</uri>, accessed on 24 March 2025]; the USC-HAD Human Activity Dataset in the USC-SIPI repository at [<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://sipi.usc.edu/HAD/">https://sipi.usc.edu/HAD/</uri>, accessed on 24 March 2025]; the PAMAP2 Physical Activity Monitoring Dataset in the UCI Machine Learning Repository at [<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring">https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring</uri>, accessed on 24 March 2025]; and the Real-World IDLab Dataset in the IDLab Cloud repository at [<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://cloud.ilabt.imec.be/index.php/s/2zkXxEDTTQgkN5y">https://cloud.ilabt.imec.be/index.php/s/2zkXxEDTTQgkN5y</uri>, accessed on 22 August 2025].</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><app-group><app id="app1-sensors-25-05765"><title>Appendix A</title><fig position="anchor" id="sensors-25-05765-f0A1" orientation="portrait"><label>Figure A1</label><caption><p>Demonstration of the HAR System Using the TGA-HAR Model (12 Activities). (<bold>a</bold>) Recognized Activity: Walking. (<bold>b</bold>) Recognized Activity: Standing. (<bold>c</bold>) Recognized Activity: Sitting. (<bold>d</bold>) Recognized Activity: Running. (<bold>e</bold>) Recognized Activity: Ascending Stairs. (<bold>f</bold>) Recognized Activity: Descending Stairs. (<bold>g</bold>) Recognized Activity: Cycling. (<bold>h</bold>) Recognized Activity: Nordic Walking. (<bold>i</bold>) Recognized Activity: Lying. (<bold>j</bold>) Recognized Activity: Rope Jumping. (<bold>k</bold>) Recognized Activity: Vacuum Cleaning. (<bold>l</bold>) Recognized Activity: Ironing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g0A1a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g0A1b.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g0A1c.jpg"/></fig></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-05765"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dentamaro</surname><given-names>V.</given-names></name><name name-style="western"><surname>Gattulli</surname><given-names>V.</given-names></name><name name-style="western"><surname>Impedovo</surname><given-names>D.</given-names></name><name name-style="western"><surname>Manca</surname><given-names>F.</given-names></name></person-group><article-title>Human activity recognition with smartphone-integrated sensors: A survey</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>246</volume><fpage>123143</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.123143</pub-id></element-citation></ref><ref id="B2-sensors-25-05765"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tahir</surname><given-names>S.B.U.D.</given-names></name><name name-style="western"><surname>Jalal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>K.</given-names></name></person-group><article-title>Wearable Inertial Sensors for Daily Activity Analysis Based on Adam Optimization and the Maximum Entropy Markov Model</article-title><source>Entropy</source><year>2020</year><volume>22</volume><elocation-id>579</elocation-id><pub-id pub-id-type="doi">10.3390/e22050579</pub-id><pub-id pub-id-type="pmid">33286351</pub-id><pub-id pub-id-type="pmcid">PMC7517099</pub-id></element-citation></ref><ref id="B3-sensors-25-05765"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>P</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pl&#246;tz</surname><given-names>T.</given-names></name></person-group><article-title>Using Graphs to Perform Effective Sensor-Based Human Activity Recognition in Smart Homes</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3944</elocation-id><pub-id pub-id-type="doi">10.3390/s24123944</pub-id><pub-id pub-id-type="pmid">38931728</pub-id><pub-id pub-id-type="pmcid">PMC11207551</pub-id></element-citation></ref><ref id="B4-sensors-25-05765"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoelzemann</surname><given-names>A.</given-names></name><name name-style="western"><surname>Romero</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Bock</surname><given-names>M.</given-names></name><name name-style="western"><surname>Laerhoven</surname><given-names>K.V.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>Q.</given-names></name></person-group><article-title>Hang-Time HAR: A Benchmark Dataset for Basketball Activity Recognition Using Wrist-Worn Inertial Sensors</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>5879</elocation-id><pub-id pub-id-type="doi">10.3390/s23135879</pub-id><pub-id pub-id-type="pmid">37447730</pub-id><pub-id pub-id-type="pmcid">PMC10346876</pub-id></element-citation></ref><ref id="B5-sensors-25-05765"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alemayoh</surname><given-names>T.T.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.H.</given-names></name><name name-style="western"><surname>Okamoto</surname><given-names>S.</given-names></name></person-group><article-title>New Sensor Data Structuring for Deeper Feature Extraction in Human Activity Recognition</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>2814</elocation-id><pub-id pub-id-type="doi">10.3390/s21082814</pub-id><pub-id pub-id-type="pmid">33923706</pub-id><pub-id pub-id-type="pmcid">PMC8073736</pub-id></element-citation></ref><ref id="B6-sensors-25-05765"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>F.</given-names></name></person-group><article-title>A Multivariate Temporal Convolutional Attention Network for Time-Series Forecasting</article-title><source>Electronics</source><year>2022</year><volume>11</volume><elocation-id>1516</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11101516</pub-id></element-citation></ref><ref id="B7-sensors-25-05765"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>J.</given-names></name></person-group><article-title>Human Activity Recognition Based on Two-Channel Residual&#8211;GRU&#8211;ECA Module with Two Types of Sensors</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>1622</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12071622</pub-id></element-citation></ref><ref id="B8-sensors-25-05765"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>N.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>Y.</given-names></name></person-group><article-title>Deep learning-driven hybrid model for short-term load forecasting and smart grid information management</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>13720</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-63262-x</pub-id><pub-id pub-id-type="pmid">38877081</pub-id><pub-id pub-id-type="pmcid">PMC11178870</pub-id></element-citation></ref><ref id="B9-sensors-25-05765"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name></person-group><article-title>Improving long-term multivariate time series forecasting with a seasonal-trend decomposition-based 2-dimensional temporal convolution dense network</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>1689</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-52240-y</pub-id><pub-id pub-id-type="pmid">38242949</pub-id><pub-id pub-id-type="pmcid">PMC10799069</pub-id></element-citation></ref><ref id="B10-sensors-25-05765"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zuo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>An Ensemble Framework for Short-Term Load Forecasting Based on TimesNet and TCN</article-title><source>Energies</source><year>2023</year><volume>16</volume><elocation-id>5330</elocation-id><pub-id pub-id-type="doi">10.3390/en16145330</pub-id></element-citation></ref><ref id="B11-sensors-25-05765"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Falaschetti</surname><given-names>L.</given-names></name><name name-style="western"><surname>Biagetti</surname><given-names>G.</given-names></name><name name-style="western"><surname>Crippa</surname><given-names>P.</given-names></name><name name-style="western"><surname>Alessandrini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Giacomo</surname><given-names>D.F.</given-names></name><name name-style="western"><surname>Turchetti</surname><given-names>C.</given-names></name></person-group><article-title>A lightweight and accurate RNN in wearable embedded systems for human activity recognition</article-title><source>Intelligent Decision Technologies: Proceedings of the 14th KES-IDT 2022 Conference</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2022</year></element-citation></ref><ref id="B12-sensors-25-05765"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bhola</surname><given-names>G.</given-names></name><name name-style="western"><surname>Vishwakarma</surname><given-names>D.K.</given-names></name></person-group><article-title>A review of vision-based indoor HAR: State-of-the-art, challenges, and future prospects</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>1965</fpage><lpage>2005</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-15443-5</pub-id><pub-id pub-id-type="pmcid">PMC10173923</pub-id><pub-id pub-id-type="pmid">37362688</pub-id></element-citation></ref><ref id="B13-sensors-25-05765"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name></person-group><article-title>Human action recognition based on multi-scale feature maps from depth video sequences</article-title><source>Multimed. Tools Appl.</source><year>2021</year><volume>80</volume><fpage>32111</fpage><lpage>32130</lpage><pub-id pub-id-type="doi">10.1007/s11042-021-11193-4</pub-id></element-citation></ref><ref id="B14-sensors-25-05765"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Newaz</surname><given-names>N.T.</given-names></name><name name-style="western"><surname>Hanada</surname><given-names>E.</given-names></name></person-group><article-title>A low-resolution infrared array for unobtrusive human activity recognition that preserves privacy</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>926</elocation-id><pub-id pub-id-type="doi">10.3390/s24030926</pub-id><pub-id pub-id-type="pmid">38339643</pub-id><pub-id pub-id-type="pmcid">PMC10857048</pub-id></element-citation></ref><ref id="B15-sensors-25-05765"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hua</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hemantha Kumar</surname><given-names>G.</given-names></name><name name-style="western"><surname>Manjunath Aradhya</surname><given-names>V.N.</given-names></name></person-group><article-title>A hybrid speed and radial distance feature descriptor using optical flow approach in HAR</article-title><source>Proceedings of the International Conference on Applied Intelligence and Informatics</source><conf-loc>Reggio Calabria, Italy</conf-loc><conf-date>1&#8211;3 September 2022</conf-date></element-citation></ref><ref id="B16-sensors-25-05765"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>A.</given-names></name></person-group><article-title>Occluded skeleton-based multi-stream model using Part-Aware Spatial&#8211;Temporal Graph Convolutional Network for human activity recognition</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>156</volume><fpage>111183</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2025.111183</pub-id></element-citation></ref><ref id="B17-sensors-25-05765"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name></person-group><article-title>Human pose-based estimation, tracking and action recognition with deep learning: A survey</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2310.13039</pub-id><pub-id pub-id-type="arxiv">2310.13039</pub-id></element-citation></ref><ref id="B18-sensors-25-05765"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pesenti</surname><given-names>M.</given-names></name><name name-style="western"><surname>Invernizzi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Mazzella</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bocciolone</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pedrocchi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gandolla</surname><given-names>M.</given-names></name></person-group><article-title>IMU-based human activity recognition and payload classification for low-back exoskeletons</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>1184</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-28195-x</pub-id><pub-id pub-id-type="pmid">36681711</pub-id><pub-id pub-id-type="pmcid">PMC9867770</pub-id></element-citation></ref><ref id="B19-sensors-25-05765"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name></person-group><article-title>Physical activity recognition from accelerometer data using multi-view aggregation</article-title><source>J. Appl. Sci. Eng.</source><year>2021</year><volume>24</volume><fpage>611</fpage><lpage>620</lpage></element-citation></ref><ref id="B20-sensors-25-05765"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lanfranchi</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ciravegna</surname><given-names>F.</given-names></name></person-group><article-title>Activity graph based convolutional neural network for human activity recognition using acceleration and gyroscope data</article-title><source>IEEE Trans. Ind. Inform.</source><year>2022</year><volume>18</volume><fpage>6619</fpage><lpage>6630</lpage><pub-id pub-id-type="doi">10.1109/TII.2022.3142315</pub-id></element-citation></ref><ref id="B21-sensors-25-05765"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Palimkar</surname><given-names>P.</given-names></name><name name-style="western"><surname>Bajaj</surname><given-names>V.</given-names></name><name name-style="western"><surname>Mal</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Shaw</surname><given-names>R.N.</given-names></name><name name-style="western"><surname>Ghosh</surname><given-names>A.</given-names></name></person-group><article-title>Unique action identifier by using magnetometer, accelerometer and gyroscope: KNN approach</article-title><source>Advanced Computing and Intelligent Technologies: Proceedings of ICACIT 2021</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year></element-citation></ref><ref id="B22-sensors-25-05765"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Najeh</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lohr</surname><given-names>C.</given-names></name><name name-style="western"><surname>Leduc</surname><given-names>B.</given-names></name></person-group><article-title>Towards supervised real-time human activity recognition on embedded equipment</article-title><source>Proceedings of the 2022 IEEE International Workshop on Metrology for Living Environment (MetroLivEn)</source><conf-loc>Cosenza, Italy</conf-loc><conf-date>25&#8211;27 May 2022</conf-date></element-citation></ref><ref id="B23-sensors-25-05765"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ryu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yun</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>I.c.</given-names></name></person-group><article-title>Exploring the Possibility of Photoplethysmography-Based Human Activity Recognition Using Convolutional Neural Networks</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>1610</elocation-id><pub-id pub-id-type="doi">10.3390/s24051610</pub-id><pub-id pub-id-type="pmid">38475146</pub-id><pub-id pub-id-type="pmcid">PMC10933912</pub-id></element-citation></ref><ref id="B24-sensors-25-05765"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shahabi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>S.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Alshurafa</surname><given-names>N.</given-names></name></person-group><article-title>Deep Learning in Human Activity Recognition with Wearable Sensors: A Review on Advances</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>1476</elocation-id><pub-id pub-id-type="doi">10.3390/s22041476</pub-id><pub-id pub-id-type="pmid">35214377</pub-id><pub-id pub-id-type="pmcid">PMC8879042</pub-id></element-citation></ref><ref id="B25-sensors-25-05765"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>K.</given-names></name></person-group><article-title>Vision transformers for human activity recognition using WiFi channel state information</article-title><source>IEEE Internet Things J.</source><year>2024</year><volume>11</volume><fpage>28111</fpage><lpage>28122</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2024.3375337</pub-id></element-citation></ref><ref id="B26-sensors-25-05765"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abuhoureyah</surname><given-names>F.S.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>Y.C.</given-names></name><name name-style="western"><surname>Isira</surname><given-names>A.S.B.M.</given-names></name></person-group><article-title>WiFi-based human activity recognition through wall using deep learning</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>127</volume><fpage>107171</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.107171</pub-id></element-citation></ref><ref id="B27-sensors-25-05765"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>W.</given-names></name></person-group><article-title>A deep learning based lightweight human activity recognition system using reconstructed WiFi CSI</article-title><source>IEEE Trans. Hum. Mach. Syst.</source><year>2024</year><volume>54</volume><fpage>68</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1109/THMS.2023.3348694</pub-id></element-citation></ref><ref id="B28-sensors-25-05765"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Challa</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Semwal</surname><given-names>V.B.</given-names></name></person-group><article-title>A multibranch CNN-BiLSTM model for human activity recognition using wearable sensor data</article-title><source>Vis. Comput.</source><year>2022</year><volume>38</volume><fpage>4095</fpage><lpage>4109</lpage><pub-id pub-id-type="doi">10.1007/s00371-021-02283-3</pub-id></element-citation></ref><ref id="B29-sensors-25-05765"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>Z.N.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>J.</given-names></name></person-group><article-title>Attention induced multi-head convolutional neural network for human activity recognition</article-title><source>Appl. Soft Comput.</source><year>2021</year><volume>110</volume><fpage>107671</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2021.107671</pub-id></element-citation></ref><ref id="B30-sensors-25-05765"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khatun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yousuf</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Uddin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Alyami</surname><given-names>S.</given-names></name><name name-style="western"><surname>Al-Ashhab</surname><given-names>S.</given-names></name><name name-style="western"><surname>Akhdar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Azad</surname><given-names>A.</given-names></name><name name-style="western"><surname>Moni</surname><given-names>M.</given-names></name></person-group><article-title>Deep CNN-LSTM with self-attention model for human activity recognition using wearable sensor</article-title><source>IEEE J. Transl. Eng. Health Med.</source><year>2022</year><volume>10</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/JTEHM.2022.3177710</pub-id><pub-id pub-id-type="pmcid">PMC9252338</pub-id><pub-id pub-id-type="pmid">35795873</pub-id></element-citation></ref><ref id="B31-sensors-25-05765"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sarkar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hossain</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Sarkar</surname><given-names>R.</given-names></name></person-group><article-title>Human activity recognition from sensor data using spatial attention-aided CNN with genetic algorithm</article-title><source>Neural Comput. Appl.</source><year>2023</year><volume>35</volume><fpage>5165</fpage><lpage>5191</lpage><pub-id pub-id-type="doi">10.1007/s00521-022-07911-0</pub-id><pub-id pub-id-type="pmid">36311167</pub-id><pub-id pub-id-type="pmcid">PMC9596348</pub-id></element-citation></ref><ref id="B32-sensors-25-05765"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lalwani</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ramasamy</surname><given-names>G.</given-names></name></person-group><article-title>Human activity recognition using a multi-branched CNN-BiLSTM-BiGRU model</article-title><source>Appl. Soft Comput.</source><year>2024</year><volume>154</volume><fpage>111344</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2024.111344</pub-id></element-citation></ref><ref id="B33-sensors-25-05765"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>TCN-attention-HAR: Human activity recognition based on attention mechanism time convolutional network</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>7414</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-57912-3</pub-id><pub-id pub-id-type="pmid">38548859</pub-id><pub-id pub-id-type="pmcid">PMC10978978</pub-id></element-citation></ref><ref id="B34-sensors-25-05765"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Essa</surname><given-names>E.</given-names></name><name name-style="western"><surname>Abdelmaksoud</surname><given-names>I.R.</given-names></name></person-group><article-title>Temporal-channel convolution with self-attention network for human activity recognition using wearable sensors</article-title><source>Knowl. Based Syst.</source><year>2023</year><volume>278</volume><fpage>110867</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2023.110867</pub-id></element-citation></ref><ref id="B35-sensors-25-05765"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dirgov&#225; Lupt&#225;kov&#225;</surname><given-names>I.</given-names></name><name name-style="western"><surname>Kubov&#269;&#237;k</surname><given-names>M.</given-names></name><name name-style="western"><surname>Posp&#237;chal</surname><given-names>J.</given-names></name></person-group><article-title>Wearable Sensor-Based Human Activity Recognition with Transformer Model</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>1911</elocation-id><pub-id pub-id-type="doi">10.3390/s22051911</pub-id><pub-id pub-id-type="pmid">35271058</pub-id><pub-id pub-id-type="pmcid">PMC8914677</pub-id></element-citation></ref><ref id="B36-sensors-25-05765"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kolter</surname><given-names>J.Z.</given-names></name><name name-style="western"><surname>Koltun</surname><given-names>V.</given-names></name></person-group><article-title>An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1803.01271</pub-id><pub-id pub-id-type="arxiv">1803.01271</pub-id></element-citation></ref><ref id="B37-sensors-25-05765"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lea</surname><given-names>C.</given-names></name><name name-style="western"><surname>Flynn</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Vidal</surname><given-names>R.</given-names></name><name name-style="western"><surname>Reiter</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hager</surname><given-names>G.D.</given-names></name></person-group><article-title>Temporal Convolutional Networks for Action Segmentation and Detection</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date></element-citation></ref><ref id="B38-sensors-25-05765"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramachandran</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zoph</surname><given-names>B.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name></person-group><article-title>Searching for activation functions</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1710.05941</pub-id><pub-id pub-id-type="arxiv">1710.05941</pub-id></element-citation></ref><ref id="B39-sensors-25-05765"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date></element-citation></ref><ref id="B40-sensors-25-05765"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>H.</given-names></name></person-group><article-title>Attention-based residual BiLSTM networks for human activity recognition</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>94173</fpage><lpage>94187</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3310269</pub-id></element-citation></ref><ref id="B41-sensors-25-05765"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cho</surname><given-names>K.</given-names></name><name name-style="western"><surname>van Merrienboer</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gulcehre</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bahdanau</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bougares</surname><given-names>F.</given-names></name><name name-style="western"><surname>Schwenk</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1406.1078</pub-id><pub-id pub-id-type="arxiv">1406.1078</pub-id></element-citation></ref><ref id="B42-sensors-25-05765"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raffel</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ellis</surname><given-names>D.P.</given-names></name></person-group><article-title>Feed-forward networks with attention can solve some long-term memory problems</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1512.08756</pub-id></element-citation></ref><ref id="B43-sensors-25-05765"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Maas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hannun</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ng</surname><given-names>A.</given-names></name></person-group><article-title>Rectifier Nonlinearities Improve Neural Network Acoustic Models</article-title><source>Proceedings of the ICML Workshop on Deep Learning for Audio, Speech, and Language Processing</source><conf-loc>Atlanta, GA, USA</conf-loc><conf-date>16 June 2013</conf-date></element-citation></ref><ref id="B44-sensors-25-05765"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwapisz</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Weiss</surname><given-names>G.M.</given-names></name><name name-style="western"><surname>Moore</surname><given-names>S.A.</given-names></name></person-group><article-title>Activity recognition using cell phone accelerometers</article-title><source>ACM SigKDD Explor. Newsl.</source><year>2011</year><volume>12</volume><fpage>74</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1145/1964897.1964918</pub-id></element-citation></ref><ref id="B45-sensors-25-05765"><label>45.</label><element-citation publication-type="webpage"><article-title>WISDM: Wireless Sensor Data Mining</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.cis.fordham.edu/wisdm/dataset.php" ext-link-type="uri">http://www.cis.fordham.edu/wisdm/dataset.php</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-24">(accessed on 24 March 2025)</date-in-citation></element-citation></ref><ref id="B46-sensors-25-05765"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sawchuk</surname><given-names>A.A.</given-names></name></person-group><article-title>USC-HAD: A Daily Activity Dataset for Ubiquitous Activity Recognition Using Wearable Sensors</article-title><source>Proceedings of the 2012 ACM Conference on Ubiquitous Computing</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>5&#8211;8 September 2012</conf-date></element-citation></ref><ref id="B47-sensors-25-05765"><label>47.</label><element-citation publication-type="webpage"><article-title>The USC-SIPI Human Activity Dataset</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://sipi.usc.edu/had/" ext-link-type="uri">https://sipi.usc.edu/had/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-24">(accessed on 24 March 2025)</date-in-citation></element-citation></ref><ref id="B48-sensors-25-05765"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Reiss</surname><given-names>A.</given-names></name><name name-style="western"><surname>Stricker</surname><given-names>D.</given-names></name></person-group><article-title>Introducing a new benchmarked dataset for activity monitoring</article-title><source>Proceedings of the 2012 16th International Symposium on Wearable Computers</source><conf-loc>Newcastle, UK</conf-loc><conf-date>18&#8211;22 June 2012</conf-date></element-citation></ref><ref id="B49-sensors-25-05765"><label>49.</label><element-citation publication-type="webpage"><article-title>PAMAP2 Physical Activity Monitoring</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring" ext-link-type="uri">https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-24">(accessed on 24 March 2025)</date-in-citation></element-citation></ref><ref id="B50-sensors-25-05765"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grandini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bagli</surname><given-names>E.</given-names></name><name name-style="western"><surname>Visani</surname><given-names>G.</given-names></name></person-group><article-title>Metrics for multi-class classification: An overview</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2008.05756</pub-id><pub-id pub-id-type="arxiv">2008.05756</pub-id></element-citation></ref><ref id="B51-sensors-25-05765"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stojchevska</surname><given-names>M.</given-names></name><name name-style="western"><surname>De Brouwer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Courteaux</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ongenae</surname><given-names>F.</given-names></name><name name-style="western"><surname>Van Hoecke</surname><given-names>S.</given-names></name></person-group><article-title>From Lab to Real World: Assessing the Effec-tiveness of Human Activity Recognition and Optimization through Personalization</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>4606</elocation-id><pub-id pub-id-type="doi">10.3390/s23104606</pub-id><pub-id pub-id-type="pmid">37430521</pub-id><pub-id pub-id-type="pmcid">PMC10220886</pub-id></element-citation></ref><ref id="B52-sensors-25-05765"><label>52.</label><element-citation publication-type="webpage"><article-title>IDLab Lifestyle-Monitoring</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://cloud.ilabt.imec.be/index.php/s/2zkXxEDTTQgkN5y" ext-link-type="uri">https://cloud.ilabt.imec.be/index.php/s/2zkXxEDTTQgkN5y</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-22">(accessed on 22 August 2025)</date-in-citation></element-citation></ref><ref id="B53-sensors-25-05765"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paszke</surname><given-names>A.</given-names></name></person-group><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1912.01703</pub-id></element-citation></ref><ref id="B54-sensors-25-05765"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sayeedi</surname><given-names>M.F.A.</given-names></name><name name-style="western"><surname>Deepti</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Osmani</surname><given-names>A.M.I.M.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>T.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>M.M.</given-names></name></person-group><article-title>A Comparative Analysis for Optimizing Machine Learning Model Deployment in IoT Devices</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>5459</elocation-id><pub-id pub-id-type="doi">10.3390/app14135459</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05765-f001" orientation="portrait"><label>Figure 1</label><caption><p>Comparative diagram of research on human activity recognition based on data sources and feature extraction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g001.jpg"/></fig><fig position="float" id="sensors-25-05765-f002" orientation="portrait"><label>Figure 2</label><caption><p>TGA-HAR Model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g002.jpg"/></fig><fig position="float" id="sensors-25-05765-f003" orientation="portrait"><label>Figure 3</label><caption><p>Architecture of the TCN model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g003.jpg"/></fig><fig position="float" id="sensors-25-05765-f004" orientation="portrait"><label>Figure 4</label><caption><p>Causal dilated convolution (K = 5).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g004.jpg"/></fig><fig position="float" id="sensors-25-05765-f005" orientation="portrait"><label>Figure 5</label><caption><p>The residual block in TCN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g005.jpg"/></fig><fig position="float" id="sensors-25-05765-f006" orientation="portrait"><label>Figure 6</label><caption><p>Res-BiGRU module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g006.jpg"/></fig><fig position="float" id="sensors-25-05765-f007" orientation="portrait"><label>Figure 7</label><caption><p>Dataset preprocessing workflow diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g007.jpg"/></fig><fig position="float" id="sensors-25-05765-f008" orientation="portrait"><label>Figure 8</label><caption><p>The training accuracy of the TGA-HAR model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g008.jpg"/></fig><fig position="float" id="sensors-25-05765-f009" orientation="portrait"><label>Figure 9</label><caption><p>Overall performance of the TGA-HAR model for three Datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g009.jpg"/></fig><fig position="float" id="sensors-25-05765-f010" orientation="portrait"><label>Figure 10</label><caption><p>WISDM dataset performance metrics comparison for activity classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g010.jpg"/></fig><fig position="float" id="sensors-25-05765-f011" orientation="portrait"><label>Figure 11</label><caption><p>Confusion matrix heatmap for WISDM dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g011.jpg"/></fig><fig position="float" id="sensors-25-05765-f012" orientation="portrait"><label>Figure 12</label><caption><p>USC-HAD dataset performance metrics comparison for activity classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g012.jpg"/></fig><fig position="float" id="sensors-25-05765-f013" orientation="portrait"><label>Figure 13</label><caption><p>Confusion matrix heatmap for USC-HAD dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g013.jpg"/></fig><fig position="float" id="sensors-25-05765-f014" orientation="portrait"><label>Figure 14</label><caption><p>PAMAP2 Dataset Performance Metrics Comparison for Activity Classification.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g014.jpg"/></fig><fig position="float" id="sensors-25-05765-f015" orientation="portrait"><label>Figure 15</label><caption><p>Confusion Matrix Heatmap for PAMAP2 Dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g015.jpg"/></fig><fig position="float" id="sensors-25-05765-f016" orientation="portrait"><label>Figure 16</label><caption><p>Confusion matrix heatmap for real-world IDLab dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g016.jpg"/></fig><fig position="float" id="sensors-25-05765-f017" orientation="portrait"><label>Figure 17</label><caption><p>Demonstration of the HAR system using the TGA-HAR model. (<bold>a</bold>) Awaiting input. (<bold>b</bold>) Recognized Activity: Standing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g017.jpg"/></fig><fig position="float" id="sensors-25-05765-f018" orientation="portrait"><label>Figure 18</label><caption><p>Recognition time of each human activity.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g018.jpg"/></fig><fig position="float" id="sensors-25-05765-f019" orientation="portrait"><label>Figure 19</label><caption><p>CPU usage for testing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05765-g019.jpg"/></fig><table-wrap position="float" id="sensors-25-05765-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t001_Table 1</object-id><label>Table 1</label><caption><p>Eight methods in existing HAR research and their used datasets, data acquisition methods, and advantages.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref. No.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Acquisition Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method </th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Advantage</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B28-sensors-25-05765" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WISDM, UCI-HAR, PAMAP2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accelerometer, Gyroscope, IMU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-branch CNN-BiLSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-scale Temporal Feature Extraction</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B29-sensors-25-05765" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WISDM, UCI-HAR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accelerometer, Gyroscope</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Attention-based Multi-head CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SE Module-based Dynamic Channel Weighting</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B30-sensors-25-05765" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H-activity, MHEALTH, UCI-HAR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accelerometer, Gyroscope,<break/>Linear acceleration</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-LSTM-Self-Attention</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multimodal Temporal Joint Feature Utilization</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B31-sensors-25-05765" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PAMAP2, WISDM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accelerometer, IMU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spatial Attention CNN + Genetic Algorithm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wavelet-based Image Encoding and Feature Optimization</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B32-sensors-25-05765" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WISDM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accelerometer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-BiGRU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bidirectional Gated Units for Long-term Dependencies</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B33-sensors-25-05765" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WISDM, PAMAP2, USC-HAD</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accelerometer, Gyroscope, IMU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TCN-Attention-HAR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Knowledge Distillation-based Multi-scale TCN</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B34-sensors-25-05765" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MHEALTH and 6 others</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi sensors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Temporal-Channel Dual-branch Self-Attention Network</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Independent Temporal and Channel Dependencies</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B35-sensors-25-05765" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KU-HAR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accelerometer, Gyroscope</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Transformer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">End-to-End Global Dependencies</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t002_Table 2</object-id><label>Table 2</label><caption><p>Symbol description table.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Symbol</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Description</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Kernel size of TCN, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Dilation factor of TCN, <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of convolutional layers within TCN, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Time step at moment <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Input data at time point <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th TCN layer </td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Convolution kernel weight at position <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Output of the <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th layer dilated causal convolution </td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Learnable parameter in Swish function (set <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in this study)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid activation function, <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Residual function comprising dilated causal convolution, batch normalization, and activation</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Input of the <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th residual block or output of the <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>-th residual block</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Update gate state of GRU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Reset gate state of GRU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Candidate hidden state of GRU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid activation function,</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">Trainable weight matrices for GRU update gate, reset gate, and candidate hidden state</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Hidden state corresponding to previous time step <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8857;</td><td align="center" valign="middle" rowspan="1" colspan="1">Element-wise multiplication</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Final state update of GRU</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature information output by the attention layer</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t003_Table 3</object-id><label>Table 3</label><caption><p>Summary of data acquisition specifications for the WISDM dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Data Acquisition Environment</td><td align="center" valign="middle" rowspan="1" colspan="1">Laboratory-controlled environment (ensuring data quality)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Participants</td><td align="center" valign="middle" rowspan="1" colspan="1">36</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Activity Types</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Device Type</td><td align="center" valign="middle" rowspan="1" colspan="1">Smartphone accelerometer</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Devices Worn</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wearing Position</td><td align="center" valign="middle" rowspan="1" colspan="1">In their front pants leg pocket</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Transmission Mode</td><td align="center" valign="middle" rowspan="1" colspan="1">File transfer during post-processing</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sampling Frequency</td><td align="center" valign="middle" rowspan="1" colspan="1">20 Hz</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sampling Duration</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Approximately 15.25 h</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t004_Table 4</object-id><label>Table 4</label><caption><p>Summary of data acquisition specifications for the USC-HAD dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Data Acquisition Environment</td><td align="center" valign="middle" rowspan="1" colspan="1">Laboratory-controlled environment (wired connection ensures signal quality)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Participants</td><td align="center" valign="middle" rowspan="1" colspan="1">14</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Activity Types</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Device Type</td><td align="center" valign="middle" rowspan="1" colspan="1">IMU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Devices Worn</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wearing Position</td><td align="center" valign="middle" rowspan="1" colspan="1">At their front right hip</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Transmission Mode</td><td align="center" valign="middle" rowspan="1" colspan="1">Wired connection</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sampling Frequency</td><td align="center" valign="middle" rowspan="1" colspan="1">100 Hz</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sampling Duration</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average 6 h per participant</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t005_Table 5</object-id><label>Table 5</label><caption><p>Description of 12 activity types in the USC-HAD dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">No.</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Activity</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking Forward</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject walks forward in a straight line.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking Left</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject walks counter-clockwise in a full circle.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking Right</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject walks clockwise in a full circle</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking Upstairs</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject goes up multiple flights. </td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5</td><td align="left" valign="middle" rowspan="1" colspan="1">Walking Downstairs</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject goes down multiple fights.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6</td><td align="left" valign="middle" rowspan="1" colspan="1">Running Forward</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject runs forward in a straight line.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">7</td><td align="left" valign="middle" rowspan="1" colspan="1">Jumping</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject stays at the same position and continuously jumps up and down.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">8</td><td align="left" valign="middle" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject sits on a chair either working or resting. Fidgeting is also considered to belong to this class.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">9</td><td align="left" valign="middle" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject stands and talks to someone.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">10</td><td align="left" valign="middle" rowspan="1" colspan="1">Sleeping</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject sleeps or lies down on a bed.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">11</td><td align="left" valign="middle" rowspan="1" colspan="1">Elevator Up</td><td align="left" valign="middle" rowspan="1" colspan="1">The subject rides in an ascending elevator.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Elevator Down</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The subject rides in a descending elevator.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t006_Table 6</object-id><label>Table 6</label><caption><p>Summary of Data Acquisition Specifications for the PAMAP2 Dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Data Acquisition Environment</td><td align="center" valign="middle" rowspan="1" colspan="1">Daily life environment (partial signal loss during acquisition)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Participants</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Activity Types</td><td align="center" valign="middle" rowspan="1" colspan="1">18</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Device Type</td><td align="center" valign="middle" rowspan="1" colspan="1">IMU, Heart rate monitor</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Devices Worn</td><td align="center" valign="middle" rowspan="1" colspan="1">IMU &#215; 3, Heart rate monitor &#215; 1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wearing Position</td><td align="center" valign="middle" rowspan="1" colspan="1">Three IMUs: wrist (dominant side), chest, ankle (dominant side); Heart rate monitor: chest</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Transmission Mode</td><td align="center" valign="middle" rowspan="1" colspan="1">Wireless transmission</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sampling Frequency</td><td align="center" valign="middle" rowspan="1" colspan="1">100 Hz</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sampling Duration</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Over 10 h per participant</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t007_Table 7</object-id><label>Table 7</label><caption><p>Description of 18 Activity Types in the PAMAP2 Dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">No.</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Activity</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lying</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lying quietly while doing nothing, small movements&#8212;e.g., changing the lying posture&#8212;are allowed.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sitting</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sitting in a chair in whatever posture the subject feels comfortable, changing sitting postures is also allowed.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standing</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Consists of standing still or standing still and talking, possibly gesticulating.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ironing</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ironing 1&#8211;2 shirts or T-shirts.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vacuuming</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vacuum cleaning one or two office rooms (which includes moving objects, e.g., chairs, placed on the floor).</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ascending stairs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ascending stairs was performed in a building between the ground and the top floors, a distance of five floors had to be covered going upstairs.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Descending stairs</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Descending stairs was performed in a building between the ground and the top floors, a distance of five floors had to be covered going downstairs.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal walking</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking outside with moderate to brisk pace with a speed of 4&#8211;6 km/h, according to what was suitable for the subject.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nordic walking</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nordic walking was performed outside on asphaltic terrain, using asphalt pads on the walking poles (it has to be noted, that none of the subjects was very familiar with this sport activity).</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cycling</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cycling was performed outside with a real bike with slow to moderate pace, as if the subject would bike to work or bike for pleasure (but not as a sport activity).</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Running</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Running meant jogging outside with a suitable speed for the individual subjects.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rope jumping</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The subjects used the technique most suitable for them, which mainly consisted of the basic jump (where both feet jump at the same time over the rope) or the alternate foot jump (where alternate feet are used to jump off the ground).</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Watching TV</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Watching TV at home, in whatever posture&#8212;lying, siting&#8212;the subject feels comfortable.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Computer work</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Working normally in the office.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Car driving</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Driving between office and subject&#8217;s home.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Folding laundry</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Folding shirts, T-shirts and/or bed linens.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">House cleaning</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dusting some shelves, including removing books and other things and putting them back again onto the shelves.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Playing soccer</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Playing 1 vs. 1 or 2 vs. 1, running with the ball, dribbling, passing the ball and shooting the ball on goal.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t008_Table 8</object-id><label>Table 8</label><caption><p>Training&#8211;test set sample distribution for human activity recognition datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Post-Preprocessing Samples</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Set Samples</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Set Samples</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">WISDM</td><td align="center" valign="middle" rowspan="1" colspan="1">26,447</td><td align="center" valign="middle" rowspan="1" colspan="1">20,838</td><td align="center" valign="middle" rowspan="1" colspan="1">5609</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">USC-HAD</td><td align="center" valign="middle" rowspan="1" colspan="1">20,769</td><td align="center" valign="middle" rowspan="1" colspan="1">15,449</td><td align="center" valign="middle" rowspan="1" colspan="1">5320</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PAMAP2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7494</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6441</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1053</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t009_Table 9</object-id><label>Table 9</label><caption><p>Experimental environment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specification</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel Core i7-13700H</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDlA GeForce RTX 4050 Laptop GPU (6 GB)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Memory</td><td align="center" valign="middle" rowspan="1" colspan="1">16 GB DDR5 4800 MHz</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Python</td><td align="center" valign="middle" rowspan="1" colspan="1">3.11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PyTorch</td><td align="center" valign="middle" rowspan="1" colspan="1">2.2.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CUDA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t010_Table 10</object-id><label>Table 10</label><caption><p>The hyperparameter configuration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">Adam</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Epochs</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch Size</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learning Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Loss Function</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cross Entropy</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t011" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t011_Table 11</object-id><label>Table 11</label><caption><p>TGA-HAR model parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Model Block</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Set</th></tr></thead><tbody><tr><td rowspan="12" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">TCN</td><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">TCN Block 1</td><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">K</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">d</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Filters</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">TCN Block 2</td><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">K</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">d</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Filters</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">TCN Block 3</td><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">K</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">d</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Filters</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Res-BiGRU</td><td align="center" valign="middle" rowspan="1" colspan="1">BiGRU 1</td><td align="center" valign="middle" rowspan="1" colspan="1">Neuron</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BiGRU 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neuron</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64</td></tr><tr><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Attention Layer</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Fully Connected Layer (FC Layer)</td><td align="center" valign="middle" rowspan="1" colspan="1">FC Layer 1</td><td align="center" valign="middle" rowspan="1" colspan="1">Neuron</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FC Layer 2</td><td align="center" valign="middle" rowspan="1" colspan="1">Neuron</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FC Layer 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neuron</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activity number</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t012" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t012_Table 12</object-id><label>Table 12</label><caption><p>Comparative experiment results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th></tr><tr><th colspan="2" align="center" valign="middle" rowspan="1">WISDM</th><th colspan="2" align="center" valign="middle" rowspan="1">USC-HAD</th><th colspan="2" align="center" valign="middle" rowspan="1">PAMAP2</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">96.80</td><td align="center" valign="middle" rowspan="1" colspan="1">96.97</td><td align="center" valign="middle" rowspan="1" colspan="1">88.95</td><td align="center" valign="middle" rowspan="1" colspan="1">89.02</td><td align="center" valign="middle" rowspan="1" colspan="1">92.21</td><td align="center" valign="middle" rowspan="1" colspan="1">92.24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Res-CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">96.98</td><td align="center" valign="middle" rowspan="1" colspan="1">97.04</td><td align="center" valign="middle" rowspan="1" colspan="1">90.73</td><td align="center" valign="middle" rowspan="1" colspan="1">90.66</td><td align="center" valign="middle" rowspan="1" colspan="1">92.78</td><td align="center" valign="middle" rowspan="1" colspan="1">92.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN-LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">98.66</td><td align="center" valign="middle" rowspan="1" colspan="1">98.67</td><td align="center" valign="middle" rowspan="1" colspan="1">92.31</td><td align="center" valign="middle" rowspan="1" colspan="1">92.36</td><td align="center" valign="middle" rowspan="1" colspan="1">94.11</td><td align="center" valign="middle" rowspan="1" colspan="1">94.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TCN-Attention-HAR [<xref rid="B34-sensors-25-05765" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">99.03</td><td align="center" valign="middle" rowspan="1" colspan="1">98.50</td><td align="center" valign="middle" rowspan="1" colspan="1">96.32</td><td align="center" valign="middle" rowspan="1" colspan="1">94.23</td><td align="center" valign="middle" rowspan="1" colspan="1">98.35</td><td align="center" valign="middle" rowspan="1" colspan="1">98.37</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TGA-HAR (This Study)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.96</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t013" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t013_Table 13</object-id><label>Table 13</label><caption><p>Ablation experiment results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th></tr><tr><th colspan="2" align="center" valign="middle" rowspan="1">WISDM</th><th colspan="2" align="center" valign="middle" rowspan="1">USC-HAD</th><th colspan="2" align="center" valign="middle" rowspan="1">PAMAP2</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">TCN-Only</td><td align="center" valign="middle" rowspan="1" colspan="1">96.70</td><td align="center" valign="middle" rowspan="1" colspan="1">96.77</td><td align="center" valign="middle" rowspan="1" colspan="1">91.22</td><td align="center" valign="middle" rowspan="1" colspan="1">91.26</td><td align="center" valign="middle" rowspan="1" colspan="1">93.45</td><td align="center" valign="middle" rowspan="1" colspan="1">93.41</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GRU-Only</td><td align="center" valign="middle" rowspan="1" colspan="1">96.43</td><td align="center" valign="middle" rowspan="1" colspan="1">96.54</td><td align="center" valign="middle" rowspan="1" colspan="1">91.47</td><td align="center" valign="middle" rowspan="1" colspan="1">91.50</td><td align="center" valign="middle" rowspan="1" colspan="1">92.88</td><td align="center" valign="middle" rowspan="1" colspan="1">92.89</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TCN-GRU-No-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">99.02</td><td align="center" valign="middle" rowspan="1" colspan="1">99.02</td><td align="center" valign="middle" rowspan="1" colspan="1">93.14</td><td align="center" valign="middle" rowspan="1" colspan="1">93.12</td><td align="center" valign="middle" rowspan="1" colspan="1">94.68</td><td align="center" valign="middle" rowspan="1" colspan="1">94.69</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TGA-HAR (This Study)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.96</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t014" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t014_Table 14</object-id><label>Table 14</label><caption><p>Comparison of accuracy and F1-Score of the TGA-HAR model on the PAMAP2 dataset under different numbers of GRU Layers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">The Number of GRU Layers</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">96.96</td><td align="center" valign="middle" rowspan="1" colspan="1">96.96</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">97.34</td><td align="center" valign="middle" rowspan="1" colspan="1">97.34</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.52</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t015" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t015_Table 15</object-id><label>Table 15</label><caption><p>Sample Distribution of the Real-World IDLab Dataset After Window Segmentation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Participant_ID</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computer Table</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Cycling</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Running</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standing Still</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Walking</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_0</td><td align="center" valign="middle" rowspan="1" colspan="1">29,970</td><td align="center" valign="middle" rowspan="1" colspan="1">138</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">187</td><td align="center" valign="middle" rowspan="1" colspan="1">1111</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_3</td><td align="center" valign="middle" rowspan="1" colspan="1">34,725</td><td align="center" valign="middle" rowspan="1" colspan="1">1576</td><td align="center" valign="middle" rowspan="1" colspan="1">29</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">8726</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_5</td><td align="center" valign="middle" rowspan="1" colspan="1">46,789</td><td align="center" valign="middle" rowspan="1" colspan="1">397</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">48</td><td align="center" valign="middle" rowspan="1" colspan="1">2228</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_6</td><td align="center" valign="middle" rowspan="1" colspan="1">34,155</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1208</td><td align="center" valign="middle" rowspan="1" colspan="1">3581</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_7</td><td align="center" valign="middle" rowspan="1" colspan="1">23,705</td><td align="center" valign="middle" rowspan="1" colspan="1">1250</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">129</td><td align="center" valign="middle" rowspan="1" colspan="1">4567</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_8</td><td align="center" valign="middle" rowspan="1" colspan="1">7621</td><td align="center" valign="middle" rowspan="1" colspan="1">595</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">104</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_9</td><td align="center" valign="middle" rowspan="1" colspan="1">1428</td><td align="center" valign="middle" rowspan="1" colspan="1">296</td><td align="center" valign="middle" rowspan="1" colspan="1">4174</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">2563</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_10</td><td align="center" valign="middle" rowspan="1" colspan="1">230</td><td align="center" valign="middle" rowspan="1" colspan="1">470</td><td align="center" valign="middle" rowspan="1" colspan="1">767</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">288</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_11</td><td align="center" valign="middle" rowspan="1" colspan="1">774</td><td align="center" valign="middle" rowspan="1" colspan="1">416</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">219</td><td align="center" valign="middle" rowspan="1" colspan="1">3693</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_12</td><td align="center" valign="middle" rowspan="1" colspan="1">564</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">395</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">388</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_13</td><td align="center" valign="middle" rowspan="1" colspan="1">6294</td><td align="center" valign="middle" rowspan="1" colspan="1">295</td><td align="center" valign="middle" rowspan="1" colspan="1">1305</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1901</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_14</td><td align="center" valign="middle" rowspan="1" colspan="1">16,487</td><td align="center" valign="middle" rowspan="1" colspan="1">3173</td><td align="center" valign="middle" rowspan="1" colspan="1">7146</td><td align="center" valign="middle" rowspan="1" colspan="1">645</td><td align="center" valign="middle" rowspan="1" colspan="1">10,351</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_15</td><td align="center" valign="middle" rowspan="1" colspan="1">260</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">196</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">184</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_16</td><td align="center" valign="middle" rowspan="1" colspan="1">43,081</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">231</td><td align="center" valign="middle" rowspan="1" colspan="1">1766</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Participant_17</td><td align="center" valign="middle" rowspan="1" colspan="1">16,019</td><td align="center" valign="middle" rowspan="1" colspan="1">444</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">47</td><td align="center" valign="middle" rowspan="1" colspan="1">1117</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sum</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">262,102</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9070</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14,012</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2714</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42,568</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t016" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t016_Table 16</object-id><label>Table 16</label><caption><p>Sample Distribution of Training Set and Test Set for the Real-World IDLab Dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset Setup</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computer Table</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Cycling</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Running</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standing Still</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Walking</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sum</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training Set</td><td align="center" valign="middle" rowspan="1" colspan="1">193,888</td><td align="center" valign="middle" rowspan="1" colspan="1">7524</td><td align="center" valign="middle" rowspan="1" colspan="1">9838</td><td align="center" valign="middle" rowspan="1" colspan="1">2354</td><td align="center" valign="middle" rowspan="1" colspan="1">33,672</td><td align="center" valign="middle" rowspan="1" colspan="1">247,276</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Test Set</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68,214</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1546</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4174</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">360</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8896</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83,190</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05765-t017" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05765-t017_Table 17</object-id><label>Table 17</label><caption><p>Experimental results for the real-world IDLab dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metrics</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">94.41</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">67.36</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.11</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>