<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12475039</article-id><article-id pub-id-type="pmcid-ver">PMC12475039.1</article-id><article-id pub-id-type="pmcaid">12475039</article-id><article-id pub-id-type="pmcaiid">12475039</article-id><article-id pub-id-type="pmid">41006661</article-id><article-id pub-id-type="doi">10.1038/s41598-025-18306-1</article-id><article-id pub-id-type="publisher-id">18306</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Evaluating the clinical utility of multimodal large language models for detecting age-related macular degeneration from retinal imaging</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Most</surname><given-names initials="JA">Jesse A.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Folk</surname><given-names initials="GA">Gillian A.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Walker</surname><given-names initials="EH">Evan H.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Nagel</surname><given-names initials="ID">Ines D.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Mehta</surname><given-names initials="NN">Nehal N.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Flester</surname><given-names initials="E">Elena</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Borooah</surname><given-names initials="S">Shyamanga</given-names></name><address><email>sborooah@health.ucsd.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0168r3w48</institution-id><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution>Shiley Eye Institute, </institution><institution>Jacobs Retina Center, University of California San Diego, </institution></institution-wrap>La Jolla, CA USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0168r3w48</institution-id><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution>School of Medicine, </institution><institution>University of California San Diego, </institution></institution-wrap>La Jolla, CA 92037 USA </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0168r3w48</institution-id><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution>Viterbi Family Department of Ophthalmology and Shiley Eye Institute, </institution><institution>University of California San Diego, </institution></institution-wrap>La Jolla, CA USA </aff></contrib-group><pub-date pub-type="epub"><day>26</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>33214</elocation-id><history><date date-type="received"><day>7</day><month>5</month><year>2025</year></date><date date-type="accepted"><day>1</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>28</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 16:25:18.507"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="41598_2025_Article_18306.pdf"/><abstract id="Abs1"><p id="Par1">This single-center retrospective study evaluated the performance of four multimodal large language models (MLLMs) (ChatGPT-4o, Claude 3.5 Sonnet, Google Gemini 1.5 Pro, Perplexity Sonar Large) in detecting and grading the severity of age-related macular degeneration (AMD) from ultrawide field fundus images. Images from 76 patients (136 eyes; mean age 81.1&#160;years; 69.7% female) seen at the University of California San Diego were graded independently for AMD severity by two junior retinal specialists (and an adjudicating senior retina specialist for disagreements) using the Age-Related Eye Disease Study (AREDS) classification. The cohort included 17 (12.5%) eyes with &#8216;No AMD&#8217;, 18 (13.2%) with &#8216;Early AMD&#8217;, 50 (36.8%) with &#8216;Intermediate AMD&#8217;, and 51 (37.5%) with &#8216;Advanced AMD&#8217;. Between December 2024 and February 2025, each MLLM was prompted with single images and standardized queries to assess the primary outcomes of accuracy, sensitivity, and specificity in binary disease classification, disease severity grading, open-ended diagnosis, and multiple-choice diagnosis (with distractor diseases). Secondary outcomes included precision, F1 scores, Cohen&#8217;s kappa, model performance comparisons, and error analysis. ChatGPT-4o demonstrated the highest accuracy for binary disease classification [mean 0.824 (95% confidence interval (CI)): 0.743, 0.875)], followed by Perplexity Sonar Large [mean 0.815 (95% CI: 0.744, 0.879)], both of which were significantly more accurate (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033) Than Gemini 1.5 Pro [mean 0.669 (95% CI: 0.581, 0.743)] and Claude 3.5 Sonnet [mean 0.301 (95% CI: 0.221, 0.375)]. For severity grading, Perplexity Sonar Large was most accurate [mean 0.463 (95% CI: 0.368, 0.537)], though differences among models were not statistically significant. ChatGPT-4o led in open-ended and multiple-choice diagnostic tasks. In summary, while MLLMs show promise for automated AMD detection and grading from fundus images, their current reliability is insufficient for clinical application, highlighting the need for further model development and validation.</p><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-18306-1.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Age-related macular degeneration</kwd><kwd>Artificial intelligence</kwd><kwd>Deep learning</kwd><kwd>Large language models</kwd><kwd>Multi-modal large language models</kwd><kwd>Ultrawide field fundus</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Macular degeneration</kwd><kwd>Retinal diseases</kwd><kwd>Diagnostic markers</kwd><kwd>Medical imaging</kwd><kwd>Software</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par16">Age-related macular degeneration (AMD) is a leading cause of central vision loss among adults aged 50&#160;years and older worldwide<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Early detection is crucial for timely interventions that minimize risk of progression to advanced stages, such as neovascular or &#8220;wet&#8221; AMD (nAMD), and to initiate therapy that can improve visual outcomes<sup><xref ref-type="bibr" rid="CR1">1</xref>&#8211;<xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par17">AMD is estimated to affect approximately 9% of older adults and is currently thought to be underdiagnosed<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Ophthalmic images, including color-fundus photography and optical coherence tomography (OCT), are traditionally used to diagnose and monitor disease progression. However, interpreting these images is time-consuming and relies on specialized expertise<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. As the aging population grows, so too will the demands for AMD screening and monitoring<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Automated methods for identifying pathologic features could help expand screening beyond eye-care settings, reach higher volumes of at-risk patients, and save valuable ophthalmic resources<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>.</p><p id="Par18">Recent advancements in artificial intelligence (AI) and deep learning (DL) offer new possibilities within healthcare for improving efficiency and diagnostic accuracy<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, and this has already been applied to AMD<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR6">6</xref>&#8211;<xref ref-type="bibr" rid="CR8">8</xref></sup>. Several recent studies have successfully used DL tools to diagnose AMD, predict short-term exudation risk, and determine the need for intravitreal injections<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Recently, there has been rapid growth of DL-based large language models (LLMs), such as ChatGPT<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>, and with the advent of multimodal capabilities, these multimodal large language models (MLLMs) are able to integrate diverse input types including images, video, audio, and text to generate predictions. While a growing number of studies have examined LLM performance in text-input ophthalmology queries<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, far fewer have focused on MLLM capabilities in imaging applications as this is a more recent advancement, and most studies have focused on ChatGPT<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. There is currently limited research evaluating the accuracy of widely accessible MLLMs in diagnosing and grading AMD severity from fundus images.</p><p id="Par19">In this study, we compare the performance of four widely accessible MLLMs [ChatGPT-4o (OpenAI, San Francisco, CA, USA), Claude 3.5 Sonnet (Anthropic, San Francisco, CA, USA), Google Gemini 1.5 Pro (Google LLC, Mountainview, CA, USA), and Perplexity Sonar Large (Perplexity AI, San Francisco, CA, USA)] in detecting and grading the severity of AMD from ultrawide field (UWF) fundus images. Additionally, we discuss the broader implications of implementing such models in clinical settings, and possible advantages over alternative AI tools.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par20">This comparative analysis study used retrospective images of patients seen at the University of California, San Diego (UCSD) from April 2023 to June 2024. UCSD institutional review board (Protocol #120,516) approved the study, including approval for a waiver of informed consent given the retrospective nature of the work. However, consent for the use of clinical data was taken at the clinical appointment per institutional policy. The study was performed in accordance with the IRB guidelines and regulations. The fundus images were anonymized with no additional patient information or metadata provided to the MLLM or human graders. The research was conducted according to the principles of the Declaration of Helsinki. The study was not registered in a publicly accessible database, however, data used in the study is available upon request. A formal study protocol was not prepared. There was no patient or public involvement during the design, conduct, reporting, interpretation, or dissemination of the study.</p><sec id="Sec3"><title>Image selection</title><p id="Par21">The study used an anonymized dataset of images acquired at our institution to avoid overlap with online data, which could potentially have been used to train MLLMs. Eligible AMD images were identified first by searching the UCSD electronic health record for patients who had completed fundus imaging, OCT, and fundus autofluorescence on the same day (these other imaging modalities were of interest in a separate study), and had a charted diagnosis of AMD as of the date of search, 16<sup>th</sup> June 2024 (Supplemental Fig.&#160;<xref rid="MOESM1" ref-type="media">1</xref>). Eyes with any history of the following were excluded: retinal pathology besides AMD, history of retinal surgery, history of ocular trauma, central retinal artery or vein occlusion, or intraocular surgery completed within 6&#160;months prior to the date of imaging. A small number of non-AMD eyes with non-excluding ocular history (such as cataracts, history of cataract extraction, pterygium, keratoconjunctivitis sicca, etc.) were identified in the EHR search and also included. Images without visible third-order arterioles were excluded due to poor image quality. A maximum of one image per eye was included to ensure heterogeneity of the dataset, with only the earliest imaging study included in cases of multiple exams over the study timeframe. Both eyes from individual patients were included, when possible. UWF images (Optos P200DTx, Optos PLC, Dunfermline, UK) had been obtained from patients approximately 15&#160;min after dilation with tropicamide 1% and phenylephrine 2.5%, according to usual clinic protocol. Demographic information was collected for each patient including age, sex, race, and ethnicity.</p></sec><sec id="Sec4"><title>Image processing</title><p id="Par22">Images were downloaded from Zeiss Forum Viewer (Version 4.2.4.15, Carl Zeiss, Oberkochen, Germany) after zooming in to maximize quality and approximate the field of view of a stereoscopic color fundus photograph. Images were saved in PNG format and fully anonymized. Two versions of each image were then saved: one with an Age-Related Eye Disease Study (AREDS) grid overlay for grading AMD severity, and a second version without the grid for all prompts unrelated to assessing disease severity. The grid overlay was designed and positioned per the methods outlined in AREDS Report No. 17 to provide a scale for assessing drusen size (Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. The grid overlay was manually added and positioned on all images using Microsoft PowerPoint (version 16.89, Microsoft Corporation, Redmond, USA).<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Example Image with AREDS Grid Overlay. Example ultrawide field fundus image with AREDS grid overlay<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> used for disease severity assessment. Grid provides scale for assessing drusen size to grade according to the AREDS criteria. Radii of the three inner grid circles are approximately one-third, one, and two optic disc diameters, respectively. AREDS&#8201;=&#8201;Age-Related Eye Disease Study.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO1" position="float" orientation="portrait" xlink:href="41598_2025_18306_Fig1_HTML.jpg"/></fig></p></sec><sec id="Sec5"><title>Image review</title><p id="Par23">Each image was then independently graded for disease severity by two junior retinal specialists (NM, IN) to establish ground truth diagnosis. Grades were assessed using the AREDS classification system per the American Academy of Ophthalmology Preferred Practice Pattern<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, which was provided to the graders. The possible severity grades were &#8216;No AMD&#8217;, &#8216;Early AMD&#8217;, &#8216;Intermediate AMD&#8217;, and &#8216;Advanced AMD&#8217;. Any images with grading differences were reviewed and adjudicated by a senior retina specialist (SB) to establish a consensus final grade.</p></sec><sec id="Sec6"><title>MLLM prompting</title><p id="Par24">Four MLLM models with image analysis capabilities were compared: ChatGPT-4o, Claude 3.5 Sonnet, Google Gemini 1.5 Pro, and Perplexity Sonar Large. The paid versions of each were used to access the most up-to-date web (non-API) models at the time of data collection (December 2024 to February 2025). The same series of images and prompts were entered into each model in identical fashion, using the default settings (temperatures of 1.0). For each patient fundus image, four unique prompts (labeled 1, 2, 3, and 4) were entered into the MLLM along with the uploaded image, one at a time. A new chat was started for every prompt to maintain the independence of each query, and to avoid MLLMs interpreting separate queries as interrelated follow-up questions.</p><p id="Par25">Prompts were designed using principles of prompt engineering<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Each prompt included a preface instructing the MLLM to &#8220;imagine [it is] an ophthalmologist&#8221;, to assume no other background information was known about the patient, and to provide the most likely answer based on the image. Figure&#160;<xref rid="Fig2" ref-type="fig">2</xref> illustrates an example input prompt and image. Prompts tested the MLLMs using both multiple-choice (MC) and open-ended formats as follows: Prompt 1&#8201;=&#8201;MC diagnosis, AMD or &#8216;No AMD&#8217; present; Prompt 2&#8201;=&#8201;MC disease severity, 4 choices ranging from &#8216;No AMD&#8217; to &#8216;Advanced AMD&#8217;; Prompt 3&#8201;=&#8201;open-ended diagnosis; Prompt 4&#8201;=&#8201;MC diagnosis, 12 retina condition choices. Prompt 2 requested grading of disease severity using the AREDS classification system, identical to the human graders, with the full grading criteria provided. For Prompts 1, 3 and 4, the images without an AREDS grid were used to avoid potentially biasing results, as the grid itself could be associated with AMD diagnosis. Due to the length of some prompts, we have only presented the full versions of these prompts in supplementary data for reference (Supplemental Table <xref rid="MOESM1" ref-type="media">1</xref>).<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Example MLLM Input. Example MLLM prompt text (Prompt 1) and ultrawide field fundus image input. AMD&#8201;=&#8201;age-related macular degeneration; MLLM&#8201;=&#8201;multimodal large language model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO2" position="float" orientation="portrait" xlink:href="41598_2025_18306_Fig2_HTML.jpg"/></fig></p></sec><sec id="Sec7"><title>Statistical analysis</title><p id="Par26">Subject-level demographic information is presented as count (%) and mean (95% confidence interval [CI]) for categorical and continuous parameters, respectively. Cohort stratifications were applied at the disease and disease severity levels, and comparisons of subject-level demographic characteristics were evaluated. Continuous subject-level characteristics were compared by AMD status using independent sample t-tests, while categorical subject-level characteristics were compared using Fisher&#8217;s exact and chi-squared tests. MLLM performance was evaluated using accuracy, sensitivity, specificity, precision, F1 score, and Cohen&#8217;s kappa. Prompts with multi-class ground truth or prediction labels were evaluated using macro-averaged performance metrics. Clustered bootstrap resampling was conducted at the subject level to produce 95% CI estimates of performance metrics. Comparisons of performance metrics between MLLMs were also evaluated using the bootstrap estimates. A Bonferroni correction was applied, resulting in a two-tailed threshold of statistical significance set at 0.00033. The statistical analysis was conducted using the R programming language for statistical computation, Version 4.4.0 (R Core Team (2024), R Foundation for Statistical Computing, Vienna, Austria).</p></sec></sec><sec id="Sec8"><title>Results</title><sec id="Sec9"><title>Cohort summary</title><p id="Par27">A total of 136 eyes from 76 patients were included, after excluding 21 eyes due to our exclusion criteria (other retina pathology, recent intraocular surgery, history of retina surgery), and 7 due to poor image quality. The causes for exclusion of images are summarized in Supplemental Table <xref rid="MOESM1" ref-type="media">2</xref>. Average patient age was 81.1 (95% CI: 79.1, 83.2) years, with 69.7% being female (Table <xref rid="Tab1" ref-type="table">1</xref>). There were no significant differences between subjects with AMD and without AMD in regard to age, race, ethnicity, or sex. For disease status, 17 (12.5%) eyes had &#8216;No AMD&#8217;, 18 (13.2%) had &#8216;Early AMD&#8217;, 50 (36.8%) had &#8216;Intermediate AMD&#8217;, and 51 (37.5%) had &#8216;Advanced AMD&#8217; (Table <xref rid="Tab1" ref-type="table">1</xref>). To compare human retinal specialist graders as we established a ground truth, interobserver agreement was evaluated with a weighted kappa analysis (this used squared distances to account for the relative distance between ordinal grades). Agreement was found to be 0.805 (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.001), indicating a high level of agreement. Eighteen (13.2%) eyes had initial grader disagreement requiring adjudication by a senior retina specialist to establish final grades. Fourteen of these represented disagreements between different AMD severity levels and 4 represented disagreements between AMD and non-AMD status. Table <xref rid="Tab2" ref-type="table">2</xref> summarizes the performance of each in terms of accuracy, sensitivity, specificity, precision, F1, and agreement/Cohen&#8217;s kappa.<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Cohort Demographic and Ground Truth Diagnosis Summary. Cohort demographic characteristics are presented. Number of eyes and percentage of the cohort are also displayed per diagnosis, as determined by human grader consensus. Means are presented with 95% confidence intervals. AMD&#8201;=&#8201;age-related macular degeneration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2" rowspan="1"><bold>Cohort Demographics</bold><break/><italic toggle="yes">(n</italic>&#8201;=&#8201;<italic toggle="yes">76 subjects)</italic></th></tr><tr><th align="left" colspan="1" rowspan="1"><bold>Age</bold></th><th align="left" colspan="1" rowspan="1">81.1 (79.1, 83.2)</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1"><bold>Race</bold></td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Asian</td><td align="left" colspan="1" rowspan="1">6 (7.9%)</td></tr><tr><td align="left" colspan="1" rowspan="1">Black or African American</td><td align="left" colspan="1" rowspan="1">2 (2.6%)</td></tr><tr><td align="left" colspan="1" rowspan="1">Other Race or Mixed Race</td><td align="left" colspan="1" rowspan="1">5 (6.6%)</td></tr><tr><td align="left" colspan="1" rowspan="1">Unreported</td><td align="left" colspan="1" rowspan="1">2 (2.6%)</td></tr><tr><td align="left" colspan="1" rowspan="1">White</td><td align="left" colspan="1" rowspan="1">61 (80.3%)</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Ethnicity</bold></td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Not Hispanic, Latino(a), or Spanish origin</td><td align="left" colspan="1" rowspan="1">67 (88.2%)</td></tr><tr><td align="left" colspan="1" rowspan="1">Other Hispanic, Latino(a) or Spanish origin</td><td align="left" colspan="1" rowspan="1">2 (2.6%)</td></tr><tr><td align="left" colspan="1" rowspan="1">Unreported</td><td align="left" colspan="1" rowspan="1">7 (9.2%)</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Sex</bold></td><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Female</td><td align="left" colspan="1" rowspan="1">53 (69.7%)</td></tr><tr><td align="left" colspan="1" rowspan="1">Male</td><td align="left" colspan="1" rowspan="1">23 (30.3%)</td></tr><tr><td align="left" colspan="2" rowspan="1"><p><bold>Cohort Ground Truth Diagnosis</bold></p><p><italic toggle="yes">(n</italic>&#8201;=&#8201;<italic toggle="yes">136 eyes)</italic></p></td></tr><tr><td align="left" colspan="1" rowspan="1"><p><bold>Disease Status</bold></p><p>AMD</p><p>No AMD</p></td><td align="left" colspan="1" rowspan="1"><p>119 (87.5%)</p><p>17 (12.5%)</p></td></tr><tr><td align="left" colspan="1" rowspan="1"><p><bold>Disease Severity</bold></p><p>No AMD</p><p>Early AMD</p><p>Intermediate AMD</p><p> Advanced AMD</p></td><td align="left" colspan="1" rowspan="1"><p>17 (12.5%)</p><p>18 (13.2%)</p><p>50 (36.8%)</p><p>51 (37.5%)</p></td></tr></tbody></table></table-wrap><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>MLLM Accuracy, Sensitivity, Specificity, Precision, F1, and Agreement for All Prompts. Performance of MLLM classification of images for each prompt. Performance metrics are presented with 95% confidence intervals.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="7" rowspan="1"><bold>Prompt 1</bold> (Binary disease classification)</th></tr><tr><th align="left" colspan="1" rowspan="1">MLLM</th><th align="left" colspan="1" rowspan="1">Accuracy</th><th align="left" colspan="1" rowspan="1">Sensitivity</th><th align="left" colspan="1" rowspan="1">Specificity</th><th align="left" colspan="1" rowspan="1">Precision</th><th align="left" colspan="1" rowspan="1">F1</th><th align="left" colspan="1" rowspan="1">Cohen&#8217;s kappa<break/>(Unweighted)</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">ChatGPT-4o</td><td align="left" colspan="1" rowspan="1">0.824 (0.743, 0.875)<sup><bold>1</bold></sup></td><td align="left" colspan="1" rowspan="1">0.832 (0.760, 0.893)<sup><bold>1</bold></sup></td><td align="left" colspan="1" rowspan="1">0.765 (0.529, 0.938)</td><td align="left" colspan="1" rowspan="1">0.961 (0.916, 0.990)</td><td align="left" colspan="1" rowspan="1">0.892 (0.848, 0.931)<sup><bold>1</bold></sup></td><td align="left" colspan="1" rowspan="1">0.425 (0.233, 0.599)<sup><bold>1</bold></sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Claude&#160;3.5 Sonnet</td><td align="left" colspan="1" rowspan="1">0.301 (0.221, 0.375)<sup><bold>4,5</bold></sup></td><td align="left" colspan="1" rowspan="1">0.227 (0.156, 0.304)<sup><bold>4,5</bold></sup></td><td align="left" colspan="1" rowspan="1">0.824 (0.615, 1.000)</td><td align="left" colspan="1" rowspan="1">0.900 (0.756, 1.000)</td><td align="left" colspan="1" rowspan="1">0.362 (0.264, 0.461)<sup><bold>4,5</bold></sup></td><td align="left" colspan="1" rowspan="1">0.016 (&#8722;0.050, 0.074)<sup><bold>5</bold></sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Gemini&#160;1.5 Pro</td><td align="left" colspan="1" rowspan="1">0.669 (0.581, 0.743)</td><td align="left" colspan="1" rowspan="1">0.647 (0.560, 0.727)</td><td align="left" colspan="1" rowspan="1">0.824 (0.589, 0.957)</td><td align="left" colspan="1" rowspan="1">0.963 (0.912, 1.000)</td><td align="left" colspan="1" rowspan="1">0.774 (0.702, 0.833)</td><td align="left" colspan="1" rowspan="1">0.237 (0.115, 0.381)</td></tr><tr><td align="left" colspan="1" rowspan="1">Perplexity Sonar Large</td><td align="left" colspan="1" rowspan="1">0.816 (0.735, 0.868)</td><td align="left" colspan="1" rowspan="1">0.815 (0.744, 0.879)</td><td align="left" colspan="1" rowspan="1">0.824 (0.600, 1.000)</td><td align="left" colspan="1" rowspan="1">0.970 (0.928, 1.000)</td><td align="left" colspan="1" rowspan="1">0.886 (0.837, 0.925)</td><td align="left" colspan="1" rowspan="1">0.432 (0.251, 0.606)</td></tr><tr><td align="left" colspan="7" rowspan="1"><bold>Prompt 2</bold> (Severity classification<bold>)</bold></td></tr><tr><td align="left" colspan="1" rowspan="1">MLLM</td><td align="left" colspan="1" rowspan="1">Accuracy</td><td align="left" colspan="1" rowspan="1">Sensitivity</td><td align="left" colspan="1" rowspan="1">Specificity</td><td align="left" colspan="1" rowspan="1">Precision</td><td align="left" colspan="1" rowspan="1">F1</td><td align="left" colspan="1" rowspan="1"><p>Cohen&#8217;s kappa</p><p>(Unweighted)</p></td></tr><tr><td align="left" colspan="1" rowspan="1">ChatGPT-4o</td><td align="left" colspan="1" rowspan="1">0.426 (0.338, 0.500)</td><td align="left" colspan="1" rowspan="1">0.299 (0.265, 0.347)</td><td align="left" colspan="1" rowspan="1">0.773 (0.757, 0.792)</td><td align="left" colspan="1" rowspan="1">0.731 (0.567, 0.822)</td><td align="left" colspan="1" rowspan="1">0.424 (0.360, 0.486)</td><td align="left" colspan="1" rowspan="1">0.095 (0.031, 0.176)</td></tr><tr><td align="left" colspan="1" rowspan="1">Claude&#160;3.5 Sonnet</td><td align="left" colspan="1" rowspan="1">0.419 (0.331, 0.500)</td><td align="left" colspan="1" rowspan="1">0.440 (0.353, 0.531)</td><td align="left" colspan="1" rowspan="1">0.791 (0.764, 0.817)</td><td align="left" colspan="1" rowspan="1">0.603 (0.507, 0.686)</td><td align="left" colspan="1" rowspan="1">0.509 (0.427, 0.590)</td><td align="left" colspan="1" rowspan="1">0.178 (0.077, 0.281)</td></tr><tr><td align="left" colspan="1" rowspan="1">Gemini&#160;1.5 Pro</td><td align="left" colspan="1" rowspan="1">0.426 (0.338, 0.500)</td><td align="left" colspan="1" rowspan="1">0.337 (0.283, 0.406)</td><td align="left" colspan="1" rowspan="1">0.775 (0.762, 0.794)</td><td align="left" colspan="1" rowspan="1">0.683 (0.558, 0.856)</td><td align="left" colspan="1" rowspan="1">0.452 (0.382, 0.535)</td><td align="left" colspan="1" rowspan="1">0.110 (0.051, 0.195)</td></tr><tr><td align="left" colspan="1" rowspan="1">Perplexity Sonar Large</td><td align="left" colspan="1" rowspan="1">0.463 (0.368, 0.537)</td><td align="left" colspan="1" rowspan="1">0.323 (0.283, 0.373)</td><td align="left" colspan="1" rowspan="1">0.789 (0.769, 0.812)</td><td align="left" colspan="1" rowspan="1">0.559 (0.379, 0.788)</td><td align="left" colspan="1" rowspan="1">0.409 (0.327, 0.487)</td><td align="left" colspan="1" rowspan="1">0.157 (0.075, 0.255)</td></tr><tr><td align="left" colspan="7" rowspan="1"><bold>Prompt 3</bold> (Open-ended diagnosis)</td></tr><tr><td align="left" colspan="1" rowspan="1">MLLM</td><td align="left" colspan="1" rowspan="1">Accuracy</td><td align="left" colspan="1" rowspan="1">Sensitivity</td><td align="left" colspan="1" rowspan="1">Specificity</td><td align="left" colspan="1" rowspan="1">Precision</td><td align="left" colspan="1" rowspan="1">F1</td><td align="left" colspan="1" rowspan="1"><p>Cohen&#8217;s kappa</p><p>(Unweighted)</p></td></tr><tr><td align="left" colspan="1" rowspan="1">ChatGPT-4o</td><td align="left" colspan="1" rowspan="1">0.478 (0.390, 0.559)<sup><bold>1,2,3</bold></sup></td><td align="left" colspan="1" rowspan="1">0.403 (0.316, 0.492)<sup><bold>1,2,3</bold></sup></td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">0.575 (0.477, 0.656)<sup><bold>1,2,3</bold></sup></td><td align="left" colspan="1" rowspan="1">0.145 (0.083, 0.226)<sup><bold>1,2,3</bold></sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Claude&#160;3.5 Sonnet</td><td align="left" colspan="1" rowspan="1">0.184 (0.118, 0.250)</td><td align="left" colspan="1" rowspan="1">0.067 (0.026, 0.116)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">0.126 (0.050, 0.217)</td><td align="left" colspan="1" rowspan="1">0.018 (0.006, 0.039)</td></tr><tr><td align="left" colspan="1" rowspan="1">Gemini&#160;1.5 Pro</td><td align="left" colspan="1" rowspan="1">0.154 (0.093, 0.213)</td><td align="left" colspan="1" rowspan="1">0.034 (0.008, 0.070)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">0.065 (0.017, 0.128)</td><td align="left" colspan="1" rowspan="1">0.009 (0.002, 0.022)</td></tr><tr><td align="left" colspan="1" rowspan="1">Perplexity Sonar Large</td><td align="left" colspan="1" rowspan="1">0.162 (0.096, 0.221)</td><td align="left" colspan="1" rowspan="1">0.050 (0.016, 0.092)</td><td align="left" colspan="1" rowspan="1">0.941 (0.769, 1.000)</td><td align="left" colspan="1" rowspan="1">0.857 (0.400, 1.000)</td><td align="left" colspan="1" rowspan="1">0.095 (0.033, 0.174)</td><td align="left" colspan="1" rowspan="1">&#8722;0.002 (&#8722;0.048, 0.020)</td></tr><tr><td align="left" colspan="7" rowspan="1"><bold>Prompt 4</bold> (Multiple-choice diagnosis)</td></tr><tr><td align="left" colspan="1" rowspan="1">MLLM</td><td align="left" colspan="1" rowspan="1">Accuracy</td><td align="left" colspan="1" rowspan="1">Sensitivity</td><td align="left" colspan="1" rowspan="1">Specificity</td><td align="left" colspan="1" rowspan="1">Precision</td><td align="left" colspan="1" rowspan="1">F1</td><td align="left" colspan="1" rowspan="1"><p>Cohen&#8217;s kappa</p><p>(Unweighted)</p></td></tr><tr><td align="left" colspan="1" rowspan="1">ChatGPT-4o</td><td align="left" colspan="1" rowspan="1">0.691 (0.603, 0.757)<sup><bold>1,2</bold></sup></td><td align="left" colspan="1" rowspan="1">0.655 (0.569, 0.735)<sup><bold>1,2</bold></sup></td><td align="left" colspan="1" rowspan="1">0.941 (0.779, 1.000)</td><td align="left" colspan="1" rowspan="1">0.987 (0.949, 1.000)</td><td align="left" colspan="1" rowspan="1">0.788 (0.723, 0.846)<sup><bold>1,2</bold></sup></td><td align="left" colspan="1" rowspan="1">0.297 (0.178, 0.440)<sup><bold>1,2</bold></sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Claude&#160;3.5 Sonnet</td><td align="left" colspan="1" rowspan="1">0.272 (0.191, 0.346)<sup><bold>5</bold></sup>
</td><td align="left" colspan="1" rowspan="1">0.168 (0.106, 0.239)<sup><bold>5</bold></sup></td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">0.288 (0.194, 0.385)<sup><bold>5</bold></sup></td><td align="left" colspan="1" rowspan="1">0.048 (0.024, 0.087)<sup><bold>5</bold></sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Gemini&#160;1.5 Pro</td><td align="left" colspan="1" rowspan="1">0.331 (0.243, 0.404)<sup><bold>6</bold></sup></td><td align="left" colspan="1" rowspan="1">0.244 (0.168, 0.325)<sup><bold>6</bold></sup></td><td align="left" colspan="1" rowspan="1">0.941 (0.769, 1.000)</td><td align="left" colspan="1" rowspan="1">0.967 (0.871, 1.000)</td><td align="left" colspan="1" rowspan="1">0.389 (0.290, 0.482)<sup><bold>6</bold></sup></td><td align="left" colspan="1" rowspan="1">0.057 (0.011, 0.108)</td></tr><tr><td align="left" colspan="1" rowspan="1">Perplexity Sonar Large</td><td align="left" colspan="1" rowspan="1">0.603 (0.507, 0.676)</td><td align="left" colspan="1" rowspan="1">0.546 (0.457, 0.629)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">1.000 (1.000, 1.000)</td><td align="left" colspan="1" rowspan="1">0.707 (0.629, 0.776)</td><td align="left" colspan="1" rowspan="1">0.231 (0.137, 0.341)</td></tr></tbody></table><table-wrap-foot><p>AMD&#8201;=&#8201;age-related macular degeneration; MLLM&#8201;=&#8201;multimodal large language model. Significant differences (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033, per Bonferroni correction) between models for overall accuracy, sensitivity, and specificity are denoted by the following: <sup><bold>1</bold></sup>ChatGPT vs. Claude; <sup><bold>2</bold></sup>ChatGPT vs. Gemini; <sup><bold>3</bold></sup>ChatGPT vs. Perplexity; <sup><bold>4</bold></sup>Claude vs. Gemini; <sup><bold>5</bold></sup>Claude vs. Perplexity; <sup><bold>6</bold></sup>Gemini vs. Perplexity.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec10"><title>AMD vs. No AMD classification (prompt 1)</title><p id="Par28">Prompt 1 requested the four MLLMs to grade images as more likely having or not having AMD. Accuracy was highest for ChatGPT-4o [0.824 (95% CI: 0.743, 0.875)], followed by Perplexity Sonar Large [0.816 (95% CI: 0.735, 0.868)], Gemini 1.5 Pro [0.669 (95% CI: 0.581, 0.743)], and Claude 3.5 Sonnet [0.301 (95% CI: 0.221, 0.375)]. Significant differences were present between the lowest performing (Claude 3.5 Sonnet) and the other three MLLMs (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033) (Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>). The same significant differences were found with F1 score, which was led by ChatGPT-4o [0.892 (0.848, 0.931)], Perplexity Sonar Large [0.886 (95% CI: 0.837, 0.925)], and Gemini 1.5 Pro [0.774 (95% CI: 0.702, 0.833)], followed by Claude [0.362 (95% CI: 0.264, 0.461)]. Sensitivity was highest for ChatGPT-4o [0.832 (95% CI: 0.760, 0.893)], followed by Perplexity Sonar Large [0.815 (95% CI: 0.744, 0.879)], Gemini 1.5 Pro [0.647 (95% CI: 0.560, 0.727)], and Claude 3.5 Sonnet [0.227 (95% CI: 0.156, 0.304)], with significant differences again only between Claude 3.5 Sonnet and the other models (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033). Specificity was non-significantly different between Claude 3.5 Sonnet [0.824 (95% CI: 0.615, 1.000)], Gemini 1.5 Pro [0.824 (95% CI: 0.589, 0.957)] and Perplexity Sonar Large [0.824 (95% CI: 0.600, 1.000)], followed by ChatGPT-4o [0.765 (95% CI: 0.529, 0.938)]. There were no significant differences in specificity between models.<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>MLLM Accuracy, Sensitivity, and Specificity Comparisons. For individual prompts, significant differences (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033) between models are present where error bars do not overlap.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="MO3" position="float" orientation="portrait" xlink:href="41598_2025_18306_Fig3_HTML.jpg"/></fig></p></sec><sec id="Sec11"><title>AMD severity classification (prompt 2)</title><p id="Par29">Prompt 2 requested the four MLLMs to grade images for the severity of AMD (ranging from &#8216;No AMD&#8217; to &#8216;Advanced AMD&#8217;) based on the AREDS grading criteria. In all comparisons between models, there were no significant differences. Accuracy was highest for Perplexity Sonar Large [0.463 (95% CI: 0.368, 0.537)], followed by ChatGPT-4o [0.426 (95% CI: 0.338, 0.500)], Gemini 1.5 Pro [0.426 (95% CI: 0.338, 0.500)], and Claude 3.5 Sonnet [0.419 (95% CI: 0.331, 0.500)]. Cohen&#8217;s kappa coefficients demonstrated universally poor agreement with human graders when calculated unweighted (range: 0.095 to 0.178), and also when calculated with squared distance weighting to account for relative nearness of responses to each other (range: &#8722;0.009 to 0.208). F1 score found no significant differences between models, and was generally poor, ranging from 0.409 (95% CI: 0.327, 0.487) to 0.509 (95% CI: 0.427, 0.590) overall.</p></sec><sec id="Sec12"><title>Open-ended diagnosis (prompt 3)</title><p id="Par30">Prompt 3 requested the four MLLMs to give the most likely diagnosis, if any, based on the image. This was intended to test MLLM performance in a query unspecific to AMD, and to establish a baseline accuracy for a broader diagnostic use case. Accuracy was highest for ChatGPT-4o [0.478 (95% CI: 0.390, 0.559)], followed by Claude 3.5 Sonnet [0.184 (95% CI: 0.118, 0.250)], Perplexity Sonar Large [0.162 (95% CI: 0.096, 0.221)], and Gemini 1.5 Pro [0.154 (95% CI: 0.093, 0.213)]. Significant differences were present between ChatGPT-4o and the other MLLMs (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033). Sensitivity was significantly higher (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033) for ChatGPT-4o [0.403 (95% CI: 0.316, 0.492)], followed by Claude 3.5 Sonnet [0.067 (95% CI: 0.026, 0.116)], Perplexity Sonar Large [0.050 (95% CI: 0.016, 0.092)] and Gemini 1.5 Pro [0.034 (95% CI: 0.008, 0.070)]. Specificity was roughly equal between models, at [1.000 (95% CI: 1.000, 1.000)] for ChatGPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro, followed by Perplexity Sonar Large [0.941, (95% CI: 0.769, 1.000)], with no significant differences between models. F1 score was also significantly highest for ChatGPT-4o at 0.575 (95% CI: 0.477, 0.656), followed by the others [range: 0.065 (95% CI: 0.017, 0.128)&#8212;0.126 (95% CI: 0.050, 0.217)].</p></sec><sec id="Sec13"><title>Multiple-choice diagnosis (prompt 4)</title><p id="Par31">Prompt 4 requested the four MLLMs to give the most likely diagnosis, if any, based on the image from a multiple-choice list of 12 common retina conditions (age-related macular degeneration, central serous chorioretinopathy, diabetic retinopathy, epiretinal membrane, glaucoma, hypertensive retinopathy, macular edema, no apparent retinopathy, retinal detachment, retinal vascular occlusion, retinitis pigmentosa, or other pathology). This was again intended to test MLLM performance in a query unspecific to AMD, and to establish a baseline accuracy for a broader diagnostic use case. Accuracy was highest for ChatGPT-4o [0.691 (95% CI: 0.603, 0.757)], followed by Perplexity Sonar Large [0.603 (95% CI: 0.507, 0.676)], Gemini 1.5 Pro [0.331 (95% CI: 0.243, 0.404)], and Claude 3.5 Sonnet [0.272 (95% CI: 0.191, 0.346)]. Significant differences (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033) were present between the top two (ChatGPT-4o, Perplexity Sonar Large) and the bottom two (Claude 3.5 Sonnet, Gemini 1.5 Pro) MLLMs. F1 scores found the same significant differences, with ChatGPT-4o [0.788 (95% CI: 0.723, 0.846)] and Perplexity Sonar Large [0.707 (95%CI: 0.629, 0.776)] leading the others. Sensitivity was highest for ChatGPT-4o [0.655 (95% CI: 0.569, 0.735)], followed by Perplexity Sonar Large [0.546 (95% CI: 0.457, 0.629)], Gemini 1.5 Pro [0.244 (95% CI: 0.168, 0.325)] and Claude 3.5 Sonnet [0.168 (95% CI: 0.106, 0.239)], with significant differences (<italic toggle="yes">P</italic>&#8201;&lt;&#8201;0.00033) again present between the top two and bottom two models. Specificity was similar for all models, at [1.000 (95% CI: 1.000, 1.000)] for Claude 3.5 Sonnet and Perplexity Sonar Large, followed by [0.941 (95% CI: 0.779, 1.000)] for ChatGPT-4o and Gemini 1.5 Pro [0.941 (95% CI: 0.769, 1.000)], and with no significant differences between models.</p></sec><sec id="Sec14"><title>Error analysis</title><p id="Par32">Additional analyses looked at misclassifications and accuracy results stratified by ground truth diagnosis (Supplemental Table <xref rid="MOESM1" ref-type="media">3</xref>) to determine any patterns in MLLM misclassification of disease. Sample MLLM outputs are provided for reference in Supplemental Fig.&#160;<xref rid="MOESM1" ref-type="media">2</xref>. Overall, accuracy varied across prompts and models. &#8216;Early AMD&#8217; was detected with poor accuracy on average across all prompts (mean 19.8% all-model average), and &#8216;No AMD&#8217; was more accurately identified (mean 72.8% all-model average). For severity classification (Prompt 2), &#8216;No AMD&#8217; was poorly identified (mean 14.7% all-model average). &#8216;Intermediate AMD&#8217; and &#8216;Advanced AMD&#8217; were identified with high variation between prompts and models.</p><p id="Par33">An inter-model agreement analysis was also performed alongside confusion matrices (Supplemental Table <xref rid="MOESM1" ref-type="media">4</xref>). For Prompt 1, agreement was highest between the two top performing models, ChatGPT-4o and Perplexity Sonar Large (80.1%), with other models showing variable agreement (39.0% to 64.0%). For Prompt 2, models exhibited a shared bias toward overclassifying &#8216;Intermediate AMD&#8217;, which contributed to elevated inter-model agreement. Prompt 3 showed high overall pairwise inter-model agreement (81.6%), driven by frequent overclassification of non-AMD pathologies for this open-ended diagnosis prompt. Prompt 4 demonstrated moderate overall pairwise agreement (65.1%), with the highest agreement again between ChatGPT-4o and Perplexity Sonar Large (79.4%). Models again heavily favored non-AMD classifications in this multiple-choice prompt.</p></sec></sec><sec id="Sec15"><title>Discussion</title><p id="Par34">The present study evaluated and compared the capabilities of four popular MLLMs in detecting AMD and grading the severity of disease from UWF fundus images. While other deep learning algorithms have been deployed in similar contexts in this and other ophthalmic diseases, the accuracy of various MLLMs for this application remains unclear. We found that results varied substantially between MLLMs, emphasizing that models cannot be interchanged with the expectation of similar results. Consistent with prompt engineering concepts<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, question type and framing also strongly influenced outcomes. Overall, MLLMs showed some promise in assessing images for AMD, particularly in binary disease classification. However, the levels of performance we observed suggest that these technologies are not yet ready for safe clinical implementation.</p><p id="Par35">The best performance was seen in binary disease classification (Prompt 1), with accuracies and F1 scores exceeding 80% for ChatGPT-4o and Perplexity Sonar Large. These results (ChatGPT-4o sensitivity/specificity: 0.832/0.765; Perplexity Sonar Large sensitivity/specificity: 0.815/0.824) are slightly below, but comparable to, those achieved by optometrists (84.5% sensitive, 88.0% specific) classifying fundus images for the presence or absence of AMD<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Notably, optometrists play an essential role in screening for asymptomatic AMD<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.</p><p id="Par36">Compared to physician accuracy in grading AMD severity (75.8%) with a 4-class AREDS scale<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> (Prompt 2), MLLMs fell short, failing to attain greater than 46.3% accuracy, and only poor F1 scores (range: 0.409&#8211;0.509). Together with the results of Prompt 1, this suggests MLLMs may be nearer to clinical readiness for use cases like AMD screening, rather than for precision diagnosis and monitoring of disease severity. It is worth noting that relatively low sample sizes for the &#8216;Early AMD&#8217; and &#8216;No AMD&#8217; groups may have limited the power of this comparison.</p><p id="Par37">Prompts 3 and 4 evaluated the capabilities of MLLMs as nonspecific diagnostic tools asked to make a diagnosis based on fundus imaging alone. With accuracies ranging from 0.154 to 0.691, reliability was both inconsistent and likely insufficient for this clinical use case. ChatGPT-4o&#8217;s ability, However, to accurately diagnose AMD 69.1% of the time from a fundus image alone [F1 score of 0.788 (0.723, 0.846)], without prompting specific to AMD, is promising at this relatively early stage in the technology&#8217;s development.</p><p id="Par38">In comparing models, ChatGPT-4o demonstrated significantly better performance Than its counterparts in 3 of 4 prompts, with Perplexity Sonar Large outperforming Claude 3.5 Sonnet and Gemini 1.5 Pro in 2 of 4. This suggests that at least the tested models of Claude and Gemini may lag in accuracy. However, as this study demonstrated, results can vary widely between prompts, and further experimentation with additional prompts and in other contexts would help to identify a clear MLLM leader in the space of ophthalmic imaging analysis.</p><p id="Par39">Differences in model architectures or training data could be responsible for variations in performance. However, it is difficult to determine the specific driving factors behind our results for several reasons. First, the performance of MLLMs varies notably depending on the prompt and the specific medical imaging task, as demonstrated in this study and other previous literature. For example, ChatGPT-4o outperformed Gemini 1.5 Pro in the present study, as well as in a separate study focused on diagnosing corneal disease where ChatGPT-4o achieved the highest accuracy, followed by Claude 3.5 Sonnet and then Gemini 1.5 Flash<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. However, the opposite was observed in a recent investigation where MLLMs interpreted magnetic resonance brain imaging<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, and another study evaluating MLLM responses to ocular inflammation questions using patient imaging found no significant performance difference between ChatGPT-4o and Gemini 1.5 Pro<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. Secondly, there is currently a lack of publicly available data regarding many MLLM technical specifications and training data, as companies such as OpenAI and Google move away from open-source models and public disclosure around training data<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>. This makes fully informed comparisons difficult. However, we could speculate that the superior performances of ChatGPT-4o and Perplexity Sonar Large observed in this study could be due to the use of training data better suited to this particular context, or perhaps certain architectural differences. ChatGPT-4o has been noted to use particularly vast and diverse datasets<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, and Perplexity models employ retrieval-augmented generation (RAG) with live information retrieval<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, which may enhance the LLM&#8217;s accuracy when new data becomes available online after the training cutoffs of other models. Gemini 1.5 Pro is a mid-sized model optimized for flexible scaling across a wide range of contexts<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>, which could limit performance in our very specific medical task of interest. Several studies have also found Gemini to underperform in visual analysis tasks, highlighting a possible relative weakness in interpreting complex medical imagery<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. Claude 3.5 Sonnet incorporates post-training refinement through human feedback, where reviewers help align model outputs to prioritize quality and safety, often resulting in more conservative results<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. This is consistent with our results, where Claude 3.5 Sonnet was found to be more specific at the expense of low sensitivity. This was also the case with Gemini 1.5 Pro, suggesting these models may have favored more conservative approaches overall. Identifying the specific technical factors driving performance differences we observed is challenging. However, further comparisons across diverse ophthalmic imaging settings may help to clarify any broader trends.</p><p id="Par40">Error analysis revealed accuracy differences across ground truth AMD severities. Average accuracy was higher for &#8216;No AMD&#8217; eyes and lowest for &#8216;Early AMD&#8217; eyes, with results for intermediate and advanced cases falling somewhere between. This could suggest that MLLMs recognize healthy eyes more reliably, and perhaps are more sensitive to certain high-risk features such as neovascularization and geographic atrophy found in advanced stages. Poor recognition of &#8216;Early AMD&#8217; eyes, particularly in Prompts 3 and 4, was reasonably expected, as this least severe form of AMD has the least obvious pathological features. Inter-model agreement analysis with confusion matrices was performed to evaluate whether inherent case difficulty might be contributing to misclassifications, or if errors were more driven by model limitations. The high agreement between top-performing models (ChatGPT-4o and Perplexity Sonar Large, 80.1%) in Prompt 1 suggested that lower-performing models may have suffered from limitations handling the task, rather Than inherent case difficulty. In Prompt 2, we observed shared biases across MLLMs with overclassification of &#8216;Intermediate AMD&#8217;. A pattern bias such as this might suggest imbalances in training data, as deep learning models can become biased toward the majority of data<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. While it is difficult to determine what may constitute the majority in the training data, this would be consistent with the relatively higher prevalence of early (including &#8216;Intermediate AMD&#8217;) vs. late-stage AMD in the US population<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Overclassifications of non-AMD pathologies were observed in Prompt 3, with relatively high agreement between models. This was likely related to higher task difficulty, but importantly highlights reduced MLLM performance when prompts did not drive at analysis for a particular pathology of interest, as Prompts 1 and 2 did. Prompt 4 again demonstrated relatively high agreement between the top performing models (ChatGPT-4o and Perplexity Sonar Large, 79.4%), as well as overclassification of non-AMD eyes and agreement between MLLMs in these negative responses. Model limitations may have hindered lower performing models, which did relatively worse with this higher difficulty task. Overall, it is difficult to determine the precise reasons behind error trends in a single study and use case. Additional studies focusing on larger MLLM error trends in ophthalmic imaging may help to confirm these hypotheses and uncover additional driving forces.</p><p id="Par41">Taken together, results from this study suggest these MLLMs are not currently ready for clinical use in AMD detection. However, results for binary disease detection may approach those of human graders. Furthermore, these are early models that are expected to improve quickly with further training and have already done so recently<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Research using other deep-learning AI algorithms has shown performance that is on par with human graders for grading disease severity<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, and as high as 98% accuracy in detecting referrable (intermediate to advanced) vs. non-referrable (none to early) AMD from color fundus photos<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. This demonstrates the potentially high performance achievable by an AI screening tool for automated detection. However, these tools are often limited as disease-specific algorithms, trained on data specific to AMD to recognize a single disease<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. This likely increases accuracy, but also narrows the scope of possible applications for the tool.</p><p id="Par42">MLLMs have the distinct advantage of being generalized tools trained on a much broader set of data<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. They are able to respond to myriad unique queries, and therefore may be able to simultaneously assist ophthalmologists through many different applications<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. For example, while screening for AMD in a patient, MLLMs could also assess the patient&#8217;s fundus imaging for glaucoma<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, as well as analyze OCT and other imaging modalities to check for numerous other pathologies<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Following the visit, the MLLM could generate an after-visit summary<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and answer patient questions regarding their diagnosis with relatively high accuracy<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. MLLMs can also process multiple types of information (i.e. visual and textual) simultaneously, allowing for a potentially deeper and more holistic understanding of a patient&#8217;s diagnosis, prognosis, or best treatment options<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. As another example, when analyzing a fundus image an MLLM could simultaneously consider relevant patient history from clinical notes that might influence the interpretation of ambiguous findings. Other advantages include the potential for users to converse with the technology, ask follow-up questions, ask for detailed explanations, or to use the MLLM as an educational tool while in training<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. MLLMs may also be cost-effective in comparison to other deep learning tools that are expensive to train for specific use cases<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Compared to other algorithms, a limited amount of MLLM fine-tuning can significantly improve accuracy, as they are trained on large amounts of data at baseline.</p><p id="Par43">While MLLMs will potentially be highly useful, careful consideration of the safety and ethical implications of deploying them to screen for and monitor ophthalmic disease is critical. Performance may improve with time, but even a small number of errors or hallucinations can cause substantial harm to patients<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Diligent auditing of MLLM outputs and provided reasoning behind answers may mitigate risks to some degree, but the &#8220;black box&#8221; nature of these and other AI tools could make complete understanding difficult. These tools may also learn from our own biases if present in training data, which should include diverse and representative datasets<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. For example, in this study MLLMs had not necessarily been trained on unbiased datasets, but likely images available on the internet. This could reduce the applicability of results for certain populations. Further research in this area will be necessary prior to real-world implementation. Patient privacy will also need to be prioritized and protected. While this study used only anonymized data, expanded use cases of MLLMs that may include medical records or other identifying information would necessitate the establishment of clear guidelines and regulations<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><p id="Par44">This study was limited by its retrospective nature with a relatively small number of cases for some disease severity levels. Images were from a single academic center, and exclusion criteria may have contributed some degree of selection bias. The intended end users (clinicians) for our use case would need to screen patient types that were excluded in this study, including those with poor image quality and complex ocular histories. However, strict exclusion criteria applied alongside anonymization were used to maintain data integrity. Observer bias was mitigated through the utilization of trained retinal specialist graders, who independently graded images, with any disagreements adjudicated by a senior retina specialist. Consensus grading using standardized AREDS criteria was applied to reduce subjective variability and increase reliability, though the influence of subjectivity, particularly in borderline cases, remains. A single camera type was used to obtain images, and only UWF imaging was provided to MLLMs to establish baseline performance using this single commonly used modality. Clinical diagnosis typically benefits from additional patient history and imaging, and imaging is often not graded using published grading criteria in the clinical setting. This is a potential source of information bias that could limit clinical generalizability of our findings to some degree. As UWF imaging is one of the most cost-effective modalities to screen for retinal disorders<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, however, this single-modality application may help elucidate minimal requirements for accurate diagnosis and/or diagnosis in lower-resource settings. Finally, some of the MLLM models studied here have been replaced with updated versions, limiting reproducibility of the work.</p><p id="Par45">Further studies around using MLLMs for AMD can widen the scope of automated disease detection to include disease prognosis and even treatment recommendations. Multimodal imaging inputs such as OCT, OCT angiography, and fluorescein angiography could also be analyzed in tandem along with demographics and history to see if MLLMs achieve higher accuracy<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. Studies assessing MLLM detection of particular prognostic features, such as reticular pseudodrusen on OCT<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, would provide further information on the diagnostic utility of MLLMs, and identify targets for fine-tuning. Additionally, given the treatment implications for wet versus dry AMD, future studies assessing the abilities of MLLMs to distinguish between nAMD and geographic atrophy could provide further insights into clinical utility. Greater accuracy may also be achievable with different prompts, few-shot learning<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, or by training these MLLMs for the specific purpose of AMD image analysis<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Lastly, requesting MLLMs to include probability estimates along with answers may be useful to calculate receiver operating characteristic areas under the curve (AUC-ROC), and to stratify accuracy results by relative confidence expressed by the algorithm.</p><p id="Par46">In conclusion, the present study represents a preliminary evaluation of four leading MLLMs in fundus image-based detection and grading of AMD severity. MLLMs show promise as emerging technologies that may one day accurately and efficiently evaluate images for this disease, thereby supporting screening and monitoring efforts while protecting resources. However, the results of this analysis suggest that these MLLMs could not be safely and reliably implemented in clinical settings in their present forms. As the field of ophthalmology continues to explore clinical adoption of AI technologies, rigorous and ongoing validation of these and prospective models is recommended to protect high standards of safety and patient care.</p></sec><sec id="Sec16" sec-type="supplementary-material"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1" position="float" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="41598_2025_18306_MOESM1_ESM.pdf" position="float" orientation="portrait"><caption><p>Supplementary Information.</p></caption></media></supplementary-material>
</p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>AMD</term><def><p id="Par2">Age-related macular degeneration</p></def></def-item><def-item><term>nAMD</term><def><p id="Par3">Neovascular age-related macular degeneration</p></def></def-item><def-item><term>OCT</term><def><p id="Par4">Optical coherence tomography</p></def></def-item><def-item><term>AI</term><def><p id="Par5">Artificial intelligence</p></def></def-item><def-item><term>DL</term><def><p id="Par6">Deep learning</p></def></def-item><def-item><term>LLMs</term><def><p id="Par7">Large language models</p></def></def-item><def-item><term>MLLMs</term><def><p id="Par8">Multimodal large language models</p></def></def-item><def-item><term>UWF</term><def><p id="Par9">Ultrawide field</p></def></def-item><def-item><term>UCSD</term><def><p id="Par10">University of California, San Diego</p></def></def-item><def-item><term>AREDS</term><def><p id="Par11">Age-Related Eye Disease Study</p></def></def-item><def-item><term>MC</term><def><p id="Par12">Multiple-choice</p></def></def-item><def-item><term>CI</term><def><p id="Par13">Confidence interval</p></def></def-item><def-item><term>RAG</term><def><p id="Par14">Retrieval-augmented generation</p></def></def-item><def-item><term>AUC-ROC</term><def><p id="Par15">Receiver operating characteristic areas under the curve</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>JM &#8211; conception, design, data acquisition, data analysis, drafted manuscript GF &#8211; data acquisition, drafted manuscript EW &#8211; data analysis IN &#8211; data acquisition NM &#8211; data acquisition EF &#8211; data acquisition SB &#8211; conception, design, revision of manuscript. All authors reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All data regarding this report can be accessed with permission (requests can be directed to JM or SB), and we take full responsibility for the integrity of the data and its analysis.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par47">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bhuiyan</surname><given-names>A</given-names></name><name name-style="western"><surname>Wong</surname><given-names>TY</given-names></name><name name-style="western"><surname>Ting</surname><given-names>DSW</given-names></name><name name-style="western"><surname>Govindaiah</surname><given-names>A</given-names></name><name name-style="western"><surname>Souied</surname><given-names>EH</given-names></name><name name-style="western"><surname>Smith</surname><given-names>RT</given-names></name></person-group><article-title>Artificial intelligence to stratify severity of age-related macular degeneration (AMD) and predict risk of progression to late AMD</article-title><source>Transl. Vis. Sci. Technol.</source><year>2020</year><volume>9</volume><issue>2</issue><fpage>25</fpage><pub-id pub-id-type="doi">10.1167/tvst.9.2.25</pub-id><pub-id pub-id-type="pmid">32818086</pub-id><pub-id pub-id-type="pmcid">PMC7396183</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Bhuiyan, A. et al. Artificial intelligence to stratify severity of age-related macular degeneration (AMD) and predict risk of progression to late AMD. <italic toggle="yes">Transl. Vis. Sci. Technol.</italic><bold>9</bold>(2), 25. 10.1167/tvst.9.2.25 (2020).<pub-id pub-id-type="pmid">32818086</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1167/tvst.9.2.25</pub-id><pub-id pub-id-type="pmcid">PMC7396183</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Keenan</surname><given-names>TDL</given-names></name><name name-style="western"><surname>Cukras</surname><given-names>CA</given-names></name><name name-style="western"><surname>Chew</surname><given-names>EY</given-names></name></person-group><article-title>Age-related macular degeneration: Epidemiology and clinical aspects</article-title><source>Adv. Exp. Med. Biol.</source><year>2021</year><volume>1256</volume><fpage>1</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-66014-7_1</pub-id><pub-id pub-id-type="pmid">33847996</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Keenan, T. D. L., Cukras, C. A. &amp; Chew, E. Y. Age-related macular degeneration: Epidemiology and clinical aspects. <italic toggle="yes">Adv. Exp. Med. Biol.</italic><bold>1256</bold>, 1&#8211;31. 10.1007/978-3-030-66014-7_1 (2021).<pub-id pub-id-type="pmid">33847996</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/978-3-030-66014-7_1</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>H</given-names></name><name name-style="western"><surname>Li</surname><given-names>F</given-names></name><name name-style="western"><surname>Fu</surname><given-names>H</given-names></name><etal/></person-group><article-title>ADAM challenge: Detecting age-related macular degeneration from fundus images</article-title><source>IEEE Trans. Med. Imaging</source><year>2022</year><volume>41</volume><issue>10</issue><fpage>2828</fpage><lpage>2847</lpage><pub-id pub-id-type="doi">10.1109/TMI.2022.3172773</pub-id><pub-id pub-id-type="pmid">35507621</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Fang, H. et al. ADAM challenge: Detecting age-related macular degeneration from fundus images. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>41</bold>(10), 2828&#8211;2847. 10.1109/TMI.2022.3172773 (2022).<pub-id pub-id-type="pmid">35507621</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2022.3172773</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>C</given-names></name><name name-style="western"><surname>Lo</surname><given-names>JE</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H</given-names></name><etal/></person-group><article-title>Artificial intelligence for diagnosing exudative age-related macular degeneration</article-title><source>Cochrane Database Syst. Rev.</source><year>2024</year><volume>10</volume><issue>10</issue><fpage>CD015522</fpage><pub-id pub-id-type="doi">10.1002/14651858.CD015522.pub2</pub-id><pub-id pub-id-type="pmid">39417312</pub-id><pub-id pub-id-type="pmcid">PMC11483348</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Kang, C. et al. Artificial intelligence for diagnosing exudative age-related macular degeneration. <italic toggle="yes">Cochrane Database Syst. Rev.</italic><bold>10</bold>(10), CD015522. 10.1002/14651858.CD015522.pub2 (2024).<pub-id pub-id-type="pmid">39417312</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/14651858.CD015522.pub2</pub-id><pub-id pub-id-type="pmcid">PMC11483348</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jalili</surname><given-names>J</given-names></name><name name-style="western"><surname>Jiravarnsirikul</surname><given-names>A</given-names></name><name name-style="western"><surname>Bowd</surname><given-names>C</given-names></name><etal/></person-group><article-title>Glaucoma detection and feature identification via GPT-4V fundus image analysis</article-title><source>Ophthalmol. Sci.</source><year>2025</year><volume>5</volume><issue>2</issue><fpage>100667</fpage><pub-id pub-id-type="doi">10.1016/j.xops.2024.100667</pub-id><pub-id pub-id-type="pmid">39877464</pub-id><pub-id pub-id-type="pmcid">PMC11773068</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Jalili, J. et al. Glaucoma detection and feature identification via GPT-4V fundus image analysis. <italic toggle="yes">Ophthalmol. Sci.</italic><bold>5</bold>(2), 100667. 10.1016/j.xops.2024.100667 (2025).<pub-id pub-id-type="pmid">39877464</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.xops.2024.100667</pub-id><pub-id pub-id-type="pmcid">PMC11773068</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Crincoli</surname><given-names>E</given-names></name><name name-style="western"><surname>Sacconi</surname><given-names>R</given-names></name><name name-style="western"><surname>Querques</surname><given-names>L</given-names></name><name name-style="western"><surname>Querques</surname><given-names>G</given-names></name></person-group><article-title>Artificial intelligence in age-related macular degeneration: State of the art and recent updates</article-title><source>BMC Ophthalmol.</source><year>2024</year><volume>24</volume><fpage>121</fpage><pub-id pub-id-type="doi">10.1186/s12886-024-03381-1</pub-id><pub-id pub-id-type="pmid">38491380</pub-id><pub-id pub-id-type="pmcid">PMC10943791</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Crincoli, E., Sacconi, R., Querques, L. &amp; Querques, G. Artificial intelligence in age-related macular degeneration: State of the art and recent updates. <italic toggle="yes">BMC Ophthalmol.</italic><bold>24</bold>, 121. 10.1186/s12886-024-03381-1 (2024).<pub-id pub-id-type="pmid">38491380</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s12886-024-03381-1</pub-id><pub-id pub-id-type="pmcid">PMC10943791</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Q</given-names></name><name name-style="western"><surname>Keenan</surname><given-names>TDL</given-names></name><name name-style="western"><surname>Allot</surname><given-names>A</given-names></name><etal/></person-group><article-title>Multimodal, multitask, multiattention (M3) deep learning detection of reticular pseudodrusen: Toward automated and accessible classification of age-related macular degeneration</article-title><source>J. Am. Med. Inform. Assoc.</source><year>2021</year><volume>28</volume><issue>6</issue><fpage>1135</fpage><lpage>1148</lpage><pub-id pub-id-type="doi">10.1093/jamia/ocaa302</pub-id><pub-id pub-id-type="pmid">33792724</pub-id><pub-id pub-id-type="pmcid">PMC8200273</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Chen, Q. et al. Multimodal, multitask, multiattention (M3) deep learning detection of reticular pseudodrusen: Toward automated and accessible classification of age-related macular degeneration. <italic toggle="yes">J. Am. Med. Inform. Assoc.</italic><bold>28</bold>(6), 1135&#8211;1148. 10.1093/jamia/ocaa302 (2021).<pub-id pub-id-type="pmid">33792724</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/jamia/ocaa302</pub-id><pub-id pub-id-type="pmcid">PMC8200273</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Tian</surname><given-names>D</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X</given-names></name><etal/></person-group><article-title>Evolutionary patterns and research frontiers of artificial intelligence in age-related macular degeneration: A bibliometric analysis</article-title><source>Quant. Imaging Med. Surg.</source><year>2025</year><volume>15</volume><issue>1</issue><fpage>813</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.21037/qims-24-1406</pub-id><pub-id pub-id-type="pmid">39839014</pub-id><pub-id pub-id-type="pmcid">PMC11744182</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Yang, Z. et al. Evolutionary patterns and research frontiers of artificial intelligence in age-related macular degeneration: A bibliometric analysis. <italic toggle="yes">Quant. Imaging Med. Surg.</italic><bold>15</bold>(1), 813&#8211;830. 10.21037/qims-24-1406 (2025).<pub-id pub-id-type="pmid">39839014</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.21037/qims-24-1406</pub-id><pub-id pub-id-type="pmcid">PMC11744182</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chia</surname><given-names>MA</given-names></name><name name-style="western"><surname>Antaki</surname><given-names>F</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y</given-names></name><name name-style="western"><surname>Turner</surname><given-names>AW</given-names></name><name name-style="western"><surname>Lee</surname><given-names>AY</given-names></name><name name-style="western"><surname>Keane</surname><given-names>PA</given-names></name></person-group><article-title>Foundation models in ophthalmology</article-title><source>Br. J. Ophthalmol.</source><year>2024</year><volume>108</volume><issue>10</issue><fpage>1341</fpage><lpage>1348</lpage><pub-id pub-id-type="doi">10.1136/bjo-2024-325459</pub-id><pub-id pub-id-type="pmid">38834291</pub-id><pub-id pub-id-type="pmcid">PMC11503093</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Chia, M. A. et al. Foundation models in ophthalmology. <italic toggle="yes">Br. J. Ophthalmol.</italic><bold>108</bold>(10), 1341&#8211;1348. 10.1136/bjo-2024-325459 (2024).<pub-id pub-id-type="pmid">38834291</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1136/bjo-2024-325459</pub-id><pub-id pub-id-type="pmcid">PMC11503093</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Agnihotri</surname><given-names>AP</given-names></name><name name-style="western"><surname>Nagel</surname><given-names>ID</given-names></name><name name-style="western"><surname>Artiaga</surname><given-names>JCM</given-names></name><name name-style="western"><surname>Guevarra</surname><given-names>MCB</given-names></name><name name-style="western"><surname>Sosuan</surname><given-names>GMN</given-names></name><name name-style="western"><surname>Kalaw</surname><given-names>FGP</given-names></name></person-group><article-title>Large language models in ophthalmology: A review of publications from top ophthalmology journals</article-title><source>Ophthalmol. Sci.</source><year>2024</year><pub-id pub-id-type="doi">10.1016/j.xops.2024.100681</pub-id><pub-id pub-id-type="pmid">40114712</pub-id><pub-id pub-id-type="pmcid">PMC11925577</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Agnihotri, A. P. et al. Large language models in ophthalmology: A review of publications from top ophthalmology journals. <italic toggle="yes">Ophthalmol. Sci.</italic>10.1016/j.xops.2024.100681 (2024).<pub-id pub-id-type="pmid">40114712</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.xops.2024.100681</pub-id><pub-id pub-id-type="pmcid">PMC11925577</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><collab>Age-Related Eye Disease Study Research Group</collab></person-group><article-title>The age-related eye disease study severity scale for age-related macular degeneration: AREDS report No. 17</article-title><source>Arch. Ophthalmol.</source><year>2005</year><volume>123</volume><issue>11</issue><fpage>1484</fpage><lpage>1498</lpage><pub-id pub-id-type="doi">10.1001/archopht.123.11.1484</pub-id><pub-id pub-id-type="pmid">16286610</pub-id><pub-id pub-id-type="pmcid">PMC1472813</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Age-Related Eye Disease Study Research Group. The age-related eye disease study severity scale for age-related macular degeneration: AREDS report No. 17. <italic toggle="yes">Arch. Ophthalmol.</italic><bold>123</bold>(11), 1484&#8211;1498. 10.1001/archopht.123.11.1484 (2005).<pub-id pub-id-type="pmid">16286610</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/archopht.123.11.1484</pub-id><pub-id pub-id-type="pmcid">PMC1472813</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Flaxel</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Adelman</surname><given-names>RA</given-names></name><name name-style="western"><surname>Bailey</surname><given-names>ST</given-names></name><etal/></person-group><article-title>Age-related macular degeneration preferred practice pattern&#174;</article-title><source>Ophthalmology</source><year>2020</year><volume>127</volume><issue>1</issue><fpage>P1</fpage><lpage>P65</lpage><pub-id pub-id-type="doi">10.1016/j.ophtha.2019.09.024</pub-id><pub-id pub-id-type="pmid">31757502</pub-id></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Flaxel, C. J. et al. Age-related macular degeneration preferred practice pattern&#174;. <italic toggle="yes">Ophthalmology</italic><bold>127</bold>(1), P1&#8211;P65. 10.1016/j.ophtha.2019.09.024 (2020).<pub-id pub-id-type="pmid">31757502</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ophtha.2019.09.024</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>JH</given-names></name><name name-style="western"><surname>Nishida</surname><given-names>T</given-names></name><name name-style="western"><surname>Moghimi</surname><given-names>S</given-names></name><name name-style="western"><surname>Weinreb</surname><given-names>RN</given-names></name></person-group><article-title>Effects of prompt engineering on large language model performance in response to questions on common ophthalmic conditions</article-title><source>Taiwan J. Ophthalmol.</source><year>2024</year><volume>14</volume><issue>3</issue><fpage>454</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.4103/tjo.TJO-D-23-00193</pub-id><pub-id pub-id-type="pmid">39430350</pub-id><pub-id pub-id-type="pmcid">PMC11488800</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Wu, J. H., Nishida, T., Moghimi, S. &amp; Weinreb, R. N. Effects of prompt engineering on large language model performance in response to questions on common ophthalmic conditions. <italic toggle="yes">Taiwan J. Ophthalmol.</italic><bold>14</bold>(3), 454&#8211;457. 10.4103/tjo.TJO-D-23-00193 (2024).<pub-id pub-id-type="pmid">39430350</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.4103/tjo.TJO-D-23-00193</pub-id><pub-id pub-id-type="pmcid">PMC11488800</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>S</given-names></name><name name-style="western"><surname>Doig</surname><given-names>GS</given-names></name><name name-style="western"><surname>Ly</surname><given-names>A</given-names></name></person-group><article-title>Diagnostic accuracy of community optometrists for age-related macular degeneration using colour fundus photographs: A pilot evaluation</article-title><source>Ophthalmic Physiol. Opt.</source><year>2024</year><volume>44</volume><issue>1</issue><fpage>17</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1111/opo.13242</pub-id><pub-id pub-id-type="pmid">37921119</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Ho, S., Doig, G. S. &amp; Ly, A. Diagnostic accuracy of community optometrists for age-related macular degeneration using colour fundus photographs: A pilot evaluation. <italic toggle="yes">Ophthalmic Physiol. Opt.</italic><bold>44</bold>(1), 17&#8211;22. 10.1111/opo.13242 (2024).<pub-id pub-id-type="pmid">37921119</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/opo.13242</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Solebo</surname><given-names>AL</given-names></name><name name-style="western"><surname>Angunawela</surname><given-names>RI</given-names></name><name name-style="western"><surname>Dasgupta</surname><given-names>S</given-names></name><name name-style="western"><surname>Marshall</surname><given-names>J</given-names></name></person-group><article-title>Recent advances in the treatment of age-related macular degeneration</article-title><source>Br. J. Gen. Pract.</source><year>2008</year><volume>58</volume><issue>550</issue><fpage>309</fpage><pub-id pub-id-type="doi">10.3399/096016408784217395</pub-id><pub-id pub-id-type="pmid">18482482</pub-id><pub-id pub-id-type="pmcid">PMC2435664</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Solebo, A. L., Angunawela, R. I., Dasgupta, S. &amp; Marshall, J. Recent advances in the treatment of age-related macular degeneration. <italic toggle="yes">Br. J. Gen. Pract.</italic><bold>58</bold>(550), 309. 10.3399/096016408784217395 (2008).<pub-id pub-id-type="pmid">18482482</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3399/096016408784217395</pub-id><pub-id pub-id-type="pmcid">PMC2435664</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burlina</surname><given-names>P</given-names></name><name name-style="western"><surname>Pacheco</surname><given-names>KD</given-names></name><name name-style="western"><surname>Joshi</surname><given-names>N</given-names></name><name name-style="western"><surname>Freund</surname><given-names>DE</given-names></name><name name-style="western"><surname>Bressler</surname><given-names>NM</given-names></name></person-group><article-title>Comparing humans and deep learning performance for grading AMD: A study in using universal deep features and transfer learning for automated AMD analysis</article-title><source>Comput. Biol. Med.</source><year>2017</year><volume>82</volume><fpage>80</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.01.018</pub-id><pub-id pub-id-type="pmid">28167406</pub-id><pub-id pub-id-type="pmcid">PMC5373654</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Burlina, P., Pacheco, K. D., Joshi, N., Freund, D. E. &amp; Bressler, N. M. Comparing humans and deep learning performance for grading AMD: A study in using universal deep features and transfer learning for automated AMD analysis. <italic toggle="yes">Comput. Biol. Med.</italic><bold>82</bold>, 80&#8211;86. 10.1016/j.compbiomed.2017.01.018 (2017).<pub-id pub-id-type="pmid">28167406</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.compbiomed.2017.01.018</pub-id><pub-id pub-id-type="pmcid">PMC5373654</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiao</surname><given-names>C</given-names></name><name name-style="western"><surname>Rosas</surname><given-names>E</given-names></name><name name-style="western"><surname>Asadigandomani</surname><given-names>H</given-names></name><etal/></person-group><article-title>Diagnostic performance of publicly available large language models in corneal diseases: A comparison with human specialists</article-title><source>Diagnostics (Basel)</source><year>2025</year><volume>15</volume><issue>10</issue><fpage>1221</fpage><pub-id pub-id-type="doi">10.3390/diagnostics15101221</pub-id><pub-id pub-id-type="pmid">40428214</pub-id><pub-id pub-id-type="pmcid">PMC12110359</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Jiao, C. et al. Diagnostic performance of publicly available large language models in corneal diseases: A comparison with human specialists. <italic toggle="yes">Diagnostics (Basel)</italic><bold>15</bold>(10), 1221. 10.3390/diagnostics15101221 (2025).<pub-id pub-id-type="pmid">40428214</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/diagnostics15101221</pub-id><pub-id pub-id-type="pmcid">PMC12110359</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Than JCM, Vong WT, Yong KSC. Comparison of Multi-Modal Large Language Models with Deep Learning Models for Medical Image Classification. In: <italic toggle="yes">2024 IEEE 8th International Conference on Signal and Image Processing Applications (ICSIPA)</italic>. ; 2024:1&#8211;5. 10.1109/ICSIPA62061.2024.10687159</mixed-citation></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Demir</surname><given-names>S</given-names></name></person-group><article-title>Comparison of ChatGPT-4o, Google Gemini 1.5 Pro, Microsoft Copilot Pro, and Ophthalmologists in the management of uveitis and ocular inflammation: A comparative study of large language models</article-title><source>J. Fran&#231;ais d&#8217;Ophtalmologie</source><year>2025</year><volume>48</volume><issue>4</issue><fpage>104468</fpage><pub-id pub-id-type="doi">10.1016/j.jfo.2025.104468</pub-id><pub-id pub-id-type="pmid">40086266</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Demir, S. Comparison of ChatGPT-4o, Google Gemini 1.5 Pro, Microsoft Copilot Pro, and Ophthalmologists in the management of uveitis and ocular inflammation: A comparative study of large language models. <italic toggle="yes">J. Fran&#231;ais d&#8217;Ophtalmologie</italic><bold>48</bold>(4), 104468. 10.1016/j.jfo.2025.104468 (2025).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jfo.2025.104468</pub-id><pub-id pub-id-type="pmid">40086266</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Sapkota R, Raza S, Karkee M. 2025 Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models. Published online. 10.48550/arXiv.2502.18505</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Neuman WR, Coleman C, Shah M. Analyzing the Ethical Logic of Six Large Language Models. Published online January 15, 2025. 10.48550/arXiv.2501.08951</mixed-citation></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silhadi</surname><given-names>M</given-names></name><name name-style="western"><surname>Nassrallah</surname><given-names>WB</given-names></name><name name-style="western"><surname>Mikhail</surname><given-names>D</given-names></name><name name-style="western"><surname>Milad</surname><given-names>D</given-names></name><name name-style="western"><surname>Harissi-Dagher</surname><given-names>M</given-names></name></person-group><article-title>Assessing the performance of Microsoft Copilot, GPT-4 and Google Gemini in ophthalmology</article-title><source>Can. J. Ophthalmol.</source><year>2025</year><volume>60</volume><issue>4</issue><fpage>e507</fpage><lpage>e514</lpage><pub-id pub-id-type="doi">10.1016/j.jcjo.2025.01.001</pub-id><pub-id pub-id-type="pmid">39863285</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Silhadi, M., Nassrallah, W. B., Mikhail, D., Milad, D. &amp; Harissi-Dagher, M. Assessing the performance of Microsoft Copilot, GPT-4 and Google Gemini in ophthalmology. <italic toggle="yes">Can. J. Ophthalmol.</italic><bold>60</bold>(4), e507&#8211;e514. 10.1016/j.jcjo.2025.01.001 (2025).<pub-id pub-id-type="pmid">39863285</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jcjo.2025.01.001</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rajaraman</surname><given-names>S</given-names></name><name name-style="western"><surname>Ganesan</surname><given-names>P</given-names></name><name name-style="western"><surname>Antani</surname><given-names>S</given-names></name></person-group><article-title>Deep learning model calibration for improving performance in class-imbalanced medical image classification tasks</article-title><source>PLoS ONE</source><year>2022</year><volume>17</volume><issue>1</issue><fpage>e0262838</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0262838</pub-id><pub-id pub-id-type="pmid">35085334</pub-id><pub-id pub-id-type="pmcid">PMC8794113</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Rajaraman, S., Ganesan, P. &amp; Antani, S. Deep learning model calibration for improving performance in class-imbalanced medical image classification tasks. <italic toggle="yes">PLoS ONE</italic><bold>17</bold>(1), e0262838. 10.1371/journal.pone.0262838 (2022).<pub-id pub-id-type="pmid">35085334</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pone.0262838</pub-id><pub-id pub-id-type="pmcid">PMC8794113</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rein</surname><given-names>DB</given-names></name><name name-style="western"><surname>Wittenborn</surname><given-names>JS</given-names></name><name name-style="western"><surname>Burke-Conte</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Prevalence of Age-Related Macular Degeneration in the US in 2019</article-title><source>JAMA Ophthalmol.</source><year>2022</year><volume>140</volume><issue>12</issue><fpage>1202</fpage><lpage>1208</lpage><pub-id pub-id-type="doi">10.1001/jamaophthalmol.2022.4401</pub-id><pub-id pub-id-type="pmid">36326752</pub-id><pub-id pub-id-type="pmcid">PMC9634594</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Rein, D. B. et al. Prevalence of Age-Related Macular Degeneration in the US in 2019. <italic toggle="yes">JAMA Ophthalmol.</italic><bold>140</bold>(12), 1202&#8211;1208. 10.1001/jamaophthalmol.2022.4401 (2022).<pub-id pub-id-type="pmid">36326752</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamaophthalmol.2022.4401</pub-id><pub-id pub-id-type="pmcid">PMC9634594</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mihalache</surname><given-names>A</given-names></name><name name-style="western"><surname>Huang</surname><given-names>RS</given-names></name><name name-style="western"><surname>Patil</surname><given-names>NS</given-names></name><etal/></person-group><article-title>Chatbot and academy preferred practice pattern guidelines on retinal diseases</article-title><source>Ophthalmol. Retina.</source><year>2024</year><volume>8</volume><issue>7</issue><fpage>723</fpage><lpage>725</lpage><pub-id pub-id-type="doi">10.1016/j.oret.2024.03.013</pub-id><pub-id pub-id-type="pmid">38499086</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Mihalache, A. et al. Chatbot and academy preferred practice pattern guidelines on retinal diseases. <italic toggle="yes">Ophthalmol. Retina.</italic><bold>8</bold>(7), 723&#8211;725. 10.1016/j.oret.2024.03.013 (2024).<pub-id pub-id-type="pmid">38499086</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.oret.2024.03.013</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burlina</surname><given-names>PM</given-names></name><name name-style="western"><surname>Joshi</surname><given-names>N</given-names></name><name name-style="western"><surname>Pacheco</surname><given-names>KD</given-names></name><name name-style="western"><surname>Freund</surname><given-names>DE</given-names></name><name name-style="western"><surname>Kong</surname><given-names>J</given-names></name><name name-style="western"><surname>Bressler</surname><given-names>NM</given-names></name></person-group><article-title>Use of deep learning for detailed severity characterization and estimation of 5-year risk among patients with age-related macular degeneration</article-title><source>JAMA Ophthalmol.</source><year>2018</year><volume>136</volume><issue>12</issue><fpage>1359</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1001/jamaophthalmol.2018.4118</pub-id><pub-id pub-id-type="pmid">30242349</pub-id><pub-id pub-id-type="pmcid">PMC6583826</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Burlina, P. M. et al. Use of deep learning for detailed severity characterization and estimation of 5-year risk among patients with age-related macular degeneration. <italic toggle="yes">JAMA Ophthalmol.</italic><bold>136</bold>(12), 1359&#8211;1366. 10.1001/jamaophthalmol.2018.4118 (2018).<pub-id pub-id-type="pmid">30242349</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamaophthalmol.2018.4118</pub-id><pub-id pub-id-type="pmcid">PMC6583826</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>D</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>S</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L</given-names></name><name name-style="western"><surname>Lu</surname><given-names>X</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y</given-names></name></person-group><article-title>The role of large language models in medical image processing: A narrative review</article-title><source>Quant. Imaging Med. Surg.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>1108</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.21037/qims-23-892</pub-id><pub-id pub-id-type="pmid">38223123</pub-id><pub-id pub-id-type="pmcid">PMC10784029</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Tian, D., Jiang, S., Zhang, L., Lu, X. &amp; Xu, Y. The role of large language models in medical image processing: A narrative review. <italic toggle="yes">Quant. Imaging Med. Surg.</italic><bold>14</bold>(1), 1108&#8211;1121. 10.21037/qims-23-892 (2024).<pub-id pub-id-type="pmid">38223123</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.21037/qims-23-892</pub-id><pub-id pub-id-type="pmcid">PMC10784029</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mihalache</surname><given-names>A</given-names></name><name name-style="western"><surname>Huang</surname><given-names>RS</given-names></name><name name-style="western"><surname>Mikhail</surname><given-names>D</given-names></name><etal/></person-group><article-title>Interpretation of clinical retinal images using an artificial intelligence chatbot</article-title><source>Ophthalmol. Sci.</source><year>2024</year><volume>4</volume><issue>6</issue><fpage>100556</fpage><pub-id pub-id-type="doi">10.1016/j.xops.2024.100556</pub-id><pub-id pub-id-type="pmid">39139542</pub-id><pub-id pub-id-type="pmcid">PMC11321281</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Mihalache, A. et al. Interpretation of clinical retinal images using an artificial intelligence chatbot. <italic toggle="yes">Ophthalmol. Sci.</italic><bold>4</bold>(6), 100556. 10.1016/j.xops.2024.100556 (2024).<pub-id pub-id-type="pmid">39139542</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.xops.2024.100556</pub-id><pub-id pub-id-type="pmcid">PMC11321281</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bommakanti</surname><given-names>N</given-names></name><name name-style="western"><surname>Rizvi</surname><given-names>F</given-names></name><name name-style="western"><surname>Rizvi</surname><given-names>A</given-names></name><etal/></person-group><article-title>Accuracy and readability of after visit summaries for retinal conditions generated by a large language model</article-title><source>Invest. Ophthalmol. Vis. Sci.</source><year>2024</year><volume>65</volume><issue>7</issue><fpage>822</fpage></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Bommakanti, N. et al. Accuracy and readability of after visit summaries for retinal conditions generated by a large language model. <italic toggle="yes">Invest. Ophthalmol. Vis. Sci.</italic><bold>65</bold>(7), 822 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheong</surname><given-names>KX</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C</given-names></name><name name-style="western"><surname>Tan</surname><given-names>TE</given-names></name><etal/></person-group><article-title>Comparing generative and retrieval-based chatbots in answering patient questions regarding age-related macular degeneration and diabetic retinopathy</article-title><source>Br. J. Ophthalmol.</source><year>2024</year><volume>108</volume><issue>10</issue><fpage>1443</fpage><lpage>1449</lpage><pub-id pub-id-type="doi">10.1136/bjo-2023-324533</pub-id><pub-id pub-id-type="pmid">38749531</pub-id><pub-id pub-id-type="pmcid">PMC11716104</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Cheong, K. X. et al. Comparing generative and retrieval-based chatbots in answering patient questions regarding age-related macular degeneration and diabetic retinopathy. <italic toggle="yes">Br. J. Ophthalmol.</italic><bold>108</bold>(10), 1443&#8211;1449. 10.1136/bjo-2023-324533 (2024).<pub-id pub-id-type="pmid">38749531</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1136/bjo-2023-324533</pub-id><pub-id pub-id-type="pmcid">PMC11716104</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tailor</surname><given-names>PD</given-names></name><name name-style="western"><surname>Dalvin</surname><given-names>LA</given-names></name><name name-style="western"><surname>Chen</surname><given-names>JJ</given-names></name><etal/></person-group><article-title>A comparative study of responses to retina questions from either experts, expert-edited large language models, or expert-edited large language models alone</article-title><source>Ophthalmol. Sci.</source><year>2024</year><volume>4</volume><issue>4</issue><fpage>100485</fpage><pub-id pub-id-type="doi">10.1016/j.xops.2024.100485</pub-id><pub-id pub-id-type="pmid">38660460</pub-id><pub-id pub-id-type="pmcid">PMC11041826</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Tailor, P. D. et al. A comparative study of responses to retina questions from either experts, expert-edited large language models, or expert-edited large language models alone. <italic toggle="yes">Ophthalmol. Sci.</italic><bold>4</bold>(4), 100485. 10.1016/j.xops.2024.100485 (2024).<pub-id pub-id-type="pmid">38660460</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.xops.2024.100485</pub-id><pub-id pub-id-type="pmcid">PMC11041826</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ono</surname><given-names>D</given-names></name><name name-style="western"><surname>Dickson</surname><given-names>DW</given-names></name><name name-style="western"><surname>Koga</surname><given-names>S</given-names></name></person-group><article-title>Evaluating the efficacy of few-shot learning for GPT-4Vision in neurodegenerative disease histopathology: A comparative analysis with convolutional neural network model</article-title><source>Neuropathol. Appl. Neurobiol.</source><year>2024</year><volume>50</volume><issue>4</issue><fpage>e12997</fpage><pub-id pub-id-type="doi">10.1111/nan.12997</pub-id><pub-id pub-id-type="pmid">39010256</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Ono, D., Dickson, D. W. &amp; Koga, S. Evaluating the efficacy of few-shot learning for GPT-4Vision in neurodegenerative disease histopathology: A comparative analysis with convolutional neural network model. <italic toggle="yes">Neuropathol. Appl. Neurobiol.</italic><bold>50</bold>(4), e12997. 10.1111/nan.12997 (2024).<pub-id pub-id-type="pmid">39010256</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/nan.12997</pub-id></mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>