<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473383</article-id><article-id pub-id-type="pmcid-ver">PMC12473383.1</article-id><article-id pub-id-type="pmcaid">12473383</article-id><article-id pub-id-type="pmcaiid">12473383</article-id><article-id pub-id-type="pmid">41012967</article-id><article-id pub-id-type="doi">10.3390/s25185729</article-id><article-id pub-id-type="publisher-id">sensors-25-05729</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Hybrid MambaVision and Transformer-Based Architecture for 3D Lane Detection</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Cap</surname><given-names initials="RM">Raul-Mihai</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4445-8091</contrib-id><name name-style="western"><surname>Popa</surname><given-names initials="CA">C&#259;lin-Adrian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-05729" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Iclodean</surname><given-names initials="C">Calin</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Barab&#225;s</surname><given-names initials="I">Istv&#225;n</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Z&#246;ldy</surname><given-names initials="M">M&#225;t&#233;</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05729">Department of Computers and Information Technology, Politehnica University of Timi&#537;oara, 300223 Timi&#537;oara, Romania; <email>raul.cap@student.upt.ro</email></aff><author-notes><corresp id="c1-sensors-25-05729"><label>*</label>Correspondence: <email>calin.popa@cs.upt.ro</email></corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5729</elocation-id><history><date date-type="received"><day>29</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>26</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>14</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05729.pdf"/><abstract><p>Lane detection is an essential task in the field of computer vision and autonomous driving. This involves identifying and locating road markings on the road surface. This capability not only helps drivers keep the vehicle in the correct lane, but also provides critical data for advanced driver assistance systems and autonomous vehicles. Traditional lane detection models work mainly on the 2D image plane and achieve remarkable results. However, these models often assume a flat-world scenario, which does not correspond to real-world conditions, where roads have elevation variations and road markings may be curved. Our approach solves this challenge by focusing on 3D lane detection without relying on the inverse perspective mapping technique. Instead, we introduce a new framework using the MambaVision-S-1K backbone, which combines Mamba-based processing with Transformer capabilities to capture both local detail and global contexts from monocular images. This hybrid approach allows accurate modeling of lane geometry in three dimensions, even in the presence of elevation variations. By replacing the traditional convolutional neural network backbone with MambaVision, our proposed model significantly improves the capability of 3D lane detection systems. Our method achieved state-of-the-art performance on the ONCE-3DLanes dataset, thus demonstrating its superiority in accurately capturing lane curvature and elevation variations. These results highlight the potential of integrating advanced backbones based on Vision Transformers in the field of autonomous driving for more robust and reliable lane detection. The code will be available online.</p></abstract><kwd-group><kwd>3D lane detection</kwd><kwd>Mamba vision</kwd><kwd>transformer networks</kwd><kwd>deformable attention</kwd><kwd>autonomous driving</kwd></kwd-group><funding-group><award-group><funding-source>Politehnica University of Timi&#537;oara</funding-source></award-group><funding-statement>This research received no external funding. The APC was funded by Politehnica University of Timi&#537;oara.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05729"><title>1. Introduction</title><p>Detecting and pinpointing lanes on roads is an important task for computer vision and self-driving vehicles. Lane detection systems can help drivers stay in their lane and provide crucial information for advanced driver assistance systems and autonomous vehicles. Traditional lane detection methods use image processing techniques to recognize lane markings in camera images.&#160;Recently, deep learning has led to the creation of more precise and durable lane detection models based on convolutional neural networks (CNNs) and Transformer models. However, lane detection remains difficult due to varying road conditions, lighting conditions, camera perspectives, and&#160;the need for real-time&#160;performance.</p><p>There are two main approaches to lane detection: 2D and 3D models. Two-dimensional methods use CNNs such as ResNet&#160;[<xref rid="B1-sensors-25-05729" ref-type="bibr">1</xref>], EfficientNet&#160;[<xref rid="B2-sensors-25-05729" ref-type="bibr">2</xref>], and&#160;MobileNet&#160;[<xref rid="B3-sensors-25-05729" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05729" ref-type="bibr">4</xref>] to&#160;detect lane markings in monocular images.&#160;These models are fast and effective in ideal conditions, but&#160;may struggle with poor weather, lighting, or&#160;degraded lane markings.&#160;In&#160;contrast, 3D models estimate the position and orientation of lane markings in 3D space, offering improved accuracy and robustness.&#160;Although&#160;most 3D models use inverse perspective mapping (IPM) [<xref rid="B5-sensors-25-05729" ref-type="bibr">5</xref>] to create a bird&#8217;s-eye view (BEV), not all rely on this method. However, 3D models are generally more computationally intensive and depend on precise&#160;transformations.</p><p>Previous state-of-the-art methods hypothesized wrongly the construction of the BEV image, assuming that the lines have no height. This made the obtained image not accurate because they started from the premise that the world is flat. The&#160;authors of those works tried to determine the coordinates in the 3D space of the traffic lanes depending on the curvature of the lane lines. It is understandable that it was very difficult, nearly impossible, to&#160;generate a more accurate top-down view image because, at&#160;that time, there were no datasets in which the exact coordinates of the traffic lanes could be mapped in 3D space. At&#160;that time, there existed only synthetic datasets such as Apollo&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>], which were obtained from various game&#160;engines.</p><p>Developing a 3D lane detection dataset poses a significant challenge, primarily due to the requirement of precise and comprehensive ground truth annotations.&#160;Typically, this process involves manually labeling lane markings in a 3D space, which can be time-consuming and requires specialized equipment, such as LiDAR&#160;sensors.</p><p>Another challenge relates to generating a diverse and representative set of scenes that accurately capture a wide range of real-world scenarios, including various road types, weather, and&#160;lighting conditions. This undertaking requires careful planning, execution, and&#160;access to appropriate data collection&#160;sites.</p><p>Fortunately, we now have at our disposal three large datasets that contain a multitude of driving scenarios from real situations, whose annotations are very well detailed. The&#160;datasets to which we refer are OpenLane&#160;[<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>], OpenLaneV2&#160;[<xref rid="B8-sensors-25-05729" ref-type="bibr">8</xref>], and&#160;ONCE-3DLanes&#160;[<xref rid="B9-sensors-25-05729" ref-type="bibr">9</xref>]. One of the things that differentiates these datasets is the evaluation metrics.&#160;A well-constructed dataset plays a crucial role for lane detection tasks because, when annotations are inconsistent, training becomes noisy and slow, and&#160;the resulting model struggles to generalize.&#160;Imperfect ground truths such as missed or misaligned lane markings can degrade the performance of deep neural networks, which is supported by recent results from the PSSCL framework [<xref rid="B10-sensors-25-05729" ref-type="bibr">10</xref>]. Furthermore, careful selection of cleaner data during training significantly improves the robustness to such label noise.</p><p>Certainly, there are several methods that can work alongside those mentioned in order to obtain better results. These methods involve the inclusion of several sensors, such as RADAR or LiDAR, in&#160;contemplation of better understanding the surrounding environment by generating point clouds or depth maps. The&#160;main disadvantage of those methods is that, as&#160;we add more sensors, the&#160;computational cost increases drastically and&#160;it becomes more and more difficult and expensive to be used in real&#160;life.</p><p>Transformers are increasingly being used in computer vision applications such as image classification, object detection, and&#160;semantic segmentation&#160;[<xref rid="B11-sensors-25-05729" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05729" ref-type="bibr">12</xref>], as&#160;they are capable of capturing long-range dependencies and contextual information in the input data. Unlike CNNs, which process input data sequentially and extract features using local receptive fields, Transformers utilize a self-attention mechanism to compute global interactions between all input elements, thereby enabling them to capture both local and global contextual information. However, their computational requirements grow exponentially with sequence length, creating challenges for training and&#160;deployment.</p><p>For computer vision tasks, Transformer-based models such as Vision Transformers (ViT) [<xref rid="B13-sensors-25-05729" ref-type="bibr">13</xref>], DETR&#160;[<xref rid="B14-sensors-25-05729" ref-type="bibr">14</xref>], and&#160;SETR&#160;[<xref rid="B15-sensors-25-05729" ref-type="bibr">15</xref>] have demonstrated state-of-the-art performance on challenging datasets such as ImageNet&#160;[<xref rid="B16-sensors-25-05729" ref-type="bibr">16</xref>], COCO&#160;[<xref rid="B17-sensors-25-05729" ref-type="bibr">17</xref>], and&#160;Cityscapes&#160;[<xref rid="B18-sensors-25-05729" ref-type="bibr">18</xref>]. Transformers have several advantages over CNNs, including the ability to model long-range dependencies and context more effectively, less dependence on predefined architectures, and&#160;the ability to handle variable-sized inputs. However, with&#160;the recent introduction of Mamba Vision models, even better results have been achieved, surpassing the performance of ViTs in many computer vision tasks by combining the strengths of state-space models and Transformers for enhanced accuracy and&#160;efficiency.</p><p>The main objective of our research is to integrate the Transformer&#160;[<xref rid="B19-sensors-25-05729" ref-type="bibr">19</xref>] and Mamba Vision&#160;[<xref rid="B20-sensors-25-05729" ref-type="bibr">20</xref>] architectures into current state-of-the-art models and replace the classic convolutional neural networks in&#160;order to achieve better&#160;results.</p><p>The main contributions of the paper are as follows:<list list-type="bullet"><list-item><p>We propose a simple hybrid backbone that mixes MambaVision-S-1K CNN blocks with a Transformer, so the model learns both fine details and global context.</p></list-item><list-item><p>Our approach works directly on front-view images and skips any IPM or bird&#8217;s-eye view step.</p></list-item><list-item><p>We fine-tune and optimize the whole model so it can run well on systems which do not require the latest and best configuration.</p></list-item><list-item><p>We achieve state-of-the-art results on ONCE-3DLanes and also very good results on all subsets of the Apollo 3D Lane Detection benchmark.</p></list-item></list></p><p>The code will be available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/raul-cap/mamba-3d-lane-detection">https://github.com/raul-cap/mamba-3d-lane-detection</uri> (accessed on 29 July 2025).</p></sec><sec id="sec2-sensors-25-05729"><title>2. Related&#160;Work</title><p>A study on 3D-LaneNet&#160;[<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>] has applied two new concepts: an&#160;intra-network inverse perspective mapping that provides a dual representation information flow for both a&#160;regular image captured by the camera and its top-down view, and&#160;an anchor per column representation, which makes the lane detection task more like an object detection problem, replacing the common methods such as outlier rejection and clustering. This is the first work to make use of the top-view representation of the standard image to determine the lane in 3D&#160;space.</p><p>Being the first work to approach this topic, 3D-LaneNet inspired most of the works that followed, as&#160;well as the work entitled &#8220;Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection&#8221;&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>].&#160;With&#160;this paper, the&#160;authors noticed and tried to remove the drawbacks of 3D-LaneNet. One of the drawbacks is that the bird&#8217;s-eye projection does not align with the image feature resulting from IPM in the presence of a non-zero slope. Ref.&#160;[<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>] uses an unsuitable coordinate frame in its anchor representation, causing ground truth lanes to be misaligned with visual features. On&#160;the other hand, the&#160;authors proposed the so-called &#8220;virtual top view&#8221;, which aligns with the IPM image. Ref.&#160;[<xref rid="B22-sensors-25-05729" ref-type="bibr">22</xref>] is also an extension of the first state-of-the-art methods&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>,<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>]. The&#160;authors have introduced a new attention mechanism called Dual Attention. Its purpose is to help the model work better and improve accuracy in more complicated driving situations and scenarios. Besides&#160;this attention module, another change compared to the previous models is the use of the linear-interpolation loss function, which is used to locate the 3D lane markings with greater&#160;precision.</p><p>Although other previously presented works all had 3D-LaneNet as their starting point and each work represented an improvement of the basic one, in&#160;CLGo&#160;[<xref rid="B23-sensors-25-05729" ref-type="bibr">23</xref>], the&#160;authors applied a different strategy.&#160;Among&#160;the only similarities to the other works is that its architecture has two stages.&#160;However, the&#160;first stage is no longer used for image segmentation, but&#160;instead for finding the best camera pose and polynomial parameters using geometric constraints.&#160;In&#160;this case, finding the best camera pose is equivalent to finding the camera height and the pitch angle of the camera.&#160;This is performed by using a&#160;Transformer&#160;encoder&#8211;decoder.</p><p>The backbone of this work replaces the classic backbones that used convolutional neural networks with one based on Transformers, which are mainly used to enhance image features. The&#160;convolution is only used to extract the convolutional features that will be inputs for the Transformer encoder (TRE). By&#160;decoding the characteristics using the Transformer decoder (TRD), the&#160;camera pose is obtained. The&#160;authors added a lane branch to interpret 3D lanes to aid in camera pose learning. First, the&#160;3D lanes are obtained, and&#160;then they are projected into the 2D plane by conducting a homographic transformation.&#160;Thus, the&#160;model learns the best camera pose by comparing the results to the ground truths in both 3D and 2D&#160;planes.</p><p>In&#160;[<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>], the&#160;authors proposed a method entitled PersFormer, which is an end-to-end monocular 3D lane detector model that can simultaneously detect 2D and 3D lanes. Like&#160;the latest state-of-the-art models, the&#160;method is based on the Transformer architecture. However, the&#160;Transformer is not used as the backbone for feature extraction, which is the first part of this network. The&#160;second part of the model is the Perspective Transformer, which is relatively complex, and&#160;also the inspiration for the name of the model. The&#160;general idea behind the Perspective Transformer is to generate an accurate BEV of the input image by taking into consideration the front view features generated by the ResNet&#160;[<xref rid="B1-sensors-25-05729" ref-type="bibr">1</xref>] backbone, the&#160;coordinates of the transformation matrix from IPM, and the camera intrinsic and extrinsic parameters, such as camera height, and&#160;the pitch angle of the camera. The&#160;authors used a Transformer to attend local context and aggregate global features to produce a solid representation in BEV rather than just projecting the one-to-one feature correspondence from front view to BEV. In&#160;addition to the proposed model, the&#160;authors highlighted one of the existing problems in the 3D lane detection task, namely, the&#160;lack of data for the learning process. They addressed this problem by introducing a new large-scale dataset called&#160;OpenLane.</p><p>With the same purpose as&#160;[<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>], previously presented, ONCE-3DLanes&#160;[<xref rid="B9-sensors-25-05729" ref-type="bibr">9</xref>] addresses the problem of insufficient data for 3D lane detection and introduces a new real-world autonomous driving dataset to encourage development on this subject. In&#160;addition to this, the&#160;authors also proposed a model for 3D lane detection called SALAD that does not require human-made anchors and also does not require transformation to BEV or&#160;top-view.</p><p>In&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>], a&#160;lane-aware query generator is introduced, which adapts to lane-specific characteristics and improves detection accuracy by jointly leveraging lane-level and point-level embeddings. Furthermore, LATR uses a Dynamic 3D Ground Positional Embedding that iteratively refines a 3D ground plane to better align with the actual road geometry. Thus, it addresses the limitations of traditional assumptions related to a fixed 3D space. Using these algorithms, the&#160;method achieved state-of-the-art results on the ONCE-3DLanes dataset and very impressive results on other datasets without using BEV image&#160;generation.</p><p>The state-space model (SSM) introduced by Mamba&#160;[<xref rid="B25-sensors-25-05729" ref-type="bibr">25</xref>] addresses ViT problems by providing linear-time complexity. Its results match or even exceed the performance of Transformers in natural language processing tasks. Mamba&#8217;s key innovation lies in the efficient handling of long sequences, making it a scalable solution compared to traditional&#160;Transformers.</p><p>Vision Mamba&#160;[<xref rid="B26-sensors-25-05729" ref-type="bibr">26</xref>] further improves upon this by incorporating bidirectional SSMs to mitigate the weaknesses of Transformers in capturing global context and spatial relationships. However, the&#160;added complexity of processing sequences in bidirectional SSMs can lead to challenges such as increased latency, overfitting, and&#160;uncertainty in accuracy improvement. Despite these challenges, ViTs and CNNs often outperform Mamba-based models in visual tasks because of their efficiency and&#160;reliability.</p><p>Recently, hybrid models that combine Mamba and Transformer architectures have gained considerable attention. Ref.&#160;[<xref rid="B20-sensors-25-05729" ref-type="bibr">20</xref>] introduced MambaVision, a&#160;novel hybrid model designed for computer vision tasks that combines the strengths of both Mamba and Transformers. It employs a hierarchical structure with multi-resolution CNN-based residual blocks to swiftly extract features across different&#160;resolutions.</p><p>Related to our design choices, we also consider advances in detection architectures beyond lane detection.&#160;Ref. [<xref rid="B27-sensors-25-05729" ref-type="bibr">27</xref>] shows how combining multiple feature streams can remain robust even when one modality, for&#160;example, thermal or RGB, is noisy or unreliable. This&#160;idea is similar to our goal of making geometric cues robust against imperfect or uncertain signals. In&#160;another line of work, ref. [<xref rid="B14-sensors-25-05729" ref-type="bibr">14</xref>] replaces dense attention with sparse, reference-point-based attention, which improves greatly the efficiency and accuracy in detection. This&#160;mechanism directly inspires the use of deformable attention in the decoder in our work.</p></sec><sec id="sec3-sensors-25-05729"><title>3. Method&#160;Overview</title><p>Motivated by the success of the latest state-of-the-art LATR&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>] model, we propose a 3D lane detection model with Dual Transformer. In&#160;this section, we will present the overall architecture and implementation&#160;details.</p><p>The objective of the proposed solution is to obtain the coordinates of the traffic lanes in a three-dimensional space, starting from an image that is captured by a camera mounted on the front of the vehicle. Because&#160;we do not generate a BEV image for the lane detection, we no longer require precise calibration of the well-defined intrinsic and extrinsic parameters of the camera, such as the height of the camera, the&#160;focal length, or&#160;the pitch angle, as&#160;we previously did in&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>,<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>]. Concretely, we still use a simple pinhole projection to map 3D points to the image, but&#160;we let the ground plane be adjusted during training with two residuals corresponding to pitch <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and height <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which are supervised in the lane pixels by <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>plane</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. On&#160;ONCE-3DLanes, the camera extrinsics are unavailable. So,&#160;we&#160;follow previous works [<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>,<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>] and use approximate camera settings, and the learned plane compensates for the resulting mismatch. Thus, we can use this method to approximate the camera extrinsics on every dataset.</p><p>The architecture of our proposed model is shown in <xref rid="sensors-25-05729-f001" ref-type="fig">Figure 1</xref>. The&#160;process begins by extracting feature maps from the input image using the MambaVision backbone, which was chosen for its ability to capture both fine-grained details and broader contextual features effectively. After&#160;this important step, the&#160;incoming process is inspired and similar to LATR. Lane-aware queries are generated, where the number of queries corresponds to the number of lanes, and&#160;the number of points represents each lane. These queries are then designed to interact with the extracted feature maps using a deformable attention mechanism, which allows the model to dynamically focus on the most relevant parts of the&#160;image.</p><p>To further enhance the model&#8217;s understanding of the scene, we employ a Dynamic 3D Ground Positional Embedding Generator. This component integrates three-dimensional spatial information into the two-dimensional features which ensure that the model accurately reflects lane geometry without relying on static 3D structures. Finally, a&#160;prediction head processes the enhanced queries and leads to the generation of the final lane predictions. Each component of this architecture will be explained in more detail in the following&#160;sections.</p><sec id="sec3dot1-sensors-25-05729"><title>3.1. Backbone</title><p>MambaVision serves as the feature extraction backbone for the object detection and segmentation models.&#160;In&#160;these frameworks, the&#160;backbone network is responsible for extracting rich feature maps from the input image, which are then used by subsequent layers to detect objects and generate segmentation masks. The&#160;hierarchical architecture of MambaVision, which includes both CNN-based residual blocks and Transformer blocks, makes it highly effective for this&#160;role.</p><p>CNN layers in MambaVision, specifically in Stages 1 and 2, quickly process high-resolution features. Given an input image <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> are the height and width of the input image, which is an RGB image with three channels, the&#160;number of channels is denoted with <italic toggle="yes">C</italic>. These layers extract feature maps <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> through a&#160;series of convolutions and pooling operations:<disp-formula><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>=</mml:mo><mml:mi>CNN</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>CNN</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the convolutional operations, including residual connections, which are formulated as<disp-formula><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>res</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Conv</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>BN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> represents the activation function, typically ReLU, and&#160;BN denotes batch normalization. These residual connections help maintain spatial details crucial for accurately locating and identifying objects by capturing both low-level and high-level&#160;features.</p><p>In Stages 3 and 4, MambaVision employs a combination of MambaVision Mixer blocks and Transformer blocks. The&#160;MambaVision Mixer blocks utilize Structured SSMs to capture both short- and long-range dependencies. The&#160;SSM in MambaVision can be expressed using the following state-space equations:<disp-formula><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the hidden state, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the input, and&#160;<inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the output. The&#160;parameters <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and&#160;<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are learnable. These parameters are discretized for computational efficiency. This allows the MambaVision Mixer to process sequences effectively and capture complex&#160;dependencies.</p><p>The output of the MambaVision Mixer block can be represented as<disp-formula><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>mixer</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Mixer</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>low</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>low</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the lower-resolution feature maps. The&#160;Mixer block combines convolutional operations with SSMs, capturing both local and global&#160;context.</p><p>The Transformer blocks in Stages 3 and 4 further enhance the model&#8217;s ability to maintain global context. Using the multi-head self-attention mechanism, the&#160;Transformer blocks process the feature maps <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>mixer</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as<disp-formula><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mi mathvariant="bold">V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow></mml:math></inline-formula>, and&#160;<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:math></inline-formula> are the query, key, and&#160;value matrices derived from <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>mixer</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#160;<inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the dimensionality of the key vectors. This self-attention mechanism allows the model to capture dependencies across the entire image to ensure a comprehensive understanding of object relationships and spatial&#160;configurations.</p><p>The combined output from the MambaVision Mixer and Transformer blocks provides enriched feature maps that are well-suited for generating accurate segmentation masks and object detections:<disp-formula><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Transformer</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>mixer</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Transformer</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the transformation applied by the Transformer block. This&#160;hierarchical combination of CNNs, SSMs, and&#160;Transformers within MambaVision makes it a powerful backbone for object detection and segmentation tasks, which balances both computational efficiency and&#160;accuracy.</p><p>The LATR model begins by processing the input image <italic toggle="yes">I</italic> using the MambaVision-S-1K backbone to extract a feature map <italic toggle="yes">X</italic>. The&#160;feature map captures the necessary visual details from the front view of the driving scene and provides a rich set of features for further processing:<disp-formula><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>MambaVision</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle="yes">C</italic> is the number of channels, <italic toggle="yes">H</italic> is the height, and <italic toggle="yes">W</italic> is the width of the feature&#160;map.</p></sec><sec id="sec3dot2-sensors-25-05729"><title>3.2. Lane-Aware Query&#160;Generation</title><p>After extracting the feature map, the&#160;model generates lane-aware queries <italic toggle="yes">Q</italic> using the lane-aware query generator. These queries are tailored to each detected lane and incorporate both lane- and point-level embeddings. The&#160;lane-aware queries <italic toggle="yes">Q</italic> are represented as<disp-formula><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>lane</mml:mi></mml:msub><mml:mo>&#8853;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>point</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>lane</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> captures the overall lane structure and is computed using instance activation maps (IAMs) derived from the feature map <italic toggle="yes">X</italic>, and&#160;<inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>point</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> captures detailed information at specific points along the lane, leveraging the learnable weights corresponding to the predefined longitudinal&#160;coordinates.</p></sec><sec id="sec3dot3-sensors-25-05729"><title>3.3. Interaction via Deformable&#160;Attention</title><p>The lane-aware queries <italic toggle="yes">Q</italic> then interact with the feature map <italic toggle="yes">X</italic> through a deformable attention mechanism. This mechanism allows the model to selectively focus on relevant regions in the image by dynamically adjusting its attention based on the queries:<disp-formula><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>DeformAttn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the refined feature map after the deformable attention operation, which uses the lane-aware queries to capture relevant features for lane&#160;detection.</p></sec><sec id="sec3dot4-sensors-25-05729"><title>3.4. Dynamic 3D Ground Positional Embedding (PE) Generation</title><p>To integrate three-dimensional context, the&#160;model employs a Dynamic 3D Ground Positional Embedding Generator. This component enhances the feature map <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> with 3D spatial information by projecting a hypothetical 3D ground plane into the 2D feature space:<disp-formula><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the 2D coordinates on the feature map <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (in pixels), <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the homogeneous scale factor of the projection, <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is a 3D point on the ground plane, and <italic toggle="yes">T</italic> is the pinhole camera projection matrix.</p><p>A 3D ground plane <italic toggle="yes">P</italic> is initialized and iteratively refined using a transformation matrix <italic toggle="yes">D</italic> to align with the real-world ground. This embedding step enhances the feature map <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which allows the model to accurately represent the lane geometry:<disp-formula><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>PEGenerator</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>During training, the&#160;ground plane is adjusted step by step by adjusting the pitch and height. We train it together with the lane losses using a simple plane alignment loss (<xref rid="sec3dot7-sensors-25-05729" ref-type="sec">Section 3.7</xref>).&#160;This learned, adjustable plane works better than a fixed plane or a&#160;fixed frustum.</p></sec><sec id="sec3dot5-sensors-25-05729"><title>3.5. Iterative Plane&#160;Update</title><p>We describe the transformation matrix <italic toggle="yes">D</italic> using the pitch angle <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the vertical shift <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. With&#160;these, we construct the matrix:<disp-formula><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">sin</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">sin</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#916;</mml:mo><mml:mspace width="3.33333pt"/><mml:mi>h</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><italic toggle="yes">D</italic> is used to update the 3D ground plane, adjusting its position to better fit the ground truth road surface. In the decoder layer <italic toggle="yes">t</italic>, the&#160;plane grid from the previous step, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, is&#160;updated by multiplying it by this matrix:<disp-formula><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The update is based on the residuals calculated between the predicted 3D ground plane and the actual 3D lane&#160;annotations.</p></sec><sec id="sec3dot6-sensors-25-05729"><title>3.6. Prediction Head for Final Lane&#160;Predictions</title><p>Finally, the&#160;enhanced feature map <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is fed into a prediction head that uses an MLP to output the final 3D lane predictions. This head estimates the 3D coordinates of the lane points and their visibility:<disp-formula><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mi>MLP</mml:mi><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This final step produces the predicted 3D positions and classifications of the lanes, providing a comprehensive 3D lane model. This workflow avoids the use of traditional 3D surrogates like IPM and leverages Transformer-based attention mechanisms for efficient and accurate 3D lane&#160;detection.</p></sec><sec id="sec3dot7-sensors-25-05729"><title>3.7. Loss&#160;Function</title><p>The loss function is computed the same as in&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>] and combines several components to ensure accurate 3D lane detection, focusing on regression, visibility, and&#160;classification.</p><p>The primary loss for 3D lane prediction <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lane</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> consists of<disp-formula><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lane</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are regression losses for the <italic toggle="yes">x</italic>- and <italic toggle="yes">z</italic>-coordinates of the lane points, <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the visibility loss, using binary cross-entropy, and&#160;<inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the classification loss, handled by focal loss to manage class&#160;imbalance.</p><p>The overall loss function is a weighted combination of 3D lane prediction, segmentation, and&#160;3D ground plane alignment losses:<disp-formula><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>plane</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lane</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> trains a simple lane versus background mask obtained by drawing the ground-truth lane lines on the image and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>plane</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> makes the learned ground plane, composed of pitch and height, to&#160;match the 3D lane annotations at labeled pixels.</p><p>We supervise the ground plane with a simple alignment loss computed only at lane pixels:<disp-formula><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>plane</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mo>&#8467;</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the 3D canvas induced by the current ground plane and <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mo>&#8467;</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> stores the metric 3D of the annotated lane points. Training is end-to-end, so this loss is optimized together with the lane losses, and&#160;gradients pass through the plane update, so the plane aligns to the road surface over time.</p><p>The values we have used for the base weights are the following: <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mspace width="3.33333pt"/><mml:mo>=</mml:mo><mml:mspace width="3.33333pt"/></mml:mrow><mml:mn>5.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mspace width="3.33333pt"/><mml:mo>=</mml:mo><mml:mspace width="3.33333pt"/></mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mspace width="3.33333pt"/><mml:mo>=</mml:mo><mml:mspace width="3.33333pt"/></mml:mrow><mml:mn>10.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and&#160;all others <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. We find that they are stable across both datasets.</p><p>This structure ensures that the model optimizes not only for accurate lane positioning but also for visibility and correct ground plane&#160;alignment.</p></sec></sec><sec id="sec4-sensors-25-05729"><title>4. Experiments</title><p>In this section, we present the training details alongside the results of the proposed model on the ONCE-3DLanes (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://once-3dlanes.github.io/3dlanes/">https://once-3dlanes.github.io/3dlanes/</uri>, accessed on 29 July 2025) and Apollo 3D Lane Detection (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developer.apollo.auto/synthetic.html">https://developer.apollo.auto/synthetic.html</uri>, accessed on 29 July 2025)&#160;datasets.</p><sec id="sec4dot1-sensors-25-05729"><title>4.1. Training&#160;Details</title><p>In our experiments, we set the batch size to 2 and used three workers during training. The&#160;number of categories for classification is 2, one representing the lane and the other the background, and&#160;we defined a positive threshold of 0.3 for classification. The labels corresponding to the lane and background created labels by drawing the ground-truth lane lines on the image and making them a bit thicker, 3 pixels at 1/4 of the resolution of the feature map. The&#160;pixels on these thick lines are labeled &#8217;lane&#8217; and all other pixels are &#8217;background&#8217;.&#160;This auxiliary head is used only during training and it is dropped at inference.</p><p>Our model follows an encoder&#8211;decoder architecture based on&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>]. We resize the input images to a resolution of 720 &#215; 960 and they are then fed to the backbone. The&#160;encoder is initialized with the MambaVision-S-1K pretrained model, and&#160;we employ a Feature Pyramid Network (FPN) as the neck, with&#160;input channels [192, 384, 768] and output dimension of 192. Those input channels correspond to the feature map channels extracted from the last three stages of the MambaVision backbone, which represent different levels of spatial detail.&#160;The&#160;FPN outputs four feature maps and each one is passed through additional convolutions with the largest scale aggregated as input for the&#160;decoder.</p><p>For the decoder, we apply deformable attention with four heads, eight sample points, and&#160;192-dimensional embeddings.&#160;A&#160;six-layer Transformer decoder is used for object detection and segmentation, with&#160;12 queries and 20 anchor points per query distributed across the <italic toggle="yes">y</italic> axis.</p><p>All models are trained using the AdamW optimizer with a learning rate of <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and a weight decay of 0.01. We set the learning rate multiplier for <monospace>sampling_offsets</monospace> to&#160;0.1.</p><p>The proposed method was trained and evaluated on a system that has the following specifications: Intel 4790K CPU (Intel, Santa Clara, CA, USA), one NVIDIA RTX 3060 Ti GPU (NVIDIA, Santa Clara, CA, USA), 16GB DDR3 RAM, and 1TB SSD storage. We strongly believe that slightly better results can be achieved by fine-tuning the hyper-parameters, but due to our system limitation, these were the best results we could achieve.</p></sec><sec id="sec4dot2-sensors-25-05729"><title>4.2. Evaluation&#160;Metrics</title><p>The evaluation metrics used to evaluate the performance of the proposed model on the ONCE-3DLanes dataset are the F1 score, precision, recall, and CD error. Precision represents the ratio between all correctly identified positive instances and all instances that were predicted as positive. Recall refers to the ratio of true positives out of all the actual positive instances in the dataset. The F1 score represents the harmonic mean between precision and recall. Those three metrics can be mathematically expressed as<disp-formula><mml:math id="mm67" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>Recall</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mspace width="0.166667em"/><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>Recall</mml:mi></mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>+</mml:mo><mml:mi>Recall</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of true positives, <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> the number of false positives, and <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> the number of false negatives.</p><p>The CD error represents the Chamfer distance between the predicted lane line and the ground truth.</p><p>The Apollo dataset uses the F1 score and, besides it, errors that are computed using the Euclidean distance and measured in meters. It compares the matched lanes for the near range (0&#8211;40 m) and the far range (40&#8211;100 m).</p></sec><sec sec-type="results" id="sec4dot3-sensors-25-05729"><title>4.3. Results on&#160;ONCE-3DLanes</title><p>The ONCE-3DLanes [<xref rid="B9-sensors-25-05729" ref-type="bibr">9</xref>] dataset is a collection of annotated data for autonomous driving, which includes lane layout information in 3D space. To create this dataset, a pipeline was developed that can automatically generate precise 3D lane location data from 2D annotations. This is achieved by utilizing the explicit relationship between point clouds and image pixels, having over 211,000 road scenes, and resulting in a high-quality dataset. The model was trained for 20 epochs, with the best results being achieved in the 11th epoch.</p><p>The results obtained by our model on the ONCE-3DLanes dataset, as shown in <xref rid="sensors-25-05729-t001" ref-type="table">Table 1</xref>, indicate a significant improvement over current state-of-the-art methods across all key metrics. Specifically, our model achieved an F1 score of 82.39%, outperforming the previous highest score of 80.59% achieved by LATR [<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>]. This indicates a superior balance between precision and recall in detecting lane markings.</p></sec><sec sec-type="results" id="sec4dot4-sensors-25-05729"><title>4.4. Results on Apollo 3D Lane&#160;Detection</title><p>The Apollo 3D Lane Detection dataset, introduced in [<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>], is an extension of the original Apollo Synthetic Dataset. The dataset was built using the Unity game engine to encourage research and development in the field of autonomous driving. The main reason behind it was the lack of labeled real-world data. The Apollo 3D Lane Detection dataset contains a total of 6000 samples from the virtual highway map, 1500 samples from the urban map, and 3000 samples from the residential area, along with the corresponding depth map, semantic segmentation map, and 3D lane line information.</p><p>The dataset is split into three sets to evaluate the algorithms. The &#8220;balanced scenes&#8221; set follows a standard five-fold split for unbiased data training and testing. The &#8220;rarely observed scenes&#8221; set uses a subset of testing data from a complex urban map to evaluate generalization capabilities. The &#8220;scenes with visual variations&#8221; set evaluates methods under illumination changes, such as 3D examples from before dawn.</p><p>The performance of our model on the Apollo 3D Lane Detection dataset, particularly in the balanced scenes category, <xref rid="sensors-25-05729-t002" ref-type="table">Table 2</xref>, highlights its strength compared to current state-of-the-art methods. Our model achieved an F1 score of <bold>97.0%</bold>, which is second after the best of <bold>97.4%</bold> achieved by [<xref rid="B30-sensors-25-05729" ref-type="bibr">30</xref>]. This performance in F1 score indicates that our model strikes an excellent balance between precision and recall, outperforming models like [<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>,<xref rid="B28-sensors-25-05729" ref-type="bibr">28</xref>], which also performed well but fell slightly short in this aspect.</p><p>For the rarely observed and visual variant scenes, as shown in <xref rid="sensors-25-05729-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05729-t004" ref-type="table">Table 4</xref>, our model delivers solid performance. In the rarely observed scenes, while [<xref rid="B32-sensors-25-05729" ref-type="bibr">32</xref>] achieves the highest F1 score, our model remains competitive with an F1 score of 95.1%, positioning it closely to other top-performing methods. Similarly, in the visual variant scenes, our model achieves an F1 score of 94.5%, indicating reliable performance across diverse environmental and visual conditions.</p><p>Our backbone, MambaVision-S-1K, is composed from the following hybrid blocks: Conv, Mamba, Transformer. The Mamba and the self-attention blocks from the Transformer have the ability to capture and model long-term relationships, but weaker inductive biases than CNNs. Thus, they rely more on the amount and diversity of data to learn these relationships correctly. In [<xref rid="B20-sensors-25-05729" ref-type="bibr">20</xref>], it was observed that, when trained only on ImageNet-1K, which has around 1.2 M images, MambaVision-S performs comparably to ResNet, but really gains advantage on larger sets such as ImageNet-22K. As the Apollo subsets have much fewer images than ONCE-3DLanes, it was expected that our configuration would not perform as well on this dataset as on ONCE-3DLanes. Although there is room for improvement in these more challenging scenarios, our model still shows good overall performance.</p></sec><sec id="sec4dot5-sensors-25-05729"><title>4.5. Runtime and Complexity&#160;Comparison</title><p>At our evaluation resolution, 720 &#215; 960, our MambaVision-S-based model counts 62.6 M parameters and 175.5 GFLOPs (FLOPs <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), in comparison to LATR, which uses the ResNet-50 backbone and has 46.8 M parameters and 127.9 GFLOPs. Those results were not presented in [<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>], but we used their official GitHub repository to download a pretrained model and calculate the complexity under the same input. Although this may seem like an increase in complexity, our model remains substantially more efficient than PersFormer, which is the baseline model for LATR in this line of work, despite being moderately heavier than [<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>]. The authors of [<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>] did not publish the complexity of the model, but thanks to GroupLane&#8217;s work [<xref rid="B33-sensors-25-05729" ref-type="bibr">33</xref>], we were able to obtain them. PersFormer working with EfficientNet-B7 [<xref rid="B2-sensors-25-05729" ref-type="bibr">2</xref>] is much heavier, having 572.4 GFLOPs, which is &#8764;3.26 times more than our computational cost.</p><p>We ran the inference on our system, which uses NVIDIA RTX 3060 Ti GPU (NVIDIA, Santa Clara, CA, USA), with batch size 1 and excluded the first five warmup iterations. We measure 14.44 FPS for our model versus 15.84 FPS for LATR under the same configuration. In terms of latency, it is &#8764;69.3 ms per frame for ours and &#8764;63.1 ms for LATR, which represents a difference of &#8764;6.1 ms (&#8764;8.8% in FPS). In practice, this is a modest difference on our system, and both models operate in essentially almost the same real-time regime at this resolution.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05729"><title>5. Conclusions</title><p>In conclusion, we have proposed an innovative 3D lane detection model that uses the MambaVision backbone integrated with Transformer-based attention mechanisms. This approach significantly improves lane detection performance in 3D space and addresses the challenges posed by non-flat surfaces and variable road conditions. Using deformable attention and lane-aware queries, the model dynamically focuses on relevant regions of the image, resulting in enhanced accuracy for lane detection tasks. Our method achieved state-of-the-art results on the ONCE-3DLanes dataset and competed closely with other methods on the Apollo 3D Lane Detection dataset, demonstrating its superiority in terms of precision, recall, and F1 score. These results highlight the potential of Transformer-enhanced backbones for robust 3D lane detection in autonomous driving applications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, R.-M.C. and C.-A.P.; methodology, R.-M.C. and C.-A.P.; software, R.-M.C.; validation, R.-M.C.; formal analysis, R.-M.C.; investigation, R.-M.C.; resources, C.-A.P.; data curation, R.-M.C.; writing&#8212;original draft preparation, R.-M.C.; writing&#8212;review and editing, C.-A.P.; visualization, R.-M.C.; supervision, C.-A.P.; project administration, C.-A.P.; funding acquisition, C.-A.P. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05729"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id></element-citation></ref><ref id="B2-sensors-25-05729"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.</given-names></name></person-group><article-title>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</article-title><source>Proceedings of the 36th International Conference on Machine Learning Research</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>9&#8211;15 June 2019</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Chaudhuri</surname><given-names>K.</given-names></name><name name-style="western"><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group><publisher-name>PMLR</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2019</year><volume>Volume 97</volume><fpage>6105</fpage><lpage>6114</lpage></element-citation></ref><ref id="B3-sensors-25-05729"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>A.G.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kalenichenko</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Weyand</surname><given-names>T.</given-names></name><name name-style="western"><surname>Andreetto</surname><given-names>M.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1704.04861</pub-id></element-citation></ref><ref id="B4-sensors-25-05729"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sandler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Howard</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhmoginov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name></person-group><article-title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><pub-id pub-id-type="doi">10.1109/cvpr.2018.00474</pub-id></element-citation></ref><ref id="B5-sensors-25-05729"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name></person-group><article-title>An approach of lane detection based on Inverse Perspective Mapping</article-title><source>Proceedings of the 17th International IEEE Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Qingdao, China</conf-loc><conf-date>8&#8211;11 October 2014</conf-date><pub-id pub-id-type="doi">10.1109/itsc.2014.6957662</pub-id></element-citation></ref><ref id="B6-sensors-25-05729"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Miao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Choe</surname><given-names>T.E.</given-names></name></person-group><article-title>Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>666</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-58589-1_40</pub-id></element-citation></ref><ref id="B7-sensors-25-05729"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sima</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2022</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2020</conf-date><publisher-name>Springer Nature Switzerland</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>550</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-19839-7_32</pub-id></element-citation></ref><ref id="B8-sensors-25-05729"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sima</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Openlane-v2: A topology reasoning benchmark for unified 3D HD mapping</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2024</year><volume>36</volume><fpage>18873</fpage><lpage>18884</lpage></element-citation></ref><ref id="B9-sensors-25-05729"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>F.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mi</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>ONCE-3DLanes: Building Monocular 3D Lane Detection</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><pub-id pub-id-type="doi">10.1109/cvpr52688.2022.01663</pub-id></element-citation></ref><ref id="B10-sensors-25-05729"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cordeiro</surname><given-names>F.R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name></person-group><article-title>PSSCL: A progressive sample selection framework with contrastive loss designed for noisy labels</article-title><source>Pattern Recognit.</source><year>2025</year><volume>161</volume><elocation-id>111284</elocation-id><pub-id pub-id-type="doi">10.1016/j.patcog.2024.111284</pub-id></element-citation></ref><ref id="B11-sensors-25-05729"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Carion</surname><given-names>N.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Synnaeve</surname><given-names>G.</given-names></name><name name-style="western"><surname>Usunier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name></person-group><article-title>End-to-End Object Detection with Transformers</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>213</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-58452-8_13</pub-id></element-citation></ref><ref id="B12-sensors-25-05729"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An Image is Worth 16 &#215; 16 Words: Transformers for Image Recognition at Scale</article-title><source>Proceedings of the International Conference on Learning Representations</source><conf-loc>Online</conf-loc><conf-date>3&#8211;7 May 2021</conf-date></element-citation></ref><ref id="B13-sensors-25-05729"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>A Survey on Vision Transformer</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>87</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3152247</pub-id><pub-id pub-id-type="pmid">35180075</pub-id></element-citation></ref><ref id="B14-sensors-25-05729"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Su</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Deformable DETR: Deformable Transformers for End-to-End Object Detection</article-title><source>Proceedings of the International Conference on Learning Representations</source><conf-loc>Online</conf-loc><conf-date>3&#8211;7 May 2021</conf-date></element-citation></ref><ref id="B15-sensors-25-05729"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Torr</surname><given-names>P.H.</given-names></name><etal/></person-group><article-title>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>19&#8211;25 June 2021</conf-date><pub-id pub-id-type="doi">10.1109/cvpr46437.2021.00681</pub-id></element-citation></ref><ref id="B16-sensors-25-05729"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Russakovsky</surname><given-names>O.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Krause</surname><given-names>J.</given-names></name><name name-style="western"><surname>Satheesh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Karpathy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Khosla</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bernstein</surname><given-names>M.</given-names></name><etal/></person-group><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title><source>Int. J. Comput. Vis.</source><year>2015</year><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="B17-sensors-25-05729"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Maire</surname><given-names>M.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hays</surname><given-names>J.</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ramanan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zitnick</surname><given-names>C.L.</given-names></name></person-group><article-title>Microsoft COCO: Common Objects in Context</article-title><source>Proceedings of the Computer Vision&#8212;ECCV</source><conf-loc>Zurich, Switzerland</conf-loc><conf-date>6&#8211;12 September 2014</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2014</year><fpage>740</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10602-1_48</pub-id></element-citation></ref><ref id="B18-sensors-25-05729"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cordts</surname><given-names>M.</given-names></name><name name-style="western"><surname>Omran</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rehfeld</surname><given-names>T.</given-names></name><name name-style="western"><surname>Enzweiler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Benenson</surname><given-names>R.</given-names></name><name name-style="western"><surname>Franke</surname><given-names>U.</given-names></name><name name-style="western"><surname>Roth</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name></person-group><article-title>The Cityscapes Dataset for Semantic Urban Scene Understanding</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#8211;1 July 2016</conf-date><pub-id pub-id-type="doi">10.1109/cvpr.2016.350</pub-id></element-citation></ref><ref id="B19-sensors-25-05729"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>L.u.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is All you Need</article-title><source>Advances in Neural Information Processing Systems</source><person-group person-group-type="editor"><name name-style="western"><surname>Guyon</surname><given-names>I.</given-names></name><name name-style="western"><surname>Luxburg</surname><given-names>U.V.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wallach</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fergus</surname><given-names>R.</given-names></name><name name-style="western"><surname>Vishwanathan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Garnett</surname><given-names>R.</given-names></name></person-group><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2017</year><volume>Volume 30</volume></element-citation></ref><ref id="B20-sensors-25-05729"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kautz</surname><given-names>J.</given-names></name></person-group><article-title>Mambavision: A hybrid mamba-transformer vision backbone</article-title><source>Proceedings of the Computer Vision and Pattern Recognition Conference</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>11&#8211;15 June 2025</conf-date><fpage>25261</fpage><lpage>25270</lpage></element-citation></ref><ref id="B21-sensors-25-05729"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Garnett</surname><given-names>N.</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pe&#8217;er</surname><given-names>T.</given-names></name><name name-style="western"><surname>Lahav</surname><given-names>R.</given-names></name><name name-style="western"><surname>Levi</surname><given-names>D.</given-names></name></person-group><article-title>3D-LaneNet: End-to-End 3D Multiple Lane Detection</article-title><source>Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><pub-id pub-id-type="doi">10.1109/iccv.2019.00301</pub-id></element-citation></ref><ref id="B22-sensors-25-05729"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><article-title>Robust Monocular 3D Lane Detection with Dual Attention</article-title><source>Proceedings of the 2021 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>19&#8211;22 September 2021</conf-date><pub-id pub-id-type="doi">10.1109/icip42928.2021.9506296</pub-id></element-citation></ref><ref id="B23-sensors-25-05729"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Z.</given-names></name></person-group><article-title>Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>online</conf-loc><conf-date>22 February&#8211;1 March 2022</conf-date><publisher-name>Association for the Advancement of Artificial Intelligence (AAAI)</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2022</year><volume>Volume 36</volume><fpage>1765</fpage><lpage>1772</lpage><pub-id pub-id-type="doi">10.1609/aaai.v36i2.20069</pub-id></element-citation></ref><ref id="B24-sensors-25-05729"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kun</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Latr: 3d lane detection from monocular images with transformer</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>1&#8211;6 October 2023</conf-date><fpage>7941</fpage><lpage>7952</lpage></element-citation></ref><ref id="B25-sensors-25-05729"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dao</surname><given-names>T.</given-names></name></person-group><article-title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</article-title><source>Proceedings of the 1st Conference on Language Modeling</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>7&#8211;9 October 2024</conf-date></element-citation></ref><ref id="B26-sensors-25-05729"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Vision Mamba: Efficient visual representation learning with bidirectional state space model</article-title><source>Proceedings of the 41st International Conference on Machine Learning (ICML&#8217;24)</source><conf-loc>Vienna, Austria</conf-loc><conf-date>21&#8211;27 July 2024</conf-date></element-citation></ref><ref id="B27-sensors-25-05729"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>He</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name></person-group><article-title>Divide-and-Conquer: Confluent Triple-Flow Network for RGB-T Salient Object Detection</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2025</year><volume>47</volume><fpage>1958</fpage><lpage>1974</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3511621</pub-id><pub-id pub-id-type="pmid">40030445</pub-id></element-citation></ref><ref id="B28-sensors-25-05729"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>Z.h.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Anchor3dlane: Learning to regress 3D anchors for monocular 3D lane detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date><fpage>17451</fpage><lpage>17460</lpage></element-citation></ref><ref id="B29-sensors-25-05729"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>J.</given-names></name></person-group><article-title>WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels, 2023</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.1109/ICRA48891.2023.10161184</pub-id><pub-id pub-id-type="arxiv">2209.11523</pub-id></element-citation></ref><ref id="B30-sensors-25-05729"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pittner</surname><given-names>M.</given-names></name><name name-style="western"><surname>Janai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Condurache</surname><given-names>A.P.</given-names></name></person-group><article-title>LaneCPP: Continuous 3D Lane Detection using Physical Priors</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>10639</fpage><lpage>10648</lpage></element-citation></ref><ref id="B31-sensors-25-05729"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>E.</given-names></name></person-group><article-title>CurveFormer: 3D Lane Detection by Curve Propagation with Curve Queries and Attention</article-title><source>Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#8211;2 June 2023</conf-date><fpage>7062</fpage><lpage>7068</lpage><pub-id pub-id-type="doi">10.1109/ICRA48891.2023.10161160</pub-id></element-citation></ref><ref id="B32-sensors-25-05729"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name></person-group><article-title>Bev-lanedet: An efficient 3d lane detection based on virtual camera via key-points</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>1002</fpage><lpage>1011</lpage></element-citation></ref><ref id="B33-sensors-25-05729"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping</article-title><source>IEEE Robot. Autom. Lett.</source><year>2024</year><volume>9</volume><fpage>10487</fpage><lpage>10494</lpage><pub-id pub-id-type="doi">10.1109/LRA.2024.3475881</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05729-f001" orientation="portrait"><label>Figure 1</label><caption><p>Proposed method architecture. The model begins by extracting a feature map <italic toggle="yes">X</italic> using the MambaVision backbone, followed by generating lane-aware queries <italic toggle="yes">Q</italic> that refine the feature map <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> through a deformable attention mechanism. Next, the Dynamic 3D Ground Positional Embedding introduces 3D context, updating the feature map to <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, after which the dynamic plane is iteratively updated to align with real-world geometry using a transformation matrix, and, finally, the prediction head processes the refined feature map to produce accurate 3D lane predictions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05729-g001.jpg"/></fig><table-wrap position="float" id="sensors-25-05729-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05729-t001_Table 1</object-id><label>Table 1</label><caption><p>Evaluation results on ONCE-3DLanes. Best values are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precis. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CD Err. (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D-LaneNet&#160;[<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.127</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gen-LaneNet&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.121</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SALAD&#160;[<xref rid="B9-sensors-25-05729" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.098</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PersFormer&#160;[<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.074</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor3DLane&#160;[<xref rid="B28-sensors-25-05729" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.060</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WS-3D-Lane&#160;[<xref rid="B29-sensors-25-05729" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.058</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LATR&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>86.12</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.052</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>82.39</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>79.85</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.055</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05729-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05729-t002_Table 2</object-id><label>Table 2</label><caption><p>Evaluation results on Apollo 3D Lane Detection balanced scenes. Best values are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">X&#160;Error Near (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">X Error Far (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Z Error Near (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Z Error Far (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D-LaneNet&#160;[<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.068</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.477</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.015</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.202</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gen-LaneNet&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.061</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.469</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.012</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.214</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CLGo&#160;[<xref rid="B23-sensors-25-05729" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.061</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.361</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.029</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.250</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PersFormer&#160;[<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.054</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.356</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.010</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.234</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor3DLane&#160;[<xref rid="B28-sensors-25-05729" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.052</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.306</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.015</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.233</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CurveFormer&#160;[<xref rid="B31-sensors-25-05729" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.078</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.326</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.219</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LATR&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.253</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.007</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.202</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEV-LaneDet&#160;[<xref rid="B32-sensors-25-05729" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.016</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.242</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.216</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LaneCPP&#160;[<xref rid="B30-sensors-25-05729" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>97.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.030</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.277</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.011</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.216</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.255</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.009</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.204</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05729-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05729-t003_Table 3</object-id><label>Table 3</label><caption><p>Evaluation results on Apollo 3D Lane Detection rarely observed scenes. Best values are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">X Error Near (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">X Error Far (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Z Error Near (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Z Error Far (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D-LaneNet&#160;[<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.166</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.855</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.039</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.521</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gen-LaneNet&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.139</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.903</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.030</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.539</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CLGo&#160;[<xref rid="B23-sensors-25-05729" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.147</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.735</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.071</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.609</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PersFormer&#160;[<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.107</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.782</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.602</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor3DLane&#160;[<xref rid="B28-sensors-25-05729" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.094</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.693</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.579</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CurveFormer&#160;[<xref rid="B31-sensors-25-05729" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.182</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.737</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.039</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.561</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LATR&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.050</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.015</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.532</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LaneCPP&#160;[<xref rid="B30-sensors-25-05729" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.073</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.651</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.543</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEV-LaneDet&#160;[<xref rid="B32-sensors-25-05729" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.031</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.594</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.040</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.556</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.076</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.626</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.536</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05729-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05729-t004_Table 4</object-id><label>Table 4</label><caption><p>Evaluation results on Apollo 3D Lane Detection visual variant scenes. Best values are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">X Error Near (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">X Error Far (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Z Error Near (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Z Error Far (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D-LaneNet&#160;[<xref rid="B21-sensors-25-05729" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.115</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.601</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.032</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.230</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gen-LaneNet&#160;[<xref rid="B6-sensors-25-05729" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.074</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.538</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.015</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.232</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CLGo&#160;[<xref rid="B23-sensors-25-05729" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.084</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.464</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.045</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.312</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PersFormer&#160;[<xref rid="B7-sensors-25-05729" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.074</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.430</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.015</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.266</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LaneCPP&#160;[<xref rid="B30-sensors-25-05729" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.054</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.327</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.222</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CurveFormer&#160;[<xref rid="B31-sensors-25-05729" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.410</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.028</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.254</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor3DLane&#160;[<xref rid="B28-sensors-25-05729" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.068</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.367</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.020</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.232</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LATR&#160;[<xref rid="B24-sensors-25-05729" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.045</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.315</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.228</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEV-LaneDet&#160;[<xref rid="B32-sensors-25-05729" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>96.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.027</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.320</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.031</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.256</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.067</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.337</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.231</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>