<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473504</article-id><article-id pub-id-type="pmcid-ver">PMC12473504.1</article-id><article-id pub-id-type="pmcaid">12473504</article-id><article-id pub-id-type="pmcaiid">12473504</article-id><article-id pub-id-type="pmid">41013012</article-id><article-id pub-id-type="doi">10.3390/s25185775</article-id><article-id pub-id-type="publisher-id">sensors-25-05775</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>CrossInteraction: Multi-Modal Interaction and Alignment Strategy for 3D Perception</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Zhao</surname><given-names initials="W">Weiyi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05775" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="X">Xinxin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05775" ref-type="aff">1</xref><xref rid="af2-sensors-25-05775" ref-type="aff">2</xref><xref rid="c1-sensors-25-05775" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5373-3270</contrib-id><name name-style="western"><surname>Ding</surname><given-names initials="Y">Yu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05775" ref-type="aff">1</xref><xref rid="af2-sensors-25-05775" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Li</surname><given-names initials="W">Wei</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05775"><label>1</label>College of Automation, Nanjing University of Information Science and Technology, Nanjing 210044, China; <email>zhaoweiyi2023@126.com</email> (W.Z.); <email>dingyu@nuist.edu.cn</email> (Y.D.)</aff><aff id="af2-sensors-25-05775"><label>2</label>Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), Nanjing 210044, China</aff><author-notes><corresp id="c1-sensors-25-05775"><label>*</label>Correspondence: <email>liuxinxin@nuist.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>16</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5775</elocation-id><history><date date-type="received"><day>12</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>06</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>09</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>16</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05775.pdf"/><abstract><p>Cameras and LiDAR are the primary sensors utilized in contemporary 3D object perception, leading to the development of various multi-modal detection algorithms for images, point clouds, and their fusion. Given the demanding accuracy requirements in autonomous driving environments, traditional multi-modal fusion techniques often overlook critical information from individual modalities and struggle to effectively align transformed features. In this paper, we introduce an improved modal interaction strategy, called CrossInteraction. This method enhances the interaction between modalities by using the output of the first modal representation as the input for the second interaction enhancement, resulting in better overall interaction effects. To further address the challenge of feature alignment errors, we employ a graph convolutional network. Finally, the prediction process is completed through a cross-attention mechanism, ensuring more accurate detection out- comes.</p></abstract><kwd-group><kwd>LiDAR and camera fusion</kwd><kwd>LiDAR sensors</kwd><kwd>feature-level fusion</kwd><kwd>transformers</kwd><kwd>multi-modal perception</kwd></kwd-group><funding-group><award-group><funding-source>the National Natural Science Foundation of China</funding-source><award-id>62203225</award-id><award-id>62033005</award-id></award-group><award-group><funding-source>the Natural Science Foundation of Jiangsu Province</funding-source><award-id>BK20220443</award-id></award-group><award-group><funding-source>Qinglan Project of Jiangsu Province</funding-source><funding-source>Jilin University</funding-source><award-id>GEIOF 20240402</award-id></award-group><funding-statement>This work was partially supported by the National Natural Science Foundation of China (62203225 and 62033005), the Natural Science Foundation of Jiangsu Province (BK20220443), the Qinglan Project of Jiangsu Province, and the Key Laboratory of Geoghysical Exploration Equipment, Ministry of Education (Jilin University, GEIOF 20240402).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05775"><title>1. Introduction</title><p>In recent years, the autonomous driving industry has developed rapidly. Object detection is becoming a vital process for the autonomous perception system. However, traditional 2D object detection methods, which rely solely on single-view images, fail to provide essential 3D information such as spatial distances. In contrast, 3D object detection techniques [<xref rid="B1-sensors-25-05775" ref-type="bibr">1</xref>] offer a comprehensive understanding of the vehicle&#8217;s surroundings by assigning category labels and constructing 3D bounding boxes to capture object shape, distance, and direction. This capability is fundamental for tasks such as motion prediction and collision avoidance. To achieve reliable target detection, the simultaneous deployment of LiDAR and camera sensors is necessary. LiDAR provides essential localization and geometric information through low-resolution point clouds, while high-resolution RGB images from cameras offer detailed appearance information. This sensor fusion approach leverages the complementary strengths of both modalities, ensuring robust performance even if one sensor fails due to adverse weather conditions. Therefore, cross-modal information fusion is crucial for enhancing 3D object detection performance.</p><p>Existing multi-modal 3D object detection methods typically employ a modal fusion strategy that combines representations from different modalities into a hybrid form. For instance, early fusion methods such as PointPainting [<xref rid="B2-sensors-25-05775" ref-type="bibr">2</xref>] integrate point cloud data by projecting them onto the output of an image semantic segmentation network. This process involves assigning category scores to each point based on the semantic information derived from the image segmentation, thereby enriching the point cloud with additional classification details. Similarly, MVP [<xref rid="B3-sensors-25-05775" ref-type="bibr">3</xref>] (multi-modal virtual point) employs multiple 2D detection networks to create dense 3D virtual point clouds, enhancing the sparse original point cloud. While these virtual points can be integrated with traditional LiDAR points, the sparsity of the data can adversely affect fusion quality, leading to semantic information loss during projection from camera to LiDAR. Moreover, minor calibration errors between sensors can further compromise this approach. Recent deep fusion methods, such as Deepfusion [<xref rid="B4-sensors-25-05775" ref-type="bibr">4</xref>] and BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>] (Bird&#8217;s Eye View), have made strides by merging image and point cloud features into a joint BEV representation. Feature fusion has always been a focus of research in autonomous driving perception. Images and point clouds are presented in different views (i.e., camera view and real 3D view), and the semantic information they contain is heterogeneous, making it difficult to find point-by point-correspondence. This semantic alignment ambiguity is one of the biggest challenges in point cloud image multimodal fusion technology.</p><p>Most current multimodal fusion strategies ignore the unique feature information of each modality, ultimately hindering further improvement in model performance. Meanwhile, it is crucial to maintain modality-specific information and enhance other modality features during modal interaction. To overcome these limitations, we build a novel modality interaction strategy for multi-modal 3D object detection. In contrast to approaches like DeepInteraction [<xref rid="B6-sensors-25-05775" ref-type="bibr">6</xref>], which utilize parallel representations for modal interaction, our core innovation involves an initial enhancement of modality-specific representations through a sequential interaction process. This is followed by a second interaction phase, where the augmented representation is utilized to derive another modality-specific augmented representation. This strategy effectively facilitates cross-modal interactions while capitalizing on the unique strengths of each modality.</p><p>Our contributions can be summarized as follows:</p><p>(1) A novel interaction framework is introduced that directly tackles the shortcomings of previous modal fusion techniques, thereby improving the robustness and accuracy of 3D object detection.</p><p>(2) Graph convolutional networks are employed to achieve effective feature alignment between the two modality-specific representations. This method enhances the interconnectivity of the features, ensuring that the complementary information from each modality is optimally integrated.</p><p>(3) A cross-attention mechanism is implemented to finalize the predictions process, ensuring that the synergistic interactions between modalities are effectively leveraged. This mechanism enhances the accuracy of 3D object detection tasks by enabling the model to focus on the most relevant information across modalities.</p></sec><sec id="sec2-sensors-25-05775"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05775"><title>2.1. Camera-Based 3D Perception</title><p>By consolidating information from multiple sources, BEV enables a more thorough understanding of the environment, thereby improving the accuracy and effectiveness of object detection in complex scenarios. There are two approaches: LSS-based explicit characterization [<xref rid="B7-sensors-25-05775" ref-type="bibr">7</xref>] and Transformer-based implicit BEV characterization [<xref rid="B8-sensors-25-05775" ref-type="bibr">8</xref>]. The Lift, Splat, Shoot (LSS) [<xref rid="B7-sensors-25-05775" ref-type="bibr">7</xref>] approach extends the Orthographic feature transform (OFT) [<xref rid="B9-sensors-25-05775" ref-type="bibr">9</xref>] by introducing latent depth distributions during the BEV feature pooling process. BEVDet [<xref rid="B10-sensors-25-05775" ref-type="bibr">10</xref>] marks a pioneering effort in combining LSS with LiDAR detector heads, utilizing LSS to extract BEV features while employing LiDAR heads to predict 3D bounding boxes. Following this, BEVDet4D [<xref rid="B11-sensors-25-05775" ref-type="bibr">11</xref>] enhances the model&#8217;s capabilities by incorporating temporal frames for velocity prediction. Meanwhile, BEVDepth [<xref rid="B12-sensors-25-05775" ref-type="bibr">12</xref>] improves depth perception by using LiDAR to generate depth truth maps, whereas BEVStereo employs temporal stereo modeling to achieve more accurate depth estimations [<xref rid="B13-sensors-25-05775" ref-type="bibr">13</xref>]. On the other hand, DETR3D [<xref rid="B14-sensors-25-05775" ref-type="bibr">14</xref>] extends the DETR framework [<xref rid="B15-sensors-25-05775" ref-type="bibr">15</xref>] for 3D applications, eliminating the need for explicit BEV feature generation and directly producing 3D bounding boxes through a Transformer-based architecture [<xref rid="B8-sensors-25-05775" ref-type="bibr">8</xref>]. PETR [<xref rid="B16-sensors-25-05775" ref-type="bibr">16</xref>] introduces a novel fusion technique, combining perspective feature maps with 3D position-embedded feature maps at an elemental level. Furthermore, BEVFormer [<xref rid="B17-sensors-25-05775" ref-type="bibr">17</xref>] employs spatial cross-attention to facilitate view transformations and temporal self-attention for effective temporal feature fusion.</p></sec><sec id="sec2dot2-sensors-25-05775"><title>2.2. LiDAR-Based 3D Perception</title><p>Voxel-based methods effectively transform irregular point clouds into 2D or 3D compact grids, which are then compressed into BEV representations. This transformation simplifies the representation of point clouds and enhances overall detection performance. Notable examples, such as VoxelNet [<xref rid="B18-sensors-25-05775" ref-type="bibr">18</xref>] and SECOND [<xref rid="B19-sensors-25-05775" ref-type="bibr">19</xref>], leverage these BEV representations, which reduce scale ambiguity and minimize occlusion. Consequently, these make them particularly effective in complex environments. In parallel, point-based methods have advanced through the development of point set deep learning techniques, including PointNet [<xref rid="B20-sensors-25-05775" ref-type="bibr">20</xref>] and other point cloud feature learning networks [<xref rid="B21-sensors-25-05775" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05775" ref-type="bibr">22</xref>]. While voxel-based detectors are recognized for their high accuracy and relatively low latency, point-based methods excel in preserving geometric information from point clouds. However, this preservation often comes at the cost of increased computational complexity.</p></sec><sec id="sec2dot3-sensors-25-05775"><title>2.3. Multi-Modal Fusion-Based 3D Perception Methods</title><p>Fusion methods can be categorized into three key types based on the timing of the fusion process: early fusion, mid-level fusion, and post-fusion. Early fusion involves integrating data from different modalities during the data preprocessing phase. This approach aims to create a unified representation before any deep feature extraction occurs. A prominent example is PointPainting [<xref rid="B2-sensors-25-05775" ref-type="bibr">2</xref>], which combines LiDAR data with image information from the outset. Similarly, PointAugmenting [<xref rid="B23-sensors-25-05775" ref-type="bibr">23</xref>] recognizes the limitations of relying solely on semantic scores. It proposes that augmenting LiDAR points with deep features extracted from a 2D object detection network, influenced by camera images, can enhance the quality of the data representation. VBRFusion [<xref rid="B24-sensors-25-05775" ref-type="bibr">24</xref>] proposes a two-branch structure that acts on the point cloud voxelization to aggregate high-dimensional features.</p><p>In contrast, mid-level fusion occurs at an intermediate stage, focusing primarily on the fusion of deep features after initial extraction. This method is often termed deep fusion and is currently regarded as one of the most promising strategies due to its ability to leverage deep learned representations. For instance, TransFusion [<xref rid="B25-sensors-25-05775" ref-type="bibr">25</xref>] defines object queries in 3D space, integrating image features directly into these proposed bounding boxes, exemplifying proposal-level fusion. ContFuse [<xref rid="B26-sensors-25-05775" ref-type="bibr">26</xref>] improves the fusion process by combining multi-scale convolutional features on a pixel-wise basis through successive convolutions. ConCs-Fusion [<xref rid="B27-sensors-25-05775" ref-type="bibr">27</xref>] utilizes a method called context clustering-based radar and camera fusion for 3D object detection. Techniques utilized by DeepFusion [<xref rid="B4-sensors-25-05775" ref-type="bibr">4</xref>], such as InverseAug and inverse rotation, further ensure precise alignment between LiDAR points and image pixels. BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>] employs a local shared attention mechanism to project image features into the BEV space, effectively linking them with LiDAR features to produce a cohesive fused representation. BEV-CFKT [<xref rid="B28-sensors-25-05775" ref-type="bibr">28</xref>], proposed in this paper, leverages knowledge transfer through Transformers for LiDAR-Camera fusion in the BEV space. RoboFusion [<xref rid="B29-sensors-25-05775" ref-type="bibr">29</xref>] utilizes the SAM (Segment Anything Model) to develop the SAM-AD for autonomous driving scenarios, improving perception performance in complex environments through an adaptive fusion mechanism. GraphBEV [<xref rid="B30-sensors-25-05775" ref-type="bibr">30</xref>] enhances the projected depth and adjacent depths from LiDAR to camera by building graph-based neighborhood information, addressing the local and global misalignment of inaccurate depth between LiDAR and camera BEV features. Fast-BEV [<xref rid="B31-sensors-25-05775" ref-type="bibr">31</xref>] uses a Fast-Ray transform to efficiently project features to multi-scale BEV. PF3Det [<xref rid="B32-sensors-25-05775" ref-type="bibr">32</xref>] combines a basic model with prompt engineering, enabling the efficient learning of perception tasks with minimal available data.</p><p>As for post-fusion, it occurs during the decision-making phase, where outputs from different modalities are combined to make final predictions. CLOCs [<xref rid="B33-sensors-25-05775" ref-type="bibr">33</xref>] exemplify this approach; however, post-fusion methods tend to work with coarser granularity and often do not fully capitalize on the complementary strengths of point cloud and image data.</p></sec></sec><sec id="sec3-sensors-25-05775"><title>3. Method</title><p>In this study, we propose an improved modal interaction framework for multi-modal 3D object detection, termed CrossInteraction. An overview of our framework is illustrated in <xref rid="sensors-25-05775-f001" ref-type="fig">Figure 1</xref>. Initially, we extract representation information separately from the LiDAR and 2D image modalities. Within the interaction encoder, we conduct two sequential modal interactions. The first interaction enhances the image representation, which is then utilized as new input to improve the LiDAR representation in the second interaction. Subsequently, the fusion encoder integrates and aligns the enhanced representations of the two modalities. Specifically, <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>c</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the images for post-interaction and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the LiDAR for post-interaction.</p><sec id="sec3dot1-sensors-25-05775"><title>3.1. Interaction Encoder</title><p>Before each modal interaction, the image modality representation <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the LiDAR BEV modal representation <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are utilized as inputs. A dense map is implied to establish a pixel-level correspondence between these two representations. The depth map provides essential information about the distance of each pixel point in the scene, enabling to map both features into the same spatial domain. This mapping facilitates precise alignment and allows for the extraction of richer, more informative features. Consequently, we will create dense mappings between the image coordinate system and the BEV coordinate system, denoted as <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>In the first modal interaction, the framework is shown in <xref rid="sensors-25-05775-f002" ref-type="fig">Figure 2</xref> and <xref rid="sensors-25-05775-f003" ref-type="fig">Figure 3</xref>. For <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> building and sampling, we begin by projecting the coordinates <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of each point in the 3D point cloud onto the multi-camera setup to create a sparse depth map, referred to as <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Then, following the depth complementation method [<xref rid="B34-sensors-25-05775" ref-type="bibr">34</xref>], we utilize this operation to obtain a dense depth map, denoted as <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Using the dense depth map, we then project each pixel in the image space into the 3D point cloud space. For a given image pixel <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with depth <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, the corresponding 3D coordinates <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are obtained. We then use these 2D coordinates <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to locate the corresponding BEV coordinates <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This mapping is formalized as <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, obtained through neighborhood sampling of size <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which results in the correspondence <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> = <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi>T</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#916;</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#916;</mml:mi><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="normal">&#916;</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#916;</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Next, we employ an attention-based feature representation interaction. Each image point is treated as a query <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, while the cross-modality neighbors <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> act as key <italic toggle="yes">K</italic> and value <italic toggle="yes">V</italic>; these are applied to achieve the cross-attention learning work:<disp-formula id="FD1-sensors-25-05775"><label>(1)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:msub><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the index of the element at position <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the 2D representation <italic toggle="yes">F</italic>. After <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:msub><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> operation we get the initial LiDAR-enhanced image interaction feature <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p><p>At the same time, we apply the local attention, as described in Equation (1), utilizing a <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> grid neighborhood as the key <italic toggle="yes">Q</italic> and value <italic toggle="yes">V</italic> for both modalities. This further facilitates the learning of intra-modal representations. For image feature representation, we denote this process as <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, through the representation integration framework, each layer outputs an enhanced image representation, which enriches the overall feature set for subsequent processing tasks.<disp-formula id="FD2-sensors-25-05775"><label>(2)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>c</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>FFN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>FFN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where FFN denotes feed-forward network, and Concat denotes concatenation operation.</p><p>In the second interaction, the framework is shown in <xref rid="sensors-25-05775-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-05775-f005" ref-type="fig">Figure 5</xref>. For <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> building and sampling, for LiDAR representations <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, given the BEV coordinates <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we first identify the LiDAR points <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that correspond to the pillars at these coordinates. Subsequently, we project these 3D points into the camera image coordinate system to obtain the corresponding pixel coordinates <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which we denote as <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Similarly, during the attention-based feature interaction process, we take a LiDAR BEV point as the query, represented as <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and determine its cross-modality neighbors <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. We apply the same mechanism outlined in Equation (1) to facilitate this second interaction, encapsulated as <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:msub><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mfenced><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, while learning the intra-modal representations, we utilize <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#981;</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> for the LiDAR feature representation. The final output of the representation integration following the second interaction is obtained as <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, which enhances the overall feature set for subsequent tasks. Here, define <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> as<disp-formula id="FD3-sensors-25-05775"><label>(3)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>FFN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>FFN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2-sensors-25-05775"><title>3.2. Fusion Encoder</title><sec id="sec3dot2dot1-sensors-25-05775"><title>3.2.1. Feature Alignment</title><p>Graph Convolutional Network Formula:<disp-formula id="FD4-sensors-25-05775"><label>(4)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">D</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">A</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">D</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the nonlinear activation function, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the node feature matrix of layer <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> indicates the node feature matrix of layer <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The matrix <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> denotes the adjacency matrix with self-connections, <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> denotes the degree matrix associated with the adjacency matrix that includes these self-connections, and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the learning weight matrix for layer <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Projection is limited by sensor errors, while attention mechanisms require massive computation. To address these issues, we use the graph convolutional network method, as shown in <xref rid="sensors-25-05775-f006" ref-type="fig">Figure 6</xref>, and we project point cloud features onto image features to identify the <italic toggle="yes">K</italic> nearest neighbors of these image features. We then perform one-to-many fusion by integrating each individual point cloud feature with its <italic toggle="yes">K</italic> neighboring image features, thereby enhancing the alignment between the two modalities.</p></sec><sec id="sec3dot2dot2-sensors-25-05775"><title>3.2.2. Feature Fusion</title><p>The fusion framework is shown in <xref rid="sensors-25-05775-f007" ref-type="fig">Figure 7</xref>. To effectively integrate features from LiDAR and camera images, we introduce a cross-attention mechanism that enhances the understanding of the interrelationships between these two modalities. In the deep feature fusion step, we first transform each LiDAR feature into a voxel representation containing multiple points, while associating the corresponding camera pixels with a polygonal region. This input data structure comprises a voxel unit and its associated <italic toggle="yes">N</italic> camera features. We design three fully-connected layers to convert the voxel features into a query module (<inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) and the camera features into a key module (<inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) and a value module (<inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), respectively. For each query (i.e., voxel unit), an attentional affinity matrix is generated by performing an inner product operation between the query module and the key module, which reflects the correlation between the <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> voxel units and their associated <italic toggle="yes">N</italic> camera features. This attentional affinity matrix is subsequently employed to weight and aggregate the <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> values, which encapsulate the camera information, by applying the softmax operator for normalization. The fused camera information is then processed through a fully connected layer and combined with the original LiDAR features. The resulting fused features are suitable for input into any standard 3D detection framework, effectively leveraging the complementary information from both modalities to enhance detection performance.</p></sec></sec></sec><sec id="sec4-sensors-25-05775"><title>4. Experiments</title><sec sec-type="intro" id="sec4dot1-sensors-25-05775"><title>4.1. Introduction of Dataset</title><p>The nuScenes dataset covers about 1.4 million camera images, 400,000 Ladar scans, and 1.4 million millimetre-wave radar scans. nuScenes selects 40,000 keyframes and annotates about 1.4 million 3D target bounding boxes, which is seven times greater than that of the KITTI dataset. The scenes are categorized into four distinct types based on environmental conditions: daytime, nighttime, sunny days, and rainy days. nuScenes support multiple tasks including detection, tracking, and BEV map segmentation. It contains driving scenarios collected in Boston and Singapore. These two cities are known for their dense traffic and challenging driving environments. Such scenes include high traffic density (e.g., intersections, construction sites), rare classes (e.g., ambulances, animals), potentially dangerous traffic situations (e.g., jaywalkers, incorrect behavior), maneuvers (e.g., lane change, turning, stopping), and difficult situations.</p><p>For evaluation purposes, nuScenes utilizes traditional metrics such as mean average precision (mAP) while also introducing the exclusive nuScenes detection score (NDS). The NDS is calculated as a weighted average of five true positive (TP) metrics: the mean translational error (mATE), which measures the 2D center distance between the predicted and true values using the Euclidean distance; the mean scale error (mASE), which quantifies the difference between threshold 1 and the 3D intersection and merger ratio; the mean angular error (mAOE), representing the smallest yaw angle difference between the predicted and true values; the mean velocity error (mAVE), which is the L2 norm of the 2D velocity difference; and the mean attribute error (mAAE), which indicates the difference between 1 and the category classification accuracy. By integrating these metrics, the NDS provides a comprehensive evaluation of the detection performance. The formula is shown below:<disp-formula id="FD5-sensors-25-05775"><label>(5)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>NDS</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="[" close="]"><mml:mn>5</mml:mn><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="double-struck">TP</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>MACs serve as a measure of computational complexity, indicating the total number of multiply&#8211;add operations required by the model to process data. A higher MAC count signifies greater model complexity and increased computational demand. Latency, on the other hand, refers to the time elapsed from the input of data to the generation of output results. Lower latency indicates a faster model response, which is particularly critical in real-time applications such as autonomous driving, where it directly impacts system responsiveness and safety.</p></sec><sec id="sec4dot2-sensors-25-05775"><title>4.2. Implementation Details</title><p>In this study, we evaluate the performance of BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>] in the integration of camera and LiDAR data for 3D object detection and BEV map segmentation, addressing both geometric and semantic challenges. Our architecture is designed to be modular, which facilitates the incorporation of additional sensor types, such as radars and event-based cameras, and allows adaptation to various 3D perception tasks, including 3D object tracking and motion prediction.</p><p>The model employs the Swin Transformer as the image processing backbone and VoxelNet for processing LiDAR data. To integrate features at multiple scales from the camera, we utilize a Feature Pyramid Network (FPN), producing a feature map that is <bold>1/8</bold> the size of the original input. The camera images are downsampled to a resolution of <bold>256 &#215; 704</bold> pixels, while the LiDAR point cloud is voxelized at resolutions of <bold>0.075</bold> m for detection tasks and <bold>0.1</bold> m for segmentation tasks. Given that detection and segmentation require different spatial coverage and feature map sizes, we implement grid sampling with bilinear interpolation before each task-specific module to facilitate the transition between various BEV feature maps.</p><p>To enhance feature alignment and ensure consistency across sensor data, we build upon cross-attention mechanisms. We conceptualize the LiDAR point cloud and camera features as nodes within a graph, establishing graph edges based on spatial correlations and feature affinities. This approach allows for a more precise and coherent integration of multi-sensor data, optimizing the performance of 3D perception tasks.</p><p>Training procedure: In contrast to existing methods that typically freeze the camera encoder [<xref rid="B2-sensors-25-05775" ref-type="bibr">2</xref>,<xref rid="B23-sensors-25-05775" ref-type="bibr">23</xref>,<xref rid="B25-sensors-25-05775" ref-type="bibr">25</xref>], we adopt an end-to-end training strategy for the entire model. To mitigate overfitting, we apply data augmentations to both the image and LiDAR modalities. The model optimization is performed using the AdamW optimizer with a weight decay of <bold><inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula></bold>.</p><p>Setup: We use the mAP across 10 foreground classes and the NDS as our detection metrics. Additionally, we measure the single-inference #MACs and latency on an RTX3090 GPU for all open-source methods. We employ a single model without any test-time augmentation for both <monospace>val</monospace> and <monospace>test</monospace> results.</p></sec><sec sec-type="results" id="sec4dot3-sensors-25-05775"><title>4.3. Results and Analysis</title><p>We achieve state-of-the-art results on the nuScenes detection benchmark, as shown in <xref rid="sensors-25-05775-t001" ref-type="table">Table 1</xref>. Compared with BEVfusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>], we achieve a 1.9% increase in mAP and a 0.9% increase in NDS, while reducing the number of MACs to <bold>249 G</bold> and measuring latency at <bold>110.9 ms</bold>. Additionally, we compare favorably against representative point-level fusion methods, such as PointPainting [<xref rid="B2-sensors-25-05775" ref-type="bibr">2</xref>] and MVP [<xref rid="B3-sensors-25-05775" ref-type="bibr">3</xref>], with about <bold>1.7</bold> &#215; speedup, <bold>1.5</bold> &#215; reduction in MACs, and <bold>4.5%</bold> increase in mAP on the <monospace>val</monospace> set. In comparison with other state-of-the-art methods [<xref rid="B35-sensors-25-05775" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05775" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05775" ref-type="bibr">37</xref>], our method can still maintain its advantages. The efficiency gains observed in our CrossInteraction method can be attributed to the dual interaction of modality features, which maximizes the utilization of camera features. This optimization allows us to maintain competitive performance with significantly lower MACs.</p><p><xref rid="sensors-25-05775-t002" ref-type="table">Table 2</xref> shows the results of BEV map segmentation, where our camera-only model demonstrates superior performance, outperforming LiDAR-only baselines by <bold>8&#8211;13%</bold>.</p><p>We further conduct a systematic analysis of CrossInteraction&#8217;s performance under varying weather and lighting conditions, as illustrated in <xref rid="sensors-25-05775-t003" ref-type="table">Table 3</xref>. Detecting objects in rainy conditions poses significant challenges for LiDAR-only models due to pronounced sensor noise. However, due to the inherent robustness of camera sensors across different weather situations, our model enhances CenterPoint by <bold>6.3</bold> mAP and exceeds BEVfusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>] by <bold>1+</bold> mAP, effectively bridging the performance gap between sunny and rainy scenarios. Lighting conditions also present challenges for all models. In daytime scenarios, we achieve a mAP exceeding <bold>70</bold>, surpassing BEVfusion by <bold>2.4</bold> mAP. The performance of camera-only models significantly deteriorates under nighttime conditions. Despite this, our multi-sensor fusion approach still exceeds BEVfusion by <bold>1.4</bold> mAP and achieves an improvement of <bold>17.1</bold> mIoU compared to CentrePoint. This gain is not only substantial but also emphasizes the importance of geometric cues in scenarios where camera sensors may underperform.</p><p>The ablation studies presented in <xref rid="sensors-25-05775-t004" ref-type="table">Table 4</xref>, <xref rid="sensors-25-05775-t005" ref-type="table">Table 5</xref>, <xref rid="sensors-25-05775-t006" ref-type="table">Table 6</xref>, <xref rid="sensors-25-05775-t007" ref-type="table">Table 7</xref> and <xref rid="sensors-25-05775-t008" ref-type="table">Table 8</xref> serve to validate our design decisions. Specifically, in <xref rid="sensors-25-05775-t004" ref-type="table">Table 4</xref>, we evaluate the effect of using different numbers of representations/modalities in decoding. We compare our multi-modal interaction framework using both representations in an alternating manner with a variant using LiDAR representation in all decoder layers. The results demonstrate that using both representations is beneficial, validating our design rationale. <xref rid="sensors-25-05775-t005" ref-type="table">Table 5</xref> highlights the effectiveness of our detection algorithm in scaling to voxel-based representations, achieving the best balance between accuracy and efficiency with a voxel size of 0.075. <xref rid="sensors-25-05775-t006" ref-type="table">Table 6</xref> shows that our model is universal and applicable to different backbone networks. Notably, the common practice to freeze the image backbone in existing multi-sensor 3D object detection research does not exploit the full potential of the camera feature extractor. <xref rid="sensors-25-05775-t007" ref-type="table">Table 7</xref> indicates that increasing the number of encoder layers up to three layers can consistently improve the performance whilst introducing negligible negative impact, validating the necessity of multi-layer encoders for multi-modal feature fusion. In <xref rid="sensors-25-05775-t008" ref-type="table">Table 8</xref>, for a fair comparison, both methods employ the same number of encoder layers as well as the same decoder. From a one-way interaction perspective, the mAP of LiDAR-enhanced camera features is lower than that of camera-enhanced LiDAR features. This discrepancy may stem from the inherent stability of LiDAR&#8217;s point cloud coordinate system, which enhances visual semantics with geometric precision. Overall, our representation interaction method is more effective than the one-way interaction approach.</p><p>In order to verify the generalization and stability of the model, we set up cross-dataset experiments, where we add the Waymo dataset to the original nuScenes dataset. The Waymo dataset is much larger than the original dataset and contains a large amount of high-quality data, especially high-frequency annotations. We let the two datasets be used as the training set and test set for experiments, respectively, and compare them with BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>] to obtain the experimental results in <xref rid="sensors-25-05775-t009" ref-type="table">Table 9</xref>. From the results, we can see from mAP that our model outperforms BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>] in terms of generalization and stability. Unlike BEVFusion&#8217;s tightly coupled fusion approach, CrossInteraction ensures the independence of LiDAR BEV features and camera panoramic features. This design preserves the advantages of each modality even when the test set distribution shifts, avoiding the information loss caused by BEVFusion&#8217;s forced unified representation. For example, when the sparsity of the LiDAR point cloud in the test set (NuScenes) is significantly higher than that in the training set (Waymo), the interaction weight of the camera features is automatically increased to compensate for the lack of geometric information, thereby reducing mAP fluctuation. Furthermore, through the <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> mapping matrices mentioned in <xref rid="sec3dot1-sensors-25-05775" ref-type="sec">Section 3.1</xref> of the article, pixel-level alignment in BEV space is established, enforcing the geometric prior trained on Waymo to adapt to the differences in sensor configuration in NuScenes (such as changes in camera perspective), reducing the drop in mAP caused by shifts.</p></sec><sec id="sec4dot4-sensors-25-05775"><title>4.4. Visualizations</title><p><xref rid="sensors-25-05775-f008" ref-type="fig">Figure 8</xref> shows the qualitative results of our multimodal representation interaction algorithm for 3D object detection and perception on the nuScenes validation set. Left is the camera image and right is LiDAR BEV. As can be seen, the camera can capture visual features such as color and texture, covering a wider field of view and providing rich semantic information, which facilitates the recognition of objects such as pedestrians and vehicles. The LiDAR BEV directly acquires 3D coordinates and depth information through laser scanning, unaffected by lighting conditions and clearly reflecting geometric features such as object shape and size. This provides important insights for future collaboration on our algorithm optimization and hardware adaptation.</p><p>As shown in <xref rid="sensors-25-05775-f009" ref-type="fig">Figure 9</xref>, it can be seen that in low-light conditions such as at night, motion blur and increased inter-frame noise can affect the effectiveness of BEV temporal fusion, weakening the ability to &#8221;visually complete&#8221; objects in distant areas.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05775"><title>5. Discussion</title><p>The fusion of multi-modal information can overcome the shortcomings of single-modality 3D object detection, thereby improving performance and accuracy. In real-world automotive systems, more robust 3D perception methods like our CrossInteraction model are expected to be further optimized and deployed within the perception modules of autonomous driving systems. This, combined with higher prediction accuracy and lower latency, could reduce the potential for accidents, thereby improving the safety and reliability of autonomous driving. However, multi-modal algorithms typically demand more computational requirements and higher operational costs to run. This raises the need for future improvements in system efficiency.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-05775"><title>6. Conclusions</title><p>In this paper, we propose an enhanced approach to 3D object detection called CrossInteraction. This method focuses on preserving and enhancing the representational information of two modalities: images and LiDAR. Specifically, it strengthens the output of the image representation to serve as the mapping input for the LiDAR data, facilitating deeper interaction through representational learning. By leveraging representational learning, CrossInteraction effectively tackles the challenge of multi-modal data fusion, where the strengths of individual modalities are often underutilized&#8212;particularly the image representation, due to the auxiliary source role processing. To overcome the problem of misaligned modal feature information, we utilize a graph convolutional network to avoid sensor accuracy errors. Additionally, pre-training with Transformers is employed to reduce overall training time, thereby improving the efficiency of the approach. Experiments demonstrate that our model achieves superior performance on both nuScenes 3D detection and BEV map segmentation tasks.</p><p>Limitations: Our multi-modal fusion components are agnostic to modality-specific representations. The explicit 2D-3D mapping relies heavily on precise sensor calibration. Alignment via graph convolutional networks (GCNs) alone may lose significant modality-specific information due to compression artifacts. Future work could integrate attention mechanisms to learn adaptive cross-modal alignment.</p><p>As shown in <xref rid="sensors-25-05775-f009" ref-type="fig">Figure 9</xref>, our method fails to mitigate camera failures under strong nighttime illumination (e.g., headlight glare). Thus, in the future, we also need to investigate if camera distortion propagates to LiDAR features via cross-modal interaction and to validate performance in extreme weather (e.g., snow, sandstorm).</p><p>Finally, processing dual-sensor data streams and maintaining multi-modal features incur significant computational overhead. Future work will develop an adaptive framework to dynamically select interaction operators, optimizing the trade-off between performance, efficiency, and robustness.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.Z.; Methodology, W.Z.; Software, W.Z.; Validation, W.Z., X.L. and Y.D.; Data curation, W.Z.; Writing&#8212;original draft, W.Z.; Writing&#8212;review and editing, X.L. and Y.D.; Supervision, X.L. and Y.D. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. NuScenes and Waymo are publicly available datasets and were used in this study. available online: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.nuscenes.org/nuscenes">https://www.nuscenes.org/nuscenes</uri> (access on 10 March 2024); available online: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://waymo.com/open/download/">https://waymo.com/open/download/</uri> (access on 10 March 2024).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05775"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><article-title>Multi-modality 3D object detection in autonomous driving: A review</article-title><source>Neurocomputing</source><year>2023</year><volume>553</volume><fpage>126587</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2023.126587</pub-id></element-citation></ref><ref id="B2-sensors-25-05775"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Helou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>PointPainting: Sequential Fusion for 3D Object Detection</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>4603</fpage><lpage>4611</lpage></element-citation></ref><ref id="B3-sensors-25-05775"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kr&#228;henb&#252;hl</surname><given-names>P.</given-names></name></person-group><article-title>Multimodal virtual point 3d detection</article-title><source>Proceedings of the NIPS &#8217;21: 35th International Conference on Neural Information Processing Systems</source><conf-loc>Red Hook, NY, USA</conf-loc><conf-date>6&#8211;14 December 2021</conf-date><fpage>16494</fpage><lpage>16507</lpage></element-citation></ref><ref id="B4-sensors-25-05775"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>A.W.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Caine</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ngiam</surname><given-names>J.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name><etal/></person-group><article-title>Deepfusion: Lidar-camera deep fusion for multi-modal 3d object detection</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>17161</fpage><lpage>17170</lpage></element-citation></ref><ref id="B5-sensors-25-05775"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Amini</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rus</surname><given-names>D.L.</given-names></name><name name-style="western"><surname>Han</surname><given-names>S.</given-names></name></person-group><article-title>Bevfusion: Multi-task multi-sensor fusion with unified bird&#8217;s-eye view representation</article-title><source>Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#8211;2 June 2023</conf-date><fpage>2774</fpage><lpage>2781</lpage></element-citation></ref><ref id="B6-sensors-25-05775"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Miao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Deepinteraction: 3d object detection via modality interaction</article-title><source>Proceedings of the NIPS &#8217;22: Proceedings of the 36th International Conference on Neural Information Processing Systems</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>28 November&#8211;9 December 2022</conf-date><fpage>1992</fpage><lpage>2005</lpage></element-citation></ref><ref id="B7-sensors-25-05775"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Philion</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fidler</surname><given-names>S.</given-names></name></person-group><article-title>Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23 August 2020</conf-date><fpage>194</fpage><lpage>210</lpage></element-citation></ref><ref id="B8-sensors-25-05775"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>L.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Proceedings of the NIPS&#8217;17: Proceedings of the 31st International Conference on Neural Information Processing Systems</source><conf-loc>Red Hook, NY, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date><fpage>6000</fpage><lpage>6010</lpage></element-citation></ref><ref id="B9-sensors-25-05775"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roddick</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>Orthographic feature transform for monocular 3d object detection</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1811.08188</pub-id><pub-id pub-id-type="doi">10.48550/arXiv.1811.08188</pub-id></element-citation></ref><ref id="B10-sensors-25-05775"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Du</surname><given-names>D.</given-names></name></person-group><article-title>Bevdet: High-performance multi-camera 3d object detection in bird-eye-view</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2112.11790</pub-id></element-citation></ref><ref id="B11-sensors-25-05775"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name></person-group><article-title>Bevdet4d: Exploit temporal cues in multi-camera 3d object detection</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2203.17054</pub-id></element-citation></ref><ref id="B12-sensors-25-05775"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Bevdepth: Acquisition of reliable depth for multi-view 3d object detection</article-title><source>Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>7&#8211;14 February 2023</conf-date><fpage>1477</fpage><lpage>1485</lpage></element-citation></ref><ref id="B13-sensors-25-05775"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Bevstereo: Enhancing depth estimation in multi-view 3d object detection with temporal stereo</article-title><source>Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence and the Twenty-Sixth Innovative Applications of Artificial Intelligence Conference and the Fifth Symposium on Educational Advances in Artificial Intelligence</source><conf-loc>Qu&#233;bec City, QC, Canada</conf-loc><conf-date>27&#8211;31 July 2014</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><publisher-name>AAAI Press</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2014</year><volume>Volume 165</volume><fpage>1486</fpage><lpage>1494</lpage></element-citation></ref><ref id="B14-sensors-25-05775"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guizilini</surname><given-names>V.C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Solomon</surname><given-names>J.</given-names></name></person-group><article-title>Detr3d: 3d object detection from multi-view images via 3d-to-2d queries</article-title><source>Proceedings of the Conference on Robot Learning, PMLR 2022</source><conf-loc>Auckland, New Zeeland</conf-loc><conf-date>14&#8211;18 December 2022</conf-date><fpage>180</fpage><lpage>191</lpage></element-citation></ref><ref id="B15-sensors-25-05775"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Carion</surname><given-names>N.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Synnaeve</surname><given-names>G.</given-names></name><name name-style="western"><surname>Usunier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name></person-group><source>End-to-end Object Detection with Transformers</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year></element-citation></ref><ref id="B16-sensors-25-05775"><label>16.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><source>Petr: Position Embedding Transformation for Multi-View 3D Object Detection</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year></element-citation></ref><ref id="B17-sensors-25-05775"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Sima</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><source>Bevformer: Learning Bird&#8217;s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year></element-citation></ref><ref id="B18-sensors-25-05775"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tuzel</surname><given-names>O.</given-names></name></person-group><article-title>Voxelnet: End-to-end learning for point cloud based 3d object detection</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>4490</fpage><lpage>4499</lpage></element-citation></ref><ref id="B19-sensors-25-05775"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>SECOND: Sparsely Embedded Convolutional Detection</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3337</elocation-id><pub-id pub-id-type="doi">10.3390/s18103337</pub-id><pub-id pub-id-type="pmid">30301196</pub-id><pub-id pub-id-type="pmcid">PMC6210968</pub-id></element-citation></ref><ref id="B20-sensors-25-05775"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Charles</surname><given-names>R.Q.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kaichun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>77</fpage><lpage>85</lpage></element-citation></ref><ref id="B21-sensors-25-05775"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Caesar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>Pointpillars: Fast encoders for object detection from point clouds</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>12689</fpage><lpage>12697</lpage></element-citation></ref><ref id="B22-sensors-25-05775"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Krahenbuhl</surname><given-names>P.</given-names></name></person-group><article-title>Center-based 3d object detection and tracking</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>11779</fpage><lpage>11788</lpage></element-citation></ref><ref id="B23-sensors-25-05775"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>PointAugmenting: Cross-Modal Augmentation for 3D Object Detection</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>11789</fpage><lpage>11798</lpage></element-citation></ref><ref id="B24-sensors-25-05775"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>W.</given-names></name></person-group><article-title>Infrastructure-assisted 3d detection networks based on camera-lidar early fusion strategy</article-title><source>Neurocomputing</source><year>2024</year><volume>600</volume><fpage>128180</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2024.128180</pub-id></element-citation></ref><ref id="B25-sensors-25-05775"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tai</surname><given-names>C.-L.</given-names></name></person-group><article-title>Transfusion: Robust lidar-camera fusion for 3d object detection with transformers</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>1080</fpage><lpage>1089</lpage></element-citation></ref><ref id="B26-sensors-25-05775"><label>26.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><source>Deep Continuous Fusion for Multi-Sensor 3D Object Detection</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2018</year></element-citation></ref><ref id="B27-sensors-25-05775"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>W.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>P.</given-names></name></person-group><article-title>ConCs-Fusion: A Context Clustering-Based Radar and Camera Fusion for Three-Dimensional Object Detection</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>5130</elocation-id><pub-id pub-id-type="doi">10.3390/rs15215130</pub-id></element-citation></ref><ref id="B28-sensors-25-05775"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>J.-G.</given-names></name></person-group><article-title>Bev-cfkt: A lidar-camera cross-modality-interaction fusion and knowledge transfer framework with transformer for bev 3d object detection</article-title><source>Neurocomputing</source><year>2024</year><volume>582</volume><fpage>127527</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2024.127527</pub-id></element-citation></ref><ref id="B29-sensors-25-05775"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Robofusion: Towards robust multi-modal 3d object detection via sam</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2401.03907</pub-id></element-citation></ref><ref id="B30-sensors-25-05775"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><source>Graphbev: Towards Robust bev Feature Alignment for Multi-Modal 3D Object Detection</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year></element-citation></ref><ref id="B31-sensors-25-05775"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name><etal/></person-group><article-title>Fast-BEV: A Fast and Strong Bird&#8217;s-Eye View Perception Baseline</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>8665</fpage><lpage>8679</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3414835</pub-id><pub-id pub-id-type="pmid">38875097</pub-id></element-citation></ref><ref id="B32-sensors-25-05775"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name></person-group><article-title>PF3Det: A Prompted Foundation Feature Assisted Visual LiDAR 3D Detector</article-title><source>Proceedings of the Computer Vision and Pattern Recognition Conference</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>11&#8211;15 June 2025</conf-date><fpage>3778</fpage><lpage>3787</lpage></element-citation></ref><ref id="B33-sensors-25-05775"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Morris</surname><given-names>D.</given-names></name><name name-style="western"><surname>Radha</surname><given-names>H.</given-names></name></person-group><article-title>CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection</article-title><source>Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>24 October 2020&#8211;24 January 2021</conf-date><fpage>10386</fpage><lpage>10393</lpage></element-citation></ref><ref id="B34-sensors-25-05775"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ku</surname><given-names>J.</given-names></name><name name-style="western"><surname>Harakeh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Waslander</surname><given-names>S.L.</given-names></name></person-group><article-title>In Defense of Classical Image Processing: Fast Depth Completion on the CPU</article-title><source>Proceedings of the 2018 15th Conference on Computer and Robot Vision (CRV)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>8&#8211;10 May 2018</conf-date><fpage>16</fpage><lpage>22</lpage></element-citation></ref><ref id="B35-sensors-25-05775"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>21674</fpage><lpage>21683</lpage></element-citation></ref><ref id="B36-sensors-25-05775"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>Fusionplanner: A multi-task motion planner for mining trucks via multi-sensor fusion</article-title><source>Mech. Syst. Sig. Process.</source><year>2024</year><volume>208</volume><fpage>111051</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2023.111051</pub-id></element-citation></ref><ref id="B37-sensors-25-05775"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Lane graph as path: Continuity-preserving path-wise modeling for online lane graph construction</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2024: 18th European Conference</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><comment>Proceedings, Part XLIV, Milan, Italy, 29 September 2024</comment><fpage>334</fpage><lpage>351</lpage></element-citation></ref><ref id="B38-sensors-25-05775"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Philion</surname><given-names>J.</given-names></name><name name-style="western"><surname>Anandkumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fidler</surname><given-names>S.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Alvarez</surname><given-names>J.M.</given-names></name></person-group><article-title>M<sup>2</sup>bev: Multi-camera joint 3d detection and segmentation with unified birds-eye view representation</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2204.05088</pub-id></element-citation></ref><ref id="B39-sensors-25-05775"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection</article-title><source>Proceedings of the 2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</source><conf-loc>Indianapolis, IN, USA</conf-loc><conf-date>19&#8211;22 September 2021</conf-date><fpage>3047</fpage><lpage>3054</lpage></element-citation></ref><ref id="B40-sensors-25-05775"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Autoalign: Pixel-instance feature aggregation for multi-modal 3d object detection</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2201.06493</pub-id></element-citation></ref><ref id="B41-sensors-25-05775"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Futr3d: A unified sensor fusion framework for 3d detection</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>172</fpage><lpage>181</lpage></element-citation></ref><ref id="B42-sensors-25-05775"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Codella</surname><given-names>N.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Cvt: Introducing convolutions to vision transformers</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>22</fpage><lpage>31</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05775-f001" orientation="portrait"><label>Figure 1</label><caption><p>CrossInteraction overall framework: LiDAR feature and camera feature are taken as inputs. Interaction encoder: enhances this information through cross-modal interactions. Fusion encoder: integrates the enhanced features from both modalities before the prediction is completed.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g001.jpg"/></fig><fig position="float" id="sensors-25-05775-f002" orientation="portrait"><label>Figure 2</label><caption><p>The first modal interaction: The LiDAR representation <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is employed to enhance the image representations <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>; this is processed by the attention machanism and we will then obtain the output <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g002.jpg"/></fig><fig position="float" id="sensors-25-05775-f003" orientation="portrait"><label>Figure 3</label><caption><p><inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> performs intra-modal representation learning a nd performs residual connection and normalization operations with the output <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>; then, we obtain the final enhanced representation <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#8217;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through FFN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g003.jpg"/></fig><fig position="float" id="sensors-25-05775-f004" orientation="portrait"><label>Figure 4</label><caption><p>The second modal interaction: further enhancement of LiDAR representations <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> using visual signals from enhanced image representations <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>c</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>; this is processed by the attention machanism and we will obtain the output <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g004.jpg"/></fig><fig position="float" id="sensors-25-05775-f005" orientation="portrait"><label>Figure 5</label><caption><p>The second modal interaction: <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> performs intra-modal representation learning and performs residual connection and normalization operations with the output <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>; then, we obtain the final enhanced representation <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>&#8217;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through FFN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g005.jpg"/></fig><fig position="float" id="sensors-25-05775-f006" orientation="portrait"><label>Figure 6</label><caption><p>Alignment Framework: (<bold>a</bold>) shows our alignment framework uses graph-based feature alignment to match more reasonable alignments between modalities. (<bold>b</bold>) shows two methods we used to project the point cloud features onto the image with one-to-one and one-to-many within alignment process, which can achieve better alignment, in order to achieve better results in the subsequent perception training tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g006.jpg"/></fig><fig position="float" id="sensors-25-05775-f007" orientation="portrait"><label>Figure 7</label><caption><p>Fusion framework: LiDAR BEV features are utilized as input to query module <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>; the camera image features are transformed into both the key module <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the value module <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g007.jpg"/></fig><fig position="float" id="sensors-25-05775-f008" orientation="portrait"><label>Figure 8</label><caption><p>Qualitative results on nuScenes val set. In the camera image (<bold>left</bold>), "pred" represents the algorithm&#8217;s prediction, "GT" represents the ground truth, blue boxes represent pedestrians, and yellow boxes represent cars. In LiDAR BEV (<bold>right</bold>), green boxes are the ground-truth and blue boxes are the predictions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g008.jpg"/></fig><fig position="float" id="sensors-25-05775-f009" orientation="portrait"><label>Figure 9</label><caption><p>Visualization of night scene recognition failure case.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05775-g009.jpg"/></fig><table-wrap position="float" id="sensors-25-05775-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t001_Table 1</object-id><label>Table 1</label><caption><p>The results of 3D Object Detection. <sup>&#8224;</sup>: w/ test-time augmentation (TTA); <sup>&#8225;</sup>: w/ model ensemble and TTA; &#8216;L&#8217; and &#8216;C&#8217; represent LiDAR and camera, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Modality</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP (<italic toggle="yes">test</italic>)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NDS (<italic toggle="yes">test</italic>)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP (<italic toggle="yes">val</italic>)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NDS (<italic toggle="yes">val</italic>)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MACs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEVDet [<xref rid="B10-sensors-25-05775" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">42.2 <sup>&#8224;</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">48.2 <sup>&#8224;</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">M<sup>2</sup>BEV [<xref rid="B38-sensors-25-05775" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">42.9</td><td align="center" valign="middle" rowspan="1" colspan="1">47.4</td><td align="center" valign="middle" rowspan="1" colspan="1">41.7</td><td align="center" valign="middle" rowspan="1" colspan="1">47.0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEVFormer [<xref rid="B17-sensors-25-05775" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">44.5</td><td align="center" valign="middle" rowspan="1" colspan="1">53.5</td><td align="center" valign="middle" rowspan="1" colspan="1">41.6</td><td align="center" valign="middle" rowspan="1" colspan="1">51.7</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEVDet4D [<xref rid="B11-sensors-25-05775" ref-type="bibr">11</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.1 <sup>&#8224;</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.9 <sup>&#8224;</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PointPillars [<xref rid="B21-sensors-25-05775" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">52.3</td><td align="center" valign="middle" rowspan="1" colspan="1">61.3</td><td align="center" valign="middle" rowspan="1" colspan="1">65.5</td><td align="center" valign="middle" rowspan="1" colspan="1">34.4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SECOND [<xref rid="B19-sensors-25-05775" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">52.8</td><td align="center" valign="middle" rowspan="1" colspan="1">63.3</td><td align="center" valign="middle" rowspan="1" colspan="1">52.6</td><td align="center" valign="middle" rowspan="1" colspan="1">63.0</td><td align="center" valign="middle" rowspan="1" colspan="1">85.0</td><td align="center" valign="middle" rowspan="1" colspan="1">69.8</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CenterPoint [<xref rid="B22-sensors-25-05775" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">153.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PointPainting [<xref rid="B2-sensors-25-05775" ref-type="bibr">2</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">65.8</td><td align="center" valign="middle" rowspan="1" colspan="1">69.6</td><td align="center" valign="middle" rowspan="1" colspan="1">370.0</td><td align="center" valign="middle" rowspan="1" colspan="1">185.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PointAugmenting [<xref rid="B23-sensors-25-05775" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">66.8 <sup>&#8224;</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">71.0 <sup>&#8224;</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">408.5</td><td align="center" valign="middle" rowspan="1" colspan="1">234.4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MVP [<xref rid="B3-sensors-25-05775" ref-type="bibr">3</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">66.4</td><td align="center" valign="middle" rowspan="1" colspan="1">70.5</td><td align="center" valign="middle" rowspan="1" colspan="1">66.1</td><td align="center" valign="middle" rowspan="1" colspan="1">70.0</td><td align="center" valign="middle" rowspan="1" colspan="1">371.7</td><td align="center" valign="middle" rowspan="1" colspan="1">187.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FusionPainting [<xref rid="B39-sensors-25-05775" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">68.1</td><td align="center" valign="middle" rowspan="1" colspan="1">71.6</td><td align="center" valign="middle" rowspan="1" colspan="1">66.5</td><td align="center" valign="middle" rowspan="1" colspan="1">70.7</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AutoAlign [<xref rid="B40-sensors-25-05775" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">66.6</td><td align="center" valign="middle" rowspan="1" colspan="1">71.1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FUTR3D [<xref rid="B41-sensors-25-05775" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">64.5</td><td align="center" valign="middle" rowspan="1" colspan="1">68.3</td><td align="center" valign="middle" rowspan="1" colspan="1">1069.0</td><td align="center" valign="middle" rowspan="1" colspan="1">321.4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TransFusion [<xref rid="B25-sensors-25-05775" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">68.9</td><td align="center" valign="middle" rowspan="1" colspan="1">71.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.5</td><td align="center" valign="middle" rowspan="1" colspan="1">71.3</td><td align="center" valign="middle" rowspan="1" colspan="1">485.8</td><td align="center" valign="middle" rowspan="1" colspan="1">156.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEVfusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>70.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>72.9</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>68.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>253.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>119.2</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VoxelNeXt [<xref rid="B35-sensors-25-05775" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>70.3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>72.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>68.6</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>777.3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>234.2</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FusionPlanner [<xref rid="B36-sensors-25-05775" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>69.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.9</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>67.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>70.3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>459.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>210.4</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LaneGAP [<xref rid="B37-sensors-25-05775" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>70.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>68.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>70.9</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>542.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>220.1</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>73.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>70.3</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>249.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>110.9</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VoxelNeXt [<xref rid="B35-sensors-25-05775" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>74.4</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>75.4</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FusionPlanner [<xref rid="B36-sensors-25-05775" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>74.6</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>75.9</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LaneGAP [<xref rid="B37-sensors-25-05775" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>74.2</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>76.0</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEVfusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>75.0</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>76.1</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>73.7</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>74.9</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>76.3</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>77.4</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>74.2</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>76.3</bold>
<sup>&#8225;</sup>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t002_Table 2</object-id><label>Table 2</label><caption><p>The results of BEV map segmentation. &#8216;L&#8217; and &#8216;C&#8217; represent LiDAR and camera, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Modality</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Drivable</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ped. Cross.</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Walkway</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Stop Line</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Carpark</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Divider</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Mean</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">OFT [<xref rid="B9-sensors-25-05775" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">74.0</td><td align="center" valign="middle" rowspan="1" colspan="1">35.3</td><td align="center" valign="middle" rowspan="1" colspan="1">45.9</td><td align="center" valign="middle" rowspan="1" colspan="1">27.5</td><td align="center" valign="middle" rowspan="1" colspan="1">35.9</td><td align="center" valign="middle" rowspan="1" colspan="1">33.9</td><td align="center" valign="middle" rowspan="1" colspan="1">42.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LSS [<xref rid="B7-sensors-25-05775" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">75.4</td><td align="center" valign="middle" rowspan="1" colspan="1">38.8</td><td align="center" valign="middle" rowspan="1" colspan="1">46.3</td><td align="center" valign="middle" rowspan="1" colspan="1">30.3</td><td align="center" valign="middle" rowspan="1" colspan="1">39.1</td><td align="center" valign="middle" rowspan="1" colspan="1">36.5</td><td align="center" valign="middle" rowspan="1" colspan="1">44.4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CVT [<xref rid="B42-sensors-25-05775" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">74.3</td><td align="center" valign="middle" rowspan="1" colspan="1">36.8</td><td align="center" valign="middle" rowspan="1" colspan="1">39.9</td><td align="center" valign="middle" rowspan="1" colspan="1">25.8</td><td align="center" valign="middle" rowspan="1" colspan="1">35.0</td><td align="center" valign="middle" rowspan="1" colspan="1">29.4</td><td align="center" valign="middle" rowspan="1" colspan="1">40.2</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M<sup>2</sup>BEV [<xref rid="B38-sensors-25-05775" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PointPillars [<xref rid="B21-sensors-25-05775" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">72.0</td><td align="center" valign="middle" rowspan="1" colspan="1">43.1</td><td align="center" valign="middle" rowspan="1" colspan="1">53.1</td><td align="center" valign="middle" rowspan="1" colspan="1">29.7</td><td align="center" valign="middle" rowspan="1" colspan="1">27.7</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5</td><td align="center" valign="middle" rowspan="1" colspan="1">43.8</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CenterPoint [<xref rid="B22-sensors-25-05775" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PointPainting [<xref rid="B2-sensors-25-05775" ref-type="bibr">2</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">75.9</td><td align="center" valign="middle" rowspan="1" colspan="1">48.5</td><td align="center" valign="middle" rowspan="1" colspan="1">57.1</td><td align="center" valign="middle" rowspan="1" colspan="1">36.9</td><td align="center" valign="middle" rowspan="1" colspan="1">34.5</td><td align="center" valign="middle" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" rowspan="1" colspan="1">49.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>85.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>60.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>67.6</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>52.0</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>57.0</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>53.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>62.7</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VoxelNeXt [<xref rid="B35-sensors-25-05775" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>85.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>57.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>63.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>53.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>55.9</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>54.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>61.7</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LaneGAP [<xref rid="B37-sensors-25-05775" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>82.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>50.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>59.0</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>52.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>54.6</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>52.0</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>58.4</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>62.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>68.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>53.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>59.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>54.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>64.7</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t003_Table 3</object-id><label>Table 3</label><caption><p>The results of robust experiment. &#8216;L&#8217; and &#8216;C&#8217; represent LiDAR and camera, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Sunny</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Rainy</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Day</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Night</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Modality
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mAP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mIoU
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mAP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mIoU
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mAP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mIoU
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mAP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mIoU
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CenterPoint [<xref rid="B22-sensors-25-05775" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">62.9</td><td align="center" valign="middle" rowspan="1" colspan="1">50.7</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">59.2</td><td align="center" valign="middle" rowspan="1" colspan="1">42.3</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">62.8</td><td align="center" valign="middle" rowspan="1" colspan="1">48.9</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">35.4</td><td align="center" valign="middle" rowspan="1" colspan="1">37.0</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEVDet [<xref rid="B10-sensors-25-05775" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.0</td><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.5</td><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.4</td><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MVP [<xref rid="B3-sensors-25-05775" ref-type="bibr">3</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">65.9 (+3.0)</td><td align="center" valign="middle" rowspan="1" colspan="1">51.0 (&#8722;8.0)</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">66.3 (+7.1)</td><td align="center" valign="middle" rowspan="1" colspan="1">42.9 (&#8722;7.6)</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">66.3 (+3.5)</td><td align="center" valign="middle" rowspan="1" colspan="1">49.2 (&#8722;8.2)</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">38.4 (+3.0)</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5 (+6.7)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" rowspan="1" colspan="1">68.2 (+5.3)</td><td align="center" valign="middle" rowspan="1" colspan="1">65.6 (+6.6)</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">69.9 (+10.7)</td><td align="center" valign="middle" rowspan="1" colspan="1">55.9 (+5.4)</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">68.5(+5.7)</td><td align="center" valign="middle" rowspan="1" colspan="1">63.1 (+5.7)</td><th align="center" valign="middle" rowspan="1" colspan="1">
</th><td align="center" valign="middle" rowspan="1" colspan="1">42.8(+7.4)</td><td align="center" valign="middle" rowspan="1" colspan="1">43.6 (+12.8)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.2 (+6.3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.9 (+9.9)</td><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.0 (+12.8)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.3 (+7.6)</td><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.9(+8.1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.2 (+7.8)</td><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.2(+8.8)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.9 (+17.1)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t004_Table 4</object-id><label>Table 4</label><caption><p>Ablation experiments and default settings (modality). &#8216;L&#8217; and &#8216;C&#8217; represent LiDAR and camera, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NDS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">58.4</td><td align="center" valign="middle" rowspan="1" colspan="1">66.7</td><td align="center" valign="middle" rowspan="1" colspan="1">57.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C+L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>64.7</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation experiments and default settings (voxel size).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">(Meters)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NDS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.075</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>72.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>72.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>64.7</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" rowspan="1" colspan="1">67.5</td><td align="center" valign="middle" rowspan="1" colspan="1">71.5</td><td align="center" valign="middle" rowspan="1" colspan="1">64.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.4</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t006_Table 6</object-id><label>Table 6</label><caption><p>Ablation experiments and default settings (feature interaction backbone).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NDS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">67.1</td><td align="center" valign="middle" rowspan="1" colspan="1">68.7</td><td align="center" valign="middle" rowspan="1" colspan="1">60.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SwinT (freeze)</td><td align="center" valign="middle" rowspan="1" colspan="1">66.3</td><td align="center" valign="middle" rowspan="1" colspan="1">71.2</td><td align="center" valign="middle" rowspan="1" colspan="1">54.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SwinT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>64.7</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t007_Table 7</object-id><label>Table 7</label><caption><p>Ablation experiments and default settings (interaction encoder layers).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NDS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">66.3</td><td align="center" valign="middle" rowspan="1" colspan="1">70.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">67.5</td><td align="center" valign="middle" rowspan="1" colspan="1">71.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>69.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.7</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t008_Table 8</object-id><label>Table 8</label><caption><p>Ablation experiments and default settings (each modal interaction).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NDS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">68.3</td><td align="center" valign="middle" rowspan="1" colspan="1">70.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">69.5</td><td align="center" valign="middle" rowspan="1" colspan="1">71.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> + <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.2</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05775-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05775-t009_Table 9</object-id><label>Table 9</label><caption><p>Cross-dataset test experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP (nuScenes&#8594;Waymo)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP (Waymo&#8594;nuScenes)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">BEVFusion [<xref rid="B5-sensors-25-05775" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">67.3</td><td align="center" valign="middle" rowspan="1" colspan="1">47.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>56.9</bold>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>