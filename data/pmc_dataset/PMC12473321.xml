<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12473321</article-id><article-id pub-id-type="pmcid-ver">PMC12473321.1</article-id><article-id pub-id-type="pmcaid">12473321</article-id><article-id pub-id-type="pmcaiid">12473321</article-id><article-id pub-id-type="pmid">41012903</article-id><article-id pub-id-type="doi">10.3390/s25185664</article-id><article-id pub-id-type="publisher-id">sensors-25-05664</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An Integrated and Robust Vision System for Internal and External Thread Defect Detection with Adversarial Defense</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Fu</surname><given-names initials="L">Liu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="fn1-sensors-25-05664" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="L">Leqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="fn1-sensors-25-05664" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="G">Gengpei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jiang</surname><given-names initials="Z">Zhihao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="c1-sensors-25-05664" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Doulamis</surname><given-names initials="A">Anastasios</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05664">The School of Electronic Information and Electrical Engineering, Yangtze University, East Campus, Jingzhou 434100, China; <email>fliu68596@gmail.com</email> (L.F.); <email>lileqi2023@outlook.com</email> (L.L.); <email>judgebill@126.com</email> (G.Z.)</aff><author-notes><corresp id="c1-sensors-25-05664"><label>*</label>Correspondence: <email>zhihaojiang800@gmail.com</email></corresp><fn id="fn1-sensors-25-05664"><label>&#8224;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>11</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>18</issue><issue-id pub-id-type="pmc-issue-id">497667</issue-id><elocation-id>5664</elocation-id><history><date date-type="received"><day>12</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>30</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>09</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>11</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>27</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-28 18:25:14.327"><day>28</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05664.pdf"/><abstract><p>In industrial automation, detecting defects in threaded components is challenging due to their complex geometry and the concealment of micro-flaws. This paper presents an integrated vision system capable of inspecting both internal and external threads with high robustness. A unified imaging platform ensures synchronized capture of thread surfaces, while advanced image enhancement techniques improve clarity under motion blur and low-light conditions. To overcome limited defect samples, we introduce a generative data augmentation strategy that diversifies training data. For detection, a lightweight and optimized deep learning model achieves higher precision and efficiency compared with existing YOLO variants. Moreover, we design a dual-defense mechanism that effectively mitigates stealthy adversarial perturbations, such as alpha channel attacks, preserving system reliability. Experimental results demonstrate that the proposed framework delivers accurate, secure, and efficient thread defect detection, offering a practical pathway toward reliable industrial vision systems.</p></abstract><kwd-group><kwd>thread defect detection</kwd><kwd>lightweight neural network</kwd><kwd>alpha channel attack</kwd><kwd>YOLO-based optimization</kwd><kwd>image data augmentation</kwd></kwd-group><funding-group><funding-statement>This research was not funded by any external sources.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05664"><title>1. Introduction</title><p>Threaded components are fundamental units widely used in modern industrial equipment for connection and power transmission, playing crucial roles across key sectors such as machinery, automotive, aerospace, energy, and rail transportation. In various assembly structures, threads are not only responsible for mechanical connection and sealing alignment but are also directly related to the overall structural reliability and safety. Due to their complex geometric structure and stringent machining precision requirements, even minute defects&#8212;such as broken threads, burrs, cracks, and corrosion&#8212;can lead to loosening, sealing failure, or even catastrophic mechanical breakdowns. Therefore, effective defect detection for industrial threaded parts is vital for ensuring product quality, improving assembly consistency, and reducing equipment failure rates.</p><p>Beyond the most primitive visual inspection, traditional defect detection methods for threads largely rely on contact-based measurements, such as thread gauges (e.g., plug and ring gauges), calipers, micrometers, and profilometers with mechanical probes [<xref rid="B1-sensors-25-05664" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05664" ref-type="bibr">2</xref>]. These techniques involve manual or semi-automatic operations to identify issues like dimensional deviations or machining defects. However, they suffer from low efficiency, reliance on operator experience, and difficulty detecting micro or structural defects&#8212;especially in reflective surfaces, deep cavities, or mass production settings&#8212;making them inadequate for modern manufacturing demands of high precision, efficiency, and automation.</p><p>In recent years, non-contact inspection technologies for industrial components have seen substantial advancements, becoming mainstream due to their non-destructive, automated, and high-accuracy advantages. These include computer vision inspection [<xref rid="B3-sensors-25-05664" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05664" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05664" ref-type="bibr">5</xref>], laser scanning [<xref rid="B6-sensors-25-05664" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05664" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05664" ref-type="bibr">8</xref>], X-ray [<xref rid="B9-sensors-25-05664" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05664" ref-type="bibr">10</xref>], and optical techniques [<xref rid="B11-sensors-25-05664" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05664" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05664" ref-type="bibr">13</xref>]. Among them, computer vision-based defect detection stands out for its lightweight nature, easy deployment, high accuracy, and low cost, making it the most general and extensible solution within non-contact inspection methods.</p><p>Image processing is the foundational technology in computer vision and has been widely adopted across visual inspection tasks. Notably, dynamic image deblurring has made significant progress through deep learning. Jang et al. (2025) [<xref rid="B14-sensors-25-05664" ref-type="bibr">14</xref>] proposed DSANet, a deep supervision attention network leveraging a ConvLSTM encoder-decoder and frequency-domain constraints for precise recovery of blurred regions. Ren et al. (2022) [<xref rid="B15-sensors-25-05664" ref-type="bibr">15</xref>] introduced a spatially variant neural network for dynamic scene blur, combining CNN and RNN to better model complex blur patterns. Gao et al. (2019) [<xref rid="B16-sensors-25-05664" ref-type="bibr">16</xref>] presented a parameter-sharing network with nested skip connections and released a benchmark dataset for dynamic deblurring. Zhang et al. (2023) [<xref rid="B17-sensors-25-05664" ref-type="bibr">17</xref>] adopted a flow-guided multi-scale RNN to improve fine detail recovery, while Chen et al. (2023) [<xref rid="B18-sensors-25-05664" ref-type="bibr">18</xref>] proposed a CNN&#8211;Transformer hybrid model using stripe attention and cross-layer feature fusion, achieving state-of-the-art performance on multiple benchmarks.</p><p>Illumination normalization, a critical step in image preprocessing, enhances robustness under suboptimal lighting for tasks such as enhancement, detection, and recognition. Vasluianu et al. (2024) [<xref rid="B19-sensors-25-05664" ref-type="bibr">19</xref>] introduced Ambient Lighting Normalization (ALN) and a corresponding dataset Ambient6K using frequency-domain fusion to restore shadowed regions. Dias Da Cruz et al. (2020) [<xref rid="B20-sensors-25-05664" ref-type="bibr">20</xref>] proposed a learning framework with partially unachievable autoencoder objectives for better illumination-invariant representation. Huang et al. (2023) [<xref rid="B21-sensors-25-05664" ref-type="bibr">21</xref>] proposed Transition-Constant Normalization (TCN) for stable enhancement under varying exposures. Rad et al. (2020) [<xref rid="B22-sensors-25-05664" ref-type="bibr">22</xref>] developed Adaptive Local Contrast Normalization (ALCN), dynamically predicting normalization parameters to boost recognition in complex lighting. Goswami (2020) [<xref rid="B23-sensors-25-05664" ref-type="bibr">23</xref>] designed a deployable deep method for correcting uneven lighting in RGB images, showing good generalization in real-world scenarios.</p><p>With the advancement of deep learning, many vision-based defect detection systems have integrated deep neural networks to improve recognition accuracy and robustness in complex settings. Jiang et al. (2024) [<xref rid="B24-sensors-25-05664" ref-type="bibr">24</xref>] combined GAN and YOLO for generating and detecting internal thread defects, achieving 94.27% and 93.92% accuracy for internal and external threads, respectively. Dou et al. (2024) [<xref rid="B25-sensors-25-05664" ref-type="bibr">25</xref>] developed a multi-camera inspection system incorporating lighting optimization and cylindrical image stitching, enabling efficient and visual thread defect localization. Xu et al. (2023) [<xref rid="B26-sensors-25-05664" ref-type="bibr">26</xref>] proposed an enhanced YOLOv5 for bearing defect detection using C2f, SPD modules, and CARAFE upsampling, achieving 97.3% mAP and 100 FPS. Wu et al. (2025) [<xref rid="B27-sensors-25-05664" ref-type="bibr">27</xref>] introduced RBS-YOLO, a lightweight version of YOLOv5 for casting defects, balancing accuracy and complexity. Patil (2024) [<xref rid="B28-sensors-25-05664" ref-type="bibr">28</xref>] compared YOLOv5 and YOLOv8 for nut thread presence detection, highlighting YOLOv8&#8242;s superior speed and accuracy. Lang et al. (2022) [<xref rid="B29-sensors-25-05664" ref-type="bibr">29</xref>] proposed MR-YOLO by integrating MobileNetV3, SE attention, and Mosaic augmentation, improving efficiency and accuracy. Tabernik et al. (2019) [<xref rid="B30-sensors-25-05664" ref-type="bibr">30</xref>] designed a DNN-based segmentation model with few-shot learning support. Wang et al. (2022) [<xref rid="B31-sensors-25-05664" ref-type="bibr">31</xref>] introduced Defect Transformer (DefT), a hybrid CNN&#8211;Transformer model that captures both local details and global semantics, enhancing detection robustness.</p><p>In the broader field of adversarial machine learning, numerous studies have revealed the vulnerability of deep neural networks. Classical gradient-based methods, such as FGSM and PGD [<xref rid="B32-sensors-25-05664" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05664" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05664" ref-type="bibr">34</xref>], can mislead classifiers by applying subtle perturbations at the pixel level. Patch attacks [<xref rid="B35-sensors-25-05664" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05664" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05664" ref-type="bibr">37</xref>] introduce locally high-contrast patterns into images, digitally or physically occluding critical regions, while color channel perturbation methods [<xref rid="B38-sensors-25-05664" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05664" ref-type="bibr">39</xref>] create global distortions by altering the ratios of RGB channels. These attacks have been shown to cause significant drops in detection and classification accuracy, highlighting the lack of robustness in current visual recognition systems. However, most existing research has either focused on generic computer vision benchmark datasets or explored only limited types of perturbations, with little attention paid to the specific requirements of industrial visual inspection. In particular, very few studies have investigated highly covert and practically deployable threats&#8212;alpha channel attacks [<xref rid="B40-sensors-25-05664" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05664" ref-type="bibr">41</xref>]. As a representative form of implicit perturbation, alpha channel attacks can inject imperceptible interference signals through the transparency channel of an image, substantially degrading model perception without altering the image as observed by the human eye. Since most industrial detection models by default process RGBA inputs without stripping the transparency channel, this type of attack is both easy to deploy and difficult to detect, making it a critical security threat to industrial vision systems. In high-security applications such as automatic thread inspection and screening, any form of image perturbation that is not promptly identified may lead to downstream assembly precision errors or failures in quality control. Therefore, developing detection frameworks with adversarial defense capabilities is not only essential for enhancing system robustness but also constitutes a fundamental security infrastructure in the advancement of industrial intelligence.</p><p>This study extends prior work in small-object detection, multi-scale adaptability, and edge-device deployment while introducing a security-aware perspective to strengthen real-world robustness. The main contributions are as follows:<list list-type="order"><list-item><p>A dual-mode industrial image acquisition setup is constructed for internal and external threads, integrating fisheye lenses and HD cameras to solve structural complexity and switching inefficiencies in traditional systems.</p></list-item><list-item><p>MLWNet and DarkIR are employed for dynamic deblurring and illumination normalization, ensuring high-quality inputs. A residual diffusion denoising model (RDDM) is introduced for generating and augmenting thread defect samples.</p></list-item><list-item><p>A novel detection model, SLF-YOLO, is developed by integrating SC_C2f, Light-SSF_Neck, and FIMetal-IoU loss, outperforming YOLOv5s to YOLOv10s while remaining suitable for real-time edge deployment.</p></list-item><list-item><p>A defense mechanism, the Histogram&#8211;MSE Defense Model (HMDM), is proposed to counter alpha channel attacks. By combining histogram overlap analysis with MSE-based detection, HMDM effectively identifies and mitigates input-level adversarial perturbations, enhancing the robustness and security of the system.</p></list-item></list></p><p>The remainder of this paper is organized as follows. <xref rid="sec2-sensors-25-05664" ref-type="sec">Section 2</xref> presents the overall principle and architecture of the proposed integrated vision system, including image acquisition, preprocessing, data augmentation, defect detection, and adversarial defense mechanisms. <xref rid="sec3-sensors-25-05664" ref-type="sec">Section 3</xref> describes the experimental setup and evaluates the performance of the system in terms of image enhancement, defect detection accuracy, robustness against adversarial perturbations, and comparative analysis with baseline models. <xref rid="sec4-sensors-25-05664" ref-type="sec">Section 4</xref> discusses the contributions of individual modules, analyzes ablation studies, and further investigates the internal mechanisms and practical implications of adversarial perturbations. Finally, <xref rid="sec5-sensors-25-05664" ref-type="sec">Section 5</xref> concludes the paper by summarizing the main findings and outlining potential directions for future research.</p></sec><sec id="sec2-sensors-25-05664"><title>2. Principle and System Overview</title><p><xref rid="sensors-25-05664-f001" ref-type="fig">Figure 1</xref> illustrates the overall processing workflow of the proposed internal and external thread defect detection system. From the initial data acquisition stage, the system integrates multi-level image enhancement and security defense mechanisms to improve model robustness and detection stability under complex industrial interference conditions.</p><p>The system begins by acquiring multi-source input data from industrial environments, which includes both normal images and potential adversarial samples. To counteract threats such as alpha channel attacks, transparent padding interference, and adversarial patches, the input data is first processed by a lightweight security defense module. This module performs perturbation suppression, anomaly filtering, and input resizing to preliminarily eliminate explicit attack characteristics and prevent malicious samples from entering the core model pipeline.</p><p>Next, the data flows into the image preprocessing submodule, which consists of two processing paths:<list list-type="simple"><list-item><label>(1)</label><p>Dynamic deblurring, designed to mitigate motion blur caused by device vibration or camera instability, thereby enhancing the visibility of thread edges and defect boundaries;</p></list-item><list-item><label>(2)</label><p>Low-light enhancement, targeted at restoring image quality in dark cavities such as internal threads, utilizing brightness normalization and edge detail enhancement to improve model perception.</p></list-item></list></p><p>The preprocessed images are then passed to the Image Data Enhancement module, where a residual diffusion denoising model (RDDM) is employed for defect diversity modeling and synthetic data generation. This enhances the model&#8217;s generalization capability and defect coverage under limited-sample conditions.</p><p>The enhanced image data is subsequently fed into a deep object detection network for defect identification and localization. The detection results are also fed back into a front-end security monitoring module, enabling output-based anomaly detection. For instance, abrupt changes in the number of bounding boxes or unusual clustering of defect categories can trigger an alarm or pause the model response, thus forming a closed-loop industrial vision security chain with perception, diagnosis, and response capabilities.</p><p>Overall, the proposed workflow not only ensures high detection accuracy but also integrates a three-stage defense pipeline&#8212;pre-processing, mid-processing, and post-processing&#8212;offering comprehensive protection against adversarial perturbations, transparent padding, and real-world industrial interference. This design ensures strong industrial adaptability and controllable system security.</p><sec id="sec2dot1-sensors-25-05664"><title>2.1. Image Acquisition</title><p>As the first and foundational stage in the internal and external thread defect detection pipeline, the quality, viewpoint completeness, and spatial accuracy of thread image acquisition directly determine the upper performance limits of subsequent feature extraction and object recognition algorithms. To obtain high-fidelity, full-coverage, and unobstructed image inputs, this study designs a unified image acquisition system tailored to industrial field applications, capable of handling both internal and external threads.</p><p>In conventional industrial inspection systems, internal and external threads are typically imaged using separate devices and workflows due to their distinct structural positions: external threads are usually captured via multi-camera setups arranged around the object, while internal threads require endoscopic probes to access deep cavities. These differences in installation, illumination strategies, and imaging paths result in complex hardware configurations, high switching costs, and low efficiency in batch inspections.</p><p>To address these challenges, we propose an integrated image acquisition architecture for industrial thread defect detection, as illustrated in <xref rid="sensors-25-05664-f002" ref-type="fig">Figure 2</xref>. The system is constructed around the multi-angle structural characteristics of threaded components, incorporating independent subsystems for internal and external thread image capture. These subsystems are synchronized using stepper motors, transmission mechanisms, and an embedded control platform to achieve precise, coordinated acquisition of dynamic thread targets.</p><p>The workpiece is fixed on a central fixture driven by a lead screw mechanism powered by a stepper motor, enabling linear axial movement. The displacement of the motor and the image acquisition signals are orchestrated by a Raspberry Pi-based control unit, ensuring closed-loop synchronization of image triggering, motion control, and data transmission.</p><p>For internal thread imaging, the system employs a fisheye lens group combined with an LED ring light source, enabling wide-angle imaging and uniform circumferential illumination within the cavity. This setup effectively mitigates the challenges of light-shadow blind spots and angle occlusion along the thread&#8217;s inner wall. For external thread imaging, a high-definition industrial camera coupled with an LED panel light source is used to achieve full circumferential coverage of the outer surface with parallel illumination, suitable for rod-like components such as screws and spindles.</p><p>Both subsystems are connected to the main control platform via the fixture linkage structure. The acquired images are transmitted in real time through the Raspberry Pi to a backend detection host, where the defect recognition network operates. Thanks to this dual-subsystem collaborative design, the platform supports unified, adjustable, and multi-angle thread image capture, forming a stable data foundation for high-precision vision-based defect detection. The hardware structure is depicted in <xref rid="sensors-25-05664-f003" ref-type="fig">Figure 3</xref>.</p><p>The combination of dual imaging subsystems with a central lead screw lifting platform and multi-angle mounting modules allows for the simultaneous and integrated acquisition of both internal and external wall images on a single device. This approach not only improves assembly consistency and reduces platform complexity but also ensures spatial-temporal alignment and structural consistency of the images. As a result, the system provides standardized input sources for downstream defect detection models, significantly enhancing overall detection accuracy, system stability, and industrial deployability. Detailed component illustrations are shown in <xref rid="sensors-25-05664-f004" ref-type="fig">Figure 4</xref>.</p></sec><sec id="sec2dot2-sensors-25-05664"><title>2.2. Dynamic Deblurring</title><p>In industrial visual inspection scenarios, image blur is a prevalent issue&#8212;particularly in regions with complex geometries such as metallic threads and tubular cavities. This type of degradation often arises from equipment vibration, insufficient exposure, or motion-induced defocus, leading to pronounced directional motion blur. Such blur is typically accompanied by attenuation of high-frequency textures, edge smearing, and structural distortion, which severely compromises image clarity and limits its usability in defect detection, depth estimation, and 3D modeling tasks. To address this, a dynamic deblurring module is introduced at the image preprocessing stage, forming a comprehensive image enhancement pipeline in conjunction with the illumination normalization module.</p><p>We adopt MLWNet (Multi-scale Network with Learnable Wavelet Transform) [<xref rid="B42-sensors-25-05664" ref-type="bibr">42</xref>] as the backbone of the dynamic deblurring module. MLWNet incorporates learnable two-dimensional discrete wavelet transform (2D-LDWT) and a multi-scale semantic fusion mechanism to effectively capture blur features across different scales and orientations. Unlike conventional spatial-domain networks, MLWNet introduces frequency modeling during the feature extraction stage, with a specific focus on restoring high-frequency details and edge structures that are severely degraded by motion blur.</p><p>At the modeling level, wavelet transform decomposes an image <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mfenced><mml:mi mathvariant="normal">t</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> into a low-frequency approximation term and multiple high-frequency directional components:<disp-formula id="FD1-sensors-25-05664"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mfenced><mml:mi mathvariant="normal">t</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi mathvariant="normal">j</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>&#968;</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mi mathvariant="normal">t</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="normal">c</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">j</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">&#981;</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">j</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mi mathvariant="normal">t</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the high-frequency detail coefficients, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">c</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">j</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the low-frequency approximation coefficients. This decomposition provides multi-scale frequency resolution capabilities.</p><p>In the network, high-pass and low-pass filters <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mover><mml:mi mathvariant="normal">a</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mover><mml:mi mathvariant="normal">a</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are used to perform recursive convolution operations, resulting in four sets of 2D filtered wavelet components&#8212;LL, LH, HL, and HH&#8212;which are concatenated to form a four-channel wavelet convolution kernel <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To ensure reversibility and energy conservation during both the forward and inverse wavelet transformations, Perfect Reconstruction Constraints are introduced:<disp-formula id="FD2-sensors-25-05664"><label>(2)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="normal">z</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mfenced><mml:mi mathvariant="normal">z</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">A</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="normal">z</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mi mathvariant="normal">z</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#8195;</mml:mo><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mfenced><mml:mi mathvariant="normal">z</mml:mi></mml:mfenced><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mfenced><mml:mi mathvariant="normal">z</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">A</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mi mathvariant="normal">z</mml:mi></mml:mfenced><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mi mathvariant="normal">z</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In terms of architecture, the input image is first processed through multiple Simple Encoder Blocks (SEBs) to extract shallow features and perform multi-scale downsampling. The central module, Wavelet Fusion Block (WFB), employs Learnable Wavelet Nodes (LWNs) to conduct forward wavelet transformation and directional detail modeling. Frequency-domain features are extracted using depthwise separable convolutions and channel reconstruction and are then fused back into the spatial domain through residual connections. The decoding phase uses several Wavelet Head Blocks (WHBs) for progressive feature upsampling and image clarity restoration, ultimately producing a high-resolution deblurred image.</p><p>For the training strategy, the network is optimized using two types of loss functions: the multi-scale loss <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>multi</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which supervises the pixel-wise discrepancies between outputs at different scales and the ground truth (GT), and the wavelet reconstruction loss <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>wavelet</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which ensures consistent frequency-domain modeling by the Learnable Wavelet Node (LWN) module. The final total loss is defined as:<disp-formula id="FD3-sensors-25-05664"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>total</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>multi</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#955;</mml:mi><mml:mo>&#8901;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>wavelet</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3-sensors-25-05664"><title>2.3. Illumination Normalization</title><p>In real-world industrial inspection environments, the surfaces of threaded metallic components frequently exhibit strong shadows, specular highlights, and non-uniform exposure due to the periodic geometry, high reflectivity of materials, and significant variations in ambient lighting. These conditions impose substantial challenges to vision-based defect detection models, often leading to unstable or inaccurate predictions.</p><p>To enhance the consistency of input images and improve illumination robustness, this study introduces an image-level illumination normalization module during the data preprocessing stage.</p><p>This study adopts an illumination processing framework based on the Retinex theory [<xref rid="B43-sensors-25-05664" ref-type="bibr">43</xref>], which models the observed image <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> as the product of a reflectance component <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and an illumination component <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD4-sensors-25-05664"><label>(4)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8901;</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the logarithmic domain, this multiplicative relationship is transformed into an additive model for easier processing:<disp-formula id="FD5-sensors-25-05664"><label>(5)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To extract reflectance components that are more sensitive to subtle structural defects, we propose a local brightness-constrained enhancement strategy, which combines local contrast amplification with gamma compression into a unified normalization framework. This approach performs spatial mean filtering in the brightness channel to suppress low-frequency illumination artifacts while adaptively adjusting the contrast range of the image. It enhances the visibility of fine textures and edge-related features that are critical for defect detection.</p><p>For practical implementation, we adopt the fast and deployable Retinex by Adaptive Filtering (RAF) method as the primary algorithm, combined with gamma compression:<disp-formula id="FD6-sensors-25-05664"><label>(6)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mi>norm</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="sans-serif">&#963;</mml:mi></mml:msub><mml:mfenced><mml:mi mathvariant="normal">I</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mi mathvariant="sans-serif">&#947;</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="sans-serif">&#963;</mml:mi></mml:msub><mml:mfenced><mml:mi mathvariant="normal">I</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Gaussian-smoothed output of the image brightness channel, and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#947;</mml:mi></mml:mrow></mml:math></inline-formula> controls the non-linear compression of brightness. This method suppresses overexposure in locally bright areas and enhances contrast in low-illumination or occluded regions. It is particularly effective for inner surfaces of metallic threads, where reflective lighting often causes pseudo-defect patterns due to structural highlights.</p><p>The network architecture, as illustrated in <xref rid="sensors-25-05664-f005" ref-type="fig">Figure 5</xref>, adopts a dual-stage cooperative design aimed at simultaneously addressing low-light enhancement and image deblurring. The overall network consists of two distinct stages: the Low-Light Enhancement Stage (LOL Stage) and the Deblurring Stage (Deblur Stage). Both stages utilize a symmetric encoder&#8211;decoder architecture with multi-scale feature extraction capabilities and are connected via skip connections to ensure efficient transmission and fusion of feature information.</p><p>The LOL Stage focuses on restoring brightness and enhancing fundamental details in low-illumination input images, thereby improving overall visibility. Building on the enhanced outputs, the Deblur Stage further strengthens edge structures and restores texture details, effectively compensating for blur caused by low lighting or acquisition jitter.</p><p>In terms of module design, DarkIR integrates an EBBlock (Enhancement Block) into the LOL Stage. This block contains two key submodules:<list list-type="simple"><list-item><label>(1)</label><p>The SpAM (Spatial Attention Module) enhances local responses via spatial attention mechanisms, improving brightness expression under uneven lighting conditions;</p></list-item><list-item><label>(2)</label><p>The Fre-MLP (Frequency-aware MLP) module centers on frequency-domain modeling, leveraging frequency information to preserve fine details and reduce noise&#8212;especially suited for handling high-frequency regions such as industrial surface textures.</p></list-item></list></p><p>The EBBlock output is fused with the main feature stream via residual connections, ensuring stability throughout the enhancement process.</p><p>In the Deblur Stage, DarkIR incorporates the DBlock module for high-quality restoration of blurred regions. DBlock consists of:<list list-type="simple"><list-item><label>(1)</label><p>Di-SpAM (Dilated Spatial Attention Module), which uses dilated convolutions to enlarge the receptive field and capture edge cues in low-contrast backgrounds;</p></list-item><list-item><label>(2)</label><p>Gated-FFN (Gated Feed-Forward Network), which enables discriminative modeling between blurred and sharp regions during information propagation, thus better preserving structural integrity and suppressing artifacts.</p></list-item></list></p><p>The entire network employs standard strided convolutions and transposed convolutions for downsampling and upsampling, respectively. Additionally, skip connections between multiple scales enable the flow of semantic and fine-grained visual information across layers, further enhancing the network&#8217;s multi-scale perceptual capability.</p></sec><sec id="sec2dot4-sensors-25-05664"><title>2.4. Image Data Augmentation</title><p>In industrial internal and external thread defect detection tasks, the acquisition of high-quality and representative image samples is often constrained by factors such as complex spatial structures, reflective metallic surfaces, and occluded viewpoints. These limitations result in a scarcity of annotated data, which restricts the robustness and generalization capability of deep learning-based detection models.</p><p>To address the limited-data problem, this study introduces a high-fidelity defect image generation framework based on diffusion modeling during the data augmentation phase. Specifically, a residual diffusion denoising model (RDDM) is employed to perform conditional sampling on original thread images, thereby simulating a broader distribution of diverse and representative defect types.</p><p>The RDDM method follows the classical forward&#8211;reverse diffusion modeling paradigm. In the forward process, the original image <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is progressively injected with Gaussian noise via a Markov chain, expressed as:<disp-formula id="FD7-sensors-25-05664"><label>(7)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">&#946;</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:mo>&#8901;</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">&#946;</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>&#8901;</mml:mo><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#946;</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the diffusion coefficient at time step <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:math></inline-formula>, controlling the noise injection intensity. The reverse generation process is guided by a residual-conditioned denoising predictor to iteratively reconstruct the original signal, defined by the target distribution:<disp-formula id="FD8-sensors-25-05664"><label>(8)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="sans-serif">&#952;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">&#956;</mml:mi><mml:mi mathvariant="sans-serif">&#952;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi mathvariant="sans-serif">&#952;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:math></inline-formula> denotes the defect category label, and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#956;</mml:mi><mml:mi mathvariant="sans-serif">&#952;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi mathvariant="sans-serif">&#952;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the conditional mean and variance estimated by the learned model.</p><p>Unlike traditional DDPM approaches, RDDM introduces a residual prediction strategy, which does not directly predict the original image <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> but instead predicts the residual information:<disp-formula id="FD9-sensors-25-05664"><label>(9)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="normal">r</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="sans-serif">&#952;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">&#956;</mml:mi><mml:mi mathvariant="sans-serif">&#952;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This residual-based formulation effectively mitigates issues such as edge blurring and texture degradation and is particularly suitable for enhancing fine-grained defects like thread breaks, burrs, and contamination in industrial images.</p><p>In this study, we utilize real-world internal and external thread defect samples as priors. The RDDM is conditioned on defect labels to perform diffusion-based sampling, generating high-fidelity defect images that not only retain geometric consistency with real samples but can also simulate diverse defect types across different sampling iterations.</p></sec><sec id="sec2dot5-sensors-25-05664"><title>2.5. Defect Detection</title><p>In this study, the YOLO (You Only Look Once) [<xref rid="B44-sensors-25-05664" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05664" ref-type="bibr">45</xref>] series is adopted as the foundational framework for metallic surface defect detection due to its high inference efficiency as a single-stage object detector. Among them, YOLOv8 [<xref rid="B46-sensors-25-05664" ref-type="bibr">46</xref>] significantly enhances feature extraction and multi-scale fusion capabilities by introducing the C2f module and BiFPN structure. However, challenges remain in accurately detecting small-scale defects under complex industrial backgrounds. The baseline network architecture is illustrated in <xref rid="sensors-25-05664-f006" ref-type="fig">Figure 6</xref>.</p><p>To overcome these limitations, we propose a lightweight enhanced architecture named SLF-YOLO, which integrates three key components:<list list-type="simple"><list-item><label>(1)</label><p>The SC_C2f module for improved channel-wise feature fusion;</p></list-item><list-item><label>(2)</label><p>The Light-SSF_Neck structure for efficient multi-scale aggregation;</p></list-item><list-item><label>(3)</label><p>A novel loss function termed FIMetal-IoU, designed to optimize bounding box regression under industrial constraints.</p></list-item></list></p><p>The overall structure of SLF-YOLO is shown in <xref rid="sensors-25-05664-f007" ref-type="fig">Figure 7</xref>. In the backbone, the SC_C2f module incorporates a Star Block for enhanced feature interaction and leverages a Channel-Gated Linear Unit (CGLU) activation to enable fine-grained dynamic channel selection.</p><p>The key computation formulas are defined as follows:</p><p>To optimize information flow and multi-scale feature fusion in the Neck stage, we adopt the Light-SSF_Neck structure. Its core component, the GSConv module, fuses Standard Convolution (SC) and Depthwise Separable Convolution (DSC) to enhance channel-wise information exchange via dual-path computation:<disp-formula id="FD10-sensors-25-05664"><label>(10)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi>GSConv</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Shuffle</mml:mi><mml:mfenced><mml:mrow><mml:mi>Concat</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi>SC</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi>DSC</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Additionally, the Scale-Sequence Fusion (SSF) module extracts multi-scale features from P3, P4, and P5 and applies convolution, upsampling, and 3D convolutional fusion to construct cross-scale contextual representations:<disp-formula id="FD11-sensors-25-05664"><label>(11)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi>fusion</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Conv</mml:mi><mml:mn>3</mml:mn><mml:mi mathvariant="normal">D</mml:mi><mml:mfenced><mml:mrow><mml:mi>Concat</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mn>4</mml:mn><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mn>5</mml:mn><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To further enhance localization accuracy, we propose a novel loss function called FIMetal-IoU, which introduces an auxiliary box mechanism and piecewise weighting scheme into the IoU computation. The auxiliary box IoU is defined as:<disp-formula id="FD12-sensors-25-05664"><label>(12)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>IoU</mml:mi></mml:mrow><mml:mrow><mml:mi>Inner</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>inter</mml:mi></mml:mrow><mml:mrow><mml:mi>inner</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>union</mml:mi></mml:mrow><mml:mrow><mml:mi>inner</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on this, we apply piecewise weighting to different IoU intervals, and the final loss function is expressed as:<disp-formula id="FD13-sensors-25-05664"><label>(13)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>FIMetal</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>IoU</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>IoU</mml:mi></mml:mrow><mml:mrow><mml:mi>Inner</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>inter</mml:mi></mml:mrow><mml:mrow><mml:mi>inner</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>union</mml:mi></mml:mrow><mml:mrow><mml:mi>inner</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>union</mml:mi></mml:mrow><mml:mrow><mml:mi>inner</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>IoU</mml:mi></mml:mrow><mml:mrow><mml:mi>Inner</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>IoU</mml:mi></mml:mrow><mml:mrow><mml:mi>Inner</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot6-sensors-25-05664"><title>2.6. Adversarial Attacks on Image-Based Systems</title><p>Adversarial attacks on images represent a major security threat to deep learning-based vision systems. Their core objective is to induce incorrect predictions or outputs by introducing subtle but intentionally crafted perturbations to the input image, thereby compromising the system&#8217;s robustness and trustworthiness.</p><p>Based on their implementation methods and attack effects, adversarial attacks can be categorized into various types. Among them, alpha attacks, CCP (color channel perturbation), and patch attacks are three representative methods, as summarized in <xref rid="sensors-25-05664-t001" ref-type="table">Table 1</xref>.</p><p>Alpha attacks combine high imperceptibility with extremely strong attack capability, making them one of the most severe threats to current image recognition systems. Therefore, it is essential to develop high-sensitivity defense mechanisms specifically targeting this form of attack.</p><p>Although CCP and Patch attacks pose relatively lower threats, they still introduce practical security risks&#8212;especially in large-scale deployments of vision systems, where adversaries may exploit their low complexity to achieve rapid system compromise.</p><p>Accordingly, the design of image-level security defense strategies should be based on a multi-layered security framework incorporating</p><list list-type="simple"><list-item><label>(1)</label><p>A robust model architecture design;</p></list-item><list-item><label>(2)</label><p>Adversarial training techniques;</p></list-item><list-item><label>(3)</label><p>Multimodal detection methods.</p></list-item></list><p>These measures collectively enhance the model&#8217;s adversarial robustness and resistance to diverse threat vectors in real-world industrial environments.</p><p>An alpha channel attack is a stealthy adversarial method based on the transparency dimension of image representation. In recent years, it has emerged as a highly concealed and engineering-feasible input-level threat in security-sensitive industrial visual inspection systems. This method exploits the structural vulnerabilities in image processing pipelines by embedding adversarial perturbations into the alpha (transparency) channel of standard image formats (e.g., PNG), which are typically not perceived by human vision systems.</p><p>While alpha channel manipulations are generally unsupported in human-viewing libraries, they remain invisible yet processable in industrial vision pipelines that rely on image pre-processing frameworks such as OpenCV, TensorRT, PIL, or PyTorch. As a result, these perturbations bypass typical input validation and are treated as valid tensors by deep neural networks, allowing attackers to create stealthy adversarial samples without altering pixel color or brightness. The attack is formulated as:<disp-formula id="FD14-sensors-25-05664"><label>(14)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi>adv</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi>orig</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#945;</mml:mi><mml:mo>&#8857;</mml:mo><mml:mi mathvariant="sans-serif">&#948;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi>orig</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the original industrial image, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#948;</mml:mi></mml:mrow></mml:math></inline-formula> is the adversarial perturbation map, and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is the transparency mask controlling the blend intensity. This operation introduces controllable perturbations through the alpha channel without altering the color and brightness distribution of the image pixels. It effectively interferes with the responses of the model in the convolution feature extraction of the previous layer, especially having a significant interference effect on the periodic textures, gap edges, and multi-scale concave structures in the threaded images.</p><p>To ensure imperceptibility and maintain image quality, the perturbation design is subject to the following constrained optimization:<disp-formula id="FD15-sensors-25-05664"><label>(15)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi mathvariant="sans-serif">&#948;</mml:mi></mml:munder><mml:mi mathvariant="script">L</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mi>adv</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>target</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#955;</mml:mi><mml:mo>&#8901;</mml:mo><mml:mo>&#8214;</mml:mo><mml:mi mathvariant="sans-serif">&#948;</mml:mi><mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant="normal">p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mfenced><mml:mo>&#8901;</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the target detection model, and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>target</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the desired misclassification output. The loss function is used to guide the model&#8217;s output to deviate from the original detection result, while the regularization term controls the magnitude of the perturbation to meet the perceptual constraints. This form enables attackers to deceive the model under multiple task settings, including common industrial errors such as misclassification of defect types, deviation in position regression, and decreases in the confidence level of bounding boxes. <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mfenced><mml:mo>&#8901;</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the loss function guiding the attack, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mi mathvariant="sans-serif">&#948;</mml:mi><mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant="normal">p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the perturbation norm, regularized by <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#955;</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>To preserve perceptual quality and system integrity, the following constraints are enforced:<disp-formula id="FD16-sensors-25-05664"><label>(16)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mi mathvariant="sans-serif">&#948;</mml:mi><mml:msub><mml:mo>&#8214;</mml:mo><mml:mo>&#8734;</mml:mo></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8195;</mml:mo><mml:mi mathvariant="sans-serif">&#945;</mml:mi><mml:mo>&lt;</mml:mo><mml:mi mathvariant="sans-serif">&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> is the maximum perturbation bound and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#964;</mml:mi></mml:mrow></mml:math></inline-formula> is the upper bound for transparency. Both are typically set below 0.1 to avoid triggering quality-based preprocessing thresholds and to ensure compatibility with image format standards.</p><p>In threaded defect detection applications, alpha channel perturbations have been experimentally demonstrated to cause typical false detections and missed detections at the output of neural network models. Specifically, such perturbations can lead to positional drift in defect localization (e.g., misaligned gap detection), destruction of edge integrity, and interference from structurally repetitive regions.</p><p>Notably, even under static image dimensions, the adversarial effect exhibits strong transferability across samples and models, indicating cross-model attack capability. Given that most industrial lightweight detection networks&#8212;such as the SLF-YOLO model proposed in this study&#8212;do not explicitly regulate or suppress four-channel inputs, alpha-based perturbations present a realistic deployment risk, warranting serious attention during the system design phase for security reinforcement.</p><p>In conclusion, alpha channel attacks represent a form of implicit input-level perturbation characterized by high stealthiness, cross-model adaptability, and engineering feasibility.</p><p>They have become an emerging but critical security threat in industrial-grade object detection systems. This work constructs an attack modeling framework tailored to thread-structured images and systematically uncovers the disruptive mechanisms and misleading effects of alpha perturbations on convolutional feature responses.</p><p>Furthermore, this study highlights the necessity of removal or masking mechanisms for the alpha channel in the image preprocessing pipeline. Combined with the proposed Histogram&#8211;MSE Defense Model (HMDM), the approach provides a practical reference for alpha channel risk assessment and mitigation strategies during the pre-deployment stage of industrial vision systems, aiming to (i) reduce vulnerability to adversarial inputs at the source and (ii) enhance the overall robustness and security of the system.</p><p>While the proposed Histogram&#8211;MSE Defense Model (HMDM) effectively mitigates alpha channel attacks by detecting discrepancies in the image histogram and pixel-level MSE, its statistical framework is inherently adaptable to other types of adversarial perturbations. For example, high-frequency noise attacks, often targeted at disrupting fine-grained textures, could be detected using frequency-domain features such as the Fourier Transform. Similarly, patch attacks, which aim to locally occlude regions of the image, could be detected by analyzing local discrepancies in image regions through local MSE or SSIM metrics. Furthermore, color channel perturbations could be identified by detecting abnormal shifts in the image&#8217;s color histogram or in the LAB color space.</p></sec></sec><sec id="sec3-sensors-25-05664"><title>3. Experiments</title><sec id="sec3dot1-sensors-25-05664"><title>3.1. Dynamic Deblurring</title><p>In industrial thread defect detection tasks, image blur is one of the key interference factors that significantly affects the accuracy of detection models. This issue is particularly pronounced in external threads, which often exhibit periodic structural features and small-scale defects. Even minor motion blur can severely degrade edge sharpness and target contrast, resulting in localization deviation, reduced confidence scores, or even complete miss detections.</p><p><xref rid="sensors-25-05664-f008" ref-type="fig">Figure 8</xref> illustrates a visual comparison of typical external thread corrosion defect images before and after dynamic deblurring.</p><p>The left image represents the original unprocessed input, where the high-speed rotation of the threaded pipe or minor camera vibration during image acquisition has introduced noticeable motion blur. This is evident in the blurred boundary of the corrosion spot (yellow area) and the streaking of background thread lines.</p><p>In contrast, the right image, processed using the proposed dynamic modeling-based deblurring enhancement method, shows sharpened defect edges, restored periodic thread patterns, and significantly improved contrast and texture clarity.</p><p>The proposed dynamic deblurring module integrates residual attention-based local blur recognition with a frequency-domain compensation mechanism, allowing for precise localization of degraded regions and adaptive restoration. While maintaining global structural consistency, it significantly enhances the separability and visual saliency of corrosion boundaries.</p><p>This method also demonstrates strong generalizability to common local blur issues in industrial thread imagery, such as exposure trailing caused by illumination variation and misalignment between motion speed and sampling rate.</p><p>It thus provides a robust and adaptable solution to blur-related challenges in real-world industrial inspection scenarios.</p></sec><sec id="sec3dot2-sensors-25-05664"><title>3.2. Impact of Illumination Normalization on Detection Accuracy</title><p>We conducted a visual comparison on internal thread images commonly found in industrial scenarios before and after enhancement.</p><p>In the left image (originally captured under extreme low-light conditions), the thread structure is nearly completely obscured by shadows, exhibiting severe black suppression and significant illumination non-uniformity.</p><p>The right image, processed using DarkIR, shows substantial improvements in detail visibility, with the hierarchical thread structures clearly recovered and the overall dynamic brightness range significantly expanded.</p><p>As an end-to-end deep learning method tailored for ultra-low-light image restoration, DarkIR adopts a learnable nonlinear mapping structure that enhances brightness while suppressing color distortion and noise amplification, issues that are frequently observed in traditional enhancement techniques. Specifically, DarkIR leverages a multi-scale attention mechanism and a dark-region-aware feature modeling module to implement an adaptive brightness compensation strategy. This makes it especially effective in industrial surface scenarios characterized by complex geometries and low reflectivity, such as threads and pipelines.</p><p>As illustrated in <xref rid="sensors-25-05664-f009" ref-type="fig">Figure 9</xref>, the step structures and inner-wall textures of the threads are clearly reconstructed after enhancement. Previously invisible micro-defects become discernible, thereby significantly improving the perceptual capability of downstream defect detection models in both localization and classification tasks.</p><p>Moreover, the enhanced images maintain sharp edge boundaries and structural integrity, which also provides a stable input foundation for further processing steps such as depth estimation and 3D reconstruction.</p></sec><sec id="sec3dot3-sensors-25-05664"><title>3.3. Image Data Augmentation Strategy and Model Generalization Analysis</title><p>To evaluate the adaptability and robustness of diffusion models in simulating industrial thread defects, this study conducted controllable generation experiments based on a residual denoising diffusion model (RDDM). The detailed parameter settings are listed in <xref rid="sensors-25-05664-t002" ref-type="table">Table 2</xref>.</p><p>As shown in <xref rid="sensors-25-05664-f010" ref-type="fig">Figure 10</xref>, the model&#8217;s progressive reconstruction results of internal and external thread defect images are visualized at different training iterations (Iter = 5000, 10,000, 15,000, 20,000), clearly demonstrating the evolution from blurred structures to highly detailed and realistic defect images.</p><p>At iteration 5000, the generated images remain in the high-noise reverse diffusion phase, with blurry defect contours, limited texture, and vague geometric structures.</p><p>By iteration 10,000, the thread contours become more defined, and the metallic surface textures along with spatial coherence of the defect areas begin to emerge, indicating the model has preliminarily learned the semantic features of industrial thread structures.</p><p>At iteration 15,000, the model is capable of synthesizing high-quality images with typical damage characteristics, such as localized wear, burr edges, and corrosion spots&#8212;reflecting its ability to accurately simulate mid-scale structural degradations.</p><p>By the final iteration 20,000, the generated images achieve high photorealism, with well-reconstructed surface textures, illumination reflections, and fine-grained defect details. These images exhibit complexity and discriminative features comparable to real-world inspection data, making them highly suitable for enhancing model generalization under limited data conditions.</p></sec><sec id="sec3dot4-sensors-25-05664"><title>3.4. Performance Evaluation of the Defect Detection Model</title><p>To comprehensively evaluate the adaptability and generalization performance of the proposed SLF-YOLO model in real-world industrial inspection scenarios, we conducted visual analyses of its detection performance on a variety of typical thread surface defect images. The results are presented in <xref rid="sensors-25-05664-f011" ref-type="fig">Figure 11</xref>.</p><p>For &#8220;scratch&#8221;-type defects, the model accurately identifies linear scratches of varying lengths and textures. It demonstrates the ability to distinguish between clear boundaries and partially blurred edges. Despite some scratch edges blending into the background due to color similarity, the model consistently provides high-confidence predictions (score &#8805; 0.9), indicating strong sensitivity to linear texture features. Moreover, it offers low-confidence indications in uncertain regions, which can support manual verification or multi-model ensemble processing.</p><p>In the case of &#8220;broken&#8221;-type defects, the model shows exceptional stability, especially in detecting vertically distributed multi-point damage, achieving high-confidence multi-object predictions (confidence &#8805; 0.9). These results suggest that SLF-YOLO has a high detection rate and localization consistency for small-scale, discrete defects, making it highly suitable for automated inspection tasks involving thread wear and microcracks.</p><p>For &#8220;corrosion&#8221;-type defects, the model successfully identifies irregularly shaped, blurred-boundary corrosion regions, assigning relatively high confidence scores despite the indistinct edges.</p><p>Overall, SLF-YOLO demonstrates excellent performance in detecting scratches, fractures, and corrosion on threaded surfaces. It exhibits strong defect type discrimination, multi-scale adaptability, and robustness under complex backgrounds. The high confidence, consistent multi-target detection, and stability across varied defect scenarios validate the effectiveness and industrial applicability of the proposed structural improvements&#8212;namely, the Channel-Gated Linear Unit (CGLU) mechanism and the SlimNeck lightweight fusion structure.</p><p>To further assess the training stability and convergence behavior of the proposed model, we visualized the variation trends of key loss functions and performance metrics throughout training, as shown in <xref rid="sensors-25-05664-f012" ref-type="fig">Figure 12</xref>.</p><p>The box regression loss, classification loss, and distribution focal loss (DFL) rapidly decreased during the first 100 epochs and gradually stabilized thereafter. This trend indicates good convergence behavior without noticeable overfitting. The consistency between training and validation loss curves further confirms the model&#8217;s generalization capability in thread defect detection.</p><p>In parallel, key performance metrics showed continuous improvement:<list list-type="simple"><list-item><label>(1)</label><p>Precision increased steadily from ~0.78 to 0.88, indicating a significant reduction in false positives.</p></list-item><list-item><label>(2)</label><p>Recall improved from 0.68 to 0.78, reflecting a substantial drop in missed detections.</p></list-item><list-item><label>(3)</label><p>The final mAP@0.5 reached 0.88, while the more stringent mAP@0.5:0.95 reached 0.56, demonstrating the model&#8217;s strong detection capability across varying object sizes and IoU thresholds.</p></list-item></list></p></sec><sec id="sec3dot5-sensors-25-05664"><title>3.5. Analysis and Visualization of Adversarial Perturbation Effects</title><p>Alpha channel attacks represent a form of adversarial input manipulation based on the transparency dimension of image data. These attacks exhibit extremely high stealthiness and practical feasibility. From a visual perspective, alpha attacks do not alter the image&#8217;s color, brightness, or structural content. However, by embedding subtle perturbations into the alpha (transparency) channel, they can significantly disrupt feature extraction and inference pathways in deep neural networks&#8212;posing a substantial threat to defect detection tasks that rely heavily on edge clarity and texture consistency, as is common in industrial imagery.</p><p>In this study, we specifically investigate whether lightweight industrial defect detection models such as YOLO are vulnerable to alpha channel exposure, particularly under default conditions where RGBA-format images are accepted without channel masking. Furthermore, we evaluate how such perturbations impact the detection accuracy and output stability of the model. Through this experiment, we aim to uncover the potential risks posed by input-level implicit attacks and provide a quantitative foundation and technical reference for the design of robust defense mechanisms in industrial vision systems.</p><p>To systematically evaluate the impact of alpha channel attacks and other input perturbation methods, we conducted a series of comparative experiments based on the trained SLF-YOLO model. The objective is to assess how three representative adversarial attack types affect the model&#8217;s accuracy, stability, and false detection risk under different mechanisms, as illustrated in <xref rid="sensors-25-05664-f013" ref-type="fig">Figure 13</xref>.</p><p>As summarized in <xref rid="sensors-25-05664-t003" ref-type="table">Table 3</xref>, the alpha channel attack caused a drastic degradation in SLF-YOLO&#8217;s detection performance.</p><p>Under the no attack condition, the model demonstrated strong performance:</p><p>Precision (P) = 0.893</p><p>Recall (R) = 0.958</p><p>mAP@0.5 = 0.969</p><p>mAP@0.5:0.95 = 0.717</p><p>These results reflect the model&#8217;s high sensitivity to subtle defects on thread surfaces.</p><p>However, under alpha perturbation, performance dropped catastrophically:</p><p>Precision = 0.017</p><p>Recall = 0.128</p><p>mAP@0.5 = 0.027</p><p>mAP@0.5:0.95 = 0.0045</p><p>This stark contrast reveals that although alpha perturbations are visually imperceptible, they can fatally disrupt the model&#8217;s discriminative mechanisms, rendering the detection task almost entirely ineffective. The attack bypasses traditional pixel-based anomaly detectors and directly targets the front end of the perception pipeline, highlighting its extreme stealth and destructive potential.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05664"><title>4. Discussion</title><sec id="sec4dot1-sensors-25-05664"><title>4.1. Contribution Analysis of the Preprocessing Module to Model Performance</title><p>Image preprocessing plays a critical role in enhancing the performance of deep learning models, particularly in tasks such as motion deblurring and low-light enhancement. To quantify the impact of preprocessing on final image quality, we compared the performance of various algorithms using two standard metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). The detailed results are summarized in <xref rid="sensors-25-05664-t004" ref-type="table">Table 4</xref>.</p><p>In the motion deblurring task, MLWNet-B achieves the best performance, with a PSNR of 30.3 dB and SSIM of 0.940, significantly outperforming DeblurGAN-v2 (27.6 dB, 0.903) and SRN (28.7 dB, 0.910). These results demonstrate MLWNet-B&#8217;s superior capability in blur modeling and multi-scale detail recovery. Notably, in cases where the blur kernel is non-estimable, MLWNet-B&#8217;s adaptive wavelet-based feature extraction mechanism effectively enhances image clarity and perceptual quality while preserving structural consistency.</p><p>In the low-light enhancement task, DarkIR attains the highest SSIM (0.945), indicating excellent structural preservation during enhancement. However, its PSNR (26.4 dB) is slightly lower than that of HWMNet (27.0 dB), suggesting marginally weaker noise suppression. Overall, HWMNet achieves a balanced trade-off between brightness enhancement and detail preservation. FLOL, while slightly lower in both PSNR (25.9 dB) and SSIM (0.920), delivers stable performance, demonstrating good generalization in illumination modeling.</p><p>In addition to comparisons with deep learning-based baselines, we further evaluated our approach against several classical image enhancement methods, including Wiener filtering, median filtering, unsharp masking, histogram equalization, CLAHE, and Retinex-based enhancement. As shown in <xref rid="sensors-25-05664-t004" ref-type="table">Table 4</xref>, these traditional techniques achieved only moderate improvements (PSNR ranging from 22.8&#8211;25.6 dB and SSIM from 0.84&#8211;0.87), which fall significantly short of the performance delivered by our MLWNet-B (30.3 dB/0.940) and DarkIR (26.4 dB/0.945). This clearly demonstrates that the proposed modules not only outperform prior deep learning solutions but also substantially exceed the capacity of conventional baselines, validating their superiority and practical value for industrial defect detection scenarios.</p><p>These results affirm that effective preprocessing modules significantly mitigate image degradation, which is common in complex industrial settings. By enhancing edge sharpness, texture contrast, and structural separability, preprocessing serves as a supportive foundation for downstream tasks such as object detection and classification.</p></sec><sec id="sec4dot2-sensors-25-05664"><title>4.2. Performance Applicability of Image Data Augmentation in Thread Defect Detection</title><p>In the task of synthetic generation of thread defect images, we evaluated the Fr&#233;chet Inception Distance (FID) trend of the residual denoising diffusion model (RDDM) under different sampling steps. As illustrated in <xref rid="sensors-25-05664-f014" ref-type="fig">Figure 14</xref>, the results clearly demonstrate the model&#8217;s capability to progressively enhance image quality during the generation process. As the number of sampling steps increases from 5 to 100, the FID value drops significantly from 69.6 to 24.92, exhibiting a distinct nonlinear decreasing trend.</p><p>Under low sampling steps, RDDM is already capable of rapidly generating defect images with a coherent global structure and basic surface morphology. These samples exhibit sufficient structural similarity and discriminative features, making them suitable for real-time pseudo-sample generation and online data augmentation, especially where computational efficiency is critical.</p><p>As the number of sampling steps increases, the model gains more capacity to refine texture, lighting, and boundary details, enabling the synthesis of high-fidelity samples that more closely resemble real-world defects. These refined images are particularly beneficial for improving the robustness of defect detection models, especially in multi-type defect scenarios involving corrosion, scratches, and fractures.</p><p>RDDM achieves this by leveraging a decoupled residual and noise diffusion mechanism, which enables the model to capture global structural patterns while incrementally enriching fine-grained details. This significantly enhances the structural expressiveness of defect regions. Furthermore, the high quality of generated samples serves as a stable input foundation for subsequent lightweight detection networks, improving system robustness against complex backgrounds, lighting variations, and sample distribution shifts.</p><p>The FID curve validates that RDDM is capable of delivering high-quality image synthesis at relatively low sampling costs, making it highly applicable for a wide range of industrial vision tasks, including multi-source defect modeling, pseudo-sample generation, and adversarial sample construction for defense training. Therefore, RDDM serves as a critical generative component in enabling intelligent thread defect detection systems.</p></sec><sec id="sec4dot3-sensors-25-05664"><title>4.3. Ablation Study of Detection Module Components</title><p>To systematically evaluate the contribution of each structural improvement, a comprehensive ablation study was conducted, as summarized in <xref rid="sensors-25-05664-t005" ref-type="table">Table 5</xref>.</p><p>Starting from the baseline model (no enhancements), the detector achieved a precision of 0.821, recall of 0.718, mAP@0.5 of 0.759, and mAP@0.5:0.95 of 0.411. These results establish a reference point for subsequent comparisons.</p><p>When the SC_C2f module was introduced, precision increased to 0.842 (an absolute gain of +0.021, or 2.6%), and recall improved to 0.742 (+0.024). The mAP@0.5 rose to 0.781 (+0.022), confirming that this shallow attention mechanism effectively strengthens feature expressiveness and improves localization of small-scale defects.</p><p>The Light-SSF_Neck module demonstrated strong spatial aggregation capability. Its precision reached 0.841, surpassing the baseline by +0.020, while recall slightly decreased to 0.691 (&#8211;0.027). This suggests that although feature fusion is enhanced, certain weak signals may be suppressed, explaining the minor drop in recall. Nonetheless, the mAP@0.5 improved modestly to 0.776, highlighting the trade-off between recall sensitivity and feature consolidation.</p><p>The FIMetal-IoU loss function introduced refined bounding box regression. While the mAP@0.5 rose only marginally to 0.774 (+0.015), the stricter mAP@0.5:0.95 increased significantly from 0.411 to 0.449 (+0.038), reflecting stronger fine-grained localization performance. This indicates that the new loss function contributes more to accurate boundary alignment than to coarse-level detection.</p><p>Combinations of modules revealed complementary effects:</p><list list-type="simple"><list-item><label>(1)</label><p>SC + Neck achieved balanced improvements: Recall jumped to 0.784 (+0.066 over baseline), precision rose to 0.847, and mAP@0.5 increased to 0.793, demonstrating synergy between shallow attention and spatial fusion.</p></list-item><list-item><label>(2)</label><p>SC + IoU further improved precision to 0.864, the second-highest among all configurations, validating the compatibility between SC_C2f-driven feature expressiveness and IoU-based localization.</p></list-item><list-item><label>(3)</label><p>Neck + IoU yielded the highest precision among sub-combinations (0.868), but recall dropped to 0.665, indicating sensitivity to difficult samples and a tendency toward conservative predictions.</p></list-item></list><p>The full integration (all modules, SLF-YOLO) produced the best overall results, with a precision of 0.881, recall of 0.794, mAP@0.5 of 0.813, and mAP@0.5:0.95 of 0.521. Compared to the baseline, these represent respective improvements of +0.060 precision (+7.3%), +0.076 recall (+10.6%), +0.054 mAP@0.5 (+7.1%), and +0.110 mAP@0.5:0.95 (+26.8%). Importantly, these gains were achieved alongside reduced complexity, with parameters reduced from 11.12M to 9.65M and FLOPs from 28.4G to 24.6G.</p><p>Overall, the results confirm that each proposed module contributes distinct strengths&#8212;SC_C2f enhances feature expressiveness, Light-SSF_Neck strengthens fusion, and FIMetal-IoU improves localization precision. Their integration in SLF-YOLO achieves not only statistically significant improvements (<italic toggle="yes">p</italic> &lt; 0.05) but also maintains computational efficiency, underscoring its strong potential for real-world deployment in industrial inspection pipelines.</p><p>To further assess the overall performance of SLF-YOLO in industrial defect detection tasks, we benchmarked it against leading lightweight YOLO variants (YOLOv5s, YOLOv8s, YOLOv9s, and YOLOv10s) under identical datasets and training configurations. The comparative results are summarized in <xref rid="sensors-25-05664-t006" ref-type="table">Table 6</xref>.</p><p>SLF-YOLO achieved the highest precision of 0.881 &#177; 0.03, surpassing YOLOv5s (0.862 &#177; 0.02) and YOLOv10s (0.850 &#177; 0.03), and slightly outperforming YOLOv9s (0.870 &#177; 0.01) and YOLOv8s (0.869 &#177; 0.02). This superior precision indicates that SLF-YOLO produces fewer false positives, a particularly critical property for industrial inspection systems where erroneous alarms can cause unnecessary re-checks and production delays.</p><p>In terms of recall, SLF-YOLO demonstrated a notable improvement, reaching 0.794 &#177; 0.03, which is significantly higher than all YOLO baselines (best baseline: YOLOv8s at 0.732 &#177; 0.01). This highlights SLF-YOLO&#8217;s enhanced resistance to missed detections, ensuring that subtle or partially occluded thread defects are not overlooked&#8212;a vital requirement for maintaining assembly quality in industrial pipelines.</p><p>For mAP@0.5, SLF-YOLO achieved 0.813 &#177; 0.04, which is marginally lower than YOLOv8s (0.832 &#177; 0.03) and YOLOv9s (0.829 &#177; 0.03) but higher than YOLOv5s (0.725 &#177; 0.01) and YOLOv10s (0.817 &#177; 0.02). Importantly, the combination of strong precision and recall values demonstrates a balanced trade-off between accuracy and robustness, suggesting that SLF-YOLO generalizes more reliably across diverse defect scenarios, even if its absolute mAP is slightly lower than the latest YOLO variants.</p><p>Overall, SLF-YOLO shows statistically stable performance, as evidenced by the relatively small standard deviations across five independent runs. These results confirm that the proposed architecture not only improves detection accuracy but also enhances stability under repeated trials, which is essential for real-world deployment. In conclusion, SLF-YOLO provides an effective and lightweight solution that outperforms or matches state-of-the-art YOLO variants in most metrics, delivering superior robustness and reliability for practical industrial defect detection applications.</p></sec><sec id="sec4dot4-sensors-25-05664"><title>4.4. Analysis of Adversarial Perturbation Effects on Detection Model Mechanisms</title><p>This section explores the internal interference mechanisms of adversarial perturbations against lightweight industrial defect detection models. We investigate three representative types of attacks:<list list-type="simple"><list-item><label>(1)</label><p>Alpha channel attack: Introduces imperceptible structural interference by embedding a low-opacity perturbation layer in the alpha channel. This simulates the risk of unfiltered RGBA input passing through pre-processing stages unhandled.</p></list-item><list-item><label>(2)</label><p>Color channel perturbation (CCP) attack: Distorts RGB channel ratios via color mapping matrix adjustments, resulting in global color shifts that hinder accurate edge and texture recognition.</p></list-item><list-item><label>(3)</label><p>Patch attack: Applies high-contrast patches to critical image regions (e.g., thread junctions or edges), simulating physical occlusions or targeted adversarial triggers.</p></list-item></list></p><p><xref rid="sensors-25-05664-f015" ref-type="fig">Figure 15</xref> illustrates the impact of these attacks. Subfigure (a) shows the original defect image, (b) is the alpha-perturbed adversarial sample, (c) shows CCP-affected input, and (d) presents the patch-augmented sample. All inputs maintain the same resolution, brightness, and content to ensure that performance changes stem solely from the applied perturbations.</p><p>A unified SLF-YOLO model, with fixed structure and weights, is used to evaluate detection performance across four standard metrics: precision, recall, mAP@0.5, and mAP@0.5:0.95. Additionally, we assess per-class recognition rate changes and feature activation shifts to further analyze the disruption patterns caused by each perturbation mechanism, as can be seen in <xref rid="sensors-25-05664-t007" ref-type="table">Table 7</xref>.</p><p>In the absence of adversarial interference, SLF-YOLO achieved strong baseline performance, with a precision of 0.881, recall of 0.794, mAP@0.5 of 0.813, and mAP@0.5&#8211;0.95 of 0.521. These results highlight the model&#8217;s robustness in clean conditions and provide a reliable reference for evaluating subsequent perturbation effects.</p><p>Under alpha channel perturbations, the model experienced catastrophic collapse across all metrics. Precision decreased from 0.881 to 0.017 (a reduction of 98.1%), recall dropped from 0.794 to 0.128 (&#8722;83.9%), and mAP@0.5&#8211;0.95 plummeted by 99.0% (from 0.521 to 0.005). Despite being visually imperceptible to humans, alpha channel noise fundamentally disrupted the feature extraction process, leading to severe misclassification and confirming its destructive stealth.</p><p>In the case of color channel perturbations (CCP), the degradation was moderate but still significant. Recall fell to 0.373 (a 53% reduction), while mAP@0.5&#8211;0.95 dropped to 0.340, reflecting the model&#8217;s vulnerability to chromatic distortions. Since thread defect recognition often relies on fine-grained texture and edge cues, alterations in RGB ratios directly interfered with the feature representation, resulting in higher false negatives.</p><p>For patch attacks, which simulate physical occlusions, the performance degradation exhibited a different pattern. Recall remained relatively high at 0.840 (close to the clean baseline), suggesting that most true defects were still detected. However, precision dropped to 0.754, and mAP@0.5&#8211;0.95 decreased from 0.521 to 0.511. This indicates that patch-induced attention shifts increased false positives, as the detector focused on high-contrast adversarial regions rather than genuine defect areas.</p><p>Overall, these results reveal distinct disruption mechanisms: alpha channel perturbations cause global collapse in recognition logic, CCP produces color-dependent confusion, and patch attacks induce localized misdirection. The consistent trends across five independent runs (variations within &#177;0.002&#8211;0.006) confirm the reliability of these findings.</p><p>To further examine the perceptual behavior shifts induced by these perturbations, we utilized Grad-CAM to visualize the model&#8217;s feature response under different attack conditions, as shown in <xref rid="sensors-25-05664-f016" ref-type="fig">Figure 16</xref>.</p><p>This analysis confirms that alpha channel perturbations pose the most severe threat, primarily due to their invisible nature and disruptive power at the feature extraction stage. Meanwhile, CCP and patch attacks, although visually perceptible, also warrant defense due to their practical deployment feasibility in industrial environments.</p><p>These results highlight the critical need for preprocessing modules that strip non-RGB channels and implement adversarial input screening as well as for designing robust network architectures resilient to both implicit and explicit perturbations in visual industrial applications.</p></sec><sec id="sec4dot5-sensors-25-05664"><title>4.5. Effectiveness and Deployment Feasibility of the Defense Strategy</title><p>To address the high stealthiness and misleading nature of alpha channel attacks in industrial vision systems, this study proposes a visual consistency analysis method based on perceptual differences. Unlike conventional defense strategies that rely on model structures or inference paths, this method operates directly at the image level, detecting latent discrepancies between AI perception and human vision to enable unsupervised identification of alpha channel attack samples.</p><p>Specifically, we reconstruct two versions of each input image: the AI-perceived image (IAI) and the human-viewed image (IEye). The IAI is obtained by averaging the RGB channels, simulating how most vision models process inputs when the alpha channel is implicitly discarded. In contrast, the IEye is reconstructed using the standard alpha compositing formula by blending the RGBA image with a default background (e.g., white, as in typical web displays), thereby approximating the actual visual experience of human observers. For normal images, these two representations should be nearly identical. However, under alpha attacks, they often exhibit significant divergence in pixel distribution and structure.</p><p>To quantify such differences, we introduce two key detection metrics:<list list-type="simple"><list-item><label>(1)</label><p>Histogram overlap, which measures the grayscale distribution similarity between IAI and IEye&#8212;lower values indicate greater discrepancy.</p></list-item><list-item><label>(2)</label><p>Mean Squared Error (MSE), which captures pixel-wise deviation between the two images.</p></list-item></list></p><p>Empirical results show that alpha-attacked images typically exhibit a histogram overlap below 0.05 and an MSE significantly higher than that of clean samples, making them robust indicators of adversarial manipulation.</p><p>As illustrated in <xref rid="sensors-25-05664-f017" ref-type="fig">Figure 17</xref>, the grayscale histogram comparison between IAI and IEye shows a highly divergent distribution, with an overlap of only 0.0015, indicating near-complete disassociation&#8212;a signature of alpha-based perturbation.</p><p><xref rid="sensors-25-05664-f018" ref-type="fig">Figure 18</xref> presents the MSE heatmap, where the orange-red regions highlight systematic pixel-level discrepancies between the original content and the adversarial overlay, with an average MSE of 0.2054, far exceeding the noise tolerance threshold of typical images.</p><p>Experiments confirm that when the histogram overlap falls below 0.05 or the MSE exceeds 0.01, the image can be reliably flagged as a potential alpha channel attack. The entire detection pipeline takes less than 20 ms on a standard CPU; requires no dependence on deep network architectures or prior knowledge of specific attacks; and features lightweight computation, clear thresholds, and strong generalization ability&#8212;making it a practical and deployable solution for mitigating alpha-based adversarial threats in real-world industrial vision systems.</p><p>The scalability of HMDM to different adversarial perturbation types remains an important direction for future research. While this work focuses on alpha channel perturbations, the model&#8217;s underlying mechanism of comparing statistical features is inherently flexible. Future experiments will explore the extension of this model to handle other types of adversarial attacks, such as high-frequency noise, color channel distortions, and localized patch occlusions. By leveraging multiple feature domains (frequency, spatial, color), HMDM could become a more general-purpose defense mechanism against a broader range of adversarial threats.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05664"><title>5. Conclusions</title><p>This work presents an intelligent vision system for detecting internal and external thread defects, featuring a closed-loop design from image acquisition and enhancement to sample generation, lightweight detection, and adversarial defense. Experimental evaluations show that integrating MLWNet and DarkIR in preprocessing substantially improves input image discriminability, while high-fidelity pseudo-samples generated via RDDM enhance model generalization under limited data. The proposed SLF-YOLO achieves high detection accuracy and stability in complex industrial environments. With its dual defense strategy&#8212;combining input perturbation suppression and output anomaly detection&#8212;the system exhibits strong resilience against alpha channel attacks. Overall, the method strikes a balanced trade-off among detection precision, security robustness, and deployment efficiency, demonstrating strong potential for industrial-scale adoption. Future work will focus on multimodal defense mechanisms and cross-device robustness optimization, driving inspection systems toward greater intelligence, security, and adaptability.</p><p>Despite the promising results, the proposed system still has several limitations that warrant further discussion. To begin with, although the integrated framework enhances robustness under common industrial conditions, its performance may deteriorate in the presence of highly complex surface textures or extremely low-quality images, suggesting the necessity for more generalized enhancement strategies. In addition, while the RDDM-based defect synthesis partially mitigates the scarcity of annotated samples, it cannot fully reflect the diversity of real-world defects, and over-reliance on synthetic data may introduce domain bias. Moreover, the adversarial defense module has been primarily validated against alpha channel perturbations; however, broader categories of physical-world attacks and adaptive adversarial strategies remain underexplored. Lastly, although the lightweight YOLO-based architecture achieves a balance between accuracy and efficiency, further refinement is required to enable deployment on ultra-low-power edge devices.</p><p>These limitations also indicate several promising directions for future research. Potential efforts may include the development of more adaptive image enhancement algorithms tailored for highly variable environments; the extension of diffusion-based modeling to multi-modal data (e.g., 3D imaging or hyperspectral analysis); and the design of more comprehensive adversarial defense frameworks that integrate robustness, detection, and recovery. Furthermore, the integration of the proposed framework into real industrial production lines and its validation on large-scale, multi-factory datasets will be essential for demonstrating practical value and scalability.</p><p>In future work, we will extend the HMDM framework to accommodate other common adversarial attack types, validating its robustness against a broader range of perturbations and enhancing its practical applicability in real-world industrial detection systems.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, L.F. and L.L.; Software, G.Z.; Validation, L.L. and Z.J.; Formal analysis, L.F. and L.L.; Resources, Z.J.; Writing&#8212;original draft, L.F. and G.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Informed Consent Statement</title><p>All the participants involved in the study have given their informed consent.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The experimental data format of this article is not available for public disclosure.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author declares no conflict of interest.</p></notes><glossary><title>Abbreviations</title><array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Abbreviation</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Full English Name</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AI</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Intelligence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ANN</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Adam</td><td align="left" valign="middle" rowspan="1" colspan="1">Adaptive Moment Estimation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AP</td><td align="left" valign="middle" rowspan="1" colspan="1">Average Precision</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CAM</td><td align="left" valign="middle" rowspan="1" colspan="1">Class Activation Map</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CAD</td><td align="left" valign="middle" rowspan="1" colspan="1">Computer-Aided Design</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CE</td><td align="left" valign="middle" rowspan="1" colspan="1">Cross-Entropy (Loss)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Central Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Deep Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGSM</td><td align="left" valign="middle" rowspan="1" colspan="1">Fast Gradient Sign Method</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FLOPs</td><td align="left" valign="middle" rowspan="1" colspan="1">Floating Point Operations per Second</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FPS</td><td align="left" valign="middle" rowspan="1" colspan="1">Frames Per Second</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GAN</td><td align="left" valign="middle" rowspan="1" colspan="1">Generative Adversarial Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphics Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IoT</td><td align="left" valign="middle" rowspan="1" colspan="1">Internet of Things</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IoU</td><td align="left" valign="middle" rowspan="1" colspan="1">Intersection over Union</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MAE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Absolute Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Squared Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mAP</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Average Precision</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mAP@0.5</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Average Precision at IoU Threshold 0.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Average Precision at IoU Threshold 0.5 to 0.95</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NMS</td><td align="left" valign="middle" rowspan="1" colspan="1">Non-Maximum Suppression</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PGD</td><td align="left" valign="middle" rowspan="1" colspan="1">Projected Gradient Descent</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSNR</td><td align="left" valign="middle" rowspan="1" colspan="1">Peak Signal-to-Noise Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ReLU</td><td align="left" valign="middle" rowspan="1" colspan="1">Rectified Linear Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RDDM</td><td align="left" valign="middle" rowspan="1" colspan="1">Robust Diffusion Defect Modeling</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SLF</td><td align="left" valign="middle" rowspan="1" colspan="1">Shuffle Lightweight Feature</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SLF-YOLO</td><td align="left" valign="middle" rowspan="1" colspan="1">Shuffle Lightweight Feature-You Only Look Once</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SGD</td><td align="left" valign="middle" rowspan="1" colspan="1">Stochastic Gradient Descent</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SNR</td><td align="left" valign="middle" rowspan="1" colspan="1">Signal-to-Noise Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SSIM</td><td align="left" valign="middle" rowspan="1" colspan="1">Structural Similarity Index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO</td><td align="left" valign="middle" rowspan="1" colspan="1">You Only Look Once</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv5</td><td align="left" valign="middle" rowspan="1" colspan="1">You Only Look Once Version 5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="left" valign="middle" rowspan="1" colspan="1">You Only Look Once Version 8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv9s</td><td align="left" valign="middle" rowspan="1" colspan="1">You Only Look Once Version 9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10s</td><td align="left" valign="middle" rowspan="1" colspan="1">You Only Look Once Version 10</td></tr></tbody></array></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05664"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.-L.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>J.-H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.-S.</given-names></name></person-group><article-title>Geometric error modeling of the contact probe in a three-dimensional screw thread measuring machine</article-title><source>Measurement</source><year>2022</year><volume>194</volume><fpage>111026</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.111026</pub-id></element-citation></ref><ref id="B2-sensors-25-05664"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>X.-M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.-L.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>J.-H.</given-names></name></person-group><article-title>Design and characteristic research of contact probe for high-precision 3D thread-measuring machine</article-title><source>Int. J. Adv. Manuf. Technol.</source><year>2022</year><volume>119</volume><fpage>2235</fpage><lpage>2245</lpage><pub-id pub-id-type="doi">10.1007/s00170-021-08345-z</pub-id></element-citation></ref><ref id="B3-sensors-25-05664"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liting</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chong</surname><given-names>S.</given-names></name></person-group><article-title>The method of thread defect detection based on machine vision</article-title><source>Proceedings of the 2019 2nd International Conference on Information Systems and Computer Aided Education (ICISCAE)</source><conf-loc>Dalian, China</conf-loc><conf-date>28&#8211;30 September 2019</conf-date><fpage>579</fpage><lpage>582</lpage></element-citation></ref><ref id="B4-sensors-25-05664"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>H.</given-names></name></person-group><article-title>Research on internal defect detection method based on machine vision</article-title><source>Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition</source><conf-loc>Guangzhou, China</conf-loc><conf-date>13&#8211;15 September 2024</conf-date><fpage>308</fpage><lpage>312</lpage></element-citation></ref><ref id="B5-sensors-25-05664"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name></person-group><article-title>Research on Deep Hole and Large Thread Defect Detection Based on Machine Vision Fusion</article-title><source>8th International Conference on Computing, Control and Industrial Engineering</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Singapore</publisher-loc><year>2024</year><fpage>490</fpage><lpage>495</lpage></element-citation></ref><ref id="B6-sensors-25-05664"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Review of optical detection technologies for inner-wall surface defects</article-title><source>Opt. Laser Technol.</source><year>2023</year><volume>162</volume><fpage>109313</fpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2023.109313</pub-id></element-citation></ref><ref id="B7-sensors-25-05664"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huo</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name></person-group><article-title>A structural scheme design of internal thread detection based on laser profile scanning</article-title><source>J. Phys. Conf. Ser.</source><year>2024</year><volume>2921</volume><fpage>012025</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/2921/1/012025</pub-id></element-citation></ref><ref id="B8-sensors-25-05664"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>G.-S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name></person-group><article-title>Knitting needle fault detection system for hosiery machine based on laser detection and machine vision</article-title><source>Text. Res. J.</source><year>2021</year><volume>91</volume><fpage>143</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1177/0040517520935210</pub-id></element-citation></ref><ref id="B9-sensors-25-05664"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zuo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>An X-ray-based automatic welding defect detection method for special equipment system</article-title><source>IEEE/ASME Trans. Mechatron.</source><year>2023</year><volume>29</volume><fpage>2241</fpage><lpage>2252</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2023.3327713</pub-id></element-citation></ref><ref id="B10-sensors-25-05664"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rafiei</surname><given-names>M.</given-names></name><name name-style="western"><surname>Raitoharju</surname><given-names>J.</given-names></name><name name-style="western"><surname>Iosifidis</surname><given-names>A.</given-names></name></person-group><article-title>Computer vision on X-ray data in industrial production and security applications: A comprehensive survey</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>2445</fpage><lpage>2477</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3234187</pub-id></element-citation></ref><ref id="B11-sensors-25-05664"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ran</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Han</surname><given-names>D.</given-names></name></person-group><article-title>A dataset for surface defect detection on complex structured parts based on photometric stereo</article-title><source>Sci. Data</source><year>2025</year><volume>12</volume><fpage>276</fpage><pub-id pub-id-type="doi">10.1038/s41597-025-04454-6</pub-id><pub-id pub-id-type="pmid">39956811</pub-id><pub-id pub-id-type="pmcid">PMC11830798</pub-id></element-citation></ref><ref id="B12-sensors-25-05664"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Han</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name></person-group><article-title>Defect detection method based on sparse scanning with laser ultrasonics</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><elocation-id>13175</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-025-95965-0</pub-id><pub-id pub-id-type="pmid">40240428</pub-id><pub-id pub-id-type="pmcid">PMC12003631</pub-id></element-citation></ref><ref id="B13-sensors-25-05664"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>G.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>K.</given-names></name></person-group><article-title>Outer surface defect detection of steel pipes with 3D vision based on multi-line structured lights</article-title><source>Meas. Sci. Technol.</source><year>2024</year><volume>35</volume><fpage>065203</fpage><pub-id pub-id-type="doi">10.1088/1361-6501/ad2da6</pub-id></element-citation></ref><ref id="B14-sensors-25-05664"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jang</surname><given-names>S.W.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>G.-Y.</given-names></name></person-group><article-title>Deep Supervised Attention Network for Dynamic Scene Deblurring</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>1896</elocation-id><pub-id pub-id-type="doi">10.3390/s25061896</pub-id><pub-id pub-id-type="pmid">40293008</pub-id><pub-id pub-id-type="pmcid">PMC11946844</pub-id></element-citation></ref><ref id="B15-sensors-25-05664"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Du</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.-H.</given-names></name></person-group><article-title>Deblurring dynamic scenes via spatially varying recurrent neural networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>3974</fpage><lpage>3987</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3061604</pub-id><pub-id pub-id-type="pmid">33621173</pub-id></element-citation></ref><ref id="B16-sensors-25-05664"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Dynamic scene deblurring with parameter selective sharing and nested skip connections</article-title><source>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>3848</fpage><lpage>3856</lpage></element-citation></ref><ref id="B17-sensors-25-05664"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>J.</given-names></name></person-group><article-title>Deep dynamic scene deblurring from optical flow</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2021</year><volume>32</volume><fpage>8250</fpage><lpage>8260</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2021.3084616</pub-id></element-citation></ref><ref id="B18-sensors-25-05664"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>Z.</given-names></name></person-group><article-title>An efficient image deblurring network with a hybrid architecture</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>7260</elocation-id><pub-id pub-id-type="doi">10.3390/s23167260</pub-id><pub-id pub-id-type="pmid">37631796</pub-id><pub-id pub-id-type="pmcid">PMC10459222</pub-id></element-citation></ref><ref id="B19-sensors-25-05664"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vasluianu</surname><given-names>F.-A.</given-names></name><name name-style="western"><surname>Seizinger</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ranjan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name></person-group><article-title>Towards image ambient lighting normalization</article-title><source>18th European Conference on Computer Vision</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><fpage>385</fpage><lpage>404</lpage></element-citation></ref><ref id="B20-sensors-25-05664"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Da Cruz</surname><given-names>S.D.</given-names></name><name name-style="western"><surname>Taetz</surname><given-names>B.</given-names></name><name name-style="western"><surname>Stifter</surname><given-names>T.</given-names></name><name name-style="western"><surname>Stricker</surname><given-names>D.</given-names></name></person-group><article-title>Illumination normalization by partially impossible encoder-decoder cost function</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2021</conf-date><fpage>1459</fpage><lpage>1468</lpage></element-citation></ref><ref id="B21-sensors-25-05664"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>F.</given-names></name></person-group><article-title>Transition-constant normalization for image enhancement</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2023</year><volume>36</volume><fpage>20562</fpage><lpage>20576</lpage></element-citation></ref><ref id="B22-sensors-25-05664"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rad</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roth</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>Lepetit</surname><given-names>V.</given-names></name></person-group><article-title>ALCN: Adaptive local contrast normalization</article-title><source>Comput. Vis. Image Underst.</source><year>2020</year><volume>194</volume><fpage>102947</fpage><pub-id pub-id-type="doi">10.1016/j.cviu.2020.102947</pub-id></element-citation></ref><ref id="B23-sensors-25-05664"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goswami</surname><given-names>S.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>S.K.</given-names></name></person-group><article-title>A simple deep learning based image illumination correction method for paintings</article-title><source>Pattern Recognit. Lett.</source><year>2020</year><volume>138</volume><fpage>392</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2020.08.013</pub-id></element-citation></ref><ref id="B24-sensors-25-05664"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Dou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name></person-group><article-title>Internal Thread Defect Generation Algorithm and Detection System Based on Generative Adversarial Networks and You Only Look Once</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5636</elocation-id><pub-id pub-id-type="doi">10.3390/s24175636</pub-id><pub-id pub-id-type="pmid">39275549</pub-id><pub-id pub-id-type="pmcid">PMC11398171</pub-id></element-citation></ref><ref id="B25-sensors-25-05664"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>Internal thread defect detection system based on multi-vision</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><elocation-id>e0304224</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0304224</pub-id><pub-id pub-id-type="pmid">38805511</pub-id><pub-id pub-id-type="pmcid">PMC11132497</pub-id></element-citation></ref><ref id="B26-sensors-25-05664"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>Surface defect detection of bearing rings based on an improved YOLOv5 network</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>7443</elocation-id><pub-id pub-id-type="doi">10.3390/s23177443</pub-id><pub-id pub-id-type="pmid">37687898</pub-id><pub-id pub-id-type="pmcid">PMC10490562</pub-id></element-citation></ref><ref id="B27-sensors-25-05664"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name></person-group><article-title>RBS-YOLO: A Lightweight YOLOv5&#8212;Based Surface Defect Detection Model for Castings</article-title><source>IET Image Process.</source><year>2025</year><volume>19</volume><fpage>e70018</fpage><pub-id pub-id-type="doi">10.1049/ipr2.70018</pub-id></element-citation></ref><ref id="B28-sensors-25-05664"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Patil</surname><given-names>R.S.</given-names></name></person-group><article-title>A Comparative Analysis of YOLOv8 and YOLOv5 for Nut Thread Classification&#8212;Deep Learning Approach</article-title><source>Int. J. Res. Appl. Sci. Eng. Technol.</source><year>2024</year><volume>12</volume><fpage>176</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.22214/ijraset.2024.58290</pub-id></element-citation></ref><ref id="B29-sensors-25-05664"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shu</surname><given-names>S.</given-names></name></person-group><article-title>MR-YOLO: An improved YOLOv5 network for detecting magnetic ring surface defects</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>9897</elocation-id><pub-id pub-id-type="doi">10.3390/s22249897</pub-id><pub-id pub-id-type="pmid">36560265</pub-id><pub-id pub-id-type="pmcid">PMC9781278</pub-id></element-citation></ref><ref id="B30-sensors-25-05664"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tabernik</surname><given-names>D.</given-names></name><name name-style="western"><surname>&#352;ela</surname><given-names>S.</given-names></name><name name-style="western"><surname>Skvar&#269;</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sko&#269;aj</surname><given-names>D.</given-names></name></person-group><article-title>Segmentation-based deep-learning approach for surface-defect detection</article-title><source>J. Intell. Manuf.</source><year>2020</year><volume>31</volume><fpage>759</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1007/s10845-019-01476-x</pub-id></element-citation></ref><ref id="B31-sensors-25-05664"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Defect transformer: An efficient hybrid transformer architecture for surface defect detection</article-title><source>Measurement</source><year>2023</year><volume>211</volume><fpage>112614</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.112614</pub-id></element-citation></ref><ref id="B32-sensors-25-05664"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Waghela</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rakshit</surname><given-names>S.</given-names></name></person-group><article-title>Robust image classification: Defensive strategies against FGSM and PGD adversarial attacks</article-title><source>Proceedings of the 2024 Asian Conference on Intelligent Technologies (ACOIT)</source><conf-loc>Kolar, India</conf-loc><conf-date>6&#8211;7 September 2024</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B33-sensors-25-05664"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>Adversarial attacks and defenses in deep learning</article-title><source>Engineering</source><year>2020</year><volume>6</volume><fpage>346</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1016/j.eng.2019.12.012</pub-id></element-citation></ref><ref id="B34-sensors-25-05664"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Madry</surname><given-names>A.</given-names></name><name name-style="western"><surname>Makelov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schmidt</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tsipras</surname><given-names>D.</given-names></name><name name-style="western"><surname>Vladu</surname><given-names>A.</given-names></name></person-group><article-title>Towards deep learning models resistant to adversarial attacks</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.06083</pub-id></element-citation></ref><ref id="B35-sensors-25-05664"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>T.B.</given-names></name><name name-style="western"><surname>Man&#233;</surname><given-names>D.</given-names></name><name name-style="western"><surname>Roy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Abadi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gilmer</surname><given-names>J.</given-names></name></person-group><article-title>Adversarial patch</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1712.09665</pub-id></element-citation></ref><ref id="B36-sensors-25-05664"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tao</surname><given-names>G.</given-names></name><name name-style="western"><surname>An</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Hard-label black-box universal adversarial patch attack</article-title><source>Proceedings of the 32nd USENIX Security Symposium (USENIX Security 23)</source><conf-loc>Anaheim, CA, USA</conf-loc><conf-date>9&#8211;11 August 2023</conf-date><fpage>697</fpage><lpage>714</lpage></element-citation></ref><ref id="B37-sensors-25-05664"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>K.</given-names></name><name name-style="western"><surname>Nevatia</surname><given-names>R.</given-names></name></person-group><article-title>PatchZero: Defending against adversarial patch attacks by detecting and zeroing the patch</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>2&#8211;7 January 2023</conf-date><fpage>4632</fpage><lpage>4641</lpage></element-citation></ref><ref id="B38-sensors-25-05664"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kantipudi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dubey</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Chakraborty</surname><given-names>S.</given-names></name></person-group><article-title>Color channel perturbation attacks for fooling convolutional neural networks and a defense against such attacks</article-title><source>IEEE Trans. Artif. Intell.</source><year>2020</year><volume>1</volume><fpage>181</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1109/TAI.2020.3046167</pub-id></element-citation></ref><ref id="B39-sensors-25-05664"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name></person-group><article-title>Color-channel adversarial attack with resolution based camouflaging</article-title><source>Soft Comput.</source><year>2025</year><volume>29</volume><fpage>3835</fpage><lpage>3846</lpage><pub-id pub-id-type="doi">10.1007/s00500-025-10660-6</pub-id></element-citation></ref><ref id="B40-sensors-25-05664"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name></person-group><article-title>AlphaDog: No-Box Camouflage Attacks via Alpha Channel Oversight</article-title><source>Proceedings of the Network and Distributed System Security (NDSS) Symposium 2025</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>24&#8211;28 February 2025</conf-date></element-citation></ref><ref id="B41-sensors-25-05664"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Noever</surname><given-names>D.</given-names></name><name name-style="western"><surname>McKee</surname><given-names>F.</given-names></name></person-group><article-title>Exploiting Alpha Transparency in Language and Vision-Based AI Systems</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2402.09671</pub-id><pub-id pub-id-type="doi">10.48550/arXiv.2402.09671</pub-id></element-citation></ref><ref id="B42-sensors-25-05664"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name></person-group><article-title>Efficient multi-scale network with learnable discrete wavelet transform for blind motion deblurring</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle WA, USA</conf-loc><conf-date>17&#8211;21 June 2024</conf-date><fpage>2733</fpage><lpage>2742</lpage></element-citation></ref><ref id="B43-sensors-25-05664"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>Z.U.</given-names></name><name name-style="western"><surname>Jobson</surname><given-names>D.J.</given-names></name><name name-style="western"><surname>Woodell</surname><given-names>G.A.</given-names></name></person-group><article-title>Retinex processing for automatic image enhancement</article-title><source>J. Electron. Imaging</source><year>2004</year><volume>13</volume><fpage>100</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1117/1.1636183</pub-id></element-citation></ref><ref id="B44-sensors-25-05664"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Terven</surname><given-names>J.</given-names></name><name name-style="western"><surname>C&#243;rdova-Esparza</surname><given-names>D.-M.</given-names></name><name name-style="western"><surname>Romero-Gonz&#225;lez</surname><given-names>J.-A.</given-names></name></person-group><article-title>A comprehensive review of yolo architectures in computer vision: From yolov1 to yolov8 and yolo-nas</article-title><source>Mach. Learn. Knowl. Extr.</source><year>2023</year><volume>5</volume><fpage>1680</fpage><lpage>1716</lpage><pub-id pub-id-type="doi">10.3390/make5040083</pub-id></element-citation></ref><ref id="B45-sensors-25-05664"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ergu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>B.</given-names></name></person-group><article-title>A Review of Yolo algorithm developments</article-title><source>Procedia Comput. Sci.</source><year>2022</year><volume>199</volume><fpage>1066</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2022.01.135</pub-id></element-citation></ref><ref id="B46-sensors-25-05664"><label>46.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sohan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ram</surname><given-names>T.S.</given-names></name><name name-style="western"><surname>Reddy</surname><given-names>C.V.R.</given-names></name></person-group><article-title>A review on yolov8 and its advancements</article-title><source>International Conference on Data Intelligence and Cognitive Informatics</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2024</year></element-citation></ref><ref id="B47-sensors-25-05664"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yaseen</surname><given-names>M.</given-names></name></person-group><article-title>What is YOLOv8: An in-depth exploration of the internal features of the next-generation object detector</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2408.15857</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05664-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overall system architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g001.jpg"/></fig><fig position="float" id="sensors-25-05664-f002" orientation="portrait"><label>Figure 2</label><caption><p>System architecture of the industrial internal and external thread image acquisition platform.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g002.jpg"/></fig><fig position="float" id="sensors-25-05664-f003" orientation="portrait"><label>Figure 3</label><caption><p>Internal and external thread imaging hardware design.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g003.jpg"/></fig><fig position="float" id="sensors-25-05664-f004" orientation="portrait"><label>Figure 4</label><caption><p>System structural breakdown and design details.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g004.jpg"/></fig><fig position="float" id="sensors-25-05664-f005" orientation="portrait"><label>Figure 5</label><caption><p>Architecture of the proposed dual-stage DarkIR network for low-light enhancement and image deblurring.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g005.jpg"/></fig><fig position="float" id="sensors-25-05664-f006" orientation="portrait"><label>Figure 6</label><caption><p>Architecture of the baseline YOLOv8 detection network. The structure is redrawn with reference to &#8220;What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector&#8221; [<xref rid="B47-sensors-25-05664" ref-type="bibr">47</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g006.jpg"/></fig><fig position="float" id="sensors-25-05664-f007" orientation="portrait"><label>Figure 7</label><caption><p>Architecture of the proposed SLF-YOLO detection network with modular enhancements.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g007.jpg"/></fig><fig position="float" id="sensors-25-05664-f008" orientation="portrait"><label>Figure 8</label><caption><p>Comparison of thread corrosion defect images before and after dynamic deblurring.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g008.jpg"/></fig><fig position="float" id="sensors-25-05664-f009" orientation="portrait"><label>Figure 9</label><caption><p>Internal thread image enhancement using DarkIR under low-light conditions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g009.jpg"/></fig><fig position="float" id="sensors-25-05664-f010" orientation="portrait"><label>Figure 10</label><caption><p>Progressive reconstruction of thread defect images using RDDM at different training stages.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g010.jpg"/></fig><fig position="float" id="sensors-25-05664-f011" orientation="portrait"><label>Figure 11</label><caption><p>Detection results of SLF-YOLO on various real-world thread defect types: scratch, broken, and corrosion.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g011.jpg"/></fig><fig position="float" id="sensors-25-05664-f012" orientation="portrait"><label>Figure 12</label><caption><p>Training and validation loss curves and performance metric trends of the SLF-YOLO model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g012.jpg"/></fig><fig position="float" id="sensors-25-05664-f013" orientation="portrait"><label>Figure 13</label><caption><p>Visual and statistical impact comparison of alpha, CCP, and patch adversarial attacks on thread defect detection using SLF-YOLO.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g013.jpg"/></fig><fig position="float" id="sensors-25-05664-f014" orientation="portrait"><label>Figure 14</label><caption><p>FID variation of RDDM-generated thread defect images under different sampling steps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g014.jpg"/></fig><fig position="float" id="sensors-25-05664-f015" orientation="portrait"><label>Figure 15</label><caption><p>Visualization of adversarial perturbation effects on industrial thread defect images under three attack types: alpha channel attack, color channel perturbation (CCP), and patch-based attack.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g015.jpg"/></fig><fig position="float" id="sensors-25-05664-f016" orientation="portrait"><label>Figure 16</label><caption><p>Grad-CAM visualizations of SLF-YOLO attention maps under different perturbation scenarios.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g016.jpg"/></fig><fig position="float" id="sensors-25-05664-f017" orientation="portrait"><label>Figure 17</label><caption><p>Grayscale histogram comparison between AI-perceived image (IAI) and human-viewed image (IEye) under alpha channel attack.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g017.jpg"/></fig><fig position="float" id="sensors-25-05664-f018" orientation="portrait"><label>Figure 18</label><caption><p>MSE heatmap highlighting structural deviations caused by alpha channel attack.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05664-g018.jpg"/></fig><table-wrap position="float" id="sensors-25-05664-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05664-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of representative adversarial attack types on visual systems.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Attack Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Perceptible</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Implementation Complexity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Attack Strength</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Applicability</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Engineering Deployment Risk</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Attack Type</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alpha</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Very High</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Alpha</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CCP</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">CCP</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Patch</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Patch</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05664-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05664-t002_Table 2</object-id><label>Table 2</label><caption><p>Training configuration of the residual denoising diffusion model (RDDM) for industrial thread defect synthesis.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Image size</td><td align="center" valign="middle" rowspan="1" colspan="1">640 &#215; 640</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Timesteps</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training steps</td><td align="center" valign="middle" rowspan="1" colspan="1">800</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">1 &#215; 10<sup>&#8722;4</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Loss function</td><td align="center" valign="middle" rowspan="1" colspan="1">L1 + LPIPS + Residual penalty</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Augmentations</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rotation, crop, contrast jitter</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05664-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05664-t003_Table 3</object-id><label>Table 3</label><caption><p>Detection performance metrics of SLF-YOLO under alpha channel attack.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (P)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (R)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5&#8211;0.95</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">No Attack</td><td align="center" valign="middle" rowspan="1" colspan="1">0.893</td><td align="center" valign="middle" rowspan="1" colspan="1">0.958</td><td align="center" valign="middle" rowspan="1" colspan="1">0.969</td><td align="center" valign="middle" rowspan="1" colspan="1">0.717</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Alpha</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.017</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0045</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05664-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05664-t004_Table 4</object-id><label>Table 4</label><caption><p>Quantitative comparison of different preprocessing algorithms in motion deblurring and low-light enhancement tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">PSNR (dB) &#8593;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Motion deblurring</td><td align="center" valign="middle" rowspan="1" colspan="1">MLWNet-B</td><td align="center" valign="middle" rowspan="1" colspan="1">30.3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.940</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeblurGAN-v2</td><td align="center" valign="middle" rowspan="1" colspan="1">27.6</td><td align="center" valign="middle" rowspan="1" colspan="1">0.903</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SRN</td><td align="center" valign="middle" rowspan="1" colspan="1">28.7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.910</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wiener filter</td><td align="center" valign="middle" rowspan="1" colspan="1">25.4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.871</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Median filter</td><td align="center" valign="middle" rowspan="1" colspan="1">24.9</td><td align="center" valign="middle" rowspan="1" colspan="1">0.862</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unsharp masking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.874</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Low-light enhancement</td><td align="center" valign="middle" rowspan="1" colspan="1">DarkIR</td><td align="center" valign="middle" rowspan="1" colspan="1">26.4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.945</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HWMNet</td><td align="center" valign="middle" rowspan="1" colspan="1">27.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.922</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FLOL</td><td align="center" valign="middle" rowspan="1" colspan="1">25.9</td><td align="center" valign="middle" rowspan="1" colspan="1">0.920</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Histogram equalization</td><td align="center" valign="middle" rowspan="1" colspan="1">22.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CLAHE</td><td align="center" valign="middle" rowspan="1" colspan="1">23.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.857</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Retinex-based method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.869</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The upward arrows (&#8593;) in the table headers indicate that higher values correspond to better performance.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05664-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05664-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation study on structural components of SLF-YOLO.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5:0.95</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Baseline</td><td align="center" valign="middle" rowspan="1" colspan="1">0.821 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.718 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.759 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.411 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">11.12</td><td align="center" valign="middle" rowspan="1" colspan="1">28.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SC_C2f</td><td align="center" valign="middle" rowspan="1" colspan="1">0.842 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.742 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.781 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.445 &#177;0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">10.16</td><td align="center" valign="middle" rowspan="1" colspan="1">25.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Light-SSF_Neck</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.691 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.776 &#177; 0.02 </td><td align="center" valign="middle" rowspan="1" colspan="1">0.445 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">10.33</td><td align="center" valign="middle" rowspan="1" colspan="1">26.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FIMetal-IoU</td><td align="center" valign="middle" rowspan="1" colspan="1">0.855 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.741 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.774 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.449 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">11.12</td><td align="center" valign="middle" rowspan="1" colspan="1">28.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SC + Neck</td><td align="center" valign="middle" rowspan="1" colspan="1">0.847 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.784 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.793 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.462 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">9.65</td><td align="center" valign="middle" rowspan="1" colspan="1">24.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SC + IoU</td><td align="center" valign="middle" rowspan="1" colspan="1">0.864 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.755 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.785 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.449 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">10.16</td><td align="center" valign="middle" rowspan="1" colspan="1">25.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Neck + IoU</td><td align="center" valign="middle" rowspan="1" colspan="1">0.868 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.665 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.785 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.458 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">10.33</td><td align="center" valign="middle" rowspan="1" colspan="1">26.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">All</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.881 &#177; 0.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.794 &#177; 0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.813 &#177; 0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.521 &#177; 0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.6</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Performance metrics (precision, recall, mAP@0.5, and mAP@0.5:0.95) are reported as mean &#177; standard deviation over five independent runs with different random seeds. Model size (Params) and computational complexity (FLOPs) are deterministic values and thus reported without variance. Statistical significance was assessed using paired t-tests, and improvements of the proposed model are significant at <italic toggle="yes">p</italic> &lt; 0.05 compared with baseline methods.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05664-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05664-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of SLF-YOLO with state-of-the-art lightweight YOLO models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Models</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.862 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.629 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.725 &#177; 0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.869 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.732 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.832 &#177; 0.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv9s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.870 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.729 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.829 &#177; 0.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv10s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.850 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.703 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.817 &#177; 0.02</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.881 &#177; 0.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.794 &#177; 0.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.813 &#177; 0.04</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Baseline results for YOLOv5s&#8211;YOLOv10s are obtained from official implementations with recommended training settings. Our results are averaged over five independent runs (&#177;std), ensuring statistical robustness.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05664-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05664-t007_Table 7</object-id><label>Table 7</label><caption><p>Performance of SLF-YOLO under different adversarial attacks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (P)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (R)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5&#8211;0.95</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">No Attack</td><td align="center" valign="middle" rowspan="1" colspan="1">0.881</td><td align="center" valign="middle" rowspan="1" colspan="1">0.794</td><td align="center" valign="middle" rowspan="1" colspan="1">0.813</td><td align="center" valign="middle" rowspan="1" colspan="1">0.521</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alpha</td><td align="center" valign="middle" rowspan="1" colspan="1">0.017</td><td align="center" valign="middle" rowspan="1" colspan="1">0.128</td><td align="center" valign="middle" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" rowspan="1" colspan="1">0.005</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CCP</td><td align="center" valign="middle" rowspan="1" colspan="1">0.732</td><td align="center" valign="middle" rowspan="1" colspan="1">0.373</td><td align="center" valign="middle" rowspan="1" colspan="1">0.515</td><td align="center" valign="middle" rowspan="1" colspan="1">0.340</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Patch</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.754</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.840</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.764</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.511</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Results are obtained from five independent attack runs; variations were consistently small (&#177;0.002&#8211;0.006). Reported values are means.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>