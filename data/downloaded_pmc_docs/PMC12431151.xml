<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431151</article-id><article-id pub-id-type="pmcid-ver">PMC12431151.1</article-id><article-id pub-id-type="pmcaid">12431151</article-id><article-id pub-id-type="pmcaiid">12431151</article-id><article-id pub-id-type="doi">10.3390/s25175236</article-id><article-id pub-id-type="publisher-id">sensors-25-05236</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>SOAR-RL: Safe and Open-Space Aware Reinforcement Learning for Mobile Robot Navigation in Narrow Spaces</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-6243-7826</contrib-id><name name-style="western"><surname>Jun</surname><given-names initials="M">Minkyung</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05236" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Park</surname><given-names initials="P">Piljae</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-05236" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7080-6630</contrib-id><name name-style="western"><surname>Jung</surname><given-names initials="H">Hoeryong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05236" ref-type="aff">1</xref><xref rid="c1-sensors-25-05236" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-25-05236"><label>1</label>Department of Mechanical Engineering, Konkuk University, 120 Neungdong-ro, Gwangjin-gu, Seoul 05029, Republic of Korea; <email>minkyung2628@konkuk.ac.kr</email></aff><aff id="af2-sensors-25-05236"><label>2</label>AI SoC Department, Electronics and Telecommunications Research Institute (ETRI), 218 Gajeong-ro, Yuseong-gu, Daejeon 34129, Republic of Korea; <email>pjpark@etri.re.kr</email></aff><author-notes><corresp id="c1-sensors-25-05236"><label>*</label>Correspondence: <email>junghl80@konkuk.ac.kr</email>; Tel.: +82-2-450-3903</corresp></author-notes><pub-date pub-type="epub"><day>22</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5236</elocation-id><history><date date-type="received"><day>10</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>20</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>22</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05236.pdf"/><abstract><p>As human&#8211;robot shared service environments become increasingly common, autonomous navigation in narrow space environments (NSEs), such as indoor corridors and crosswalks, becomes challenging. Mobile robots must go beyond reactive collision avoidance and interpret surrounding risks to proactively select safer routes in dynamic and spatially constrained environments. This study proposes a deep reinforcement learning (DRL)-based navigation framework that enables mobile robots to interact with pedestrians while identifying and traversing open and safe spaces. The framework fuses 3D LiDAR and RGB camera data to recognize individual pedestrians and estimate their position and velocity in real time. Based on this, a human-aware occupancy map (HAOM) is constructed, combining both static obstacles and dynamic risk zones, and used as the input state for DRL. To promote proactive and safe navigation behaviors, we design a state representation and reward structure that guide the robot toward less risky areas, overcoming the limitations of traditional approaches. The proposed method is validated through a series of simulation experiments, including straight, L-shaped, and cross-shaped layouts, designed to reflect typical narrow space environments. Various dynamic obstacle scenarios were incorporated during both training and evaluation. The results demonstrate that the proposed approach significantly improves navigation success rates and reduces collision incidents compared to conventional navigation planners across diverse NSE conditions.</p></abstract><kwd-group><kwd>deep reinforcement learning</kwd><kwd>mobile robot navigation</kwd><kwd>narrow space environments (NSEs)</kwd><kwd>sector-based spatial representation</kwd><kwd>socially aware planning</kwd></kwd-group><funding-group><award-group><funding-source>Ministry of Trade, Industry and Energy</funding-source><award-id>RS-2023-00237035</award-id><award-id>RS-2024-00446197</award-id></award-group><award-group><funding-source>Ministry of the Interior and Safety</funding-source><award-id>20018247</award-id></award-group><funding-statement>This research was supported in part by the Human Resources Development Program of the Korea Institute of Energy Technology Evaluation and Planning (KETEP), funded by the Ministry of Trade, Industry and Energy (No. RS-2023-00237035), and by the Technology Innovation Program (No. RS-2024-00446197), also funded by the Ministry of Trade, Industry and Energy (MOTIE). This work was additionally supported by a grant from the Korea Evaluation Institute of Industrial Technology (KEIT), funded by the Ministry of the Interior and Safety (MOIS) (No. 20018247).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05236"><title>1. Introduction</title><p>Mobile robots are increasingly being deployed across a range of service domains, such as last-mile delivery and security patrols. Particularly, the widespread adoption of door-to-door delivery services has led to more frequent robotic navigation in outdoor pedestrian areas, such as sidewalks and crosswalks [<xref rid="B1-sensors-25-05236" ref-type="bibr">1</xref>], and indoor environments, such as building corridors [<xref rid="B2-sensors-25-05236" ref-type="bibr">2</xref>], with dense human traffic. As robots begin to coexist with humans within constrained areas, known as narrow-space environments (NSEs), ensuring both safety and social acceptability in autonomous navigation has become a critical challenge.</p><p>In constrained and dynamic spaces [<xref rid="B3-sensors-25-05236" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05236" ref-type="bibr">4</xref>], robots frequently operate in close proximity to humans, where traditional reactive collision avoidance is insufficient to achieve smooth and natural navigation. Instead, robots must be capable of recognizing and predicting pedestrian movements in real time [<xref rid="B5-sensors-25-05236" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05236" ref-type="bibr">6</xref>] while comprehending the surrounding spatial structure to generate behaviors that are both safe and socially coherent [<xref rid="B7-sensors-25-05236" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05236" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05236" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05236" ref-type="bibr">10</xref>].</p><p>Conventional path planning frameworks typically rely on static maps of predefined environments, combining global planners such as A* [<xref rid="B11-sensors-25-05236" ref-type="bibr">11</xref>] or Dijkstra&#8217;s algorithm [<xref rid="B12-sensors-25-05236" ref-type="bibr">12</xref>] with local reactive methods such as the dynamic window approach (DWA) [<xref rid="B13-sensors-25-05236" ref-type="bibr">13</xref>] or timed elastic band (TEB) [<xref rid="B14-sensors-25-05236" ref-type="bibr">14</xref>]. Although these methods perform well in static environments, they fall short when dealing with newly detected or dynamic obstacles [<xref rid="B15-sensors-25-05236" ref-type="bibr">15</xref>]. Consequently, recent studies have emphasized socially-aware navigation where robots adapt their paths in anticipation of human behavior and implicit norms [<xref rid="B16-sensors-25-05236" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05236" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05236" ref-type="bibr">18</xref>].</p><p>To address this need, recent studies have explored deep reinforcement learning (DRL) and multi-sensor fusion. The fusion of 3D LiDAR and RGB camera data enables accurate and real-time pedestrian detection and tracking [<xref rid="B19-sensors-25-05236" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05236" ref-type="bibr">20</xref>], whereas human trajectory prediction facilitates dynamic risk estimation and yields informed navigation decisions [<xref rid="B5-sensors-25-05236" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05236" ref-type="bibr">6</xref>,<xref rid="B21-sensors-25-05236" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05236" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05236" ref-type="bibr">23</xref>]. Several studies introduced the concepts of dynamic danger zones [<xref rid="B24-sensors-25-05236" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05236" ref-type="bibr">25</xref>], risk maps, and socially informed state encodings [<xref rid="B26-sensors-25-05236" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05236" ref-type="bibr">27</xref>] to enhance robot decision making in crowded environments. Although these advances surpass traditional planning approaches, several methods primarily focus on collision avoidance and often neglect the importance of identifying and leveraging relatively safe or open regions during navigation.</p><p>To overcome these challenges, we propose a reinforcement learning (RL) framework for NSEs, where robots and pedestrians move in close proximity. In contrast to traditional approaches that focus solely on collision avoidance, the proposed method enables robots to identify and utilize open spaces for navigation. The framework fuses 3D LiDAR and RGB camera data to detect pedestrians in real time and constructs a risk-aware map based on perceived information. This map is then used to extract spatial obstacle features that serve as inputs for robot learning. Consequently, the robot can safely navigate confined NSEs by considering obstacle avoidance and traversable space selection. The key contributions of this study are summarized as follows.</p><list list-type="order"><list-item><p>Sensor fusion-based risk-aware map generation: We implement a pedestrian-aware perception system by fusing 3D LiDAR and RGB camera inputs to individually detect pedestrians and track their positions, velocities, and directions. This information is used to define personalized danger zones [<xref rid="B24-sensors-25-05236" ref-type="bibr">24</xref>] and construct a human-aware occupancy map (HAOM) by integrating dynamic risk with static obstacles.</p></list-item><list-item><p>Sector-based spatial feature extraction: Building on the generated HAOM, we propose sector-based spatial encoding that extracts obstacle proximity, pedestrian velocity, and available free space as relative spatial features between the robot and surrounding objects. These features provide structured spatial information that allows the robot to effectively perceive and comprehend its surrounding environment during navigation.</p></list-item><list-item><p>RL for open-space-seeking behavior: To encourage the robot to actively seek open and safe directions, we incorporate the open-space preference into the RL framework by designing reward functions that go beyond simple collision avoidance. The reward combines multiple components, including goal proximity, dynamic risk avoidance, and open-space alignment, enabling the robot to learn safer and more traversable paths, even in complex and crowded environments.</p></list-item></list></sec><sec id="sec2-sensors-25-05236"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-05236"><title>2.1. System Overview</title><p>This study proposes an integrated path planning system for NSEs that enables safe and efficient navigation of mobile robots near dynamic pedestrians. As shown in <xref rid="sensors-25-05236-f001" ref-type="fig">Figure 1</xref>, the overall framework combines (a) sensor fusion for pedestrian detection; (b) HAOM generation, integrating static and dynamic obstacles; and (c) RL-based path planning using encoded spatial information.</p><p>The 3D LiDAR point cloud is pre-processed to extract points within the robot&#8217;s forward field of view (FOV) and fused with pedestrian bounding boxes detected from the RGB camera images. This multimodal fusion enables the identification and tracking of individual pedestrians and the estimation of their positions, velocities, and heading directions in real time. Based on these estimates, personalized danger zones [<xref rid="B24-sensors-25-05236" ref-type="bibr">24</xref>] are assigned for each pedestrian. Combined with static obstacles, this information is used to generate an HAOM that reflects both spatial constraints and dynamic risks posed by pedestrians. The HAOM is then converted into a structured state vector containing obstacle proximity, pedestrian dynamics, and open-space information, which serves as an input for the RL agent. The RL policy learns to navigate by not only avoiding collisions but also proactively selecting safer and more traversable paths based on real-time spatial awareness.</p><p><xref rid="sensors-25-05236-f002" ref-type="fig">Figure 2</xref> illustrates the system&#8217;s execution pipeline during real-world deployment. The robot detects pedestrians using onboard RGB and LiDAR sensors, whose outputs are fused to generate the HAOM. This representation, together with the robot&#8217;s current state (pose and previous action) and the designated target goal, is provided to the trained ONNX-based policy network. The policy network then produces velocity commands, which are transmitted via ROS to actuate the robot. This closed-loop process operates continuously, enabling real-time navigation in dynamic environments.</p></sec><sec id="sec2dot2-sensors-25-05236"><title>2.2. Individual Human Recognition via 3D LiDAR&#8211;RGB Camera Fusion</title><p>To enable safe and precise navigation in NSEs, we developed a multi-sensor fusion module to recognize individual pedestrians using 3D LiDAR and an RGB camera. As shown in <xref rid="sensors-25-05236-f003" ref-type="fig">Figure 3</xref>, the pipeline consists of three primary stages: (a) pre-processing of 3D LiDAR point clouds; (b) human detection through RGB&#8211;LiDAR data fusion; and (c) individual-level human clustering and tracking.</p><sec id="sec2dot2dot1-sensors-25-05236"><title>2.2.1. Preprocessing of 3D LiDAR Data</title><p><xref rid="sensors-25-05236-f003" ref-type="fig">Figure 3</xref>a shows the preprocessing of the 3D LiDAR point cloud data. The raw point cloud acquired from 3D LiDAR is first segmented into ground and nonground components using the patchwork algorithm [<xref rid="B28-sensors-25-05236" ref-type="bibr">28</xref>], which exploits the local smoothness and elevation priors, and the ground points are subsequently removed. This segmentation is based on the premise that navigation-relevant obstacles, such as pedestrians or indoor furniture, are primarily located within nonground regions. The region of interest (ROI) is restricted to the robot&#8217;s forward-facing zone to facilitate spatial alignment with the FOV of the RGB camera. The extracted point clouds are down sampled to ensure computational efficiency.</p></sec><sec id="sec2dot2dot2-sensors-25-05236"><title>2.2.2. RGB&#8211;LiDAR Fusion for Individual Human Detection</title><p>Pedestrian detection is performed on the RGB image using a YOLO-based object detector that outputs bounding boxes for each detected person. In our implementation, we used the YOLOv12 small model to ensure real-time performance. The detection threshold is set with confidence &#8805; 0.5, and the detection module is configured to maintain the identity of tracked pedestrians across frames while restricting recognition to the person class only. For each bounding box, a unique ID is assigned to its center point. The ROI-filtered LiDAR point cloud obtained from <xref rid="sec2dot2dot1-sensors-25-05236" ref-type="sec">Section 2.2.1</xref> is then projected onto the image plane, aligning the 3D points with the 2D bounding boxes. Each projected point is matched to the nearest bounding box by computing its Euclidean distance to the box center and is assigned to the corresponding group of that bounding box.</p><p>Because the original LiDAR points are 3D vectors, but projection onto the 2D image removes depth information, points belonging to non-pedestrian objects behind the pedestrian (e.g., walls) can get incorrectly assigned to the group of pedestrians. The Euclidean distances from the robot to all the points in each group are computed to eliminate distant and potentially irrelevant points in the pedestrian point cloud. The minimum of these distances is defined as <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">min_range</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and an experimentally determined margin, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is added to define a threshold. Only points within <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">min_range</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are retained, whereas those beyond this threshold are considered outliers and removed. This filtering strategy effectively excludes outliers as illustrated in <xref rid="sensors-25-05236-f004" ref-type="fig">Figure 4</xref>.</p><p>After filtering the irrelevant points, each group retains its initially assigned unique ID. To visualize the clustering results, each pedestrian cluster is assigned a distinct color corresponding to its ID.</p></sec><sec id="sec2dot2dot3-sensors-25-05236"><title>2.2.3. Human Tracking and Dynamic Risk Estimation</title><p>The centroid is computed and tracked for each identified pedestrian cluster. Based on the centroid positions <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, velocity vectors <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are estimated by calculating the temporal differences across consecutive frames. These vectors represent both the heading direction and walking speed of each pedestrian. Using this information, individual-specific dynamic danger zones [<xref rid="B24-sensors-25-05236" ref-type="bibr">24</xref>] are generated, enabling localized risk estimation around moving agents. These zones dynamically adapt to pedestrian movement and are critical for modeling human-aware spatial constraints. The danger zone for each pedestrian is defined by Equations (1) and (2) as follows:<disp-formula id="FD1-sensors-25-05236"><label>(1)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05236"><label>(2)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>11</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.4</mml:mn><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the sector radius, and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the sector angle of i-th pedestrian, both determined based on the pedestrian&#8217;s walking velocity <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The sector radius <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> increases linearly with speed, where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the stride length of a stationary person, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is an empirically derived scaling factor. The sector angle <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is exponentially adjusted according to <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to reflect narrower danger zones at higher speeds, as described in Equation (2). This allows the danger zone to adaptively reflect pedestrian motion and risk levels. Thus, each pedestrian is individually recognized, and their position, velocity, and corresponding danger zone can be extracted in real time based on cluster centroids. The visual representation of this formulation, including the radius <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and sector angle <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is illustrated in <xref rid="sensors-25-05236-f003" ref-type="fig">Figure 3</xref>c.</p></sec></sec><sec id="sec2dot3-sensors-25-05236"><title>2.3. HAOM Generation</title><p>To generate the HAOM, the ROI-filtered point cloud is segmented into static and dynamic components. Points that do not overlap with any detected pedestrian clusters are classified as static obstacles, representing structures such as walls or fixtures. For the dynamic components, the centroid positions of pedestrian clusters <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and their corresponding danger zones (defined in <xref rid="sec2dot2dot3-sensors-25-05236" ref-type="sec">Section 2.2.3</xref>) are considered. Using this information, the HAOM is constructed as a multilayered occupancy map, as illustrated in <xref rid="sensors-25-05236-f005" ref-type="fig">Figure 5</xref>.</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: Represents static obstacles identified from the point cloud;</p></list-item><list-item><p><inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: Contains the centroid positions of the detected pedestrian clusters;</p></list-item><list-item><p><inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Z</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: Encodes individualized danger zones around each pedestrian.</p></list-item></list><p>The regions, which are not covered by the static, human, or danger zone layers, constitute the open-space region, implicitly capturing the remaining traversable area around the robot.</p><p>The final HAOM encodes both the spatial geometry and dynamic pedestrian-induced risk, enabling spatial reasoning for safe navigation. The map is generated with a resolution of 0.01 m, and the grid consists of 1000 &#215; 1000 pixels covering a 10 m &#215; 10 m region, corresponding to the robot&#8217;s forward FOV.</p></sec><sec id="sec2dot4-sensors-25-05236"><title>2.4. Design of the Reinforcement Learning System for Open-Space-Seeking Behavior</title><sec id="sec2dot4dot1-sensors-25-05236"><title>2.4.1. State Definition</title><p>We designed a structured and multidimensional state vector for safe and spatially aware navigation in NSEs, as described in Equation (3):<disp-formula id="FD3-sensors-25-05236"><label>(3)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the state vector at time <italic toggle="yes">t</italic>. The HAOM integrates static obstacles, centroid positions of pedestrian clusters, personalized danger zones, and open spaces. The HAOM is partitioned into uniformly distributed sectors, and spatial features are extracted for each sector. The distance calculation between the robot and obstacles is defined in Equation (4):<disp-formula id="FD4-sensors-25-05236"><label>(4)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the distance between the robot and nearest static obstacle or pedestrian cluster centroid within sector <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the half-length of the HAOM, and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> is the normalized distance used as the distance feature. In <xref rid="sensors-25-05236-f006" ref-type="fig">Figure 6</xref>, the front area of the robot is evenly divided into angular sectors, and each divided region is denoted by sector <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Within each sector <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the Euclidean distance between the robot&#8217;s position and the closest pixel, labeled as either <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is computed. The minimum distance <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is normalized by <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, yielding the normalized distance feature <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> used in the state vector. If no obstacle is detected within the sector, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 0.</p><p>The velocity vector feature is defined by Equation (5):<disp-formula id="FD5-sensors-25-05236"><label>(5)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the projected velocity vector of a pedestrian within sector <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, with components <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denoting the velocities along the x- and y-axes, respectively. The area within the camera&#8217;s FOV is uniformly divided into <italic toggle="yes">m</italic> angular sectors, each denoted as <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref rid="sensors-25-05236-f007" ref-type="fig">Figure 7</xref>. The individually identified pedestrians are tracked to estimate their velocity vectors using the perception system described in <xref rid="sec2dot2dot3-sensors-25-05236" ref-type="sec">Section 2.2.3</xref>. For each tracked pedestrian, the tracking marker <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is projected onto one of the sectors <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and the corresponding velocity vector <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, computed from the displacement over time, is assigned to that sector. Furthermore, since the robot is also in motion, the velocity is represented as the relative velocity between each pedestrian and the robot. When no dynamic obstacle is present in a sector, the corresponding velocity value is set to zero. If multiple pedestrians fall into the same sector, the velocity of the pedestrian closest to the robot is selected.</p><p>The traversable distance information <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is defined in Equation (6):<disp-formula id="FD6-sensors-25-05236"><label>(6)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the maximum obstacle-free traversable distance along sector <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the half-length of the HAOM and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> is the normalized open-space value used as the open-space feature. Open-space features are computed by conservatively accounting for static and dynamic obstacles using danger zones. As shown in <xref rid="sensors-25-05236-f008" ref-type="fig">Figure 8</xref>, within each sector <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the traversable distance <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated along the sector direction by excluding all obstacles while including static elements and dynamic agents, along with their respective danger zones. This value is normalized by <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to produce the open-space feature <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Unlike <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as in Equation (4), which indicates the proximity to an obstacle and becomes zero when no obstacle is detected, <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> quantifies the amount of free space in a sector and reaches 1 when the sector is entirely free of obstacles.</p><p>In addition to the sector-based features, the state vector includes the robot&#8217;s current position, goal position, and action at the previous time step to ensure smoother behavior and temporal consistency.<disp-formula id="FD7-sensors-25-05236"><label>(7)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05236"><label>(8)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05236"><label>(9)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the robot&#8217;s current position, goal position, and the previous action at timestep <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. By combining sector-based observations with these additional variables, the state vectors capture the static structures, dynamic agent behaviors, and temporal information. This structured state representation supports socially aware decision making and robust path planning in crowded environments.</p></sec><sec id="sec2dot4dot2-sensors-25-05236"><title>2.4.2. Reward Design</title><p>We define a reward function consisting of seven distinct components to enable the robot to avoid collisions and proactively seek navigable open spaces in NSEs. These components are designed to complement each other and are combined as shown in Equation (10):<disp-formula id="FD10-sensors-25-05236"><label>(10)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>dz</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where the total reward <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is computed as a weighted sum of the individual components, with each term multiplied by its corresponding weight <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The weights are manually tuned through iterative experiments. The reward components are listed in <xref rid="sensors-25-05236-t001" ref-type="table">Table 1</xref> and formulated as follows:<disp-formula id="FD11-sensors-25-05236"><label>(11)</label><mml:math id="mm73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#160;</mml:mo><mml:mo>+</mml:mo><mml:mn>1.0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#160;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.0</mml:mn></mml:mtd><mml:mtd><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05236"><label>(12)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1.0</mml:mn></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#183;</mml:mo><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05236"><label>(13)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD14-sensors-25-05236"><label>(14)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#160;</mml:mo><mml:mn>0.1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>0.1</mml:mn><mml:mo>&#183;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mo>&#160;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>z</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.0</mml:mn></mml:mtd><mml:mtd><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD15-sensors-25-05236"><label>(15)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-05236"><label>(16)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#183;</mml:mo><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-05236"><label>(17)</label><mml:math id="mm79" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>&#8722;</mml:mo><mml:mn>10.0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#160;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.0</mml:mn></mml:mtd><mml:mtd><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To determine the coefficients of each reward term, a manual tuning process was conducted through iterative testing. The tuning followed a progressive strategy according to the complexity of the environment. First, in an environment with no obstacles, the weights of <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were tuned to ensure that the agent could reach the goal reliably without considering obstacle avoidance. Next, for environments with static obstacles, <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was introduced. However, we observed that using <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> alone often resulted in overly conservative behavior rather than active wall avoidance. To address this, <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was introduced to encourage the robot to maintain a safe buffer from obstacles. At this stage, the weight of the <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was also increased to reinforce goal-oriented navigation. Finally, in environments that included both static and dynamic obstacles, we designed and added two additional reward terms: <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. These were specifically developed to guide the agent in selecting safe and traversable paths, particularly under crowded conditions. The weights were adjusted to balance obstacle avoidance and goal-reaching performance.</p></sec><sec id="sec2dot4dot3-sensors-25-05236"><title>2.4.3. RL Configuration</title><p>The action space of the robot is defined by two continuous control variables: linear (<inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and angular velocity (<inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>):<disp-formula id="FD18-sensors-25-05236"><label>(18)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>At each timestep, the policy network outputs a pair of variables [<inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>]. The linear velocity is constrained to the range [&#8722;2.0, 2.0] m/s, and the angular velocity is limited to [&#8722;1.5, 1.5] rad/s, based on the specifications of the unmanned ground vehicle Jackal from Clearpath. Each episode terminates under one of the following three conditions: the robot reaches the goal, the robot collides with an obstacle, or the number of steps exceeds a predefined limit. Training was conducted using the proximal policy optimization (PPO) algorithm [<xref rid="B29-sensors-25-05236" ref-type="bibr">29</xref>], with the following key parameters: learning rate of 0.001, discount factor <italic toggle="yes">&#947;</italic> of 0.99, GAE coefficient <italic toggle="yes">&#955;</italic> of 0.95, clipping range of 0.2, entropy coefficient of 0.01, mini-batch size of 512, and mini-epoch number of 8. The training lasted for 1000 epochs, utilizing 512 parallel environment instances for maximal training efficiency.</p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-25-05236"><title>3. Results</title><sec id="sec3dot1-sensors-25-05236"><title>3.1. Experimental Setup</title><p>The reinforcement learning and performance evaluation of the proposed method is conducted in the simulation environment constructed using NVIDIA Isaac Sim. Owing to the high computational cost of sensor-based simulations involving real-time LiDAR and RGB streaming, a lightweight environment based on preprocessed state vectors, excluding real-time sensor streams, was used during training. The simulation was performed on a system equipped with a NVIDIA GeForce RTX 4090 GPU and an Intel Core i9-14900K CPU. The demonstration of the experimental results can be found in the <xref rid="app1-sensors-25-05236" ref-type="app">Supplementary Video (Video.mp4)</xref>.</p><sec id="sec3dot1dot1-sensors-25-05236"><title>3.1.1. Environmental Layout and Course Design</title><p>To evaluate the proposed method across varying levels of difficulty in constrained environments, three corridor configurations were implemented, as illustrated in <xref rid="sensors-25-05236-f009" ref-type="fig">Figure 9</xref>.</p><list list-type="bullet"><list-item><p>Straight course: A simple, one-directional corridor resembling a sidewalk or indoor hallway. This configuration involves minimal occlusions and low collision risks and serves as a baseline scenario.</p></list-item><list-item><p>L-shaped course: A right-angled corridor introducing moderate difficulty owing to limited visibility around the corners. This poses challenges for predicting pedestrian movement and planning in occluded areas.</p></list-item><list-item><p>Intersection course: A four-way junction allowing multidirectional pedestrian flow. This complex layout involves frequent dynamic interactions and high uncertainty, which makes it the most challenging scenario.</p></list-item></list><p>In each scenario, the starting position and goal of the robot were placed at opposite ends or junctions. The obstacles were initialized randomly, producing diverse navigation scenarios dependent on both the corridor structure and dynamic agent configuration. This setup enabled a comprehensive evaluation of the generalization and robustness of the learned navigation policy in NSEs.</p></sec><sec id="sec3dot1dot2-sensors-25-05236"><title>3.1.2. Dynamic Obstacle Modeling</title><p>The dynamic obstacles were modeled as cylinders, as shown in <xref rid="sensors-25-05236-f009" ref-type="fig">Figure 9</xref>. At the beginning of each episode, obstacles are randomly positioned within predefined regions, and their speeds are randomly set within the range of [0.0&#8211;1.0 m/s], reflecting the typical walking speed of pedestrians in narrow environments [<xref rid="B30-sensors-25-05236" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05236" ref-type="bibr">31</xref>]. The obstacle controller ensures that obstacles remain within their designated movement boundaries, allowing the reinforcement learning policy to generalize across diverse human-like motion patterns and initial configurations.</p></sec><sec id="sec3dot1dot3-sensors-25-05236"><title>3.1.3. Evaluation Metrics</title><p>The evaluation metrics used for quantitative performance assessment are listed in <xref rid="sensors-25-05236-t002" ref-type="table">Table 2</xref>.</p></sec></sec><sec id="sec3dot2-sensors-25-05236"><title>3.2. Performance of the Proposed Method Under Various Environmental Conditions</title><p>To evaluate the robustness of the proposed method under diverse environmental settings, experiments were conducted by combining three scenario types (straight, L-shaped, and intersecting) with varying numbers of dynamic obstacles, ranging from one to four. <xref rid="sensors-25-05236-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05236-f010" ref-type="fig">Figure 10</xref> summarize the performance of the proposed method under varying dynamic obstacle densities across the three scenarios.</p><p>As the number of dynamic obstacles increases from one to four, the success rate generally decreases, and the collision rate increases. The success rates ranged from 98.0% to 87.0%, whereas the collision rates ranged from 1.0% to 13.0% across all scenarios. Scenario 3 yielded the highest average returns across all tested scenarios, with a value of 225.2 &#177; 39.5 for a single dynamic obstacle, and a slight reduction to 209.8 &#177; 32.7 as the number of dynamic obstacles increased to four. Scenario 1 exhibited a decreasing trend from 157.1 &#177; 23.7 (1 obstacle) to 144.5 &#177; 40.2 (4 obstacles), whereas Scenario 2 exhibited a modest increase from 149.4 &#177; 30.5 to 166.9 &#177; 34.4 as the number of dynamic obstacles increased. Overall, although the absolute average return values differed across the scenarios, the intra-scenario variations due to the increasing number of dynamic obstacles remained moderate, generally within 10&#8211;12%.</p><p>To further assess the performance of the proposed method under more challenging conditions, additional tests were conducted in the intersection course with six dynamic obstacles, as shown in <xref rid="sensors-25-05236-f010" ref-type="fig">Figure 10</xref>. In this densely crowded intersection scenario, the proposed method achieved a success rate exceeding 80%.</p></sec><sec id="sec3dot3-sensors-25-05236"><title>3.3. Ablation Study on Open-Dir Alignment Reward (<inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>)</title><p>The contribution of the open-direction alignment reward <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as in Equation (16) was evaluated through ablation experiments in all scenarios. As listed in <xref rid="sensors-25-05236-t004" ref-type="table">Table 4</xref>, the presence of <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> consistently enhanced both SR and CR.</p><p>The addition of open-direction alignment reward <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> led to consistent improvements across all scenarios. In Scenario 1, the success rate increased by +14.0% (from 75.0% to 89.0%), and the collision rate decreased by 14.0% (from 25.0% to 11.0%). In Scenario 2, <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> improved the success rate by +15.0% and reduced the collision rate by &#8211;10.0%. In Scenario 3, the success rate rose by +16.0%, and the collision rate decreased by 14.0%.</p></sec><sec id="sec3dot4-sensors-25-05236"><title>3.4. Performance Comparison with Traditional Navigation Methods</title><p>This section compares the proposed safe and open-space-aware RL (SOAR-RL) method with conventional ROS-based local planners. The baseline methods include three widely used planners: DWA [<xref rid="B13-sensors-25-05236" ref-type="bibr">13</xref>], TEB [<xref rid="B14-sensors-25-05236" ref-type="bibr">14</xref>], and trajectory rollout (TR) [<xref rid="B32-sensors-25-05236" ref-type="bibr">32</xref>], which is a variant of DWA. These local planners have been extensively adopted in both industrial and academic domains and serve as standard benchmarks for mobile robot navigation. The experiments were conducted in a simulated environment using NVIDIA Isaac Sim integrated with ROS. All planners processed the sensor data simulated using a Velodyne VLP-16 LiDAR system. The global planner was fixed using Dijkstra&#8217;s algorithm, whereas each local planner and the proposed method were evaluated under the same maps and initial conditions, with 100 trials per scenario. <xref rid="sensors-25-05236-t005" ref-type="table">Table 5</xref> lists the quantitative analysis results.</p><p>In Scenario 1, the proposed method (SOAR-RL) achieved the highest success rate of 93.0%, significantly outperforming the TR [<xref rid="B32-sensors-25-05236" ref-type="bibr">32</xref>] (49.0%), DWA [<xref rid="B13-sensors-25-05236" ref-type="bibr">13</xref>] (39.0%), and TEB [<xref rid="B14-sensors-25-05236" ref-type="bibr">14</xref>] (32.0%) methods. It also had the lowest collision rate (7.0%) and zero timeouts. In contrast, TR exhibited a 13.0% collision rate and a 38.0% timeout rate. Although as shown in <xref rid="sensors-25-05236-f011" ref-type="fig">Figure 11</xref>c, TR showed a path length similar to that of (d) SOAR-RL, substantially more time was required to reach the goal. DWA (<xref rid="sensors-25-05236-f011" ref-type="fig">Figure 11</xref>a) and TEB (<xref rid="sensors-25-05236-f011" ref-type="fig">Figure 11</xref>b) frequently failed early in the trajectory, often resulting in immediate collisions with nearby obstacles. These early terminations explain the shorter average path lengths and reduced navigation times of DWA and TEB compared with those of SOAR-RL.</p><p>In Scenario 2, SOAR-RL exhibited superior performance, achieving a success rate of 94.0% compared with TR (48.0%), DWA (36.0%), and TEB (35.0%). It also achieved the lowest collision rate (6.0%), with no timeouts, whereas TR experienced 31.0% timeouts and 21.0% collisions. As depicted in <xref rid="sensors-25-05236-f012" ref-type="fig">Figure 12</xref>, DWA and TEB frequently failed while turning around the L-shaped corner, where restricted visibility and limited space made trajectory adjustment difficult. These failures resulted in shorter path lengths and times (DWA: 3.26 m, 12.64 s; TEB: 3.89 m, 15.17 s) compared with those of SOAR-RL, which followed a longer yet successful trajectory (15.10 m, 25.27 s). The higher average speed of SOAR-RL (0.61 m/s) also demonstrates efficient movement despite extended paths.</p><p>In Scenario 3, SOAR-RL maintained a strong performance in the most complex intersection layout, with a success rate of 88.0%, outperforming TR (49.0%), TEB (41.0%), and DWA (42.0%). The collision rate was 12.0%, and timeout occurred only 4.0% of the time. As shown in <xref rid="sensors-25-05236-f013" ref-type="fig">Figure 13</xref>, traditional planners, such as TR and DWA, often collided midway or failed to reach the goal within the allowed time, particularly in dense zones where crossing pedestrians obstructed the path. These behaviors resulted in higher timeout rates and shorter navigation durations for most methods. In contrast, SOAR-RL achieved the longest path length (20.99 m) and navigation time (34.03 s), thereby successfully avoiding collisions and delays.</p></sec><sec id="sec3dot5-sensors-25-05236"><title>3.5. Performance Comparison with AI-Based Crowd Navigation Methods</title><p>This section presents a comparative analysis of the proposed method against several AI-based crowd navigation methods. The comparison includes a traditional geometric approach, ORCA [<xref rid="B33-sensors-25-05236" ref-type="bibr">33</xref>], and state-of-the-art deep learning models, such as CADRL [<xref rid="B34-sensors-25-05236" ref-type="bibr">34</xref>], LSTM-RL [<xref rid="B35-sensors-25-05236" ref-type="bibr">35</xref>], and DSRNN [<xref rid="B36-sensors-25-05236" ref-type="bibr">36</xref>]. All experiments were carried out in a 6 m &#215; 6 m square environment populated with five dynamic obstacles. Each methodology was evaluated over 500 trials, and the quantitative results are summarized in <xref rid="sensors-25-05236-t006" ref-type="table">Table 6</xref>. As presented in <xref rid="sensors-25-05236-t006" ref-type="table">Table 6</xref>, the proposed SOAR-RL achieved a success rate of 94.0% and a collision rate of 6.0%. This rate is higher than those of other AI-based models, such as DSRNN (93.0%) and LSTM-RL (89.0%). The low collision rate with zero timeouts indicates that the proposed method shows stable and reliable navigation performance.</p><p>In terms of efficiency, SOAR-RL achieved the shortest average navigation time of 5.28 s with an average path length of 4.21 m. By contrast, DSRNN and LSTM-RL followed longer paths (10.01 m and 7.05 m, respectively), which reflects more conservative avoidance strategies and resulted in longer navigation times. ORCA generated a shorter path of 2.91 m, but this was associated with more aggressive maneuvering, leading to a collision rate of 68.0%. These results suggest that the proposed SOAR-RL provides an effective trade-off between safety and efficiency compared to the previous methods.</p></sec><sec id="sec3dot6-sensors-25-05236"><title>3.6. Real-Time Performance Analysis</title><p>To verify whether the proposed RL-based navigation framework satisfies real-time operation requirements, we evaluated its computational performance across multiple test scenarios. <xref rid="sensors-25-05236-t007" ref-type="table">Table 7</xref> presents the FPS recorded over the last 100 steps of simulation for each scenario. Scenarios 1, 2, and 3 were conducted with four dynamic obstacles in different spatial configurations, while Scenario 4 included the intersection and six dynamic obstacles. As shown in <xref rid="sensors-25-05236-t006" ref-type="table">Table 6</xref>, all scenarios achieved an average FPS above 21, which corresponds to an update interval of approximately 47 ms or faster. This confirms that the RL-based navigation system can process sensor data and make decisions within the time constraints required for real-time pedestrian interaction, assuming a maximum human walking speed of 2 m/s.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05236"><title>4. Discussion</title><p>The proposed method consistently maintained high performance as environmental complexity and dynamic obstacle density increased. As listed in <xref rid="sensors-25-05236-t003" ref-type="table">Table 3</xref>, the success rate declined moderately as the number of obstacles increased from one to four but remained above 87% in all scenarios. Similarly, the collision rate increased slightly but did not exceed 13%, indicating that the spatially informed state design and reward structure effectively supported reliable navigation, even in cluttered environments. In addition to the success rate, the average return was included as a metric to illustrate the consistency of learning performance. Even as the number of obstacles increased, the average return remained relatively stable, indicating robust learning. The variation in the average return across scenarios is attributed to structural differences such as the goal distance and corridor width. Notably, the highest average return was observed in Scenario 3, likely due to the extended goal path and additional rewards accumulated through strategic detours and obstacle avoidance in the more complex layout. To test the robustness of the system further, an additional experiment was conducted in an extremely dense scenario with six dynamic obstacles. As shown in <xref rid="sensors-25-05236-f010" ref-type="fig">Figure 10</xref>, the robot successfully reached the goal while avoiding collisions with both moving agents and static walls. Even under high-density conditions with frequent interactions, the method achieved a success rate exceeding 80%, demonstrating its strong adaptability to complex social navigation environments.</p><p>The Open-Dir alignment reward proved essential for enhancing the path planning performance. Moreover, the ablation study showed that removing this reward caused the robot to rely heavily on reactive collision avoidance, often resulting in frequent collisions and timeouts owing to a lack of directional consistency. In contrast, with the reward enabled, the robot not only avoided obstacles but also maintained a consistent heading toward the traversable open space. This led to notable performance improvements, with success rates increasing from 75% to 89% in Scenario 1, from 78% to 93% in Scenario 2, and from 75% to 91% in Scenario 3. Both collision and timeout rates were reduced in all scenarios. Although a few timeout cases remained in Scenario 3, they likely stemmed from the robot choosing conservative detours at complex intersection layouts. Additionally, training took longer when an open-direction alignment reward was applied. This is because, without the reward, episodes often ended quickly owing to early collisions, resulting in shorter episode durations. In contrast, when the reward was enabled, the robot tended to reach the goal more frequently, which naturally extended the length of each episode. Therefore, the difference in total training time for 1000 epochs reflects the fact that the robot learned to avoid collisions and successfully reach the goal only with the open-direction alignment reward. Overall, the inclusion of the open-direction alignment reward encouraged more strategic path selection and contributed to more stable and efficient policy behavior.</p><p>The proposed method (SOAR-RL) demonstrated a robust balance between safety and efficiency compared to both conventional ROS-based local planners, such as DWA [<xref rid="B13-sensors-25-05236" ref-type="bibr">13</xref>], TEB [<xref rid="B14-sensors-25-05236" ref-type="bibr">14</xref>], and TR [<xref rid="B32-sensors-25-05236" ref-type="bibr">32</xref>], as well as other AI-based crowd navigation baselines, including CADRL [<xref rid="B34-sensors-25-05236" ref-type="bibr">34</xref>], LSTM-RL [<xref rid="B35-sensors-25-05236" ref-type="bibr">35</xref>], and DSRNN [<xref rid="B36-sensors-25-05236" ref-type="bibr">36</xref>]. Traditional planners rely primarily on reactive control based on immediate sensor inputs. As a result, they often face limitations in anticipating the future motion of dynamic obstacles. In contrast, SOAR-RL incorporates dynamic motion information into its state representation. This design contributed to higher success rates and lower collision rates across the evaluated scenarios. The comparison with specialized crowd navigation methods shows a similar trend. ORCA, for instance, produced relatively short paths but, given its limited consideration of longer-term motion, exhibited a substantially higher collision rate. Conversely, AI-based methods such as DSRNN and LSTM-RL generally employed more conservative avoidance strategies. This led to longer navigation times and increased path lengths. The proposed method generates trajectories that achieve a more favorable balance between safety and efficiency compared to both traditional planners and previous AI-based crowd navigation approaches.</p><p>Despite the advantages of the proposed method, several limitations must be addressed. First, the reward function exhibited high sensitivity to the relative weights of its components, and minor adjustments often led to noticeable changes in performance. Because the current weights were selected through manual tuning, future work should explore automated reward optimization techniques. Second, the HAOM-based state representation relies solely on the RGB camera&#8217;s field of view, which limits the robot&#8217;s situational awareness. This constraint can lead to collisions in lateral or rear blind spots outside the camera&#8217;s coverage. Moreover, the pedestrian detection module depends on YOLO-based bounding box detection, which may occasionally fail due to occlusion, rapid pedestrian movement, or challenging lighting conditions. Such missed detections can produce incomplete or inaccurate risk zone maps, reducing the reliability of the HAOM and increasing the likelihood of unsafe path decisions. In future work, we plan to integrate a pedestrian tracking module with motion prediction to preserve awareness of nearby pedestrians even during temporary occlusions or detection failures. This enhancement will also enable smoother temporal continuity in risk zone estimation. Future studies should include unified benchmarks for the methods. Moreover, real-world robot experiments and broader scenario tests are planned to further assess the generalizability and deployment feasibility of this method.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05236"><title>5. Conclusions</title><p>This study proposed a DRL-based path planning framework for mobile robots operating in NSEs, where frequent close interactions with dynamic pedestrians present significant navigation challenges. To enable socially aware behavior in such complex and constrained settings, we developed a sensor-fusion-based perception module that integrates 3D LiDAR and RGB camera data for real-time pedestrian detection. An HAOM was constructed by combining static obstacle positions with dynamically generated danger zones [<xref rid="B24-sensors-25-05236" ref-type="bibr">24</xref>], which reflect the estimated pedestrian motion trajectories. For effective learning, a 98-dimensional state vector was designed to represent the spatial distribution of obstacles, dynamic velocities, and traversable directions within a sector-based FOV. Additionally, a multi-objective reward function was established to promote collision avoidance and encourage the selection of safer and more open directions toward the goal.</p><p>Experimental validation in simulated environments featuring straight, L-shaped, and intersection corridors with varying numbers of dynamic obstacles confirmed the robustness of the proposed method. It consistently achieved high success rates and low collision rates across all scenarios, demonstrating reliable and adaptive path planning performance as complexity increased. Our findings demonstrate that SOAR-RL achieves robust and socially compliant navigation in NSEs, thus paving the way for real-world service deployment and academic exploration. This study contributes to the development of safe, human-aware autonomy. It provides a foundation for future extensions, including deployment in physical settings, improved sensing capabilities, and benchmarking against state-of-the-art RL-based planners.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-05236"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mdpi.com/article/10.3390/s25175236/s1">https://www.mdpi.com/article/10.3390/s25175236/s1</uri>, Video S1: Video.mp4</p><supplementary-material id="sensors-25-05236-s001" position="float" content-type="local-data" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05236-s001.zip" position="float" orientation="portrait"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization, M.J., P.P. and H.J.; Software, M.J.; Validation, M.J.; Formal analysis, M.J.; Resources, H.J.; Data curation, M.J.; Writing&#8212;original draft, M.J.; Writing&#8212;review &amp; editing, P.P. and H.J.; Supervision, H.J.; Project administration, H.J.; Funding acquisition, P.P. and H.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author. The data are not publicly available due to institutional restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05236"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pieroni</surname><given-names>R.</given-names></name><name name-style="western"><surname>Corno</surname><given-names>M.</given-names></name><name name-style="western"><surname>Parravicini</surname><given-names>F.</given-names></name><name name-style="western"><surname>Savaresi</surname><given-names>S.M.</given-names></name></person-group><article-title>Design of an automated street crossing management module for a delivery robot</article-title><source>Control. Eng. Pract.</source><year>2024</year><volume>153</volume><fpage>106095</fpage><pub-id pub-id-type="doi">10.1016/j.conengprac.2024.106095</pub-id></element-citation></ref><ref id="B2-sensors-25-05236"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Farina</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fontanelli</surname><given-names>D.</given-names></name><name name-style="western"><surname>Garulli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Giannitrapani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Prattichizzo</surname><given-names>D.</given-names></name></person-group><article-title>Walking ahead: The headed social force model</article-title><source>PLoS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0169734</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0169734</pub-id><pub-id pub-id-type="pmid">28076435</pub-id><pub-id pub-id-type="pmcid">PMC5226724</pub-id></element-citation></ref><ref id="B3-sensors-25-05236"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Trautman</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Murray</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Krause</surname><given-names>A.</given-names></name></person-group><article-title>Robot navigation in dense human crowds: Statistical models and experimental studies of human&#8211;robot cooperation</article-title><source>Int. J. Robot. Res.</source><year>2015</year><volume>34</volume><fpage>335</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1177/0278364914557874</pub-id></element-citation></ref><ref id="B4-sensors-25-05236"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Senft</surname><given-names>E.</given-names></name><name name-style="western"><surname>Satake</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kanda</surname><given-names>T.</given-names></name></person-group><article-title>Would you mind me if I pass by you? Socially-appropriate behaviour for an omni-based social robot in narrow environment</article-title><source>Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction</source><conf-loc>Cambridge, UK</conf-loc><conf-date>23&#8211;26 March 2020</conf-date></element-citation></ref><ref id="B5-sensors-25-05236"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>P&#233;rez-D&#8217;Arpino</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Goebel</surname><given-names>P.</given-names></name><name name-style="western"><surname>Mart&#237;n-Mart&#237;n</surname><given-names>R.</given-names></name><name name-style="western"><surname>Savarese</surname><given-names>S.</given-names></name></person-group><article-title>Robot navigation in constrained pedestrian environments using reinforcement learning</article-title><source>Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>30 May&#8211;5 June 2021</conf-date></element-citation></ref><ref id="B6-sensors-25-05236"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name></person-group><article-title>Towards Safe and Efficient Last-Mile Delivery: A Multi-Modal Socially Aware Navigation Framework for Autonomous Robots on Pedestrian-Crowded Sidewalks</article-title><source>Proceedings of the 2024 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE International Conference on Robotics, Automation and Mechatronics (RAM)</source><conf-loc>Hangzhou, China</conf-loc><conf-date>8&#8211;11 August 2024</conf-date></element-citation></ref><ref id="B7-sensors-25-05236"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fujioka</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kanda</surname><given-names>T.</given-names></name></person-group><article-title>I Need to Pass Through! Understandable Robot Behavior for Passing Interaction in Narrow Environment</article-title><source>Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</source><conf-loc>Boulder, CO, USA</conf-loc><conf-date>11&#8212;15 March 2024</conf-date></element-citation></ref><ref id="B8-sensors-25-05236"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ying</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>L.</given-names></name></person-group><article-title>Dynamic Obstacle Avoidance with Enhanced Social Force Model and DWA Algorithm Using SparkLink</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>992</elocation-id><pub-id pub-id-type="doi">10.3390/s25040992</pub-id><pub-id pub-id-type="pmid">40006221</pub-id><pub-id pub-id-type="pmcid">PMC11859799</pub-id></element-citation></ref><ref id="B9-sensors-25-05236"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>&#198;gidius</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chac&#243;n-Quesada</surname><given-names>R.</given-names></name><name name-style="western"><surname>Delfaki</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Kanoulas</surname><given-names>D.</given-names></name><name name-style="western"><surname>Demiris</surname><given-names>Y.</given-names></name></person-group><article-title>ASFM: Augmented Social Force Model for Legged Robot Social Navigation</article-title><source>Proceedings of the 2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)</source><conf-loc>Nancy, France</conf-loc><conf-date>22&#8211;24 November 2024</conf-date></element-citation></ref><ref id="B10-sensors-25-05236"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Du</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hetherington</surname><given-names>N.J.</given-names></name><name name-style="western"><surname>Oon</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>W.P.</given-names></name><name name-style="western"><surname>Quintero</surname><given-names>C.P.</given-names></name><name name-style="western"><surname>Croft</surname><given-names>E.</given-names></name><name name-style="western"><surname>Van der Loos</surname><given-names>H.M.</given-names></name></person-group><article-title>Group surfing: A pedestrian-based approach to sidewalk robot navigation</article-title><source>Proceedings of the 2019 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#8211;24 May 2019</conf-date></element-citation></ref><ref id="B11-sensors-25-05236"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hart</surname><given-names>P.E.</given-names></name><name name-style="western"><surname>Nilsson</surname><given-names>N.J.</given-names></name><name name-style="western"><surname>Raphael</surname><given-names>B.</given-names></name></person-group><article-title>A formal basis for the heuristic determination of minimum cost paths</article-title><source>IEEE Trans. Syst. Sci. Cybern.</source><year>1968</year><volume>4</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1109/TSSC.1968.300136</pub-id></element-citation></ref><ref id="B12-sensors-25-05236"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dijkstra</surname><given-names>E.W.</given-names></name></person-group><article-title>A note on two problems in connexion with graphs</article-title><source>Edsger Wybe Dijkstra: His Life, Work, and Legacy</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2022</year><fpage>287</fpage><lpage>290</lpage></element-citation></ref><ref id="B13-sensors-25-05236"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fox</surname><given-names>D.</given-names></name><name name-style="western"><surname>Burgard</surname><given-names>W.</given-names></name><name name-style="western"><surname>Thrun</surname><given-names>S.</given-names></name></person-group><article-title>The dynamic window approach to collision avoidance</article-title><source>IEEE Robot. Autom. Mag.</source><year>1997</year><volume>4</volume><fpage>23</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1109/100.580977</pub-id></element-citation></ref><ref id="B14-sensors-25-05236"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>R&#246;smann</surname><given-names>C.</given-names></name><name name-style="western"><surname>Feiten</surname><given-names>W.</given-names></name><name name-style="western"><surname>W&#246;sch</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hoffmann</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bertram</surname><given-names>T.</given-names></name></person-group><article-title>Trajectory modification considering dynamic constraints of autonomous robots</article-title><source>Proceedings of the ROBOTIK 2012: 7th German Conference on Robotics</source><conf-loc>Munich, Germany</conf-loc><conf-date>21&#8211;22 May 2012</conf-date></element-citation></ref><ref id="B15-sensors-25-05236"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shigemoto</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tasaki</surname><given-names>R.</given-names></name></person-group><article-title>Motion Planning of a Human-Aware Mobile Robot Merging into Dynamic Pedestrian Flow</article-title><source>Proceedings of the 9th International Conference on Control and Robotics Engineering (ICCRE)</source><conf-loc>Osaka, Japan</conf-loc><conf-date>10&#8211;12 May 2024</conf-date></element-citation></ref><ref id="B16-sensors-25-05236"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.M.</given-names></name><name name-style="western"><surname>Nazeri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Payandeh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Datar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>X.</given-names></name></person-group><article-title>Toward human-like social robot navigation: A large-scale, multi-modal, social human navigation dataset</article-title><source>Proceedings of the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Detroit, MI, USA</conf-loc><conf-date>1&#8211;5 October 2023</conf-date></element-citation></ref><ref id="B17-sensors-25-05236"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Francis</surname><given-names>A.</given-names></name><name name-style="western"><surname>P&#233;rez-d&#8217;Arpino</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>F.</given-names></name><name name-style="western"><surname>Alahi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Alami</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bera</surname><given-names>A.</given-names></name><name name-style="western"><surname>Biswas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Biswas</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chandra</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Principles and guidelines for evaluating social robot navigation algorithms</article-title><source>ACM Trans. Hum. Robot Interact.</source><year>2025</year><volume>14</volume><fpage>1</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1145/3700599</pub-id></element-citation></ref><ref id="B18-sensors-25-05236"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Raj</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Karnan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chandra</surname><given-names>R.</given-names></name><name name-style="western"><surname>Payandeh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Stone</surname><given-names>P.</given-names></name><name name-style="western"><surname>Biswas</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>X.</given-names></name></person-group><article-title>Rethinking social robot navigation: Leveraging the best of two worlds</article-title><source>Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>13&#8211;17 May 2024</conf-date></element-citation></ref><ref id="B19-sensors-25-05236"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eirale</surname><given-names>A.</given-names></name><name name-style="western"><surname>Martini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chiaberge</surname><given-names>M.</given-names></name></person-group><article-title>Human following and guidance by autonomous mobile robots: A comprehensive review</article-title><source>IEEE Access</source><year>2025</year><volume>13</volume><fpage>42214</fpage><lpage>42253</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2025.3548134</pub-id></element-citation></ref><ref id="B20-sensors-25-05236"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>K.-T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>P.-C.</given-names></name></person-group><article-title>Collision-avoidance control for human side-following navigation of an autonomous mobile robot</article-title><source>Proceedings of the 2024 International Conference on System Science and Engineering (ICSSE)</source><conf-loc>Hsinchu, Taiwan</conf-loc><conf-date>26&#8211;28 June 2024</conf-date></element-citation></ref><ref id="B21-sensors-25-05236"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Selvan</surname><given-names>C.P.</given-names></name><name name-style="western"><surname>Ramanujam</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Jasim</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>M.J.M.</given-names></name></person-group><article-title>Enhancing robotic navigation in dynamic environments</article-title><source>Int. J. Comput. Math. Control. Syst.</source><year>2024</year><volume>8</volume><fpage>2</fpage><pub-id pub-id-type="doi">10.69942/313319/20240101/03</pub-id></element-citation></ref><ref id="B22-sensors-25-05236"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tiwari</surname><given-names>R.</given-names></name><name name-style="western"><surname>Srinivaas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Velamati</surname><given-names>R.K.</given-names></name></person-group><article-title>Adaptive navigation in collaborative robots: A reinforcement learning and sensor fusion approach</article-title><source>Appl. Syst. Innov.</source><year>2025</year><volume>8</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.3390/asi8010009</pub-id></element-citation></ref><ref id="B23-sensors-25-05236"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Sousa Bezerra</surname><given-names>C.D.</given-names></name><name name-style="western"><surname>Teles Vieira</surname><given-names>F.H.</given-names></name><name name-style="western"><surname>Queiroz Carneiro</surname><given-names>D.P.</given-names></name></person-group><article-title>Autonomous robotic navigation using DQN late fusion and people detection-based collision avoidance</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>12350</elocation-id><pub-id pub-id-type="doi">10.3390/app132212350</pub-id></element-citation></ref><ref id="B24-sensors-25-05236"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Samsani</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Muhammad</surname><given-names>M.S.</given-names></name></person-group><article-title>Socially compliant robot navigation in crowded environment by human behavior resemblance using deep reinforcement learning</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>5223</fpage><lpage>5230</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3071954</pub-id></element-citation></ref><ref id="B25-sensors-25-05236"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mok</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>H.</given-names></name></person-group><article-title>Reward function design of model-free reinforcement learning for path planning of mobile robots in crowded environment</article-title><source>Proceedings of the 37th ICROS Annual Conference 2022</source><conf-loc>Gyeongju, Korea</conf-loc><conf-date>22&#8211;24 June 2022</conf-date></element-citation></ref><ref id="B26-sensors-25-05236"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name></person-group><article-title>Risk-aware deep reinforcement learning for robot crowd navigation</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>4744</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12234744</pub-id></element-citation></ref><ref id="B27-sensors-25-05236"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name></person-group><article-title>RMRL: Robot navigation in crowd environments with risk map-based deep reinforcement learning</article-title><source>IEEE Robot. Autom. Lett.</source><year>2023</year><volume>8</volume><fpage>7930</fpage><lpage>7937</lpage><pub-id pub-id-type="doi">10.1109/LRA.2023.3322093</pub-id></element-citation></ref><ref id="B28-sensors-25-05236"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>H.</given-names></name><name name-style="western"><surname>Oh</surname><given-names>M.</given-names></name><name name-style="western"><surname>Myung</surname><given-names>H.</given-names></name></person-group><article-title>Patchwork: Concentric zone-based region-wise ground segmentation with ground likelihood estimation using a 3D LiDAR sensor</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>4217</fpage><lpage>4224</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3093009</pub-id></element-citation></ref><ref id="B29-sensors-25-05236"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schulman</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wolski</surname><given-names>F.</given-names></name><name name-style="western"><surname>Dhariwal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name><name name-style="western"><surname>Klimov</surname><given-names>O.</given-names></name></person-group><article-title>Proximal policy optimization algorithms</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1707.06347</pub-id><pub-id pub-id-type="arxiv">1707.06347</pub-id></element-citation></ref><ref id="B30-sensors-25-05236"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Knoblauch</surname><given-names>R.L.</given-names></name><name name-style="western"><surname>Pietrucha</surname><given-names>M.T.</given-names></name><name name-style="western"><surname>Nitzburg</surname><given-names>M.</given-names></name></person-group><article-title>Field studies of pedestrian walking speed and start-up time</article-title><source>Transp. Res. Rec.</source><year>1996</year><volume>1538</volume><fpage>27</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1177/0361198196153800104</pub-id></element-citation></ref><ref id="B31-sensors-25-05236"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rastogi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chandra</surname><given-names>S.</given-names></name><name name-style="western"><surname>Vamsheedhar</surname><given-names>J.</given-names></name><name name-style="western"><surname>Das</surname><given-names>V.R.</given-names></name></person-group><article-title>Parametric study of pedestrian speeds at midblock crossings</article-title><source>J. Urban Plann. Dev.</source><year>2011</year><volume>137</volume><fpage>381</fpage><lpage>389</lpage><pub-id pub-id-type="doi">10.1061/(ASCE)UP.1943-5444.0000083</pub-id></element-citation></ref><ref id="B32-sensors-25-05236"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gerkey</surname><given-names>B.P.</given-names></name><name name-style="western"><surname>Konolige</surname><given-names>K.</given-names></name></person-group><article-title>Planning and control in unstructured terrain</article-title><source>Proceedings of the ICRA Workshop on Path Planning on Costmaps</source><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2008</year></element-citation></ref><ref id="B33-sensors-25-05236"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Van Den Berg</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guy</surname><given-names>S.J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Manocha</surname><given-names>D.</given-names></name></person-group><article-title>Reciprocal N-Body Collision Avoidance</article-title><source>Robotics Research</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2011</year><fpage>3</fpage><lpage>19</lpage></element-citation></ref><ref id="B34-sensors-25-05236"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.F.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Everett</surname><given-names>M.</given-names></name><name name-style="western"><surname>How</surname><given-names>J.P.</given-names></name></person-group><article-title>Decentralized Non-Communicating Multiagent Collision Avoidance with Deep Reinforcement Learning</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Singapore</conf-loc><conf-date>29 May&#8211;3 June 2017</conf-date><fpage>285</fpage><lpage>292</lpage></element-citation></ref><ref id="B35-sensors-25-05236"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Everett</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.F.</given-names></name><name name-style="western"><surname>How</surname><given-names>J.P.</given-names></name></person-group><article-title>Motion Planning among Dynamic, Decision-Making Agents with Deep Reinforcement Learning</article-title><source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>1&#8211;5 October 2018</conf-date><fpage>3052</fpage><lpage>3059</lpage></element-citation></ref><ref id="B36-sensors-25-05236"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chakraborty</surname><given-names>N.</given-names></name><name name-style="western"><surname>Driggs-Campbell</surname><given-names>K.</given-names></name></person-group><article-title>Decentralized Structural-RNN for robot crowd navigation with deep reinforcement learning</article-title><source>Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>30 May&#8211;5 June 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2021</year><fpage>3517</fpage><lpage>3521</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05236-f001" orientation="portrait"><label>Figure 1</label><caption><p>Architecture of the sensor fusion and deep reinforcement learning (DRL)-based path planning. The mobile robot used was a Clearpath Jackal, equipped with a Velodyne VLP-16 LiDAR and an Intel RealSense D435 camera, running on Ubuntu 20.04 with ROS Noetic.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g001.jpg"/></fig><fig position="float" id="sensors-25-05236-f002" orientation="portrait"><label>Figure 2</label><caption><p>Real-world execution pipeline of the proposed method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g002.jpg"/></fig><fig position="float" id="sensors-25-05236-f003" orientation="portrait"><label>Figure 3</label><caption><p>Sensor fusion pipeline for human-aware navigation. (<bold>a</bold>) 3D LiDAR preprocessing with ground removal and extracting region of interest (ROI) point cloud; (<bold>b</bold>) human detection using RGB image and bounding box fusion with LiDAR; and (<bold>c</bold>) individual-level human clustering with tracking.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g003.jpg"/></fig><fig position="float" id="sensors-25-05236-f004" orientation="portrait"><label>Figure 4</label><caption><p>Procedure for removing non-pedestrian point clouds.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g004.jpg"/></fig><fig position="float" id="sensors-25-05236-f005" orientation="portrait"><label>Figure 5</label><caption><p>Human-aware occupancy map (HAOM) comprising static obstacles, centroid positions of pedestrian clusters, dynamic danger zones, and open space.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g005.jpg"/></fig><fig position="float" id="sensors-25-05236-f006" orientation="portrait"><label>Figure 6</label><caption><p>Sector-based distance feature extraction for HAOM. The robot&#8217;s front-facing area is divided into <italic toggle="yes">n</italic> uniform angular sectors <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. For each sector, the minimum Euclidean distance <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to static obstacles or the human clusters&#8217; centroid is measured, normalized by the maximum sensing range <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and encoded as a distance feature <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g006.jpg"/></fig><fig position="float" id="sensors-25-05236-f007" orientation="portrait"><label>Figure 7</label><caption><p>Sector-based velocity feature extraction in the camera&#8217;s field of view (FOV). The area is evenly divided into angular sectors <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and tracked pedestrian positions <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and velocity vectors <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are projected into the corresponding sectors.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g007.jpg"/></fig><fig position="float" id="sensors-25-05236-f008" orientation="portrait"><label>Figure 8</label><caption><p>Sector-based open-space feature extraction for HAOM in the camera&#8217;s FOV. Each angular sector <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is analyzed to compute the normalized traversable distance <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> considering both static and dynamic obstacles with danger zones.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g008.jpg"/></fig><fig position="float" id="sensors-25-05236-f009" orientation="portrait"><label>Figure 9</label><caption><p>Three simulation environments used for performance evaluation. The black represents the wall structures, purple indicates dynamic obstacles, cyan denotes the goal, and gray represents the robot.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g009.jpg"/></fig><fig position="float" id="sensors-25-05236-f010" orientation="portrait"><label>Figure 10</label><caption><p>Visualizations of our method in extreme scenarios: (<bold>a</bold>) success cases; (<bold>b</bold>) failure cases. The red line represents the robot&#8217;s path.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g010.jpg"/></fig><fig position="float" id="sensors-25-05236-f011" orientation="portrait"><label>Figure 11</label><caption><p>Comparison of navigation trajectories in Scenario 1: (<bold>a</bold>) dynamic window approach (DWA), (<bold>b</bold>) timed elastic band (TEB), (<bold>c</bold>) trajectory rollout (TR), (<bold>d</bold>) safe and open-space-aware RL (SAOR-RL; ours). The red line represents the robot&#8217;s path.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g011.jpg"/></fig><fig position="float" id="sensors-25-05236-f012" orientation="portrait"><label>Figure 12</label><caption><p>Comparison of navigation trajectories in Scenario 2: (<bold>a</bold>) dynamic window approach (DWA), (<bold>b</bold>) timed elastic band (TEB), (<bold>c</bold>) trajectory rollout (TR), (<bold>d</bold>) safe and open-space-aware RL (SAOR-RL; ours). The red line represents the robot&#8217;s path.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g012.jpg"/></fig><fig position="float" id="sensors-25-05236-f013" orientation="portrait"><label>Figure 13</label><caption><p>Comparison of navigation trajectories in Scenario 3: (<bold>a</bold>) dynamic window approach (DWA), (<bold>b</bold>) timed elastic band (TEB), (<bold>c</bold>) trajectory rollout (TR), (<bold>d</bold>) safe and open-space-aware RL (SAOR-RL; ours). The red line represents the robot&#8217;s path.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05236-g013.jpg"/></fig><table-wrap position="float" id="sensors-25-05236-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05236-t001_Table 1</object-id><label>Table 1</label><caption><p>Description of reward functions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Reward</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Explanation</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Arrive: If the robot goal distance <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is less than <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, gets <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Heading to goal: Measures the cosine similarity <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between the robot&#8217;s current direction <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the direction to the goal <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. If <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, gets + 1.0.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Progress: Calculates the distance reduction to the goal between time steps.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Danger zone penalty: Applies a linearly increasing penalty when the robot enters a human&#8217;s danger zone [<xref rid="B24-sensors-25-05236" ref-type="bibr">24</xref>]. <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the robot&#8211;obstacles distance; <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the danger zone radius.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Distance penalty: Penalizes proximity to static obstacles and human danger zones to reduce collision risk. <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the open space length in each sector <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>.</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is constant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Open-Dir alignment: Computes the cosine similarity <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between the robot&#8217;s current direction <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the central direction vector of the widest open space <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Collision penalty: Applies upon collision with obstacles.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05236-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05236-t002_Table 2</object-id><label>Table 2</label><caption><p>Evaluation metrics used for performance assessment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metrics</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Explanation</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Success rate: Percentage of episodes in which the robot successfully reaches the goal without collisions or timeouts.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Collision rate: Percentage of episodes in which the robot collides with any obstacle.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TOR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Timeout rate: Percentage of episodes in which the robot fails to reach the goal within the time limit.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#956;R &#177; &#963;R</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cumulative reward: Mean and standard deviation of cumulative rewards across episodes, indicating overall policy performance.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#956;PL</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Path length: Average distance (in meters) the robot travels to reach the goal.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#956;S</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average speed: Average velocity of the robot during successful navigation.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NT</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Navigation time: Time (in seconds) taken per episode, regardless of whether the robot succeeds, collides, or times out.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05236-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05236-t003_Table 3</object-id><label>Table 3</label><caption><p>Performance of the proposed method in various environments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Scenario 1</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Scenario 2</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Scenario 3</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"># of Dynamic Obstacles</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SR (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#956;R &#177; &#963;R</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SR (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#956;R &#177; &#963;R</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SR (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#956;R &#177; &#963;R</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">157.07 &#177; 23.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">149.39 &#177; 30.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">225.24 &#177; 39.49</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">145.82 &#177; 17.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">163.76 &#177; 33.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">215.11 &#177; 40.43</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">147.38 &#177; 32.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">164.84 &#177; 38.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">193.51 &#177; 32.33</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">144.48 &#177; 40.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">166.86 &#177; 34.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">209.77 &#177; 32.69</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05236-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05236-t004_Table 4</object-id><label>Table 4</label><caption><p>Quantitative comparison with and without Open-Dir alignment reward <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in three scenarios with four dynamic obstacles. Upward &#8593; and downward &#8595; arrows indicate values that are higher and lower than the value w/o <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SR (%) &#8593;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CR (%) &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TOR (%) &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">&#956;R &#177; &#963;R &#8593;</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Scenario 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.51 &#177; 11.26</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/<inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">144.48 &#177; 40.17</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Scenario 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.96 &#177; 25.99</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/<inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">166.86 &#177; 34.36</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Scenario 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">138.40 &#177; 27.41</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/<inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">209.77 &#177; 32.69</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05236-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05236-t005_Table 5</object-id><label>Table 5</label><caption><p>Quantitative performance comparison of the proposed method and traditional navigation methods with four dynamic obstacles. Upward &#8593; and downward &#8595; arrows indicate values that are higher and lower than the result of traditional navigation methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario </th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SR (%) &#8593;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CR (%) &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TOR (%) &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">&#956;PL (m) &#8593;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">&#956;S (m/s) &#8593;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NT (s)</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Scenario 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWA [<xref rid="B13-sensors-25-05236" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.26</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TEB [<xref rid="B14-sensors-25-05236" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TR [<xref rid="B32-sensors-25-05236" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.73</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SOAR-RL (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.39</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Scenario 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWA [<xref rid="B13-sensors-25-05236" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.64</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TEB [<xref rid="B14-sensors-25-05236" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TR [<xref rid="B32-sensors-25-05236" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.78</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SOAR-RL (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.27</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Scenario 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DWA [<xref rid="B13-sensors-25-05236" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.24</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TEB [<xref rid="B14-sensors-25-05236" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.87</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TR [<xref rid="B32-sensors-25-05236" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.57</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SOAR-RL (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.03</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05236-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05236-t006_Table 6</object-id><label>Table 6</label><caption><p>Quantitative performance comparison of the proposed method and crowd navigation methods with five dynamic obstacles. Upward &#8593; and downward &#8595; arrows indicate values that are higher and lower than the result of AI-based crowd navigation methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SR (%) &#8593;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CR (%) &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TOR (%) &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">&#956;PL (m) &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">&#956;S (m/s)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NT (s) &#8595;</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ORCA [<xref rid="B33-sensors-25-05236" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.41</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CADRL [<xref rid="B34-sensors-25-05236" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.04</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM-RL [<xref rid="B35-sensors-25-05236" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.10</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DSRNN [<xref rid="B36-sensors-25-05236" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.78</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SOAR-RL (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.28</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05236-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05236-t007_Table 7</object-id><label>Table 7</label><caption><p>Runtime performance summary across different navigation scenarios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenarios</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Avg. FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scenario 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.03</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scenario 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.53</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scenario 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.24</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scenario 4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.19</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>