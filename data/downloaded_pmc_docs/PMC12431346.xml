<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431346</article-id><article-id pub-id-type="pmcid-ver">PMC12431346.1</article-id><article-id pub-id-type="pmcaid">12431346</article-id><article-id pub-id-type="pmcaiid">12431346</article-id><article-id pub-id-type="doi">10.3390/s25175366</article-id><article-id pub-id-type="publisher-id">sensors-25-05366</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>RGB-Based Visual&#8211;Inertial Odometry via Knowledge Distillation from Self-Supervised Depth Estimation with Foundation Models</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-6483-5263</contrib-id><name name-style="western"><surname>Song</surname><given-names initials="J">Jimin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9312-6299</contrib-id><name name-style="western"><surname>Lee</surname><given-names initials="SJ">Sang Jun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-05366" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Gao</surname><given-names initials="Y">Yabin</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05366">Division of Electronic Engineering, Jeonbuk National University, 567 Baekje-daero, Deokjin-gu, Jeonju 54896, Republic of Korea; <email>jimin_song@jbnu.ac.kr</email></aff><author-notes><corresp id="c1-sensors-25-05366"><label>*</label>Correspondence: <email>sj.lee@jbnu.ac.kr</email>; Tel.: +82-63-270-2463</corresp></author-notes><pub-date pub-type="epub"><day>30</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5366</elocation-id><history><date date-type="received"><day>18</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>28</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>30</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05366.pdf"/><abstract><p>Autonomous driving represents a transformative advancement with the potential to significantly impact daily mobility, including enabling independent vehicle operation for individuals with visual disabilities. The commercialization of autonomous driving requires guaranteed safety and accuracy, underscoring the need for robust localization and environmental perception algorithms. In cost-sensitive platforms such as delivery robots and electric vehicles, cameras are increasingly favored for their ability to provide rich visual information at low cost. Despite recent progress, existing visual&#8211;inertial odometry systems still suffer from degraded accuracy in challenging conditions, which limits their reliability in real-world autonomous navigation scenarios. Estimating 3D positional changes using only 2D image sequences remains a fundamental challenge primarily due to inherent scale ambiguity and the presence of dynamic scene elements. In this paper, we present a visual&#8211;inertial odometry framework incorporating a depth estimation model trained without ground-truth depth supervision. Our approach leverages a self-supervised learning pipeline enhanced with knowledge distillation via foundation models, including both self-distillation and geometry-aware distillation. The proposed method improves depth estimation performance and consequently enhances odometry estimation without modifying the network architecture or increasing the number of parameters. The effectiveness of the proposed method is demonstrated through comparative evaluations on both the public KITTI dataset and a custom campus driving dataset, showing performance improvements over existing approaches.</p></abstract><kwd-group><kwd>simultaneous localization and mapping</kwd><kwd>visual&#8211;inertial odometry</kwd><kwd>deep learning</kwd><kwd>foundation model</kwd><kwd>self-supervised depth estimation</kwd><kwd>knowledge distillation</kwd></kwd-group><funding-group><award-group><funding-source>Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP)</funding-source><award-id>IITP-2025-RS-2024-00439292</award-id></award-group><award-group><funding-source>Korea government (MSIT)</funding-source><award-id>RS-2024-00346415</award-id></award-group><funding-statement>This work was partly supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP)-Innovative Human Resource Development for Local Intellectualization program grant funded by the Korea government (MSIT) (IITP-2025-RS-2024-00439292) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00346415).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05366"><title>1. Introduction</title><p>Recent advances in deep learning, driven by improvements in both hardware and software, have enabled its widespread application across various industries. In the field of computer vision, significant progress has been made not only in well-established areas such as object detection [<xref rid="B1-sensors-25-05366" ref-type="bibr">1</xref>] and semantic segmentation [<xref rid="B2-sensors-25-05366" ref-type="bibr">2</xref>], but also in depth estimation [<xref rid="B3-sensors-25-05366" ref-type="bibr">3</xref>]. Depth estimation is the task of inferring the real-world distance from the camera to the scene surface at each pixel. This technique has become key for applications in robotics, including the autonomous navigation of mobile robots and drones [<xref rid="B4-sensors-25-05366" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05366" ref-type="bibr">5</xref>], as well as for providing auxiliary information in minimally invasive surgery [<xref rid="B6-sensors-25-05366" ref-type="bibr">6</xref>]. Due to its broad applicability across various domains, depth estimation continues to receive increasing attention. Although depth estimation has seen significant progress, it still faces fundamental challenges. Monocular methods are limited by scale ambiguity, making absolute depth recovery ill-posed without additional cues. In contrast, stereo approaches must trade off between long-range accuracy and occlusion handling, which depends on the baseline length. In this paper, we propose a novel monocular depth estimation method based on a foundation model, which is aimed at improving the performance of simultaneous localization and mapping (SLAM).</p><p>As a data-driven approach, the performance of deep neural networks is largely determined by the quality and quantity of the training data. Constructing high-quality datasets typically requires ensuring domain diversity, filtering out noisy or erroneous samples, and generating accurate ground-truth annotations. In recent years, increasing the awareness of ethical and legal concerns&#8212;such as privacy and copyright issues&#8212;has made data collection more cautious. To address these challenges, recent research has increasingly focused on the use of foundation models. Foundation models refer to large-scale models pretrained using extensive computational resources and massive datasets. In this work, we propose a method that fine-tunes a depth foundation model pretrained on a large-scale dataset comprising synthetic data generated from carefully designed virtual environments, using a target dataset.</p><p>A straightforward and widely used approach for training depth estimation models is the supervised learning pipeline. This method minimizes a loss function defined based on the error between the estimated depth map and the ground-truth depth map. It typically yields high performance, as the model is trained directly using explicit supervisory signals. However, generating accurate ground-truth depth data requires expensive light detection and ranging (LiDAR) sensor and precise calibration between sensors. Moreover, such ground-truth data are often domain-specific, leading to overfitting to the data acquisition environment and resulting in limited generalization performance. As an alternative, self-supervised learning approaches based on image reprojection have been proposed to mitigate these limitations. These methods estimate a depth map from an input image and leverage adjacent frames to reproject their content into the viewpoint of the input image, using the photometric error between the synthesized and original views as the supervision signal. Due to their complex and indirect supervision pipelines, self-supervised methods typically exhibit lower accuracy than supervised approaches. To address this limitation, we introduce a self-supervised framework that improves depth estimation by incorporating auxiliary depth cues via knowledge distillation, as shown in <xref rid="sensors-25-05366-f001" ref-type="fig">Figure 1</xref>, while preserving image reprojection-based supervision. Knowledge distillation refers to the transfer of learned representations from a teacher model to a student model.</p><p>SLAM is a technique that enables a robot to simultaneously construct a map of its environment and estimate its position within it in real time. The performance of SLAM algorithms is primarily influenced by the sensor modality. Recent advancements in SLAM research have given rise to two primary system categories: LiDAR-based approaches and camera-based approaches. LiDAR-based methods are known for their high accuracy, which is primarily due to their capability to directly capture detailed and reliable 3D points. However, they face several challenges, including high cost and reduced reliability in environments such as highways, which are characterized by repetitive or low-texture geometric features. Additionally, they require extra processing to mitigate issues caused by light reflection and material transparency. Camera-based SLAM systems, on the other hand, are generally more robust in such scenarios and offer a more cost-effective alternative. Nevertheless, due to the absence of direct 3D measurements, camera-based SLAM systems generally underperform compared to LiDAR-based approaches. In this work, we demonstrate that the proposed camera-based SLAM system with integrated depth estimation outperforms existing methods on a real-world outdoor dataset.</p><p>To summarize, the key contributions of this work are as follows: (1) We propose a knowledge distillation framework to enhance the capability of fine-detail recovery in self-supervised monocular depth estimation, addressing a common shortcoming observed in previous approaches. Our approach incorporates not only recently proposed self-distillation techniques but also introduces a novel geometry-aware distillation method leveraging a foundation depth model. (2) To ensure the practical deployment of the trained student depth model in visual&#8211;inertial odometry systems, we adopt a ViT-Small-based architecture. Compared to ViT-Large, our model significantly reduces the number of parameters with only marginal accuracy loss. Furthermore, it outperforms simple fine-tuning baselines in reconstructing fine details and estimating reasonable depth for transparent objects. (3) We extensively evaluate our approach on both public datasets and real-world outdoor driving sequences collected in house, demonstrating its effectiveness in both depth estimation and odometry performance compared to existing methods.</p></sec><sec id="sec2-sensors-25-05366"><title>2. Related Work</title><p>Monodepth2 [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>] serves as a commonly used baseline in recent self-supervised depth estimation research. Godard et al. introduced a method to ensure that only pixels satisfying the photometric consistency assumption in reprojection-based training are used for supervision. To address occlusion issues, they introduced a loss function that warps multiple source images into a single target image and computes the reprojection error, selecting only the minimum error across the sources. In addition, they proposed an automatic masking strategy to exclude pixels that violate the parallax assumption, such as those belonging to static background regions. In some cases, this strategy may also filter out moving objects whose motion is consistent with the camera, as these can otherwise degrade the quality of the supervision signal. These contributions enhanced the reliability of the underlying assumptions in the training pipeline and led to significant performance improvements. In contrast to previous methods that primarily suppress unreliable training signals, we introduce a knowledge distillation framework designed to provide more direct and semantically enriched supervision for self-supervised depth estimation.</p><p>Knowledge distillation has been extensively studied in deep learning across various tasks, including depth estimation. In the field of depth estimation, knowledge distillation has been applied both to reduce model complexity [<xref rid="B11-sensors-25-05366" ref-type="bibr">11</xref>] and to transfer informative geometric cues [<xref rid="B12-sensors-25-05366" ref-type="bibr">12</xref>] that contribute to improved depth prediction performance. In recent study, Poggi et al. [<xref rid="B13-sensors-25-05366" ref-type="bibr">13</xref>] proposed a self-distillation framework where the student shares the same structure as the teacher and can even outperform it. Their method incorporates pixel-level uncertainty into the loss function via a negative log-likelihood formulation, allowing the student model to account for uncertainty during training. Song et al. [<xref rid="B14-sensors-25-05366" ref-type="bibr">14</xref>] designed a modified model architecture and loss function tailored for the effective knowledge distillation of a foundation depth model. Their method demonstrated state-of-the-art performance on the KITTI online benchmark, providing empirical evidence of the strong generalization capability of foundation models when used as sources of transferable knowledge. Foundation models often require networks with a large number of parameters to fully leverage their representational capacity. However, practical applications such as SLAM benefit from models with significantly fewer parameters due to computational and resource constraints. To bridge this gap, we propose a knowledge distillation framework in which a lightweight student model is guided by both segmentation and depth foundation models through geometry-aware supervision.</p><p>Foundation models were initially developed for natural language processing tasks and have subsequently exhibited substantial impact in computer vision. Oquab et al. introduced DINOv2 [<xref rid="B15-sensors-25-05366" ref-type="bibr">15</xref>], which is a robust vision transformer capable of extracting generalizable visual features from unseen images, enabling its use across various downstream tasks such as classification, segmentation, and depth estimation. Liu et al. proposed GroundingDINO [<xref rid="B7-sensors-25-05366" ref-type="bibr">7</xref>], a vision-language model designed for object detection tasks involving unseen classes, which leverages diverse forms of text prompts to achieve strong performance. Kirillov et al. introduced the Segment Anything Model (SAM) [<xref rid="B8-sensors-25-05366" ref-type="bibr">8</xref>], which is a prompt-driven segmentation framework capable of processing various input types&#8212;such as points, bounding boxes, masks, and text&#8212;alongside image data. Following recent work [<xref rid="B16-sensors-25-05366" ref-type="bibr">16</xref>], we employ GroundingDINO and SAM to explicitly identify sky regions, which are unsuitable for quantitative evaluation and may adversely affect depth model training. Ranftl et al. proposed the Dense Prediction Transformer (DPT) [<xref rid="B17-sensors-25-05366" ref-type="bibr">17</xref>], which is trained on a large meta-dataset constructed by aggregating multiple existing depth datasets. Yang et al. introduced Depth Anything v2 [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>], a model composed of a DINOv2-based encoder and a DPT-based decoder, which is trained using both labeled synthetic images and unlabeled real-world images to enhance generalization. The foundation depth model can reliably estimate relative depth even for unseen images; however, fine tuning is required to achieve accurate absolute depth estimation. To address this limitation, we do not directly incorporate the absolute outputs of the foundation model into the knowledge distillation process. Instead, a surface normal map is derived from relative depth prediction and incorporated as auxiliary geometric supervision in the training framework.</p><p>SLAM has increasingly integrated Inertial Measurement Unit (IMU) sensors, which provide measurements of linear acceleration and angular velocity. While IMUs provide valuable measurements, they are prone to cumulative drift due to inherent sensor noise characteristics. Therefore, integrating complementary sensor modalities is often necessary to improve the robustness of state estimation. Representative examples include LiDAR&#8211;Inertial Odometry (LIO), which integrates LiDAR and IMU data, and Visual&#8211;Inertial Odometry (VIO), which combines camera and IMU measurements. Bai et al. proposed a Faster-LIO [<xref rid="B18-sensors-25-05366" ref-type="bibr">18</xref>], introducing efficient data structures for handling point cloud representations. Their method offers computational efficiency while maintaining reliable and accurate state estimation. In this study, it is utilized to generate ground-truth odometry for our custom dataset. This enables a quantitative evaluation of odometry accuracy in VIO. Qin et al. introduced VINS-Mono [<xref rid="B19-sensors-25-05366" ref-type="bibr">19</xref>], which is a widely adopted baseline VIO framework that fuses monocular images with IMU data. Building upon the original VINS-Mono framework, Shan et al. introduced VINS-RGBD [<xref rid="B20-sensors-25-05366" ref-type="bibr">20</xref>], which incorporates depth data from an RGB-D camera. Leveraging prior frameworks, we propose an RGB-based VIO pipeline guided by foundation models, which was designed to enhance the robustness of vision-based autonomous navigation systems.</p></sec><sec id="sec3-sensors-25-05366"><title>3. Method</title><p>A detailed explanation of each component in the proposed VIO pipeline is presented in this section, as shown in <xref rid="sensors-25-05366-f002" ref-type="fig">Figure 2</xref>. <xref rid="sec3dot1-sensors-25-05366" ref-type="sec">Section 3.1</xref> and <xref rid="sec3dot2-sensors-25-05366" ref-type="sec">Section 3.2</xref> describe the image reprojection-based supervision strategy and the self-distillation scheme using a transformer-based foundation depth model. <xref rid="sec3dot3-sensors-25-05366" ref-type="sec">Section 3.3</xref> describes the process of generating sky segmentation masks from foundation models for use in geometry-aware distillation. <xref rid="sec3dot4-sensors-25-05366" ref-type="sec">Section 3.4</xref> presents a geometry-aware distillation strategy that transfers boundary and geometric cues from a pretrained foundation model to enhance student prediction accuracy. Finally, <xref rid="sec3dot5-sensors-25-05366" ref-type="sec">Section 3.5</xref> presents the integration of the student depth model into a VIO pipeline.</p><sec id="sec3dot1-sensors-25-05366"><title>3.1. Image Reprojection Based Training</title><p>Our proposed depth estimation training pipeline incorporates an image reprojection-based self-supervised learning strategy, following prior successful work in the field [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>]. Given a target image <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, a corresponding depth map <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is predicted by the student depth model. The relative camera transformation from the target view <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to a reference view <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msub></mml:mrow></mml:math></inline-formula> is denoted as <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8594;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to facilitate reprojection. In stereo settings, this transformation is derived from known extrinsic calibration parameters. In monocular settings, it is estimated by a ResNet [<xref rid="B21-sensors-25-05366" ref-type="bibr">21</xref>]-based pose network that takes the image pair <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as input. Given the camera intrinsic matrix <italic toggle="yes">K</italic>, the reprojected view <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is synthesized from the reference image <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msub></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD1-sensors-25-05366"><label>(1)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msub><mml:mrow><mml:mo>&#9001;</mml:mo><mml:mi>proj</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8594;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#9002;</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Here, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>proj</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the projection operation into the target frame, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#9001;</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>&#9002;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates bilinear sampling. A photometric loss <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is employed, combining the Structural Similarity Index Measure (SSIM) [<xref rid="B22-sensors-25-05366" ref-type="bibr">22</xref>] and the L1-norm with a weighting factor <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>: <disp-formula id="FD2-sensors-25-05366"><label>(2)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:munder><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>SSIM</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>To account for occlusions, the minimum photometric error is computed across multiple reference frames, ensuring supervision is derived from the most photometrically consistent view. Since this approach assumes a static scene with a moving camera, a binary mask <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> is introduced to restrict the loss computation to valid regions. It is computed as follows: <disp-formula id="FD3-sensors-25-05366"><label>(3)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:munder><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:munder><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Iverson bracket. And we adopted an auxiliary loss term known as the edge-aware smoothness loss <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which encourages depth smoothness in textureless regions while preserving object boundaries: <disp-formula id="FD4-sensors-25-05366"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mo>&#8706;</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mo>&#8706;</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mo>&#8706;</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>|</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mo>&#8706;</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Here, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#8706;</mml:mo><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#8706;</mml:mo><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the partial derivatives with respect to the <italic toggle="yes">x</italic>-axis and <italic toggle="yes">y</italic>-axis, respectively. The final reprojection loss is defined as a weighted combination of the masked photometric loss and the smoothness loss: <disp-formula id="FD5-sensors-25-05366"><label>(5)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#956;</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>In all experiments, the hyperparameter <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is empirically set to 0.001. The reprojection-based approach is supported by established theoretical principles and demonstrates reliable performance in coarse depth estimation. However, it tends to struggle with fine-grained details such as object boundaries and transparent surfaces. This is because masking strategies such as Equation (<xref rid="FD3-sensors-25-05366" ref-type="disp-formula">3</xref>) and smoothness losses like Equation (<xref rid="FD4-sensors-25-05366" ref-type="disp-formula">4</xref>) aim to exclude cases that violate the underlying assumptions of the reprojection principles, which can lead to a lack of learning cues in those regions. To address these limitations, two knowledge distillation techniques are additionally employed, as detailed in the following sections.</p></sec><sec id="sec3dot2-sensors-25-05366"><title>3.2. Self-Distillation of Dense Prediction Transformer</title><p>Recent approaches [<xref rid="B13-sensors-25-05366" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05366" ref-type="bibr">14</xref>] have explored self-distillation techniques in which a model fine-tuned on the target dataset is used to generate pseudo-ground truth depth map <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The pseudo-label is then used as a supervision signal to train the student model by minimizing a depth loss function. Although the generated pseudo-depth may be imperfect, the availability of dense supervision provides explicit guidance during training, often resulting in improved performance. Prior studies have shown that even when the teacher and student models share identical architectures, the student model can achieve superior performance through self-distillation. In contrast, our work aims to bridge the performance gap between a high-capacity teacher model and a lightweight student model with significantly fewer parameters. The depth models adopt the architecture of a depth foundation model based on DPT [<xref rid="B17-sensors-25-05366" ref-type="bibr">17</xref>], utilizing a ViT-based encoder [<xref rid="B23-sensors-25-05366" ref-type="bibr">23</xref>] to extract rich visual representations. The decoder consists of a reassemble block and a fusion block, which is followed by a prediction head that reconstructs the depth map in the image space. To better leverage dense depth cues, the student model is augmented with parameter-shared multi-prediction heads. Deep supervision is applied to all intermediate predictions to encourage the extraction of semantically meaningful features. For the self-distillation loss, we adopt the scale-invariant error, a widely used metric in supervised depth estimation [<xref rid="B3-sensors-25-05366" ref-type="bibr">3</xref>], which compares the predicted depth <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> with the pseudo-ground truth <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> in logarithmic space:<disp-formula id="FD6-sensors-25-05366"><label>(6)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#955;</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Here, <italic toggle="yes">n</italic> denotes the number of valid pixels, while <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the predicted and pseudo-depth values at pixel <italic toggle="yes">i</italic>.</p></sec><sec id="sec3dot3-sensors-25-05366"><title>3.3. Sky Mask Generation via Foundation Model</title><p>In depth estimation, the sky region is inherently unsuitable for accurate prediction due to its near-infinite depth, which fundamentally differs from other regions that exhibit observable geometric structure. Recent approaches, such as Depth Anything v2 [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>], address this issue by training models to predict values that are inversely proportional to depth, thereby encouraging near-zero outputs for sky pixels. However, when applying similar strategies to a relatively limited real-world custom dataset, we observed a substantial degradation in both quantitative accuracy and generalization performance. Therefore, a sky segmentation mask <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is generated for input image <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to explicitly exclude these regions from the distillation process. The mask <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is generated in two stages, beginning with the input of the RGB image <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the textual prompt &#8220;sky&#8221; into GroundingDINO [<xref rid="B7-sensors-25-05366" ref-type="bibr">7</xref>], which is a state-of-the-art open-vocabulary object detection model recognized for its strong performance in zero-shot scenarios. This model produces bounding boxes that roughly localize sky regions. These bounding boxes then serve as strong prompts for SAM [<xref rid="B8-sensors-25-05366" ref-type="bibr">8</xref>], which takes the RGB image <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and outputs a high-quality segmentation mask corresponding to the detected sky area.</p></sec><sec id="sec3dot4-sensors-25-05366"><title>3.4. Geometry-Aware Knowledge Distillation</title><p>Depth foundation models pretrained on large-scale datasets are capable of producing sharp and geometrically consistent relative depth maps <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, even for previously unseen images. Leveraging this capability, we propose two geometry-aware distillation losses designed to transfer the boundary-aware and surface-consistent knowledge from the pretrained model to the student model. We introduce a structure consistency loss <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which promotes edge preservation by evaluating local structural patterns instead of penalizing absolute depth differences. Inspired by the structural similarity term in the SSIM metric, the loss measures local geometric coherence between the foundation model prediction <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and the student output <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD7-sensors-25-05366"><label>(7)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:msubsup><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the local covariance between <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> at pixel <italic toggle="yes">i</italic>, while <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the local standard deviations of <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively. These statistics are computed over a <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> window centered at each pixel. The constant <italic toggle="yes">k</italic>, which is empirically set to 30 in accordance with the SSIM formulation, stabilizes the computation by preventing division by near-zero values.</p><p>To enhance the model&#8217;s understanding of object shapes through geometric context, we incorporate surface normal supervision, which provides richer structural information than depth values alone. To generate surface normal supervision, we employ Depth-to-Normal Transformer (D2NT) [<xref rid="B24-sensors-25-05366" ref-type="bibr">24</xref>], which is a recently proposed method that achieves both high accuracy and computational efficiency. As normal estimation directly impacts the training speed of our pipeline, fast and reliable normal computation is a critical requirement. Both <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, along with the corresponding sky mask <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, are passed through D2NT to produce surface normal maps <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively. We then define a geometric consistency loss <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> based on cosine similarity between the predicted and reference normal vectors:<disp-formula id="FD8-sensors-25-05366"><label>(8)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">m</italic> denotes the number of pixels outside the masked region defined by <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the normal vectors at pixel <italic toggle="yes">i</italic> in <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively.</p><p>By incorporating both boundary-sensitive and surface-aware supervision, the proposed method facilitates enhanced structural understanding within the student network, thereby promoting more stable and accurate depth estimation. Consequently, the total loss used for training the student depth model is formulated as a weighted combination of the following components:<disp-formula id="FD9-sensors-25-05366"><label>(9)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>In all experiments, we set the loss weights to <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> based on empirical tuning. This choice is intended to balance the contributions of each component during training while avoiding overfitting to the pretrained teacher models.</p></sec><sec id="sec3dot5-sensors-25-05366"><title>3.5. VIO with Depth Estimation</title><p>We construct a VIO pipeline based on VINS-RGBD [<xref rid="B20-sensors-25-05366" ref-type="bibr">20</xref>], integrating RGB images, the predicted depth maps from the student model, and inertial measurements from an IMU sensor. Given the distinct characteristics of visual and inertial modalities, we apply modality-specific preprocessing steps. The IMU operates at a significantly higher sampling rate than the camera and is subject to considerable sensor noise; thus, we employ pre-integration techniques [<xref rid="B19-sensors-25-05366" ref-type="bibr">19</xref>] to fuse the high-rate inertial data effectively. In the visual processing pipeline, feature points are identified in each RGB frame using the Shi&#8211;Tomasi corner detection algorithm [<xref rid="B25-sensors-25-05366" ref-type="bibr">25</xref>], and their inter-frame correspondences are established via the Kanade&#8211;Lucas&#8211;Tomasi (KLT) sparse optical flow method [<xref rid="B26-sensors-25-05366" ref-type="bibr">26</xref>]. In contrast to conventional RGB-based VIO systems that estimate depth through the perspective-n-point (PnP) algorithm, the proposed method directly incorporates depth values aligned with tracked features, as obtained from the predicted depth map <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. During system initialization, visual&#8211;inertial initialization [<xref rid="B19-sensors-25-05366" ref-type="bibr">19</xref>] is conducted to jointly estimate the metric scale, gravity direction, and initial pose. Upon successful completion of the initialization phase, subsequent preprocessing outputs are propagated into a sliding-window-based local VIO optimization framework. When a previously visited location is recognized using a Bag-of-Words-based visual retrieval method, the accumulated drift in relative pose estimates is corrected through pose graph optimization. Our experiments demonstrate that the proposed method enhances the robustness of localization in challenging outdoor environments.</p></sec></sec><sec id="sec4-sensors-25-05366"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05366"><title>4.1. Experimental Setup</title><p>All experiments were conducted on a workstation equipped with 64 GB RAM, an AMD EPYC 7313P 16-core processor and two NVIDIA RTX 4090 GPUs. The workstation was operated on Ubuntu, with the PyTorch framework [<xref rid="B27-sensors-25-05366" ref-type="bibr">27</xref>] and the OpenCV library [<xref rid="B28-sensors-25-05366" ref-type="bibr">28</xref>] being primarily utilized. For training Monodepth2 [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>], the ResNet-18 encoder [<xref rid="B21-sensors-25-05366" ref-type="bibr">21</xref>] pretrained on ImageNet was used to initialize the model parameters. Pretrained weights from the foundation model trained on the meta-dataset [<xref rid="B17-sensors-25-05366" ref-type="bibr">17</xref>] were used to initialize both the teacher and student depth networks. The models were optimized using the Adam optimizer [<xref rid="B29-sensors-25-05366" ref-type="bibr">29</xref>] with a weight decay of <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where the initial learning rate was set to <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for the baseline [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>] and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for the foundation-based models. To mitigate overfitting, standard data augmentation techniques were employed. These included horizontal flipping and color jittering with brightness, contrast, and saturation factors randomly sampled from the range 0.8 to 1.2 and hue randomly sampled from the range &#8722;0.1 to 0.1. Each augmentation was applied with a probability of 50%. Although all sequences were captured at a frame rate of 10 Hz, different temporal triplet configurations were adopted to account for variations in motion dynamics: the triplet [&#8722;1, 0, 1] was used for the KITTI dataset, while [&#8722;3, 0, 3] was applied to the custom dataset.</p><p>Depth estimation was evaluated using three accuracy metrics (<inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) and six error metrics (RMSE, RMSEi, AbsRel, SqRel, log10, SIlog). The accuracy metric <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the percentage of pixels satisfying <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>/</mml:mo><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>25</mml:mn><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">d</italic> denote the predicted and ground-truth depth values, respectively. Detailed definitions and formulations of the error metrics are available in previous work [<xref rid="B3-sensors-25-05366" ref-type="bibr">3</xref>]. In the tables presenting quantitative results for depth estimation, error metrics and accuracy metrics are indicated with a down arrow and up arrow, respectively. For the odometry evaluation, ground-truth trajectories were generated using a LiDAR-based SLAM algorithm [<xref rid="B18-sensors-25-05366" ref-type="bibr">18</xref>], which is known to provide sufficiently reliable estimation performance in outdoor environments rich in features. The evaluation was conducted only on cases where upon returning to the starting position, small objects such as tree poles were mapped at the same location again. As evaluation metrics, we used the relative pose error (RPE) in both translation and rotation and the RMSE of the absolute trajectory error (ATE) as defined in the RGB-D SLAM benchmark [<xref rid="B30-sensors-25-05366" ref-type="bibr">30</xref>]. Bold and underlined values indicate the best and second-best performance, respectively, for each task.</p></sec><sec id="sec4dot2-sensors-25-05366"><title>4.2. KITTI Dataset</title><p>To empirically validate the effectiveness of the proposed depth estimation framework, we conduct experiments on the KITTI public dataset [<xref rid="B31-sensors-25-05366" ref-type="bibr">31</xref>]. The KITTI dataset was constructed to facilitate a broad spectrum of computer vision tasks, including depth estimation, stereo matching, optical flow, and 3D object detection and tracking. It was collected using a vehicle equipped with stereo cameras, a 3D LiDAR scanner, and GPS/IMU sensors, driving through real-world urban, rural, and highway environments in Karlsruhe, Germany. We employ the Eigen split [<xref rid="B3-sensors-25-05366" ref-type="bibr">3</xref>], which is a standardized data partitioning widely adopted for evaluating depth estimation methods. This split comprises 39,810 monocular triplets for training, 4424 for validation, and 697 for evaluation.</p><p>Quantitative and qualitative evaluations on the KITTI dataset are presented in <xref rid="sensors-25-05366-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05366-f003" ref-type="fig">Figure 3</xref>, respectively. We compare the proposed method against Monodepth2 [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>] and a fine-tuning strategy that uses pretrained parameters from Depth Anything V2 [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>] under a reprojection-based training pipeline. All methods were evaluated using monocular inputs at a fixed resolution of <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1024</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>320</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels, and median scaling was applied following standard practice. The number of parameters in Depth Anything V2 varies depending on the Vision Transformer [<xref rid="B23-sensors-25-05366" ref-type="bibr">23</xref>] backbone: ViT-Large (vitl) yields 335.3 M parameters, while ViT-Small (vits) results in 24.7 M. While the vitl-based model achieves strong performance due to its high representational capacity, its substantially higher computational cost may hinder deployment in resource-constrained settings. In contrast, Monodepth2, with only 14.8 M parameters, is highly efficient in terms of computational resources but requires improvement in estimation accuracy for real-world deployment. The vits-based Depth Anything V2 model achieves better performance than Monodepth2 when fine-tuned, but still falls short of the vitl model overall. Although our proposed method shares the same backbone architecture as the vits-based model, it integrates additional loss functions for training, which lead to the best performance on four error metrics and two accuracy metrics. A slight decrease was observed in AbsRel and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, indicating a minor reduction in absolute distance estimation accuracy. However, improvements in SIlog and <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> suggest that the model has better learned the relative geometric structure within scenes. The qualitative results in <xref rid="sensors-25-05366-f003" ref-type="fig">Figure 3</xref> further support these findings. In columns 1 and 2, the proposed method better captures fine structures such as pedestrians and bicycles. Furthermore, as shown in columns 3 and 4, the method that relies solely on reprojection loss in the third row exhibits texture-induced artifacts in the predicted depth, whereas the proposed method produces smoother and more geometrically consistent depth maps.</p><p><xref rid="sensors-25-05366-t002" ref-type="table">Table 2</xref> presents an ablation analysis that investigates the individual contributions of each loss component in addition to the baseline reprojection loss <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which is consistently employed across all model variants. To ensure a consistent and equitable evaluation, all experiments were carried out using stereo image inputs with a fixed resolution of <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>192</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. When incorporating <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> during training, we observed moderate performance gains in SIlog, RMSE, and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, indicating improvements in both relative depth accuracy and structural consistency. The adoption of <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which supervises surface normals, led to performance improvements across four metrics, including AbsRel. The full model trained with all loss terms <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> achieved superior performance on most metrics, with only a marginal decline in AbsRel, indicating improved overall depth quality.</p></sec><sec id="sec4dot3-sensors-25-05366"><title>4.3. Campus Driving Dataset</title><p>We further evaluate the performance of the proposed algorithm on depth and odometry estimation using a custom-collected real-world dataset, and then we compare it against existing methods. As illustrated in <xref rid="sensors-25-05366-f004" ref-type="fig">Figure 4</xref>, the dataset was acquired using a compact electric vehicle equipped with an RGB stereo camera and a 3D LiDAR sensor. The vehicle was driven at speeds ranging from 6 to 12 km/h within a university campus under clear weather conditions. Although the campus is an outdoor environment, it features rich textures and geometric structures in both 3D point clouds and camera images of resolution <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, making it suitable for generating reliable ground truth using LiDAR-based SLAM and for validating the application of vision-based SLAM systems. The RGB stereo and LiDAR data were recorded at 10 Hz, while the IMU embedded in the LiDAR system was recorded at 100 Hz. The dataset consists of 12 driving sequences, each lasting between 100 and 400 s. For both tasks, the dataset is partitioned into 8 sequences for training, 1 for validation, and 3 for testing, corresponding to 15,025 stereo pairs for training, 1205 for validation, and 6416 for testing. Reliable ground truth was established through offline extrinsic calibration [<xref rid="B32-sensors-25-05366" ref-type="bibr">32</xref>] using optimization-based methods between the camera&#8211;IMU and camera&#8211;LiDAR sensor pairs.</p><p><xref rid="sensors-25-05366-t003" ref-type="table">Table 3</xref> presents depth estimation results on the custom campus driving dataset, which reveal patterns consistent with those observed on the public dataset. Foundation model-based approaches significantly outperformed Monodepth2 across all evaluation metrics, demonstrating the effectiveness of pretraining on large-scale data. Compared to the baseline fine-tuning strategy using only reprojection loss, the proposed method achieved further improvements, particularly in SIlog and RMSE, indicating enhanced relative and overall depth accuracy. As illustrated in <xref rid="sensors-25-05366-f005" ref-type="fig">Figure 5</xref>, the qualitative comparisons highlight distinct improvements in visual prediction quality. Models trained solely with reprojection loss tend to overfit to image textures, resulting in depth artifacts around regions such as tree foliage and ground shadows. In contrast, the proposed method effectively suppresses such artifacts while providing sharper delineation of structural elements like building pillars and bollard edges along with globally smoother and more coherent depth predictions.</p><p>As an RGB-based method, the proposed VIO system leverages depth predictions from the student network and is evaluated against both RGB-only [<xref rid="B19-sensors-25-05366" ref-type="bibr">19</xref>] and RGB-D [<xref rid="B20-sensors-25-05366" ref-type="bibr">20</xref>] VIO baselines, as shown in <xref rid="sensors-25-05366-t004" ref-type="table">Table 4</xref>. In terms of the RMSE of ATE, which serves as a principal metric for evaluating odometry accuracy, the proposed method consistently outperformed the RGB-only baseline across all test cases, achieving significantly lower error values. In case 1, the proposed method demonstrated slightly superior performance even compared to the RGB-D baseline. Although the translation error of the RPE varied across individual cases, the proposed method achieved a higher average performance. In terms of the rotation error of the RPE, the RGB-based method exhibited slightly better performance than the RGB-D based approach. However, as all VIO methods demonstrated consistently low rotation errors, the differences were not statistically significant. As shown in <xref rid="sensors-25-05366-f006" ref-type="fig">Figure 6</xref>, the trajectory estimated by the existing RGB-based method shows substantial scale errors compared to the ground-truth trajectory. In contrast, the proposed method achieves robust overall odometry estimation, including precise scale estimation, comparable to the performance of the RGB-D-based method in all test cases. These experimental results indicate that the proposed method, despite relying solely on RGB input, could achieve performance comparable to that of the RGB-D approach in outdoor driving environments.</p></sec><sec id="sec4dot4-sensors-25-05366"><title>4.4. Zero-Shot Depth Estimation</title><p>High generalization capability is one of the key characteristics of deep learning models, as it enables faster convergence and higher inference accuracy when fine tuning on a target dataset. To compare the generalization performance of our proposed method with existing approaches, we conducted an ablation study on the NYU dataset [<xref rid="B33-sensors-25-05366" ref-type="bibr">33</xref>]. Specifically, we inferred on 654 test images of resolution <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> using the Depth Anything V2 model pretrained by [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>], as well as monodepth2 [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>], Depth Anything V2, and our method, which were all fine-tuned on KITTI. The median scaling was applied to the depth maps predicted by each model, and both qualitative and quantitative evaluations were performed, as shown in <xref rid="sensors-25-05366-f007" ref-type="fig">Figure 7</xref> and <xref rid="sensors-25-05366-t005" ref-type="table">Table 5</xref>. With the exception of Monodepth2 [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>], the foundation models were observed to produce qualitatively reasonable depth maps. Among them, the pretrained Depth Anything V2 achieved relatively lower performance, partly due to its use of scale-only alignment instead of the original scale-and-shift alignment, as well as the limited handling of transparent objects in the benchmark. Despite the domain difference, the Depth Anything V2 fine-tuned on the real-world KITTI dataset showed substantial quantitative improvements but exhibited qualitatively noisier predictions for low-texture surfaces such as walls and tables. In contrast, the model trained using the proposed method attained the highest scores in six out of nine evaluation metrics and, in qualitative evaluations, generated geometrically consistent predictions that closely matched the ground truth.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05366"><title>5. Conclusions</title><p>In this study, we fine-tune a pretrained foundation model through a self-supervised learning framework and integrate it into a VIO system, resulting in performance that surpasses existing RGB-based VIO. Our self-supervised training strategy effectively distills knowledge from the pretrained foundation models into a lightweight student depth network, enabling it to inherit the structural understanding learned from large-scale data. In the depth estimation, the proposed method demonstrates notable improvements in qualitative performance. However, metrics related to absolute depth accuracy exhibit slight degradation. In addition, our method requires further experiments to determine optimal loss-function weighting and entails greater complexity in the training pipeline. Nevertheless, the proposed network exhibits robust performance in capturing object boundaries and recognizing transparent surfaces, achieving results comparable to those of the pretrained foundation model. These capabilities cannot be reliably obtained using reprojection-based supervision alone, highlighting the necessity of our knowledge distillation approach. Furthermore, as transparent objects pose inherent challenges not only in depth prediction but also in reliable ground-truth acquisition, future research should investigate both improved estimation techniques and more appropriate evaluation strategies for such regions.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.S. and S.J.L.; methodology, J.S.; software, J.S.; validation, J.S.; data curation, J.S.; writing&#8212;original draft preparation, J.S.; writing&#8212;review and editing, J.S. and S.J.L.; visualization, J.S.; supervision, S.J.L.; project administration, S.J.L.; funding acquisition, S.J.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The KITTI dataset is publicly available online. The public dataset can be found at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cvlibs.net/datasets/kitti">https://www.cvlibs.net/datasets/kitti</uri>, accessed on 10 June 2025.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05366"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>J.</given-names></name></person-group><article-title>Object detection in 20 years: A survey</article-title><source>Proc. IEEE</source><year>2023</year><volume>111</volume><fpage>257</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2023.3238524</pub-id></element-citation></ref><ref id="B2-sensors-25-05366"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Minaee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Boykov</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Porikli</surname><given-names>F.</given-names></name><name name-style="western"><surname>Plaza</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kehtarnavaz</surname><given-names>N.</given-names></name><name name-style="western"><surname>Terzopoulos</surname><given-names>D.</given-names></name></person-group><article-title>Image segmentation using deep learning: A survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>3523</fpage><lpage>3542</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3059968</pub-id><pub-id pub-id-type="pmid">33596172</pub-id></element-citation></ref><ref id="B3-sensors-25-05366"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eigen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Puhrsch</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fergus</surname><given-names>R.</given-names></name></person-group><article-title>Depth map prediction from a single image using a multi-scale deep network</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2014</year><volume>27</volume><fpage>2366</fpage><lpage>2374</lpage></element-citation></ref><ref id="B4-sensors-25-05366"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>M.Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Autonomous navigation by mobile robots in human environments: A survey</article-title><source>Proceedings of the IEEE International Conference on Robotics and Biomimetics (ROBIO)</source><conf-loc>Kuala Lumpur, Malaysia</conf-loc><conf-date>12&#8211;15 December 2018</conf-date><fpage>1981</fpage><lpage>1986</lpage></element-citation></ref><ref id="B5-sensors-25-05366"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Manzoor</surname><given-names>U.</given-names></name><name name-style="western"><surname>Murray</surname><given-names>J.</given-names></name></person-group><article-title>A review of UAV autonomous navigation in GPS-denied environments</article-title><source>Robot. Auton. Syst.</source><year>2023</year><volume>170</volume><fpage>104533</fpage><pub-id pub-id-type="doi">10.1016/j.robot.2023.104533</pub-id></element-citation></ref><ref id="B6-sensors-25-05366"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pei</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>D.</given-names></name><name name-style="western"><surname>Doermann</surname><given-names>D.</given-names></name></person-group><article-title>Self-supervised learning for monocular depth estimation on minimally invasive surgery scenes</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>30 May&#8211;5 June 2021</conf-date><fpage>7159</fpage><lpage>7165</lpage></element-citation></ref><ref id="B7-sensors-25-05366"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><etal/></person-group><article-title>Grounding dino: Marrying dino with grounded pre-training for open-set object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Milan, Italy</conf-loc><conf-date>2 September&#8211;4 October 2024</conf-date><fpage>38</fpage><lpage>55</lpage></element-citation></ref><ref id="B8-sensors-25-05366"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mintun</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ravi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rolland</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gustafson</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Whitehead</surname><given-names>S.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Lo</surname><given-names>W.</given-names></name><etal/></person-group><article-title>Segment anything</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>4015</fpage><lpage>4026</lpage></element-citation></ref><ref id="B9-sensors-25-05366"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Depth anything v2</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2024</year><volume>37</volume><fpage>21875</fpage><lpage>21911</lpage></element-citation></ref><ref id="B10-sensors-25-05366"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Godard</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mac</surname><given-names>A.O.</given-names></name><name name-style="western"><surname>Firman</surname><given-names>M.</given-names></name><name name-style="western"><surname>Brostow</surname><given-names>G.J.</given-names></name></person-group><article-title>Digging into self-supervised monocular depth estimation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>3828</fpage><lpage>3838</lpage></element-citation></ref><ref id="B11-sensors-25-05366"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xian</surname><given-names>K.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name></person-group><article-title>Knowledge distillation for fast and accurate monocular depth estimation on mobile devices</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Virtual</conf-loc><conf-date>19&#8211;25 June 2021</conf-date><fpage>2457</fpage><lpage>2465</lpage></element-citation></ref><ref id="B12-sensors-25-05366"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pilzer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lathuiliere</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sebe</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ricci</surname><given-names>E.</given-names></name></person-group><article-title>Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;20 June 2019</conf-date><fpage>9768</fpage><lpage>9777</lpage></element-citation></ref><ref id="B13-sensors-25-05366"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Poggi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Aleotti</surname><given-names>F.</given-names></name><name name-style="western"><surname>Tosi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Mattoccia</surname><given-names>S.</given-names></name></person-group><article-title>On the uncertainty of self-supervised monocular depth estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Virtual</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>3227</fpage><lpage>3237</lpage></element-citation></ref><ref id="B14-sensors-25-05366"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.J.</given-names></name></person-group><article-title>Knowledge distillation of multi-scale dense prediction transformer for self-supervised depth estimation</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>18939</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-46178-w</pub-id><pub-id pub-id-type="pmid">37919392</pub-id><pub-id pub-id-type="pmcid">PMC10622578</pub-id></element-citation></ref><ref id="B15-sensors-25-05366"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Oquab</surname><given-names>M.</given-names></name><name name-style="western"><surname>Darcet</surname><given-names>T.</given-names></name><name name-style="western"><surname>Moutakanni</surname><given-names>T.</given-names></name><name name-style="western"><surname>Vo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Szafraniec</surname><given-names>M.</given-names></name><name name-style="western"><surname>Khalidov</surname><given-names>V.</given-names></name><name name-style="western"><surname>Fernandez</surname><given-names>P.</given-names></name><name name-style="western"><surname>Haziza</surname><given-names>D.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>El-Nouby</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Dinov2: Learning robust visual features without supervision</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.07193</pub-id></element-citation></ref><ref id="B16-sensors-25-05366"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>F.</given-names></name><etal/></person-group><article-title>Grounded sam: Assembling open-world models for diverse visual tasks</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2401.14159</pub-id><pub-id pub-id-type="arxiv">2401.14159</pub-id></element-citation></ref><ref id="B17-sensors-25-05366"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ranftl</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Koltun</surname><given-names>V.</given-names></name></person-group><article-title>Vision transformers for dense prediction</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>12179</fpage><lpage>12188</lpage></element-citation></ref><ref id="B18-sensors-25-05366"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bai</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name></person-group><article-title>Faster-LIO: Lightweight tightly coupled LiDAR-inertial odometry using parallel sparse incremental voxels</article-title><source>IEEE Robot. Autom. Lett.</source><year>2022</year><volume>7</volume><fpage>4861</fpage><lpage>4868</lpage><pub-id pub-id-type="doi">10.1109/LRA.2022.3152830</pub-id></element-citation></ref><ref id="B19-sensors-25-05366"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>S.</given-names></name></person-group><article-title>Vins-mono: A robust and versatile monocular visual-inertial state estimator</article-title><source>IEEE Trans. Robot.</source><year>2018</year><volume>34</volume><fpage>1004</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1109/TRO.2018.2853729</pub-id></element-citation></ref><ref id="B20-sensors-25-05366"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Schwertfeger</surname><given-names>S.</given-names></name></person-group><article-title>RGBD-inertial trajectory estimation and mapping for ground robots</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>2251</elocation-id><pub-id pub-id-type="doi">10.3390/s19102251</pub-id><pub-id pub-id-type="pmid">31096683</pub-id><pub-id pub-id-type="pmcid">PMC6567327</pub-id></element-citation></ref><ref id="B21-sensors-25-05366"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B22-sensors-25-05366"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bovik</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Sheikh</surname><given-names>H.R.</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>E.P.</given-names></name></person-group><article-title>Image quality assessment: From error visibility to structural similarity</article-title><source>IEEE Trans. Image Process.</source><year>2004</year><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="B23-sensors-25-05366"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An image is worth 16&#215;16 words: Transformers for image recognition at scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B24-sensors-25-05366"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>R.</given-names></name></person-group><article-title>D2nt: A high-performing depth-to-normal translator</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#8211;2 June 2023</conf-date><fpage>12360</fpage><lpage>12366</lpage></element-citation></ref><ref id="B25-sensors-25-05366"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tomasi</surname></name></person-group><article-title>Good features to track</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>21&#8211;23 June 1994</conf-date><fpage>593</fpage><lpage>600</lpage></element-citation></ref><ref id="B26-sensors-25-05366"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lucas</surname><given-names>B.D.</given-names></name><name name-style="western"><surname>Kanade</surname><given-names>T.</given-names></name></person-group><article-title>An iterative image registration technique with an application to stereo vision</article-title><source>Proceedings of the International Joint Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>24&#8211;28 August 1981</conf-date><fpage>674</fpage><lpage>679</lpage></element-citation></ref><ref id="B27-sensors-25-05366"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paszke</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gross</surname><given-names>S.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lerer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bradbury</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chanan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Killeen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gimelshein</surname><given-names>N.</given-names></name><name name-style="western"><surname>Antiga</surname><given-names>L.</given-names></name><etal/></person-group><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2019</year><volume>32</volume><fpage>8024</fpage><lpage>8035</lpage></element-citation></ref><ref id="B28-sensors-25-05366"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bradski</surname><given-names>G.</given-names></name></person-group><article-title>The OpenCV Library</article-title><source>Dr. Dobb&#8217;s J. Softw. Tools Prof. Program.</source><year>2000</year><volume>25</volume><fpage>120</fpage><lpage>123</lpage></element-citation></ref><ref id="B29-sensors-25-05366"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1412.6980</pub-id></element-citation></ref><ref id="B30-sensors-25-05366"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sturm</surname><given-names>J.</given-names></name><name name-style="western"><surname>Engelhard</surname><given-names>N.</given-names></name><name name-style="western"><surname>Endres</surname><given-names>F.</given-names></name><name name-style="western"><surname>Burgard</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cremers</surname><given-names>D.</given-names></name></person-group><article-title>A benchmark for the evaluation of RGB-D SLAM systems</article-title><source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Vilamoura, Algarve, Portugal</conf-loc><conf-date>7&#8211;12 October 2012</conf-date><fpage>573</fpage><lpage>580</lpage></element-citation></ref><ref id="B31-sensors-25-05366"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Geiger</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lenz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Are we ready for autonomous driving? the kitti vision benchmark suite</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Providence, RI, USA</conf-loc><conf-date>16&#8211;21 June 2012</conf-date><fpage>3354</fpage><lpage>3361</lpage></element-citation></ref><ref id="B32-sensors-25-05366"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tsai</surname><given-names>D.</given-names></name><name name-style="western"><surname>Worrall</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lohr</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nebot</surname><given-names>E.</given-names></name></person-group><article-title>Optimising the selection of samples for robust lidar camera calibration</article-title><source>Proceedings of the IEEE International Intelligent Transportation Systems Conference (ITSC)</source><conf-loc>Indianapolis, IN, USA</conf-loc><conf-date>19&#8211;22 September 2021</conf-date><fpage>2631</fpage><lpage>2638</lpage></element-citation></ref><ref id="B33-sensors-25-05366"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Silberman</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hoiem</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kohli</surname><given-names>P.</given-names></name><name name-style="western"><surname>Fergus</surname><given-names>R.</given-names></name></person-group><article-title>Indoor segmentation and support inference from rgbd images</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Firenze, Italy</conf-loc><conf-date>7&#8211;13 October 2012</conf-date><fpage>746</fpage><lpage>760</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05366-f001" orientation="portrait"><label>Figure 1</label><caption><p>Geometric cues used for knowledge distillation and their effects. Sky mask, relative depth, and surface normal are derived using foundation model-based methods [<xref rid="B7-sensors-25-05366" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05366" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]. These geometric cues guide our knowledge distillation framework, which outperforms [<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>] particularly in handling transparent objects challenging for reprojection-based approaches, as highlighted in the red box.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05366-g001.jpg"/></fig><fig position="float" id="sensors-25-05366-f002" orientation="portrait"><label>Figure 2</label><caption><p>Overview of the proposed visual&#8211;inertial odometry pipeline. Yellow boxes represent non-learnable geometric modules, blue boxes indicate deep neural networks, and red boxes denote loss functions used for training the student network. In particular, geometry-aware distillation serves as a key module for learning the geometric consistency of the pretrained foundation model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05366-g002.jpg"/></fig><fig position="float" id="sensors-25-05366-f003" orientation="portrait"><label>Figure 3</label><caption><p>Qualitative comparisons of depth estimation results on the Eigen validation split of the KITTI dataset. The proposed method clearly distinguishes the details of object and recognizes transparent object, as highlighted in the red box.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05366-g003.jpg"/></fig><fig position="float" id="sensors-25-05366-f004" orientation="portrait"><label>Figure 4</label><caption><p>Sensor setup for the custom dataset collection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05366-g004.jpg"/></fig><fig position="float" id="sensors-25-05366-f005" orientation="portrait"><label>Figure 5</label><caption><p>Qualitative comparisons of depth estimation results on the custom dataset. DAv2 denotes Depth Anything V2 [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]. The proposed method clearly distinguishes the details of object, as highlighted in the red box.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05366-g005.jpg"/></fig><fig position="float" id="sensors-25-05366-f006" orientation="portrait"><label>Figure 6</label><caption><p>Qualitative comparisons of odometry estimation results on the custom dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05366-g006.jpg"/></fig><fig position="float" id="sensors-25-05366-f007" orientation="portrait"><label>Figure 7</label><caption><p>Qualitative comparisons of zero-shot depth estimation results on the NYU dataset. DAv2-pre and DAv2-fine denote the Depth Anything V2 model pretrained by [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>] and fine-tuned on KITTI in our work. The proposed method clearly distinguishes the details of object and recognizes transparent object, as highlighted in the red box.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05366-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05366-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05366-t001_Table 1</object-id><label>Table 1</label><caption><p>Quantitative evaluation of depth estimation performance on the Eigen validation split of the KITTI dataset. All methods use monocular input with a resolution of <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1024</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>320</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. DAv2 denotes Depth Anything V2 [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]. Bold and underline represent the best performance and the next best performance between lightweight models, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" colspan="1">Params</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-right:solid thin" rowspan="1">Error Metrics &#8595;</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Accuracy Metrics &#8593;</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SIlog</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>AbsRel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SqRel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RMSE</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RMSEi</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>log10</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm89" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Monodepth2&#160;[<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">14.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">18.293</td><td align="center" valign="middle" rowspan="1" colspan="1">0.109</td><td align="center" valign="middle" rowspan="1" colspan="1">0.832</td><td align="center" valign="middle" rowspan="1" colspan="1">4.648</td><td align="center" valign="middle" rowspan="1" colspan="1">0.186</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">0.048</td><td align="center" valign="middle" rowspan="1" colspan="1">0.888</td><td align="center" valign="middle" rowspan="1" colspan="1">0.963</td><td align="center" valign="middle" rowspan="1" colspan="1">0.982</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DAv2(vits)&#160;[<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">24.7 M</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>16.994</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.099
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.740</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>4.314</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.173</underline>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>0.043</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.908</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.969</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.985</underline>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">24.7 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>16.856</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.101</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.687</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.280</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.172</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<underline>0.044</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.899</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.969</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.986</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAv2(vitl)&#160;[<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">335.3 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.406</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.090</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.639</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.040</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.166</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">0.040</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.924</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.985</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05366-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05366-t002_Table 2</object-id><label>Table 2</label><caption><p>Ablation study of loss function configurations on the Eigen validation split of the KITTI dataset. All methods use stereo input with a resolution of <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>192</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Bold and underline represent the best performance and the next best performance, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold">repr</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold">self</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold">stru</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold">geom</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SIlog &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AbsRel &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula> &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">19.688</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.118</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">5.230</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">18.926</td><td align="center" valign="middle" rowspan="1" colspan="1">0.120</td><td align="center" valign="middle" rowspan="1" colspan="1">5.055</td><td align="center" valign="middle" rowspan="1" colspan="1">0.852</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">18.872</td><td align="center" valign="middle" rowspan="1" colspan="1">0.121</td><td align="center" valign="middle" rowspan="1" colspan="1">5.067</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.854</underline>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>18.70</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.115</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>5.024</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.861</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>18.521</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.119</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.017</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.861</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05366-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05366-t003_Table 3</object-id><label>Table 3</label><caption><p>Quantitative evaluation of depth estimation performance on the custom dataset. DAv2 denotes Depth Anything V2 [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]. Bold and underline represent the best performance and the next best performance, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" colspan="1">Method</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-right:solid thin" rowspan="1">Error Metrics &#8595;</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Accuracy Metrics &#8593;</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SIlog</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>AbsRel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SqRel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RMSE</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RMSEi</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>log10</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Monodepth2&#160;[<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">19.448</td><td align="center" valign="middle" rowspan="1" colspan="1">0.121</td><td align="center" valign="middle" rowspan="1" colspan="1">0.810</td><td align="center" valign="middle" rowspan="1" colspan="1">3.106</td><td align="center" valign="middle" rowspan="1" colspan="1">0.197</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">0.049</td><td align="center" valign="middle" rowspan="1" colspan="1">0.886</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" rowspan="1" colspan="1">0.979</td></tr><tr><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">DAv2(vits)&#160;[<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>17.883</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.096</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.777</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>2.997</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.181</underline>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>0.039</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.918</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.964</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.980</underline>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>17.631</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.097</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.712</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.857</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.180</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<underline>0.041</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.903</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.964</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.981</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05366-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05366-t004_Table 4</object-id><label>Table 4</label><caption><p>Quantitative evaluation of odometry estimation performance on the campus driving dataset. Bold represent the best performance of odometry estimation at the each metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-right:solid thin" rowspan="1">Driving Scenario</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Case 1</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Case 2</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Case 3</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" colspan="1">Average</th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1">
<bold>Driving Distance [m]</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>318.12</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>394.75</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>502.55</bold>
</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" colspan="1">
<bold>RMSE of ATE [m]</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">VINS-Mono&#160;[<xref rid="B19-sensors-25-05366" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">4.9564</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2617</td><td align="center" valign="middle" rowspan="1" colspan="1">7.8689</td><td align="center" valign="middle" rowspan="1" colspan="1">7.0290</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.1027</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.2186</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>7.0312</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.4508</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">VINS-RGBD&#160;[<xref rid="B20-sensors-25-05366" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.3050</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.7788</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.1443</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0760</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" colspan="1">
<bold>Translation error of RPE [m]</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">VINS-Mono&#160;[<xref rid="B19-sensors-25-05366" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.3712</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6332</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.3648</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4564</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3993</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.4627</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3921</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.4180</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">VINS-RGBD&#160;[<xref rid="B20-sensors-25-05366" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3957</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4092</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3399</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3816</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" colspan="1">
<bold>Rotation error of RPE [deg]</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">VINS-Mono&#160;[<xref rid="B19-sensors-25-05366" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.3116</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.2635</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.3291</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.3014</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3319</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3666</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3733</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3573</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">VINS-RGBD&#160;[<xref rid="B20-sensors-25-05366" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2975</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2959</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3440</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3125</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05366-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05366-t005_Table 5</object-id><label>Table 5</label><caption><p>Quantitative evaluation of zero-shot depth estimation on the NYU dataset. DAv2-pre and DAv2-fine denote the Depth Anything V2 model pretrained by [<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>] and fine-tuned on KITTI in our work. Bold and underline represent the best performance and the next best performance, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" colspan="1">Method</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-right:solid thin" rowspan="1">Error Metrics &#8595;</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Accuracy Metrics &#8593;</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SIlog</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>AbsRel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SqRel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RMSE</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RMSEi</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>log10</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#948;</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Monodepth2&#160;[<xref rid="B10-sensors-25-05366" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">35.065</td><td align="center" valign="middle" rowspan="1" colspan="1">0.336</td><td align="center" valign="middle" rowspan="1" colspan="1">0.498</td><td align="center" valign="middle" rowspan="1" colspan="1">1.079</td><td align="center" valign="middle" rowspan="1" colspan="1">0.370</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">0.129</td><td align="center" valign="middle" rowspan="1" colspan="1">0.484</td><td align="center" valign="middle" rowspan="1" colspan="1">0.778</td><td align="center" valign="middle" rowspan="1" colspan="1">0.913</td></tr><tr><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">DAv2-pre&#160;[<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">35.086</td><td align="center" valign="middle" rowspan="1" colspan="1">0.326</td><td align="center" valign="middle" rowspan="1" colspan="1">0.787</td><td align="center" valign="middle" rowspan="1" colspan="1">1.602</td><td align="center" valign="middle" rowspan="1" colspan="1">0.365</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">0.124</td><td align="center" valign="middle" rowspan="1" colspan="1">0.515</td><td align="center" valign="middle" rowspan="1" colspan="1">0.772</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td></tr><tr><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">DAv2-fine&#160;[<xref rid="B9-sensors-25-05366" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>19.446</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.163</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.160</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.702</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.203</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>0.068</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.759</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.958</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.990</underline>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>19.464</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.166</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.140</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.661</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.203</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<underline>0.070</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.732</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.966</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.996</bold>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>