<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430961</article-id><article-id pub-id-type="pmcid-ver">PMC12430961.1</article-id><article-id pub-id-type="pmcaid">12430961</article-id><article-id pub-id-type="pmcaiid">12430961</article-id><article-id pub-id-type="doi">10.3390/s25175378</article-id><article-id pub-id-type="publisher-id">sensors-25-05378</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>2SLOD-HCG: HCG Test Strip Concentration Prediction Network</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Hu</surname><given-names initials="Q">Qi</given-names></name><xref rid="af1-sensors-25-05378" ref-type="aff">1</xref><xref rid="c1-sensors-25-05378" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhao</surname><given-names initials="J">Jinshu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af2-sensors-25-05378" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Kan</surname><given-names initials="S">Shimin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-05378" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shi</surname><given-names initials="Q">Qiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af3-sensors-25-05378" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="N">Ning</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af2-sensors-25-05378" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="J">Jiajian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05378" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ma</surname><given-names initials="Z">Zhifang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af3-sensors-25-05378" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Barbu</surname><given-names initials="A">Adrian</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05378"><label>1</label>School of Artificial Intelligence, Changchun University of Science and Technology, Changchun 130022, China; <email>2023102067@mails.cust.edu.cn</email> (S.K.); </aff><aff id="af2-sensors-25-05378"><label>2</label>School of Electronic Information Engineering, Changchun University of Science and Technology, Changchun 130022, China; <email>2023100778@mails.cust.edu.cn</email> (J.Z.); <email>2023100718@mails.cust.edu.cn</email> (N.W.)</aff><aff id="af3-sensors-25-05378"><label>3</label>State Key Laboratory of Polymer Physics and Chemistry, Changchun Institute of Applied Chemistry, Changchun 130022, China; <email>shiqiang@ciac.ac.cn</email> (Q.S.); <email>zfma@ciac.ac.cn</email> (Z.M.)</aff><author-notes><corresp id="c1-sensors-25-05378"><label>*</label>Correspondence: <email>huqi@cust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>01</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5378</elocation-id><history><date date-type="received"><day>08</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>01</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05378.pdf"/><abstract><p>Human chorionic gonadotropin (HCG) is an essential biomarker for the evaluation and diagnosis of early pregnancy, multiple pregnancies, and ectopic pregnancies. However, the accuracy of test strip interpretation is often compromised by inconvenient and uncomfortable professional testing, the black-box nature of AI-based detection methods, and variations in image quality caused by mobile photography and lighting conditions. To address these challenges, we propose 2SLOD-HCG, a novel network for test strip concentration detection. Our approach introduces an enhanced spatial pyramid pooling (SPP) module to better integrate multi-scale receptive field information and incorporates an elastic variational cross-FPN structure augmented with lightweight transformer blocks to strengthen global feature perception. Furthermore, a SimAM attention mechanism is applied to highlight critical local features. These improvements collectively enhance the network&#8217;s ability to capture both fine-grained and global contextual information. We constructed a dataset of 50,000 augmented test strip images collected under three lighting conditions and four mobile photography scenarios. The results demonstrate that 2SLOD-HCG achieves superior accuracy and robustness compared to existing YOLO-based baselines, particularly in detecting the small color-developing regions of test strips.</p></abstract><kwd-group><kwd>concentration detection</kwd><kwd>multi-scale fusion</kwd><kwd>lightweight attention mechanism</kwd><kwd>2SLOD-HCG</kwd></kwd-group><funding-group><award-group><funding-source>Changchun Institute of Applied Chemistry</funding-source><award-id>20230204111YY</award-id></award-group><funding-statement>This research was funded by Changchun Institute of Applied Chemistry grant number 20230204111YY.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05378"><title>1. Introduction</title><p>Human chorionic gonadotropin (HCG) is a hormone secreted by the placenta that plays a key role in supporting pregnancy and fetal development [<xref rid="B1-sensors-25-05378" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05378" ref-type="bibr">2</xref>]. Quantitative detection of HCG levels is clinically valuable for diagnosing early pregnancy, multiple pregnancies, and ectopic pregnancies. In particular, monitoring the changes in HCG concentration over time can provide important guidance for the early identification of ectopic pregnancy.</p><p>Point-of-care testing (POCT) based on colloidal gold immunochromatographic strips has become an attractive option due to its simplicity and rapid results. However, traditional strip interpretation typically relies on specialized readers, which are expensive and often limited to specific manufacturers&#8217; products [<xref rid="B3-sensors-25-05378" ref-type="bibr">3</xref>]. To overcome these limitations, there is an urgent need for a more intelligent, practical, and accurate approach that can analyze test strip data conveniently in real-world settings. With the widespread use of smartphones equipped with high-resolution cameras, mobile-based strip analysis combined with image recognition offers a promising alternative, enabling real-time interpretation without professional devices.</p><p>Test strips can generally be divided into two categories according to the labeling materials: those based on fluorescent dyes and those based on visible chromogenic reagents [<xref rid="B3-sensors-25-05378" ref-type="bibr">3</xref>]. Fluorescent test strips are highly sensitive but require specialized excitation light sources, making them costly and less accessible. In contrast, colloidal gold-based visible strips are more affordable and suitable for home use, but their color development is strongly affected by variations in illumination, imaging devices, and background conditions [<xref rid="B4-sensors-25-05378" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05378" ref-type="bibr">5</xref>]. For example, differences in smartphone camera resolution or ambient lighting (e.g., cloudy or dim conditions) often lead to color inconsistencies in captured images. Although closed-box imaging devices can mitigate these effects, they are inconvenient for routine use.</p><p>To address these challenges, this study proposes a smartphone-based strip concentration detection algorithm inspired by the YOLO object detection framework. By improving feature extraction and enhancing robustness to environmental variations, the proposed method aims to achieve reliable, real-time interpretation of colloidal gold test strips. <xref rid="sensors-25-05378-f001" ref-type="fig">Figure 1</xref> illustrates the test strips before and after a reaction, and where the intensity of the T-line color corresponds to different HCG concentrations.</p></sec><sec id="sec2-sensors-25-05378"><title>2. Related Work</title><p>Research on medical image classification and test strip interpretation has developed rapidly with the advancement of image processing technologies. Conventional strip reading typically relies on visual observation of the color development of the test line (T-line) and control line (C-line), which is subjective and prone to error [<xref rid="B6-sensors-25-05378" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05378" ref-type="bibr">7</xref>]. Although professional readers provide more accurate and quantitative results, their high cost and limited compatibility restrict widespread adoption. Recent progress in smartphone-based imaging and biosensing has shown strong potential for replacing professional readers due to their affordability, portability, and integration with intelligent algorithms [<xref rid="B8-sensors-25-05378" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05378" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05378" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05378" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05378" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05378" ref-type="bibr">13</xref>].</p><sec id="sec2dot1-sensors-25-05378"><title>2.1. Medical Testing Methods</title><p>Two mainstream approaches are currently used in strip concentration detection: visible light imaging and fluorescence imaging. Fluorescence-based systems achieve high sensitivity but generally require specialized excitation sources and closed-box imaging, which increases complexity and cost. Several studies [<xref rid="B14-sensors-25-05378" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05378" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05378" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05378" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05378" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05378" ref-type="bibr">19</xref>] have proposed smartphone-compatible fluorescence systems using 3D-printed dark boxes, ultraviolet LEDs, or optical adapters. While these methods improve detection accuracy, they remain impractical for routine or home use.</p><p>In contrast, colloidal gold strips based on visible chromogenic reagents are low-cost and user-friendly. Smartphone cameras can directly capture the test results for analysis [<xref rid="B20-sensors-25-05378" ref-type="bibr">20</xref>]. However, this approach is vulnerable to variations in ambient lighting, imaging devices, and backgrounds, leading to inconsistent color interpretation. These challenges highlight the need for more robust algorithms that can compensate for environmental variability while maintaining accuracy.</p></sec><sec id="sec2dot2-sensors-25-05378"><title>2.2. Current Status of Mobile Terminal Detection Applications</title><p>Smartphones have been widely applied as portable imaging and diagnostic platforms in healthcare [<xref rid="B9-sensors-25-05378" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05378" ref-type="bibr">10</xref>], biosafety [<xref rid="B11-sensors-25-05378" ref-type="bibr">11</xref>], environmental monitoring [<xref rid="B12-sensors-25-05378" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05378" ref-type="bibr">13</xref>], and food safety [<xref rid="B14-sensors-25-05378" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05378" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05378" ref-type="bibr">16</xref>]. Researchers have explored various methods to improve recognition accuracy on mobile devices, including texture-based feature extraction [<xref rid="B21-sensors-25-05378" ref-type="bibr">21</xref>], color histogram analysis [<xref rid="B22-sensors-25-05378" ref-type="bibr">22</xref>], and SURF-based recognition [<xref rid="B23-sensors-25-05378" ref-type="bibr">23</xref>]. In medical applications, cloud-assisted systems have been developed for urine test strip analysis [<xref rid="B24-sensors-25-05378" ref-type="bibr">24</xref>] and mobile disease diagnosis [<xref rid="B25-sensors-25-05378" ref-type="bibr">25</xref>]. More recently, deep learning models such as YOLO and Darknet have been deployed on Android platforms for crop disease detection, demonstrating the feasibility of mobile AI for real-time recognition tasks [<xref rid="B26-sensors-25-05378" ref-type="bibr">26</xref>]. These works collectively indicate that mobile platforms can support complex image recognition tasks but adapting them to medical test strips requires specialized solutions for robustness and precision.</p></sec><sec id="sec2dot3-sensors-25-05378"><title>2.3. Current Status of Test Line Detection</title><p>Deep learning-based object detection methods can be categorized into two groups: two-stage detectors, such as R-CNN [<xref rid="B27-sensors-25-05378" ref-type="bibr">27</xref>], Faster R-CNN [<xref rid="B28-sensors-25-05378" ref-type="bibr">28</xref>], and Mask R-CNN [<xref rid="B29-sensors-25-05378" ref-type="bibr">29</xref>], which generate region proposals before classification, and one-stage detectors, such as YOLO [<xref rid="B30-sensors-25-05378" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05378" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05378" ref-type="bibr">32</xref>] and SSD [<xref rid="B33-sensors-25-05378" ref-type="bibr">33</xref>], which directly perform classification and regression in an end-to-end manner. One-stage methods are particularly suitable for real-time applications due to their efficiency. In addition, image segmentation approaches such as FCN [<xref rid="B34-sensors-25-05378" ref-type="bibr">34</xref>], DeconvNet [<xref rid="B35-sensors-25-05378" ref-type="bibr">35</xref>], and SegNet [<xref rid="B36-sensors-25-05378" ref-type="bibr">36</xref>] have been explored for biomedical imaging tasks.</p><p>For test strip analysis, accurately detecting small color bands under varying conditions is a key challenge. Existing methods have demonstrated the feasibility of using YOLO-like architectures for this task, but their performance is limited by sensitivity to illumination, insufficient feature fusion, and lack of adaptive attention mechanisms. These limitations motivate the development of improved architectures that combine multi-scale feature fusion, global context modeling, and lightweight attention mechanisms to enhance both detection accuracy and computational efficiency.</p><p>In order to determine the number of color bands and evaluate the reliability of the test results, the primary goal of this paper is to develop an HCG test strip image enhancement method based on grayscale normalization. To do this, image preprocessing algorithms are used to precisely locate and completely enclose the color reaction region on the test strip. Next, under typical lighting conditions, we construct standard color reference spots on the test strip and determine their grayscale values. To ascertain if the image is overexposed or underexposed, these values are compared with the grayscale values at the same locations under various lighting circumstances. Depending on the results, different techniques are used to remove the impacts of lighting. In order to boost test strip feature extraction, we adjust the multi-scale feature fusion module, improve the YOLO model to increase its capacity to detect small objects, and use artificial intelligence algorithms for real-time diagnosis. The HCG concentration and its fluctuation are quantitatively determined by the system. Accurate ectopic pregnancy risk prediction is made possible by real-time diagnosis using AI systems.</p><p>While some previous studies have employed similar architectural designs, this approach achieves higher detection accuracy and generalization by incorporating multi-scale feature fusion and an adaptive attention mechanism. Compared to existing AI products on the market, our approach requires fewer computational resources and achieves faster inference speed, demonstrating its potential for practical application. While related techniques have been extensively discussed in the literature, the innovation of this study lies in improving the model structure and training strategy based on the specific characteristics of the task. The following are our primary contributions:<list list-type="order"><list-item><p>For the first time, we employ artificial intelligence to determine test strip concentrations from smartphone images. The chromogenic capacity of colloidal gold was enhanced to lower the detection limit, and AI-based techniques were integrated to accurately quantify HCG levels. This approach enables rapid detection of ectopic pregnancy using colloidal gold-based POCT test strips;</p></list-item><list-item><p>We suggest the S-5 module design at the bottom of the backbone network to fulfill the requirement for small-region target detection. The SimAM lightweight attention mechanism is added after the C2f module to increase detection accuracy;</p></list-item><list-item><p>To improve the model&#8217;s ability to perceive global characteristics, we add FPN operators to the bidirectional multi-scale fusion cross-FPN in the neck section and integrate a lightweight transformer encoder structure.</p></list-item></list></p></sec></sec><sec id="sec3-sensors-25-05378"><title>3. Main Research Content</title><p>In test strip concentration detection, the concentration is primarily determined by feature extraction of the T-line and C-line. To address the challenge of small object detection, this study enhances detection accuracy by introducing an improved spatial pyramid pooling (SPP) module at the terminal stage of the YOLOv8 backbone. In addition, feature pyramid network (FPN) operators are incorporated into the bidirectional multi-scale fusion cross-FPN of the neck section, while a lightweight transformer encoder is embedded to strengthen global feature representation. To further improve robustness against image degradation caused by lighting or focus variations during smartphone photography, the SimAM attention mechanism is integrated into the backbone, enabling the model to emphasize the color-developing regions of test strips.</p><p>The proposed framework, termed 2SLOD-HCG, is illustrated in <xref rid="sensors-25-05378-f002" ref-type="fig">Figure 2</xref> and consists of three principal components: backbone, neck, and head. While retaining the design philosophy of YOLOv5, YOLOv8 introduces several architectural optimizations, including the use of a PAN-FPN structure for feature fusion and the lightweight C2f module. The detection head adopts a decoupled-head design and employs a task-aligned assignment strategy in place of conventional anchor-based methods. Moreover, YOLOv8 leverages an anchor-free mechanism, utilizing VariFocal loss (VFL) for classification and a combination of distribution-evolving loss (DEL) with complete IoU (CIoU) loss for regression [<xref rid="B37-sensors-25-05378" ref-type="bibr">37</xref>]. These advancements provide a robust foundation upon which the improvements in this study are developed.</p><sec id="sec3dot1-sensors-25-05378"><title>3.1. Backbone Network Improvement</title><p>The backbone of the YOLO model is primarily responsible for hierarchical feature extraction from input images. To better accommodate the challenge of small-scale chromogenic regions in test strip detection, we introduce two modifications: (i) an enhanced spatial pyramid pooling (SPP) module, termed S-5, and (ii) the integration of the SimAM attention mechanism after the C2f module. These improvements aim to enrich multi-scale representation while enhancing feature discrimination under varying imaging conditions.</p><sec id="sec3dot1dot1-sensors-25-05378"><title>3.1.1. Improved SPP Module&#8212;S-5 Module</title><p>In convolutional neural networks (CNNs), deeper layers often lose fine-grained spatial details, which constrains the detection of small objects. To alleviate this issue, spatial pyramid pooling (SPP) [<xref rid="B38-sensors-25-05378" ref-type="bibr">38</xref>] aggregates contextual information at multiple scales. However, in test strip detection, the target region is extremely small, and the standard SPP is unable to provide sufficient local detail.</p><p>To address this limitation, we propose an improved module named S-5, which expands the receptive field by introducing larger pooling kernels (1 &#215; 1, 4 &#215; 4, 7 &#215; 7, 10 &#215; 10, and 13 &#215; 13). This design enhances multi-scale context modeling while preserving fine local features that are critical for detecting the narrow color bands on test strips.</p><p>As shown in <xref rid="sensors-25-05378-f002" ref-type="fig">Figure 2</xref>, the S-5 module is placed at the bottom layer of the YOLOv8 backbone. Its output is explicitly merged into the feature fusion pathway of the neck, as shown in <xref rid="sensors-25-05378-f003" ref-type="fig">Figure 3</xref>, ensuring that small-scale cues are retained during subsequent feature aggregation. This explicit integration corrects the limitation of standard SPP and reinforces the flow of fine-grained features throughout the detection pipeline. The experimental results demonstrate that replacing SPP with S-5 yields higher mAP, confirming the effectiveness of this modification in small object detection.</p></sec><sec id="sec3dot1dot2-sensors-25-05378"><title>3.1.2. Lightweight Attention Mechanism&#8212;SimAM</title><p>Test strip images taken by end users often suffer from image quality degradation due to factors such as uneven lighting or lack of focus, which obscures fine color lines and reduces recognition accuracy. The attention mechanism provides an effective strategy to alleviate this problem by guiding the network to prioritize regions with higher discriminative value. Specifically, the SimAM attention mechanism highlights salient color regions and reduces the impact of blurred or noisy regions, thereby preserving key feature information under uncertain imaging conditions [<xref rid="B39-sensors-25-05378" ref-type="bibr">39</xref>].</p><p>Unlike traditional channel-only or spatial-only attention modules (such as SE [<xref rid="B40-sensors-25-05378" ref-type="bibr">40</xref>] and CBAM [<xref rid="B41-sensors-25-05378" ref-type="bibr">41</xref>]), SimAM [<xref rid="B40-sensors-25-05378" ref-type="bibr">40</xref>] directly estimates the three-dimensional attention weights within the network layer. By simultaneously modeling the correlations in the spatial and channel dimensions, SimAM can capture more comprehensive dependencies without introducing additional parameters. This lightweight and efficient design makes it particularly suitable for mobile-based test strip analysis, where computational efficiency and robustness to image artifacts are crucial.</p><p>As shown in <xref rid="sensors-25-05378-f004" ref-type="fig">Figure 4</xref>, SimAM enhances the YOLOv8 backbone by adaptively weighting informative regions and suppressing redundant background responses. This allows the model to maintain recognition performance even under challenging conditions, such as illumination changes or partially blurred color lines.</p><p>The SimAM attention mechanism is integrated into the YOLOv8 backbone following the C2f module to enhance feature extraction. Its primary function is to suppress irrelevant background responses surrounding the test strip while adaptively emphasizing the chromogenic regions that are critical for concentration detection. By applying neuron-level spatial suppression to areas adjacent to the target regions, SimAM effectively mitigates the interference caused by illumination variations and background clutter. This targeted enhancement enables the backbone to capture more discriminative chromogenic features, thereby improving the robustness and accuracy of small-object detection in test strip analysis.</p></sec></sec><sec id="sec3dot2-sensors-25-05378"><title>3.2. Neck Network Improvement</title><p>The neck of YOLOv8 is responsible for multi-scale feature aggregation, which is critical for detecting small targets such as the chromogenic regions on test strips. To address the limitations of the original design, we introduce two complementary improvements: (1) an enhanced FPN structure tailored for small object detection, and (2) the integration of a lightweight transformer encoder to strengthen global feature perception while maintaining computational efficiency.</p><sec id="sec3dot2dot1-sensors-25-05378"><title>3.2.1. FPN Module Enhancement</title><p>Traditional FPN structures primarily emphasize top-down feature fusion but often fail to adequately preserve the fine-grained information required for small object recognition. To mitigate this, we augment the original YOLOv8 FPN with additional FPN operators for cross-layer fusion, enabling more effective integration of channel-wise and spatial features. Furthermore, instead of relying on deeper <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> layers that are less sensitive to small objects, we introduce a dedicated <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> detection layer optimized for fine-grained features. This modification significantly enhances the representation of small-scale chromogenic regions while preserving multi-scale fusion. The fusion process between <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is formulated as:</p><p>The <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> layers&#8217; fusion calculation formula is:<disp-formula id="FD1-sensors-25-05378"><label>(1)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#949;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05378"><label>(2)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#949;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This adjustment strengthens the sensitivity of the network to low-level features that are crucial for accurate HCG concentration detection.</p></sec><sec id="sec3dot2dot2-sensors-25-05378"><title>3.2.2. Lightweight Transformer Enhancement in the Neck</title><p>While FPN enhancement improves local detail preservation, global context is equally important to mitigate background interference and illumination variations in test strip images. Inspired by the vision transformer (ViT) [<xref rid="B42-sensors-25-05378" ref-type="bibr">42</xref>], we replace the neck&#8217;s C2f module with a lightweight transformer encoder, which dynamically models long-range dependencies across feature maps. Unlike conventional attention mechanisms restricted to either spatial or channel domains, this design captures joint spatial&#8211;channel interactions, thereby improving robustness in complex imaging conditions.</p><p>To ensure real-time applicability, the transformer encoder is simplified using grid-based input, reduced feature dimensionality, and depthwise separable convolutions, together with lightweight positional encoding and pruning techniques. These strategies maintain high detection accuracy while reducing parameter count and computational complexity, making the model more efficient in resource-constrained environments.</p></sec></sec><sec id="sec3dot3-sensors-25-05378"><title>3.3. Loss Function</title><p>The loss function employed in this study follows the YOLOv8 design, integrating both classification and regression objectives to optimize detection performance. It consists of three components: (1) distribution focal loss (DFL) for bounding box regression, (2) binary cross-entropy (BCE) loss for classification, and (3) complete intersection over union (CIoU) loss for localization. The total loss is computed as a weighted sum:<disp-formula id="FD3-sensors-25-05378"><label>(3)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where the weighting coefficients are set to <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> = 1.5, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> = 0.5, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> = 0.5, balancing the contributions of each term.</p><list list-type="simple"><list-item><label>(1)</label><p>Distribution Focal Loss (DFL)</p></list-item></list><p>DFL enhances localization accuracy by modeling the bounding box offset as a discrete distribution over predefined bins (Reg_max = 16). The loss is defined as:<disp-formula id="FD4-sensors-25-05378"><label>(4)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the target offset, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the neighboring bin centers, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the predicted probabilities for these bins. This formulation allows the model to capture fine-grained spatial details, which is crucial for precise localization of small color-developing regions.</p><list list-type="simple"><list-item><label>(2)</label><p>Classification Loss</p></list-item></list><p>The classification branch employs binary cross-entropy loss to handle the one-hot encoded class labels:<disp-formula id="FD5-sensors-25-05378"><label>(5)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ln</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ln</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the predicted confidence for class <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the corresponding ground truth label. This term is computed for all anchor positions.</p><list list-type="simple"><list-item><label>(3)</label><p>CIoU Loss</p></list-item></list><p>The CIoU loss term refines bounding box regression by considering not only the overlap but also the center point distance and aspect ratio consistency:<disp-formula id="FD6-sensors-25-05378"><label>(6)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the intersection over union between the predicted and ground truth boxes, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the Euclidean distance between their center points, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the diagonal length of the smallest enclosing box, and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> measures aspect ratio similarity. The weight <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> adjusts the influence of the aspect ratio term. This formulation is particularly effective in aligning the predicted boxes with the elongated and narrow chromogenic regions on test strips.</p><p>Overall, this composite loss function balances classification confidence with localization precision, enabling robust detection of fine-scale chromogenic features even under variable lighting and imaging conditions.</p></sec></sec><sec id="sec4-sensors-25-05378"><title>4. Experiment</title><sec id="sec4dot1-sensors-25-05378"><title>4.1. Experimental Environment and Parameters</title><p>The model was trained using the PyTorch 1.10 framework, running in Ubuntu 20.04 on an NVIDIA RTX 2080Ti (NVIDIA Corporation, Santa Clara, CA, USA) graphics card with 11 GB of video memory. The stochastic gradient descent (SGD) optimizer was used, with an initial learning rate of 0.01 and a batch size of 8. Training lasted 50 epochs, with a linear warm-up strategy applied in the first three epochs, gradually increasing the learning rate back to the initial value. Data augmentation strategies included random rotation (&#177;10&#176;), translation (&#177;10 pixels), and brightness adjustment (&#177;20%). Mosaic augmentation was disabled in the last 10 epochs to prevent overfitting. Performance on the validation set was monitored during training, and early stopping was used to prevent overfitting.</p></sec><sec id="sec4dot2-sensors-25-05378"><title>4.2. Dataset and Preprocessing</title><p>This study focused on the color-developed areas of test strips. Because finding equivalent or comparable samples in existing datasets is challenging, we created a unique dataset using test strip images collected from the internet and personal photographs. To create this dataset, HCG solution was diluted to varying concentrations and applied in equal amounts to test strips from the same batch under carefully monitored conditions. We photographed the test strips using various devices, including the Oppo Find X7 Ultra (OPPO, Dongguan, China), Vivo S17 (Vivo, Dongguan, China), iPhone 14 (Apple Inc., Cupertino, CA, USA), and Huawei Mate 60 (Huawei Technologies Co., Ltd., Shenzhen, China), under various lighting conditions (including low light, bright light, and natural light) and against various photographic backgrounds.</p><p>After image acquisition, we used the LabelImg program to annotate the dataset in YOLO format. Each image generated an equal amount of annotations and a dataset of 50,000 photos, along with a corresponding .txt file. <xref rid="sensors-25-05378-f005" ref-type="fig">Figure 5</xref> shows sample photos. To provide finer-grained concentration-based segmentation, the bin size for low-concentration values was set to 50. Starting at 100, the bin size for higher concentrations gradually increased. The number of samples within each concentration range was nearly equal.</p><p>The evaluation was based on a dataset consisting of 120 HCG test strips from eight production batches, covering a concentration range of 0 to 200 IU/L, as determined by the laboratory&#8217;s gold standard assay (chemiluminescent immunoassay). A total of 1440 images were acquired under all conditions, with each test strip captured from multiple angles and distances.</p><p>Under baseline conditions (iPhone 14 Pro (Apple Inc., Cupertino, CA, USA), uniform 5000 K LED illumination (Aputure, Shenzhen, China), 25 cm vertical imaging), the model achieved a mean absolute error (MAE) of 1.8 IU/L at the clinically relevant threshold of 25 IU/L, an R<sup>2</sup> of 0.982, a sensitivity of 98.5%, and a specificity of 97.9%. Performance remained stable under slight variations in lighting conditions, such as warm (3000 K) or cool (6500 K), with MAE increases of only 2.1% and 1.8%, respectively. However, under strong glare or extreme tilt angle conditions, detection accuracy decreased significantly, with MAE increasing by 12.4% and 15.8%, respectively, and corresponding sensitivity decreasing by 5.3% and 7.1%, respectively.</p></sec><sec id="sec4dot3-sensors-25-05378"><title>4.3. Quantitative Concentration Prediction</title><p>Concentration estimation was formulated as a regression task and evaluated using MAE, MSE, and R<sup>2</sup>. As summarized in <xref rid="sensors-25-05378-t001" ref-type="table">Table 1</xref>, the proposed 2SLOD-HCG model achieved an overall MAE of 2.3 IU/L and an R<sup>2</sup> of 0.975 across all test conditions. A scatter plot of predicted versus measured concentrations (<xref rid="sensors-25-05378-f006" ref-type="fig">Figure 6</xref>) further confirmed a strong linear correlation (Pearson r = 0.988, <italic toggle="yes">p</italic> &lt; 0.001).</p><p>To assess diagnostic utility, we examined performance at thresholds of 25, 50, and 100 IU/L. At the 25 IU/L threshold, the model achieved 98.5% sensitivity, 97.9% specificity, 97.6% PPV, and 98.7% NPV, which were comparable to those of trained human interpreters. Similar robustness was observed across other thresholds. <xref rid="sensors-25-05378-t001" ref-type="table">Table 1</xref> shows the performance under varied imaging conditions. The model remained stable under moderate changes (e.g., low light, warm/cool illumination), while conditions such as strong glare and oblique angles resulted in higher errors (MAE up to 10.8 IU/L). Nevertheless, R<sup>2</sup> remained above 0.94, indicating clinically acceptable prediction accuracy. <xref rid="sensors-25-05378-t002" ref-type="table">Table 2</xref> reports detection accuracy and quantitative concentration prediction under different lighting setups. Prediction remained close to the ground truth, with deviations within 2 IU/L across most conditions. Even under challenging glare scenarios, the accuracy exceeded 90%.</p><p>To further validate our method, we compared it with baseline detectors including YOLOv8 (original), MobileViT, and TIMESAVER. As shown in <xref rid="sensors-25-05378-t003" ref-type="table">Table 3</xref>, 2SLOD-HCG consistently outperformed these models in terms of MAE and sensitivity, confirming the effectiveness of the S-5 module and multi-scale attention mechanism in small-object detection for test strips.</p></sec><sec id="sec4dot4-sensors-25-05378"><title>4.4. Ablation Study</title><p>Precision and recall serve as evaluation metrics for comparison analysis in this experiment. The modified lightweight transformer method (LIVT), the improved SPP module (S-5), the SimAM attention mechanism, and the original YOLOv8 algorithm serve as the baseline. <xref rid="sensors-25-05378-t004" ref-type="table">Table 4</xref> displays the outcomes of the experiment.</p><p>As shown in <xref rid="sensors-25-05378-t004" ref-type="table">Table 4</xref>, C-FPN improved multi-scale feature fusion, yielding a +3.0% mAP gain without increasing computational cost. Adding LIVT further enhanced global context modeling and contributed an additional +1.4% improvement. The proposed S-5 module, designed to better capture small-scale contextual features, provided another +2.4% gain. Finally, the SimAM attention mechanism helped the network focus on key regions, leading to the best overall performance (54.5% mAP, +8.9% compared to baseline).</p><p>To validate the effectiveness of our method against existing approaches, we further compared it with lightweight detectors such as MobileViT, EfficientDet-D0, and YOLOv7-tiny. The results (<xref rid="sensors-25-05378-t003" ref-type="table">Table 3</xref>) show that 2SLOD-HCG consistently outperformed these methods, particularly in small-object detection scenarios, confirming the value of the combined architectural improvements.</p></sec><sec id="sec4dot5-sensors-25-05378"><title>4.5. Baseline Evaluation</title><p>To evaluate the effectiveness of the proposed 2SLOD-HCG network, we compared it against well-established algorithms, including MobileViT [<xref rid="B43-sensors-25-05378" ref-type="bibr">43</xref>], TIMESAVER [<xref rid="B44-sensors-25-05378" ref-type="bibr">44</xref>], and YOLOv8. The quantitative results are summarized in <xref rid="sensors-25-05378-t003" ref-type="table">Table 3</xref>.</p><p>As shown, 2SLOD-HCG achieves the highest performance across all evaluation metrics. Specifically, compared with YOLOv8, the proposed model improves precision from 89.0% to 96.1% (+7.1%) and recall from 89.5% to 95.6% (+6.1%). The F1-score also increases from 89.25% to 95.85%, confirming a more balanced detection ability. In addition, the mean absolute error (MAE) is reduced from 28.6 mIU/mL to 15.2 mIU/mL, nearly halving the prediction error, which is particularly important for clinical applications where accurate quantitative estimation of HCG concentration is critical.</p><p>The superiority of 2SLOD-HCG is further confirmed by the PR curves in <xref rid="sensors-25-05378-f006" ref-type="fig">Figure 6</xref>. Unlike the baselines, whose curves deviate from the ideal top-right corner, the 2SLOD-HCG curve is consistently closer to the optimal region, reflecting a favorable trade-off between sensitivity and specificity. This demonstrates that our method not only improves classification accuracy but also enhances robustness under varying conditions.</p><p>In addition, the confusion matrix in <xref rid="sensors-25-05378-f007" ref-type="fig">Figure 7</xref> demonstrates that 2SLOD-HCG yields fewer false negatives than YOLOv8, particularly in test strips with low HCG concentrations. This confirms the model&#8217;s enhanced capability to detect weak chromogenic responses, which is critical for early pregnancy detection. Overall, these results validate the effectiveness of the improvements introduced in 2SLOD-HCG and highlight its advantage in real-world diagnostic applications.</p><p><xref rid="sensors-25-05378-f008" ref-type="fig">Figure 8</xref> displays the test strips&#8217; visual detection findings under various lighting and background circumstances. It is evident that the suggested algorithm is capable of precisely and unambiguously identifying the test strips&#8217; reactive areas. The color development of the T-line is not visible to the unaided eye and is almost undetectable for test strips responding to low HCG concentrations. Nonetheless, the suggested technique is able to detect the T-line correctly and execute exactly.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05378"><title>5. Conclusions</title><p>This study proposes a multi-scale attention YOLO model that enhances the YOLOv8 framework for test strip concentration detection using smartphone images. An SPP module improves the model&#8217;s ability to detect small objects, while the integration of the SimAM attention mechanism and the FPN operator in the neck section strengthens the recognition of global features. By extracting image features from smartphone-captured test strip photos, the model can accurately estimate test strip concentrations. Compared to existing methods, the proposed YOLO model demonstrates higher detection accuracy.</p><p>However, the model has certain limitations. Its adaptability to complex scenarios is limited, and the restricted diversity of the training dataset affects its generalization capability. Overfitting may also occur during inference despite high accuracy in the current evaluation.</p><p>In recent years, multimodal large models have shown significant advancements. Incorporating multimodal data&#8212;such as images, text, sensor readings, and biological signals&#8212;into concentration detection could improve adaptability to diverse and complex scenarios, offering more precise and customized results.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Q.H. and Z.M.; Methodology, Q.H.; Software, Q.H. and N.W.; Validation, S.K., N.W. and J.L.; Formal analysis, Q.H.; Investigation, Q.H. and S.K.; Resources, Q.S. and Z.M.; Data curation, J.Z.; Writing&#8212;original draft, Q.H. and J.Z.; Writing&#8212;review &amp; editing, J.L. and Z.M.; Supervision, Q.S. and Z.M.; Funding acquisition, Z.M. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset used in this study was self-constructed and specifically designed for the methods proposed in the paper. It was generated through training and tailored to the requirements of this research. Due to the lack of participant consent for public data sharing, the dataset cannot be made openly available. However, interested researchers may contact the corresponding author with a formal request stating the intended use. Access to the data may be granted on a case-by-case basis.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05378"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Korevaar</surname><given-names>T.I.M.</given-names></name><name name-style="western"><surname>Steegers</surname><given-names>E.A.P.</given-names></name><name name-style="western"><surname>de Rijke</surname><given-names>Y.B.</given-names></name><name name-style="western"><surname>Schalekamp-Timmermans</surname><given-names>S.</given-names></name><name name-style="western"><surname>Visser</surname><given-names>W.E.</given-names></name><name name-style="western"><surname>Hofman</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jaddoe</surname><given-names>V.W.V.</given-names></name><name name-style="western"><surname>Tiemeier</surname><given-names>H.</given-names></name><name name-style="western"><surname>Visser</surname><given-names>T.J.</given-names></name><name name-style="western"><surname>Medici</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Reference ranges and determinants of total hCG levels during pregnancy: The Generation R study</article-title><source>Eur. J. Epidemiol.</source><year>2015</year><volume>30</volume><fpage>1057</fpage><lpage>1066</lpage><pub-id pub-id-type="doi">10.1007/s10654-015-0039-0</pub-id><pub-id pub-id-type="pmid">25963653</pub-id><pub-id pub-id-type="pmcid">PMC4584104</pub-id></element-citation></ref><ref id="B2-sensors-25-05378"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>P.F.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>D.M.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>L.Y.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>Y.G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Dou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>N.</given-names></name></person-group><article-title>Effects of high progesterone on outcomes of in vitro fertilization-embryo transfer in patients with different ovarian responses</article-title><source>Syst. Biol. Reprod. Med.</source><year>2015</year><volume>61</volume><fpage>161</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.3109/19396368.2015.1033779</pub-id><pub-id pub-id-type="pmid">25915151</pub-id></element-citation></ref><ref id="B3-sensors-25-05378"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ying</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name></person-group><article-title>Research progress on lateral flow immunoassay rapid detection technology based on smartphone image analysis</article-title><source>Anal. Chem.</source><year>2022</year><volume>50</volume><fpage>1</fpage><lpage>11</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B4-sensors-25-05378"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>T.</given-names></name></person-group><article-title>Remote sensing image fusion based on spectral response function and global variance matching</article-title><source>Acta Photonica Sin.</source><year>2020</year><volume>49</volume><fpage>148</fpage><lpage>157</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B5-sensors-25-05378"><label>5.</label><element-citation publication-type="patent"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name></person-group><article-title>A Calibration Method for the Spectral Response Curve of an RGB Camera</article-title><patent>CN107170013B</patent><day>21</day><month>April</month><year>2020</year><comment>(In Chinese)</comment></element-citation></ref><ref id="B6-sensors-25-05378"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name></person-group><article-title>Application of artificial intelligence in pathological images for precision medicine</article-title><source>Mod. Inf. Technol.</source><year>2018</year><volume>2</volume><fpage>170</fpage><lpage>172</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B7-sensors-25-05378"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ongkum</surname><given-names>C.</given-names></name><name name-style="western"><surname>Keawmitr</surname><given-names>K.</given-names></name><name name-style="western"><surname>Boonchieng</surname><given-names>E.</given-names></name></person-group><article-title>Analysis system for urine strip test using image processing technique</article-title><source>Proceedings of the 9th Biomedical Engineering International Conference</source><conf-loc>Laung Prabang, Laos</conf-loc><conf-date>7&#8211;9 December 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2016</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B8-sensors-25-05378"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roda</surname><given-names>A.</given-names></name><name name-style="western"><surname>Michelini</surname><given-names>E.</given-names></name><name name-style="western"><surname>Zangheri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Di Fusco</surname><given-names>M.</given-names></name><name name-style="western"><surname>Calabria</surname><given-names>D.</given-names></name><name name-style="western"><surname>Simoni</surname><given-names>P.</given-names></name></person-group><article-title>Smartphone-based biosensors: A critical review and perspectives</article-title><source>TrAC Trends Anal. Chem.</source><year>2016</year><volume>79</volume><fpage>317</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1016/j.trac.2015.10.019</pub-id></element-citation></ref><ref id="B9-sensors-25-05378"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>Y.-C.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.-J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Paulsen</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>Q.M.</given-names></name><name name-style="western"><surname>Khalid</surname><given-names>Z.M.</given-names></name><name name-style="western"><surname>Bhalli</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Waheed</surname><given-names>U.</given-names></name><name name-style="western"><surname>Simpson</surname><given-names>C.D.</given-names></name><etal/></person-group><article-title>An ultra low-cost smartphone device for in-situ monitoring of acute organophosphorus poisoning for agricultural workers</article-title><source>Sens. Actuators B Chem.</source><year>2018</year><volume>275</volume><fpage>300</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.snb.2018.08.009</pub-id><pub-id pub-id-type="pmid">37576435</pub-id><pub-id pub-id-type="pmcid">PMC10422983</pub-id></element-citation></ref><ref id="B10-sensors-25-05378"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shui</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Smartphone-assisted immunodetection of HIV p24 antigen using reusable, centrifugal microchannel array chip</article-title><source>Talanta</source><year>2019</year><volume>203</volume><fpage>83</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/j.talanta.2019.05.042</pub-id><pub-id pub-id-type="pmid">31202353</pub-id></element-citation></ref><ref id="B11-sensors-25-05378"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Choodum</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kanatharana</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wongniramaikul</surname><given-names>W.</given-names></name><name name-style="western"><surname>Nic Daeid</surname><given-names>N.</given-names></name></person-group><article-title>Using the iPhone as a device for a rapid quantitative analysis of trinitrotoluene in soil</article-title><source>Talanta</source><year>2013</year><volume>115</volume><fpage>143</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.talanta.2013.04.037</pub-id><pub-id pub-id-type="pmid">24054571</pub-id></element-citation></ref><ref id="B12-sensors-25-05378"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sumriddetchkajorn</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chaitavon</surname><given-names>K.</given-names></name><name name-style="western"><surname>Intaravanne</surname><given-names>Y.</given-names></name></person-group><article-title>Mobile-platform based colorimeter for monitoring chlorine concentration in water</article-title><source>Sens. Actuators B Chem.</source><year>2014</year><volume>191</volume><fpage>561</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.1016/j.snb.2013.10.024</pub-id></element-citation></ref><ref id="B13-sensors-25-05378"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeinhom</surname><given-names>M.M.A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Du</surname><given-names>D.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.-J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name></person-group><article-title>Smartphone-based immunosensor coupled with nanoflower signal amplification for rapid detection of Salmonella enteritidis in milk, cheese and water</article-title><source>Sens. Actuators B Chem.</source><year>2018</year><volume>261</volume><fpage>75</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.snb.2017.11.093</pub-id></element-citation></ref><ref id="B14-sensors-25-05378"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>F.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name></person-group><article-title>Smartphone-based visualized microarray detection for multiplexed harmful substances in milk</article-title><source>Biosens. Bioelectron.</source><year>2017</year><volume>87</volume><fpage>874</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1016/j.bios.2016.09.046</pub-id><pub-id pub-id-type="pmid">27662581</pub-id></element-citation></ref><ref id="B15-sensors-25-05378"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>D.</given-names></name></person-group><article-title>Smartphone-based dual-modality imaging system for quantitative detection of color or fluorescent lateral flow immunochromatographic strips</article-title><source>Nanoscale Res. Lett.</source><year>2017</year><volume>12</volume><fpage>291</fpage><pub-id pub-id-type="doi">10.1186/s11671-017-2078-9</pub-id><pub-id pub-id-type="pmid">28438012</pub-id><pub-id pub-id-type="pmcid">PMC5400777</pub-id></element-citation></ref><ref id="B16-sensors-25-05378"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>M.L.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ling</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Household fluorescent lateral flow strip platform for sensitive and quantitative prognosis of heart failure using dual-color upconversion nanoparticles</article-title><source>ACS Nano</source><year>2017</year><volume>11</volume><fpage>6261</fpage><lpage>6270</lpage><pub-id pub-id-type="doi">10.1021/acsnano.7b02466</pub-id><pub-id pub-id-type="pmid">28482150</pub-id></element-citation></ref><ref id="B17-sensors-25-05378"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.M.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>D.W.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roh</surname><given-names>Y.J.</given-names></name><name name-style="western"><surname>Jo</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>H.Y.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>H.-J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.-R.</given-names></name><etal/></person-group><article-title>Biosensor-linked immunosorbent assay for the quantification of methionine oxidation in target proteins</article-title><source>ACS Sens.</source><year>2021</year><volume>7</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1021/acssensors.1c01819</pub-id><pub-id pub-id-type="pmid">34936330</pub-id></element-citation></ref><ref id="B18-sensors-25-05378"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zangheri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cevenini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Anfossi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Baggiani</surname><given-names>C.</given-names></name><name name-style="western"><surname>Simoni</surname><given-names>P.</given-names></name><name name-style="western"><surname>Di Nardo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Roda</surname><given-names>A.</given-names></name></person-group><article-title>A simple and compact smartphone accessory for quantitative chemiluminescence-based lateral flow immunoassay for salivary cortisol detection</article-title><source>Biosens. Bioelectron.</source><year>2015</year><volume>64</volume><fpage>63</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.bios.2014.08.048</pub-id><pub-id pub-id-type="pmid">25194797</pub-id></element-citation></ref><ref id="B19-sensors-25-05378"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>N.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>Smartphone-based fluorescent lateral flow immunoassay platform for highly sensitive point-of-care detection of Zika virus non-structural protein 1. Anal</article-title><source>Chim. Acta</source><year>2019</year><volume>1055</volume><fpage>140</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.aca.2018.12.043</pub-id><pub-id pub-id-type="pmid">30782365</pub-id></element-citation></ref><ref id="B20-sensors-25-05378"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ruppert</surname><given-names>C.</given-names></name><name name-style="western"><surname>Phogat</surname><given-names>N.</given-names></name><name name-style="western"><surname>Laufer</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kohl</surname><given-names>M.</given-names></name><name name-style="western"><surname>Deigner</surname><given-names>H.-P.</given-names></name></person-group><article-title>A smartphone readout system for gold nanoparticle-based lateral flow assays: Application to monitoring of digoxigenin</article-title><source>Microchim. Acta</source><year>2019</year><volume>186</volume><fpage>119</fpage><pub-id pub-id-type="doi">10.1007/s00604-018-3195-6</pub-id><pub-id pub-id-type="pmid">30661134</pub-id><pub-id pub-id-type="pmcid">PMC6339659</pub-id></element-citation></ref><ref id="B21-sensors-25-05378"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Design of an image retrieval system based on Android and content</article-title><source>Inf. Technol.</source><year>2014</year><volume>38</volume><fpage>200</fpage><lpage>203</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B22-sensors-25-05378"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mutholib</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gunawan</surname><given-names>T.S.</given-names></name><name name-style="western"><surname>Chebil</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kartiwi</surname><given-names>M.</given-names></name></person-group><article-title>Development of portable automatic number plate recognition system on Android mobile phone</article-title><source>IOP Conf. Ser. Mater. Sci. Eng.</source><year>2013</year><volume>53</volume><fpage>012066</fpage><pub-id pub-id-type="doi">10.1088/1757-899X/53/1/012066</pub-id></element-citation></ref><ref id="B23-sensors-25-05378"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>Q.</given-names></name></person-group><article-title>Research on an Android-Based Image Recognition Client System</article-title><source>Bachelor&#8217;s Thesis</source><publisher-name>Hainan University</publisher-name><publisher-loc>Haikou, China</publisher-loc><year>2014</year><comment>(In Chinese)</comment></element-citation></ref><ref id="B24-sensors-25-05378"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Research and Development of a Health Diagnosis Program Based on Test Strip Image Feature Recognition</article-title><source>Master&#8217;s Thesis</source><publisher-name>Xidian University</publisher-name><publisher-loc>Xi&#8217;an, China</publisher-loc><year>2020</year><comment>(In Chinese)</comment></element-citation></ref><ref id="B25-sensors-25-05378"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>A wheat disease diagnosis system based on image processing technology and Android phones</article-title><source>J. Anhui Univ. Nat. Sci.</source><year>2016</year><volume>40</volume><fpage>26</fpage><lpage>31</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B26-sensors-25-05378"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Su</surname><given-names>T.</given-names></name></person-group><article-title>Design of a crop disease and pest image recognition APP system based on deep learning</article-title><source>Comput. Appl. Softw.</source><year>2022</year><volume>39</volume><fpage>341</fpage><lpage>345</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B27-sensors-25-05378"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Donahue</surname><given-names>J.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>J.</given-names></name></person-group><article-title>Rich feature hierarchies for accurate object detection and semantic segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Columbus, OH, USA</conf-loc><conf-date>23&#8211;28 June 2014</conf-date><fpage>580</fpage><lpage>587</lpage></element-citation></ref><ref id="B28-sensors-25-05378"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Fast r-cnn</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Santiago, Chile</conf-loc><conf-date>7&#8211;13 December 2015</conf-date><fpage>1440</fpage><lpage>1448</lpage></element-citation></ref><ref id="B29-sensors-25-05378"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gkioxari</surname><given-names>G.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Mask R-CNN</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2961</fpage><lpage>2969</lpage></element-citation></ref><ref id="B30-sensors-25-05378"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified, real-time object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="B31-sensors-25-05378"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><article-title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>7464</fpage><lpage>7475</lpage></element-citation></ref><ref id="B32-sensors-25-05378"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name></person-group><article-title>Real-time object detection based on YOLO-v2 for tiny vehicle object</article-title><source>Procedia Comput. Sci.</source><year>2021</year><volume>183</volume><fpage>61</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2021.02.031</pub-id></element-citation></ref><ref id="B33-sensors-25-05378"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name></person-group><article-title>SSD: Single shot multibox detector</article-title><source>Proceedings of the European Conference on Computer Vision&#8212;ECCV 2016</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#8211;14 October 2016</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2016</year><volume>Volume 14</volume><fpage>21</fpage><lpage>37</lpage></element-citation></ref><ref id="B34-sensors-25-05378"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shelhamer</surname><given-names>E.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B35-sensors-25-05378"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Noh</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Han</surname><given-names>B.</given-names></name></person-group><article-title>Learning deconvolution network for semantic segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>1520</fpage><lpage>1528</lpage></element-citation></ref><ref id="B36-sensors-25-05378"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>SegNet: A deep convolutional encoder-decoder architecture for image segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>2481</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type="pmid">28060704</pub-id></element-citation></ref><ref id="B37-sensors-25-05378"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ju</surname><given-names>R.-Y.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>W.</given-names></name></person-group><article-title>Fracture detection in pediatric wrist trauma X-ray images using YOLOv8 algorithm</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>20077</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-47460-7</pub-id><pub-id pub-id-type="pmid">37973984</pub-id><pub-id pub-id-type="pmcid">PMC10654405</pub-id></element-citation></ref><ref id="B38-sensors-25-05378"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>37</volume><fpage>1904</fpage><lpage>1916</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2389824</pub-id><pub-id pub-id-type="pmid">26353135</pub-id></element-citation></ref><ref id="B39-sensors-25-05378"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>L.</given-names></name></person-group><article-title>Foreign object detection for belt conveyors in coal mines based on improved YOLOv7</article-title><source>Ind. Min. Autom.</source><year>2022</year><volume>48</volume><fpage>26</fpage><lpage>32</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B40-sensors-25-05378"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage></element-citation></ref><ref id="B41-sensors-25-05378"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Woo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>CBAM: Convolutional block attention module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>3</fpage><lpage>19</lpage></element-citation></ref><ref id="B42-sensors-25-05378"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>E.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Transformer in transformer</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>15908</fpage><lpage>15919</lpage></element-citation></ref><ref id="B43-sensors-25-05378"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hashemi</surname><given-names>S.M.H.</given-names></name><name name-style="western"><surname>Safari</surname><given-names>L.</given-names></name><name name-style="western"><surname>Dadashzade Taromi</surname><given-names>A.</given-names></name></person-group><article-title>Realism in action: Anomaly-aware diagnosis of brain tumors from medical images using YOLOv8 and DeiT</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2401.03302</pub-id><pub-id pub-id-type="arxiv">2401.03302</pub-id></element-citation></ref><ref id="B44-sensors-25-05378"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mehta</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rastegari</surname><given-names>M.</given-names></name></person-group><article-title>MobileViT: Light-weight, general-purpose, and mobile-friendly vision transformer</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2110.02178</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05378-f001" orientation="portrait"><label>Figure 1</label><caption><p>Test strip types and their appearance after reaction. The colors of the T-line and C-line after the test paper reacts and develops color. The T-line shows different colors after reacting with different concentrations.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g001.jpg"/></fig><fig position="float" id="sensors-25-05378-f002" orientation="portrait"><label>Figure 2</label><caption><p>2SLOD&#8211;HCG model architecture. This figure shows the backbone network, multi&#8211;scale feature fusion, and adaptive attention modules, with feature flow and module sizes annotated.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g002.jpg"/></fig><fig position="float" id="sensors-25-05378-f003" orientation="portrait"><label>Figure 3</label><caption><p>S-5 module structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g003.jpg"/></fig><fig position="float" id="sensors-25-05378-f004" orientation="portrait"><label>Figure 4</label><caption><p>Schematic diagram of SimAM.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g004.jpg"/></fig><fig position="float" id="sensors-25-05378-f005" orientation="portrait"><label>Figure 5</label><caption><p>The data contained in our dataset, and pictures taken with different lighting and different mobile phones.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g005.jpg"/></fig><fig position="float" id="sensors-25-05378-f006" orientation="portrait"><label>Figure 6</label><caption><p>Model PR curve.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g006.jpg"/></fig><fig position="float" id="sensors-25-05378-f007" orientation="portrait"><label>Figure 7</label><caption><p>Confusion matrix of our model and YOLOV8 meta-model in test strip detection, (<bold>a</bold>) confusion matrix of the improved model; (<bold>b</bold>) original model confusion matrix.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g007.jpg"/></fig><fig position="float" id="sensors-25-05378-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visualization of detection results.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05378-g008.jpg"/></fig><table-wrap position="float" id="sensors-25-05378-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05378-t001_Table 1</object-id><label>Table 1</label><caption><p>Detection performance of 2SLOD-HCG under different imaging conditions, reporting MAE, R<sup>2</sup>, sensitivity, specificity, PPV, and NPV at a 25 IU/L threshold. The baseline (iPhone 14 Pro, standard lighting, 25 cm) is compared with variations in lighting, glare, angle, and distance, showing notable accuracy drops under glare and strong sunlight.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Condition</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAE (IU/L)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity (25 IU/L Threshold)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specificity (25 IU/L Threshold)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">PPV</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NPV</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Baseline (iPhone 14 Pro, standard lighting, front-facing, 25 cm)</td><td align="center" valign="middle" rowspan="1" colspan="1">6.2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.982</td><td align="center" valign="middle" rowspan="1" colspan="1">0.984</td><td align="center" valign="middle" rowspan="1" colspan="1">0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">0.982</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Low light (indoor 100 lux)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">0.976</td><td align="center" valign="middle" rowspan="1" colspan="1">0.964</td><td align="center" valign="middle" rowspan="1" colspan="1">0.970</td><td align="center" valign="middle" rowspan="1" colspan="1">0.973</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Strong direct light (sunlight)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.955</td><td align="center" valign="middle" rowspan="1" colspan="1">0.962</td><td align="center" valign="middle" rowspan="1" colspan="1">0.940</td><td align="center" valign="middle" rowspan="1" colspan="1">0.951</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Warm light (3000 K)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.975</td><td align="center" valign="middle" rowspan="1" colspan="1">0.980</td><td align="center" valign="middle" rowspan="1" colspan="1">0.972</td><td align="center" valign="middle" rowspan="1" colspan="1">0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">0.975</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cool light (6500 K)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.973</td><td align="center" valign="middle" rowspan="1" colspan="1">0.979</td><td align="center" valign="middle" rowspan="1" colspan="1">0.970</td><td align="center" valign="middle" rowspan="1" colspan="1">0.975</td><td align="center" valign="middle" rowspan="1" colspan="1">0.973</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Glare/reflection present</td><td align="center" valign="middle" rowspan="1" colspan="1">10.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.942</td><td align="center" valign="middle" rowspan="1" colspan="1">0.950</td><td align="center" valign="middle" rowspan="1" colspan="1">0.928</td><td align="center" valign="middle" rowspan="1" colspan="1">0.944</td><td align="center" valign="middle" rowspan="1" colspan="1">0.936</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Angle 30&#176;</td><td align="center" valign="middle" rowspan="1" colspan="1">8.4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.963</td><td align="center" valign="middle" rowspan="1" colspan="1">0.970</td><td align="center" valign="middle" rowspan="1" colspan="1">0.960</td><td align="center" valign="middle" rowspan="1" colspan="1">0.968</td><td align="center" valign="middle" rowspan="1" colspan="1">0.962</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Distance 40 cm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.958</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.965</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.952</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.960</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.958</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05378-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05378-t002_Table 2</object-id><label>Table 2</label><caption><p>Detection accuracy and HCG concentration prediction under different lighting conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Lighting Condition</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detection Accuracy (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">HCG Concentration (ng/mL)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Actual Concentration (ng/mL)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Predicted Concentration (ng/mL)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Normal light</td><td align="center" valign="middle" rowspan="1" colspan="1">98.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Low light</td><td align="center" valign="middle" rowspan="1" colspan="1">95.2</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">9.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Strong light</td><td align="center" valign="middle" rowspan="1" colspan="1">93.8</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">19.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Normal light + shadow</td><td align="center" valign="middle" rowspan="1" colspan="1">94.7</td><td align="center" valign="middle" rowspan="1" colspan="1">50</td><td align="center" valign="middle" rowspan="1" colspan="1">50</td><td align="center" valign="middle" rowspan="1" colspan="1">48.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Low light + shadow</td><td align="center" valign="middle" rowspan="1" colspan="1">92.3</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">98.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Strong light + glare</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">197.3</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05378-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05378-t003_Table 3</object-id><label>Table 3</label><caption><p>Precision and recall of different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAE (mIU/mL)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specificity (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">MobileViT</td><td align="center" valign="middle" rowspan="1" colspan="1">84.8</td><td align="center" valign="middle" rowspan="1" colspan="1">86.8</td><td align="center" valign="middle" rowspan="1" colspan="1">85.788</td><td align="center" valign="middle" rowspan="1" colspan="1">50.5</td><td align="center" valign="middle" rowspan="1" colspan="1">35.4</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td><td align="center" valign="middle" rowspan="1" colspan="1">85.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TIMESAVER</td><td align="center" valign="middle" rowspan="1" colspan="1">85.3</td><td align="center" valign="middle" rowspan="1" colspan="1">86.1</td><td align="center" valign="middle" rowspan="1" colspan="1">85.698</td><td align="center" valign="middle" rowspan="1" colspan="1">50.7</td><td align="center" valign="middle" rowspan="1" colspan="1">33.7</td><td align="center" valign="middle" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" rowspan="1" colspan="1">86.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOV8</td><td align="center" valign="middle" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td><td align="center" valign="middle" rowspan="1" colspan="1">89.249</td><td align="center" valign="middle" rowspan="1" colspan="1">53.0</td><td align="center" valign="middle" rowspan="1" colspan="1">28.6</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2SLOD-HCG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.849</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.3</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05378-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05378-t004_Table 4</object-id><label>Table 4</label><caption><p>Test results after fusion improvement.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">45.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8 + C-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">48.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8 + C-FPN + LIVT</td><td align="center" valign="middle" rowspan="1" colspan="1">50.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8 + C-FPN + LIVT + S-5</td><td align="center" valign="middle" rowspan="1" colspan="1">52.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv8 + LIVT + C-FPN + S-5 + SimAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.5</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>