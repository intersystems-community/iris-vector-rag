<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431135</article-id><article-id pub-id-type="pmcid-ver">PMC12431135.1</article-id><article-id pub-id-type="pmcaid">12431135</article-id><article-id pub-id-type="pmcaiid">12431135</article-id><article-id pub-id-type="doi">10.3390/s25175512</article-id><article-id pub-id-type="publisher-id">sensors-25-05512</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Heterogeneous Information Fusion for Robot-Based Automated Monitoring of Bearings in Harsh Environments via Ensemble of Classifiers with Dynamic Weighted Voting</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2067-374X</contrib-id><name name-style="western"><surname>Siami</surname><given-names initials="M">Mohammad</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05512" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4026-708X</contrib-id><name name-style="western"><surname>D&#261;bek</surname><given-names initials="P">Przemys&#322;aw</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05512" ref-type="aff">1</xref><xref rid="c1-sensors-25-05512" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1878-4718</contrib-id><name name-style="western"><surname>Shiri</surname><given-names initials="H">Hamid</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af2-sensors-25-05512" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1297-3590</contrib-id><name name-style="western"><surname>Michalak</surname><given-names initials="A">Anna</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05512" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3163-8678</contrib-id><name name-style="western"><surname>Wodecki</surname><given-names initials="J">Jacek</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05512" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1656-4930</contrib-id><name name-style="western"><surname>Barszcz</surname><given-names initials="T">Tomasz</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af3-sensors-25-05512" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4781-9972</contrib-id><name name-style="western"><surname>Zimroz</surname><given-names initials="R">Rados&#322;aw</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05512" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Cedric Yiu</surname><given-names initials="KF">Ka-Fai</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05512"><label>1</label>Faculty of Geoengineering, Mining and Geology, Wroc&#322;aw University of Science and Technology, Na Grobli 15, 50-421 Wroc&#322;aw, Poland</aff><aff id="af2-sensors-25-05512"><label>2</label>School of Electronics and Computer Science (ECS), University of Southampton, Southampton SO17 1BJ, UK</aff><aff id="af3-sensors-25-05512"><label>3</label>Faculty of Mechanical Engineering and Robotics, AGH University of Krak&#243;w, Al. Mickiewicza 30, 30-059 Krak&#243;w, Poland</aff><author-notes><corresp id="c1-sensors-25-05512"><label>*</label>Correspondence: <email>przemyslaw.dabek@pwr.edu.pl</email></corresp></author-notes><pub-date pub-type="epub"><day>04</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5512</elocation-id><history><date date-type="received"><day>23</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>29</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05512.pdf"/><abstract><p>Modern inspection mobile robots can carry multiple sensors that can provide opportunities to take advantage of the fusion of information obtained from different sensors. In real-world condition monitoring, harsh environmental conditions can significantly affect the sensor&#8217;s accuracy. To address this issue in this paper, we introduced a fusion approach around information gaps to handle the portion of false information that can be captured by the employed sensors. To test our idea, we looked at various types of data, such as sounds, color images, and infrared images taken by a mobile robot inspecting a mining site to check the condition of the belt conveyor idlers. The RGB images are used to classify the rotating idlers as stuck ones (late-stage faults); on the other hand, the acoustic signals are employed to identify early-stage faults. In this work, the cyclostationary analysis approach is employed to process the captured acoustic data to visualize the bearing fault signature in the form of Cyclic Spectral Coherence. Since convolutional neural networks (CNNs) and their transfer learning (TL) forms are popular approaches for performing classification tasks, a comparison study of eight CNN-TL models was conducted to find the best models to classify different fault signatures in captured RGB images and acquired Cyclic Spectral Coherence. Finally, to combine the collected information, we suggest a method called dynamic weighted majority voting, where each model&#8217;s importance is regularly adjusted for each sample based on the surface temperature of the idler taken from IR images. We demonstrate that our method of combining information from multiple classifiers can work better than using just one sensor for monitoring conditions in real-world situations.</p></abstract><kwd-group><kwd>condition monitoring</kwd><kwd>bearing monitoring</kwd><kwd>CNN</kwd><kwd>information fusion</kwd><kwd>dynamic voting</kwd><kwd>transfer learning</kwd></kwd-group><funding-group><award-group><funding-source>European Commission via the Marie Sklodowska Curie program</funding-source><award-id>955681</award-id></award-group><funding-statement>This work was supported by the European Commission via the Marie Sklodowska Curie program through the ETN MOIRA project (GA 955681) (Mohammad Siami and Hamid Shiri).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05512"><title>1. Introduction</title><p>In recent decades, different mobile robotic platforms have been developed to perform inspection tasks in hazardous industries. Human&#8211;robot collaboration in the mining industry is considered a solution to improve human safety and production quality [<xref rid="B1-sensors-25-05512" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05512" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05512" ref-type="bibr">3</xref>]. Monitoring the condition of critical industrial infrastructure is considered a vital task in reducing the possibility of sudden breakdowns in production lines. Therefore, to ensure production safety, the machines involved in the process must be inspected in a timely manner [<xref rid="B4-sensors-25-05512" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05512" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05512" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05512" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05512" ref-type="bibr">8</xref>].</p><p>Bearings are considered an integral part of every rotating element, an important example being the idlers used on belt conveyors (BCs), which on mining sites are the logistic machinery responsible for the transport of the mining production [<xref rid="B9-sensors-25-05512" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05512" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05512" ref-type="bibr">11</xref>]. The average length of a mining BC can be counted in kilometers, with thousands of idlers that need to be monitored to ensure the safety of the production line [<xref rid="B12-sensors-25-05512" ref-type="bibr">12</xref>].</p><p>The mobile robot can carry multiple sensors that can be intelligently used to monitor rotating machines in harsh and difficult-to-reach environments for humans [<xref rid="B13-sensors-25-05512" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05512" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05512" ref-type="bibr">15</xref>]. Non-contact instruments are considered proper tools to assess the machine&#8217;s condition, as they can reduce the complexity of measurement through robot-based inspection tasks. Mobile robots might be capable of carrying various non-contact instruments, such as laser scanners based on light detection and ranging (LiDAR), laser Doppler vibrometers, high-speed cameras, microphones, or IR cameras. However, considering harsh conditions in mining sites, laser Doppler vibrometers and high-speed cameras can be ineffective considering factors such as uneven surfaces (unstable movement of the robot) or low ambient light. However, IR and RGB images, together with the acoustic samples collected by a microphone, could be more robust to environmental noise in a harsh environment.</p><p>Although single-sensor measurements are complementary to the condition monitoring of rotating machines, to accomplish more complex tasks, multi-sensor configurations are becoming increasingly important. The processing of data collected in a multi-sensor configuration is considered an extremely complex task, as the availability of heterogeneous data leads to the need to develop fusion methods that are compatible with the complexity of the monitored equipment [<xref rid="B16-sensors-25-05512" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05512" ref-type="bibr">17</xref>].</p><p>The continuous development of deep learning methods has received attention due to their strong nonlinear feature extraction performance. However, due to the limitation of the extraction of multiple features in single-modal samples, improving deep learning approaches could be limited to the novelty of the information in the extracted samples. Therefore, to improve the performance of models driven by deep learning in fault diagnosis, heterogeneous information fusion approaches have received the interest of researchers.</p><p>In our proposed condition monitoring approach, an inspection mobile robot equipped with three different cameras, including RGB and IR imaging cameras, and a microphone is used to capture heterogeneous sources of data that can be used to identify different fault stages of damaged idlers. We individually analyze the advantages of each data source while mentioning their limitations.</p><p>Information fusion approaches might be separated into two categories, including feature-based fusion and decision-based fusion. The feature-based fusion is more suitable for problems where the fused information (homogeneous data) is captured by sensors of the same type. However, in problems where the dimensions of captured data are different due to sensor properties, the fault information (heterogeneous data) properties cannot be easily recognized due to the different characteristics and distribution of the features studied. Therefore, decision-level fusion methods can be used to make deep learning-based classifiers more accurate in identifying bearing faults.</p><p>Ensemble learning refers to learning approaches that combine several baseline models&#8212;in our case deep learning-based classifiers&#8212;to take advantage of fused information from individual classifiers. It can be used to build a single large model that is more accurate than every individual classifier [<xref rid="B18-sensors-25-05512" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05512" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05512" ref-type="bibr">20</xref>]. In this work, we implemented a dynamic weighted average voting fusion approach to fuse the decisions of the CNN models employed for classification of the pre-processed RGB images and acoustic signals captured from idlers.</p><p>A thermal anomaly on the idler surface should be considered a sign of a serious defect in the idler bearing. To improve the accuracy of the fusion results, we defined the idler surface temperature for each of the measured idlers. The normalized value of the idler temperature in each case with respect to the temperature of other inspected idlers is used to dynamically set the weights in the employed voting method to improve the accuracy of the proposed fusion approach.</p><p>In this paper, we investigate the use of an inspection mobile robot for condition monitoring of BC idlers and present a practical, multi-sensor data-processing pipeline that exploits heterogeneous, robot-acquired inputs&#8212;RGB and infrared (IR) images together with acoustic recordings&#8212;to detect and stage idler faults. The principal technical contribution is a systematic evaluation of TL variants of classical CNN backbones as sample-efficient feature extractors on real, in-field robot data; we demonstrate that TL-based feature extraction, paired with conventional machine learning classifiers, provides robust classification performance under the constraints of limited, imbalanced field datasets. The experimental comparison comprises eight deep models derived from four CNN backbones, and two widely used classifiers&#8212;Random Forest (RF) and XGBoost&#8212;applied to the extracted features. Compared with end-to-end multimodal deep architectures, our approach emphasizes operational practicality and annotation efficiency for inspection robots, while enabling straightforward decision-level fusion of visual, thermal, and acoustic modalities. The general contributions of the paper are summarized below:<list list-type="bullet"><list-item><p>We apply cyclostationary analysis to robot-recorded acoustic signals (e.g., cyclic spectral coherence) to extract fault-specific features that are robust to the non-Gaussian, high-interference noise typical of in-field mining environments, enabling earlier detection of bearing/roller defects.</p></list-item><list-item><p>We propose a dynamic, temperature-aware weighted voting fusion scheme in which per-sample classifier weights are adaptively adjusted using normalized idler surface temperature from IR imagery; this improves sensitivity to thermally driven fault stages.</p></list-item><list-item><p>We demonstrate that transfer learning-based feature extraction combined with classical classifiers (Random Forest, XGBoost) and the proposed fusion strategy yields accurate and stable classification performance on highly imbalanced, small-sample datasets.</p></list-item><list-item><p>We validate the approach on BC datasets acquired in operational mining sites, reporting real-case performance and conducting experiments to quantify the contribution of each modality and the fusion rule.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05512"><title>2. Literature Review</title><p>Information fusion is a fundamental process that involves combining observations or information from multiple different sources to provide a robust, complete, or more precise understanding of an environment or process of interest [<xref rid="B21-sensors-25-05512" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05512" ref-type="bibr">22</xref>]. The goal is to achieve a fuller or more accurate description of reality than can be gained by considering individual sources separately.</p><p>Most current data fusion methods employ probabilistic descriptions of observations and processes, often using Bayes&#8217; Rule to combine information. In mathematical terms, Bayes&#8217; theorem can be described as the probability of a hypothesis conditional on a given body of data, to the &#8220;inverse&#8221; probability of the data conditional on the hypothesis [<xref rid="B23-sensors-25-05512" ref-type="bibr">23</xref>].</p><p>Bayes&#8217; Rule is central to most data fusion methods, enabling inferences about an object or environment (described by a state) given an observation. For multi-sensor inference, Bayes&#8217; Rule requires conditional independence of observations and results in the posterior probability being proportional to the product of the prior probability and individual likelihoods from each information source. The recursive form of Bayes&#8217; Rule is advantageous as it only requires the storage and computation of the posterior density, which summarizes all past information, allowing for sequential updates as new observations arrive [<xref rid="B22-sensors-25-05512" ref-type="bibr">22</xref>].</p><p>Basic probabilistic modeling and fusion techniques include Grid-based models [<xref rid="B24-sensors-25-05512" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05512" ref-type="bibr">25</xref>] and Kalman Filters [<xref rid="B26-sensors-25-05512" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05512" ref-type="bibr">27</xref>]. The Information Filter, a dual of the Kalman filter, is noted for its relative simplicity in the update stage, especially for systems with multiple sensors, as it translates products of likelihoods (from Bayes&#8217; Rule) into sums. This property is exploited in robotic networks and navigation problems [<xref rid="B28-sensors-25-05512" ref-type="bibr">28</xref>].</p><p>Beyond the basic probabilistic approaches, there exists a wide array of different methodologies. The reputation-based approach in generalized and unified form has been discussed in [<xref rid="B29-sensors-25-05512" ref-type="bibr">29</xref>], where the authors used the beta reputation system based on the Bayesian formulation. The approach there is explained from the transactional point of view, where two cooperating nodes (such as sensors) exchange information (data from measurement). Each exchange generates a &#8220;cooperativeness rating&#8221;, which will later be used to determine the probability of future exchanges between the nodes. Similar algorithms have found their use in the recent rise of multi-agent systems relevant in artificial intelligence advancements. Examples are Distributed Reputation Mechanism [<xref rid="B30-sensors-25-05512" ref-type="bibr">30</xref>], Deep Reinforcement Learning-based reputation model with Multi-Agent Deep Deterministic Policy Gradient [<xref rid="B31-sensors-25-05512" ref-type="bibr">31</xref>] or information search applications [<xref rid="B32-sensors-25-05512" ref-type="bibr">32</xref>].</p><p>Metric-based fusion operates on the concept of similarity (through metrics such as distance or correlation) between observations from different sensors for proper data integration. The difference in value of chosen metric allows one to define the trust in individual sensors (reduced upon large value disagreement). This approach is especially effective for data measured in common feature space, such as fusion of LiDAR and camera sensors [<xref rid="B33-sensors-25-05512" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05512" ref-type="bibr">34</xref>] or multiple sensors of the same type [<xref rid="B35-sensors-25-05512" ref-type="bibr">35</xref>].</p><p>The authors of [<xref rid="B36-sensors-25-05512" ref-type="bibr">36</xref>] discussed applications of the Dempster&#8211;Shaffer evidence theory in multi-source data fusion. This method and its further generalizations (e.g., Dezert&#8211;Smarandache Theory) in contrast to most other approaches work under uncertainty and can deal with conflicting information. Techniques such as fuzzy logic-enhanced Kalman filters are another option to deal with this problem, noticeably used in robot localization and positioning problems [<xref rid="B37-sensors-25-05512" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05512" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05512" ref-type="bibr">39</xref>].</p><p>The voting approach as proposed in this article has already been successfully used in sensor fusion applications. Examples can be an axlebox bearing fault diagnosis, where fusion is used to merge multichannel data information into the final result [<xref rid="B40-sensors-25-05512" ref-type="bibr">40</xref>] or in the fusion of SAR images with optical sensor data [<xref rid="B41-sensors-25-05512" ref-type="bibr">41</xref>]. The voting methodologies are still being improved, such as in [<xref rid="B42-sensors-25-05512" ref-type="bibr">42</xref>], where the authors implemented universal generating function, or in [<xref rid="B43-sensors-25-05512" ref-type="bibr">43</xref>], where the authors proposed a dual weighted voting algorithm for K-nearest neighbor classification.</p><p>Recent developments in sensor technologies have enabled researchers to propose new fusion-based condition monitoring methodologies to identify faults in rotating machines with higher precision [<xref rid="B44-sensors-25-05512" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05512" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05512" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05512" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-05512" ref-type="bibr">48</xref>]. While traditional vibration-based analysis remains prevalent, different sensors, including acoustic, infrared (IR), RGB cameras, and current, offer complementary insights into equipment health. However, the fusion of multimodal data presents challenges due to differences in sampling rates, signal resolutions, and environmental susceptibility. For example, acoustic emission (AE) sensors excel in capturing high-frequency stress waves generated by incipient faults such as micro-cracks or lubrication failures [<xref rid="B1-sensors-25-05512" ref-type="bibr">1</xref>]. However, their efficacy diminishes in noisy environments such as mining sites, where mechanical collisions could corrupt signal integrity. In contrast, non-contact IR thermography and RGB imaging provide robust visual indicators of overheating or surface defects in harsh settings but lack sensitivity to early-stage faults, as they primarily detect thermally or visually manifested anomalies [<xref rid="B49-sensors-25-05512" ref-type="bibr">49</xref>].</p><p>To address these challenges, deep learning architectures&#8212;particularly convolutional neural networks (CNNs)&#8212;have emerged as powerful tools for fusing heterogeneous data streams. G&#252;ltekin et al. [<xref rid="B45-sensors-25-05512" ref-type="bibr">45</xref>] pioneered a deep residual network (DRN)-based fusion framework to diagnose bearing faults under variable load and speed conditions. Their method converts raw vibration and current signals from six synchronized sensors into time&#8211;frequency representations via the short-time Fourier transform (STFT), enabling the DRN to learn cross-sensor spectral patterns. Similarly, Kou et al. [<xref rid="B46-sensors-25-05512" ref-type="bibr">46</xref>] fused vibration, motor current, and IR images for CNC machine tool wear monitoring. They employed Gramian angular difference fields (GADFs) [<xref rid="B47-sensors-25-05512" ref-type="bibr">47</xref>] to encode 1D time-series data into 2D texture images, preserving temporal correlations. A hybrid CNN processed these alongside the IR images.</p><p>Despite progress, critical gaps persist. First, most studies evaluate fusion models under controlled laboratory conditions, neglecting real-world constraints such as sensor misalignment, intermittent data loss, and variable sampling rates. Second, considering the fact that CNN models need to have access to be successfully trained makes them challenging models to choose, as, in industrial settings, it could be rather expensive and, in some case studies, impossible to acquire enough samples for training the models. Although techniques like transfer learning [<xref rid="B48-sensors-25-05512" ref-type="bibr">48</xref>] and synthetic data generation are proposed as remedies, their efficacy in multi-sensor fusion contexts lacks rigorous validation. Lastly, as long as different variations of CNN architectures have been rapidly developed in the past decade, their performance evaluation in different sources of data is necessary, which is merely studied at present. In this direction, the practicality of the deep learning approach needs to be measured when it comes to training the models on a limited number of samples, as a known major drawback of a CNN model is the requirement for a large amount of training data.</p></sec><sec id="sec3-sensors-25-05512"><title>3. Material and Methods</title><p>In this work, we propose a robotics-based approach for the automation of belt conveyor idler monitoring at mining sites. The mobile inspection robot in our work has collected different sources of data, including acoustic signals, IR, and RGB images from BC idlers in real-world scenarios. The simplified flow diagram of the proposed data fusion approach is shown in (see <xref rid="sensors-25-05512-f001" ref-type="fig">Figure 1</xref>).</p><p>The proposed methodology consists of five stages, including the acquisition and pre-processing stage, where data acquired by the inspection mobile robot are first stored and undergo various pre-processing methods to increase the chance of identifying the fault pattern. In the third phase, we used CNN architectures as feature extractors. Moreover, we study the application of two different feature classification methods, namely, RF and XGBoost. In the fourth phase, a dynamic weighted voting ensemble-based approach was considered to fuse the classifiers&#8217; decisions to make a final prediction. The main idea of this voting approach is that the chance of facing classification errors in individual classifiers can be reduced by merging particular decisions through a dynamic weighted average voting scheme. Finally, we demonstrate the overall performance of the proposed data fusion approach in comparison to single-sensor measurement methods.</p><sec id="sec3dot1-sensors-25-05512"><title>3.1. Cyclic Spectral Coherence</title><p>In the analysis of rotating machinery, the identification of modulation frequencies is essential for different carrier frequencies. To address this, the cyclic spectral analysis is introduced. Antoni [<xref rid="B50-sensors-25-05512" ref-type="bibr">50</xref>] introduced cyclic spectral coherence (CSC) to quantify this phenomenon. Let us begin by recalling the cyclic power spectrum (CPS) <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> of the signal <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD1-sensors-25-05512"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8594;</mml:mo><mml:mo>&#8734;</mml:mo></mml:mrow></mml:munder><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac></mml:mstyle><mml:mi mathvariant="double-struck">E</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mover><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>f</mml:mi><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the Fourier transform of the signal <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:math></inline-formula> calculated over an interval of length <italic toggle="yes">L</italic>; <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the modulating frequency; and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the carrier frequency. According to Equation (<xref rid="FD1-sensors-25-05512" ref-type="disp-formula">1</xref>), CPS measures the dependence of the spectral components spaced by a given modulation frequency <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> for a given carrier frequency <italic toggle="yes">f</italic>. The cyclostationary signal should show <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>S</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for some modulation frequency <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8800;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Based on the CPS definition, the formula for SC is introduced as follows [<xref rid="B50-sensors-25-05512" ref-type="bibr">50</xref>]:<disp-formula id="FD2-sensors-25-05512"><label>(2)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>S</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:msub><mml:mi>S</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>f</mml:mi><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This normalized statistic, within the interval <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, quantifies the spectral cyclic autocorrelation of the signal. It serves as an indicator of cyclostationarity. A value close to one implies a cyclostationarity property of the signal at the carrier frequency (<italic toggle="yes">f</italic>) with a modulation period of <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The estimation of SC, as per Equation (<xref rid="FD2-sensors-25-05512" ref-type="disp-formula">2</xref>), can be performed directly using the CPS estimator. Specifically, the estimator of CSC is given by<disp-formula id="FD3-sensors-25-05512"><label>(3)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mover accent="true"><mml:mi>&#947;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>X</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>X</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>f</mml:mi><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is an estimator of the CPS, with various methods presented and compared in [<xref rid="B50-sensors-25-05512" ref-type="bibr">50</xref>]. In this article, the Welch method is applied.</p><p>In this study, we utilized acoustical analysis to investigate the operational condition of idlers, integral components of belt conveyor systems utilized for the transportation of bulk materials in the mining industry. The acoustic signals emanating from these idlers were captured using a mobile robot, resulting in a dataset composed of recordings from 17 distinct idlers. The acquired signals exhibit a temporal extent of 6 s, sampled at a frequency of 48 kHz.</p><p>Illustrated in <xref rid="sensors-25-05512-f002" ref-type="fig">Figure 2</xref> are representative examples that feature a healthy idler, a faulty idler, and a scenario involving the influence of a conveyor belt joint. The panels <xref rid="sensors-25-05512-f002" ref-type="fig">Figure 2</xref>a and <xref rid="sensors-25-05512-f002" ref-type="fig">Figure 2</xref>b present the raw signal and the corresponding CSC map of the pristine idlers, respectively, showcasing the baseline acoustical profile. In contrast, panels <xref rid="sensors-25-05512-f002" ref-type="fig">Figure 2</xref>c,d depict the raw signal and the Cyclic Spectral Coherence of faulty idlers, thereby highlighting deviations from the norm. Furthermore, in <xref rid="sensors-25-05512-f003" ref-type="fig">Figure 3</xref>, the panels <xref rid="sensors-25-05512-f003" ref-type="fig">Figure 3</xref>a,b offer information on the raw signal and the Cyclic Spectral Coherence associated with the sound emanating from the conveyor belt joint during signal acquisition (see <xref rid="sensors-25-05512-f004" ref-type="fig">Figure 4</xref>). This is an important example to show that although other noise sources can show cyclic behavior, it will have a different characteristic.</p><p>This comprehensive analysis allows for a nuanced understanding of acoustic characteristics, facilitating the identification and differentiation of healthy and defective idlers, as well as discerning the impact of conveyor belt joints on the audio profile.</p></sec><sec id="sec3dot2-sensors-25-05512"><title>3.2. RGB Image Pre-Processing</title><p>During the examination, the inspection mobile captured continuous RGB videos of the wing idlers that were located at the top of the BC. The healthy idler must rotate continuously to move the belt along the conveyor. In some severe cases, the bearings in the idlers can be damaged, resulting in a sudden stop in idler rotation. The failed idlers can be recognized in the IR camera due to heat generated due to friction between the idler and the belt. However, because of the absence of rotation, the fault cannot be recognized in the captured acoustic signal; therefore, it is essential to analyze the health status of the idlers using the RGB images.</p><p>Initially, an input video is divided into separate frames <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mi>f</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> where each frame must be processed and classified individually. The original size of the frames extracted from the raw video file were (720 &#215; 720 &#215; 3) pixels, which is too large to be processed using the CNN models employed. Therefore, the size of the frame sequence is reduced to (256 &#215; 256 &#215; 3) pixels for training and testing the CNN models. In <xref rid="sensors-25-05512-f005" ref-type="fig">Figure 5</xref>, we demonstrate the two idlers to compare the difference between the rotating idler in pre-processed RGB frames.</p></sec><sec id="sec3dot3-sensors-25-05512"><title>3.3. IR Image Processing</title><p>In <xref rid="sensors-25-05512-f006" ref-type="fig">Figure 6</xref>, we show the evolutionary signs of the faults on conveyor belt idlers due to damage over time. It can be seen that there is a continuous relationship between the fault signature in different stages of development. Moreover, one can notice that temperature changes on idler bearings can be detected when the idler condition is close to failure; therefore, anomalies in the idler surface should be considered as an important measure to identify faulty idlers.</p><p>In previous sections, we discussed the advantages of RGB images and acoustic signals in the diagnosis of idler bearings. Both measures are important tools for identifying the fault at its early and late stages; however, the IR image as an efficient tool can give us additional robust information in a time frame that the supervisor would have enough time to replace the faulty idler.</p><p>In this study, to improve the overall performance of the proposed information fusion scheme, we extracted the idler surface temperature from the examined idler using the IR camera and introduced the normalized value as a weight in the dynamic weighted voting approach.</p><p>In <xref rid="sensors-25-05512-f007" ref-type="fig">Figure 7</xref>, we demonstrate the IR image captured from the idler represented in <xref rid="sensors-25-05512-f005" ref-type="fig">Figure 5</xref>. As can be seen, friction between the stuck idler and the moving belt generates huge heat that could be captured by the IR camera carried by the inspection robot.</p></sec><sec id="sec3dot4-sensors-25-05512"><title>3.4. Data Description and Augmentation</title><p>Through this research, we studied the different sources of data, including acoustic signals and RGB and IR images captured from 17 different idlers using a mobile robot. After initial data pre-processing, we noticed that only 4 out of 17 monitored idlers were faulty. Two acoustic samples indicate early-stage faults (idler numbers 12 and 13); however, we did not capture temperature anomalies on the surfaces of diagnosed idlers with early-stage faults. The reason was that since the faults did not fully develop in the idler bearings, there was no sign of a thermal anomaly on the idler surfaces. On the other hand, we notice two stuck idlers (final stage faults) using the captured RGB image with signs of thermal anomalies (idlers numbers 15 and 17).</p><p>These numbers can indicate that our original dataset suffers from the class imbalance problem, which can significantly affect the performance of the semantic segmentation model in the correct detection of overheated idlers. In this way, training deep learning classifier models can become a crucial issue [<xref rid="B51-sensors-25-05512" ref-type="bibr">51</xref>].</p><p>Oversampling and undersampling are the most common techniques for addressing model overfitting and class imbalance issues. Data augmentation can be considered an oversampling method to amplify minority classes [<xref rid="B52-sensors-25-05512" ref-type="bibr">52</xref>,<xref rid="B53-sensors-25-05512" ref-type="bibr">53</xref>]. In this direction, we employed a different approach for oversampling the pre-processed RGB images and acoustic signals.</p><sec id="sec3dot4dot1-sensors-25-05512"><title>3.4.1. RGB Image Augmentation</title><p>In our case study, the inspection mobile robot passed along with each idler and collected the RGB image samples from different angles of 17 different idlers. To create a balanced dataset, we increased the number of positive samples (stuck idlers). We selected 15 samples captured from different angles of faulty idlers while selecting 2 samples from healthy idlers. In doing so, we created a base dataset with 60 samples that includes an equal number of positive and negative samples.</p><p>In addition, we employed different image augmentation techniques to increase the number of samples to train the classifiers in the next step. The image augmentation techniques can be divided into three different categories: geometric and color space transformations and pixel point operations. In this work, different data augmentation techniques, namely vertical flip, random rotation at 90 degrees, horizontal flip, and transpositions, have been applied to RGB image datasets. It is worth mentioning that image augmentation techniques have been applied to datasets using the Albumentations package [<xref rid="B54-sensors-25-05512" ref-type="bibr">54</xref>]. After augmentation, we created a dataset that included 240 samples with an equal number of positive and negative samples to train and test the classifiers.</p></sec><sec id="sec3dot4dot2-sensors-25-05512"><title>3.4.2. CSC Map Augmentation</title><p>To increase the set of CSC maps obtained from positive (faulty) samples, a two-step augmentation was proposed:<list list-type="bullet"><list-item><p>Step 1: In this step, the speckle noise is added to the CSC map, which is usually modeled as the multiplicative noise (Rayleigh noise). The degraded data point in a CSC map (denoted as <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) can be defined as follows:<disp-formula id="FD4-sensors-25-05512"><label>(4)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the original CSC map in point <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the multiplicative Gaussian noise with mean equal to 1 and standard deviation equal to 0.05.</p></list-item><list-item><p>Step 2: The noisy map is then convolved with a 2D Gaussian kernel that is used to blur images. A 2D Gaussian smoothing kernel is applied to the samples <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using the following equation:<disp-formula id="FD5-sensors-25-05512"><label>(5)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> is sampled uniformly from the interval <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for every augmented instance. This controlled blurring reduces artifacts while introducing additional intra-class variability.</p></list-item></list></p><p>The sequential application of added noise, followed by scale-randomized smoothing, produces realistic yet distinct CSC realizations, enhancing class balance and improving model generalization in downstream fault detection tasks.</p><p>Spectral correlation maps were assigned to three categories: faulty, belt joint (external disturbance), and healthy. The original corpus comprised 17 maps in total (2 faulty, 4 external-disturbance, and 11 healthy samples). To reduce the pronounced class imbalance, samples in the underrepresented classes (faulty and belt joint) were synthetically expanded using the augmentation procedures described previously. After augmentation, the dataset was expanded to 35 examples and adjusted to achieve near-balanced representation across the three classes. This augmented, more balanced set was then used for training and evaluation to mitigate bias towards the majority class and to improve the classifiers&#8217; ability to learn discriminative spectral features for rare fault conditions.</p></sec></sec><sec id="sec3dot5-sensors-25-05512"><title>3.5. Feature Extraction</title><p>A CNN is a type of neural network that is designed based on the cognitive mechanism of the biological visual system [<xref rid="B55-sensors-25-05512" ref-type="bibr">55</xref>]. CNN-based approaches are widely regarded as the most popular methods in the field of graphics processing due to their strong performance in image processing and the ability to directly handle raw images. CNN uses convolution filters, pooling, and other operations to extract image features. The model is trained using gradient descent and back propagation algorithms to perform tasks, including image classification [<xref rid="B56-sensors-25-05512" ref-type="bibr">56</xref>]. CNN architecture typically consists of five layers: the input layer, the convolution layer, the activation layer, the pooling layer, and the fully connected layer. In the following, we briefly explain each layer:<list list-type="bullet"><list-item><p><bold>Input layer</bold></p><p>This is the access point for unprocessed image data. Within this layer, images can undergo pre-processing through various operations, such as normalization, principal component analysis, and whitening. Pre-processing standardizes images, which could accelerate the training of network models and consequently enhance model performance.</p></list-item><list-item><p><bold>Convolution Layer</bold></p><p>The primary layer of a CNN is responsible for performing convolutions on the input images in order to extract relevant visual features. In general, a convolution layer consists of multiple convolution kernels, which act as filters to extract various features of the image.</p></list-item><list-item><p><bold>Activation Layer</bold></p><p>The purpose of this layer is to apply a nonlinear mapping to the convolution results, allowing the multilayer network to exhibit nonlinearity and enhance its expressive capacity. The rectified linear unit (ReLU) function and the Sigmoid function are frequently employed as activation functions.</p></list-item><list-item><p><bold>Pooling Layer</bold></p><p>This layer is commonly referred to as the down-sampling layer. Its purpose is to reduce the dimensionality of the extracted features and compress the data. This helps to mitigate overfitting and enhance the model&#8217;s fault tolerance. Pooling techniques encompass MaxPooling and AveragePooling, with MaxPooling widely used at present.</p></list-item><list-item><p><bold>Fully Connected Layer</bold></p><p>This layer serves as the output layer and is responsible for achieving the function of classifying objects. The function of this layer is to consolidate the feature information obtained from each individual neuron in the layer above and subsequently categorize the images according to the desired outcome.</p></list-item></list></p><sec><title>Classical CNN Architectures and TL</title><p>Some of the CNN architectures proposed in the past decade have received considerable attention due to their exceptional performance in performing different image processing tasks. The advantages of CNN in image recognition are revealed annually in the ImageNet Large-Scale Visual Recognition competition [<xref rid="B57-sensors-25-05512" ref-type="bibr">57</xref>]. In this work, we select the four classical CNN architectures to perform feature extraction on pre-processed RGB images and acoustic samples.</p><p>TL enhances the performance of CNN by leveraging pre-existing knowledge from a source domain into a target domain. This enables the CNN model to increase its pattern recognition capabilities or handle new tasks in limited labeled data. Empirical evidence has demonstrated that CNN-TL variants exhibit a commendable generalization. Furthermore, compared to prototype-based approaches, CNN-TL models showcase a more robust ability to extract patterns beyond the scope of the training data.</p><p>In this paper, the used model was initially trained on the ImageNet dataset. The ImageNet dataset contains more than 13 million pictures from 20,000 categories, allowing the network to be deeply trained on a different range of images [<xref rid="B58-sensors-25-05512" ref-type="bibr">58</xref>]. The training weights were used to perform feature extraction on our dataset. In the following, we briefly describe the key advantages of each employed model:<list list-type="bullet"><list-item><p><bold>VGGNet16</bold></p><p>In VGG16 architecture, there are 13 convolutional layers, five Max Pooling layers, and three dense layers. It was first introduced by Oxford University and Google DeepMind first introduced it in 2014 to improve the AlexNet architecture by replacing large filters with sequences of smaller 3 &#215; 3 filters [<xref rid="B59-sensors-25-05512" ref-type="bibr">59</xref>].</p></list-item><list-item><p><bold>ResNet 50</bold></p><p>The Residual Network (ResNet) architectures take advantage of the concept of skip connections, which allow the network to learn deeper representations without overfitting. There are multiple versions of the ResNet architecture with different numbers of layers. In this article, we use ResNet50, first introduced in 2015 [<xref rid="B60-sensors-25-05512" ref-type="bibr">60</xref>]. It consists of 48 convolutional layers, one MaxPool layer, and one average pool layer.</p></list-item><list-item><p><bold>InceptionV3</bold></p><p>The Inception architecture was first developed by Szegedy et al., who changed the straight-up and straight-down serial network to the parallel sparse connection network [<xref rid="B61-sensors-25-05512" ref-type="bibr">61</xref>]. Furthermore, the researchers used the global average pooling layer to replace the fully connected layer. In this paper, we employed the inceptionv3 variation to perform feature extraction tasks. Inceptionv3 was designed to allow the use of deeper networks, while also controlling the growth of parameters.</p></list-item><list-item><p><bold>Xception</bold></p><p>As an improvement over Inception, the Xception was first introduced in 2017 and uses the depth-wise separable convolution layer to improve the convolution layer within InceptionV3 [<xref rid="B62-sensors-25-05512" ref-type="bibr">62</xref>]. Xception is a CNN that is 71 layers deep, has fewer parameters and, therefore, is faster than Inception.</p></list-item></list></p></sec></sec><sec id="sec3dot6-sensors-25-05512"><title>3.6. Feature Classification</title><p>Generally, at the latest stage of a CNN architecture, a fully connected layer is responsible for performing the feature classification task. In our work, we switched out the fully connected layers for XGBoost and RF models. The proposed approach based on CNN fusion with RF and XGBboost has been studied to see how the extracted features in the convolutional layer can be used to classify input samples&#8212;in our case, pre-processed RGB images&#8212;and spectral coherence maps into the desired classes. To perform this, the classifier models have been separately trained using the features extracted from the training dataset. Subsequently, the test dataset was used to measure the final performance of the proposed approach in performing a true classification.</p></sec><sec id="sec3dot7-sensors-25-05512"><title>3.7. Random Forest</title><p>The Random Forest algorithm is a type of supervised learning. Creates a &#8220;forest&#8221; by combining many decision trees that are trained using the &#8220;bagging&#8221; method. The fundamental principle of the bagging approach is that the aggregation of multiple learning models enhances the final result. Furthermore, it can be implemented by creating multiple decision trees during training, and the output can be obtained by averaging the predictions of each unique tree [<xref rid="B63-sensors-25-05512" ref-type="bibr">63</xref>].</p></sec><sec id="sec3dot8-sensors-25-05512"><title>3.8. XGboost</title><p>XGBoost utilizes randomization approaches, such as random subsamples and column subsampling, to minimize training time and reduce the risk of overfitting. By employing a compressed, presorted columnar data storage system, the computational expense of finding the best split can be reduced. Utilizing a columnar storage structure allows for concurrent examination of the most efficient partitioning of each attribute being assessed. XGBoost employs an approach that does not involve scanning all possible candidate splits. Instead, it uses data percentiles to evaluate a reduced subset of probable splits and determine their benefit utilizing aggregated statistics. Thus, the implementation of this notion has been achieved through the process of subsampling data at the node level [<xref rid="B64-sensors-25-05512" ref-type="bibr">64</xref>].</p></sec><sec id="sec3dot9-sensors-25-05512"><title>3.9. Ensemble Learning</title><p>Classification models play the main role in identifying the fault pattern in the acquired data, as we previously discussed. The accuracy of the CNN models is highly dependent on the dimension and type of dataset. As a single source of data cannot be used to identify faults in their different stages, fusion algorithms have been employed in this article to improve the overall performance of the introduced bearing diagnosis framework.</p><p>The ensemble model can be generated by combining the base models to develop a robust one. The ensemble model can employ CNN models with different architectures to solve a classification problem that cannot be easily addressed by either of the individual models. In this work, we used an ensemble learning approach using dynamic weighted voting to fuse the decisions of the employed models that were separately trained and tested on pre-processed coherence maps and RGB images.</p><p>In the ensemble learning approach, we consider the output of each classifier as input to the fusion models. Generally, ensemble frameworks can be defined based on two characteristics. The first characteristic can be defined as the trained baseline models, whether they are sequential or parallel [<xref rid="B65-sensors-25-05512" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-05512" ref-type="bibr">66</xref>]. The second can be defined as the fusion method, which is the selected approach to combine the output of the baseline classifiers using different voting approaches.</p><sec><title>Dynamic Weighted Voting Method</title><p>Ensemble learning systems generally rely on an aggregation function <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that combines the outputs of <italic toggle="yes">h</italic> base classifiers <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to predict a single output. The dataset can be defined as<disp-formula id="FD6-sensors-25-05512"><label>(6)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mn>1</mml:mn><mml:mo>&#10877;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#10877;</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a feature vector of dimension <italic toggle="yes">m</italic> which represents the sample after pre-processing and feature extraction, <italic toggle="yes">n</italic> is the size of the dataset, and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the corresponding label. The prediction of the output based on this ensemble method can be defined as [<xref rid="B67-sensors-25-05512" ref-type="bibr">67</xref>]:<disp-formula id="FD7-sensors-25-05512"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#981;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this paper, we utilize the concept of the parallel ensemble technique [<xref rid="B65-sensors-25-05512" ref-type="bibr">65</xref>], where decisions are generated simultaneously, as there is no data dependency. Therefore, each classifier was trained using a different source of data, including a pre-processed RGB image and an acoustic signal. The main reason for this is that it leverages the independence between the base learners. Therefore, the errors generated by one classifier differ from those found in another independent classifier, allowing the ensemble model to calculate the average errors [<xref rid="B68-sensors-25-05512" ref-type="bibr">68</xref>].</p><p>To integrate the outputs of the baseline classifiers into a single output, we employed a dynamic weighted average voting approach. The voting method can be used in classification problems to improve predictive performance. The idea of averaging voting is that the predictions are extracted from multiple different classifiers, and an average of the predictions is used to make the final prediction. The main limitation of average voting is that it is assumed that all baseline models are equally effective. The average prediction can be computed using arithmetic mean, which is the sum of the predictions divided by the total predictions, as describe below:<disp-formula id="FD8-sensors-25-05512"><label>(8)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">argmax</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>h</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>h</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> class label of the <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> classifier [<xref rid="B67-sensors-25-05512" ref-type="bibr">67</xref>].</p><p>The weighted average voting method is a slightly modified version of averaging voting where different weights are given to the baseline classifiers to indicate the degree of importance of each model in prediction. There are two main methods of weighting that can be identified: dynamic weighting and static weighting of classifiers [<xref rid="B69-sensors-25-05512" ref-type="bibr">69</xref>]. During the operational phase, the dynamic method allows the weights assigned to individual classifiers to vary for each input vector. In the static technique, weights are calculated for each classifier during the training phase and remain constant during the classification of input patterns.</p><p>The dynamic weighted average voting method is better in terms of accuracy compared to the simple average voting method. The difficulty in employing a weighted average ensemble lies in determining the correct weight for each classifier. Furthermore, the computation involved in this method is more complex due to the need to calculate the weighted average of the prediction results from all baseline models.</p><p>As shown in <xref rid="sensors-25-05512-f006" ref-type="fig">Figure 6</xref>, this study employs three complementary sensors: a microphone to capture audio signals, a thermal camera to measure idler surface temperature, and an RGB camera to monitor idler rotation and visible damage. The RGB camera is used primarily to detect gross mechanical failures (e.g., severely damaged or seized idlers) by identifying the absence of rotation or other clear visual defects.</p><p>To produce a balanced decision-making scheme that takes advantage of information from all sensors, we propose a dynamic weighting function based on normalized idler surface temperature. This approach allows the system to recognize cases in which severe damage produces a non-rotating idler, and therefore, no anomaly appears in the captured audio. Because the audio signal classifier is most effective in detecting early-stage faults in rotating idlers, it is assigned a constant weight of 1 so its importance remains unchanged across samples. The classifier responsible for detecting late-stage faults from RGB images receives a temperature-dependent weight: when thermal evidence indicates an elevated risk, the RGB contribution is increased, improving detection of severe damage, particularly in cases where distinguishing rotating from stalled idlers from visual data alone is challenging (see <xref rid="sensors-25-05512-f008" ref-type="fig">Figure 8</xref>).</p><p>For a dataset of size <italic toggle="yes">n</italic>, each input sample has an associated temperature value. In this direction, the value of <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which is the weight of the <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> classifier, is defined based on the normalized value of the idler temperature studied in the input sample. Normalization ensures that the weight falls within the range <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and is calculated as<disp-formula id="FD9-sensors-25-05512"><label>(9)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the temperature of the idler in the <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> input sample, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the minimum and maximum observed idler temperatures in the dataset, respectively.</p><p>The RGB-based classifier produces a continuous score in the interval [0, 1], where values closer to 1 indicate a higher probability that the idler is experiencing a faulty rotation (i.e., mechanically compromised or seized). Similarly, the dynamic temperature coefficient is normalized to the same range, with 1 corresponding to the hottest idler in the dataset. Because stalled or severely damaged idlers tend to produce elevated frictional heating, their temperature coefficients and the RGB classifier scores are both biased toward higher values.</p><p>Leveraging the temperature coefficient as a multiplicative or gating factor therefore increases the likelihood of correctly identifying stuck idlers: When thermal evidence is strong (a coefficient closer to 1), the contribution of a high RGB score is amplified, improving true positive detection of severe, non-rotating faults. Conversely, when the temperature coefficient is low, the influence of ambiguous visual evidence is reduced, which helps avoid false positives from visual artifacts. This joint interpretation of the normalized classifier score and the temperature coefficient thus improves detection reliability by aligning the fusion decision with physically meaningful thermal evidence.</p><p>Given this temperature-derived weight, the fusion of the two baseline classifiers (i.e., <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) is achieved using a weighted function, denoted by <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, defined as<disp-formula id="FD10-sensors-25-05512"><label>(10)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the weight applied to the RGB classifier&#8217;s output based on the temperature corresponding to the <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> input sample, and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> refer to the CSC map and RGB image classifiers, respectively.</p></sec></sec></sec><sec id="sec4-sensors-25-05512"><title>4. Performance Metrics</title><p>For the evaluation of the proposed classifier, we calculated the following performance metrics: sensitivity, precision, accuracy, and the F1 score.<disp-formula id="FD11-sensors-25-05512"><label>(11)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="4.pt"/><mml:mi>Accuracy</mml:mi><mml:mspace width="4.pt"/><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05512"><label>(12)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="4.pt"/><mml:mi>Sensitivity</mml:mi><mml:mspace width="4.pt"/><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05512"><label>(13)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="4.pt"/><mml:mi>Precision</mml:mi><mml:mspace width="4.pt"/><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD14-sensors-25-05512"><label>(14)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="4.pt"/><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mspace width="4.pt"/><mml:mi>Score</mml:mi><mml:mspace width="4.pt"/><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, precision is the proportion of correctly classified overheated idlers among the entire population. Sensitivity is measured as the proportion of true positive cases that are correctly predicted by the classifier, while specificity is the prediction of true negative cases that are correctly predicted. Precision is the proportion of correct predictions in the confusion matrix of all positive predictions. Furthermore, the F1 score is the harmonic mean of precision and sensitivity. The coefficient takes into account the factors TP (true positive), TN (true negative), FP (false positive), and FN (false negative) to score the model. The ideal value of these metrics is 1 and it is the target for the models in this study.</p></sec><sec id="sec5-sensors-25-05512"><title>5. Data Collection</title><p>In this study, we used a set of data from various sources, such as acoustic signals, RGB images, and IR images taken by a mobile robot at an open-pit mining site, to check the condition of conveyor belt idlers (see <xref rid="sensors-25-05512-f009" ref-type="fig">Figure 9</xref>). The open-pit mining site in this study is located in Jarosz&#243;w, 50 km west of Wroclaw. The length of the parts inspected in the conveyor systems was 150 m, where there was a space of 1.45 m between each idler. The detailed description of the mobile robot employed in this research is described in our previous work [<xref rid="B49-sensors-25-05512" ref-type="bibr">49</xref>,<xref rid="B70-sensors-25-05512" ref-type="bibr">70</xref>].</p><p>Through the inspection, the mobile robot moved along the belt conveyor system and captured continuous thermal and RGB images from the wing idlers located on the upper side of the conveyor belt. It is worth mentioning that all the videos were captured from the left side of the studied belt conveyor system. Furthermore, IR videos were captured using a FLIR T640 camera (Wilsonville, OR, USA) with a 45-degree field of view. The format of the captured videos was 768 &#215; 584 pixels, 16-bit-colored videos. The RGB camera with a resolution of 1920 &#215; 584 pixels was used to capture RGB images from idlers as well.</p><p>A total of 100 idlers were inspected during the field campaign (see <xref rid="sensors-25-05512-f010" ref-type="fig">Figure 10</xref>). Of these, four idlers were identified as faulty and were selected, together with thirteen healthy idlers, to form the dataset used for classifier development (seventeen idlers in total). The four faulty cases represent 4% of the inspected population, reflecting the low prevalence of failures in real operational conveyor systems. To construct a balanced training set suitable for supervised learning, we intentionally selected this subset for model training. Acoustic data were obtained from the RGB camera&#8217;s onboard microphone: six-second audio clips were extracted from the recorded video at a sampling rate of 48 kHz for each sample, yielding acoustic records corresponding to the 17 selected idlers.</p></sec><sec id="sec6-sensors-25-05512"><title>6. Training Process</title><p>As discussed previously, different sources of data captured from 17 idlers were studied. We used balanced datasets to train and test the classifiers from a single-sensor monitoring perspective. As long as different test sets were used to initially train the classifiers, we selected nine synchronized data points from nine idlers to study the performance of the proposed fusion method.</p><p>The hardware environment used in this study included the following: an AMD Ryzen 5800H (Santa Clara, CA, USA), an NVIDIA GTX 3060 Ti GPU (Santa Clara, CA, USA), and 16 GB of RAM. The software environment includes the following: Windows 10 OS, Python 3.6, Keras 2.2.4, and Tensorflow-gpu1.12.0. Based on the time complexity of our models using training and validation datasets, we carefully set the experimental parameters of XGBoost and RF to balance the resources used while achieving good performance. The values and meanings of the selected hyperparameters for the RF and XGBoost methods are presented in <xref rid="sensors-25-05512-t001" ref-type="table">Table 1</xref>.</p></sec><sec sec-type="results" id="sec7-sensors-25-05512"><title>7. Results and Discussion</title><p>The trained deep learning models were tested to understand the usability and working performance of the models. Here, the performance factors used are accuracy, precision, and F1 score. An F1 score above 0.9 indicates the usability of the model in real-world applications. The performance of the model using the test dataset is shown in <xref rid="sensors-25-05512-t002" ref-type="table">Table 2</xref>.</p><p>The VGG16 architecture, used as a feature extractor with XGboost as a classifier, achieved the highest F1 score (0.9333) for accurately classifying captured RGB into two clusters: rotating idlers (healthy) and stuck idlers (faulty). The ResNet-50 architecture with RF as a classifier also reached the qualified level of testing performance (0.90). On the other hand, the Xception architecture with RF as the classifier has the lowest F1-test score (0.5882) of the rotating/stuck idler classification.</p><p>For the classification of acquired Cyclic Spectral Coherence, we first define three different classes, as we discussed earlier. The performance of two Inceptionv3 and Xception architectures with RF as a classifier was the highest among the studied models in the true classification of Cyclic Spectral Coherence with the F1 score (1). However, the F1 scores of the other models studied were below 0.90, which indicates their unsatisfactory performance in the true classification of optical coherence.</p><p>In <xref rid="sensors-25-05512-f011" ref-type="fig">Figure 11</xref> and <xref rid="sensors-25-05512-f012" ref-type="fig">Figure 12</xref>, we show that the confusion matrix selected four of the best models with the highest F1 score. The ordinate axis of the confusion matrix represents the actual label of each class, and the horizontal axis represents the predicted label.</p><p><xref rid="sensors-25-05512-f011" ref-type="fig">Figure 11</xref> highlights a recurrent failure mode across the evaluated models: permanently stuck idlers are often misclassified as healthy. This deficiency is principally attributable to the scarcity of representative stuck-idler examples in our training set (only 240 augmented stalled-idler samples for training and validation), which constrains the models&#8217; ability to learn robust, discriminative visual features for this class. Nevertheless, the proposed hybrid strategy&#8212;using pre-trained convolutional networks as feature extractors, combined with fine-tuned machine learning classifiers (e.g., Random Forest, XGBoost)&#8212;delivered strong overall performance, as reported in <xref rid="sensors-25-05512-t002" ref-type="table">Table 2</xref>. This fusion paradigm is therefore particularly attractive for real-world industrial condition-monitoring pipelines, where labeled fault data are limited and computationally efficient, and generalizable solutions are required.</p><p>In operational mining environments, permanently damaged idlers are typically removed and replaced immediately because the sustained belt&#8211;idler friction they produce can create a serious fire hazard; consequently, collecting large numbers of real stuck-idler cases in the field is challenging. To mitigate this limitation, we introduce a dynamic weighted voting scheme that incorporates the normalized surface temperature of each idler as an auxiliary weighting factor on the RGB classifier output. Because stalled idlers generally exhibit elevated surface temperatures due to frictional heating, the temperature weight increases the influence of high RGB scores for suspected stuck cases, thus reducing false negatives. The effectiveness of this temperature-modulated fusion is demonstrated later in this section.</p><p>The Cyclic Spectral Coherence (CSC) maps exhibited well-separated cluster structures in the studied dataset, enabling the classifiers to discriminate the three target classes with relatively high accuracy. Accordingly, the results in <xref rid="sensors-25-05512-f012" ref-type="fig">Figure 12</xref> and <xref rid="sensors-25-05512-t002" ref-type="table">Table 2</xref> show that classification performance on CSC inputs exceeded that obtained on RGB images; this improvement is attributable to the lower intrinsic complexity and clearer class-specific patterns present in the CSC representations.</p><p>Nevertheless, this finding should be interpreted with caution because the CSC experiments relied on a very limited training corpus (35 augmented maps), which reduces statistical confidence and may overstate generalization performance. To mitigate data scarcity, we exploited convolutional backbones pre-trained on large-scale natural-image datasets (ImageNet) for feature extraction; these pre-trained models effectively transfer to CSC inputs because the maps have simpler, lower-dimensional structure than typical RGB scenes, enabling robust feature encoding even with few labeled examples. The downstream classifiers (Random Forest and XGBoost) trained on these high-quality deep features proved sample-efficient and delivered strong results, illustrating that classical machine learning classifiers can perform well when supplied with informative, pre-extracted representations.</p><p>The performance of individual classifiers demonstrates that automated, robot-based inspection can effectively substitute manual idler condition monitoring under field conditions. However, each sensing modality possesses distinct failure modes and information gaps, so relying on any single model limits reliability. To address this, we adopt a decision-level ensemble strategy: the best-performing base classifier (selected by validation F1) is promoted as a strong expert, and its outputs are incorporated as inputs to the fusion stage (together with the other classifiers and the normalized IR temperature). This ensemble-based refinement leverages complementary strengths across modalities, reduces modality-specific false negatives, and produces a more robust detection model for idler diagnosis.</p><p>To compare the performance of the base classifier with the proposed ensemble learning (data fusion model), we redefine the prediction indicator with respect to the actual state of the idler. Therefore, TP indicates the faulty idler, whether the fault is in an early or late stage, while TN indicates a healthy idler.</p><p>As shown in <xref rid="sensors-25-05512-t003" ref-type="table">Table 3</xref>, combining Inceptionv3-RF (Cyclic Spectral Coherence classifier) and VGG16-Xgboost (RGB image classifier) using the normalized temperature of idlers (captured from IR images) as an additional weight to reduce the number of FN in the RGB image classifier results in a lower misclassification rate than individual classifiers. Our approach can accurately identify those faulty idlers that, due to bearing permanent damage, cannot be rotated, and hence they are identified in captured acoustic signals.</p></sec><sec sec-type="conclusions" id="sec8-sensors-25-05512"><title>8. Conclusions</title><p>Early detection and precise localization of overheated idlers are essential to prevent unplanned shutdowns in BC systems. The offline workflow discussed in this work enables the use of computationally intensive pre-processing and deep learning models on control-room servers rather than onboard the robot, facilitating more sophisticated analysis without increasing the robot&#8217;s payload or power budget. Moreover, by decoupling data acquisition and heavy computation, the robot can resume inspection tasks immediately while the analysis proceeds in parallel, improving operational throughput.</p><p>The experiment was carried out during the standard workflow of the facility; therefore, any existing malfunction was not critical to its functionality. Although any other malfunction other than idler-related malfunctions was not taken into consideration (actively looked for), most operational problems would have some sort of reflection in the malfunction occurrence on the idlers, such as some of them not moving, becoming excessively hot, or exhibiting noise. All these faults can be detected through this method&#8212;by detecting some malfunction rather than finding the direct cause&#8212;which should be further investigated after the faulty idler detection. It is important to note that this article is focused on the faulty idler detection rather than the classification of the problem.</p><p>In this work, we developed and validated a multimodal, robot-based condition-monitoring framework for conveyor belt idlers deployed in a mining environment. The system combines acoustic recordings processed using cyclostationary analysis (Cyclic Spectral Coherence), RGB image classification of rotating versus stuck idlers, and IR thermography to extract idler surface temperature. Feature extraction was performed using transfer learning variants of classical CNNs, and the extracted features were classified with RF and XGBoost; final decisions were produced by a dynamic weighted voting ensemble in which the RGB branch weight is modulated by the normalized IR temperature, while the acoustic branch retains a constant prior. Experimental evaluation of field data demonstrates that the temperature-aware late-fusion ensemble reduces false negatives from single-sensor classifiers and improves overall F1 performance compared to individual modalities.</p><p>The manuscript&#8217;s principal contributions are as follows: Firstly, we apply cyclostationary analysis (CSC) to a mobile-robot&#8217;s acoustic data collected under harsh, non-laboratory mining conditions, which improves the visualization and detectability of early fault signatures. Secondly, we introduce a practical dynamic weighted majority-voting fusion rule that adapts classifier weights on a per-sample basis using normalized idler surface temperature from IR images, thereby grounding fusion decisions in physically meaningful thermal evidence. Third, we conduct a comparative evaluation of multiple CNN transfer learning architectures combined with RF and XGBoost classifiers to assess their relative effectiveness within the proposed fusion pipeline. Collectively, these contributions deliver a pragmatic multi-sensor fusion strategy that addresses real-world information gaps and class imbalance typical of robotic inspection scenarios, especially for condition-monitoring idlers in conveyor systems located in mining sites.</p><p>This study demonstrates the feasibility of using an inspection mobile robot for condition monitoring of idlers in belt conveyor (BC) systems. While the proposed pipeline shows clear operational advantages, several limitations remain and will be addressed in future work. First, our evaluation is based on a relatively small, site-specific dataset with severe class imbalance; although transfer learning and data augmentation reduce some effects, the limited sample size restricts statistical generalization and necessitates extensive balancing procedures. Second, we evaluated only cyclic spectral coherence for acoustic pre-processing; comparative assessments against other time&#8211;frequency and signal-processing techniques are needed to determine the most informative representations for early fault detection. Third, deploying sensors on a mobile platform introduces operational challenge occlusions, variable viewing geometry, intermittent data loss, and high environmental noise that can degrade modality fidelity and hamper out-of-the-box transferability to other sites. To strengthen and extend these findings, we plan to collect larger, multi-site datasets (different mines, conveyor designs, and operating regimes), perform cross-site validation, and investigate robustness measures such as sensor calibration protocols, domain adaptation, redundancy, and missing-modality handling. Implementing these extensions will improve reliability, enhance generalization, and provide stronger evidence of the method&#8217;s practical utility for industrial deployments.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors (M. Siami and H.shiri) gratefully acknowledge the European Commission for its support of the Marie Sklodowska Curie program through the ETN MOIRA project (GA 955681).</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.S.; methodology, M.S.; software, M.S. and H.S.; validation, P.D. and A.M.; formal analysis, A.M.; investigation, P.D., J.W., H.S. and R.Z.; resources, R.Z. and T.B.; data curation, P.D.; writing&#8212;original draft preparation, M.S., P.D. and H.S.; writing&#8212;review and editing, A.M., J.W., T.B. and R.Z.; visualization, M.S.; supervision, R.Z. and T.B. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Archived datasets cannot be accessed publicly according to the NDA agreement signed by the authors.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05512"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shiri</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zi&#553;tek</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Inspection robotic UGV platform and the procedure for an acoustic signal-based fault detection in belt conveyor idler</article-title><source>Energies</source><year>2021</year><volume>14</volume><elocation-id>7646</elocation-id><pub-id pub-id-type="doi">10.3390/en14227646</pub-id></element-citation></ref><ref id="B2-sensors-25-05512"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dabek</surname><given-names>P.</given-names></name><name name-style="western"><surname>Szrek</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name></person-group><article-title>An Automatic Procedure for Overheated Idler Detection in Belt Conveyors Using Fusion of Infrared and RGB Images Acquired during UGV Robot Inspection</article-title><source>Energies</source><year>2022</year><volume>15</volume><elocation-id>601</elocation-id><pub-id pub-id-type="doi">10.3390/en15020601</pub-id></element-citation></ref><ref id="B3-sensors-25-05512"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barszcz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Semantic segmentation of thermal defects in belt conveyor idlers using thermal image augmentation and U-Net-based convolutional neural networks</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>5748</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-55864-2</pub-id><pub-id pub-id-type="pmid">38459162</pub-id><pub-id pub-id-type="pmcid">PMC10923815</pub-id></element-citation></ref><ref id="B4-sensors-25-05512"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boloz</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bialy</surname><given-names>W.</given-names></name></person-group><article-title>Automation and Robotization of Underground Mining in Poland</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>7221</elocation-id><pub-id pub-id-type="doi">10.3390/app10207221</pub-id></element-citation></ref><ref id="B5-sensors-25-05512"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>B&#322;a&#380;ej</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kirjan&#243;w</surname><given-names>A.</given-names></name><name name-style="western"><surname>Koz&#322;owski</surname><given-names>T.</given-names></name></person-group><article-title>A high resolution system for automatic diagnosing the condition of the core of conveyor belts with steel cords</article-title><source>Diagnostyka</source><year>2014</year><volume>15</volume><fpage>41</fpage><lpage>45</lpage></element-citation></ref><ref id="B6-sensors-25-05512"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>D&#261;bek</surname><given-names>P.</given-names></name><name name-style="western"><surname>Krot</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Szrek</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Measurement of idlers rotation speed in belt conveyors based on image data analysis for diagnostic purposes</article-title><source>Measurement</source><year>2022</year><volume>202</volume><fpage>111869</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.111869</pub-id></element-citation></ref><ref id="B7-sensors-25-05512"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bortnowski</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kr&#243;l</surname><given-names>R.</given-names></name><name name-style="western"><surname>Nowak-Szpak</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ozdoba</surname><given-names>M.</given-names></name></person-group><article-title>A Preliminary Studies of the Impact of a Conveyor Belt on the Noise Emission</article-title><source>Sustainability</source><year>2022</year><volume>14</volume><elocation-id>2785</elocation-id><pub-id pub-id-type="doi">10.3390/su14052785</pub-id></element-citation></ref><ref id="B8-sensors-25-05512"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bortnowski</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kr&#243;l</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ozdoba</surname><given-names>M.</given-names></name></person-group><article-title>Modelling of transverse vibration of conveyor belt in aspect of the trough angle</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>19897</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-46534-w</pub-id><pub-id pub-id-type="pmid">37963899</pub-id><pub-id pub-id-type="pmcid">PMC10645942</pub-id></element-citation></ref><ref id="B9-sensors-25-05512"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bortnowski</surname><given-names>P.</given-names></name><name name-style="western"><surname>Doroszuk</surname><given-names>B.</given-names></name><name name-style="western"><surname>Krol</surname><given-names>R.</given-names></name><name name-style="western"><surname>Marasova</surname><given-names>D.</given-names></name><name name-style="western"><surname>Moravic</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ozdoba</surname><given-names>M.</given-names></name></person-group><article-title>Forecasting blockades of conveyor transfer points based on vibrodiagnostics</article-title><source>Measurement</source><year>2023</year><volume>216</volume><fpage>112884</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.112884</pub-id></element-citation></ref><ref id="B10-sensors-25-05512"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bortnowski</surname><given-names>P.</given-names></name><name name-style="western"><surname>Gondek</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kr&#243;l</surname><given-names>R.</given-names></name><name name-style="western"><surname>Marasova</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ozdoba</surname><given-names>M.</given-names></name></person-group><article-title>Detection of Blockages of the Belt Conveyor Transfer Point Using an RGB Camera and CNN Autoencoder</article-title><source>Energies</source><year>2023</year><volume>16</volume><elocation-id>1666</elocation-id><pub-id pub-id-type="doi">10.3390/en16041666</pub-id></element-citation></ref><ref id="B11-sensors-25-05512"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barszcz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Advanced Image Analytics for Mobile Robot-Based Condition Monitoring in Hazardous Environments: A Comprehensive Thermal Defect Processing Framework</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3421</elocation-id><pub-id pub-id-type="doi">10.3390/s24113421</pub-id><pub-id pub-id-type="pmid">38894210</pub-id><pub-id pub-id-type="pmcid">PMC11174847</pub-id></element-citation></ref><ref id="B12-sensors-25-05512"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bogacz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cie&#347;lik</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Osowski</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kochaj</surname><given-names>P.</given-names></name></person-group><article-title>Analysis of the Scope for Reducing the Level of Energy Consumption of Crew Transport in an Underground Mining Plant Using a Conveyor Belt System Mining Plant</article-title><source>Energies</source><year>2022</year><volume>15</volume><elocation-id>7691</elocation-id><pub-id pub-id-type="doi">10.3390/en15207691</pub-id></element-citation></ref><ref id="B13-sensors-25-05512"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Topolsky</surname><given-names>D.</given-names></name><name name-style="western"><surname>Topolskaya</surname><given-names>I.</given-names></name><name name-style="western"><surname>Plaksina</surname><given-names>I.</given-names></name><name name-style="western"><surname>Shaburov</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yumagulov</surname><given-names>N.</given-names></name><name name-style="western"><surname>Fedorov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zvereva</surname><given-names>E.</given-names></name></person-group><article-title>Development of a Mobile Robot for Mine Exploration</article-title><source>Processes</source><year>2022</year><volume>10</volume><elocation-id>865</elocation-id><pub-id pub-id-type="doi">10.3390/pr10050865</pub-id></element-citation></ref><ref id="B14-sensors-25-05512"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Siami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barszcz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Deep Learning-Based Semantic Segmentation of Thermal Defects Using AResU-Net and REAL-ESRGAN for the Infrared Image Resolution Enhancement</article-title><source>Proceedings of the UNIfied Conference of DAMAS, IncoME and TEPEN Conferences (UNIfied 2023)</source><person-group person-group-type="editor"><name name-style="western"><surname>Ball</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sinha</surname><given-names>J.K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><fpage>957</fpage><lpage>964</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-49413-0_74</pub-id></element-citation></ref><ref id="B15-sensors-25-05512"><label>15.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Siami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barszcz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Automated ir Image Segmentation for Identification of Overheated Idlers in Belt Conveyor Systems Based on Outlier-Detection Method</article-title><year>2022</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4054247" ext-link-type="uri">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4054247</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-10">(accessed on 10 August 2025)</date-in-citation></element-citation></ref><ref id="B16-sensors-25-05512"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>P.</given-names></name></person-group><article-title>Multi-sensors based condition monitoring of rotary machines: An approach of multidimensional time-series analysis</article-title><source>Measurement</source><year>2019</year><volume>134</volume><fpage>326</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2018.10.089</pub-id></element-citation></ref><ref id="B17-sensors-25-05512"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Siami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Shiri</surname><given-names>H.</given-names></name><name name-style="western"><surname>Barszcz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Information Fusion of Infrared Images and Acoustic Signals for Bearing Fault Diagnosis of Rotating Machinery</article-title><source>Proceedings of the Surveillance, Vibrations, Shock and Noise, Institut Sup&#233;rieur de l&#8242;A&#233;ronautique et de l&#8242;Espace [ISAE-SUPAERO]</source><conf-loc>Toulouse, France</conf-loc><conf-date>10&#8211;13 July 2023</conf-date></element-citation></ref><ref id="B18-sensors-25-05512"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Polikar</surname><given-names>R.</given-names></name></person-group><article-title>Ensemble learning</article-title><source>Ensemble Machine Learning: Methods and Applications</source><publisher-name>Springer</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2012</year><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1007/978-1-4419-9326-7_1</pub-id></element-citation></ref><ref id="B19-sensors-25-05512"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sagi</surname><given-names>O.</given-names></name><name name-style="western"><surname>Rokach</surname><given-names>L.</given-names></name></person-group><article-title>Ensemble learning: A survey</article-title><source>Wiley Interdiscip. Rev. Data Min. Knowl. Discov.</source><year>2018</year><volume>8</volume><fpage>e1249</fpage><pub-id pub-id-type="doi">10.1002/widm.1249</pub-id></element-citation></ref><ref id="B20-sensors-25-05512"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Rokach</surname><given-names>L.</given-names></name></person-group><source>Ensemble Learning: Pattern Classification Using Ensemble Methods</source><publisher-name>World Scientific</publisher-name><publisher-loc>Singapore</publisher-loc><year>2019</year><pub-id pub-id-type="doi">10.1142/9789811201967_0001</pub-id></element-citation></ref><ref id="B21-sensors-25-05512"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kannan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Dao</surname><given-names>D.V.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>An information fusion approach for increased reliability of condition monitoring with homogeneous and heterogeneous sensor systems</article-title><source>Struct. Health Monit.</source><year>2023</year><volume>22</volume><fpage>1601</fpage><lpage>1612</lpage><pub-id pub-id-type="doi">10.1177/14759217221112451</pub-id></element-citation></ref><ref id="B22-sensors-25-05512"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Durrant-Whyte</surname><given-names>H.</given-names></name><name name-style="western"><surname>Henderson</surname><given-names>T.C.</given-names></name></person-group><article-title>Multisensor Data Fusion</article-title><source>Springer Handbook of Robotics</source><person-group person-group-type="editor"><name name-style="western"><surname>Siciliano</surname><given-names>B.</given-names></name><name name-style="western"><surname>Khatib</surname><given-names>O.</given-names></name></person-group><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2016</year><fpage>867</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-32552-1_35</pub-id></element-citation></ref><ref id="B23-sensors-25-05512"><label>23.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Joyce</surname><given-names>J.</given-names></name></person-group><article-title>Bayes&#8217; Theorem</article-title><year>2003</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://plato.stanford.edu/entries/bayes-theorem/" ext-link-type="uri">https://plato.stanford.edu/entries/bayes-theorem/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-10">(accessed on 10 August 2025)</date-in-citation></element-citation></ref><ref id="B24-sensors-25-05512"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Matthies</surname><given-names>L.</given-names></name><name name-style="western"><surname>Elfes</surname><given-names>A.</given-names></name></person-group><article-title>Integration of sonar and stereo range data using a grid-based representation</article-title><source>Proceedings of the 1988 IEEE International Conference on Robotics and Automation</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>19&#8211;24 April 1988</conf-date><volume>Volume 2</volume><fpage>727</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1109/ROBOT.1988.12145</pub-id></element-citation></ref><ref id="B25-sensors-25-05512"><label>25.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bell</surname><given-names>K.</given-names></name><name name-style="western"><surname>Corwin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Stone</surname><given-names>L.</given-names></name><name name-style="western"><surname>Streit</surname><given-names>R.</given-names></name></person-group><source>Bayesian Multiple Target Tracking</source><edition>2nd ed.</edition><publisher-name>Artech</publisher-name><publisher-loc>Morristown, NJ, USA</publisher-loc><year>2013</year></element-citation></ref><ref id="B26-sensors-25-05512"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Babu</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Karri</surname><given-names>R.K.</given-names></name><name name-style="western"><surname>Nisha</surname><given-names>M.S.</given-names></name></person-group><article-title>Sensor data fusion using Kalman filter</article-title><source>Proceedings of the 2018 International Conference on Design Innovations for 3Cs Compute Communicate Control (ICDI3C)</source><conf-loc>Bangalore, India</conf-loc><conf-date>25&#8211;28 April 2018</conf-date><fpage>29</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1109/ICDI3C.2018.00015</pub-id></element-citation></ref><ref id="B27-sensors-25-05512"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Abdulhafiz</surname><given-names>W.A.</given-names></name><name name-style="western"><surname>Khamis</surname><given-names>A.</given-names></name></person-group><article-title>Bayesian approach to multisensor data fusion with Pre-and Post-Filtering</article-title><source>Proceedings of the 2013 10th IEEE International Conference on Networking, Sensing and Control (ICNSC)</source><conf-loc>Evry, France</conf-loc><conf-date>10&#8211;12 April 2013</conf-date><fpage>373</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1109/ICNSC.2013.6548766</pub-id></element-citation></ref><ref id="B28-sensors-25-05512"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thrun</surname><given-names>S.</given-names></name></person-group><article-title>Probabilistic robotics</article-title><source>Commun. ACM</source><year>2002</year><volume>45</volume><fpage>52</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1145/504729.504754</pub-id></element-citation></ref><ref id="B29-sensors-25-05512"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ganeriwal</surname><given-names>S.</given-names></name><name name-style="western"><surname>Balzano</surname><given-names>L.K.</given-names></name><name name-style="western"><surname>Srivastava</surname><given-names>M.B.</given-names></name></person-group><article-title>Reputation-based framework for high integrity sensor networks</article-title><source>Acm Trans. Sens. Netw. (TOSN)</source><year>2008</year><volume>4</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1145/1362542.1362546</pub-id></element-citation></ref><ref id="B30-sensors-25-05512"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Casavola</surname><given-names>A.</given-names></name><name name-style="western"><surname>Franz&#232;</surname><given-names>G.</given-names></name><name name-style="western"><surname>Tedesco</surname><given-names>F.</given-names></name></person-group><article-title>Sensors Selection via a Distributed Reputation Mechanism: An Information Fusion Approach</article-title><source>Proceedings of the 2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)</source><conf-loc>Vasteras, Sweden</conf-loc><conf-date>7&#8211;10 September 2021</conf-date><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/ETFA45728.2021.9613378</pub-id></element-citation></ref><ref id="B31-sensors-25-05512"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Al-Maslamani</surname><given-names>N.M.</given-names></name><name name-style="western"><surname>Abdallah</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ciftler</surname><given-names>B.S.</given-names></name></person-group><article-title>Reputation-aware multi-agent DRL for secure hierarchical federated learning in IoT</article-title><source>IEEE Open J. Commun. Soc.</source><year>2023</year><volume>4</volume><fpage>1274</fpage><lpage>1284</lpage><pub-id pub-id-type="doi">10.1109/OJCOMS.2023.3280359</pub-id></element-citation></ref><ref id="B32-sensors-25-05512"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zuo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>A reputation-based model for mobile agent migration for information search and retrieval</article-title><source>Int. J. Inf. Manag.</source><year>2017</year><volume>37</volume><fpage>357</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1016/j.ijinfomgt.2017.04.002</pub-id></element-citation></ref><ref id="B33-sensors-25-05512"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Rong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name></person-group><article-title>Hybrid metric-feature mapping based on camera and Lidar sensor fusion</article-title><source>Measurement</source><year>2023</year><volume>207</volume><fpage>112411</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.112411</pub-id></element-citation></ref><ref id="B34-sensors-25-05512"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brell</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rogass</surname><given-names>C.</given-names></name><name name-style="western"><surname>Segl</surname><given-names>K.</given-names></name><name name-style="western"><surname>Bookhagen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Guanter</surname><given-names>L.</given-names></name></person-group><article-title>Improving sensor fusion: A parametric method for the geometric coalignment of airborne hyperspectral and LiDAR data</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2016</year><volume>54</volume><fpage>3460</fpage><lpage>3474</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2016.2518930</pub-id></element-citation></ref><ref id="B35-sensors-25-05512"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>G.Z.</given-names></name><name name-style="western"><surname>Andreu-Perez</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Thiemjarus</surname><given-names>S.</given-names></name></person-group><article-title>Multi-sensor fusion</article-title><source>Body Sensor Networks</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2014</year><fpage>301</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1007/978-1-4471-6374-9_8</pub-id></element-citation></ref><ref id="B36-sensors-25-05512"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pedrycz</surname><given-names>W.</given-names></name><name name-style="western"><surname>Aritsugi</surname><given-names>M.</given-names></name></person-group><article-title>Complex evidence theory for multisource data fusion</article-title><source>Chin. J. Inf. Fusion</source><year>2024</year><volume>1</volume><fpage>134</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.62762/CJIF.2024.999646</pub-id></element-citation></ref><ref id="B37-sensors-25-05512"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shitsukane</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cheruiyot</surname><given-names>W.</given-names></name><name name-style="western"><surname>Otieno</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mvurya</surname><given-names>M.</given-names></name></person-group><article-title>Fuzzy logic sensor fusion for obstacle avoidance mobile robot</article-title><source>Proceedings of the 2018 IST-Africa Week Conference (IST-Africa)</source><conf-loc>Gaborone, Botswana</conf-loc><conf-date>9&#8211;11 May 2018</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B38-sensors-25-05512"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kobayashi</surname><given-names>K.</given-names></name><name name-style="western"><surname>Cheok</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>Watanabe</surname><given-names>K.</given-names></name><name name-style="western"><surname>Munekata</surname><given-names>F.</given-names></name></person-group><article-title>Accurate differential global positioning system via fuzzy logic Kalman filter sensor fusion technique</article-title><source>IEEE Trans. Ind. Electron.</source><year>2002</year><volume>45</volume><fpage>510</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1109/41.679010</pub-id></element-citation></ref><ref id="B39-sensors-25-05512"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Nam</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gon-Woo</surname><given-names>K.</given-names></name></person-group><article-title>Learning type-2 fuzzy logic for factor graph based-robust pose estimation with multi-sensor fusion</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2023</year><volume>24</volume><fpage>3809</fpage><lpage>3821</lpage><pub-id pub-id-type="doi">10.1109/TITS.2023.3234595</pub-id></element-citation></ref><ref id="B40-sensors-25-05512"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Song</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name></person-group><article-title>A Novel Bearing Fault Diagnosis Method Based on Improved Convolutional Neural Network and Multi-Sensor Fusion</article-title><source>Machines</source><year>2025</year><volume>13</volume><elocation-id>216</elocation-id><pub-id pub-id-type="doi">10.3390/machines13030216</pub-id></element-citation></ref><ref id="B41-sensors-25-05512"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Waske</surname><given-names>B.</given-names></name><name name-style="western"><surname>van der Linden</surname><given-names>S.</given-names></name></person-group><article-title>Classifying multilevel imagery from SAR and optical sensors by decision fusion</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2008</year><volume>46</volume><fpage>1457</fpage><lpage>1466</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2008.916089</pub-id></element-citation></ref><ref id="B42-sensors-25-05512"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Levitin</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lisnianski</surname><given-names>A.</given-names></name></person-group><article-title>Reliability optimization for weighted voting system</article-title><source>Reliab. Eng. Syst. Saf.</source><year>2001</year><volume>71</volume><fpage>131</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/S0951-8320(00)00089-2</pub-id></element-citation></ref><ref id="B43-sensors-25-05512"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kuang</surname><given-names>Y.</given-names></name></person-group><article-title>A Novel Weighted Voting for K-Nearest Neighbor Rule</article-title><source>J. Comput.</source><year>2011</year><volume>6</volume><fpage>833</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.4304/jcp.6.5.833-840</pub-id></element-citation></ref><ref id="B44-sensors-25-05512"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mian</surname><given-names>T.</given-names></name><name name-style="western"><surname>Choudhary</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fatima</surname><given-names>S.</given-names></name></person-group><article-title>A sensor fusion based approach for bearing fault diagnosis of rotating machine</article-title><source>Proc. Inst. Mech. Eng. Part O J. Risk Reliab.</source><year>2022</year><volume>236</volume><fpage>661</fpage><lpage>675</lpage><pub-id pub-id-type="doi">10.1177/1748006X211044843</pub-id></element-citation></ref><ref id="B45-sensors-25-05512"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>G&#252;ltekin</surname><given-names>&#214;.</given-names></name><name name-style="western"><surname>&#199;inar</surname><given-names>E.</given-names></name><name name-style="western"><surname>&#214;zkan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yaz&#305;c&#305;</surname><given-names>A.</given-names></name></person-group><article-title>A novel deep learning approach for intelligent fault diagnosis applications based on time-frequency images</article-title><source>Neural Comput. Appl.</source><year>2022</year><volume>34</volume><fpage>4803</fpage><lpage>4812</lpage><pub-id pub-id-type="doi">10.1007/s00521-021-06668-2</pub-id></element-citation></ref><ref id="B46-sensors-25-05512"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kou</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lian</surname><given-names>S.w.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>N.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>B.e.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.m.</given-names></name></person-group><article-title>Image-based tool condition monitoring based on convolution neural network in turning process</article-title><source>Int. J. Adv. Manuf. Technol.</source><year>2022</year><volume>119</volume><fpage>3279</fpage><lpage>3291</lpage><pub-id pub-id-type="doi">10.1007/s00170-021-08282-x</pub-id></element-citation></ref><ref id="B47-sensors-25-05512"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Oates</surname><given-names>T.</given-names></name></person-group><article-title>Imaging time-series to improve classification and imputation</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arXiv.1506.00327</pub-id><pub-id pub-id-type="arxiv">1506.00327</pub-id></element-citation></ref><ref id="B48-sensors-25-05512"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karabacak</surname><given-names>Y.E.</given-names></name><name name-style="western"><surname>G&#252;rsel &#214;zmen</surname><given-names>N.</given-names></name><name name-style="western"><surname>G&#252;m&#252;&#351;el</surname><given-names>L.</given-names></name></person-group><article-title>Worm gear condition monitoring and fault detection from thermal images via deep learning method</article-title><source>Eksploat. I Niezawodn.</source><year>2020</year><volume>22</volume><fpage>544</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.17531/ein.2020.3.18</pub-id></element-citation></ref><ref id="B49-sensors-25-05512"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barszcz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Automated Identification of Overheated Belt Conveyor Idlers in Thermal Images with Complex Backgrounds Using Binary Classification with CNN</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>10004</elocation-id><pub-id pub-id-type="doi">10.3390/s222410004</pub-id><pub-id pub-id-type="pmid">36560373</pub-id><pub-id pub-id-type="pmcid">PMC9785391</pub-id></element-citation></ref><ref id="B50-sensors-25-05512"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Antoni</surname><given-names>J.</given-names></name></person-group><article-title>Cyclic spectral analysis of rolling-element bearing signals: Facts and fictions</article-title><source>J. Sound Vib.</source><year>2007</year><volume>304</volume><fpage>497</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1016/j.jsv.2007.02.029</pub-id></element-citation></ref><ref id="B51-sensors-25-05512"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leevy</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Khoshgoftaar</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Bauder</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Seliya</surname><given-names>N.</given-names></name></person-group><article-title>A survey on addressing high-class imbalance in big data</article-title><source>J. Big Data</source><year>2018</year><volume>5</volume><fpage>42</fpage><pub-id pub-id-type="doi">10.1186/s40537-018-0151-6</pub-id></element-citation></ref><ref id="B52-sensors-25-05512"><label>52.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Masko</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hensman</surname><given-names>P.</given-names></name></person-group><article-title>The Impact of Imbalanced Training Data for Convolutional Neural Networks</article-title><year>2015</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.semanticscholar.org/paper/The-Impact-of-Imbalanced-Training-Data-for-Neural-Masko-Hensman/62e81797fff75603a3d7c7759e6efac4fd2b6b31" ext-link-type="uri">https://www.semanticscholar.org/paper/The-Impact-of-Imbalanced-Training-Data-for-Neural-Masko-Hensman/62e81797fff75603a3d7c7759e6efac4fd2b6b31</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-10">(accessed on 10 August 2025)</date-in-citation></element-citation></ref><ref id="B53-sensors-25-05512"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name><name name-style="western"><surname>Park</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name></person-group><article-title>Plankton classification on imbalanced large scale database via convolutional neural networks with transfer learning</article-title><source>Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Phoenix, AZ, USA</conf-loc><conf-date>25&#8211;28 September 2016</conf-date><fpage>3713</fpage><lpage>3717</lpage><pub-id pub-id-type="doi">10.1109/ICIP.2016.7533053</pub-id></element-citation></ref><ref id="B54-sensors-25-05512"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jumaboev</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jurakuziev</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>M.</given-names></name></person-group><article-title>Photovoltaics Plant Fault Detection Using Deep Learning Techniques</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>3728</elocation-id><pub-id pub-id-type="doi">10.3390/rs14153728</pub-id></element-citation></ref><ref id="B55-sensors-25-05512"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lindsay</surname><given-names>G.W.</given-names></name></person-group><article-title>Convolutional neural networks as a model of the visual system: Past, present, and future</article-title><source>J. Cogn. Neurosci.</source><year>2021</year><volume>33</volume><fpage>2017</fpage><lpage>2031</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01544</pub-id><pub-id pub-id-type="pmid">32027584</pub-id></element-citation></ref><ref id="B56-sensors-25-05512"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="B57-sensors-25-05512"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rezazadeh Azar</surname><given-names>E.</given-names></name><name name-style="western"><surname>McCabe</surname><given-names>B.</given-names></name></person-group><article-title>Automated visual recognition of dump trucks in construction videos</article-title><source>J. Comput. Civ. Eng.</source><year>2012</year><volume>26</volume><fpage>769</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1061/(ASCE)CP.1943-5487.0000179</pub-id></element-citation></ref><ref id="B58-sensors-25-05512"><label>58.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Socher</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group><article-title>Imagenet: A large-scale hierarchical image database</article-title><source>Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Miami, FL, USA</conf-loc><conf-date>20&#8211;25 June 2009</conf-date><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1061/(ASCE)CP.1943-5487.0000179</pub-id></element-citation></ref><ref id="B59-sensors-25-05512"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B60-sensors-25-05512"><label>60.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="B61-sensors-25-05512"><label>61.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sermanet</surname><given-names>P.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Vanhoucke</surname><given-names>V.</given-names></name><name name-style="western"><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group><article-title>Going deeper with convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></element-citation></ref><ref id="B62-sensors-25-05512"><label>62.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><article-title>Xception: Deep learning with depthwise separable convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>1251</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.195</pub-id></element-citation></ref><ref id="B63-sensors-25-05512"><label>63.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>T.K.</given-names></name></person-group><article-title>Random Decision Forest</article-title><source>Proceedings of the 3rd International Conference on Document Analysis and Recognition</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>14&#8211;16 August 1995</conf-date><fpage>278</fpage><lpage>282</lpage></element-citation></ref><ref id="B64-sensors-25-05512"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hao</surname><given-names>Y.</given-names></name></person-group><article-title>Deep pre-trained networks as a feature extractor with XGBoost to detect tuberculosis from chest X-ray</article-title><source>Comput. Electr. Eng.</source><year>2021</year><volume>93</volume><fpage>107252</fpage><pub-id pub-id-type="doi">10.1016/j.compeleceng.2021.107252</pub-id></element-citation></ref><ref id="B65-sensors-25-05512"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Su</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Su</surname><given-names>B.</given-names></name><name name-style="western"><surname>Fong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>X.</given-names></name></person-group><article-title>Parallel ensemble learning of convolutional neural networks and local binary patterns for face recognition</article-title><source>Comput. Methods Programs Biomed.</source><year>2020</year><volume>197</volume><elocation-id>105622</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105622</pub-id><pub-id pub-id-type="pmid">32629293</pub-id></element-citation></ref><ref id="B66-sensors-25-05512"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sultana</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>K.P.</given-names></name><name name-style="western"><surname>Verma</surname><given-names>S.</given-names></name></person-group><article-title>A sequential ensemble model for communicable disease forecasting</article-title><source>Curr. Bioinform.</source><year>2020</year><volume>15</volume><fpage>309</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.2174/1574893614666191202153824</pub-id></element-citation></ref><ref id="B67-sensors-25-05512"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mohammed</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kora</surname><given-names>R.</given-names></name></person-group><article-title>A comprehensive review on ensemble deep learning: Opportunities and challenges</article-title><source>J. King Saud-Univ.-Comput. Inf. Sci.</source><year>2023</year><volume>35</volume><fpage>757</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.1016/j.jksuci.2023.01.014</pub-id></element-citation></ref><ref id="B68-sensors-25-05512"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Valle</surname><given-names>C.</given-names></name><name name-style="western"><surname>Saravia</surname><given-names>F.</given-names></name><name name-style="western"><surname>Allende</surname><given-names>H.</given-names></name><name name-style="western"><surname>Monge</surname><given-names>R.</given-names></name><name name-style="western"><surname>Fern&#225;ndez</surname><given-names>C.</given-names></name></person-group><article-title>Parallel approach for ensemble learning with locally coupled neural networks</article-title><source>Neural Process. Lett.</source><year>2010</year><volume>32</volume><fpage>277</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1007/s11063-010-9157-6</pub-id></element-citation></ref><ref id="B69-sensors-25-05512"><label>69.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Valdovinos</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>S&#225;nchez</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Barandela</surname><given-names>R.</given-names></name></person-group><article-title>Dynamic and static weighting in classifier fusion</article-title><source>Proceedings of the Pattern Recognition and Image Analysis: Second Iberian Conference, IbPRIA 2005, Estoril, Portugal, 7&#8211;9 June 2005</source><series>Proceedings, Part II 2</series><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2005</year><fpage>59</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1007/11492542_8</pub-id></element-citation></ref><ref id="B70-sensors-25-05512"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barszcz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wodecki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zimroz</surname><given-names>R.</given-names></name></person-group><article-title>Design of an Infrared Image Processing Pipeline for Robotic Inspection of Conveyor Systems in Opencast Mining Sites</article-title><source>Energies</source><year>2022</year><volume>15</volume><elocation-id>6771</elocation-id><pub-id pub-id-type="doi">10.3390/en15186771</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05512-f001" orientation="portrait"><label>Figure 1</label><caption><p>Simplified flowchart of the proposed fusion-based diagnostic method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g001.jpg"/></fig><fig position="float" id="sensors-25-05512-f002" orientation="portrait"><label>Figure 2</label><caption><p>Comparison of the common energy patterns in Cyclic Spectral Coherence for a faulty and healthy idler.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g002.jpg"/></fig><fig position="float" id="sensors-25-05512-f003" orientation="portrait"><label>Figure 3</label><caption><p>Time&#8211;frequency representation of the signal with impulsive disturbance due to belt joint using a metal clip.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g003.jpg"/></fig><fig position="float" id="sensors-25-05512-f004" orientation="portrait"><label>Figure 4</label><caption><p>Belt joint using a metal clip.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g004.jpg"/></fig><fig position="float" id="sensors-25-05512-f005" orientation="portrait"><label>Figure 5</label><caption><p>Comparison of RGB images captured from a moving and stuck idler.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g005.jpg"/></fig><fig position="float" id="sensors-25-05512-f006" orientation="portrait"><label>Figure 6</label><caption><p>Evolution of failure signals in belt conveyor idlers.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g006.jpg"/></fig><fig position="float" id="sensors-25-05512-f007" orientation="portrait"><label>Figure 7</label><caption><p>Comparison of IR image from a cold (moving idler) and overheated (stuck idler).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g007.jpg"/></fig><fig position="float" id="sensors-25-05512-f008" orientation="portrait"><label>Figure 8</label><caption><p>Simplified flowchart of the proposed dynamic weighting method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g008.jpg"/></fig><fig position="float" id="sensors-25-05512-f009" orientation="portrait"><label>Figure 9</label><caption><p>View of the robot during inspection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g009.jpg"/></fig><fig position="float" id="sensors-25-05512-f010" orientation="portrait"><label>Figure 10</label><caption><p>General view of the raw material storage area, showing the belt conveyor used for material transport.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g010.jpg"/></fig><fig position="float" id="sensors-25-05512-f011" orientation="portrait"><label>Figure 11</label><caption><p>Comparison of confusion matrix of 4 models with the highest F1 score on the classification of RGB images.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g011.jpg"/></fig><fig position="float" id="sensors-25-05512-f012" orientation="portrait"><label>Figure 12</label><caption><p>Comparison of confusion matrix of 4 models with the highest F1 score on the classification of Cyclic Spectral Coherence.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05512-g012.jpg"/></fig><table-wrap position="float" id="sensors-25-05512-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05512-t001_Table 1</object-id><label>Table 1</label><caption><p>Hyperparameters of the employed RF and XGBoost methods and their values.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Models</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Hyperparameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Meaning</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Values</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RF</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Number of trees used in the forest</td><td align="center" valign="middle" rowspan="1" colspan="1">50</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of random variables used in each tree</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">XGBoost</td><td align="left" valign="middle" rowspan="1" colspan="1">Learning rate</td><td align="left" valign="middle" rowspan="1" colspan="1">Shrinkage coefficient of each tree</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Maximum tree depth</td><td align="left" valign="middle" rowspan="1" colspan="1">Maximum depth of a tree</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Subsample ratio</td><td align="left" valign="middle" rowspan="1" colspan="1">Subsample ratio of training samples</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Column subsample ratio</td><td align="left" valign="middle" rowspan="1" colspan="1">Subsample ratio of columns for tree construction</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Maximum delta step</td><td align="left" valign="middle" rowspan="1" colspan="1">Maximum depth of a tree</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gamma</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Minimum loss reduction required to make a further partition</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05512-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05512-t002_Table 2</object-id><label>Table 2</label><caption><p>Results of the studied deep learning model on the classification of extracted RGB images and Cyclic Spectral Coherences.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Depth</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Number of <break/> Parameters</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Accuracy</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Precision</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">F1 Score</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Image</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic Signal</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Image</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic Signal</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Image</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic Signal</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">VGG16-RF</td><td rowspan="2" align="center" valign="middle" colspan="1">16</td><td rowspan="2" align="center" valign="middle" colspan="1">138.4 M</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8958</td><td align="left" valign="middle" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7916</td><td align="left" valign="middle" rowspan="1" colspan="1">0.7777</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8837</td><td align="left" valign="middle" rowspan="1" colspan="1">0.7555</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VGG16-XGboost</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9375</td><td align="left" valign="middle" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8750</td><td align="left" valign="middle" rowspan="1" colspan="1">0.7222</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9333</td><td align="left" valign="middle" rowspan="1" colspan="1">0.7301</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Inceptionv3-RF</td><td rowspan="2" align="center" valign="middle" colspan="1">189</td><td rowspan="2" align="center" valign="middle" colspan="1">23.9 M</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7083</td><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4166</td><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5882</td><td align="left" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Inceptionv3-XGboost</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7916</td><td align="left" valign="middle" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5833</td><td align="left" valign="middle" rowspan="1" colspan="1">0.7222</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7368</td><td align="left" valign="middle" rowspan="1" colspan="1">0.7388</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet-50-RF</td><td rowspan="2" align="center" valign="middle" colspan="1">107</td><td rowspan="2" align="center" valign="middle" colspan="1">25.6 M</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9166</td><td align="left" valign="middle" rowspan="1" colspan="1">0.625</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8333</td><td align="left" valign="middle" rowspan="1" colspan="1">0.6666</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9090</td><td align="left" valign="middle" rowspan="1" colspan="1">0.6238</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet-50-XGboost</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8541</td><td align="left" valign="middle" rowspan="1" colspan="1">0.50</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7083</td><td align="left" valign="middle" rowspan="1" colspan="1">0.5555</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8292</td><td align="left" valign="middle" rowspan="1" colspan="1">0.4330</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Xception-RF</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">81</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">22.9 M</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7708</td><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5416</td><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7027</td><td align="left" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Xception-XGboost</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7291</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.875</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.50</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8888</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6486</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8666</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05512-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05512-t003_Table 3</object-id><label>Table 3</label><caption><p>Performance comparison of proposed sensor fusion in true detection method of damaged idlers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Source of Information</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 Score</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Acoustic signal (Inceptionv3-RF)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.75</td><td align="left" valign="middle" rowspan="1" colspan="1">0.6</td><td align="left" valign="middle" rowspan="1" colspan="1">0.75</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RGB image (VGG16-XGboost)</td><td align="left" valign="middle" rowspan="1" colspan="1">0.5</td><td align="left" valign="middle" rowspan="1" colspan="1">0.2</td><td align="left" valign="middle" rowspan="1" colspan="1">0.33</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fusion (Acoustic, IR image, RGB image)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.80</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.88</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>