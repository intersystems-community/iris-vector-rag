<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431115</article-id><article-id pub-id-type="pmcid-ver">PMC12431115.1</article-id><article-id pub-id-type="pmcaid">12431115</article-id><article-id pub-id-type="pmcaiid">12431115</article-id><article-id pub-id-type="doi">10.3390/s25175331</article-id><article-id pub-id-type="publisher-id">sensors-25-05331</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>GICEDCam: A Geospatial Internet of Things Framework for Complex Event Detection in Camera Streams</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Honarparvar</surname><given-names initials="S">Sepehr</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05331" ref-type="aff">1</xref><xref rid="c1-sensors-25-05331" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Honarparvar</surname><given-names initials="Y">Yasaman</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-05331" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5475-8923</contrib-id><name name-style="western"><surname>Ashena</surname><given-names initials="Z">Zahra</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05331" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liang</surname><given-names initials="S">Steve</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05331" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1480-6382</contrib-id><name name-style="western"><surname>Saeedi</surname><given-names initials="S">Sara</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05331" ref-type="aff">2</xref><xref rid="c1-sensors-25-05331" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Rizzardi</surname><given-names initials="A">Alessandra</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05331"><label>1</label>Department of Geomatics Engineering, University of Calgary, Calgary, AB T2N 1N4, Canada; <email>zahra.bagheriashena@ucalgary.ca</email> (Z.A.); <email>liangs@ucalgary.ca</email> (S.L.)</aff><aff id="af2-sensors-25-05331"><label>2</label>Department of Electrical and Software Engineering, University of Calgary, Calgary, AB T2N 1N4, Canada; <email>yasaman.honarparvar@ucalgary.ca</email></aff><author-notes><corresp id="c1-sensors-25-05331"><label>*</label>Correspondence: <email>sepehr.honarparvar@ucalgary.ca</email> (S.H.); <email>ssaeedi@ucalgary.ca</email> (S.S.)</corresp></author-notes><pub-date pub-type="epub"><day>27</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5331</elocation-id><history><date date-type="received"><day>06</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>24</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>27</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05331.pdf"/><abstract><p>Complex event detection (CED) adds value to camera stream data in various applications such as workplace safety, task monitoring, security, and health. Recent CED frameworks have addressed the issues of limited spatiotemporal labels and costly training by decomposing the CED into low-level features, as well as spatial and temporal relationship extraction. However, these frameworks suffer from high resource costs, low scalability, and an increased number of false positives and false negatives. This paper proposes GICEDCAM, which distributes CED across edge, stateless, and stateful layers to improve scalability and reduce computation cost. Additionally, we introduce a Spatial Event Corrector component that leverages geospatial data analysis to minimize false negatives and false positives in spatial event detection. We evaluate GICEDCAM on 16 camera streams covering four complex events. Relative to a strong open-source baseline configured for our setting, GICEDCAM reduces end-to-end latency by 36% and total computational cost by 45%, with the advantage widening as objects per frame increase. Among corrector variants, Bayesian Network (BN) yields the lowest latency, Long Short-Term Memory (LSTM) achieves the highest accuracy, and trajectory analysis offers the best accuracy&#8211;latency trade-off for this architecture.</p></abstract><kwd-group><kwd>complex event detection</kwd><kwd>video processing</kwd><kwd>internet of things</kwd><kwd>cloud computing</kwd><kwd>spatial relationships detection</kwd><kwd>trajectory analysis</kwd><kwd>object tracking</kwd><kwd>object detection</kwd><kwd>computer vision</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05331"><title>1. Introduction</title><p>Merriam-Webster defines an event as &#8220;the occurrence of something&#8221; [<xref rid="B1-sensors-25-05331" ref-type="bibr">1</xref>]. Events can be categorized as either simple or complex. Simple events are atomic and independent, and cannot be decomposed into smaller events [<xref rid="B2-sensors-25-05331" ref-type="bibr">2</xref>]. A complex event consists of a series of timestamped simple events that are logically or temporally interconnected [<xref rid="B3-sensors-25-05331" ref-type="bibr">3</xref>]. While simple event detection is as straightforward as reading sensor measurements or alarms [<xref rid="B4-sensors-25-05331" ref-type="bibr">4</xref>], complex event detection (CED) refers to the process of composing such timestamped simple events into meaningful higher-level events [<xref rid="B3-sensors-25-05331" ref-type="bibr">3</xref>]. In video streams, a simple event is the presence of an object within a frame. When these objects are linked through spatial and temporal relationships, complex events can be formed [<xref rid="B5-sensors-25-05331" ref-type="bibr">5</xref>]. In this paper, we define two mid-level events (Low-level events are simple events, and high-level events are complex events. Mid-level events are composed of low-level events and can form high-level events.). A spatial event occurs when two or more objects exhibit spatial relationships at the same time. A temporal event occurs when two or more spatial events exhibit temporal relationships. At the highest level, a complex event occurs when multiple temporal events co-occur or occur in sequence within a specific time window, conforming to a predefined pattern.</p><p>The rapid increase in video and camera content and advancements in Artificial Intelligence (AI) have led to a growing demand for automated video content analysis [<xref rid="B6-sensors-25-05331" ref-type="bibr">6</xref>]. Effective CED enhances workplace safety, improves situational awareness, and enables automated monitoring of human activity [<xref rid="B7-sensors-25-05331" ref-type="bibr">7</xref>], surveillance systems [<xref rid="B8-sensors-25-05331" ref-type="bibr">8</xref>], healthcare [<xref rid="B9-sensors-25-05331" ref-type="bibr">9</xref>], workers&#8217; tasks monitoring [<xref rid="B10-sensors-25-05331" ref-type="bibr">10</xref>], assembly operations monitoring [<xref rid="B11-sensors-25-05331" ref-type="bibr">11</xref>], and contact tracing [<xref rid="B12-sensors-25-05331" ref-type="bibr">12</xref>]. Many studies and frameworks have focused on improving CED accuracy, performance, cost efficiency, and scalability. Honarparvar et al. (2024) categorized CED methods into four main types [<xref rid="B5-sensors-25-05331" ref-type="bibr">5</xref>]. This paper focuses on Object Detection and Spatiotemporal Matching (ODSM) methods. ODSM methods break CED into object detection within frames and the matching of event patterns based on spatiotemporal relationships. These patterns are typically queried from a complex event knowledge graph.</p><p><bold>Problem Statement:</bold> Existing ODSM frameworks mostly rely on fully stateful computing architectures. In these frameworks, all relevant data for a complex event must be stored to enable querying against the knowledge graph. Maintaining these states across time windows is resource-intensive and increases the computational cost of CED. This becomes a critical issue when many objects are detected per frame or when a network of multiple camera streams is involved. This problem severely limits the scalability of CED. Another challenge is the low accuracy of spatial relationship matching. Traditional methods rely solely on bounding-box (bbox) topology to infer spatial relationships. This approach is highly sensitive to object detection errors, such as occlusions, low-quality frames, false positives and negatives, or perspective distortions. These issues propagate into incorrect spatial matches and ultimately result in incorrect complex event matching.</p><p><bold>Proposed approach</bold>: GICEDCAM reorders the pipeline to address these issues: (i) a stateless, knowledge graph-driven gating and enrichment stage filters detections before they reach any stateful operator; (ii) a geospatial projection with mobility-aware routing standardizes geometry and directs moving versus fixed objects to specialized operators; and (iii) an online Spatial Event Corrector leverages short-horizon trajectories and fuzzy zone membership to impute likely missing spatial predicates prior to temporal composition. This placement and orchestration (rather than new detectors) underpins the scalability and robustness demonstrated in our evaluation.</p><p>Accordingly, the main contributions of this paper are summarized as follows:<list list-type="simple"><list-item><label>-</label><p>The design and implementation of a three-layer architecture that distributes CED workloads to reduce computational cost and increase scalability.</p></list-item><list-item><label>-</label><p>The development of a Spatial Event Corrector component that predicts missing spatial events and reduces false positives and false negatives in relationship matching.</p></list-item></list></p><p>The proposed method is based on three main assumptions:<list list-type="simple"><list-item><label>-</label><p>We can access sufficient calibration data (e.g., camera calibration files or ground control points) to transform frame-based coordinates into geospatial coordinates.</p></list-item><list-item><label>-</label><p>Camera locations and orientations are fixed throughout the CED process.</p></list-item><list-item><label>-</label><p>Cameras submit frames to edge devices and the cloud.</p></list-item></list></p><p>In real-world deployments, the assumptions stated above may not always hold. First, calibration data may be incomplete or outdated due to hardware changes, camera relocation, or drift over time. This can be addressed through automated self-calibration algorithms, the use of natural scene features as reference points, or periodic maintenance checks. Second, maintaining stable bandwidth can be difficult in distributed camera networks, which may lead to delays or dropped events. Finally, environmental variability (e.g., changes in lighting or weather conditions) can further degrade detection performance. While this paper does not implement countermeasures for these conditions, prior work on adaptive and robust event detection offers potential solutions. For instance, Li et al. [<xref rid="B13-sensors-25-05331" ref-type="bibr">13</xref>] discuss hierarchical feature learning to improve adaptability to varying contexts, Kang et al. [<xref rid="B14-sensors-25-05331" ref-type="bibr">14</xref>] explore adaptive thresholding for resource-constrained video analytics, and MERN [<xref rid="B15-sensors-25-05331" ref-type="bibr">15</xref>] demonstrates the benefits of multimodal data fusion to handle environmental challenges. Such approaches could be integrated in future extensions of GICEDCAM to improve robustness in operational deployments.</p><p>The remainder of this paper is organized into six sections. <xref rid="sec2-sensors-25-05331" ref-type="sec">Section 2</xref> reviews existing methods and frameworks for complex event detection. <xref rid="sec3-sensors-25-05331" ref-type="sec">Section 3</xref> describes the proposed GICEDCAM architecture and components. <xref rid="sec4-sensors-25-05331" ref-type="sec">Section 4</xref> presents the implementation details and experimental results and discusses the evaluation of outcomes. <xref rid="sec5-sensors-25-05331" ref-type="sec">Section 5</xref> concludes the paper and outlines future research directions.</p></sec><sec id="sec2-sensors-25-05331"><title>2. Literature Review</title><p>Various methods have been proposed to detect complex events in videos, and numerous categorizations of CED approaches exist. Since this paper aims to improve complex event matching, we review existing work based on event-matching strategies, as categorized by Honarparvar et al. (2024) [<xref rid="B5-sensors-25-05331" ref-type="bibr">5</xref>]. The first category includes Training and Predicting Videos (TPV) approaches. TPV approaches are based on supervised or semi-supervised learning models trained on labelled video datasets to predict complex events. The major challenges of these methods are the limited training data [<xref rid="B16-sensors-25-05331" ref-type="bibr">16</xref>] and noisy labels [<xref rid="B17-sensors-25-05331" ref-type="bibr">17</xref>]. TPV methods typically require large-scale, high-quality labelled datasets, which are expensive and time-consuming to produce. The second category is Zero-Example Prediction (ZEP). ZEP frameworks use semantic search algorithms to match user-generated queries with video corpus metadata [<xref rid="B18-sensors-25-05331" ref-type="bibr">18</xref>]. These search algorithms reduce the cost of labelling and training. However, they depend heavily on the availability of textual metadata, which restricts their applicability to web-based videos. The third category, known as Multi-Source Fusion (MSF), incorporates auxiliary data sources (e.g., audio, motion sensors) to improve event detection accuracy [<xref rid="B19-sensors-25-05331" ref-type="bibr">19</xref>]. Despite their benefits, MSF methods are constrained by their dependency on the quality of external data and often suffer from processing latency [<xref rid="B20-sensors-25-05331" ref-type="bibr">20</xref>].</p><p>Another category of event matching methods is called ODSM-based. ODSM approaches do not require spatiotemporal labelling. These approaches rely on lightweight models for object detection, followed by pattern-matching techniques to identify spatial and temporal relationships. Unlike other methods, ODSM operates solely on video stream data and does not require textual information or external sources. Understanding object interactions and spatiotemporal relationships plays a central role in aligning detected events with predefined patterns in CED. Techniques such as entity-centric feature pooling focus on object-human interactions by extracting localized spatiotemporal relationships [<xref rid="B21-sensors-25-05331" ref-type="bibr">21</xref>]. EventNet was among the earliest ODSM frameworks, which introduced a video ontology framework to link object relationships with complex event concepts and enable semantic querying within video content [<xref rid="B22-sensors-25-05331" ref-type="bibr">22</xref>]. Other innovations include trajectory-based models, where event semantics are assigned through hypergraph pairing [<xref rid="B23-sensors-25-05331" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05331" ref-type="bibr">24</xref>], and frameworks for abnormal human behavior detection via trajectory clustering using dense descriptors [<xref rid="B25-sensors-25-05331" ref-type="bibr">25</xref>]. Hierarchical models have also been developed to reduce error propagation in CED by creating layered relationships from frame-level features to the temporal activity concept [<xref rid="B13-sensors-25-05331" ref-type="bibr">13</xref>].</p><p>As querying performance became a bottleneck, several frameworks emerged to improve efficiency. VideoStorm addressed resource allocation with lag- and quality-aware querying [<xref rid="B26-sensors-25-05331" ref-type="bibr">26</xref>], while BLAZEIT introduced FrameQL to reduce DNN inference costs in video analytics [<xref rid="B14-sensors-25-05331" ref-type="bibr">14</xref>]. Hybrid workflows (e.g., Khan et al.) leveraged logical reasoning over simple event detection to identify complex patterns [<xref rid="B27-sensors-25-05331" ref-type="bibr">27</xref>]. VIDCEP represents a major advancement by combining a flexible Video Event Query Language (VEQL) with a Video Event Knowledge Graph (VEKG) to enable multistep spatiotemporal event matching [<xref rid="B15-sensors-25-05331" ref-type="bibr">15</xref>,<xref rid="B28-sensors-25-05331" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05331" ref-type="bibr">29</xref>]. Subsequent models like MERN, TAG, and NLP-guided ontologies enhanced semantic representation and reduced processing complexity [<xref rid="B30-sensors-25-05331" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05331" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05331" ref-type="bibr">32</xref>]. The Notification-Oriented Paradigm (NOP) recently proposed an efficient chain-based querying mechanism, although it remains constrained to specific event types [<xref rid="B33-sensors-25-05331" ref-type="bibr">33</xref>]. Collectively, these developments underscore a growing emphasis on semantic-rich modelling and computational efficiency in complex event-matching frameworks.</p><p>Despite these advancements, ODSM-based frameworks still face critical limitations. Frameworks like VIDCEP, BLAZEIT, and NOP are often not cost-efficient. Given the high video streaming rate (e.g., 30 fps), storing and maintaining frame-level data in memory for extended periods leads to significant computational overhead. Moreover, these frameworks typically do not offer generalized solutions to reduce spatial and temporal matching errors caused by object occlusion or missed detections. As a result, they remain prone to false negatives and incur high processing costs. Although ODSM frameworks reduce reliance on labels and improve querying mechanisms, challenges remain regarding cost efficiency, scalability, and robustness to detection errors. In the next section, we propose a novel framework to address these issues by distributing CED workloads across computational layers and introducing error-correcting components. <xref rid="sensors-25-05331-t001" ref-type="table">Table 1</xref> summarizes the reviewed ODSM-based CED frameworks&#8217; strengths and limitations.</p></sec><sec id="sec3-sensors-25-05331"><title>3. Method Overview</title><p>This section introduces the methodology of the proposed GICEDCAM framework for improving CED in camera streams. It is organized into three main subsections. <xref rid="sec3dot1-sensors-25-05331" ref-type="sec">Section 3.1</xref> presents the architecture of the GICEDCAM data pipeline and the structure of the proposed complex event knowledge graph. <xref rid="sec3dot2-sensors-25-05331" ref-type="sec">Section 3.2</xref> details the Spatial Event Corrector component, including three approaches for predicting missing spatial events. <xref rid="sec3dot3-sensors-25-05331" ref-type="sec">Section 3.3</xref> describes the Real-Time Trajectory Corrector, which includes the tracking re-identifier and trajectory spatial enhancer.</p><sec id="sec3dot1-sensors-25-05331"><title>3.1. GICEDCAM Framework Design</title><p><bold>Problem recap</bold> (see <xref rid="sec1-sensors-25-05331" ref-type="sec">Section 1</xref>). Prior stateful pipelines inflate memory/latency, and bbox-only spatial matching propagates detector errors; we address these with the following innovations:<list list-type="simple"><list-item><label>-</label><p><bold>Enhancing the Complex Event Knowledge Graph</bold>: The complex event knowledge graph (KG) is revised to incorporate additional geospatial functions and entities. This enhancement is crucial for effectively addressing the challenges posed by false negatives and false positives associated with spatial events.</p></list-item><list-item><label>-</label><p><bold>Improving Computational Resource Allocation</bold>: The computational burden of low-level feature extraction is relocated to the edge computing components. This strategic shift will allow cloud computing components to concentrate on the more intricate tasks of spatial events, temporal events, and complex events matching.</p></list-item><list-item><label>-</label><p><bold>Stateless Spatial Matching</bold>: The entire process of spatial event matching is transitioned to stateless data processing components. In contrast, the temporal and complex event matching will remain within the purview of stateful matching components.</p></list-item></list></p><p>We next describe the GICEDCAM knowledge graph (<xref rid="sec3dot1dot1-sensors-25-05331" ref-type="sec">Section 3.1.1</xref>) and data pipeline (<xref rid="sec3dot1dot2-sensors-25-05331" ref-type="sec">Section 3.1.2</xref>).</p><sec id="sec3dot1dot1-sensors-25-05331"><title>3.1.1. GICEDCAM Knowledge Graph</title><p>KGs are foundational to every CED framework, as they define the semantic structure of events and the relationships among involved entities. Designing an effective KG requires addressing criteria such as event pattern matching efficiency, scalability, and the ability to support complex spatial and temporal relationships [<xref rid="B31-sensors-25-05331" ref-type="bibr">31</xref>]. In the context of GICEDCAM, we design a KG that meets these criteria while aligning with the framework&#8217;s multi-layered data pipeline and processing flow.</p><p>As shown in <xref rid="sensors-25-05331-f001" ref-type="fig">Figure 1</xref>, the KG schema includes essential entities, attributes, and relationships. Based on the schema, at the lowest level of event features, every object must have at least two attributes (i.e., geospatial zone and mobility status). The geospatial zone is necessary to address the missing spatial events issue. The mobility status attribute helps reduce computational costs as we no longer need to run complex processes for fixed objects. There could be more attributes per object, such as color histogram, object speed, and other low-level features, which are stored in the attribute vector. Each object must have a local ID that is generated by an IoT sensor or a camera. Objects must also have a global ID, which identifies the object in the real world and is the result of the matching between various local IDs. Objects include at least one tracklet (A tracklet is a temporal sequence formed by linking consecutive detections of a single object across video frames, which share the same tracking ID.), if its mobility status is the &#8220;moving object&#8221;. Each tracklet is a part of one trajectory, and it also has a local ID (which is generated by the camera tracking algorithm) or a global ID (which identifies the tracklet ID in all joined trajectories of the object). Tracklets and trajectories are important parts of the KG since moving objects are mandatory entities of any complex event. In short: &#8220;no movement, no event.&#8221;</p><p>Each spatial event has one instance of geospatial and frame-based spatial relationships. The frame-based spatial relationships are defined as topological relationships of multiple objects&#8217; bbox in a frame. Therefore, every frame-based spatial relationship involves at least two objects. Topological relationships in videos can be encoded using the Egenhofer matrix for 2D objects [<xref rid="B34-sensors-25-05331" ref-type="bibr">34</xref>]. To avoid false positives and false negatives in detecting spatial events, we need to discover spatial relationships in a geospatial coordinate system. For example, a person&#8217;s hand bbox intersects a door handle while standing away from the door. We can avoid this false positive error by determining geospatial relationships between objects. The geospatial relationships are directional (e.g., west of), distance-based (e.g., nearby), or contextual (e.g., objects within the same region).</p><p>Temporal events can be encoded in 13 time intervals, including before, meets, overlaps, during, starts, finishes, equals, is met by, is overlapped by, contains, is started by, is finished by, and after. For example, the &#8220;<italic toggle="yes">person picks up the bottle</italic>&#8221; spatial event time window overlaps the &#8220;<italic toggle="yes">bottle is on the table</italic>&#8221; spatial event, which forms the &#8220;<italic toggle="yes">person picks the bottle from the table</italic>&#8221; temporal event.</p><p>The following is the entity set (nodes) of the proposed KG:<list list-type="simple"><list-item><label>-</label><p><bold>Camera</bold>: {camera_id, name, H (3 &#215; 3), ts_calib, crs: cartesian-m}</p></list-item><list-item><label>-</label><p><bold>Zone</bold>: {zone_id, polygon, version, crs, bbox, semantics}</p></list-item><list-item><label>-</label><p><bold>Object</bold>: {global_id, class, attrs, ts_first, ts_last}</p></list-item><list-item><label>-</label><p><bold>LocalObject (per sensor)</bold>: {local_id, camera_id, class}</p></list-item><list-item><label>-</label><p><bold>Tracklet</bold>: {tracklet_id, camera_id, local_id, ts_start, ts_end}</p></list-item><list-item><label>-</label><p><bold>Trajectory</bold>: {traj_id, global_id, ts_start, ts_end}</p></list-item><list-item><label>-</label><p><bold>Observation (immutable)</bold>: {obs_id, ts, u, v, x, y, conf, bbox}</p></list-item><list-item><label>-</label><p><bold>SpatialEvent</bold>: {se_id, type, ts, corrected:boolean, conf}</p></list-item><list-item><label>-</label><p><bold>TemporalEvent</bold>: {te_id, type, t_start, t_end}</p></list-item><list-item><label>-</label><p><bold>ComplexEvent</bold>: {ce_id, type, t_start, t_end, query_id}</p></list-item></list></p><p>The following is the relationships set (edges) of the KG:<list list-type="simple"><list-item><label>-</label><p><bold>(:LocalObject)-[:SAME_AS]-&gt;(:Object)</bold> (many&#8594;1)</p></list-item><list-item><label>-</label><p><bold>(:Object)-[:HAS_TRACKLET]-&gt;(:Tracklet)</bold> (1&#8594;many)</p></list-item><list-item><label>-</label><p><bold>(:Tracklet)-[:PART_OF_TRAJ]-&gt;(:Trajectory)</bold> (many&#8594;1)</p></list-item><list-item><label>-</label><p><bold>(:Observation)-[:OF]-&gt;(:LocalObject)</bold> (many&#8594;1)</p></list-item><list-item><label>-</label><p><bold>(:Observation)-[:PROJECTED_BY {camera_id}]-&gt;(:Camera)</bold></p></list-item><list-item><label>-</label><p><bold>(:Observation)-[:IN_ZONE {ts, &#956;}]-&gt;(:Zone)</bold></p></list-item><list-item><label>-</label><p><bold>(:SpatialEvent)-[:ABOUT]-&gt;(:Object)</bold> (many&#8594;1)</p></list-item><list-item><label>-</label><p><bold>(:TemporalEvent)-[:COMPOSES]-&gt;(:SpatialEvent)</bold> (many&#8594;many)</p></list-item><list-item><label>-</label><p><bold>(:ComplexEvent)-[:COMPOSES]-&gt;(:TemporalEvent)</bold> (many&#8594;many)</p></list-item></list></p><p>The following includes the required integrity constraints and the indexes script to build the proposed KG (<xref rid="sensors-25-05331-box001" ref-type="boxed-text">Box 1</xref>):</p><boxed-text id="sensors-25-05331-box001" position="float" orientation="portrait"><label>Box 1.</label><caption><title>KG integrity constraints and indexes.</title></caption><p>// Uniqueness</p><p><bold>CREATE CONSTRAINT</bold> obj_gid <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">o</named-content>:Object) <bold>REQUIRE</bold>&#160;<named-content content-type="color:#0070C0">o</named-content>.global_id <bold>IS UNIQUE</bold>;</p><p><bold>CREATE CONSTRAINT</bold> loc_lid <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">l</named-content>:LocalObject) <bold>REQUIRE</bold> (<named-content content-type="color:#0070C0">l</named-content>.camera_id, <named-content content-type="color:#0070C0">l</named-content>.local_id) <bold>IS UNIQUE</bold>;</p><p><bold>CREATE CONSTRAINT</bold> trk_id<bold>&#160;&#160;IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">t</named-content>:Tracklet)&#160;&#160;&#160;&#160;<bold>REQUIRE</bold>&#160;<named-content content-type="color:#0070C0">t</named-content>.tracklet_id <bold>IS UNIQUE</bold>;</p><p><bold>CREATE CONSTRAINT</bold> traj_id <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">r</named-content>:Trajectory)&#160;&#160;<bold>REQUIRE</bold>&#160;<named-content content-type="color:#0070C0">r</named-content>.traj_id <bold>IS UNIQUE</bold>;</p><p><bold>CREATE CONSTRAINT</bold> zone_id <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">z</named-content>:Zone)&#160;&#160;<bold>REQUIRE</bold> (<named-content content-type="color:#0070C0">z</named-content>.zone_id, <named-content content-type="color:#0070C0">z</named-content>.version) <bold>IS UNIQUE</bold>;</p><p><bold>CREATE CONSTRAINT</bold> se_id<bold>&#160;&#160;&#160;IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">s</named-content>:SpatialEvent)&#160;&#160;<bold>REQUIRE</bold>&#160;<named-content content-type="color:#0070C0">s</named-content>.se_id <bold>IS UNIQUE</bold>;</p><p><bold>CREATE CONSTRAINT</bold> te_id <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">t</named-content>:TemporalEvent) <bold>REQUIRE</bold>&#160;<named-content content-type="color:#0070C0">t</named-content>.te_id <bold>IS UNIQUE</bold>;</p><p>&#8192;</p><p>// Query-speed indexes</p><p><bold>CREATE INDEX</bold> obs_ts <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">o</named-content>:Observation)&#160;&#160;<bold>ON</bold> (<named-content content-type="color:#0070C0">o</named-content>.ts);</p><p><bold>CREATE INDEX</bold> se_type_ts <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">s</named-content>:SpatialEvent)&#160;&#160;<bold>ON</bold> (<named-content content-type="color:#0070C0">s</named-content>.type, <named-content content-type="color:#0070C0">s</named-content>.ts);</p><p><bold>CREATE INDEX</bold> te_type_t <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">t</named-content>:TemporalEvent)&#160;&#160;<bold>ON</bold> (<named-content content-type="color:#0070C0">t</named-content>.type, <named-content content-type="color:#0070C0">t</named-content>.t_start);</p><p><bold>CREATE INDEX</bold> obj_class <bold>IF NOT EXISTS</bold></p><p><bold>FOR</bold> (<named-content content-type="color:#0070C0">o</named-content>:Object)&#160;&#160;&#160;&#160;<bold>ON</bold> (<named-content content-type="color:#0070C0">o</named-content>.class, <named-content content-type="color:#0070C0">o</named-content>.ts_last);</p></boxed-text><p>The proposed KG is scalable because the hierarchical layering (complex &#8594; temporal &#8594; spatial) enables efficient graph partitioning, indexing, and parallel traversal. Hierarchical layering also allows for progressive and modular event querying and processing. Furthermore, a clear distinction between geospatial and frame-based relationships supports hybrid spatial queries. Finally, the proposed KG accommodates multi-entity and multi-layered complex relationships by breaking down events into low-level, mid-level, and high-level events.</p></sec><sec id="sec3dot1dot2-sensors-25-05331"><title>3.1.2. GICEDCAM Data Pipeline</title><p>The GICEDCAM data pipeline performs data ingestion, transformation, and event detection. The main criteria for designing the GICEDCAM data pipeline are to increase scalability, reduce cost, and increase the accuracy of processing and detecting complex events in near real-time compared to traditional CED frameworks like VIDCEP [<xref rid="B15-sensors-25-05331" ref-type="bibr">15</xref>]. The GICEDCAM framework is designed as shown in <xref rid="sensors-25-05331-f002" ref-type="fig">Figure 2</xref>.</p><p>The GICEDCAM data pipeline comprises three computation layers: edge, stateless, and stateful. In the edge computation layer, low-level feature extraction is handled using edge computations. Low-level feature extraction converts raw frames and sensor readings into compact features. In the first task, objects are detected using a Convolutional Neural Network (CNN) for object detection. Honarparvar et al. demonstrated that YOLO is a suitable CNN architecture for detecting simple events or objects in videos [<xref rid="B35-sensors-25-05331" ref-type="bibr">35</xref>]. The next step is to track objects and identify them as unique objects in the camera streams. The DeepSORT algorithm has been widely used for tracking multiple objects in one video stream and is well-aligned with CNN object detection, such as YOLO [<xref rid="B36-sensors-25-05331" ref-type="bibr">36</xref>]. Other sensors (e.g., Inertial Measurement Unit (IMU), voice recorder, or temperature sensor) may also publish low-level features of the objects. The extracted low-level features should include time, location, and observation type as mandatory fields. Moving low-level feature extraction to the edge computation layer reduces latency and bandwidth since this layer publishes small-sized constructed data instead of large-sized camera streams. Moreover, the scalability is increased since edge devices easily scale linearly, and the computational costs are distributed to edge devices.</p><p>The stateless computation layer runs the processes that do not need the reserved state. This layer consists of three sublayers. In the first sublayer, observations are filtered and enriched with mobility status and geospatial zones. Filtering objects against the KG database reduces the number of rows for CED and increases computational efficiency. The filtering component queries the objects&#8217; type in the KG database and returns all complex-event patterns that involve any of the detected object types. The sample Cypher query to find registered objects is as follows (<xref rid="sensors-25-05331-box002" ref-type="boxed-text">Box 2</xref>):</p><boxed-text id="sensors-25-05331-box002" position="float" orientation="portrait"><label>Box 2.</label><caption><title>The sample Cypher query to find registered objects.</title></caption><p><bold>MATCH</bold>(<named-content content-type="color:#4472C4">ce</named-content>:ComplexEvent)-[:<bold>HAS_TEMPORAL</bold>]-&gt;(<named-content content-type="color:#4472C4">te</named-content>:TemporalEvent)-[:<bold>HAS_SPATIAL</bold>]-&gt;(<named-content content-type="color:#4472C4">se</named-content>:SpatialEvent)-[:<bold>INVOLVES</bold>]-&gt;(<named-content content-type="color:#4472C4">ob</named-content>:Object)</p><p><bold>WHERE</bold>&#160;<named-content content-type="color:#4472C4">op</named-content>.label = $label</p><p>
<bold>RETURN TRUE</bold>
</p></boxed-text><p>The next component enriches the objects by the mobility status by querying the objects&#8217; type from the KG database. The mobility status is either &#8220;fixed&#8221; or &#8220;moving&#8221;. Assigning the mobility status helps process fixed or moving objects in different ways. For example, a fixed object&#8217;s location could be simply an average of all recorded frames&#8217; locations. However, moving objects are required to be processed by more complicated algorithms (see <xref rid="sec3dot2dot3-sensors-25-05331" ref-type="sec">Section 3.2.3</xref>). Objects are also labelled based on the geospatial zones where they are located. The zone assignment would help reduce false negatives of spatial event detection (see <xref rid="sec3dot2dot3-sensors-25-05331" ref-type="sec">Section 3.2.3</xref>).</p><p>The next sublayer performs projection and trajectory enhancement. This component retrieves the enriched objects and projects them onto a geospatial plane. Some well-known approaches, such as homography-based transformation [<xref rid="B37-sensors-25-05331" ref-type="bibr">37</xref>] or camera calibration techniques [<xref rid="B38-sensors-25-05331" ref-type="bibr">38</xref>] transform the location of the detected object in the frame coordinate system to a geospatial coordinate system like the Universal Transverse Mercator (UTM). We provided the source code and samples of efficient coordinate transformations in the CameraObjectMapper v1.0 (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/sepehr89/CameraObjectMapper">https://github.com/sepehr89/CameraObjectMapper</uri> accessed on 1 March 2025) GitHub repository. For each detection of object <italic toggle="yes">o</italic> at time <italic toggle="yes">t</italic>, we select an image point (<italic toggle="yes">U<sub>t</sub></italic><sup>(<italic toggle="yes">o</italic>)</sup>, <italic toggle="yes">V<sub>t</sub></italic><sup>(<italic toggle="yes">o</italic>)</sup>) (center of the object&#8217;s bbox) and map it to world coordinates via the static camera homography <italic toggle="yes">H<sub>c</sub></italic> &#8712; R<sup>3&#215;3</sup> (one per camera). Equation (1) shows how to project the image coordinates into a geospatial planar coordinate system. The <italic toggle="yes">x</italic><sub>1</sub> and <italic toggle="yes">x</italic><sub>2</sub> are the unnormalized ground-plane coordinates, and <italic toggle="yes">x</italic><sub>3</sub> is the projective scale.<disp-formula id="FD1-sensors-25-05331"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The projected coordinates are combined with other location data taken from IMU, Global Positioning System (GPS), or Wi-Fi to reduce the false positives in spatial-event detection. For example, a person&#8217;s hand intersects with a door handle while he/she is far from the door. The component returns the person&#8217;s geospatial coordinates to determine proximity to the door handle. The next component of this sublayer enhances the trajectory data in terms of tracking identification accuracy and trajectory geometry. This component fixes the gaps in moving objects&#8217; trajectories and reduces false negatives in temporal event detection. More details are explained in <xref rid="sec3dot2dot3-sensors-25-05331" ref-type="sec">Section 3.2.3</xref>.</p><p>The mid-level event-matching sublayer handles spatial event matching and spatial event enhancement components. Both of these components are handled by stateless computations because spatial event matching considers objects&#8217; spatial relationships at a specific point in time and does not require storing the state. Spatial event detection matches the detected spatial relationships with patterns of frame-based spatial relationships and geospatial relationships in the KG database. The following Cypher query is an example of how we can detect a spatial event (<xref rid="sensors-25-05331-box003" ref-type="boxed-text">Box 3</xref>).</p><boxed-text id="sensors-25-05331-box003" position="float" orientation="portrait"><label>Box 3.</label><caption><title>Cypher query for spatial event detection.</title></caption><p><bold>MATCH</bold> (<named-content content-type="color:#4472C4">se</named-content>:SpatialEvent)-[:<bold>INVOLVES</bold>]-&gt;(<named-content content-type="color:#4472C4">o1</named-content>:Object),</p><p>&#8195;&#8195;&#8195;(<named-content content-type="color:#4472C4">se</named-content>)-[:<bold>INVOLVES</bold>]-&gt;(<named-content content-type="color:#4472C4">o2</named-content>:Object),</p><p>&#8195;&#8195;&#8195;(<named-content content-type="color:#4472C4">se</named-content>)-[:<bold>USES_RELATIONSHIP</bold>]-&gt;(<named-content content-type="color:#4472C4">r</named-content>:SpatialRule)</p><p><bold>WHERE</bold>&#160;<named-content content-type="color:#4472C4">o1</named-content>.label = &#8216;person&#8217; <bold>AND</bold>&#160;<named-content content-type="color:#4472C4">o2</named-content>.label = &#8216;car&#8217; <bold>AND</bold>&#160;<named-content content-type="color:#4472C4">r</named-content>.type = &#8216;near&#8217;</p><p><bold>RETURN</bold>&#160;<named-content content-type="color:#4472C4">se</named-content></p></boxed-text><p>However, spatial events may be overlooked due to various factors, including frame quality issues, false negatives in object detection, or occlusion phenomena. The primary objective of spatial event enhancement is to minimize the likelihood of false negatives by employing knowledge-based, data-driven, or trajectory-focused approaches. <xref rid="sec3dot2-sensors-25-05331" ref-type="sec">Section 3.2</xref> provides more details on how GICEDCAM identifies the gaps in spatial events. This stateless sublayer promotes scalability. The atomic processes are handled by one-time processing functions, which use resources more efficiently. These functions are triggered only when needed and terminate upon completion. Such a design significantly reduces processing costs. Additionally, these stateless computations enhance fault tolerance due to the modular nature of the computational units, as well as the loosely coupled microservices and components they comprise.</p><p>In the last layer, we leverage stateful computations to identify temporal relationships among spatial events and detect complex events. This functionality cannot be adequately managed through stateless processing, as temporal events are inherently not isolated or atomic; they encompass various factors such as pattern accumulation, the sequence of patterns, time windows, and the continuity of objects over time. In this layer, we account for event windows to detect patterns within fixed temporal intervals. We implemented session tracking to maintain the spatial events state for the defined event time window. Additionally, we utilize pattern memory to accumulate partial matches, which contribute towards the identification of full complex events. The sample Cypher query to match temporal and complex events with the KG patterns database is presented as follows (<xref rid="sensors-25-05331-box004" ref-type="boxed-text">Box 4</xref>):</p><boxed-text id="sensors-25-05331-box004" position="float" orientation="portrait"><label>Box 4.</label><caption><title>Cypher query to match temporal and complex events with the KG patterns database.</title></caption><p><bold>MATCH</bold>(<named-content content-type="color:#4472C4">ce</named-content>:ComplexEvent)-[:<bold>HAS_TEMPORAL</bold>]-&gt;(<named-content content-type="color:#4472C4">te</named-content>:TemporalEvent)</p><p>&#8195;&#8195;&#8195;-[:<bold>HAS_SPATIAL</bold>]-&gt;(<named-content content-type="color:#4472C4">se</named-content>:SpatialEventPattern)</p><p><bold>RETURN</bold>&#160;<named-content content-type="color:#4472C4">ce, te, se</named-content></p></boxed-text><p>Algorithm 1 provides more details on how the stateful event matching works. The stateful matcher in GICEDCAM operates over a stream of spatial events, a query window <italic toggle="yes">W</italic> (maximum allowed duration for a complex event), and a watermark <italic toggle="yes">&#969;</italic> (maximum event time observed minus allowed lateness). Each spatial event is represented as <italic toggle="yes">e</italic> = &#10216;type, bindings, <italic toggle="yes">t</italic>, conf, corrected&#10217;, where <italic toggle="yes">type</italic> is the spatial event predicate, <italic toggle="yes">bindings</italic> is the map from pattern variables to KG identifiers, <italic toggle="yes">t</italic> is the event time of the spatial event, <italic toggle="yes">conf</italic> is the event confidence, and <italic toggle="yes">corrected</italic> is a Boolean flag (true if emitted by the Spatial Event Corrector). The query pattern is compiled into a finite-state machine; the engine maintains partial matches <italic toggle="yes">M</italic>[<italic toggle="yes">q</italic>] keyed by automaton state <italic toggle="yes">q</italic> and current bindings, each storing {{bindings, start_time, last_time, (optional) confidence}}. Upon receipt of event <italic toggle="yes">e</italic>, the engine advances any partial whose next transition is enabled (i.e., the label matches <italic toggle="yes">e</italic>.type, the event&#8217;s bindings unify with the partial&#8217;s bindings without conflict, the required Allen-interval guard and gap constraints hold relative to last_time, and the resulting span (<italic toggle="yes">t</italic> &#8722; start_time) &#8804; <italic toggle="yes">W</italic> (optionally conf &#8805; <italic toggle="yes">&#952;</italic>)). If the pattern can start with <italic toggle="yes">e</italic>, a new partial is seeded with start_time = last_time = <italic toggle="yes">t</italic>. A partial that reaches an accepting state is emitted as a complex event only when (last_time &#8722; start_time) &#8804; <italic toggle="yes">W</italic> and last_time &#8804; <italic toggle="yes">&#969;</italic>, ensuring finality under out-of-order and corrected inputs. Corrected events are additive only; if their event time <italic toggle="yes">t</italic> &#8804; <italic toggle="yes">&#969;</italic> on arrival, they are too late. This makes the event matching robust to missing events since the corrector is additive; when imputed events arrive before <italic toggle="yes">&#969;</italic>, they can complete/extend matches without mutating prior facts. Finally, any partial with (now &#8722; start_time) &gt; <italic toggle="yes">W</italic> is pruned to bound memory and enforce the semantic duration. The pruning stage helps release the memory and reduces computational cost.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1.</bold> Stateful Complex Event Matching in GICEDCAM.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Input</bold>: stream of spatial events <named-content content-type="color:#0070C0">e </named-content>= &#10216;<named-content content-type="color:#0070C0">type</named-content>, <named-content content-type="color:#0070C0">bindings</named-content>, <named-content content-type="color:#0070C0">t</named-content>, <named-content content-type="color:#0070C0">conf</named-content>, <named-content content-type="color:#0070C0">corrected</named-content>&#10217;<break/>&#8195;&#8195;&#8195;window <named-content content-type="color:#0070C0">W</named-content>, watermark <named-content content-type="color:#0070C0">&#969;</named-content><break/><bold>State</bold>: partial matches <named-content content-type="color:#0070C0">M</named-content>[<named-content content-type="color:#0070C0">q</named-content>] keyed by (<named-content content-type="color:#0070C0">q</named-content>, <named-content content-type="color:#0070C0">bindings</named-content>)<break/><break/>upon spatial event <named-content content-type="color:#0070C0">e </named-content>at time <named-content content-type="color:#0070C0">t</named-content>:&#8195;&#8195;<named-content content-type="color:#70AD47">// events are pre-filtered &amp; projected</named-content><break/>&#8195;<named-content content-type="color:#70AD47">// 1) advance existing partials</named-content><break/>&#8195;<bold>for each</bold> (<named-content content-type="color:#0070C0">q</named-content>, <named-content content-type="color:#0070C0">b</named-content>) <bold>in</bold>&#160;<named-content content-type="color:#0070C0">M </named-content><bold>where</bold> &#948;(<named-content content-type="color:#0070C0">q</named-content>, <named-content content-type="color:#0070C0">e</named-content>.<named-content content-type="color:#0070C0">type</named-content>, <named-content content-type="color:#0070C0">b</named-content>, <named-content content-type="color:#0070C0">t</named-content>) <bold>is enabled</bold><break/>&#8195;&#8195;&#8195;<named-content content-type="color:#0070C0">q&#8242;</named-content> &#8592; &#948;(<named-content content-type="color:#0070C0">q</named-content>, <named-content content-type="color:#0070C0">e</named-content>.<named-content content-type="color:#0070C0">type</named-content>, <named-content content-type="color:#0070C0">b</named-content>, <named-content content-type="color:#0070C0">t</named-content>)&#8195;&#160;<named-content content-type="color:#70AD47">// checks Allen relation &amp; gap constraints</named-content><break/>&#8195;&#8195;&#8195;<named-content content-type="color:#0070C0">b&#8242;</named-content> &#8592; unify(<named-content content-type="color:#0070C0">b</named-content>, <named-content content-type="color:#0070C0">e</named-content>.<named-content content-type="color:#0070C0">bindings</named-content>)&#8195;&#8195;<named-content content-type="color:#70AD47">// variable binding; reject on conflict</named-content><break/>&#8195;&#8195;&#8195;<bold>upsert</bold>&#160;<named-content content-type="color:#0070C0">M</named-content>[<named-content content-type="color:#0070C0">q&#8242;</named-content>] <bold>with</bold> {<named-content content-type="color:#0070C0">bindings </named-content>= <named-content content-type="color:#0070C0">b&#8242;</named-content>, <named-content content-type="color:#0070C0">start_time</named-content>(<named-content content-type="color:#0070C0">b</named-content>), <named-content content-type="color:#0070C0">last_time </named-content>= <named-content content-type="color:#0070C0">t</named-content>}<break/>&#8195;<named-content content-type="color:#70AD47">// 2) possibly start a new partial</named-content><break/>&#8195;<bold>if</bold> &#948;(<named-content content-type="color:#0070C0">q0</named-content>, <named-content content-type="color:#0070C0">e</named-content>.<named-content content-type="color:#0070C0">type</named-content>, <named-content content-type="color:#0070C0">e</named-content>.<named-content content-type="color:#0070C0">bindings</named-content>, <named-content content-type="color:#0070C0">t</named-content>) <bold>enabled</bold><break/>&#8195;&#8195;&#8195;<bold>upsert</bold>&#160;<named-content content-type="color:#0070C0">M</named-content>[<named-content content-type="color:#0070C0">q1</named-content>] <bold>with</bold> {<named-content content-type="color:#0070C0">bindings </named-content>= <named-content content-type="color:#0070C0">e</named-content>.<named-content content-type="color:#0070C0">bindings</named-content>, <named-content content-type="color:#0070C0">start_time </named-content>= <named-content content-type="color:#0070C0">t</named-content>, <named-content content-type="color:#0070C0">last_time </named-content>= <named-content content-type="color:#0070C0">t</named-content>}<break/>&#8195;<named-content content-type="color:#70AD47">// 3) emit completes when safe</named-content><break/>&#8195;<bold>for each</bold> partial <bold>in</bold>&#160;<named-content content-type="color:#0070C0">M </named-content><bold>with</bold> state <named-content content-type="color:#0070C0">q </named-content><bold>&#8712;</bold>&#160;<named-content content-type="color:#0070C0">F</named-content><break/>&#8195;&#8195;&#8195;<bold>if</bold> (<named-content content-type="color:#0070C0">last_time </named-content>&#8722; <named-content content-type="color:#0070C0">start_time</named-content>) &#8804; <named-content content-type="color:#0070C0">W</named-content>&#8195;<bold>and</bold>&#8195;<named-content content-type="color:#0070C0">last_time</named-content> &#8804; <named-content content-type="color:#0070C0">&#969;</named-content><break/>&#8195;&#8195;&#8195;&#8195;&#8195;<bold>emit</bold> ComplexEvent(partial.bindings, [<named-content content-type="color:#0070C0">start_time</named-content>, <named-content content-type="color:#0070C0">last_time</named-content>])<break/>&#8195;&#8195;&#8195;&#8195;&#8195;<bold>delete</bold> partial<break/>&#8195;<named-content content-type="color:#70AD47">// 4) prune stale partials (semantic bound)</named-content><break/>&#8195;delete any partial with (<named-content content-type="color:#0070C0">current_time </named-content>&#8722; <named-content content-type="color:#0070C0">start_time</named-content>) &gt; <named-content content-type="color:#0070C0">W</named-content></td></tr></tbody></array></p></sec></sec><sec id="sec3dot2-sensors-25-05331"><title>3.2. Spatial Event Corrector</title><p>The Spatial Event Corrector is triggered when the system identifies a potential gap in spatial event detection based on KG-defined patterns. The Spatial Event Corrector runs in parallel with spatial-event detection. It estimates the probability of a missing event using contextual cues and past observations. For instance, consider a scenario where a person is observed at time (t<sub>0</sub>) and a bottle positioned at an intersection with a table is noted at time (t<sub>1</sub>). Subsequently, at time (t<sub>2</sub>), the individual is seen standing near the table, apparently preparing to &#8220;<italic toggle="yes">pick up the bottle</italic>&#8221;, but this action goes undetected due to the occlusion of the bottle by the person&#8217;s body. By the time (t<sub>3</sub>), the object detector successfully identifies the person holding the bottle. Further, the system detects the individual opening the door and exiting the room at time (t<sub>4</sub>). The observation that the person holds the bottle after passing the table indicates that the bottle was picked up earlier.</p><p>In this context, the GICEDCAM framework employs the Spatial Event Corrector component to initially identify potential gaps in spatial event detection and subsequently predict these missing spatial events with a calculated level of confidence, utilizing contextual clues and the knowledge graph. The Spatial Event Corrector in GICEDCAM evaluates three representative approaches, including Bayesian Networks (BN), Long Short-Term Memory (LSTM), and trajectory analysis, which are chosen for their complementary strengths in addressing uncertainty, temporal dependencies, and spatial continuity. BN was selected for its capability to explicitly represent probabilistic dependencies among spatial events and perform inference with incomplete or noisy observations, as demonstrated in hierarchical reasoning approaches such as Li et al. [<xref rid="B23-sensors-25-05331" ref-type="bibr">23</xref>]. LSTM was included due to its proven capacity to capture long-range temporal dynamics in sequential event data, consistent with the temporal modelling advantages highlighted in Kang et al. [<xref rid="B25-sensors-25-05331" ref-type="bibr">25</xref>]. The trajectory analysis method was chosen for its computational efficiency and interpretability, leveraging geometric movement constraints to correct missing events, in line with object movement modelling seen in MERN [<xref rid="B27-sensors-25-05331" ref-type="bibr">27</xref>]. While alternative methods such as Graph Neural Networks (GNN) can model complex relational structures in spatiotemporal graphs, our selection prioritized methods with lower training data requirements, established interpretability, and reduced computational overhead; these are all key considerations for real-time, resource-constrained deployments. Future extensions of GICEDCAM may explore GNN-based correctors when computational budgets and labelled training data are sufficient. In this section, we explain the approaches in detail, and in <xref rid="sec4dot3-sensors-25-05331" ref-type="sec">Section 4.3</xref>, we will compare them in terms of accuracy and latency.</p><p>Algorithm 2 explains more details on the Spatial Event Corrector component. The corrector operates on active expectations <italic toggle="yes">E</italic><sub><italic toggle="yes">S</italic></sub> = &#10216;<italic toggle="yes">S</italic>, bindings, [<italic toggle="yes">t</italic><sub><italic toggle="yes">a</italic></sub>,<italic toggle="yes">t</italic><sub><italic toggle="yes">b</italic></sub>]&#10217; derived from the query pattern (anchors define when a spatial predicate <italic toggle="yes">S</italic> should occur), an observation buffer <italic toggle="yes">O</italic><sub><italic toggle="yes">K</italic></sub> containing the last <italic toggle="yes">K</italic> seconds of projected positions <italic toggle="yes">x</italic>(<italic toggle="yes">t</italic>), fuzzy zone memberships <italic toggle="yes">&#956;</italic><sub><italic toggle="yes">Z</italic></sub>(<italic toggle="yes">x</italic>) class labels, and frame-topology flags, and a configuration that selects exactly one method &#8712; {BN, LSTM, TRAJ} together with thresholds (<italic toggle="yes">&#964;</italic><sub>BN</sub>, <italic toggle="yes">&#964;</italic><sub>LSTM</sub>, <italic toggle="yes">&#964;</italic><sub>traj</sub>), a small probe delay &#916;probe (patience before correction), de-duplication (dedup) tolerance <italic toggle="yes">&#949;</italic><sub><italic toggle="yes">t</italic></sub>, padding &#948;, and a wall-clock budget B ms. Method-specific parameters are: BN structure/CPDs &#920;BN; LSTM model weights <italic toggle="yes">M</italic><sub>LSTM</sub> (plus window length/stride); and trajectory assets (ROI maps <italic toggle="yes">&#956;</italic><sub>ROI</sub>, historical exemplars <italic toggle="yes">H</italic>, FastDTW radius <italic toggle="yes">r</italic>, blend <italic toggle="yes">&#946;</italic>). For each <italic toggle="yes">E</italic><sub><italic toggle="yes">S</italic></sub>, the component performs gap detection: if the current time is &#8805;<italic toggle="yes">t</italic><sub><italic toggle="yes">b</italic></sub>+&#916;<sub>probe</sub> and no raw instance of <italic toggle="yes">S</italic> with the given bindings exists in [<italic toggle="yes">t</italic><sub><italic toggle="yes">a</italic></sub>, <italic toggle="yes">t</italic><sub><italic toggle="yes">b</italic></sub>], it proceeds; otherwise, it returns. It then assembles evidence by slicing <italic toggle="yes">O</italic><sub><italic toggle="yes">K</italic></sub> over [<italic toggle="yes">t</italic><sub><italic toggle="yes">a</italic></sub>&#8722;<italic toggle="yes">&#948;</italic>, <italic toggle="yes">t</italic><sub><italic toggle="yes">b</italic></sub>+<italic toggle="yes">&#948;</italic>]. In BN mode, it runs exact inference to obtain <italic toggle="yes">p</italic><sub><italic toggle="yes">t</italic></sub> = P<sub>r</sub>[<italic toggle="yes">S</italic> = 1&#8739;<italic toggle="yes">E</italic><sub><italic toggle="yes">t</italic></sub>] and selects <italic toggle="yes">t</italic>* = argmax<italic toggle="yes">t</italic> &#8712; [<italic toggle="yes">t</italic><sub><italic toggle="yes">a</italic></sub>,<italic toggle="yes">t</italic><sub><italic toggle="yes">b</italic></sub>]<italic toggle="yes">p</italic><sub><italic toggle="yes">t</italic></sub>; if <italic toggle="yes">p</italic><sub><italic toggle="yes">t</italic></sub>* &#8805; <italic toggle="yes">&#964;</italic><sub>BN</sub>, it emits with conf = <italic toggle="yes">p</italic><sub><italic toggle="yes">t</italic></sub>*. In LSTM mode, it evaluates the sequence with weights <italic toggle="yes">M</italic><sub>LSTM</sub> over sliding windows on [<italic toggle="yes">t</italic><sub><italic toggle="yes">a</italic></sub>, <italic toggle="yes">t</italic><sub><italic toggle="yes">b</italic></sub>], chooses <italic toggle="yes">t</italic>* = argmax <italic toggle="yes">p</italic><sub><italic toggle="yes">t</italic></sub>, and emits if <italic toggle="yes">p</italic><sub><italic toggle="yes">t</italic></sub>* &#8805; <italic toggle="yes">&#964;</italic><sub>LSTM</sub>. In trajectory mode, it partitions the trajectory by MDL, scores candidate segments via a blend <italic toggle="yes">&#946;</italic><sub>FastDTW_sim</sub> + (1&#8722;<italic toggle="yes">&#946;</italic>)<italic toggle="yes">&#956;</italic><sup>&#8722;</sup><sub>ROI</sub>, and, if the best score &#8805; <italic toggle="yes">&#964;</italic><sub>traj</sub>, sets <italic toggle="yes">t</italic>* = argmax<sub><italic toggle="yes">p</italic>&#8712;<italic toggle="yes">&#963;</italic></sub>
<italic toggle="yes">&#956;</italic><sub>ROI</sub>(<italic toggle="yes">p</italic>) with conf = score. Before emission, the corrector de-duplicates against raw observations: if a raw <italic toggle="yes">S</italic> exists within <italic toggle="yes">&#949;</italic><sub><italic toggle="yes">t</italic></sub> of <italic toggle="yes">t</italic>*, it suppresses the correction. Otherwise, it adds (never mutates) a new spatial-event record &#10216;type = <italic toggle="yes">S</italic>, bindings, <italic toggle="yes">t</italic>*, conf, corrected = true&#10217;. Finality and eligibility are enforced downstream by the stateful matcher (watermark <italic toggle="yes">&#969;</italic> and window <italic toggle="yes">W</italic>); the corrector itself remains stateless and respects the budget <italic toggle="yes">B</italic>. More details on LSTM, BN, and trajectory-based methods are explained in <xref rid="sec3dot2dot1-sensors-25-05331" ref-type="sec">Section 3.2.1</xref>, <xref rid="sec3dot2dot2-sensors-25-05331" ref-type="sec">Section 3.2.2</xref> and <xref rid="sec3dot2dot3-sensors-25-05331" ref-type="sec">Section 3.2.3</xref>.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2.</bold> Spatial Event Corrector.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Inputs</bold>:<break/>&#8195;Active expectations E_S = &#10216;S, bindings, [t_a, t_b]&#10217; <named-content content-type="color:#70AD47">// from pattern context &amp; anchors</named-content><break/>&#8195;Observation buffer&#8195;&#160;O_K&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#70AD47">// last K s of {x(t), &#956;_Z(x), class, bbox flags}</named-content><break/>&#8195;Config&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#160;method &#8712; {BN, LSTM, TRAJ}, &#8710;_probe, &#949;_t, &#948;, B<break/>&#8195;Thresholds&#8195;&#8195;&#8195;&#8195;&#8195;&#160;&#964;_BN, &#964;_LSTM, &#964;_TRAJ<break/>&#8195;BN params&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#920;_BN<break/>&#8195;LSTM params&#8195;&#8195;&#8195;&#8195;&#8195;M_LSTM (model weights), T (window), stride<break/>&#8195;Trajectory params&#8195;&#8195;&#956;_ROI(&#183;), exemplars H, FastDTW radius r, blend &#946;<break/><bold>State</bold>: none (stateless; queries only)<break/><bold>procedure</bold> CORRECTOR(<named-content content-type="color:#2E74B5">E_S</named-content>):<break/>&#8195;<named-content content-type="color:#70AD47">// ----- GAP DETECTION -----</named-content><break/>&#8195;<bold>let</bold> &#10216;<named-content content-type="color:#2E74B5">S</named-content>, <named-content content-type="color:#2E74B5">bindings</named-content>, [<named-content content-type="color:#2E74B5">t_a</named-content>, <named-content content-type="color:#2E74B5">t_b</named-content>]&#10217; = <named-content content-type="color:#2E74B5">E_S</named-content><break/>&#8195;<bold>if</bold> now &lt; t_b + &#916;_probe: <bold>return</bold>&#8195;&#8195;&#8195;&#8195;&#8195;&#160;<named-content content-type="color:#70AD47">// wait a small patience window</named-content><break/>&#8195;<bold>if exists</bold> RAW SpatialEvent(<named-content content-type="color:#2E74B5">type </named-content>= <named-content content-type="color:#2E74B5">S</named-content>, <named-content content-type="color:#2E74B5">bindings</named-content>, <named-content content-type="color:#2E74B5">t_raw </named-content><bold>&#8712;</bold> [<named-content content-type="color:#2E74B5">t_a</named-content>, <named-content content-type="color:#2E74B5">t_b</named-content>]) in <named-content content-type="color:#2E74B5">O_K</named-content>:<break/>&#8195;&#8195;&#8195;&#160;<bold>return</bold>&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#160;<named-content content-type="color:#70AD47">// no gap &#8594; nothing to correct</named-content><break/>&#8195;<named-content content-type="color:#70AD47">// ----- EVIDENCE ASSEMBLY -----</named-content><break/>&#8195;<named-content content-type="color:#2E74B5">E </named-content>&#8592; slice O_K for variables in &#8216;bindings&#8217; over [t_a &#8722; &#948;, t_b + &#948;]<break/>&#8195;<bold>if</bold>&#160;<named-content content-type="color:#2E74B5">E </named-content>is <bold>empty</bold>: <bold>return</bold><break/>&#8195;<named-content content-type="color:#70AD47">// ----- SINGLE-METHOD IMPUTATION (chosen by config) -----</named-content><break/>&#8195;<bold>switch</bold> method:<break/>&#8195;&#8195;<bold>case</bold>&#160;<named-content content-type="color:#2E74B5">BN</named-content>:<break/>&#8195;&#8195;&#8195;&#8230;<break/>&#8195;&#8195;&#8195;<bold>if</bold>&#160;<named-content content-type="color:#2E74B5">p </named-content>&lt; <named-content content-type="color:#2E74B5">&#964;_BN</named-content>: <bold>return</bold><break/>&#8195;&#8195;&#8195;<named-content content-type="color:#2E74B5">conf</named-content> &#8592; <named-content content-type="color:#2E74B5">p</named-content><break/>&#8195;&#8195;<bold>case</bold>&#160;<named-content content-type="color:#2E74B5">LSTM</named-content>:<break/>&#8195;&#8195;&#8195;&#8230;<break/>&#8195;&#8195;&#8195;<bold>if</bold>&#160;<named-content content-type="color:#2E74B5">p_t</named-content>(<named-content content-type="color:#2E74B5">t</named-content>*) &lt; <named-content content-type="color:#2E74B5">&#964;_LSTM</named-content>: <bold>return</bold><break/>&#8195;&#8195;&#8195;<bold>conf</bold> &#8592; <named-content content-type="color:#2E74B5">p_t</named-content>(<named-content content-type="color:#2E74B5">t</named-content>*)<break/>&#8195;&#8195;<bold>case</bold> TRAJ:<break/>&#8195;&#8195;&#8195;&#8230;<break/>&#8195;&#8195;&#8195;<bold>if</bold> best = &#216; <bold>or</bold> best.<named-content content-type="color:#2E74B5">score </named-content>&lt; <named-content content-type="color:#2E74B5">&#964;_TRAJ</named-content>: <bold>return</bold><break/>&#8195;&#8195;&#8195;<named-content content-type="color:#2E74B5">t</named-content>* &#8592; <bold>argmax</bold>_{<named-content content-type="color:#2E74B5">p</named-content>&#8712;best.<named-content content-type="color:#2E74B5">&#963;</named-content>} &#956;_ROI(<named-content content-type="color:#2E74B5">p</named-content>)&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#160;<named-content content-type="color:#70AD47">// most plausible time inside </named-content>segment<break/>&#8195;&#8195;&#8195;<bold>conf</bold> &#8592; best.<named-content content-type="color:#2E74B5">score</named-content><break/>&#8195;<named-content content-type="color:#70AD47">// ----- DEDUP AGAINST RAW EVENTS -----</named-content><break/>&#8195;<bold>if exists</bold> RAW SpatialEvent(<named-content content-type="color:#2E74B5">type </named-content>= <named-content content-type="color:#2E74B5">S</named-content>, <named-content content-type="color:#2E74B5">bindings</named-content>, t_raw) <bold>with</bold> |<named-content content-type="color:#2E74B5">t_raw </named-content>&#8722; <named-content content-type="color:#2E74B5">t</named-content>*| &#8804; <named-content content-type="color:#2E74B5">&#949;_t</named-content> in <named-content content-type="color:#2E74B5">O_K</named-content>:<break/>&#8195;&#8195;&#8195;<bold>return</bold>&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;&#160;<named-content content-type="color:#70AD47">// prefer raw observation</named-content><break/>&#8195;<named-content content-type="color:#70AD47">// ----- EMIT ADDITIVE CORRECTED EVENT -----</named-content><break/>&#8195;<bold>emit</bold> SpatialEvent &#10216;type = <named-content content-type="color:#2E74B5">S</named-content>, bindings, <named-content content-type="color:#2E74B5">t</named-content>* = <named-content content-type="color:#2E74B5">t</named-content>*, conf = <named-content content-type="color:#2E74B5">conf</named-content>, corrected = <bold>true</bold>&#10217;<break/>&#8195;<named-content content-type="color:#70AD47">// ----- TIME BUDGET GUARD (optional) -----</named-content><break/>&#8195;<bold>ensure wall-clock</bold> time &#8804; <named-content content-type="color:#2E74B5">B </named-content>ms (degrade by skipping LSTM or lowering DTW radius if needed)</td></tr></tbody></array></p><sec id="sec3dot2dot1-sensors-25-05331"><title>3.2.1. Bayesian Networks</title><p>Bayesian Networks (BNs) are widely used for event prediction and predictive analytics. [<xref rid="B39-sensors-25-05331" ref-type="bibr">39</xref>]. They are particularly effective in managing missing data, seamlessly integrating domain knowledge [<xref rid="B40-sensors-25-05331" ref-type="bibr">40</xref>], and demonstrating high efficacy with small sample sizes [<xref rid="B41-sensors-25-05331" ref-type="bibr">41</xref>]. Additionally, BNs are beneficial in preventing overfitting, making them versatile for various predictive tasks [<xref rid="B42-sensors-25-05331" ref-type="bibr">42</xref>]. A BN is a probabilistic graphical model that represents a set of variables and their conditional dependencies using a directed acyclic graph (DAG). In this network, each node corresponds to a random variable, while the edges signify the probabilistic dependencies that exist between these variables. The network is characterized by Conditional Probability Distributions (CPDs), which quantitatively describe how the probability of a specific variable is influenced by its parent nodes.</p><p>BNs are useful in CED, particularly when some events are missing or uncertain. They facilitate probabilistic reasoning, allowing for inference even when only partial data is accessible. The foundation of a BN lies in Bayes&#8217; theorem, which is succinctly encapsulated in Equation (2). This equation estimates the probability of the evidence <italic toggle="yes">E</italic>, under the condition <italic toggle="yes">C,</italic> while <italic toggle="yes">H</italic> is considered our hypothesis. More details of the BN approach are explained in <xref rid="secAdot1-sensors-25-05331" ref-type="sec">Appendix A.1</xref>.<disp-formula id="FD3-sensors-25-05331"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mo mathsize="70%">&#8721;</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This approach incurs little latency and computational overhead, but its effectiveness heavily relies on the probability values assigned by experts to each node in the BN. These assigned values influence the accuracy of detecting missing events. Therefore, in this paper, we use a dynamic probability estimation method to update CPD values in real-time. Specifically, we define a time window (e.g., 8 h) during which we calculate the likelihood of observing specific CPD values for the BN nodes (e.g., &#8220;<italic toggle="yes">person near the table</italic>&#8221;) at regular intervals. For the initial time window, we utilize the probability values determined by experts.</p></sec><sec id="sec3dot2dot2-sensors-25-05331"><title>3.2.2. Long Short-Term Memory</title><p>LSTM networks, a specialized type of Recurrent Neural Networks (RNNs), are particularly adept at modelling sequential dependencies within time-series data [<xref rid="B43-sensors-25-05331" ref-type="bibr">43</xref>]. When some events are occluded or not explicitly observed in a dataset, LSTM can leverage past and future contexts to infer their occurrence [<xref rid="B44-sensors-25-05331" ref-type="bibr">44</xref>]. Traditional probabilistic approaches, such as BN, require predefined CPDs, but LSTMs dynamically learn patterns from training data, rendering them particularly effective for estimating missing events in real-world applications, such as video-based event detection. Furthermore, while BNs struggle to capture the temporal dependencies inherent in event sequences, LSTMs excel at identifying and learning time-series patterns [<xref rid="B45-sensors-25-05331" ref-type="bibr">45</xref>].</p><p>LSTMs operate by maintaining a memory state that selectively retains relevant information from past time steps using gates (input, forget, and output gates). When an event is missing, the network reconstructs the missing pattern based on previously observed movement sequences. For example, in a human-object interaction scenario, if a &#8220;<italic toggle="yes">bottle pickup</italic>&#8221; event is occluded, but the person&#8217;s trajectory suggests movement near a table followed by the bottle appearing in their possession, the LSTM can learn this transition from historical data and predict the missing event with a high degree of confidence. This capability arises from the LSTM&#8217;s ability to capture spatiotemporal dependencies, allowing it to infer events based on temporal patterns rather than relying solely on instantaneous observations. More details of LSTM approach is elaborated in <xref rid="secAdot2-sensors-25-05331" ref-type="sec">Appendix A.2</xref>.</p><p>LSTMs depend heavily on data quality and can achieve high accuracy if the prediction model is well-trained. However, the requirement for labelled data makes this approach less cost-effective. Furthermore, in the existing stateless architecture, LSTM-trained models need to be loaded each time the Spatial Event Corrector is triggered. This repeated loading consumes significant memory resources and contributes to increased latency.</p></sec><sec id="sec3dot2dot3-sensors-25-05331"><title>3.2.3. Trajectory Analysis</title><p>Complex events in videos involve moving and fixed objects. In general, a complex event cannot occur unless at least one object changes its location across consecutive frames. Therefore, tracking objects&#8217; locations and understanding objects&#8217; trajectory patterns can play a significant role in detecting complex events. When dynamic trajectory patterns are correctly identified, missing spatial or temporal events can also be inferred more reliably.</p><p>Trajectories provide valuable information about spatiotemporal patterns of moving objects [<xref rid="B46-sensors-25-05331" ref-type="bibr">46</xref>]. We can partition and cluster trajectories to see where and when moving objects follow unique patterns [<xref rid="B47-sensors-25-05331" ref-type="bibr">47</xref>]. Subsequently, these identified patterns can be integrated with semantic data to infer potential activities occurring at specific Points of Interest (POIs) or Regions of Interest (ROIs) [<xref rid="B48-sensors-25-05331" ref-type="bibr">48</xref>]. A variety of methods have been developed that rely solely on trajectory data for activity recognition, including neural networks [<xref rid="B49-sensors-25-05331" ref-type="bibr">49</xref>], Hierarchical Hidden Markov Model (HHMM) [<xref rid="B50-sensors-25-05331" ref-type="bibr">50</xref>], Principal Component Analysis (PCA) [<xref rid="B51-sensors-25-05331" ref-type="bibr">51</xref>], and Latent Dirichlet Allocation (LDA) [<xref rid="B52-sensors-25-05331" ref-type="bibr">52</xref>]. However, these methods require manual labelling or the assignment of probabilistic transition values. Moreover, these methods often involve complex computations that demand considerable computational resources. On the other hand, there is an opportunity to enhance analysis by leveraging complementary information, such as the locations of static objects and relevant spatial events.</p><p><xref rid="sensors-25-05331-f003" ref-type="fig">Figure 3</xref> presents a detailed workflow for identifying missing events through trajectory data analysis. The first step is to query historical trajectory datasets which are related to the target complex event and the potential spatial event. If a trajectory is found, we run the trajectory similarity function to calculate the similarity between the current trajectory and the found historical trajectory. We can claim that two objects did similar activities if their trajectories are similar enough [<xref rid="B53-sensors-25-05331" ref-type="bibr">53</xref>]. Hence, we conclude that the missing spatial event occurred at the corresponding location on the established complex event&#8217;s trajectory.</p><p>Among trajectory similarity calculation algorithms, Dynamic Time Warping (DTW) stands out for its high accuracy in measuring temporal similarities [<xref rid="B54-sensors-25-05331" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-05331" ref-type="bibr">55</xref>]. However, DWT suffers from high complexity <italic toggle="yes">O</italic>(<italic toggle="yes">n</italic><sup>2</sup>) due to the nonlinear calculation nature [<xref rid="B56-sensors-25-05331" ref-type="bibr">56</xref>]. To address this limitation, FastDTW has been developed as a linear approximation of DTW, which significantly enhances the computational speed when analyzing large datasets. [<xref rid="B57-sensors-25-05331" ref-type="bibr">57</xref>]. Unlike the traditional DTW method, which necessitates the computation of a full <italic toggle="yes">O</italic>(<italic toggle="yes">n</italic><sup>2</sup>) distance matrix, FastDTW only refines certain regions, thereby accelerating the overall computation process. The FastDTW algorithm operates by reducing the trajectory size to <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> at each level k, executing the standard DTW on a coarser resolution. Then it applies DTW at each level, resulting in complexity that can vary between <italic toggle="yes">O</italic>(<italic toggle="yes">n</italic>) to <italic toggle="yes">O</italic>(<italic toggle="yes">nlog</italic>(<italic toggle="yes">n</italic>)). After applying FastDTW calculations, the similarity is calculated based on Equation (3), and if it exceeds the threshold of 90%, the spatial event location from the similar trajectory is adopted and subsequently reported as the missing event for the current trajectory.<disp-formula id="FD4-sensors-25-05331"><label>(3)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Meanwhile, we check whether we previously extracted ROIs from the datasets. In this context, ROIs represent potential areas where spatial events may occur. For example, a buffered zone around a table could serve as a probable location for the &#8220;<italic toggle="yes">picking up a bottle</italic>&#8221; spatial event. ROIs can be manually determined by defining the geometry and location of the region of interest. Alternatively, they may be estimated based on observational data, such as the detected location of a table, along with a buffer zone surrounding it. These ROIs can overlap since some regions have the potential for more than one spatial event occurrence. For example, certain regions have a high likelihood of both &#8220;person holds bottle&#8221; and &#8220;near table&#8221; events. Therefore, ROIs are conceptualized as fuzzy regions based on the probability of event occurrence. <xref rid="sensors-25-05331-f004" ref-type="fig">Figure 4</xref> illustrates an example of fuzzy ROIs in an area. There are three ROIs. ROI<sub>A</sub> identifies the ROI for a person who picks up the bottle on the table. The closer the person is to location A, the more membership value he gets for ROI<sub>A</sub>. The ROI<sub>B</sub> is the region where the person holds the bottle and moves towards the door. The ROI<sub>C</sub> represents the area where it is most likely for a person to pause and open the door. Fuzzy membership functions can be formulated uniquely for each ROI. For example, the membership value of ROI<sub>A</sub> could be quantified in Equation (4), where <italic toggle="yes">R</italic> is the maximum radius of ROI from point A.<disp-formula id="FD5-sensors-25-05331"><label>(4)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8805;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>&#160;</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>&#8804;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>O</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These ROIs intersect with the partitioned trajectories. The points within the trajectory segment that intersect these ROIs are identified and assigned membership values based on the corresponding fuzzy function for each ROI. Then the detection time range of all intersected points is calculated, and if they fall within the allowed time range, the points are selected as the missing events. The confidence level of the missing event is reported as the average of the fuzzy membership values of all intersected points.</p><p>Trajectory partitioning is another essential component of detecting missing events. The objective of this component is to divide the trajectory points into non-overlapping segments that have different movement patterns. Mashud et al. proposed a partitioning algorithm based on the Minimum Description Length (MDL) with the complexity of <italic toggle="yes">O</italic>(<italic toggle="yes">N</italic>) [<xref rid="B58-sensors-25-05331" ref-type="bibr">58</xref>]. MDL finds the optimal values of precision and conciseness of the trajectory partitioning based on Equation (5).<disp-formula id="FD6-sensors-25-05331"><label>(5)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8869;</mml:mo></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8869;</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the perpendicular distance, and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the angular distance between consecutive points. The objective of MDL is to find the characteristic points (i.e., the points where the movement pattern changes). To achieve this, the trajectory is segmented, and the MDL cost is calculated for each segment both before and after the proposed split. If the MDL cost is reduced, the corresponding point is accepted as a characteristic point, leading to further segmentation of adjacent parts of the trajectory. We used this algorithm to partition trajectories since it is well-suited for activity recognition with large datasets. This method is based purely on trajectory data and is fast. Moreover, it does not require labelling and does not highly depend on domain knowledge. However, this method&#8217;s accuracy is highly dependent on stop and movement pattern recognition and is sensitive to abrupt high-speed movements.</p></sec></sec><sec id="sec3dot3-sensors-25-05331"><title>3.3. Real-Time Trajectories Corrector</title><p>The spatial and attribute accuracy of trajectories significantly impacts the accuracy of complex events. Therefore, GICEDCAM should address and fix trajectory data in real-time before it enters the event-matching step. There are two major issues in analyzing moving object trajectories. The first challenge pertains to incorrect tracking identification due to occlusions or disjoint cameras&#8217; Field of View (FOV). This issue is addressed by the tracking re-identifier component in <xref rid="sec3dot3dot1-sensors-25-05331" ref-type="sec">Section 3.3.1</xref>. The second issue involves noise and disturbances in trajectory data due to random errors in object detection or inconsistencies between the camera inputs and other sensors&#8217; observations. <xref rid="sec3dot3dot2-sensors-25-05331" ref-type="sec">Section 3.3.2</xref> introduces the trajectory of the spatial enhancer as a component to fix this issue.</p><sec id="sec3dot3dot1-sensors-25-05331"><title>3.3.1. Tracking Re-Identifier</title><p>To detect complex events in a network of smart cameras and sensors, we need to track objects across different camera FOVs (i.e., inter-camera tracking). Additionally, object occlusions, object detection false negatives, or frame light inconsistencies might cause gaps in moving object tracking (i.e., intra-camera tracking). DeepSORT is a widely used algorithm that operates in both inter- and intra-camera tracking, has low complexity, and delivers high accuracy for both small and large-scale datasets [<xref rid="B59-sensors-25-05331" ref-type="bibr">59</xref>]. However, DeepSORT remains vulnerable to appearance variations and is sensitive to detection errors [<xref rid="B60-sensors-25-05331" ref-type="bibr">60</xref>]. This means that if an object is not detected for several consecutive frames, DeepSORT concludes that the object is no longer present.</p><p>In some cases, considering the continuity of spatial events (e.g., a person holds a bottle) based on the KG pattern would help match tracklets more efficiently. For example, three people left the room, which is monitored by camera A, and one of them held the bottle, and all of them left the room at the same time, while camera B captured a person holding a bottle. Then the continuity of the &#8220;holding a bottle&#8221; spatial event can be evidence that the trajectories seen in camera A and camera B belong to the same person. Therefore, we propose a modified DeepSORT algorithm (event-aware DeepSORT) that integrates the semantics of spatial events into the tracking process. This enhancement aims to improve both the performance and accuracy of object tracking. <xref rid="sensors-25-05331-f005" ref-type="fig">Figure 5</xref> illustrates the linear workflow of the event-aware DeepSORT algorithm.</p><p>In the first step, for each camera, moving objects are tracked, and local tracking IDs are assigned to them using the standard DeepSORT algorithm. Standard DeepSORT tracks objects using appearance embeddings, motion, and timestamped features. In the next step, a similarity graph of tracklets is built based on the appearance, motion, and spatial event features of tracklets. Spatial events are detected and assigned to tracklets using the spatial event detector component. In the similarity graph, nodes are tracklets from all cameras, and edges are built based on the affinity scores (i.e., association score), which are calculated based on Equation (6).<disp-formula id="FD7-sensors-25-05331"><label>(6)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>0,1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>S<sub>apperance</sub> is the cosine similarity of the appearance features of tracklets. To calculate S<sub>apperance</sub>, the appearance of the tracklets is stored in two vectors <italic toggle="yes">f<sub>1</sub></italic> and <italic toggle="yes">f<sub>2</sub></italic>. We can consider the last <italic toggle="yes">k</italic> tracklets average for more robust tracklet matching. Equation (7) provides more details on how the appearance score is calculated.<disp-formula id="FD8-sensors-25-05331"><label>(7)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>.</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (6), S<sub>motion</sub> is calculated based on the estimated time and the position of moving objects between tracklets. DeepSORT uses the Kalman Filter (KF) to predict the next tracklet location. The S<sub>motion</sub> is calculated based on Equation (8). In this equation, <italic toggle="yes">d<sub>M</sub></italic> is the Mahalanobis distance, <italic toggle="yes">x</italic> is the measurement (bounding box center) of the current detection, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted mean, and <italic toggle="yes">S</italic> is the predicted covariance matrix from the KF.<disp-formula id="FD10-sensors-25-05331"><label>(8)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (6), S<sub>event</sub> is 1 if two tracklets share the same expected event (e.g., &#8220;<italic toggle="yes">person holds bottle</italic>&#8221; before and after the transition), 0 otherwise. <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are hyperparameters and are set based on the reliability or expectations of scores. For example, if spatial events are well annotated, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> receives higher weight/values.</p><p>Sometimes, the spatial event is detected in the first tracklet of the second trajectory. For example, the spatial event (e.g., person picks the bottle) is not detected as the first observed tracklet in camera B. Therefore, we need to reevaluate past edges of the tracklet graph if a spatial event (e.g., a person picks up a bottle) is detected in the middle of the second trajectory. Prior to the detection of this spatial event, the scores of the candidate tracklet graph edges were relatively low. Therefore, it is necessary to update these candidates&#8217; edge scores considering the newly detected spatial event. Equation (9) illustrates the procedure for updating the scores of candidate edges, where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the past candidate edge score, <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the confidence boost scale for delayed events, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the decay rate that indicates how much trust is given to earlier frames, and t and t&#8217; represent the times of the detected spatial event and the candidate tracklet.<disp-formula id="FD11-sensors-25-05331"><label>(9)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#947;</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the last step, we aim to optimize the graph edges, merge tracklets, and assign global IDs to tracklets. To do so, we prune the graph edges based on a predefined threshold, which represents the expected level of confidence in the connections between tracklets. Subsequently, we optimize the graph by merging tracklets and assigning global IDs through the use of a Hierarchical Clustering (HC) approach, a greedy matching algorithm that finds the best tracklet matches [<xref rid="B61-sensors-25-05331" ref-type="bibr">61</xref>]. It employs the uses an agglomerative approach, merging the most similar tracklets and stopping when inter-cluster similarity falls below the threshold. This threshold can be determined through empirical tuning, starting with a default value that can be adjusted based on the specific characteristics of the dataset and application requirements.</p></sec><sec id="sec3dot3dot2-sensors-25-05331"><title>3.3.2. Trajectory Spatial Enhancer</title><p>GICEDCAM uses various data sources, such as camera streams, IMU, or GPS, to detect complex events. These data sources are ingested at different rates and levels of accuracy. This inconsistency of rates and accuracy is a potential source of trajectory noise. For example, the GPS data rate is 1 (Hz) while the camera stream rate is 30 fps (30 Hz). This means we expect denser trajectories from cameras, whereas GPS trajectories are sparser. Another issue is the objects&#8217; bbox jittering due to the small changes in detection confidence or Non-Maximum Suppression (NMS) jittering. The jittering causes noise in the moving object&#8217;s trajectory.</p><p>To solve the issue of integrating low-rate and high-rate trajectory data, we can apply upsampling of low-rate data or downsampling of high-rate data. Downsampling high-rate data would improve the performance of detecting spatial events, but it would reduce confidence in tracking and matching tracklets. On the other hand, upsampling low-rate data would increase the accuracy of tracking re-identification, but it reduces the performance of high-level event detection, and it is vulnerable to the low accuracy of low-rate data. Thus, we keep all data and update it in real-time. <xref rid="sensors-25-05331-f006" ref-type="fig">Figure 6</xref> illustrates the workflow of the trajectory data correction procedure. In this workflow, high-rate data locations are continuously predicted using the KF algorithm, a widely used algorithm for the prediction and estimation of location data [<xref rid="B62-sensors-25-05331" ref-type="bibr">62</xref>]. In the next step, for each high-rate trajectory, the predicted value (<italic toggle="yes">x&#8217;</italic>) is compared to the current low-rate value (<italic toggle="yes">x</italic>) based on the velocity difference <italic toggle="yes">&#8710;V</italic> and Mahalanobis distance <italic toggle="yes">M</italic>. If they exceed the threshold values (i.e., <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), they are considered outliers and removed from the trajectory data. If more than one high-rate trajectory is found to match the low-rate data, temporal matching is triggered. It checks which trajectory the past k low-rate points matched. It then chooses that trajectory for fusion; otherwise, the algorithm proceeds to the fusion step with all candidates. In this step, the predicted value and current state values are used in the KF to update the low-rate data. As a result, high-rate trajectory data are refined, and low-rate data drift is corrected.</p></sec></sec></sec><sec id="sec4-sensors-25-05331"><title>4. Implementations, Results, and Discussions</title><sec id="sec4dot1-sensors-25-05331"><title>4.1. Data</title><p>To evaluate the proposed framework, we used the data recorded by the GeoSensorWeb lab (The GeoSensorWeb lab is one of the University of Calgary labs that focuses on Geospatial data analysis and IoT) team. The data comprises 14 video streams captured by three smartphones from five distinct locations in the Calgary Center of Innovative Technologies (CCIT) building. The map of the building, including the camera points (A, B, C, E, and F), is illustrated in <xref rid="sensors-25-05331-f007" ref-type="fig">Figure 7</xref>. The Sony Xperia 10 III XQ-BT52 camera (made in Pathum Thani, Thailland) is in location A. The camera of the Apple iPhone 11 Pro Max (made in Zhengzhou, China) is in locations B and E. The camera of the Apple iPhone 14 Pro (made in Zhengzhou, China) is located on locations F and C.</p><p>We also used Bosch Sensortec 6-axis IMU sensor data for one person acting as a moving object, along with signal strength data from three access points corresponding to the person in the scenarios. In total, this amounts to four datasets, comprising both IMU and Wi-Fi signal strength measurements. IMU data includes accelerometer, gyro, and orientation readings of moving objects and cameras. We use the sensor readings as inputs to the GICEDCAM framework.</p><p>To evaluate the proposed framework under higher object loads and more complicated scenarios, we used the UIT-Adrone dataset [<xref rid="B63-sensors-25-05331" ref-type="bibr">63</xref>]. The dataset includes 12 videos taken of a roundabout, which is located in the International University&#8212;VNU-HCM community area. The videos were recorded by a drone and captured cars, motorbikes, motorcycles, tricycles, bicycles, buses, trucks, vans, and pedestrian movements in the roundabout and the connected streets. We trained a YOLOv8 model on the vehicle data with eight classes. Details of the training results are available in the UIT-Drone-YOLO Kaggle repository, and the trained model is available in the corresponding model repository (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/models/sepehrhonarparvar/uit-adrone-yolo">https://www.kaggle.com/models/sepehrhonarparvar/uit-adrone-yolo</uri> accessed on 1 August 2025). We considered the roundabout right-of-way violation as a complex event. <xref rid="sensors-25-05331-f008" ref-type="fig">Figure 8</xref> shows the roundabout map used in our evaluation.</p><p>Three scenarios are acted on and captured by smartphone cameras, and one scenario is defined for the roundabout right-of-way violation, which is summarized in <xref rid="sensors-25-05331-t002" ref-type="table">Table 2</xref>. To illustrate how GICEDCAM detects complex events, we provide demo videos for Scenario 2 (<xref rid="app1-sensors-25-05331" ref-type="app">Video S1</xref>) and Scenario 3 (<xref rid="app1-sensors-25-05331" ref-type="app">Video S2</xref>) in the <xref rid="app1-sensors-25-05331" ref-type="app">Supplementary Materials</xref>.</p></sec><sec id="sec4dot2-sensors-25-05331"><title>4.2. GICEDCAM Framework Implementation</title><p>To evaluate GICEDCAM&#8217;s performance against traditional CED frameworks, we designed and implemented the GICEDCAM data pipeline as shown in <xref rid="sensors-25-05331-f009" ref-type="fig">Figure 9</xref>. The camera streams and sensor readings are fed into a PC with an Intel Core i7 CPU and an NVIDIA GeForce RTX 2070 GPU. In this node, objects are detected, tracked, formatted, and published as Message Queuing Telemetry Transport (MQTT) messages to AWS IoT Core. Then, an Amazon Web Services (AWS) Lambda function is triggered to filter objects that are stored in a Neo4j Aura graph database. The results are published to AWS IoT Core to trigger the enrichment lambda. The enrichment lambda retrieves the geospatial zones from an AWS DynamoDB table and obtains the mobility status from Neo4j to enrich the objects. The enriched objects are published to the IoT Core to trigger the projection lambda function. This lambda takes the transformation parameters from DynamoDB and projects the objects onto a geospatial plane. The projected values are published to the IoT Core in GeoJSON format, serving as triggers for the appropriate processing functions for each object&#8217;s mobility status. If the mobility status indicates that the object is &#8220;moving&#8221;, the trajectory enhancer lambda function is activated. Conversely, if the mobility status indicates that the object is &#8220;fixed&#8221;, the spatial event detection lambda function is triggered. The trajectory enhancer lambda uses selected points from the historical trajectory to enhance the current moving object&#8217;s location. Then, the spatial event detection lambda is triggered once the trajectory enhancer lambda execution is completed. The spatial event detection queries spatial event patterns from the knowledge graph based on the detected objects and matches the found patterns with the processed spatial relationships. This function triggers the Spatial Event Corrector lambda. The Spatial Event Corrector lambda retrieves historical spatial events from DynamoDB and predicts whether any potential events are missed within the time window of events. This is the last lambda function that publishes corrected spatial events to the IoT Core. The IoT Core triggers the AWS Kinesis data stream handler to buffer spatial events and then passes them to Apache Flink to match temporal events based on reserved states, and publishes detected complex events to an AWS SNS notification service.</p><p>One of the most widely used video&#8211;CED frameworks is VIDCEP [<xref rid="B15-sensors-25-05331" ref-type="bibr">15</xref>]. We use VIDCEP as the primary baseline because it is (i) open-source and end-to-end executable; (ii) allows online/streaming, supporting stateful spatial and temporal event operators that align with our task; and (iii) is domain-agnostic, enabling reproducible experiments with our detector outputs. Several recent CED systems discussed in our literature review either lack publicly released implementations or are specialized (e.g., domain- or operator-specific) and are thus not directly comparable within our streaming stack without substantial re-implementation, which would confound fairness and exceed the scope of this paper. We deployed VIDCEP to an AWS EC2 instance (four vCPUs, 16 GB RAM). We considered latency under load, end-to-end latency, and CPU and memory usage to compare the scalability, latency, and cost efficiency of GICEDCAM with VIDCEP. Edge node: Intel i7-10700, 32 GB RAM, RTX-2070. Cloud: AWS Lambda (Python 3.10, 512 MB, timeout 2 s, provisioned concurrency 5 for the corrector), AWS IoT Core (MQTT), DynamoDB (zones/calibration), Neo4j Aura (v2.0), Kinesis Data Streams (1 shard), Apache Flink (v1.0), parallelism 2. We use event time processing with allowed lateness <italic toggle="yes">L</italic> = 2.0 s and query window <italic toggle="yes">W</italic> = 11 s; complex events are finalized when <italic toggle="yes">t</italic><sub>end</sub> &#8804; <italic toggle="yes">&#969;</italic> = max(event_time) &#8722; <italic toggle="yes">L</italic>. The data batches between the edge and stateless layer are JSON Lines (JSONL) format and the Objects schema. The data batches between stateless components are in GeoJSON Lines format and the Projected Objects schema. The data batches between the stateless and stateful layers are in the JSONL format and follow the Events Schema. The data schemas are illustrated in <xref rid="app3-sensors-25-05331" ref-type="app">Appendix B</xref>. For the Spatial Event Corrector, we used the following configurations:</p><p>
<named-content content-type="color:#569CD6">corrector</named-content>
<named-content content-type="color:#CCCCCC">:</named-content>
</p><p><named-content content-type="color:#CCCCCC">c </named-content><named-content content-type="color:#569CD6">method</named-content><named-content content-type="color:#CCCCCC">: </named-content><named-content content-type="color:#CE9178">TRAJ</named-content>&#8195;&#8195;&#8195;&#160;<named-content content-type="color:#6A9955"># BN | LSTM | TRAJ</named-content></p><p>&#8195;<named-content content-type="color:#569CD6">delta_probe</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.3</named-content></p><p>&#8195;<named-content content-type="color:#569CD6">epsilon_time</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.3</named-content></p><p>&#8195;<named-content content-type="color:#569CD6">budget_ms</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">30</named-content></p><p>&#8195;<named-content content-type="color:#569CD6">BN</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#160;[ </named-content>
<named-content content-type="color:#569CD6">threshold</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.70</named-content>
<named-content content-type="color:#CCCCCC"> ]</named-content></p><p>&#8195;<named-content content-type="color:#569CD6">LSTM</named-content>
<named-content content-type="color:#CCCCCC">: [ </named-content>
<named-content content-type="color:#569CD6">weights_uri</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">s3://&#8230;/lstm.pt</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#569CD6">window</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">32</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#569CD6">stride</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#569CD6">threshold</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.80</named-content>
<named-content content-type="color:#CCCCCC"> ]</named-content></p><p>&#8195;<named-content content-type="color:#569CD6">TRAJ</named-content>
<named-content content-type="color:#CCCCCC">: [ </named-content>
<named-content content-type="color:#569CD6">dtw_radius</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">5</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#569CD6">beta</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.6</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#569CD6">threshold</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.90</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#569CD6">roi_uri</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">s3://&#8230;/rois.json</named-content>
<named-content content-type="color:#CCCCCC"> ]</named-content></p><p>Latency under load is defined as the wall-clock time from the first object written to the KG to the emission of the corresponding complex event while increasing concurrent stream fan-out N&#8712;(1, 2, 4, 8, 16, 20). We simulate load by replaying a single video N times in parallel and publish detections after the edge node to isolate downstream ingestion and event processing; detectors/trackers, queries, window W = 11 s, and hardware are held constant. <xref rid="sensors-25-05331-t003" ref-type="table">Table 3</xref> reports the mean latency for each N.</p><p>Results demonstrated that the latency under load for GICEDCAM was similar up to eight simultaneous video streams. The latency difference between GICEDCAM and VIDCEP increased dramatically when processing eight or more streams. GICEDCAM broke down spatial event detection into smaller stateless processes. The use of parallel, event-driven serverless functions allows GICEDCAM to process high input rates without severe performance degradation, making it better suited for large-scale, multi-camera deployments. This architecture increased the horizontal scalability of the data pipeline. It means that GICEDCAM performs better under higher loads, and it is more suitable for large Internet of Smart Cameras (IoSC) networks. On the other hand, VIDCEP could be a good candidate for small networks.</p><p><xref rid="sensors-25-05331-t004" ref-type="table">Table 4</xref> includes the latency measurements of GICEDCAM and VIDCEP for all four scenarios. To test end-to-end latency, we measured the sum of all processing times in the time window that starts from the time that the first object of a complex event is detected and ends with the time that the complex event is detected. This time is measured by reading AWS CloudWatch logs. For GICEDCAM, it is the total execution time of all processing nodes (i.e., lambda functions, Kinesis Data Stream handler, and Apache Flink), and, for VIDCEP, it would be EC2 execution time. We measured the end-to-end latency for scenarios one, two, three, and four.</p><p>Across all t. scenarios, GICEDCAM reduced end-to-end processing time by approximately 30% relative to VIDCEP, with the <italic toggle="yes">p</italic>-value (0.0029) confirming the statistical significance of this improvement. The effect size (Cohen&#8217;s d = 10.68) is exceptionally large, indicating that the latency reduction is not only statistically robust but also operationally impactful for time-critical applications. These gains can be attributed to the decoupled, event-triggered processing pipeline and optimized data flow in GICEDCAM. Also, as the number of objects per frame grows, the latency gap between GICEDCAM and VIDCEP widens. This difference is due to the parallel processing of objects&#8217; row batches and breaking down complicated queries into simple calls in GICEDCAM nodes.</p><p>To measure computational cost, we measured CPU and memory usage of the entire data pipeline over the interval from the first to the last frame. CPU and memory usage are read by AWS CloudWatch logs. For GICEDCAM, it is the total CPU and memory usage of lambda functions, Kinesis Data Stream, and Apache Flink, and for VIDCEP, it is the EC2 CPU and memory usage for the time window. <xref rid="sensors-25-05331-t005" ref-type="table">Table 5</xref> illustrates the log measurements for GICEDCAM and VIDCEP. The results showed that GICEDCAM used less memory and CPU for the entire end-to-end CED. GICEDCAM&#8217;s memory usage is 59% lower than VIDCEP&#8217;s, and GICEDCAM&#8217;s CPU usage is 36% lower than VIDCEP&#8217;s. In total, GICEDCAM is about 48% cheaper than VIDCEP. Breaking down spatial event detection and correction into smaller stateless components helped us use memory and CPU more efficiently. In other words, a large portion of computations was handled by AWS Lambda functions, which utilize computational resources only when needed. The results also demonstrated that the VIDCEP computational cost is significantly higher for longer camera streams and a larger number of objects.</p><p>To probe generality beyond a single baseline, we introduce objects per frame (OPF) scaling on Scenario 3. We vary the median detections per frame over {5, 10, 20, 40} using controlled fan-out/thresholding while holding videos, frame rate, temporal window (8 s), queries, and hardware constant. <xref rid="sensors-25-05331-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05331-f011" ref-type="fig">Figure 11</xref> illustrate OPF impacts on the latency and memory usage as OPF curves for GICEDCAM and VIDCEP.</p><p><xref rid="sensors-25-05331-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05331-f011" ref-type="fig">Figure 11</xref> illustrate end-to-end latency and peak memory versus OPF for GICEDCAM and VIDCEP. Across all loads, GICEDCAM is faster and more memory-efficient, and the advantage grows with OPF. At 5&#8594;10&#8594;20&#8594;40 OPF, the latency reductions are 18.6% &#8594; 24.0% &#8594; 29.1% &#8594; 33.8%; memory reductions are 52.2% &#8594; 53.9% &#8594; 55.4% &#8594; 56.9%. Over this range, both systems scale approximately linearly with OPF (R<sup>2</sup> &#8805; 0.999), but VIDCEP&#8217;s slope is markedly steeper (latency 0.310 s/OPF vs. 0.199 s/OPF; memory 92.1 MB/OPF vs. 39.0 MB/OPF). This indicates that GICEDCAM&#8217;s stateless filtering and distributed processing keep growing closer to linear with a lower cost per additional object, leading to an increasing margin at higher scene densities.</p></sec><sec id="sec4dot3-sensors-25-05331"><title>4.3. Spatial Event Correction</title><p>One of the most important components of the GICEDCAM framework is the Spatial Event Corrector. To evaluate this component, we used 8 video streams of Scenario 4. The targeted missing event is the &#8220;person picks the bottle&#8221;, and the knowledge graph of the Scenario 2 complex event is shown in <xref rid="sensors-25-05331-f012" ref-type="fig">Figure 12</xref>. In the figure, the red oval is a complex event, the yellow ovals are the temporal events, the green ovals are the spatial events, and the blue ovals are the objects.</p><p>The objective is to evaluate the accuracy and performance of detecting the missing event for the BN method, LSTM, and trajectory analysis approach. For accuracy, we compared the false positives and false negatives of all three methods. <xref rid="sensors-25-05331-t006" ref-type="table">Table 6</xref> includes the counted precision, recall, and F-score for the BN approach, LSTM, and trajectory analysis method.</p><p>To evaluate the performance of the approaches, we used the processing time for the Spatial Event Corrector for all three methods over all four scenarios. <xref rid="sensors-25-05331-t007" ref-type="table">Table 7</xref> provides the average processing time of the three spatial event correction methods over all four scenarios.</p><p>A comparison between the results of spatial event correction methods demonstrated the highest value of recall for LSTM and the lowest for the BN approach. This means that LSTM is the most reliable method in cases where we do not want to miss any spatial event. It is better to use LSTM and trajectory analysis approaches when we do not want to let any incorrect spatial event come into the CED process. F-score values proved that LSTM delivers the highest precision and recall together in total. The BN approach provides the lowest latency and can be easily fitted to the GICEDCAM architecture. Results demonstrated that the complexity of the complex event KG impacts the latency values, but LSTM is the most affected method. In general, the trajectory analysis approach provides moderate and high accuracy and low latency in detecting missing spatial events, and it is well-fitted to the GICEDCAM architecture. However, the results also demonstrated the greater drop in recall for Scenario 3, which means the higher sensitivity of the proposed trajectory-based approach against a larger number of objects.</p></sec><sec id="sec4dot4-sensors-25-05331"><title>4.4. Trajectory Corrector</title><p>To evaluate the tracking re-identifier component, we tested the trajectory of Scenario 2 with a single moving object, Scenario 3 with multiple moving objects, and Scenario 4 with intentional occlusions. The trajectory points global IDs are labelled manually as ground truth. We considered precision as the main criterion to evaluate the tracking re-identifier module since the most important error of tracking algorithms is incorrect tracking ID assignment (i.e., false positives). <xref rid="sensors-25-05331-t008" ref-type="table">Table 8</xref> includes the latency and precision values of the proposed event-aware DeepSORT for the three scenarios. We also measured the statistical significance, such as <italic toggle="yes">p</italic>-value and effect size (Cohen&#8217;s d). The precision comparison between event-aware DeepSORT and standard DeepSORT yields a <italic toggle="yes">p</italic>-value of 0.098, which is above the 0.05 significance threshold, indicating that the observed improvement is not statistically significant. However, the Cohen&#8217;s d of 1.03 represents a large effect size, suggesting that the precision gains are practically meaningful, even if the small sample size prevents reaching statistical significance. For latency, the <italic toggle="yes">p</italic>-value of 0.0399 is below 0.05, showing a statistically significant increase in processing time when using event-aware DeepSORT. The Cohen&#8217;s d of 1.43 corresponds to a very large effect size, confirming that the latency increase is substantial in practice. This indicates a trade-off: event-aware DeepSORT offers large precision gains (practically speaking) but also significantly higher latency compared to standard DeepSORT.</p><p>We also visualized the person trajectories before and after applying the proposed event-aware DeepSORT in <xref rid="sensors-25-05331-f013" ref-type="fig">Figure 13</xref>. The points are classified and colored by the global tracking_IDs.</p><p>The results show that the Trajectory Corrector component has a direct effect on CED accuracy. Any discontinuity or incorrect tracklets would cause false negatives and false positives in CED. However, the effect is obvious, and it is not essential to evaluate or test the impact of trajectories on CED. Therefore, in this section, we evaluate tracking the re-identifier component in terms of precision and latency. The proposed event-aware DeepSORT improved tracking precision across all tested scenarios compared to standard DeepSORT, with practical significance supported by a large effect size (Cohen&#8217;s d = 1.03). Although the <italic toggle="yes">p</italic>-value for precision (0.098) did not reach statistical significance&#8212;likely due to limited sample size&#8212;the latency increase (<italic toggle="yes">p</italic> = 0.0399, d = 1.43) was both statistically and practically significant. This confirms a trade-off: while event-aware DeepSORT enhances tracking quality, it also introduces measurable processing overhead. Finally, <xref rid="sensors-25-05331-f010" ref-type="fig">Figure 10</xref> shows that most of the false positives belong to the points which are farther from others. It means that, in Equation (6), the spatial event hyperparameter has a lower value than the motion hyperparameter. Consequently, motion velocity and distance between points are more impactful than spatial event reasoning.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05331"><title>5. Conclusions</title><p>GICEDCAM is a geospatial, multi-layer streaming pipeline for complex event detection that reorders the conventional workflow. Stateless KG-gating and projection standardize detections before any stateful operators, and a gap-triggered Spatial Event Corrector imputes missed predicates using one of BN/LSTM/trajectory methods. A finite-state (NFA) matcher then composes temporal and complex events under explicit event-time watermark and window policies. The design targets fixed cameras with static homographies and exchanges data in GeoJSONL; our implementation details the KG schema, operators, and configuration needed for reproducibility.</p><p>On our testbed, GICEDCAM delivers lower latency and resource use than VIDCEP, the only open-source, end-to-end baseline aligned with our ODSM setting. Scenario evaluations show an average &#8776;30% reduction in end-to-end latency. Under objects per frame (OPF) scaling from 5 to 40, both systems remain roughly linear, but GICEDCAM exhibits smaller slopes, indicating a lower per-object cost that widens the margin at higher scene densities. For spatial event correction, LSTM attains the highest recall/F1 at higher latency, BN is fastest with lower recall, and the trajectory method offers a practical accuracy&#8211;latency trade-off that fits the stateless design.</p><p>This study&#8217;s scope is limited to one indoor multi-camera environment and a UIT-Drone roundabout case, assumes fixed cameras with static calibrations, and compares primarily against VIDCEP due to the scarcity of reproducible recent frameworks. Future work will broaden evaluation to additional public benchmarks and diverse conditions (lighting, weather, crowding), incorporate new open-source baselines as they become available, and examine dimensions such as query expressiveness, optimizer behavior, and explainability, and extend ablations (e.g., window/&#969; sensitivity, corrector selection policies). We also plan to investigate calibration-drift handling and, longer-term, adaptations to moving/unstable cameras.</p></sec></body><back><ack><title>Acknowledgments</title><p>We thank Sina Kiaei, Jin-Ya Wang, Humaid Kidwai, and Mahnoush Mohammadi Jahromi for their invaluable assistance with data collection.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-05331"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mdpi.com/article/10.3390/s25175331/s1">https://www.mdpi.com/article/10.3390/s25175331/s1</uri>, Video S1: Demo video of complex event detection of scenario two, Video S2: Demo video of complex event detection of scenario three.</p><supplementary-material id="sensors-25-05331-s001" position="float" content-type="local-data" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05331-s001.zip" position="float" orientation="portrait"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization, S.H., Z.A., S.L. and S.S.; Methodology, S.H. and S.L.; Software, S.H. and Y.H.; Validation, S.H. and Y.H.; Investigation, S.H.; Data curation, S.H. and Y.H.; Writing&#8212;original draft, S.H.; Writing&#8212;review &amp; editing, Z.A. and S.S.; Visualization, S.H.; Supervision, S.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw video data is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://zenodo.org/records/15817431">https://zenodo.org/records/15817431</uri>, accessed on 26 August 2025.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><app-group><app id="app2-sensors-25-05331"><title>Appendix A</title><sec id="secAdot1-sensors-25-05331"><title>Appendix A.1</title><p>To demonstrate how BN identifies the missing spatial events, we examine the &#8220;take the bottle out of the room&#8221; complex event. In <xref rid="sensors-25-05331-f011" ref-type="fig">Figure 11</xref>, &#8220;person intersects bottle&#8221; and &#8220;person intersects table&#8221; as spatial events form the &#8220;picks the bottle up from the table&#8221; temporal event. The &#8220;person intersects the door&#8221; and &#8220;person disappears&#8221; spatial events form the &#8220;leaves the room&#8221; temporal event. The &#8220;picks the bottle up from the table&#8221; should happen before the &#8220;leaves the room&#8221; temporal event.</p><fig position="anchor" id="sensors-25-05331-f0A1" orientation="portrait"><label>Figure A1</label><caption><p>&#8220;Take the bottle out of the room&#8221; complex event knowledge graph.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g0A1.jpg"/></fig><p>We assume that the person occluded the bottle and consequently went undetected at the moment the person picked it up. As a result, the spatial event &#8220;<italic toggle="yes">bottle intersects person</italic>&#8221; is missed, even though the system detected the event &#8220;person intersects bottle&#8221; shortly after &#8220;<italic toggle="yes">person intersects table</italic>&#8221;. This situation allows us to construct the BN graph as illustrated in <xref rid="sensors-25-05331-f012" ref-type="fig">Figure 12</xref>.</p><fig position="anchor" id="sensors-25-05331-f0A2" orientation="portrait"><label>Figure A2</label><caption><p>Bayesian Network for the person who takes the bottle out of the room complex event.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g0A2.jpg"/></fig><p>We want to calculate the probability of the &#8220;<italic toggle="yes">picks up the bottle</italic>&#8221; complex event given that the person nears the table and the bottle is seen with the person. We need to compute the probability of the missing event <italic toggle="yes">P(Person Picks Bottle|Person Near Table = 1, Bottle Seen With Person Afterwards = 1)</italic>. Based on the Bayesian theorem, Equation (A1) calculates the missing event probability.<disp-formula id="FD12-sensors-25-05331"><label>(A1)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>N</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>N</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mi>N</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where PPB means Person Picks Bottle, PNT means Person Near Table, and BSP means Bottle Seen with Person afterward. Assuming <xref rid="sensors-25-05331-t008" ref-type="table">Table 8</xref> includes the probability values that experts have assigned as CPD values.</p><table-wrap position="anchor" id="sensors-25-05331-t0A1" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t0A1_Table A1</object-id><label>Table A1</label><caption><p>Example probability values of CPD.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CPD Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Probability</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bottle Seen With Person = 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Person Near Table = 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Complex Event Detected&#8739;Person Moves To Door = 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Person Moves To Door&#8739;Person Picks Bottle</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Person Near Table = 1, Bottle Seen With Person = 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Person Near Table = 1, Bottle Seen With Person = 0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Person Near Table = 0, Bottle Seen With Person = 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Person Near Table = 0, Bottle Seen With Person = 0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td></tr></tbody></table></table-wrap><p>The probability of the missing spatial event (i.e., picking up the bottle) under conditions that the person was near the table and seen with the bottle afterward is equal to 0.8 &#215; 0.7 &#215; 0.6/(0.8 &#215; 0.7 &#215; 0.6+ 0.2 &#215; 0.7 &#215; 0.6) ~= 0.8.</p></sec><sec id="secAdot2-sensors-25-05331"><title>Appendix A.2</title><p>In the following example, we detect &#8220;<italic toggle="yes">Picks up the Bottle</italic>&#8221; as the missing spatial event.</p><p>In this case, we define LSTM variables as the tuple of person coordinates and spatial events at time t. Therefore, the input of the LSTM is defined in Equation (A2), where X<sub>t</sub> and Y<sub>t</sub> are the person&#8217;s coordinates and S<sub>t</sub> is the spatial event.<disp-formula id="FD13-sensors-25-05331"><label>(A2)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (A1) defines the output label for the missing event. Here, LSTM learns from past movement patterns and infers whether the missing event happened.<disp-formula id="FD14-sensors-25-05331"><label>(A3)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>Person</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Picks</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Bottle</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We consider a five-time step per sequence to predict the missing event. <xref rid="sensors-25-05331-t0A2" ref-type="table">Table A2</xref> represents a numerical value for the five steps.</p><table-wrap position="anchor" id="sensors-25-05331-t0A2" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t0A2_Table A2</object-id><label>Table A2</label><caption><p>Example of LSTM sequence input values.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Time</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">X</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Y</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Person Intersects Table</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bottle Seen with Person</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr></tbody></table></table-wrap><p>Here, the bottle intersects with the person who is missing, but LSTM learns from past movement sequences and predicts the missing event. LSTM takes past movement sequences and predicts the missing event using Equations (A4) and (A5).<disp-formula id="FD15-sensors-25-05331"><label>(A4)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif">&#963;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-05331"><label>(A5)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">h<sub>t</sub></italic> is the LSTM hidden state at time <italic toggle="yes">t</italic>, <italic toggle="yes">W<sub>h</sub></italic> (maps the input <italic toggle="yes">X<sub>t</sub></italic> to the h<sub>t</sub> state) and U<sub>h</sub> (maps previously hidden state <italic toggle="yes">h<sub>t</sub></italic><sub>&#8722;1</sub> to <italic toggle="yes">h<sub>t</sub></italic>) are LSTM weight matrices, <italic toggle="yes">Y<sub>t</sub></italic> is the probability of the missing event, <italic toggle="yes">&#963;</italic> is the Sigmoid activation function, and <italic toggle="yes">b<sub>h</sub></italic> is the bias term. We use binary cross-entropy to predict the missing event.<disp-formula id="FD17-sensors-25-05331"><label>(A6)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:msubsup><mml:mo mathsize="70%">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The weights are initiated randomly and are updated during the training based on the Back Propagation Through Time (BPTT) algorithm, which is summarized in Equations (A7)&#8211;(A9).<disp-formula id="FD18-sensors-25-05331"><label>(A7)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#951;</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#8706;</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#8706;</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD19-sensors-25-05331"><label>(A8)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#951;</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#8706;</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#8706;</mml:mi><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD20-sensors-25-05331"><label>(A9)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#951;</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#8706;</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#8706;</mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif-italic">&#8706;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the gradient of loss function over the weights and the bias term.</p><p>Assuming that the weight values output from the training procedure are as follows:<disp-formula id="FD21-sensors-25-05331"><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, the state value would be:<disp-formula id="FD24-sensors-25-05331"><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif">&#963;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>6.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>0.8</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The output will be 0.54 if <italic toggle="yes">W</italic><sub>0</sub> is 0.7 and <italic toggle="yes">b</italic><sub>0</sub> is &#8722;0.5. This indicates the LSTM predicts a 54% probability that the person picks up the bottle at time t.</p></sec></app><app id="app3-sensors-25-05331"><title>Appendix B</title><sec id="secBdot1-sensors-25-05331"><title>Appendix B.1. Object Schema</title><p>
<named-content content-type="color:#CCCCCC">{</named-content>
</p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;$schema&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;https://json-schema.org/draft/2020-12/schema&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;$id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;https://.../schemas/object.schema.json&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;title&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Object (raw detection)&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;object&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;additionalProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">false</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;required&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#CE9178">&#8220;object_type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;camera_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;local_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;x_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#CE9178">&#8221;y_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#CE9178">&#8221;x_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#CE9178">&#8221;y_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;detection_time&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;confidence&#8221;</named-content>
<named-content content-type="color:#CCCCCC">],</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;object_type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">},</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;camera_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Sensor/camera identifier&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;local_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Per-camera track/detection ID&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;global_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Cross-camera/global ID if known&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;X min of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Y min of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;X max of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Y max of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;detection_time&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Event-time (epoch seconds, float)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;confidence&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;minimum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;maximum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;frame_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;integer&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;minimum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;speed&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;estimated object speed by Yolo model&#8221;</named-content>
<named-content content-type="color:#CCCCCC">},</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;color_histogram&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;array&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;estimated object speed by Yolo model&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;items&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;},</named-content>
</p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;source&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Producer tag (e.g., &#8216;edge-detector&#8217;)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> }</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;},</named-content>
</p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;examples&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;{</named-content>
</p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;object_type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;person&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;camera_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;A&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;local_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;trk-0123&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">642.5</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">311.2</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">670.3</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">422.4</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;</named-content>&#160;
<named-content content-type="color:#9CDCFE">detection_time&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1723645223.412</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;confidence&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.92</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;frame_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1084</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;color_histogram&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#B5CEA8">2</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#F44747">...</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#B5CEA8">12</named-content>
<named-content content-type="color:#CCCCCC">],</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;speed&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">21.2</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;]</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">}</named-content>
</p></sec><sec id="secBdot2-sensors-25-05331"><title>Appendix B.2. Projected Object Schema</title><p>
<named-content content-type="color:#CCCCCC">{</named-content>
</p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;$schema&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;https://json-schema.org/draft/2020-12/schema&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;$id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;https://gicedcam.org/schemas/projected-object.schema.json&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;title&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;ProjectedObject (GeoJSON Feature)&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;object&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;additionalProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">false</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;required&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#CE9178">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;geometry&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">],</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;const&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Feature&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;geometry&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;object&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;additionalProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">false</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;required&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#CE9178">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;coordinates&#8221;</named-content>
<named-content content-type="color:#CCCCCC">],</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;const&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Point&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;coordinates&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;array&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;minItems&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">2</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;maxItems&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">2</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;items&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;[x, y] in meters, CRS = &#8216;cartesian-m&#8217;&#8221;</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;},</named-content>
</p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;object&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;additionalProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">false</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;required&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#CE9178">&#8220;record_type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;camera_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;local_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;class&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;detection_time&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;conf&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;mobility&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;crs&#8221;</named-content>
<named-content content-type="color:#CCCCCC">],</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;camera_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;local_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;global_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;class&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;detection_time&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Event-time (epoch seconds, float)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;mobility&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;enum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#CE9178">&#8220;moving&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;fixed&#8221;</named-content>
<named-content content-type="color:#CCCCCC">] },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;zone_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Assigned geospatial zone (if any)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;mu&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;minimum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;maximum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Fuzzy membership to zone_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;speed_m_s&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;traj_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;crs&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;const&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;cartesian-m&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;X min of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Y min of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;X max of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Y max of object bbox&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;confidence&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;minimum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;maximum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;frame_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;integer&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;minimum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;speed&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;estimated object speed by Yolo model&#8221;</named-content>
<named-content content-type="color:#CCCCCC">},</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;color_histogram&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;array&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;estimated object speed by Yolo model&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;items&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;&#8195;&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;&#8195;&#8195;},</named-content>
</p><p>&#8192;</p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;source&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Producer (e.g., &#8216;projector&#8217;, &#8216;traj-enhancer&#8217;)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> }</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;},</named-content>
</p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;examples&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;{</named-content>
</p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Feature&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;geometry&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Point&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;coordinates&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#B5CEA8">12.43</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#B5CEA8">8.77</named-content>
<named-content content-type="color:#CCCCCC">] },</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;camera_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;A&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;local_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;trk-0123&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;global_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;P-00042&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;class&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;person&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;detection_time&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1723645223.412</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;confidence&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.92</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;mobility&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;moving&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;zone_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;361A-table&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;mu&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.83</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;speed_m_s&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.9</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;traj_id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;traj-007&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;crs&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;cartesian-m&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;source&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;projector&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">642.5</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_min&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">311.2</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;x_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">670.3</named-content>
<named-content content-type="color:#CCCCCC">, </named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;y_max&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">422.4</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;detection_time&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1723645223.412</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;color_histogram&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#B5CEA8">2</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#F44747">&#8230;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content>
<named-content content-type="color:#B5CEA8">12</named-content>
<named-content content-type="color:#CCCCCC">],</named-content></p><p>&#8195;&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;speed&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">21.2</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;]</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">}</named-content>
</p><p>&#8192;</p></sec><sec id="secBdot3-sensors-25-05331"><title>Appendix B.3. Events Schema</title><p>
<named-content content-type="color:#CCCCCC">{</named-content>
</p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;$schema&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;https://json-schema.org/draft/2020-12/schema&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;$id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;https://../schemas/spatial-event.schema.json&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;title&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;SpatialEvent&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;object&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;additionalProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">false</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;required&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content>
<named-content content-type="color:#CE9178">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;predicate&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;bindings&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;t&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;conf&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#CE9178">&#8220;corrected&#8221;</named-content>
<named-content content-type="color:#CCCCCC">],</named-content></p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;properties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: {</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;const&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;SpatialEvent&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;predicate&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;pattern&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;^[A-Za-z][A-Za-z0-9_]*$&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;e.g., Near, InZone, Touches&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;bindings&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;{</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;object&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Map from pattern variables to KG IDs (Object/Zone IDs)&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;minProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;additionalProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> }</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;},</named-content>
</p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;t&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;&#160;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Event-time (epoch seconds, float)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;conf&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;number&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;minimum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;maximum&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;corrected&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;boolean&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;true if emitted by the Spatial Event Corrector&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Deterministic record ID (optional)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;source&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;string&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Producer tag (e.g., &#8216;detector&#8217;,&#8216;corrector&#8217;)&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;meta&#8221;</named-content>
<named-content content-type="color:#CCCCCC">:&#8195;&#8195;&#8195;{ </named-content>
<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;object&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;description&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Optional method metadata&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;additionalProperties&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">true</named-content>
<named-content content-type="color:#CCCCCC"> }</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;},</named-content>
</p><p>&#8195;<named-content content-type="color:#9CDCFE">&#8220;examples&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: [</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;{</named-content>
</p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;SpatialEvent&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;predicate&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Near&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;bindings&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;A&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;P-00042&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;B&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Table-1&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;Z&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;361A&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;t&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1723645224.105</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;conf&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.91</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;corrected&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">false</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;source&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;spatial-detector&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;id&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;se:Near:P-00042:Table-1:361A:1723645224.105&#8221;</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;},</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;{</named-content>
</p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;type&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;SpatialEvent&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;predicate&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;PickUp&#8221;</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;bindings&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: { </named-content>
<named-content content-type="color:#9CDCFE">&#8220;A&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;P-00042&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;obj&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;Bottle-7&#8221;</named-content>
<named-content content-type="color:#CCCCCC">, </named-content>
<named-content content-type="color:#9CDCFE">&#8220;Z&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;361A&#8221;</named-content>
<named-content content-type="color:#CCCCCC"> },</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;t&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">1723645225.330</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;conf&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#B5CEA8">0.86</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;corrected&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#569CD6">true</named-content>
<named-content content-type="color:#CCCCCC">,</named-content></p><p>&#8195;&#8195;&#8195;<named-content content-type="color:#9CDCFE">&#8220;source&#8221;</named-content>
<named-content content-type="color:#CCCCCC">: </named-content>
<named-content content-type="color:#CE9178">&#8220;corrector:TRAJ&#8221;</named-content></p><p>
<named-content content-type="color:#CCCCCC">&#8195;&#8195;}</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">&#8195;]</named-content>
</p><p>
<named-content content-type="color:#CCCCCC">}</named-content>
</p><p>&#8192;</p></sec></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-05331"><label>1.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Merriam-Webster</collab></person-group><article-title>Event</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.merriam-webster.com/dictionary/event" ext-link-type="uri">https://www.merriam-webster.com/dictionary/event</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-04-10">(accessed on 10 April 2025)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-05331"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vandenhouten</surname><given-names>R.</given-names></name><name name-style="western"><surname>Holland-Moritz</surname><given-names>R.</given-names></name></person-group><article-title>A software architecture for intelligent facility management based on complex event processing</article-title><source>Wiss. Beitr&#228;ge 2012</source><year>2012</year><volume>16</volume><fpage>57</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.15771/0949-8214_2012_1_7</pub-id></element-citation></ref><ref id="B3-sensors-25-05331"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Giatrakos</surname><given-names>N.</given-names></name><name name-style="western"><surname>Alevizos</surname><given-names>E.</given-names></name><name name-style="western"><surname>Artikis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Deligiannakis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Garofalakis</surname><given-names>M.</given-names></name></person-group><article-title>Complex event recognition in the big data era: A survey</article-title><source>VLDB J.</source><year>2020</year><volume>29</volume><fpage>313</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s00778-019-00557-w</pub-id></element-citation></ref><ref id="B4-sensors-25-05331"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>J.</given-names></name></person-group><article-title>Complex event detection in probabilistic stream</article-title><source>Proceedings of the 2010 12th International Asia-Pacific Web Conference</source><conf-loc>Busan, Republic of Korea</conf-loc><conf-date>6&#8211;8 April 2010</conf-date><fpage>361</fpage><lpage>363</lpage></element-citation></ref><ref id="B5-sensors-25-05331"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Honarparvar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ashena</surname><given-names>Z.B.</given-names></name><name name-style="western"><surname>Saeedi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>S.</given-names></name></person-group><article-title>A Systematic Review of Event-Matching Methods for Complex Event Detection in Video Streams</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7238</elocation-id><pub-id pub-id-type="doi">10.3390/s24227238</pub-id><pub-id pub-id-type="pmid">39599015</pub-id><pub-id pub-id-type="pmcid">PMC11597915</pub-id></element-citation></ref><ref id="B6-sensors-25-05331"><label>6.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>MRFR</collab></person-group><source>AI Camera Market Research Report: By Type (Smartphone Cameras, Surveillance Cameras, DSLRs, Others), by Technology (Image/Face Recognition, Speech/Voice Recognition, Computer Vision, Others) and by Region (North America, Europe, Asia-Pacific, Middle East &amp; Africa and South America)&#8212;Forecast till 2027</source><publisher-name>Market Research Future</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2021</year><fpage>111</fpage></element-citation></ref><ref id="B7-sensors-25-05331"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yadav</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sarkar</surname><given-names>D.</given-names></name><name name-style="western"><surname>Salwala</surname><given-names>D.</given-names></name><name name-style="western"><surname>Curry</surname><given-names>E.</given-names></name></person-group><article-title>Traffic prediction framework for OpenStreetMap using deep learning based complex event processing and open traffic cameras</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2008.00928</pub-id></element-citation></ref><ref id="B8-sensors-25-05331"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shahad</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Bein</surname><given-names>L.G.</given-names></name><name name-style="western"><surname>Saad</surname><given-names>M.H.M.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>A.</given-names></name></person-group><article-title>Complex event detection in an intelligent surveillance system using CAISER platform</article-title><source>Proceedings of the 2016 International Conference on Advances in Electrical, Electronic and Systems Engineering (ICAEES)</source><conf-loc>Putrajaya, Malaysia</conf-loc><conf-date>14&#8211;16 November 2016</conf-date><fpage>129</fpage><lpage>133</lpage></element-citation></ref><ref id="B9-sensors-25-05331"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahmani</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Babaei</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Souri</surname><given-names>A.</given-names></name></person-group><article-title>Event-driven IoT architecture for data analysis of reliable healthcare application using complex event processing</article-title><source>Clust. Comput.</source><year>2021</year><volume>24</volume><fpage>1347</fpage><lpage>1360</lpage><pub-id pub-id-type="doi">10.1007/s10586-020-03189-w</pub-id></element-citation></ref><ref id="B10-sensors-25-05331"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bazhenov</surname><given-names>N.</given-names></name><name name-style="western"><surname>Korzun</surname><given-names>D.</given-names></name></person-group><article-title>Event-driven video services for monitoring in edge-centric internet of things environments</article-title><source>Proceedings of the 2019 25th Conference of Open Innovations Association (FRUCT)</source><conf-loc>Helsinki, Finland</conf-loc><conf-date>5&#8211;8 November 2019</conf-date><fpage>47</fpage><lpage>56</lpage></element-citation></ref><ref id="B11-sensors-25-05331"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Knoch</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ponpathirkoottam</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>T.</given-names></name></person-group><article-title>Video-to-model: Unsupervised trace extraction from videos for process discovery and conformance checking in manual assembly</article-title><source>Proceedings of the Business Process Management: 18th International Conference, BPM 2020</source><conf-loc>Seville, Spain</conf-loc><conf-date>13&#8211;18 September 2020</conf-date><comment>Proceedings 18</comment><fpage>291</fpage><lpage>308</lpage></element-citation></ref><ref id="B12-sensors-25-05331"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Saeedi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ojagh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Honarparvar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kiaei</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jahromi</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Squires</surname><given-names>J.</given-names></name></person-group><article-title>An Interoperable Architecture for the Internet of COVID-19 Things (IoCT) Using Open Geospatial Standards&#8212;Case Study: Workplace Reopening</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>50</elocation-id><pub-id pub-id-type="doi">10.3390/s21010050</pub-id><pub-id pub-id-type="pmid">33374208</pub-id><pub-id pub-id-type="pmcid">PMC7796058</pub-id></element-citation></ref><ref id="B13-sensors-25-05331"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>H.T.</given-names></name></person-group><article-title>Hierarchical latent concept discovery for video event detection</article-title><source>IEEE Trans. Image Process.</source><year>2017</year><volume>26</volume><fpage>2149</fpage><lpage>2162</lpage><pub-id pub-id-type="doi">10.1109/TIP.2017.2670782</pub-id><pub-id pub-id-type="pmid">28221996</pub-id></element-citation></ref><ref id="B14-sensors-25-05331"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bailis</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zaharia</surname><given-names>M.</given-names></name></person-group><article-title>BlazeIt: Optimizing declarative aggregation and limit queries for neural network-based video analytics</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1805.01046</pub-id><pub-id pub-id-type="doi">10.14778/3372716.3372725</pub-id></element-citation></ref><ref id="B15-sensors-25-05331"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yadav</surname><given-names>P.</given-names></name><name name-style="western"><surname>Curry</surname><given-names>E.</given-names></name></person-group><article-title>VidCEP: Complex Event Processing Framework to Detect Spatiotemporal Patterns in Video Streams</article-title><source>Proceedings of the 2019 IEEE International Conference on Big Data (Big Data)</source><conf-loc>Los Angeles, CA, USA</conf-loc><conf-date>9&#8211;12 December 2019</conf-date><fpage>2513</fpage><lpage>2522</lpage></element-citation></ref><ref id="B16-sensors-25-05331"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vilamala</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>T.</given-names></name><name name-style="western"><surname>Taylor</surname><given-names>H.</given-names></name><name name-style="western"><surname>Garcia</surname><given-names>L.</given-names></name><name name-style="western"><surname>Srivastava</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kaplan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Preece</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kimmig</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cerutti</surname><given-names>F.</given-names></name></person-group><article-title>DeepProbCEP: A neuro-symbolic approach for complex event processing in adversarial settings</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>215</volume><fpage>119376</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2022.119376</pub-id></element-citation></ref><ref id="B17-sensors-25-05331"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hauptmann</surname><given-names>A.G.</given-names></name></person-group><article-title>How related exemplars help complex event detection in web videos?</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Sydney, Australia</conf-loc><conf-date>2&#8211;8 December 2013</conf-date><fpage>2104</fpage><lpage>2111</lpage></element-citation></ref><ref id="B18-sensors-25-05331"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Boer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Schutte</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kraaij</surname><given-names>W.</given-names></name></person-group><article-title>Knowledge based query expansion in complex multimedia event detection</article-title><source>Multimed. Tools Appl.</source><year>2016</year><volume>75</volume><fpage>9025</fpage><lpage>9043</lpage><pub-id pub-id-type="doi">10.1007/s11042-015-2757-4</pub-id></element-citation></ref><ref id="B19-sensors-25-05331"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brousmiche</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rouat</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dupont</surname><given-names>S.</given-names></name></person-group><article-title>Multimodal Attentive Fusion Network for audio-visual event recognition</article-title><source>Inf. Fusion</source><year>2022</year><volume>85</volume><fpage>52</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2022.03.001</pub-id></element-citation></ref><ref id="B20-sensors-25-05331"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kiaei</surname><given-names>S.</given-names></name><name name-style="western"><surname>Honarparvar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Saeedi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>S.</given-names></name></person-group><article-title>Design and Development of an Integrated Internet of Audio and Video Sensors for COVID-19 Coughing and Sneezing Recognition</article-title><source>Proceedings of the 2021 IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)</source><conf-loc>Vancouver, Canada</conf-loc><conf-date>27&#8211;30 October 2021</conf-date><fpage>0583</fpage><lpage>0589</lpage></element-citation></ref><ref id="B21-sensors-25-05331"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chakraborty</surname><given-names>I.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Javed</surname><given-names>O.</given-names></name></person-group><article-title>Entity Centric Feature Pooling for Complex Event Detection</article-title><source>Proceedings of the 1st ACM International Workshop on Human Centered Event Understanding from Multimedia</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>7 November 2014</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B22-sensors-25-05331"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>S.-F.</given-names></name></person-group><article-title>Eventnet: A large scale structured concept library for complex event detection in video</article-title><source>Proceedings of the 23rd ACM international conference on Multimedia</source><conf-loc>Brisbane, Australia</conf-loc><conf-date>26&#8211;30 October 2015</conf-date><fpage>471</fpage><lpage>480</lpage></element-citation></ref><ref id="B23-sensors-25-05331"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>J.H.</given-names></name><name name-style="western"><surname>Sung</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.-F.</given-names></name><name name-style="western"><surname>Jou</surname><given-names>E.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>M.-W.</given-names></name></person-group><article-title>Complex event processing for the internet of things and its applications</article-title><source>Proceedings of the 2014 IEEE International Conference on Automation Science and Engineering (CASE)</source><conf-loc>New Taipei, Taiwan</conf-loc><conf-date>18&#8211;22 August 2014</conf-date><fpage>1144</fpage><lpage>1149</lpage></element-citation></ref><ref id="B24-sensors-25-05331"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ke</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.-J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.-D.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.-G.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.-M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.-R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.-B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.-Q.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>W.-H.</given-names></name></person-group><article-title>Complex Event Detection in Video Streams</article-title><source>Proceedings of the 2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)</source><conf-loc>Oxford, UK</conf-loc><conf-date>29 March&#8211;2 April 2016</conf-date><fpage>172</fpage><lpage>179</lpage></element-citation></ref><ref id="B25-sensors-25-05331"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Co&#351;ar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Donatiello</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bogorny</surname><given-names>V.</given-names></name><name name-style="western"><surname>Garate</surname><given-names>C.</given-names></name><name name-style="western"><surname>Alvares</surname><given-names>L.O.</given-names></name><name name-style="western"><surname>Br&#233;mond</surname><given-names>F.</given-names></name></person-group><article-title>Toward abnormal trajectory and event detection in video surveillance</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2016</year><volume>27</volume><fpage>683</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2016.2589859</pub-id></element-citation></ref><ref id="B26-sensors-25-05331"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ananthanarayanan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bodik</surname><given-names>P.</given-names></name><name name-style="western"><surname>Philipose</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bahl</surname><given-names>P.</given-names></name><name name-style="western"><surname>Freedman</surname><given-names>M.J.</given-names></name></person-group><article-title>Live video analytics at scale with approximation and {Delay-Tolerance}</article-title><source>Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>27&#8211;29 March 2017</conf-date><fpage>377</fpage><lpage>392</lpage></element-citation></ref><ref id="B27-sensors-25-05331"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Serafini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bozzato</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lazzerini</surname><given-names>B.</given-names></name></person-group><article-title>Event Detection from Video Using Answer Set Programing</article-title><source>Proceedings of the 34th Italian Conference on Computational Logic (CILC)</source><conf-loc>Trieste, Italy</conf-loc><conf-date>18&#8211;21 June 2019</conf-date><fpage>48</fpage><lpage>58</lpage></element-citation></ref><ref id="B28-sensors-25-05331"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yadav</surname><given-names>P.</given-names></name><name name-style="western"><surname>Curry</surname><given-names>E.</given-names></name></person-group><article-title>Vekg: Video event knowledge graph to represent video streams for complex event pattern matching</article-title><source>Proceedings of the 2019 First International Conference on Graph Computing (GC)</source><conf-loc>Laguna Hills, CA, USA</conf-loc><conf-date>25&#8211;27 September 2019</conf-date><fpage>13</fpage><lpage>20</lpage></element-citation></ref><ref id="B29-sensors-25-05331"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yadav</surname><given-names>P.</given-names></name></person-group><article-title>High-performance complex event processing framework to detect event patterns over video streams</article-title><source>Proceedings of the 20th International Middleware Conference Doctoral Symposium</source><conf-loc>Davis, CA, USA</conf-loc><conf-date>9&#8211;13 December 2019</conf-date><fpage>47</fpage><lpage>50</lpage></element-citation></ref><ref id="B30-sensors-25-05331"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yadav</surname><given-names>P.</given-names></name><name name-style="western"><surname>Curry</surname><given-names>E.</given-names></name></person-group><article-title>Visual Semantic Multimedia Event Model for Complex Event Detection in Video Streams</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2009.14525</pub-id><pub-id pub-id-type="arxiv">2009.14525</pub-id></element-citation></ref><ref id="B31-sensors-25-05331"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yadav</surname><given-names>P.</given-names></name><name name-style="western"><surname>Salwala</surname><given-names>D.</given-names></name><name name-style="western"><surname>Das</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Curry</surname><given-names>E.</given-names></name></person-group><article-title>Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing</article-title><source>Int. J. Semant. Comput.</source><year>2020</year><volume>14</volume><fpage>423</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1142/S1793351X20500051</pub-id></element-citation></ref><ref id="B32-sensors-25-05331"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Patel</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Merlino</surname><given-names>G.</given-names></name><name name-style="western"><surname>Puliafito</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vyas</surname><given-names>R.</given-names></name><name name-style="western"><surname>Vyas</surname><given-names>O.</given-names></name><name name-style="western"><surname>Ojha</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tiwari</surname><given-names>V.</given-names></name></person-group><article-title>An NLP-guided ontology development and refinement approach to represent and query visual information</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>213</volume><fpage>118998</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2022.118998</pub-id></element-citation></ref><ref id="B33-sensors-25-05331"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kossoski</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sim&#227;o</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Lopes</surname><given-names>H.S.</given-names></name></person-group><article-title>Modelling and Performance Analysis of a Notification-Based Method for Processing Video Queries on the Fly</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>3566</elocation-id><pub-id pub-id-type="doi">10.3390/app14093566</pub-id></element-citation></ref><ref id="B34-sensors-25-05331"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Petkovi&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jonker</surname><given-names>W.</given-names></name><name name-style="western"><surname>Petkovi&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jonker</surname><given-names>W.</given-names></name></person-group><article-title>Spatio-Temporal Formalization of Video Events</article-title><source>Content-Based Video Retrieval: A Database Perspective</source><publisher-name>Springer</publisher-name><publisher-loc>Boston, MA, USA</publisher-loc><year>2004</year><fpage>55</fpage><lpage>71</lpage></element-citation></ref><ref id="B35-sensors-25-05331"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Honarparvar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Saeedi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Squires</surname><given-names>J.</given-names></name></person-group><article-title>Design and Development of an Internet of Smart Cameras Solution for Complex Event Detection in COVID-19 Risk Behaviour Recognition</article-title><source>ISPRS Int. J. Geo-Inf.</source><year>2021</year><volume>10</volume><elocation-id>81</elocation-id><pub-id pub-id-type="doi">10.3390/ijgi10020081</pub-id></element-citation></ref><ref id="B36-sensors-25-05331"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Agarwal</surname><given-names>S.</given-names></name></person-group><article-title>Model Transfer for Event Tracking as Transcript Understanding for Videos of Small Group Interaction</article-title><source>Proceedings of the International Conference on Computational Linguistics</source><conf-loc>Gyeongju, Republic of Korea</conf-loc><conf-date>12&#8211;17 October 2022</conf-date></element-citation></ref><ref id="B37-sensors-25-05331"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Partanen</surname><given-names>T.</given-names></name><name name-style="western"><surname>M&#252;ller</surname><given-names>P.</given-names></name><name name-style="western"><surname>Collin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bj&#246;rklund</surname><given-names>J.</given-names></name></person-group><article-title>Implementation and accuracy evaluation of fixed camera-based object positioning system employing cnn-detector</article-title><source>Proceedings of the 2021 9th European Workshop on Visual Information Processing (EUVIP)</source><conf-loc>Paris, France</conf-loc><conf-date>23&#8211;25 June 2021</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B38-sensors-25-05331"><label>38.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Robinson</surname><given-names>B.</given-names></name><name name-style="western"><surname>Langford</surname><given-names>D.</given-names></name><name name-style="western"><surname>Jetton</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cannan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Patterson</surname><given-names>K.</given-names></name><name name-style="western"><surname>Diltz</surname><given-names>R.</given-names></name><name name-style="western"><surname>English</surname><given-names>W.</given-names></name></person-group><article-title>Real-time object detection and geolocation using 3D calibrated camera/LiDAR pair</article-title><source>Proceedings of the Autonomous Systems: Sensors, Processing, and Security for Vehicles and Infrastructure, Online, 12&#8211;17 April 2021</source><publisher-name>SPIE</publisher-name><publisher-loc>Bellingham, WA, USA</publisher-loc><year>2021</year><fpage>57</fpage><lpage>77</lpage></element-citation></ref><ref id="B39-sensors-25-05331"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name></person-group><article-title>Predictive complex event processing based on evolving Bayesian networks</article-title><source>Pattern Recognit. Lett.</source><year>2018</year><volume>105</volume><fpage>207</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2017.05.008</pub-id></element-citation></ref><ref id="B40-sensors-25-05331"><label>40.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Heckerman</surname><given-names>D.</given-names></name></person-group><article-title>A tutorial on learning with Bayesian networks</article-title><source>Learning in Graphical Models</source><publisher-name>Springer</publisher-name><publisher-loc>Dordrecht, The Netherlands</publisher-loc><year>1998</year><fpage>301</fpage><lpage>354</lpage></element-citation></ref><ref id="B41-sensors-25-05331"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Uusitalo</surname><given-names>L.</given-names></name></person-group><article-title>Advantages and challenges of Bayesian networks in environmental modelling</article-title><source>Ecol. Model.</source><year>2007</year><volume>203</volume><fpage>312</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1016/j.ecolmodel.2006.11.033</pub-id></element-citation></ref><ref id="B42-sensors-25-05331"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kontkanen</surname><given-names>P.</given-names></name><name name-style="western"><surname>Myllym&#228;ki</surname><given-names>P.</given-names></name><name name-style="western"><surname>Silander</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tirri</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gr&#252;nwald</surname><given-names>P.</given-names></name></person-group><article-title>Comparing predictive inference methods for discrete domains</article-title><source>Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics</source><conf-loc>Fort Lauderdale, FL, USA</conf-loc><conf-date>4&#8211;7 January 1997</conf-date><fpage>311</fpage><lpage>318</lpage></element-citation></ref><ref id="B43-sensors-25-05331"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Al-Selwi</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Hassan</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Abdulkadir</surname><given-names>S.J.</given-names></name><name name-style="western"><surname>Muneer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sumiea</surname><given-names>E.H.</given-names></name><name name-style="western"><surname>Alqushaibi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ragab</surname><given-names>M.G.</given-names></name></person-group><article-title>RNN-LSTM: From applications to modelling techniques and beyond&#8212;Systematic review</article-title><source>J. King Saud Univ. Comput. Inf. Sci.</source><year>2024</year><volume>36</volume><fpage>102068</fpage><pub-id pub-id-type="doi">10.1016/j.jksuci.2024.102068</pub-id></element-citation></ref><ref id="B44-sensors-25-05331"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Rao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Y.</given-names></name></person-group><article-title>Explicit speed-integrated LSTM network for non-stationary gearbox vibration representation and fault detection under varying speed conditions</article-title><source>Reliab. Eng. Syst. Saf.</source><year>2025</year><volume>254</volume><fpage>110596</fpage><pub-id pub-id-type="doi">10.1016/j.ress.2024.110596</pub-id></element-citation></ref><ref id="B45-sensors-25-05331"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name></person-group><article-title>LSTM-based traffic flow prediction with missing data</article-title><source>Neurocomputing</source><year>2018</year><volume>318</volume><fpage>297</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2018.08.067</pub-id></element-citation></ref><ref id="B46-sensors-25-05331"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raj</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>George</surname><given-names>S.N.</given-names></name><name name-style="western"><surname>Raja</surname><given-names>K.</given-names></name></person-group><article-title>Leveraging spatio-temporal features using graph neural networks for human activity recognition</article-title><source>Pattern Recognit.</source><year>2024</year><volume>150</volume><fpage>110301</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2024.110301</pub-id></element-citation></ref><ref id="B47-sensors-25-05331"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>G.-B.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bae</surname><given-names>H.-Y.</given-names></name></person-group><article-title>Human activity recognition with trajectory data in multi-floor indoor environment</article-title><source>Proceedings of the Rough Sets and Knowledge Technology: 7th International Conference, RSKT 2012</source><conf-loc>Chengdu, China</conf-loc><conf-date>17&#8211;20 August 2012</conf-date><fpage>257</fpage><lpage>266</lpage></element-citation></ref><ref id="B48-sensors-25-05331"><label>48.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name></person-group><source>Computing with Spatial Trajectories</source><publisher-name>Springer Science &amp; Business Media</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2011</year></element-citation></ref><ref id="B49-sensors-25-05331"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guendel</surname><given-names>R.G.</given-names></name><name name-style="western"><surname>Yarovoy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fioranelli</surname><given-names>F.</given-names></name></person-group><article-title>Continuous human activity recognition with distributed radar sensor networks and CNN&#8211;RNN architectures</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3189746</pub-id></element-citation></ref><ref id="B50-sensors-25-05331"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Asghari</surname><given-names>P.</given-names></name><name name-style="western"><surname>Soleimani</surname><given-names>E.</given-names></name><name name-style="western"><surname>Nazerfard</surname><given-names>E.</given-names></name></person-group><article-title>Online human activity recognition employing hierarchical hidden Markov models</article-title><source>J. Ambient Intell. Humaniz. Comput.</source><year>2020</year><volume>11</volume><fpage>1141</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1007/s12652-019-01380-5</pub-id></element-citation></ref><ref id="B51-sensors-25-05331"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ouyang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>G.</given-names></name></person-group><article-title>Clusterfl: A similarity-aware federated learning system for human activity recognition</article-title><source>Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services</source><conf-loc>Virtual</conf-loc><conf-date>24 June&#8211;2 July 2021</conf-date><fpage>54</fpage><lpage>66</lpage></element-citation></ref><ref id="B52-sensors-25-05331"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Characterizing the activity patterns of outdoor jogging using massive multi-aspect trajectory data</article-title><source>Comput. Environ. Urban Syst.</source><year>2022</year><volume>95</volume><fpage>101804</fpage><pub-id pub-id-type="doi">10.1016/j.compenvurbsys.2022.101804</pub-id></element-citation></ref><ref id="B53-sensors-25-05331"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Azizi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hirst</surname><given-names>R.J.</given-names></name><name name-style="western"><surname>Newell</surname><given-names>F.N.</given-names></name><name name-style="western"><surname>Kenny</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Setti</surname><given-names>A.</given-names></name></person-group><article-title>Audio-visual integration is more precise in older adults with a high level of long-term physical activity</article-title><source>PLoS ONE</source><year>2023</year><volume>18</volume><elocation-id>e0292373</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0292373</pub-id><pub-id pub-id-type="pmid">37792786</pub-id><pub-id pub-id-type="pmcid">PMC10550131</pub-id></element-citation></ref><ref id="B54-sensors-25-05331"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Mueen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>H.</given-names></name><name name-style="western"><surname>Trajcevski</surname><given-names>G.</given-names></name><name name-style="western"><surname>Scheuermann</surname><given-names>P.</given-names></name><name name-style="western"><surname>Keogh</surname><given-names>E.</given-names></name></person-group><article-title>Experimental comparison of representation methods and distance measures for time series data</article-title><source>Data Min. Knowl. Discov.</source><year>2013</year><volume>26</volume><fpage>275</fpage><lpage>309</lpage><pub-id pub-id-type="doi">10.1007/s10618-012-0250-5</pub-id></element-citation></ref><ref id="B55-sensors-25-05331"><label>55.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yi</surname><given-names>B.-K.</given-names></name><name name-style="western"><surname>Jagadish</surname><given-names>H.V.</given-names></name><name name-style="western"><surname>Faloutsos</surname><given-names>C.</given-names></name></person-group><article-title>Efficient retrieval of similar time sequences under time warping</article-title><source>Proceedings of the 14th International Conference on Data Engineering</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>23&#8211;27 February 1998</conf-date><fpage>201</fpage><lpage>208</lpage></element-citation></ref><ref id="B56-sensors-25-05331"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>Y.</given-names></name></person-group><article-title>An anomaly detection method based on ship behavior trajectory</article-title><source>Ocean Eng.</source><year>2024</year><volume>293</volume><fpage>116640</fpage><pub-id pub-id-type="doi">10.1016/j.oceaneng.2023.116640</pub-id></element-citation></ref><ref id="B57-sensors-25-05331"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Midtb&#248;</surname><given-names>T.</given-names></name></person-group><article-title>A grid-based approach for measuring similarities of taxi trajectories</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>3118</elocation-id><pub-id pub-id-type="doi">10.3390/s20113118</pub-id><pub-id pub-id-type="pmid">32486430</pub-id><pub-id pub-id-type="pmcid">PMC7309046</pub-id></element-citation></ref><ref id="B58-sensors-25-05331"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rana</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>A.</given-names></name><name name-style="western"><surname>Smith</surname><given-names>D.</given-names></name></person-group><article-title>A semi-supervised approach for activity recognition from indoor trajectory data</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2301.03134</pub-id><pub-id pub-id-type="arxiv">2301.03134</pub-id></element-citation></ref><ref id="B59-sensors-25-05331"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zaman</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>Bajwa</surname><given-names>U.I.</given-names></name><name name-style="western"><surname>Saleem</surname><given-names>G.</given-names></name><name name-style="western"><surname>Raza</surname><given-names>R.H.</given-names></name></person-group><article-title>A robust deep networks based multi-object multi-camera tracking system for city scale traffic</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>17163</fpage><lpage>17181</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-16243-7</pub-id></element-citation></ref><ref id="B60-sensors-25-05331"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amosa</surname><given-names>T.I.</given-names></name><name name-style="western"><surname>Sebastian</surname><given-names>P.</given-names></name><name name-style="western"><surname>Izhar</surname><given-names>L.I.</given-names></name><name name-style="western"><surname>Ibrahim</surname><given-names>O.</given-names></name><name name-style="western"><surname>Ayinla</surname><given-names>L.S.</given-names></name><name name-style="western"><surname>Bahashwan</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Bala</surname><given-names>A.</given-names></name><name name-style="western"><surname>Samaila</surname><given-names>Y.A.</given-names></name></person-group><article-title>Multi-camera multi-object tracking: A review of current trends and future advances</article-title><source>Neurocomputing</source><year>2023</year><volume>552</volume><fpage>126558</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2023.126558</pub-id></element-citation></ref><ref id="B61-sensors-25-05331"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Berclaz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fleuret</surname><given-names>F.</given-names></name><name name-style="western"><surname>Turetken</surname><given-names>E.</given-names></name><name name-style="western"><surname>Fua</surname><given-names>P.</given-names></name></person-group><article-title>Multiple object tracking using k-shortest paths optimization</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2011</year><volume>33</volume><fpage>1806</fpage><lpage>1819</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.21</pub-id><pub-id pub-id-type="pmid">21282851</pub-id></element-citation></ref><ref id="B62-sensors-25-05331"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alaba</surname><given-names>S.Y.</given-names></name></person-group><article-title>GPS-IMU Sensor Fusion for Reliable Autonomous Vehicle Position Estimation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2405.08119</pub-id><pub-id pub-id-type="arxiv">2405.08119</pub-id></element-citation></ref><ref id="B63-sensors-25-05331"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tran</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Vu</surname><given-names>T.N.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>T.V.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>K.</given-names></name></person-group><article-title>UIT-ADrone: A novel drone dataset for traffic anomaly detection</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2023</year><volume>16</volume><fpage>5590</fpage><lpage>5601</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2023.3285905</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05331-f001" orientation="portrait"><label>Figure 1</label><caption><p>GICEDCAM knowledge graph.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g001.jpg"/></fig><fig position="float" id="sensors-25-05331-f002" orientation="portrait"><label>Figure 2</label><caption><p>GICEDCAM framework data pipeline.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g002.jpg"/></fig><fig position="float" id="sensors-25-05331-f003" orientation="portrait"><label>Figure 3</label><caption><p>Workflow of the trajectory analysis approach to detect a missing event.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g003.jpg"/></fig><fig position="float" id="sensors-25-05331-f004" orientation="portrait"><label>Figure 4</label><caption><p>Fuzzy ROI map of the person picking the bottle, the person holding the bottle and moving to the door, and the person opening the door.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g004.jpg"/></fig><fig position="float" id="sensors-25-05331-f005" orientation="portrait"><label>Figure 5</label><caption><p>Event-aware DeepSORT workflow.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g005.jpg"/></fig><fig position="float" id="sensors-25-05331-f006" orientation="portrait"><label>Figure 6</label><caption><p>Workflow of enhancing trajectory data.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g006.jpg"/></fig><fig position="float" id="sensors-25-05331-f007" orientation="portrait"><label>Figure 7</label><caption><p>Camera locations in the CCIT building.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g007.jpg"/></fig><fig position="float" id="sensors-25-05331-f008" orientation="portrait"><label>Figure 8</label><caption><p>The zones map of the roundabout right-of-way violation scenario. A1, and A2 are the streets located in the south of the roundabout SQ. B1, and B2 are the sreets crossing the east of the roundabout SQ. C1, and C2 are located in the north of the roundabout SQ.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g008.jpg"/></fig><fig position="float" id="sensors-25-05331-f009" orientation="portrait"><label>Figure 9</label><caption><p>Implemented GICEDCAM data pipeline architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g009.jpg"/></fig><fig position="float" id="sensors-25-05331-f010" orientation="portrait"><label>Figure 10</label><caption><p>Latency values of GICEDCAM and VIDCEP vs. objects per frame.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g010.jpg"/></fig><fig position="float" id="sensors-25-05331-f011" orientation="portrait"><label>Figure 11</label><caption><p>Memory usage values of GICEDCAM and VIDCEP vs. objects per frame.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g011.jpg"/></fig><fig position="float" id="sensors-25-05331-f012" orientation="portrait"><label>Figure 12</label><caption><p>Knowledge graph of the Scenario 2 complex event.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g012.jpg"/></fig><fig position="float" id="sensors-25-05331-f013" orientation="portrait"><label>Figure 13</label><caption><p>Person trajectories classified by global tracking_id: (<bold>left</bold>) before applying event-aware deep sort. (<bold>right</bold>) after applying event-aware deep sort.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05331-g013.jpg"/></fig><table-wrap position="float" id="sensors-25-05331-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t001_Table 1</object-id><label>Table 1</label><caption><p>ODSM-based CED frameworks&#8217; strengths and limitations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Framework</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Event Matching Strategy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Strengths</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Limitations</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>EventNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Video ontology linking object relationships to complex event concepts; semantic querying</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Structured semantic representation; supports ontology-driven search</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Requires manual ontology creation; limited scalability for real-time multi-camera networks</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Trajectory-based Models (e.g., hypergraph pairing)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Assign event semantics through trajectory pairing; cluster-based abnormal behavior detection</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Good for motion-based behavior recognition; captures trajectory semantics</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitive to tracking errors; limited handling of static object interactions</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Hierarchical Models</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-layered feature aggregation from frame to temporal concepts</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reduces error propagation; modular event detection</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">overfits on relatively simple events, leading to lower performance than certain baseline methods</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>VideoStorm</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lag- and quality-aware query processing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resource-efficient allocation; adaptable query performance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Focused on query performance, not false positive/negative reduction</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>BLAZEIT</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FrameQL-based query optimization over DNN inference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lowers DNN computation cost; selective frame analysis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Limited to simpler patterns; less effective for multi-camera spatial relationships</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Logical Reasoning Hybrid</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Combines logical reasoning with simple event detection</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Enhances semantic interpretation; flexible rule creation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Higher computation cost; less optimized for large-scale streaming</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>VIDCEP</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ODSM with VEQL and Video Event Knowledge Graph (VEKG)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flexible spatiotemporal queries; domain-independent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully stateful &#8594; high computational cost; scalability issues with large object counts</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>MERN</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Semantic-rich event representation; multi-entity relation network</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Strong semantic modelling; supports multi-entity interactions</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Relies heavily on the completeness and accuracy of domain ontologies, which may limit generalization to new domains; integration of DL models with semantic CEP adds system complexity</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>NLP-guided Ontologies</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Uses NLP to enhance video event ontologies</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improved semantic matching; better generalization from text cues</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dependent on high-quality NLP models; limited spatial reasoning</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>NOP</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Notification-Oriented Paradigm with chain-based queries</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Efficient event chaining for specific domains</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Restricted to certain event types; lacks general-purpose scalability</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05331-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t002_Table 2</object-id><label>Table 2</label><caption><p>Complex event detection test scenarios details.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Camera Locations</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Videos</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A person picks up the bottle from the chair in Room 361, then passes 361B corridor, then pours it into the mug located on the shelf in 361A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F, A, E</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A person enters corridor 361B and picks up the bottle next to the printer. Then enter corridor 361Z and exit it. Then, enter 301Z and put the bottle on the bin door.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A, B, C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Roundabout right-of-way violation: A motorbike enters the roundabout SQ from street C1 before a car enters the roundabout SQ from street B1. Then, the car blocks the motorbike&#8217;s path and leaves the Square earlier than the motorbike.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">International University&#8212;VNU-HCM roundabout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A person picks up the bottle from the table, moves to the door, opens the door, and exits corridor 361B. Every time a mandatory error is added to the spatial event (persons pick up the bottle). Occultation, losing track, and false negative object detection. </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05331-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t003_Table 3</object-id><label>Table 3</label><caption><p>Latency under load for GICEDCAM and VIDCEP.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Video Streams</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">End-to-End Processing Time GICEDCAM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Latency Under Load GICEDCAM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">End-to-End Processing Time VIDCEP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Latency Under Load VIDCEP</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.4 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.1 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.3 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.2 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.1 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.1 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.3 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.8 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.6 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.9 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.8 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.2 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.8 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.2 s </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.4 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.4 s</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05331-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t004_Table 4</object-id><label>Table 4</label><caption><p>End-to-end latency values for GICEDCAM and VIDCEP.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GICEDCAM Latency</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">VIDCEP Latency</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">One</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.0 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.1 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Two</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.9 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Three</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.5 s</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05331-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t005_Table 5</object-id><label>Table 5</label><caption><p>GICEDCAM and VIDCEP CPU and memory usage values.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GICEDCAM Memory (MB)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">VIDCEP Memory (MB)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GICEDCAM CPU (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">VIDCEP CPU (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">One</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">540</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1320</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Two</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">670</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1580</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Three</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">780</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1750</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05331-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t006_Table 6</object-id><label>Table 6</label><caption><p>Precision and recall of BN, LSTM, and trajectory-based approaches for spatial event correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.74</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trajectory analysis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.78</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05331-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t007_Table 7</object-id><label>Table 7</label><caption><p>Processing time values for BN, LSTM, and trajectory analysis spatial event correction methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BN Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">LSTM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Trajectory Analysis</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">One</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.12 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.60 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.70 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Two</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.14 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.65 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Three</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.20 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.75 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Four</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.10 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.42 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.51 s</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05331-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05331-t008_Table 8</object-id><label>Table 8</label><caption><p>Latency and the precision values of the proposed event-aware DeepSORT for Scenarios 2, 3, and 4.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario 2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario 3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Scenario 4</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision (DeepSORT)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision (Event aware DeepSORT)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency (DeepSORT)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.15 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latency (Event aware DeepSORT)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.22 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.37 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.40 s</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>