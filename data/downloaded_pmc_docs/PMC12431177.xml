<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431177</article-id><article-id pub-id-type="pmcid-ver">PMC12431177.1</article-id><article-id pub-id-type="pmcaid">12431177</article-id><article-id pub-id-type="pmcaiid">12431177</article-id><article-id pub-id-type="doi">10.3390/s25175306</article-id><article-id pub-id-type="publisher-id">sensors-25-05306</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>TriQuery: A Query-Based Model for Surgical Triplet Recognition</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Yao</surname><given-names initials="M">Mengrui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05306" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-9897-8416</contrib-id><name name-style="western"><surname>Zhang</surname><given-names initials="W">Wenjie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05306" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="L">Lin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-05306" ref-type="aff">2</xref><xref rid="af3-sensors-25-05306" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhao</surname><given-names initials="Z">Zhongwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-05306" ref-type="aff">2</xref><xref rid="c1-sensors-25-05306" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9426-0051</contrib-id><name name-style="western"><surname>Jia</surname><given-names initials="X">Xiao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05306" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Kollias</surname><given-names initials="S">Stefanos</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05306"><label>1</label>School of Control Science and Engineering, Shandong University, Jinan 250100, China; <email>akiliyao7@gmail.com</email> (M.Y.); <email>jiaxiao@sdu.edu.cn</email> (X.J.)</aff><aff id="af2-sensors-25-05306"><label>2</label>Department of Urology, Qilu Hospital of Shandong University, Shandong University, Jinan 250100, China</aff><aff id="af3-sensors-25-05306"><label>3</label>Cheeloo College of Medicine, Shandong University, Jinan 250100, China</aff><author-notes><corresp id="c1-sensors-25-05306"><label>*</label>Correspondence: <email>zhaozhongweixy@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5306</elocation-id><history><date date-type="received"><day>16</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>16</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>22</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05306.pdf"/><abstract><p>Artificial intelligence has shown great promise in advancing intelligent surgical systems. Among its applications, surgical video action recognition plays a critical role in enabling accurate intraoperative understanding and decision support. However, the task remains challenging due to the temporal continuity of surgical scenes and the long-tailed, semantically entangled distribution of action triplets composed of instruments, verbs, and targets. To address these issues, we propose TriQuery, a query-based model for surgical triplet recognition and classification. Built on a multi-task Transformer framework, TriQuery decomposes the complex triplet task into three semantically aligned subtasks using task-specific query tokens, which are processed through specialized attention mechanisms. We introduce a Multi-Query Decoding Head (MQ-DH) to jointly model structured subtasks and a Top-K Guided Query Update (TKQ) module to incorporate inter-frame temporal cues. Experiments on the CholecT45 dataset demonstrate that TriQuery achieves improved overall performance over existing baselines across multiple classification tasks. Attention visualizations further show that task queries consistently attend to semantically relevant spatial regions, enhancing model interpretability. These results highlight the effectiveness of TriQuery for advancing surgical video understanding in clinical environments.</p></abstract><kwd-group><kwd>surgical triplet recognition</kwd><kwd>query-based learning</kwd><kwd>transformer</kwd><kwd>surgical video analysis</kwd><kwd>multi-task learning</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62303279</award-id></award-group><award-group><funding-source>Shandong Provincial Natural Science Foundation</funding-source><award-id>2023HWYQ-027</award-id></award-group><award-group><funding-source>Taishan Scholars Youth Expert Program</funding-source><award-id>tsqn202306086</award-id></award-group><award-group><funding-source>Shandong Provincial Natural Science Foundation</funding-source><award-id>ZR2024QH065</award-id></award-group><award-group><funding-source>China Postdoctoral Science Foundation</funding-source><award-id>2024M761859</award-id></award-group><funding-statement>This research was funded in part by the National Natural Science Foundation of China under Grant 62303279, in part by the Shandong Provincial Natural Science Foundation under Grant 2023HWYQ-027, in part by the Taishan Scholars Youth Expert Program under Grant tsqn202306086, in part by the Shandong Provincial Natural Science Foundation under Grant ZR2024QH065, and in part by the China Postdoctoral Science Foundation under Grant 2024M761859.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05306"><title>1. Introduction</title><p>Minimally invasive surgeries, such as laparoscopy and arthroscopy, are increasingly recorded using high-resolution intraoperative video systems. These videos contain rich spatiotemporal information that, if accurately and efficiently parsed, can support intraoperative decision-making, postoperative skill evaluation, and complication risk prediction [<xref rid="B1-sensors-25-05306" ref-type="bibr">1</xref>]. In robotic surgeries such as those conducted with the da Vinci system, such analysis can further enhance manipulation precision and safety. Ultimately, the development of autonomous surgical systems hinges on reliable video-based scene understanding.</p><p>Among various video understanding tasks, surgical triplet recognition has gained increasing attention for its ability to interpret fine-grained surgical interactions by identifying structured triplets in the form of instrument&#8211;verb&#8211;target (e.g., hook&#8211;dissect&#8211;gallbladder). This structured information supports real-time risk alerts, objective skill assessment, and prior knowledge and safety constraints for intelligent surgical systems [<xref rid="B2-sensors-25-05306" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05306" ref-type="bibr">3</xref>].</p><p>However, surgical triplet recognition in video analysis remains challenging due to two main factors. The first is the class imbalance in triplet distributions, where a few frequent combinations dominate and many rare but clinically important ones are underrepresented. The second is the temporal inconsistency caused by factors like bleeding, tool switching, and abrupt camera motion, which disrupt the coherence of frame-level predictions. Moreover, clinical deployment demands model interpretability, necessitating visual explanations such as attention maps or saliency regions.</p><p>In recent years, deep learning has significantly advanced surgical video understanding. Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) architectures have been widely applied but often struggle with capturing both fine spatial details and long-range temporal dependencies [<xref rid="B4-sensors-25-05306" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05306" ref-type="bibr">5</xref>]. Transformer-based models, especially the Swin Transformer and its lightweight variant Swin Transformer Tiny (Swin-T), offer improved multi-scale context modeling through hierarchical attention mechanisms [<xref rid="B6-sensors-25-05306" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05306" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05306" ref-type="bibr">8</xref>]. Multi-task learning and temporal attention methods have also been explored to better represent complex surgical interactions [<xref rid="B9-sensors-25-05306" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05306" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05306" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05306" ref-type="bibr">12</xref>]. Despite these advances, current methods often overlook the dual challenges of long-tail distributions and temporal instability. Many operate at the frame level and lack mechanisms for semantic disentanglement and robust temporal modeling, limiting their generalizability and reliability in real-world clinical settings. A more comprehensive review of related methods is provided in <xref rid="sec2-sensors-25-05306" ref-type="sec">Section 2</xref> to highlight their strengths and limitations in detail.</p><p>In this paper, we propose TriQuery, a lightweight and query-based framework tailored for fine-grained surgical triplet recognition. TriQuery integrates several carefully designed components to handle both spatial and temporal challenges in surgical video analysis. Specifically, it leverages a Swin-T-based backbone for multi-scale visual encoding, enabling effective extraction of spatial and contextual features from high-resolution surgical frames. To handle the problem of class imbalance and semantic entanglement, we design a Multi-Query Decoding Head (MQ-DH) that introduces structured multi-task learning. By associating task-specific query groups with instruments, verbs, and targets, MQ-DH enables semantic disentanglement and strengthens the model&#8217;s ability to generalize across underrepresented categories. To address temporal inconsistency, we propose the Top-K Guided Query Update (TKQ) module, which selectively reuses high-confidence queries from the previous frame in a lightweight manner. This design enhances temporal coherence across frames without incurring significant computational overhead, ensuring stable and efficient inference. Furthermore, TriQuery supports interpretable attention visualization by exposing cross-attention weights between queries and image features, offering insights into the model&#8217;s decision process and supporting trustworthy deployment in clinical settings.</p><p>Our contributions are summarized as follows:<list list-type="simple"><list-item><label>1.</label><p>We propose TriQuery, a query-centric multi-task framework for surgical video recognition. It jointly models instruments, verbs, targets, and their triplet combinations through task-specific queries and a dedicated MQ-DH module, enabling semantic disentanglement and mitigating long-tailed class imbalance.</p></list-item><list-item><label>2.</label><p>We introduce the TKQ module, which reuses high-confidence queries from the previous frame to guide current decoding, enhancing temporal consistency with minimal overhead.</p></list-item><list-item><label>3.</label><p>Our framework achieves superior performance on the CholecT45 dataset, outperforming baseline methods while providing attention visualizations that facilitate clinical applicability.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05306"><title>2. Related Work</title><p>Building on the challenges and motivations outlined in <xref rid="sec1-sensors-25-05306" ref-type="sec">Section 1</xref>, this section reviews the relevant prior work in surgical video analysis and related domains. We focus on four methodological directions: spatiotemporal modeling strategies, windowed vision transformers, learnable query-based structured recognition, and temporal consistency modeling.</p><p><bold>Spatiotemporal Modeling with Convolutional and Recurrent Networks.</bold> Early approaches typically decomposed videos into individual frames, employing 2D CNNs for spatial feature extraction and using RNNs or temporal convolutions for sequential modeling. Alternatively, 3D CNN-based models such as C3D and I3D were introduced to jointly learn spatiotemporal representations. However, these methods struggle to capture long-range semantic dependencies and often fail to model multi-scale features effectively. Moreover, their computational complexity tends to increase significantly with longer temporal windows. To address these limitations, recent studies have explored enhancing CNN backbones with attention modules to improve feature expressiveness [<xref rid="B13-sensors-25-05306" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05306" ref-type="bibr">14</xref>]. Wu et al. [<xref rid="B15-sensors-25-05306" ref-type="bibr">15</xref>] proposed a CNN&#8211;self-attention hybrid architecture for wireless signal recognition, demonstrating that attention mechanisms can boost discriminative capacity even in non-visual domains. These findings further support the incorporation of self-attention into spatial feature extractors for video-based recognition tasks.</p><p><bold>Windowed Vision Transformers.</bold> Swin Transformer [<xref rid="B6-sensors-25-05306" ref-type="bibr">6</xref>] introduces a window-based self-attention scheme that computes attention within local non-overlapping windows. Through shifted windowing across layers, it enables global information exchange while maintaining computational scalability. This architecture has demonstrated strong performance in medical image segmentation and detection tasks. However, its single-scale token representation may struggle to capture small structures or subtle semantic cues, especially in scenarios with class-imbalanced distributions. In related visual modeling work, Xu and Wang [<xref rid="B16-sensors-25-05306" ref-type="bibr">16</xref>] proposed a cascaded non-local mean network with dual-path fusion for single-image super-resolution, underscoring the value of multi-branch architectures and non-local interactions. Their findings emphasize the importance of complementary pathways and long-range dependencies in preserving fine-grained information, which aligns with the challenges faced in surgical action recognition under low-resolution or cluttered visual conditions.</p><p><bold>Learnable query-based structured recognition.</bold> Inspired by the DEtection TRansformer (DETR) [<xref rid="B17-sensors-25-05306" ref-type="bibr">17</xref>], learnable query-based paradigms have been adapted for structured recognition tasks across domains. In surgical video analysis, Nwoye et al. [<xref rid="B18-sensors-25-05306" ref-type="bibr">18</xref>] proposed Rendezvous, a query-based model that adopts separate sets of task-specific queries to independently recognize instruments, verbs, and targets, enabling explicit modeling of role-specific semantics. While this design offers strong modularity and interpretability, it lacks mechanisms for inter-frame temporal modeling and operates on single-scale spatial features. As a result, it struggles with temporal discontinuities and ambiguous instrument&#8211;verb&#8211;target associations under motion blur or occlusion. Similarly, Tufail et al. [<xref rid="B19-sensors-25-05306" ref-type="bibr">19</xref>] leveraged task-specific queries for document layout and entity extraction, and Liu et al. [<xref rid="B20-sensors-25-05306" ref-type="bibr">20</xref>] employed query-driven target generation in video keyframes. These efforts underscore the versatility of query mechanisms across modalities. However, most existing approaches process frames independently and overlook temporal dynamics, limiting their robustness in visually complex or temporally unstable scenarios.</p><p><bold>Temporal Consistency and Inter-frame Modeling.</bold> To enhance temporal reasoning in video understanding, prior works have explored approaches such as optical flow alignment, two-stream architectures, and temporal Transformers [<xref rid="B21-sensors-25-05306" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05306" ref-type="bibr">22</xref>]. While these methods effectively capture motion dynamics, they often incur high computational costs, are sensitive to occlusions, and are less suited for clinical deployment. Moreover, most existing query-based models treat frames independently and lack mechanisms for lightweight temporal integration within the query space, limiting their ability to enforce temporal continuity or smooth predictions across frames.</p></sec><sec sec-type="methods" id="sec3-sensors-25-05306"><title>3. Methods</title><p>In this paper, we propose TriQuery, a lightweight and query-based framework for surgical video action recognition. It is designed to address the challenges of class imbalance with semantic entanglement and temporal inconsistency in surgical video analysis by integrating a Swin-T visual encoder, a multi-query attention decoder, and a cross-frame guidance module. TriQuery takes each laparoscopic video frame as input and performs multi-task classification to predict the instrument&#8211;verb&#8211;target triplet and its individual components.</p><sec id="sec3dot1-sensors-25-05306"><title>3.1. TriQuery Architecture Overview</title><p>An overview of the TriQuery architecture is illustrated in <xref rid="sensors-25-05306-f001" ref-type="fig">Figure 1</xref>. TriQuery is an end-to-end framework that integrates three core components: a Swin-T backbone for visual encoding, a Multi-Query Decoding Head (MQ-DH) module for structured multi-task classification, and a Top-K Query Update (TKQ) module for lightweight temporal consistency enhancement.</p><p>Given a sequence of laparoscopic video frames as input (e.g., <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), TriQuery first employs the Swin-T encoder to extract hierarchical multi-scale spatial features. These features are then processed by the MQ-DH module, which supports four parallel classification tasks: one triplet classifier and three auxiliary branches for the recognition of instrument, verb, and target. The three auxiliary branches are equipped with dedicated sets of learnable task-specific queries, which interact with the shared visual features through cross-attention mechanisms, enabling the model to distill semantically disentangled representations for each subtask. The triplet classification head operates without explicit query interaction but benefits from the supervision signals and feature regularization provided by the auxiliary branches. To enhance temporal stability, the TKQ module uses the top-K high-confidence query predictions from the previous frame and fuses them with the current frame&#8217;s queries through residual addition. This mechanism preserves causal inference while injecting temporal priors to smooth frame-wise predictions. The entire model is jointly optimized using a combination of cross-entropy loss for classification and Kullback&#8211;Leibler (KL) divergence loss between consecutive predictions to enforce temporal consistency and reduce prediction jitter. Further details are provided in the following sections.</p></sec><sec id="sec3dot2-sensors-25-05306"><title>3.2. Swin-T Backbone for Hierarchical Visual Feature Extraction</title><p>We adopt Swin-T as the backbone of TriQuery for hierarchical visual feature extraction. Each video frame is first partitioned into non-overlapping <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> patches, which are flattened and linearly projected into fixed-dimensional embeddings. These embeddings are then processed by a four-stage encoder composed of Swin Transformer blocks. This backbone is also used as a baseline model in our ablation studies to facilitate fair comparisons with TriQuery&#8217;s enhanced variants.</p><p>Each block alternates between window-based multi-head self-attention (W-MSA) and shifted window multi-head self-attention (SW-MSA). W-MSA operates locally within each window to preserve fine-grained spatial details, while SW-MSA introduces cross-window interactions via a cyclic shift strategy. This design effectively balances local spatial precision with global contextual awareness.</p><p>As the encoding proceeds through stages, the model progressively downsamples the feature maps and expands the semantic receptive field, producing multi-scale and multi-level contextual features that are well-suited for downstream query-based decoding and classification tasks. The feature outputs are projected to a common dimensionality and fed into the MQ-DH module, enabling the model to extract fine-grained and high-level semantic information effectively.</p></sec><sec id="sec3dot3-sensors-25-05306"><title>3.3. Multi-Query Decoding Head for Structured Multi-Task Learning</title><p>To address the structured nature of surgical action recognition, particularly the long-tailed distribution and entangled label semantics inherent in triplet annotations, we propose the MQ-DH composed of four classification branches. These include a direct 100-class triplet classifier and three auxiliary query-based branches that separately predict instruments (6 classes), verbs (10 classes), and targets (15 classes). The number of classes in each subtask follows the category definitions provided in the CholecT45 dataset, as described in <xref rid="sec4dot1-sensors-25-05306" ref-type="sec">Section 4.1</xref>. The auxiliary branches decompose each action triplet into its semantic components (e.g., instrument, verb, and target) and utilize dedicated groups of learnable task-specific queries. By jointly modeling component-level and triplet-level semantics, the MQ-DH facilitates multi-task learning and helps mitigate the effects of class imbalance.</p><p>Each query group interacts with shared visual features extracted from the Swin-T backbone through a Transformer-based cross-attention mechanism, enabling task-oriented semantic representation learning. This design promotes semantic disentanglement across subtasks, allows for direct supervision on individual components, and helps to alleviate the long-tailed distribution problem by decoupling rare triplets into more balanced subtasks, thereby improving generalization on underrepresented classes.</p><p>To support multi-task learning in parallel, each input frame <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is first passed through the backbone to produce patch-level visual features <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">L</italic> is the number of image patches and <italic toggle="yes">C</italic> the feature dimension. For each task <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mi>triplet</mml:mi><mml:mo>,</mml:mo><mml:mi>instrument</mml:mi><mml:mo>,</mml:mo><mml:mi>verb</mml:mi><mml:mo>,</mml:mo><mml:mi>target</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, a set of learnable query vectors <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>&#964;</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is defined, where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>&#964;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the number of classes for task <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula>. The task-specific cross-attention is computed as follows:<disp-formula id="FD1-sensors-25-05306"><label>(1)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>Attn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are learnable projection matrices for the query, key, and value embeddings, and <italic toggle="yes">d</italic> is the attention head dimension.</p><p>The resulting attention features are refined through residual connections, layer normalization (LN), and a feedforward network (FFN) to produce the task-specific representations:<disp-formula id="FD2-sensors-25-05306"><label>(2)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">Q</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>FFN</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>LN</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>Attn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Each query group is followed by a lightweight classification head, and all tasks are jointly optimized using a weighted cross-entropy loss, which ensures multi-task consistency and addresses label imbalance. Additionally, the learned attention weights provide interpretability by highlighting task-specific spatial focus regions, enabling the generation of attention maps that offer insights into model behavior and support clinical trust. The output <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">Q</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> from each task-specific decoding head is further pooled and flattened to obtain a compact task embedding <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is then fed into the corresponding classification heads for final prediction.</p></sec><sec id="sec3dot4-sensors-25-05306"><title>3.4. Top-K Guided Query Update for Temporal Coherence</title><p>To enhance temporal consistency and preserve semantic continuity across consecutive video frames, we introduce a lightweight TKQ module. During both training and inference, the top-5 class predictions from the previous frame&#8217;s triplet task logits are selected. This choice is based on the empirical observation that each frame in the CholecT45 dataset typically contains no more than three distinct action components (i.e., instruments, verbs, or targets). Selecting five top predictions for each component group (<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) allows for a small buffer to capture potentially emerging or uncertain actions while avoiding excessive noise. The corresponding query vectors are then used to guide the initialization of the current frame&#8217;s triplet task queries via a residual fusion mechanism. This design enables soft temporal memory propagation across frames, improving frame-level stability without relying on recurrent structures. The detailed flow of the TKQ module is illustrated in <xref rid="sensors-25-05306-f002" ref-type="fig">Figure 2</xref>, which shows how the queries are updated in our model.</p><p>Specifically, the previous frame <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is passed through the shared backbone and the decoder to obtain the class logits <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The top class indices <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:math></inline-formula> with the highest confidence scores are selected, and their corresponding query vectors <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> from the embedding table are projected to the decoder dimension <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> via a linear transformation.</p><p>These selected vectors are then integrated into the current frame&#8217;s query set <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> through residual blending:<disp-formula id="FD3-sensors-25-05306"><label>(3)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8592;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is a predefined fusion weight selected empirically, which is set to 0.7 to balance temporal continuity from the previous frame with adaptability to current frame features.</p><p>This fusion injects temporal priors into the current queries while preserving their adaptability, thereby promoting smooth transitions across frames. The mechanism introduces a lightweight and causal form of temporal memory without relying on explicit motion modeling (e.g., optical flow). The enhanced queries then attend to current visual features through cross-attention.</p><p>To further stabilize temporal dynamics, we impose a KL divergence constraint between consecutive frame predictions, which penalizes abrupt changes and regularizes inter-frame consistency. Compared to heavy temporal models or frame-stacking strategies, the proposed TKQ module adds minimal computational overhead while substantially improving prediction smoothness and robustness, particularly in challenging scenarios involving occlusions, camera motion, or frequent tool switching. Furthermore, the design preserves causality, making it well-suited for online inference settings.</p></sec><sec id="sec3dot5-sensors-25-05306"><title>3.5. Loss Function</title><p>The total loss <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-script">L</mml:mi><mml:mi>total</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is designed to jointly optimize all four classification branches unified under the MQ-DH, including the triplet classification and three auxiliary subtasks for instrument, verb, and target recognition. Each branch is supervised using an independent cross-entropy loss, and their contributions are combined as a weighted sum. In addition, we incorporate KL divergence loss between the triplet prediction distributions of consecutive frames to promote temporal consistency across video frames. The final training objective is formulated as<disp-formula id="FD4-sensors-25-05306"><label>(4)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-script">L</mml:mi><mml:mi>total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi>Tri</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold-script">L</mml:mi><mml:mi>KL</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi>Tri</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the binary cross-entropy loss for overall multi-label recognition, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>ce</mml:mi></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> are the task-specific classification losses, and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-script">L</mml:mi><mml:mi>KL</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the KL divergence loss between the prediction distributions of the current frame and the previous frame. The weights <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are empirically chosen to balance classification and temporal regularization.</p><p>During inference, the model first extracts spatial&#8211;temporal features from each input frame using the Swin Transformer backbone. The feature map is processed with global average pooling to produce a compact representation <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This representation is passed to four independent classification heads to produce logits for each task:<disp-formula id="FD5-sensors-25-05306"><label>(5)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>Tri</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Tri</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>Tri</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="normal">I</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="normal">V</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="normal">T</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold">f</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the element-wise sigmoid function, and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:math></inline-formula> are learnable parameters of the linear classifier heads. The final multi-label predictions are obtained by thresholding the outputs:<disp-formula id="FD6-sensors-25-05306"><label>(6)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the indicator function, and the function is used to convert the predicted probability into the encoding of the predicted label.</p></sec></sec><sec id="sec4-sensors-25-05306"><title>4. Experimental Setup</title><sec id="sec4dot1-sensors-25-05306"><title>4.1. Dataset</title><p>We conduct experiments on the CholecT45 dataset [<xref rid="B18-sensors-25-05306" ref-type="bibr">18</xref>], a fine-grained surgical video action recognition benchmark specifically designed for laparoscopic cholecystectomy procedures. CholecT45 is derived from the CholecT50 collection and was officially released in April 2022. It comprises 45 endoscopic videos recorded at 1 FPS, with a total of 90,489 annotated frames and 127,385 labeled triplet instances. Each frame is annotated with one or more action triplets in the form of instrument&#8211;verb&#8211;target, comprising 100 unique combinations derived from 6 instrument classes, 10 verb classes, and 15 target classes.</p><p>Compared to earlier surgical datasets with coarse-grained annotations (e.g., surgical phases), CholecT45 adopts a fine-grained and semantically structured labeling scheme, enabling a more detailed understanding of surgical actions. The dataset also provides mapping files to recover individual component labels, enabling structured learning.</p><p>To highlight the class imbalance in CholecT45, we visualize the distribution of triplet classes using both a pie chart and a bar chart, as shown in <xref rid="sensors-25-05306-f003" ref-type="fig">Figure 3</xref>. The two most frequent classes account for nearly half of all instances, while the 89 least frequent classes together comprise only 14.6% of the dataset. This long-tailed distribution presents difficulties for multi-class classification tasks.</p></sec><sec id="sec4dot2-sensors-25-05306"><title>4.2. Evaluation Metrics</title><p>To ensure fair and reproducible evaluation, we follow the standard five-fold cross-validation protocol recommended for CholecT45 [<xref rid="B23-sensors-25-05306" ref-type="bibr">23</xref>]. Each fold contains 36 videos for training and 9 for testing, and we report the averaged results over all five folds. All models are trained and evaluated under this consistent setting.</p><p>We evaluate all classification tasks (triplets and subtasks), using the accuracy score (Acc), F1-score (F1), and average precision (AP). For consistency, metric names are reused across tasks without explicit task-specific notation and are assumed to refer to the current task under evaluation unless otherwise stated.</p><p>Given the ground-truth label matrix <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the predicted binary label matrix <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">N</italic> is the number of samples and <italic toggle="yes">C</italic> is the number of classes, the accuracy score is defined as<disp-formula id="FD7-sensors-25-05306"><label>(7)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Acc</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mi mathvariant="double-struck">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the indicator function. The F1-score is computed based on the total number of True Positives (TP), False Positives (FP), and False Negatives (FN) over the evaluation set, using the standard formula<disp-formula id="FD8-sensors-25-05306"><label>(8)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>Precision</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>Recall</mml:mi></mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>+</mml:mo><mml:mi>Recall</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05306"><label>(9)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>Precision</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05306"><label>(10)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>Recall</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
with <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> being a small constant to prevent division by zero. The AP is computed as the area under the precision&#8211;recall curve for each class and then averaged across all classes:<disp-formula id="FD11-sensors-25-05306"><label>(11)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>AP</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the precision as a function of recall <italic toggle="yes">r</italic> for class <italic toggle="yes">j</italic>.</p></sec><sec id="sec4dot3-sensors-25-05306"><title>4.3. Implementation Details</title><p>All experiments were conducted on an NVIDIA A100-SXM4-40GB GPU. We employ the AdamW optimizer with <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, an initial learning rate of <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and weight decay of 0.05. A cosine annealing learning rate schedule is used with 2 warmup epochs. The batch size is set to 32, and gradient accumulation is used (2 steps) to simulate a larger batch size while keeping memory consumption manageable. The training is stabilized using gradient clipping with a max norm of 1.0. The model is built upon the Swin-T backbone, which performs hierarchical feature extraction with local window-based multi-head self-attention. Based on this backbone, we further introduce a multi-task attention module to jointly model the instrument, verb, and target in a query-based framework. The dataset is split into five folds for cross-validation, and each fold is trained for 10 epochs. To mitigate overfitting, a dropout rate of 0.3 is applied to the Transformer layers. The loss function used is binary cross-entropy (BCE) with logits, appropriate for multi-label triplet prediction.</p></sec></sec><sec id="sec5-sensors-25-05306"><title>5. Experimental Results</title><sec id="sec5dot1-sensors-25-05306"><title>5.1. Comparison with State-of-the-Art Methods</title><p>In this experiment, we evaluate the effectiveness of our proposed TriQuery framework on the CholecT45 dataset by conducting comparative experiments against several widely adopted action recognition methods, including CNN+RNN [<xref rid="B4-sensors-25-05306" ref-type="bibr">4</xref>], CNN&#8211;LSTM (Long Short-Term Memory) [<xref rid="B24-sensors-25-05306" ref-type="bibr">24</xref>], Dual-stream CNN [<xref rid="B25-sensors-25-05306" ref-type="bibr">25</xref>], 3D CNN [<xref rid="B5-sensors-25-05306" ref-type="bibr">5</xref>], Rendezvous [<xref rid="B18-sensors-25-05306" ref-type="bibr">18</xref>], and the Swin-T baseline [<xref rid="B6-sensors-25-05306" ref-type="bibr">6</xref>]. Detailed results are presented in <xref rid="sensors-25-05306-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05306-t002" ref-type="table">Table 2</xref>.</p><p>We present the triplet classification results in <xref rid="sensors-25-05306-t001" ref-type="table">Table 1</xref>, which compares several state-of-the-art methods on the CholecT45 dataset using five-fold cross-validation. For clarity, results from the first three folds are shown along with the average performance. Our proposed TriQuery model outperforms the competing approaches, achieving an accuracy of 71.53% in the 100-way triplet classification task. Compared to Rendezvous (69.11%), CNN+RNN (69.59%), CNN-LSTM (70.54%), Dual-stream CNN (70.78%), and 3D CNN (70.97%), TriQuery demonstrates superior performance, highlighting the strength of our multi-task Transformer-based architecture in modeling both local and global semantics. Notably, TriQuery improves upon the Swin Transformer baseline (68.10%) by 3.43%, demonstrating the effectiveness of our query-driven decoder and temporal consistency modeling. Furthermore, TriQuery achieves an AP of 45.19%, outperforming both Rendezvous (37.98%) and the Swin baseline (40.15%), reflecting its advantage in handling fine-grained semantic associations.</p><p><xref rid="sensors-25-05306-t002" ref-type="table">Table 2</xref> shows the results on the three sub-classification tasks&#8212;instrument (6 classes), verb (10 classes), and target (15 classes)&#8212;averaged across all five folds. TriQuery consistently achieves high performance across the three subtasks as well as the triplet task, validating the effectiveness of its multi-query decoding mechanism for structured multi-task learning. These results also confirm that the Swin-T-based backbone outperforms conventional CNN architectures and that our query-augmented design brings further gains in fine-grained surgical activity recognition.</p></sec><sec id="sec5dot2-sensors-25-05306"><title>5.2. Attention Map Visualization</title><p>To enhance interpretability and gain deeper insights into the semantic behavior and spatial focus of our model, we visualize the attention distributions produced by the task-specific query vectors in the decoding heads.</p><p><xref rid="sensors-25-05306-f004" ref-type="fig">Figure 4</xref> compares the attention maps from the triplet classification branch without auxiliary queries (second row) and those obtained after integrating the multi-query modules (third row). The first row shows the original surgical video frames, over which attention maps are overlaid to better illustrate the spatial distribution of focus. When auxiliary queries are not used, the attention is relatively diffuse and often fails to concentrate on semantically meaningful regions, which can affect classification accuracy. For example, in the rightmost sample (d), a surgical hook appears on the right side of the original image but receives little attention in the second row. After applying the multi-query modules, the attention map in the third row clearly highlights this area, indicating improved semantic alignment. Moreover, the full model exhibits a stronger ability to focus on relevant regions such as instrument tips and active interaction areas. These enhanced attention patterns demonstrate the effectiveness of auxiliary queries in guiding the model toward clinically important features. The aggregated heatmaps from the multi-head attention mechanism further align with areas of surgical interest, such as tool&#8211;tissue contact zones, confirming the benefits of our query-based design in refining spatial focus.</p><p><xref rid="sensors-25-05306-f005" ref-type="fig">Figure 5</xref> presents a comprehensive visualization of attention maps generated by different query groups across three representative surgical frames. Each row corresponds to a representative video frame, with the first column displaying the original image, followed by attention maps generated by three task-specific query decoding heads: instrument, verb, and target. The last column presents the attention map produced by the triplet classification head. The instrument query consistently concentrates on surgical tools such as graspers or hooks; the verb query broadens the focus to include both instruments and dynamic interaction regions; the target query highlights the relevant anatomical structures undergoing manipulation or dissection. These results demonstrate that task-specific queries effectively capture distinct semantic cues, enabling the model to learn disentangled and interpretable representations aligned with the visual semantics of each subtask. Moreover, the results confirm that attention distributions across task-specific queries remain specialized and non-redundant.</p><p>These visualizations collectively demonstrate that the proposed multi-query architecture facilitates semantic separation, producing more localized and interpretable attention patterns compared to baseline methods. By guiding the model to focus on task-relevant regions, the auxiliary queries improve both recognition performance and model interpretability for potential clinical applications.</p><p>To further analyze the temporal modeling behavior of the proposed TKQ module, we visualize the attention maps of the instrument query <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over a short sequence of consecutive frames, as shown in <xref rid="sensors-25-05306-f006" ref-type="fig">Figure 6</xref>. This visualization highlights how <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> propagates its focus across time through query updates guided by the previous frame. The attention regions exhibit smooth transitions and consistently emphasize relevant anatomical structures and surgical instruments. In these four consecutive frames, the grabber, typically located on the left, and the hook, typically located on the right, are effectively tracked during movement, with spatial regions receiving continuous attention across frames clearly highlighted. These results demonstrate that the model maintains temporal coherence and that the attention flow of <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> aligns well with the expected continuity of surgical actions, providing qualitative evidence for the temporal interpretability of our method.</p></sec><sec id="sec5dot3-sensors-25-05306"><title>5.3. Prediction Result Visualization</title><p>To further assess the classification performance of our model, we visualize the frame-wise prediction results of the auxiliary tasks on the CholecT45 test set and compare them with the ground-truth annotations and predictions from the Rendezvous baseline [<xref rid="B18-sensors-25-05306" ref-type="bibr">18</xref>]. Due to the challenge of directly visualizing the full 100-way triplet classification, we decompose the task into three subtasks, including instrument, verb, and target, and generate color-coded visualizations for each.</p><p><xref rid="sensors-25-05306-f007" ref-type="fig">Figure 7</xref>a&#8211;c present visualizations for instrument, verb, and target classification, respectively. Each figure illustrates a timeline of predictions for all frames across the nine test videos (totaling approximately 16,000 frames). The horizontal axis represents the temporal progression of concatenated frames, while the vertical axis contains three label slots per frame to accommodate concurrent actions, such as the simultaneous use of multiple tools. For each frame, the top row displays the ground truth, the middle row shows predictions from the Rendezvous method, and the bottom row presents results from our TriQuery model. Distinct colors are assigned to each class and shown in the legend to facilitate intuitive comparison. The consistent layout across subplots allows direct visual comparison between models and tasks.</p><p>The predictions from TriQuery exhibit strong alignment with the ground truth, demonstrating high frame-level accuracy and smooth temporal transitions. Minor discrepancies are primarily observed during visually challenging conditions, such as occlusion by smoke or abrupt camera motion. In contrast, predictions from Rendezvous tend to fluctuate more and show greater temporal noise. Among the three subtasks, the target classification appears more fragmented, which likely reflects higher intra-class variability and greater semantic ambiguity. Nevertheless, TriQuery still maintains notable consistency with the ground truth. Compared to Rendezvous, our model demonstrates improved performance, especially in the red-highlighted regions, where it more accurately captures fine-grained transitions in challenging frames. This observation aligns well with the quantitative improvements reported in <xref rid="sensors-25-05306-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05306-t002" ref-type="table">Table 2</xref>.</p><p>These visualizations also offer additional insights into the behavior of our model. The task-specific queries are capable of learning distinct semantic focuses for instrument, verb, and target recognition. Moreover, the auxiliary query branches stabilize the triplet head by providing dense gradients, which are particularly beneficial for underrepresented categories. The TKQ module further enhances inter-frame consistency without relying on computationally expensive components such as optical flow or 3D convolutions, making our approach suitable for surgical decision support and robotic control.</p><p>In summary, the visual comparisons across tasks demonstrate the temporal stability and consistency of our model&#8217;s predictions while also highlighting specific frame intervals where errors are likely to occur. These findings offer valuable guidance for future improvements and clinical deployment.</p></sec></sec><sec sec-type="discussion" id="sec6-sensors-25-05306"><title>6. Discussion</title><sec id="sec6dot1-sensors-25-05306"><title>6.1. Ablation Study: Task Queries and Temporal Guidance</title><p>To evaluate the effectiveness of the proposed MQ-DH and TKQ modules, we conducted an ablation study across the instrument, verb, target, and triplet classification tasks. Starting from a baseline model that performs 100-way triplet classification without a query mechanism, we progressively introduced the MQ-DH and TKQ modules into the TriQuery framework. We then assessed both the overall triplet classification performance and the individual accuracy of each subtask (instrument, verb, target). All experiments were conducted using five-fold cross-validation on the CholecT45 dataset. This setup allows us to systematically examine the contribution of each module to the final classification performance and validate the design choices of the proposed architecture.</p><p>As shown in <xref rid="sensors-25-05306-t003" ref-type="table">Table 3</xref>, the baseline model, which uses only the Swin Transformer to extract hierarchical spatiotemporal features, achieved an average triplet classification accuracy of 68.10% and an AP of 40.15%. Upon introducing the MQ-DH module, the model&#8217;s performance improved significantly, reaching 71.32% accuracy and 45.18% AP. Further incorporating the TKQ module led to additional gains, demonstrating the benefit of incorporating temporal context via task-specific guidance. When both modules were integrated, the model achieved the highest performance across all metrics, confirming the complementary nature and effectiveness of the proposed design. These results collectively validate the contribution of each component and highlight the importance of both task-disentangled query learning and temporal consistency for improving performance in fine-grained surgical phase classification.</p></sec><sec id="sec6dot2-sensors-25-05306"><title>6.2. Ablation Study: Module Design Analysis</title><p>To further validate the design of the proposed MQ-DH module, we conduct an ablation study to assess the individual and combined contributions of its components. <xref rid="sensors-25-05306-t004" ref-type="table">Table 4</xref> presents the results across instrument, verb, target, and triplet classification tasks using multiple evaluation metrics.</p><p>The first row reports the baseline performance using the Swin Transformer backbone without any query mechanism. The following rows progressively introduce the instrument query (<inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), verb query (<inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and target query (<inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) to examine their respective impact. The sixth row includes all three task-specific queries, representing the full MQ-DH design, while the final row adds an additional fused triplet query (<inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>Tri</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) for comparison, which, as shown, does not yield further improvement.</p><p>The last two columns of <xref rid="sensors-25-05306-t004" ref-type="table">Table 4</xref> summarize the average triplet classification performance and the best single-fold result. The former reflects the model&#8217;s generalization ability across varying subsets, which is important in the context of severe class imbalance, while the latter highlights its performance potential under favorable data distributions. Since <xref rid="sensors-25-05306-t004" ref-type="table">Table 4</xref> focuses on the fine-grained design of the MQ-DH&#8217;s internal query composition, we primarily describe the triplet (best) performance in the following analysis to better reflect the upper-bound capability of each configuration. In contrast, the performance results in the last column of <xref rid="sensors-25-05306-t001" ref-type="table">Table 1</xref>, along with <xref rid="sensors-25-05306-t002" ref-type="table">Table 2</xref> and <xref rid="sensors-25-05306-t003" ref-type="table">Table 3</xref>, are reported as five-fold averages to ensure fair and benchmarking across methods.</p><p>Focusing on the triplet classification task (100 classes), we observe that the baseline model achieves an average accuracy of 72.24% and an AP of 43.05%. Adding task-specific queries sequentially results in consistent improvements. When all three queries are incorporated, accuracy rises to 75.55% and AP to 48.63%, representing gains of 3.31% and 5.58% over the baseline, respectively. These results support the effectiveness of decomposing the triplet task into structured subtasks and demonstrate that task-specific queries enhance semantic discrimination while mitigating class imbalance.</p><p>Notably, introducing the fused triplet query (<inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>Tri</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) slightly reduces performance, confirming the advantage of disentangled task-specific query design over naive query fusion. This also validates our decision to exclude the fused query in the final architecture.</p></sec><sec id="sec6dot3-sensors-25-05306"><title>6.3. Opportunities for Enhancement</title><p>While the proposed TriQuery framework demonstrates strong performance on the CholecT45 dataset, several opportunities remain for further refinement. At present, the TKQ module leverages only the immediately preceding frame to model temporal context. This design supports causal inference and efficient processing, making it suitable for real-time deployment, and is partly motivated by the characteristics of surgical video data, where sudden scene changes or frame drops can occur. However, this short-term focus may limit the ability to capture long-range dependencies in complex procedural workflows. Future extensions could explore incorporating richer temporal context through memory banks, sliding-window attention, or other temporal aggregation strategies. Moreover, while the current fusion weight <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> in TKQ and the loss weights were empirically chosen to ensure stable convergence and performance, they have been retained as tunable hyperparameters to support future extensions. Incorporating dynamic weighting mechanisms that adapt to different surgical scenarios or training stages may further enhance the model&#8217;s robustness and generalizability.</p><p>Another promising direction for improvement is to enhance the flexibility and adaptability of the query design. While the current use of predefined task-specific queries aligns well with the dataset&#8217;s triplet annotation scheme and offers clear interpretability, it may not generalize effectively to datasets with different task formulations or structures. This limitation motivates the exploration of dynamic query generation strategies, such as those informed by self-supervised learning or hierarchical clustering, which could enable broader adaptability without sacrificing semantic clarity. Furthermore, as the effectiveness of auxiliary queries (instrument, verb, and target) can vary across procedural contexts, incorporating adaptive weighting or dynamic query routing mechanisms may further improve the model&#8217;s robustness and overall performance.</p><p>In addition, we aim to incorporate domain-specific prior knowledge into query initialization and evaluate the framework on broader surgical datasets and deployment scenarios. These directions are critical for building robust, generalizable, and clinically applicable intelligent video understanding systems.</p></sec></sec><sec sec-type="conclusions" id="sec7-sensors-25-05306"><title>7. Conclusions</title><p>In this study, we propose a novel query-based multi-task learning framework for fine-grained surgical video action recognition, TriQuery, which leverages the Swin Transformer backbone and a task-oriented query decoder, and incorporates a query update mechanism to address the inherent challenges of class imbalance and temporal dynamics in laparoscopic surgery scenarios. By decomposing the triplet classification task into three semantically meaningful subtasks&#8212;instrument, verb, and target&#8212;and introducing learnable queries for each task, the model constructs explicit attention on spatially relevant regions. In addition, a cross-frame initialization strategy enhances temporal continuity and context awareness. This design improves interpretability and enhances recognition performance through auxiliary supervision. Experiments on the CholecT45 dataset demonstrate that TriQuery achieves improved performance compared to baseline methods, particularly in recognizing underrepresented categories. It also generates interpretable attention maps that align well with clinical expectations. The model remains lightweight and scalable, making it well-suited for deployment in clinical environments with constrained computational resources. These characteristics underscore the potential of TriQuery for effective integration into intelligent surgical systems. In addition, its query-driven design allows for flexible adaptation to other medical video analysis tasks, including skill assessment, workflow stage recognition, and intraoperative decision support.</p><p>Future work will focus on improving long-range temporal modeling through richer aggregation methods, enhancing the adaptability of query mechanisms via dynamic or self-supervised strategies, and incorporating domain knowledge to boost generalization across broader surgical datasets and deployment environments.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.Y., W.Z. and X.J.; methodology, M.Y., W.Z. and X.J.; software, M.Y. and W.Z.; validation, M.Y., W.Z., L.W., Z.Z. and X.J.; formal analysis, M.Y., W.Z. and L.W.; investigation, M.Y., W.Z. and L.W.; resources, Z.Z. and X.J.; data curation, M.Y., W.Z. and L.W.; writing&#8212;original draft preparation, M.Y. and W.Z.; writing&#8212;review and editing, M.Y., W.Z., Z.Z. and X.J.; visualization, M.Y. and W.Z.; supervision, Z.Z. and X.J.; project administration, X.J.; funding acquisition, Z.Z. and X.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study were composed of the dataset CholecT45. The CholecT45 dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/CAMMA-public/cholect45">https://github.com/CAMMA-public/cholect45</uri> (accessed on 21 August 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05306"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bain</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Holcomb</surname><given-names>C.N.</given-names></name><name name-style="western"><surname>Zeh III</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Sankaranarayanan</surname><given-names>G.</given-names></name></person-group><article-title>Artificial intelligence for improving intraoperative surgical care</article-title><source>Glob. Surg. Educ.-J. Assoc. Surg. Educ.</source><year>2024</year><volume>3</volume><fpage>73</fpage><pub-id pub-id-type="doi">10.1007/s44186-024-00268-z</pub-id></element-citation></ref><ref id="B2-sensors-25-05306"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luk&#225;cs</surname><given-names>E.</given-names></name><name name-style="western"><surname>Levendovics</surname><given-names>R.</given-names></name><name name-style="western"><surname>Haidegger</surname><given-names>T.</given-names></name></person-group><article-title>Enhancing autonomous skill assessment of robot-assisted minimally invasive surgery: A comprehensive analysis of global and gesture-level techniques applied on the JIGSAWS dataset</article-title><source>Acta Polytech. Hung</source><year>2023</year><volume>20</volume><fpage>133</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.12700/APH.20.8.2023.8.8</pub-id></element-citation></ref><ref id="B3-sensors-25-05306"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aspart</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bolmgren</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Lavanchy</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Beldi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Woods</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Padoy</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hosgor</surname><given-names>E.</given-names></name></person-group><article-title>ClipAssistNet: Bringing real-time safety feedback to operating rooms</article-title><source>Int. J. Comput. Assist. Radiol. Surg.</source><year>2022</year><volume>17</volume><fpage>5</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1007/s11548-021-02441-x</pub-id><pub-id pub-id-type="pmid">34297269</pub-id><pub-id pub-id-type="pmcid">PMC8739308</pub-id></element-citation></ref><ref id="B4-sensors-25-05306"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>M.Z.</given-names></name><name name-style="western"><surname>Asraf</surname><given-names>A.</given-names></name><name name-style="western"><surname>Al-Rakhami</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sodhro</surname><given-names>A.H.</given-names></name></person-group><article-title>Diagnosis of COVID-19 from X-rays using combined CNN-RNN architecture with transfer learning</article-title><source>Benchcouncil Trans. Benchmarks Stand. Eval.</source><year>2022</year><volume>2</volume><fpage>100088</fpage><pub-id pub-id-type="doi">10.1016/j.tbench.2023.100088</pub-id></element-citation></ref><ref id="B5-sensors-25-05306"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name></person-group><article-title>Two-stream 3d convolutional neural network for skeleton-based action recognition</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1705.08106</pub-id></element-citation></ref><ref id="B6-sensors-25-05306"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ning</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name></person-group><article-title>Video swin transformer</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date><fpage>3202</fpage><lpage>3211</lpage></element-citation></ref><ref id="B7-sensors-25-05306"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>K.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name></person-group><article-title>All are worth words: A vit backbone for diffusion models</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date></element-citation></ref><ref id="B8-sensors-25-05306"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiao</surname><given-names>E.</given-names></name><name name-style="western"><surname>Leng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>Vision Transformer with window sequence merging mechanism for image classification</article-title><source>Appl. Soft Comput.</source><year>2025</year><volume>171</volume><fpage>112811</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2025.112811</pub-id></element-citation></ref><ref id="B9-sensors-25-05306"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luvizon</surname><given-names>D.C.</given-names></name><name name-style="western"><surname>Picard</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tabia</surname><given-names>H.</given-names></name></person-group><article-title>Multi-task deep learning for real-time 3D human pose estimation and action recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>43</volume><fpage>2752</fpage><lpage>2764</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.2976014</pub-id><pub-id pub-id-type="pmid">32091993</pub-id></element-citation></ref><ref id="B10-sensors-25-05306"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name></person-group><article-title>ASQuery: A query-based model for action segmentation</article-title><source>Proceedings of the 2024 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Niagara Falls, ON, Canada</conf-loc><conf-date>15&#8211;19 July 2024</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B11-sensors-25-05306"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name></person-group><article-title>An overview of multi-task learning</article-title><source>Natl. Sci. Rev.</source><year>2018</year><volume>5</volume><fpage>30</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1093/nsr/nwx105</pub-id></element-citation></ref><ref id="B12-sensors-25-05306"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name></person-group><article-title>Visual SLAM for Dynamic Environment Using Pre-Frame Semantic</article-title><source>Proceedings of the 2024 IEEE International Conference on Unmanned Systems (ICUS)</source><conf-loc>Nanjing, China</conf-loc><conf-date>18&#8211;20 October 2024</conf-date><fpage>37</fpage><lpage>42</lpage></element-citation></ref><ref id="B13-sensors-25-05306"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>N.</given-names></name></person-group><article-title>Action recognition by an attention-aware temporal weighted convolutional neural network</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>1979</elocation-id><pub-id pub-id-type="doi">10.3390/s18071979</pub-id><pub-id pub-id-type="pmid">29933555</pub-id><pub-id pub-id-type="pmcid">PMC6069475</pub-id></element-citation></ref><ref id="B14-sensors-25-05306"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>H.W.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>Y.S.</given-names></name></person-group><article-title>Fusion attention for action recognition: Integrating sparse-dense and global attention for video action recognition</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6842</elocation-id><pub-id pub-id-type="doi">10.3390/s24216842</pub-id><pub-id pub-id-type="pmid">39517739</pub-id><pub-id pub-id-type="pmcid">PMC11548316</pub-id></element-citation></ref><ref id="B15-sensors-25-05306"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>P.</given-names></name></person-group><article-title>Application of a Transfer Learning Model Combining CNN and Self-Attention Mechanism in Wireless Signal Recognition</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>4202</elocation-id><pub-id pub-id-type="doi">10.3390/s25134202</pub-id><pub-id pub-id-type="pmid">40648456</pub-id><pub-id pub-id-type="pmcid">PMC12252502</pub-id></element-citation></ref><ref id="B16-sensors-25-05306"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Single-Image Super-Resolution via Cascaded Non-Local Mean Network and Dual-Path Multi-Branch Fusion</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>4044</elocation-id><pub-id pub-id-type="doi">10.3390/s25134044</pub-id><pub-id pub-id-type="pmid">40648300</pub-id><pub-id pub-id-type="pmcid">PMC12252056</pub-id></element-citation></ref><ref id="B17-sensors-25-05306"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Carion</surname><given-names>N.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Synnaeve</surname><given-names>G.</given-names></name><name name-style="western"><surname>Usunier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name></person-group><article-title>End-to-end object detection with transformers</article-title><source>Computer Vision&#8212;ECCV 2020, Proceedings of the 16th European Conference on Computer Vision, Glasgow, UK, 23&#8211;28 August 2020</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B18-sensors-25-05306"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nwoye</surname><given-names>C.I.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Gonzalez</surname><given-names>C.</given-names></name><name name-style="western"><surname>Seeliger</surname><given-names>B.</given-names></name><name name-style="western"><surname>Mascagni</surname><given-names>P.</given-names></name><name name-style="western"><surname>Mutter</surname><given-names>D.</given-names></name><name name-style="western"><surname>Marescaux</surname><given-names>J.</given-names></name><name name-style="western"><surname>Padoy</surname><given-names>N.</given-names></name></person-group><article-title>Rendezvous: Attention mechanisms for the recognition of surgical action triplets in endoscopic videos</article-title><source>Med. Image Anal.</source><year>2022</year><volume>78</volume><fpage>102433</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102433</pub-id><pub-id pub-id-type="pmid">35398658</pub-id></element-citation></ref><ref id="B19-sensors-25-05306"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tufail</surname><given-names>H.</given-names></name><name name-style="western"><surname>Naseer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tamoor</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>A.R.</given-names></name></person-group><article-title>Advancements in Query-Based Tabular Data Retrieval: Detecting Image Data Tables and Extracting Text using Convolutional Neural Networks</article-title><source>Preprint</source><year>2024</year><fpage>2024080108</fpage><pub-id pub-id-type="doi">10.20944/preprints202408.0108.v1</pub-id></element-citation></ref><ref id="B20-sensors-25-05306"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Bo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name></person-group><article-title>Query-Based Object Visual Tracking with Parallel Sequence Generation</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>4802</elocation-id><pub-id pub-id-type="doi">10.3390/s24154802</pub-id><pub-id pub-id-type="pmid">39123848</pub-id><pub-id pub-id-type="pmcid">PMC11314661</pub-id></element-citation></ref><ref id="B21-sensors-25-05306"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name></person-group><article-title>Upscale-a-video: Temporal-consistent diffusion model for real-world video super-resolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>17&#8211;21 June 2024</conf-date><fpage>2535</fpage><lpage>2545</lpage></element-citation></ref><ref id="B22-sensors-25-05306"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hori</surname><given-names>S.</given-names></name><name name-style="western"><surname>Omi</surname><given-names>K.</given-names></name><name name-style="western"><surname>Tamaki</surname><given-names>T.</given-names></name></person-group><article-title>Query matching for spatio-temporal action detection with query-based object detector</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2409.18408</pub-id></element-citation></ref><ref id="B23-sensors-25-05306"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nwoye</surname><given-names>C.I.</given-names></name><name name-style="western"><surname>Padoy</surname><given-names>N.</given-names></name></person-group><article-title>Data splits and metrics for benchmarking methods on surgical action triplet datasets</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2204.05235</pub-id></element-citation></ref><ref id="B24-sensors-25-05306"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Attention-Enhanced CNN-LSTM Model for Exercise Oxygen Consumption Prediction with Multi-Source Temporal Features</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>4062</elocation-id><pub-id pub-id-type="doi">10.3390/s25134062</pub-id><pub-id pub-id-type="pmid">40648318</pub-id><pub-id pub-id-type="pmcid">PMC12252311</pub-id></element-citation></ref><ref id="B25-sensors-25-05306"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Paul</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>A dual-stream CNN-BiLSTM for human motion recognition with raw radar data</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>25094</fpage><lpage>25105</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3415078</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05306-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overview of the proposed TriQuery framework. TriQuery features two key modules: a Multi-Query Decoding Head (MQ-DH) for structured multi-task classification (triplet, instrument, verb, target) and a Top-K Guided Query Update (TKQ) module for lightweight temporal consistency. The Swin-T backbone extracts hierarchical features, which are attended by task-specific queries. <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote three consecutive video frames, each processed individually through the Swin-T backbone to extract hierarchical features. TKQ injects temporal priors by reusing top-<italic toggle="yes">K</italic> queries from the previous frame to refine current predictions. FFN and MLP denote feedforward networks and multi-layer perceptrons, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05306-g001.jpg"/></fig><fig position="float" id="sensors-25-05306-f002" orientation="portrait"><label>Figure 2</label><caption><p>Illustration of the TKQ module. Given the current frame <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the previous frame <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the TKQ module injects temporal priors into task-specific queries. Logits <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> from the previous frame are used to identify top-K confident predictions, from which corresponding query embeddings <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are retrieved. These historical queries are blended with the current learnable query embeddings <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for the instrument, verb, and target branches, respectively. The updated queries <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are then used for cross-attention in downstream classification heads.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05306-g002.jpg"/></fig><fig position="float" id="sensors-25-05306-f003" orientation="portrait"><label>Figure 3</label><caption><p>Illustration of imbalanced surgical behavior categories. (<bold>a</bold>) The pie chart presents the proportion of instances for each surgical procedure triplet class. (<bold>b</bold>) The bar chart displays the same classes sorted by frequency, using the same color scheme as the pie chart for visual coherence. The two most frequent triplet classes account for nearly half of all instances, while 89 classes with proportions below 2% collectively contribute only 14.6% of the total, highlighting the severity of the class imbalance.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05306-g003.jpg"/></fig><fig position="float" id="sensors-25-05306-f004" orientation="portrait"><label>Figure 4</label><caption><p>Visualization of attention maps guided by query-based decoding. (<bold>a</bold>&#8211;<bold>d</bold>) present four representative surgical frames. The first row shows original images. The second row shows attention maps from the triplet classification branch without incorporating auxiliary queries. The third row presents refined attention maps after integrating the multi-query decoding modules, including task-specific queries for instrument, verb, and target recognition. The enhanced focus on semantically relevant regions demonstrates the effectiveness of our multi-query design.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05306-g004.jpg"/></fig><fig position="float" id="sensors-25-05306-f005" orientation="portrait"><label>Figure 5</label><caption><p>Visualization of attention maps from different task-specific query groups (instrument, verb, target) across three representative frames (rows (<bold>a</bold>&#8211;<bold>c</bold>)). The triplet attention map is produced by the decoding head responsible for triplet classification. The visualization demonstrates how each query group focuses on distinct but complementary semantic regions, reflecting their respective recognition objectives and highlighting the task-awareness of our multi-query decoding framework.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05306-g005.jpg"/></fig><fig position="float" id="sensors-25-05306-f006" orientation="portrait"><label>Figure 6</label><caption><p>Temporal attention visualization of the instrument query <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> across consecutive frames. Each column (<bold>a</bold>&#8211;<bold>d</bold>) represents one frame from a continuous surgical sequence. The top row shows the original frames, and the bottom row overlays the attention maps of <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> generated by the TKQ module. The color gradient highlights the regions where the query focuses in each frame, illustrating how the model maintains temporal coherence and consistently attends to the instrument across time, thereby enhancing the stability and reliability of recognition.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05306-g006.jpg"/></fig><fig position="float" id="sensors-25-05306-f007" orientation="portrait"><label>Figure 7</label><caption><p>Visualization of classification results on the CholecT45 test set, comparing the ground truth, Rendezvous baseline, and our TriQuery model. (<bold>a</bold>) Instrument classification (6 classes), (<bold>b</bold>) verb classification (10 classes), and (<bold>c</bold>) target classification (15 classes). All nine test videos are concatenated along the horizontal axis, while the vertical axis displays up to three concurrent label slots to accommodate multiple co-occurring labels, such as simultaneous instrument usage. This stacked color representation allows intuitive comparison of prediction accuracy and temporal consistency across models.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05306-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05306-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05306-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison with state-of-the-art methods for surgical triplet classification on the CholecT45 dataset using five-fold cross-validation. For clarity, results from the first three folds are reported, along with the overall mean performance. Acc, F1, and AP are reported to evaluate triplet classification performance. The best results are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-right:solid thin" rowspan="1" colspan="1">Method</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Fold 1</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Fold 2</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Fold 3</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Mean</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
AP
</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">CNN+RNN [<xref rid="B4-sensors-25-05306" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">71.86</td><td align="center" valign="middle" rowspan="1" colspan="1">53.46</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">41.44</td><td align="center" valign="middle" rowspan="1" colspan="1">70.52</td><td align="center" valign="middle" rowspan="1" colspan="1">56.57</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">42.91</td><td align="center" valign="middle" rowspan="1" colspan="1">69.73</td><td align="center" valign="middle" rowspan="1" colspan="1">58.59</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">44.78</td><td align="center" valign="middle" rowspan="1" colspan="1">69.59</td><td align="center" valign="middle" rowspan="1" colspan="1">54.52</td><td align="center" valign="middle" rowspan="1" colspan="1">41.37</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">CNN-LSTM [<xref rid="B24-sensors-25-05306" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">72.52</td><td align="center" valign="middle" rowspan="1" colspan="1">54.87</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">42.29</td><td align="center" valign="middle" rowspan="1" colspan="1">69.02</td><td align="center" valign="middle" rowspan="1" colspan="1">57.91</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">43.86</td><td align="center" valign="middle" rowspan="1" colspan="1">68.96</td><td align="center" valign="middle" rowspan="1" colspan="1">58.01</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">43.83</td><td align="center" valign="middle" rowspan="1" colspan="1">70.54</td><td align="center" valign="middle" rowspan="1" colspan="1">54.18</td><td align="center" valign="middle" rowspan="1" colspan="1">41.17</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Dual-stream CNN [<xref rid="B25-sensors-25-05306" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">73.06</td><td align="center" valign="middle" rowspan="1" colspan="1">54.65</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">43.28</td><td align="center" valign="middle" rowspan="1" colspan="1">70.39</td><td align="center" valign="middle" rowspan="1" colspan="1">56.41</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">43.06</td><td align="center" valign="middle" rowspan="1" colspan="1">70.14</td><td align="center" valign="middle" rowspan="1" colspan="1">56.69</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">43.16</td><td align="center" valign="middle" rowspan="1" colspan="1">70.78</td><td align="center" valign="middle" rowspan="1" colspan="1">53.56</td><td align="center" valign="middle" rowspan="1" colspan="1">41.35</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">3D CNN [<xref rid="B5-sensors-25-05306" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">74.05</td><td align="center" valign="middle" rowspan="1" colspan="1">50.65</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">39.22</td><td align="center" valign="middle" rowspan="1" colspan="1">69.73</td><td align="center" valign="middle" rowspan="1" colspan="1">50.62</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">38.32</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>72.96</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">53.52</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">41.64</td><td align="center" valign="middle" rowspan="1" colspan="1">70.97</td><td align="center" valign="middle" rowspan="1" colspan="1">49.49</td><td align="center" valign="middle" rowspan="1" colspan="1">38.06</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Rendezvous [<xref rid="B18-sensors-25-05306" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">72.18</td><td align="center" valign="middle" rowspan="1" colspan="1">51.23</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">34.93</td><td align="center" valign="middle" rowspan="1" colspan="1">69.88</td><td align="center" valign="middle" rowspan="1" colspan="1">52.14</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">39.13</td><td align="center" valign="middle" rowspan="1" colspan="1">49.41</td><td align="center" valign="middle" rowspan="1" colspan="1">50.11</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">36.87</td><td align="center" valign="middle" rowspan="1" colspan="1">69.11</td><td align="center" valign="middle" rowspan="1" colspan="1">50.97</td><td align="center" valign="middle" rowspan="1" colspan="1">37.98</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Swin-T (Baseline) [<xref rid="B6-sensors-25-05306" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">72.24</td><td align="center" valign="middle" rowspan="1" colspan="1">54.47</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">43.05</td><td align="center" valign="middle" rowspan="1" colspan="1">67.58</td><td align="center" valign="middle" rowspan="1" colspan="1">55.57</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">41.21</td><td align="center" valign="middle" rowspan="1" colspan="1">68.06</td><td align="center" valign="middle" rowspan="1" colspan="1">56.75</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">42.08</td><td align="center" valign="middle" rowspan="1" colspan="1">68.10</td><td align="center" valign="middle" rowspan="1" colspan="1">53.69</td><td align="center" valign="middle" rowspan="1" colspan="1">40.15</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">TriQuery (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.55</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>59.19</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>48.63</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.79</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>60.77</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>47.33</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>59.85</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>46.70</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.53</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>57.79</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>45.19</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05306-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05306-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison with state-of-the-art methods on per-component classification tasks (instrument, verb, and target) using the CholecT45 dataset. All metrics are averaged across five folds. For each task, Acc, F1, and AP are reported. The triplet scores are also included for reference. The best results are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-right:solid thin" rowspan="1" colspan="1">Method</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Instrument</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Verb</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Target</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Triplet</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
AP
</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">CNN+RNN [<xref rid="B4-sensors-25-05306" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">95.99</td><td align="center" valign="middle" rowspan="1" colspan="1">81.00</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">85.80</td><td align="center" valign="middle" rowspan="1" colspan="1">94.81</td><td align="center" valign="middle" rowspan="1" colspan="1">71.47</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">69.55</td><td align="center" valign="middle" rowspan="1" colspan="1">84.67</td><td align="center" valign="middle" rowspan="1" colspan="1">63.00</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">54.89</td><td align="center" valign="middle" rowspan="1" colspan="1">69.59</td><td align="center" valign="middle" rowspan="1" colspan="1">54.52</td><td align="center" valign="middle" rowspan="1" colspan="1">41.37</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">CNN-LSTM [<xref rid="B24-sensors-25-05306" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">96.04</td><td align="center" valign="middle" rowspan="1" colspan="1">81.04</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">85.95</td><td align="center" valign="middle" rowspan="1" colspan="1">94.79</td><td align="center" valign="middle" rowspan="1" colspan="1">71.48</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">69.38</td><td align="center" valign="middle" rowspan="1" colspan="1">84.09</td><td align="center" valign="middle" rowspan="1" colspan="1">62.81</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">54.60</td><td align="center" valign="middle" rowspan="1" colspan="1">70.54</td><td align="center" valign="middle" rowspan="1" colspan="1">54.18</td><td align="center" valign="middle" rowspan="1" colspan="1">41.17</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Dual-stream CNN [<xref rid="B25-sensors-25-05306" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">95.99</td><td align="center" valign="middle" rowspan="1" colspan="1">80.98</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">85.76</td><td align="center" valign="middle" rowspan="1" colspan="1">94.82</td><td align="center" valign="middle" rowspan="1" colspan="1">71.39</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">69.73</td><td align="center" valign="middle" rowspan="1" colspan="1">84.78</td><td align="center" valign="middle" rowspan="1" colspan="1">62.73</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">54.67</td><td align="center" valign="middle" rowspan="1" colspan="1">70.78</td><td align="center" valign="middle" rowspan="1" colspan="1">53.56</td><td align="center" valign="middle" rowspan="1" colspan="1">41.35</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">3D CNN [<xref rid="B5-sensors-25-05306" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">95.96</td><td align="center" valign="middle" rowspan="1" colspan="1">80.97</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">85.64</td><td align="center" valign="middle" rowspan="1" colspan="1">94.69</td><td align="center" valign="middle" rowspan="1" colspan="1">70.29</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">68.77</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.93</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">60.75</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">53.09</td><td align="center" valign="middle" rowspan="1" colspan="1">70.97</td><td align="center" valign="middle" rowspan="1" colspan="1">49.49</td><td align="center" valign="middle" rowspan="1" colspan="1">38.06</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Rendezvous [<xref rid="B18-sensors-25-05306" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">95.48</td><td align="center" valign="middle" rowspan="1" colspan="1">79.23</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">84.08</td><td align="center" valign="middle" rowspan="1" colspan="1">94.51</td><td align="center" valign="middle" rowspan="1" colspan="1">69.28</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">67.67</td><td align="center" valign="middle" rowspan="1" colspan="1">82.65</td><td align="center" valign="middle" rowspan="1" colspan="1">53.22</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">46.33</td><td align="center" valign="middle" rowspan="1" colspan="1">69.11</td><td align="center" valign="middle" rowspan="1" colspan="1">50.97</td><td align="center" valign="middle" rowspan="1" colspan="1">37.98</td></tr><tr><td align="left" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">Swin-T (Baseline) [<xref rid="B6-sensors-25-05306" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">95.91</td><td align="center" valign="middle" rowspan="1" colspan="1">81.14</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">85.41</td><td align="center" valign="middle" rowspan="1" colspan="1">94.70</td><td align="center" valign="middle" rowspan="1" colspan="1">71.53</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">69.05</td><td align="center" valign="middle" rowspan="1" colspan="1">83.59</td><td align="center" valign="middle" rowspan="1" colspan="1">62.39</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">53.54</td><td align="center" valign="middle" rowspan="1" colspan="1">68.10</td><td align="center" valign="middle" rowspan="1" colspan="1">53.69</td><td align="center" valign="middle" rowspan="1" colspan="1">40.15</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">TriQuery (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>96.88</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>83.51</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>88.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>98.29</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>74.30</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>71.82</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>66.47</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>57.69</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.53</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>57.79</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>45.19</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05306-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05306-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation study of the Multi-Query Decoding Head (MQ-DH) and the Top-K Guided Query Update (TKQ) module on the CholecT45 dataset. &#10003; indicates that the corresponding component is enabled. The comparison demonstrates the individual and combined effectiveness of MQ-DH and TKQ. The best results are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">MQ-DH</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" colspan="1">TKQ</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Instrument</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Verb</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Target</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Triplet</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
AP
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">95.91</td><td align="center" valign="middle" rowspan="1" colspan="1">81.14</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">85.41</td><td align="center" valign="middle" rowspan="1" colspan="1">94.70</td><td align="center" valign="middle" rowspan="1" colspan="1">71.53</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">69.05</td><td align="center" valign="middle" rowspan="1" colspan="1">83.59</td><td align="center" valign="middle" rowspan="1" colspan="1">62.39</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">53.54</td><td align="center" valign="middle" rowspan="1" colspan="1">68.10</td><td align="center" valign="middle" rowspan="1" colspan="1">53.69</td><td align="center" valign="middle" rowspan="1" colspan="1">40.15</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1"/><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>96.90</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">83.50</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>88.82</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">95.28</td><td align="center" valign="middle" rowspan="1" colspan="1">74.29</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>72.29</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">83.91</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>66.48</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">57.60</td><td align="center" valign="middle" rowspan="1" colspan="1">71.32</td><td align="center" valign="middle" rowspan="1" colspan="1">57.78</td><td align="center" valign="middle" rowspan="1" colspan="1">45.18</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>83.51</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">88.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>98.29</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>74.30</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">71.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>84.00</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.47</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<bold>57.69</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.53</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>57.79</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>45.19</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05306-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05306-t004_Table 4</object-id><label>Table 4</label><caption><p>Ablation study on the query composition of the MQ-DH on the CholecT45 dataset. We progressively introduce instrument (<inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), verb (<inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and target (<inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) task-specific queries. The last row additionally includes a fused triplet query (<inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>Tri</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) for comparison, which does not yield further improvement and is not included in the final architecture. The final column (Triplet (Best)) reports the best fold performance across five-fold cross-validation. The best results are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">I</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">V</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-right:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">Tri</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Instrument</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Verb</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Target</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;border-right:solid thin" rowspan="1">Triplet</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Triplet (Best)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">
AP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
F1
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
AP
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">95.91</td><td align="center" valign="middle" rowspan="1" colspan="1">81.14</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">85.41</td><td align="center" valign="middle" rowspan="1" colspan="1">94.70</td><td align="center" valign="middle" rowspan="1" colspan="1">71.53</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">69.05</td><td align="center" valign="middle" rowspan="1" colspan="1">83.59</td><td align="center" valign="middle" rowspan="1" colspan="1">62.39</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">53.54</td><td align="center" valign="middle" rowspan="1" colspan="1">68.10</td><td align="center" valign="middle" rowspan="1" colspan="1">53.69</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">40.15</td><td align="center" valign="middle" rowspan="1" colspan="1">72.24</td><td align="center" valign="middle" rowspan="1" colspan="1">54.47</td><td align="center" valign="middle" rowspan="1" colspan="1">43.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>96.89</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">83.41</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>88.83</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">70.88</td><td align="center" valign="middle" rowspan="1" colspan="1">57.73</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">45.07</td><td align="center" valign="middle" rowspan="1" colspan="1">73.52</td><td align="center" valign="middle" rowspan="1" colspan="1">58.84</td><td align="center" valign="middle" rowspan="1" colspan="1">47.21</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">95.23</td><td align="center" valign="middle" rowspan="1" colspan="1">74.08</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">72.03</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">71.04</td><td align="center" valign="middle" rowspan="1" colspan="1">57.15</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">45.08</td><td align="center" valign="middle" rowspan="1" colspan="1">74.21</td><td align="center" valign="middle" rowspan="1" colspan="1">58.13</td><td align="center" valign="middle" rowspan="1" colspan="1">48.33</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">83.81</td><td align="center" valign="middle" rowspan="1" colspan="1">66.26</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">57.39</td><td align="center" valign="middle" rowspan="1" colspan="1">71.07</td><td align="center" valign="middle" rowspan="1" colspan="1">57.52</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">44.97</td><td align="center" valign="middle" rowspan="1" colspan="1">74.47</td><td align="center" valign="middle" rowspan="1" colspan="1">58.77</td><td align="center" valign="middle" rowspan="1" colspan="1">47.77</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">96.91</td><td align="center" valign="middle" rowspan="1" colspan="1">83.47</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">88.78</td><td align="center" valign="middle" rowspan="1" colspan="1">95.28</td><td align="center" valign="middle" rowspan="1" colspan="1">74.21</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>72.28</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">71.49</td><td align="center" valign="middle" rowspan="1" colspan="1">57.79</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">45.12</td><td align="center" valign="middle" rowspan="1" colspan="1">75.43</td><td align="center" valign="middle" rowspan="1" colspan="1">58.49</td><td align="center" valign="middle" rowspan="1" colspan="1">48.04</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">96.88</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>83.51</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">88.67</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>98.29</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>74.30</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">71.82</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.00</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>66.47</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>57.69</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.53</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>57.79</bold>
</td><td align="center" valign="middle" style="border-right:solid thin" rowspan="1" colspan="1">
<bold>45.19</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>75.55</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>59.19</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>48.63</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.32</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">88.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.92</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">71.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.06</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">56.83</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.77</td><td align="center" valign="middle" style="border-bottom:solid thin;border-right:solid thin" rowspan="1" colspan="1">44.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.40</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>