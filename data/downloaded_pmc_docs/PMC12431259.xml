<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431259</article-id><article-id pub-id-type="pmcid-ver">PMC12431259.1</article-id><article-id pub-id-type="pmcaid">12431259</article-id><article-id pub-id-type="pmcaiid">12431259</article-id><article-id pub-id-type="doi">10.3390/s25175282</article-id><article-id pub-id-type="publisher-id">sensors-25-05282</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Comparative Benchmark of Sampling-Based and DRL Motion Planning Methods for Industrial Robotic Arms</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0951-2307</contrib-id><name name-style="western"><surname>Astorquia</surname><given-names initials="IF">Ignacio Fidalgo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05282" ref-type="aff">1</xref><xref rid="c1-sensors-25-05282" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-0783-7984</contrib-id><name name-style="western"><surname>Villate-Castillo</surname><given-names initials="G">Guillermo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-05282" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9236-1951</contrib-id><name name-style="western"><surname>Tellaeche</surname><given-names initials="A">Alberto</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05282" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6385-5717</contrib-id><name name-style="western"><surname>Vazquez</surname><given-names initials="JI">Juan-Ignacio</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05282" ref-type="aff">1</xref></contrib></contrib-group><aff id="af1-sensors-25-05282"><label>1</label>Department of Computing, Electronics and Communication Technologies, University of Deusto, Avenida de las Universidades 24, 48007 Bilbao, Spain; <email>alberto.tellaeche@deusto.es</email> (A.T.); <email>ivazquez@deusto.es</email> (J.-I.V.)</aff><aff id="af2-sensors-25-05282"><label>2</label>TECNALIA, Basque Research and Technology Alliance (BRTA), 48160 Derio, Spain</aff><author-notes><corresp id="c1-sensors-25-05282"><label>*</label>Correspondence: <email>ignacio.fidalgo@deusto.es</email></corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5282</elocation-id><history><date date-type="received"><day>11</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>22</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>25</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05282.pdf"/><abstract><p>This study presents a comprehensive comparison between classical sampling-based motion planners from the Open Motion Planning Library (OMPL) and a learning-based planner based on Soft Actor&#8211;Critic (SAC) for motion planning in industrial robotic arms. Using a UR3e robot equipped with an RG2 gripper, we constructed a large-scale dataset of over 100,000 collision-free trajectories generated with MoveIt-integrated OMPL planners. These trajectories were used to train a DRL agent via curriculum learning and expert demonstrations. Both approaches were evaluated on key metrics such as planning time, success rate, and trajectory smoothness. Results show that the DRL-based planner achieves higher success rates and significantly lower planning times, producing more compact and deterministic trajectories. Time-optimal parameterization using TOPPRA ensured the dynamic feasibility of all trajectories. While classical planners retain advantages in zero-shot adaptability and environmental generality, our findings highlight the potential of DRL for real-time and high-throughput motion planning in industrial contexts. This work provides practical insights into the trade-offs between traditional and learning-based planning paradigms, paving the way for hybrid architectures that combine their strengths.</p></abstract><kwd-group><kwd>deep reinforcement learning (DRL)</kwd><kwd>motion planning</kwd><kwd>industrial robotics</kwd><kwd>sampling-based planners</kwd><kwd>Open Motion Planning Library (OMPL)</kwd><kwd>hybrid motion planning</kwd><kwd>curriculum learning</kwd></kwd-group><funding-group><award-group><funding-source>European Union</funding-source><award-id>101017284</award-id></award-group><funding-statement>This research was carried out within the project ACROBA, which has received funding from the European Union&#8217;s Horizon 2020 research and innovation programme, under grant agreement No. 101017284.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05282"><title>1. Introduction</title><p>Motion planning plays a foundational role in industrial robotics, enabling the autonomous execution of complex tasks such as pick-and-place, assembly, welding, and inspection. Traditionally, trajectory generation in industrial environments has been treated as a deterministic problem: robots are programmed to follow predefined joint trajectories learned through teaching or pendant-based demonstration&#160;[<xref rid="B1-sensors-25-05282" ref-type="bibr">1</xref>]. Once recorded, these trajectories are stored and reused with minimal variation, providing cycle&#8211;time consistency and satisfying safety and certification requirements. This approach is well-suited for structured, repetitive operations in static workcells&#160;[<xref rid="B2-sensors-25-05282" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05282" ref-type="bibr">3</xref>].</p><p>However, modern manufacturing trends increasingly demand greater flexibility, adaptability, and autonomy&#160;[<xref rid="B4-sensors-25-05282" ref-type="bibr">4</xref>]. Applications such as collaborative robotics, bin picking, machine tending, and dynamic part assembly require robots to plan motions in partially unknown or variable environments. In such scenarios, predefined trajectories may no longer suffice, and the ability to compute feasible motions on-the-fly becomes essential. As a result, general-purpose motion planning has become a key capability in industrial robotic systems.</p><p>Current solutions to this challenge rely predominantly on two families of motion planning algorithms. Optimization-based planners such as Covariant Hamiltonian Optimization for Motion Planning (CHOMP)&#160;[<xref rid="B5-sensors-25-05282" ref-type="bibr">5</xref>], Stochastic Trajectory Optimization for Motion Planning (STOMP)&#160;[<xref rid="B6-sensors-25-05282" ref-type="bibr">6</xref>], and TrajOpt&#160;[<xref rid="B7-sensors-25-05282" ref-type="bibr">7</xref>] generate locally smooth trajectories by minimizing cost functions under dynamic and geometric constraints. Sampling-based planners, on the other hand, such as those implemented in the OMPL&#160;[<xref rid="B8-sensors-25-05282" ref-type="bibr">8</xref>], explore the configuration space using randomized sampling and graph construction techniques. While sampling-based methods excel in exploring complex, high-dimensional spaces and are widely supported in frameworks like MoveIt&#160;[<xref rid="B9-sensors-25-05282" ref-type="bibr">9</xref>], which itself is built on the Robot Operating System (ROS)&#160;[<xref rid="B10-sensors-25-05282" ref-type="bibr">10</xref>], optimization-based methods are often used for fine-grained refinement of motion plans. Hybrid approaches that combine these two paradigms&#8212;using sampling for global planning and optimization for local refinement&#8212;are also gaining traction in both research and deployment.</p><p>Despite their wide applicability, these classical planners exhibit important limitations in the context of industrial automation. Their planning time is often non-deterministic, varying significantly depending on the complexity of the environment and the robot&#8217;s configuration. This variability hinders real-time decision-making and complicates integration with deterministic control pipelines. Additionally, the paths produced by sampling-based planners often require extensive post-processing to become dynamically feasible, while optimization-based planners may fail to escape local minima or require careful initialization&#160;[<xref rid="B11-sensors-25-05282" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05282" ref-type="bibr">12</xref>]. These factors can impede the scalability of classical approaches for modern, responsive industrial applications.</p><p>In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising alternative for industrial robot motion planning&#160;[<xref rid="B13-sensors-25-05282" ref-type="bibr">13</xref>]. DRL enables the training of policies that directly map observed states to actions through experience, allowing robots to generate goal-directed motions in real time via inference. Once trained, DRL policies exhibit time-deterministic behavior, can generalize across a wide range of goal configurations, and often encode collision avoidance and joint limit constraints implicitly. These properties make DRL particularly attractive in flexible industrial setups where planning speed and robustness are critical.</p><p>Beyond motion planning, artificial intelligence methods are increasingly integrated into industrial robotics pipelines for tasks such as perception, inspection, and quality control. In manufacturing settings, vision-based defect detection plays a crucial role in ensuring product quality and reducing downtime. Recent advances in deep learning, such as novel convolutional neural network architectures for surface defect detection&#160;[<xref rid="B14-sensors-25-05282" ref-type="bibr">14</xref>], demonstrate the growing potential of AI to enhance multiple stages of the production process. Although our work focuses on motion planning, these complementary advances highlight the broader trend of embedding learning-based components throughout industrial robotic systems.</p><p>Importantly, DRL can be seen as a hybrid approach, combining elements of both sampling-based and optimization-based planners, as it samples actions while using feedback to optimize a reward function. These paradigms attempt to solve the global motion planning problem, generating trajectories from arbitrary start to goal configurations without requiring local initialization. However, unlike sampling-based and optimization-based methods, DRL policies amortize the cost of planning over training and can produce motions with fixed latency&#8212;an appealing property for integration with industrial controllers.</p><p>While the literature on classical and learning-based planning is extensive, direct comparisons between DRL and sampling-based motion planners remain scarce. Most studies either evaluate classical planners in fixed-scenario benchmarks or investigate DRL agents in simulation, often with limited environment variability&#160;[<xref rid="B15-sensors-25-05282" ref-type="bibr">15</xref>]. Furthermore, the comparisons that do exist typically do not operate under equivalent conditions, making it difficult to draw conclusions about performance trade-offs&#160;[<xref rid="B16-sensors-25-05282" ref-type="bibr">16</xref>]. In particular, few works evaluate both approaches using a shared workspace, a common set of goal configurations, and unified performance metrics. The role of expert demonstrations from classical planners in accelerating DRL training also remains underexplored in systematic, large-scale studies&#160;[<xref rid="B17-sensors-25-05282" ref-type="bibr">17</xref>].</p><p>In this work, we hypothesize that a DRL-based planner can outperform traditional sampling-based methods in terms of planning latency, success rate, and trajectory compactness in dynamic and flexible industrial settings&#8212;such as collaborative robotics and agile manufacturing&#8212;once sufficient training data is available, albeit at the cost of upfront computational effort for policy training. Adaptive motion planning is increasingly critical for modern factories, where robots must safely and efficiently react to changing tasks, human co-workers, and unpredictable environments.</p><sec id="sec1dot1-sensors-25-05282"><title>1.1. Main Research and Contributions</title><p>In this study, we present a comprehensive comparative analysis of classical motion planning techniques and Deep Reinforcement Learning (DRL)-based motion planning for industrial robotic arms, specifically applied to a UR3e robot equipped with an RG2 gripper. The primary objective of this work is to evaluate the performance of a DRL-based planner, specifically a Soft Actor&#8211;Critic (SAC)&#160;[<xref rid="B18-sensors-25-05282" ref-type="bibr">18</xref>] approach, against classical sampling-based planners integrated within the MoveIt framework, focusing on large-scale motion planning tasks.</p><p>Our key contributions are as follows:<list list-type="bullet"><list-item><p>We constructed a large-scale dataset of over 100,000 motion planning queries generated using MoveIt-integrated OMPL planners to evaluate and compare both classical and learning-based motion planners.</p></list-item><list-item><p>We introduced a DRL-based planner trained using curriculum learning and expert demonstrations to solve motion planning tasks. The DRL planner was evaluated on key performance metrics, including planning success rate, planning time, trajectory smoothness, and path compactness.</p></list-item><list-item><p>We benchmarked the DRL-based planner against classical sampling-based planners, highlighting the advantages of DRL in terms of lower planning times, higher success rates, and more compact trajectories.</p></list-item></list></p><p>Our results indicate that while classical planners are still advantageous in certain contexts, DRL offers significant potential for real-time, high-throughput motion planning, particularly in industrial contexts requiring low-latency, deterministic performance.</p><p>This benchmark&#8212;conducted on a dense, uniformly sampled dataset covering the full workspace of the UR3e arm&#8212;provides new insights into the relative performance of learning-based and classical methods in terms of planning time, success rate, and trajectory quality, under consistent experimental conditions.</p><p>This work provides valuable insights into the trade-offs between traditional and learning-based motion planning paradigms, offering a foundation for the development of hybrid planning architectures that combine the strengths of both approaches.</p></sec><sec id="sec1dot2-sensors-25-05282"><title>1.2. Structure of the Paper</title><p>The paper is organized as follows: <xref rid="sec2-sensors-25-05282" ref-type="sec">Section 2</xref> provides a review of related work on existing motion planning techniques, including classical sampling-based and optimization-based methods, as well as the recent applications of DRL in motion planning. This section also highlights the gaps in comparative benchmarks and the need for systematic evaluations of both DRL and classical methods. In <xref rid="sec3-sensors-25-05282" ref-type="sec">Section 3</xref>, the Materials and Methods outlines the experimental setup and methodology used to generate the large-scale dataset and train the DRL-based planner. It details the construction of the dataset, including inverse kinematics solutions, trajectory generation, and the motion planning pipeline. <xref rid="sec4-sensors-25-05282" ref-type="sec">Section 4</xref> reports the outcomes of the large-scale benchmarking, including the performance of the DRL-based planner and classical planners in terms of success rates, planning times, trajectory smoothness, and computational efficiency. Detailed comparisons and statistical analyses are provided to illustrate the relative performance of each approach. <xref rid="sec5-sensors-25-05282" ref-type="sec">Section 5</xref> examines the findings, emphasizing the advantages of the DRL-based approach and its limitations compared to classical methods. This section also explores potential future research directions, including the application of transfer learning and the integration of hybrid planning architectures that combine classical and learning-based&#160;methods. In <xref rid="sec6-sensors-25-05282" ref-type="sec">Section 6</xref>, the Conclusions summarize the main findings and contributions of the paper, emphasizing the potential of DRL for real-time, high-throughput motion planning in industrial applications, and suggest avenues for future work, particularly in the areas of adaptability and scalability.</p></sec></sec><sec id="sec2-sensors-25-05282"><title>2. Related Work</title><p>This section reviews the two main classical approaches, sampling-based and optimization-based planners, and discusses their standardization through frameworks such as ROS and MoveIt. We highlight the rise of hybrid planning architectures that integrate these paradigms, as well as their respective limitations in industrial settings. Furthermore, we review recent trends in applying DRL to motion planning, analyzing its advantages and current challenges. Finally, we identify gaps in comparative benchmarking between classical and learning-based methods, motivating the need for standardized, large-scale evaluations such as the one proposed in this work.</p><sec id="sec2dot1-sensors-25-05282"><title>2.1. Sampling-Based Motion Planning and Standardization Through ROS and MoveIt</title><p>Sampling-based motion planners have played a central role in the advancement of motion planning for robotic manipulators, particularly due to their effectiveness in high-dimensional and constrained environments. Algorithms such as Rapidly-exploring Random Trees (RRT)&#160;[<xref rid="B19-sensors-25-05282" ref-type="bibr">19</xref>], Probabilistic Roadmaps (PRM)&#160;[<xref rid="B20-sensors-25-05282" ref-type="bibr">20</xref>], and their many variants have become the standard for generating feasible paths when exact solutions are intractable. These planners construct a roadmap or search tree by incrementally sampling the configuration space and attempting to connect sampled points using local planning primitives. Their appeal lies in their probabilistic completeness, general-purpose applicability, and minimal reliance on task-specific heuristics.</p><p>It is important to distinguish between single-query and multi-query planners within this context. Single-query planners, such as RRT and its variants, are designed to solve individual planning problems efficiently by constructing a search tree from the start configuration to the goal. These planners are particularly effective in dynamic or frequently changing environments where precomputing a roadmap is impractical. On the other hand, multi-query planners, such as PRM, precompute a roadmap of the configuration space that can be reused for multiple planning queries. While this approach incurs a higher upfront computational cost, it is advantageous in static environments where multiple start and goal configurations need to be planned efficiently. The choice between single-query and multi-query planners depends heavily on the specific application requirements, including environment dynamics, computational resources, and the expected number of planning&#160;queries.</p><p>A major factor in the widespread adoption and continued evolution of these planners is their standardization and accessibility through the ROS ecosystem. Within ROS, the OMPL has emerged as the de facto standard for sampling-based motion planning. OMPL offers a comprehensive suite of planning algorithms, ranging from basic RRT and PRM to more advanced variants such as KPIECE&#160;[<xref rid="B21-sensors-25-05282" ref-type="bibr">21</xref>], providing a clean separation between planning algorithms and robot- or environment-specific logic such as collision checking and state validity. This design allows OMPL to serve as a reusable planning core across a wide variety of robotic platforms.</p><p>Later advancements in RRT and PRM variants introduced several enhancements to improve their efficiency and applicability in complex motion planning scenarios&#160;[<xref rid="B22-sensors-25-05282" ref-type="bibr">22</xref>]. Lazy evaluation techniques, such as LazyPRM&#160;[<xref rid="B23-sensors-25-05282" ref-type="bibr">23</xref>] and LazyRRT, defer collision checks until absolutely necessary, significantly reducing computational overhead. Optimality-focused algorithms like RRT* and PRM* ensure asymptotic convergence to the shortest path by incorporating rewiring and cost-based heuristics. Bidirectional approaches, such as Bidirectional RRT (BiRRT) and Bidirectional PRM, simultaneously grow trees or roadmaps from both the start and goal configurations, accelerating convergence in high-dimensional spaces. Sparsity-aware methods, including Sparse-RRT and SPARS, aim to maintain a minimal yet sufficient set of samples to represent the configuration space, reducing memory usage while preserving solution quality. Additionally, heuristic-guided planners, such as Informed-RRT*&#160;[<xref rid="B24-sensors-25-05282" ref-type="bibr">24</xref>] and BIT*&#160;[<xref rid="B25-sensors-25-05282" ref-type="bibr">25</xref>], leverage domain-specific knowledge or cost-to-go estimates to focus exploration on promising regions, further improving planning efficiency and success rates.</p><p>The integration of OMPL within MoveIt, ROS&#8217;s principal motion planning framework for manipulators, has further elevated the role of sampling-based planners in both academic and industrial robotics. Its modularity has made MoveIt a powerful tool for industrial applications, allowing practitioners to tailor the planning pipeline to address use cases ranging from simple pick-and-place to dual-arm coordinated manipulation and dynamic reconfiguration tasks.</p><p>Numerous research efforts have demonstrated how ROS-Industrial&#160;[<xref rid="B26-sensors-25-05282" ref-type="bibr">26</xref>], as an extension of ROS tailored for factory automation, has enabled the development of flexible and hardware-agnostic robot solutions across manufacturing sectors. For instance, the work by Malvido-Fresnillo et al.&#160;[<xref rid="B27-sensors-25-05282" ref-type="bibr">27</xref>] presented critical extensions to MoveIt aimed at addressing real-world manufacturing needs, such as automatic tool changers, precise end-effector trajectory control, and coordinated dual-arm motion. These enhancements were validated on a dual-arm industrial manipulator, showing that even complex behaviors can be handled within the ROS-MoveIt stack when extended appropriately.</p><p>Similarly, Martinez et al.&#160;[<xref rid="B28-sensors-25-05282" ref-type="bibr">28</xref>] documented the deployment of a dual-arm Motoman robot using ROS-Industrial in a dynamic research-driven industrial environment. Their work highlighted the practicality of configuring and controlling complex manipulators in ROS, and emphasized how ROS-Industrial lowers the barrier for developing advanced applications in academic and industrial collaborations.</p><p>Beyond static manufacturing environments, ROS has also been applied in cloud-distributed industrial tasks. Rahimi et al.&#160;[<xref rid="B29-sensors-25-05282" ref-type="bibr">29</xref>] implemented an ROS-based system for surface blending using distributed computing over a wide-area network. Their setup included 3D scanning, segmentation, and path planning distributed across remote servers, demonstrating how ROS can support scalable, data-intensive tasks with real-time requirements in production settings.</p><p>More recently, researchers have leveraged ROS to develop hybrid platforms that combine mobile and dual-arm manipulators. Xu et al.&#160;[<xref rid="B30-sensors-25-05282" ref-type="bibr">30</xref>] presented a mobile dual-arm system simulated and tested in ROS, demonstrating coordinated coupling behaviors for manipulation in logistics scenarios. Similarly, Li et al.&#160;[<xref rid="B31-sensors-25-05282" ref-type="bibr">31</xref>] implemented a full pick-and-place pipeline using ROS-I and Gazebo in a conveyor-based industrial sorting system, showing how ROS&#8217;s perception, planning, and control packages can be integrated into tightly constrained cycle-time-sensitive tasks.</p><p>Collectively, these applications underline the growing maturity of ROS and MoveIt in industrial contexts. They demonstrate that, with appropriate extensions, these tools are not only viable for complex, sensor-integrated applications but can also support real-time performance constraints critical in manufacturing. Their continued evolution and open-source nature ensure that innovations in motion planning algorithms and system architectures remain accessible, reproducible, and adaptable across use cases.</p></sec><sec id="sec2dot2-sensors-25-05282"><title>2.2. Optimization-Based Motion Planning</title><p>While sampling-based planners have proven highly effective for exploring complex configuration spaces, they often require post-processing to improve path smoothness and dynamic feasibility. Optimization-based motion planners address this need by formulating the motion planning problem as a continuous optimization problem, where an initial trajectory is iteratively refined to minimize a cost function subject to kinematic and dynamic constraints. These methods are particularly valuable in applications requiring smooth, precise, and dynamically feasible paths, such as robotic assembly, high-speed pick-and-place, and surface processing.</p><p>One of the most influential methods in this category is CHOMP&#160;[<xref rid="B5-sensors-25-05282" ref-type="bibr">5</xref>], which introduced a trajectory optimization algorithm capable of incorporating smoothness and obstacle cost gradients into the optimization process. CHOMP operates in the space of trajectories and uses functional gradient descent to minimize an objective that balances obstacle avoidance with motion smoothness. Although CHOMP requires an initial trajectory to converge, it is highly efficient and has been demonstrated in cluttered environments and constrained&#160;spaces.</p><p>STOMP&#160;[<xref rid="B6-sensors-25-05282" ref-type="bibr">6</xref>] builds upon these ideas but introduces a sampling-based update mechanism that does not require gradient information, making it more robust to non-differentiable cost functions. STOMP evaluates multiple noisy perturbations of a trajectory and updates it based on weighted averaging of improved samples. Its robustness and flexibility have made it appealing in scenarios with complex or hard-to-model constraints.</p><p>TrajOpt further advances the field by applying sequential convex optimization techniques to trajectory planning problems&#160;[<xref rid="B7-sensors-25-05282" ref-type="bibr">7</xref>]. TrajOpt linearizes collision constraints and trajectory costs, solving a series of convex subproblems that converge rapidly to locally optimal solutions. This approach offers a good balance between speed and path quality, and has been widely adopted for online trajectory refinement in both simulation and real-time&#160;applications.</p><p>These optimization-based methods are widely supported by MoveIt, where they are often used as trajectory post-processing modules following an initial plan computed by sampling-based algorithms. They are also well-suited to direct use in constrained industrial tasks where workspace structure or motion policy requirements are well understood, for example, applications involving robotic welding, insertion, or toolpath following, which often benefit from optimization-based planning due to the need for continuity in end-effector motion and precise velocity control.</p><p>Despite their advantages, optimization-based planners are inherently local: their performance depends heavily on the quality of the initial trajectory, and they may fail to converge in environments with complex constraints or narrow passages. To mitigate this, they are frequently embedded within hybrid planning pipelines, where a sampling-based planner provides a feasible initialization that is then refined using an optimization-based solver. This division of labor has become a common pattern in industrial and research applications alike, enabling the construction of planners that are both reliable and&#160;high-performing.</p></sec><sec id="sec2dot3-sensors-25-05282"><title>2.3. Hybrid Planning Approaches</title><p>In practice, no single motion planning paradigm fully satisfies the diverse requirements of modern robotic systems, particularly in industrial settings that demand both reliability and high-performance motion generation. As a result, hybrid planning architectures have emerged that combine the strengths of sampling-based and optimization-based planners within unified pipelines. These hybrid methods aim to mitigate the limitations of each class of algorithm by exploiting their complementary characteristics.</p><p>A common approach in hybrid planning involves using a sampling-based planner such as RRT-Connect&#160;[<xref rid="B32-sensors-25-05282" ref-type="bibr">32</xref>] or PRM to generate an initial collision-free trajectory, followed by refinement through an optimization-based planner like CHOMP, STOMP, or TrajOpt&#160;[<xref rid="B33-sensors-25-05282" ref-type="bibr">33</xref>]. This two-stage pipeline allows for global exploration of the configuration space while ensuring the final trajectory is smooth, dynamically feasible, and compliant with end-effector constraints. Several studies have demonstrated the effectiveness of this approach for applications involving obstacle avoidance in tight spaces, precision path tracking, and multi-step industrial tasks.</p><p>The integration of OMPL as a pre-processor for optimization-based planners like CHOMP and STOMP is a common practice in standardized motion planning pipelines such as MoveIt. This hybrid approach leverages the strengths of both sampling-based and optimization-based methods to achieve efficient and high-quality motion planning&#160;[<xref rid="B34-sensors-25-05282" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05282" ref-type="bibr">35</xref>]. OMPL planners are used to generate an initial collision-free trajectory in the configuration space. This trajectory serves as a seed for CHOMP/STOMP, which then refines it to meet additional constraints such as smoothness, obstacle clearance, and dynamic feasibility. The initial trajectory provided by OMPL ensures that CHOMP/STOMP starts from a feasible solution, significantly reducing the risk of convergence to local minima and improving overall planning efficiency. MoveIt provides a modular framework that seamlessly integrates OMPL and optimization-based planners. This hybrid pipeline combines the global exploration capabilities of OMPL with the local refinement strengths of CHOMP/STOMP, offering improved convergence, higher-quality trajectories, and modularity for diverse&#160;applications.</p><p>Recent extensions of this idea include dynamic planning architectures, where replanning is performed in real time by alternating between fast, sparse sampling and rapid trajectory refinement steps&#160;[<xref rid="B36-sensors-25-05282" ref-type="bibr">36</xref>]. Additionally, frameworks such as the MoveIt Task Constructor enable modular construction of hybrid pipelines by chaining sampling, optimization, and constraint satisfaction stages in a task-specific manner, facilitating deployment in manufacturing cells with varying workspace layouts and task demands.</p><p>Hybrid planners have also been deployed in manipulation tasks involving dual-arm or humanoid robots, coordinated motion planning, and tool-use operations, where both global coordination and local precision are critical&#160;[<xref rid="B37-sensors-25-05282" ref-type="bibr">37</xref>]. In such settings, the initial planner may focus on gross motion feasibility&#8212;ensuring collision-free paths between goal poses&#8212;while the optimizer enforces task-space requirements such as synchronized movement or compliance with tool constraints.</p><p>Beyond traditional planning, hybridization also serves as a foundation for integrating learning-based components into classical pipelines&#160;[<xref rid="B38-sensors-25-05282" ref-type="bibr">38</xref>]. For example, learned models can predict initial trajectories or bias the sampling distribution, while classical planners ensure feasibility and safety. This synergy between data-driven priors and algorithmic guarantees reflects a growing trend toward &#8220;learning-augmented&#8221; planning, which is especially attractive in industrial contexts where both reliability and adaptability are&#160;paramount.</p><p>Recent research has begun to merge sampling-based methods with learning components to overcome their respective weaknesses. For example, learned heuristics have been used to bias the sampling distribution in informed RRT* and BIT*&#160;[<xref rid="B24-sensors-25-05282" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05282" ref-type="bibr">25</xref>], reducing search time in complex spaces. Other works integrate neural network predictors to generate warm-start trajectories or estimate cost-to-go, which sampling planners then refine for feasibility and optimality&#160;[<xref rid="B39-sensors-25-05282" ref-type="bibr">39</xref>]. These hybrid frameworks demonstrate that combining data-driven priors with algorithmic guarantees can yield both fast convergence and high-quality paths, pointing toward a promising direction for industrial motion planning.</p><p>As the planning demands of industrial robotics continue to grow&#8212;especially with the rise of human&#8211;robot collaboration, reconfigurable cells, and mobile manipulation&#8212;hybrid architectures are poised to become a standard design pattern. They offer a flexible way to balance global planning robustness with local refinement precision, while also providing a natural interface for integrating optimization, sampling, and learning-based methods into cohesive motion generation systems.</p></sec><sec id="sec2dot4-sensors-25-05282"><title>2.4. Limitations of Classical Planning Approaches</title><p>While both sampling-based and optimization-based motion planning methods have demonstrated significant success across academic and industrial applications, each presents limitations that constrain their applicability in dynamic, time-sensitive, or high-precision industrial scenarios. These limitations become particularly critical in contexts that demand planning determinism, low-latency responses, and consistent integration into real-time control pipelines.</p><p>Sampling-based planners such as those implemented in OMPL offer strong generality and probabilistic completeness, but at the cost of stochasticity and computational unpredictability. Even under fixed conditions, planners like RRT-Connect or KPIECE can yield variable performance due to their reliance on random sampling and exploration heuristics&#160;[<xref rid="B22-sensors-25-05282" ref-type="bibr">22</xref>,<xref rid="B40-sensors-25-05282" ref-type="bibr">40</xref>]. This non-determinism leads to varying planning times and success rates, making it difficult to guarantee consistent behavior within tight industrial cycle-time constraints. Furthermore, the paths they generate are often jagged or suboptimal, necessitating post-processing such as shortcutting, spline fitting, or velocity profiling via time-parameterization algorithms like Time-Optimal Trajectory Generation (TOTG)&#160;[<xref rid="B41-sensors-25-05282" ref-type="bibr">41</xref>].</p><p>Optimization-based planners, while capable of producing smoother and more dynamically feasible trajectories, suffer from their own limitations. Methods such as CHOMP, STOMP, and TrajOpt rely heavily on the quality of the initial trajectory and may converge to local minima or fail to find solutions in highly constrained or cluttered workspaces&#160;[<xref rid="B5-sensors-25-05282" ref-type="bibr">5</xref>,<xref rid="B7-sensors-25-05282" ref-type="bibr">7</xref>]. Their computational performance is typically sensitive to the number of waypoints, the presence of sharp cost gradients, and the linearity of collision constraints. Moreover, they often require careful parameter tuning&#8212;such as cost weights, step sizes, and regularization terms&#8212;which complicates deployment in heterogeneous task setups or dynamically reconfigured environments.</p><p>In industrial robotics, these shortcomings have practical implications. Non-deterministic planning latency impedes system-level scheduling and validation. Local convergence failures can block entire workflows if alternative solutions are not readily available. The need for extensive tuning or manual debugging undermines reusability and adaptability, especially in flexible automation cells where robots must be repurposed for different products or workspaces on short notice.</p><p>Although hybrid pipelines that combine both approaches can mitigate some of these issues, they introduce additional complexity and often still rely on per-task tuning or human intervention to ensure performance. These persistent challenges motivate the exploration of alternative paradigms, such as learning-based planning, which aim to bypass online search altogether through policy amortization and experience-driven generalization.</p></sec><sec id="sec2dot5-sensors-25-05282"><title>2.5. Deep Reinforcement Learning for Motion Planning</title><p>DRL has emerged as a powerful framework for robot motion planning, particularly in tasks requiring generalization, adaptability, and real-time reactivity. By optimizing policies that map high-dimensional observations to actions through trial-and-error interaction with an environment, DRL methods can learn control strategies that are difficult to specify explicitly or solve analytically&#160;[<xref rid="B42-sensors-25-05282" ref-type="bibr">42</xref>]. This makes them attractive for robotic manipulation tasks in unstructured or partially observable environments, such as warehouse picking, dynamic assembly, and human&#8211;robot collaboration&#160;[<xref rid="B43-sensors-25-05282" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05282" ref-type="bibr">44</xref>].</p><p>DRL planners, once trained, provide inference&#8211;time trajectory generation without the need to explicitly solve a planning problem online. This property contrasts with both sampling-based and optimization-based planners, which typically recompute a solution for each planning query. Algorithms such as SAC&#160;[<xref rid="B18-sensors-25-05282" ref-type="bibr">18</xref>], Proximal Policy Optimization (PPO)&#160;[<xref rid="B45-sensors-25-05282" ref-type="bibr">45</xref>], and Twin Delayed Deep Deterministic Policy Gradient (TD3)&#160;[<xref rid="B46-sensors-25-05282" ref-type="bibr">46</xref>] have been successfully applied to continuous robot control&#160;[<xref rid="B16-sensors-25-05282" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05282" ref-type="bibr">17</xref>,<xref rid="B47-sensors-25-05282" ref-type="bibr">47</xref>], including reaching, grasping, insertion, and trajectory tracking&#160;[<xref rid="B13-sensors-25-05282" ref-type="bibr">13</xref>,<xref rid="B43-sensors-25-05282" ref-type="bibr">43</xref>]. These methods leverage function approximation via deep neural networks and can scale to high-dimensional action spaces, including joint-space control of 6- or 7-DOF manipulators.</p><p>Several research efforts have demonstrated the feasibility of DRL-based motion planning for industrial robots. For example, policies have been trained to solve Cartesian reaching tasks using sparse rewards&#160;[<xref rid="B44-sensors-25-05282" ref-type="bibr">44</xref>], perform path-constrained tool motions, or follow orientation-sensitive end-effector constraints. These studies often employ curriculum learning or reward shaping to guide training, and some inject expert demonstrations from classical planners to bootstrap learning in sparse reward settings&#160;[<xref rid="B45-sensors-25-05282" ref-type="bibr">45</xref>]. This ability to integrate prior knowledge aligns DRL with hybrid planning frameworks, allowing it to serve as a global planner, an initialization policy, or a local control module&#160;[<xref rid="B48-sensors-25-05282" ref-type="bibr">48</xref>].</p><p>In the context of motion planning, DRL is particularly well-suited to problems where traditional planners struggle&#8212;such as those with partially observed dynamics, unknown cost landscapes, or non-convex constraints. Moreover, DRL policies produce deterministic outputs at inference time, making them appealing in applications where latency and repeatability are critical&#160;[<xref rid="B49-sensors-25-05282" ref-type="bibr">49</xref>]. For instance, recent benchmarks have shown that DRL-based planners can match or outperform classical planners in both planning time and path smoothness once trained, particularly in structured but dynamic environments&#160;[<xref rid="B50-sensors-25-05282" ref-type="bibr">50</xref>].</p><p>Nevertheless, the adoption of DRL in industrial motion planning remains limited by several factors. Chief among these is the challenge of sample efficiency: policies often require millions of environment interactions to converge. This is compounded by the sim-to-real gap, where discrepancies between simulated and physical environments cause trained policies to fail when deployed on real hardware. Solutions such as domain randomization&#160;[<xref rid="B51-sensors-25-05282" ref-type="bibr">51</xref>], privileged training, and fine-tuning with real-world demonstrations have been proposed to mitigate these challenges, but their deployment in time-constrained manufacturing setups remains an open problem&#160;[<xref rid="B52-sensors-25-05282" ref-type="bibr">52</xref>].</p><p>Recent years have also seen the emergence of standardized, robot-centric DRL benchmarks that include reaching and motion generation tasks for multi-DoF arms. PandaGym provides goal-conditioned reaching and manipulation tasks for the Franka arm with strong baselines for SAC/TD3/PPO and reproducible evaluation protocols&#160;[<xref rid="B53-sensors-25-05282" ref-type="bibr">53</xref>]. Robosuite offers a modular MuJoCo-based suite with arm-centric reaching/manipulation tasks and controllers, widely used for algorithmic comparisons&#160;[<xref rid="B54-sensors-25-05282" ref-type="bibr">54</xref>]. RLBench contributes a large set of manipulation tasks (including reaching as a primitive) designed to stress perception and control and is frequently used to test generalization across task variants&#160;[<xref rid="B55-sensors-25-05282" ref-type="bibr">55</xref>]. Complementing these platforms, recent surveys synthesize progress and open challenges in applying DRL to robotic control and planning, emphasizing lessons for real-world deployment and evaluation&#160;[<xref rid="B56-sensors-25-05282" ref-type="bibr">56</xref>]. On the application side for manipulators, recent studies continue to deploy SAC/PPO/TD3 variants for collision-aware arm trajectory planning and human-aware avoidance, highlighting typical pain points (reward shaping, sample efficiency, stability) that motivate hybridization with classical modules and stronger benchmarking&#160;[<xref rid="B57-sensors-25-05282" ref-type="bibr">57</xref>,<xref rid="B58-sensors-25-05282" ref-type="bibr">58</xref>].</p><p>Alternative intelligent control strategies for robotic arms have also been proposed outside of the DRL paradigm. For instance, adaptive critic designs have been applied to achieve safety-optimal Fault-Tolerant Control (FTC) in nonlinear systems with asymmetric constrained inputs&#160;[<xref rid="B59-sensors-25-05282" ref-type="bibr">59</xref>], demonstrating how actor&#8211;critic architectures can be tailored for safety-critical operation. Similarly, event-triggered <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mo>&#8734;</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> control schemes have been developed for unknown constrained nonlinear systems, with direct applications to robotic arms&#160;[<xref rid="B60-sensors-25-05282" ref-type="bibr">60</xref>] offering robustness guarantees under limited actuation and dynamic uncertainties. While these approaches differ from the DRL-based motion planning considered in this work, they share the goal of enabling the safe and efficient operation of robotic manipulators under real-world constraints.</p><p>In contrast to platform-oriented or algorithm-proposal papers, our contribution is a large-scale, ROS/MoveIt-parity comparison between OMPL sampling-based planners and an SAC-based planner under identical pipeline adapters and post-processing. Methodologically, our DRL training uses a coupled position&#8211;orientation curriculum (rather than independent thresholds) and a dual-use dataset (benchmark + expert demonstrations) built from uniform workspace sampling with distance-balanced start&#8211;goal pairs. Prior DRL reaching or motion-planning works do not report head-to-head comparisons with OMPL under ROS-equivalent interfaces at this scale nor enforce coupled pose accuracy as an explicit training constraint&#160;[<xref rid="B53-sensors-25-05282" ref-type="bibr">53</xref>,<xref rid="B54-sensors-25-05282" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-05282" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-05282" ref-type="bibr">56</xref>].</p></sec><sec id="sec2dot6-sensors-25-05282"><title>2.6. Comparative Benchmarks and Research Gaps</title><p>Systematic benchmarking plays a crucial role in evaluating and comparing the performance of motion planning algorithms. Over the past decade, several studies have attempted to provide structured evaluations of classical planning strategies, often focusing on sampling-based planners due to their algorithmic diversity and widespread adoption in practical robotics. Frameworks such as the MoveIt benchmarking suite and MotionBenchMaker&#160;[<xref rid="B61-sensors-25-05282" ref-type="bibr">61</xref>] have enabled quantitative comparisons across planners like RRT, PRM, KPIECE, and BIT*, using metrics such as planning success rate, computation time, and trajectory smoothness. These tools have helped identify strengths and limitations across a range of scenarios, including cluttered environments, narrow passages, and high-DOF manipulators. Other resources such as Planner Arena&#160;[<xref rid="B62-sensors-25-05282" ref-type="bibr">62</xref>] provide visual analytics and standardized reporting for OMPL benchmarking results, allowing performance to be explored interactively. While these infrastructures have been instrumental for evaluating classical planners, they are primarily designed for scene-centric evaluations and do not directly accommodate the requirements of benchmarking learning-based planners, such as large-scale uniform workspace sampling, redundancy exposure, or replay-buffer data generation.</p><p>Kroemer et al.&#160;[<xref rid="B63-sensors-25-05282" ref-type="bibr">63</xref>] provide an extensive overview of the role of machine learning in robot manipulation. They explore the challenges of building robots that can effectively interact with their environment and manipulate objects to achieve specific goals. The paper emphasizes that traditional approaches often fail to capture the complexity and variability of real-world tasks, making learning-based techniques essential. The authors discuss various machine learning approaches used in robot manipulation, including deep learning and reinforcement learning, and propose a unified framework for understanding these methods. Their work identifies key research opportunities in the field, such as improving the efficiency of learning algorithms and enhancing their ability to generalize across diverse tasks and environments. This research serves as a foundational reference for the integration of machine learning in robotic manipulation, directly influencing the design of advanced motion planning algorithms for industrial applications.</p><p>Elbanhawi et al.&#160;[<xref rid="B12-sensors-25-05282" ref-type="bibr">12</xref>] focus on the computational efficiency and limitations of classical sampling-based motion planning algorithms, such as RRT and PRM. They discuss the probabilistic completeness of these methods, which guarantees a solution if one exists, but also emphasize that the probability of reaching an optimal solution diminishes over time. To address this limitation, they introduce optimal sampling planners, which offer asymptotic optimality as planning time approaches infinity. However, the authors point out the slow convergence of these planners and propose heuristic and post-processing methods to improve their efficiency. Their work underscores the complexity of tuning sampling-based algorithms and the need for improved convergence rates, highlighting a key challenge in classical motion planning that has driven interest in hybrid approaches combining classical and learning-based methods.</p><p>Recent work by Orthey et al.&#160;[<xref rid="B22-sensors-25-05282" ref-type="bibr">22</xref>] provided one of the most comprehensive comparative reviews of sampling-based planning to date, analyzing the empirical performance of over 40 planners on 25 motion planning problems. Their study emphasized that no single planner is universally superior and that performance varies significantly depending on problem structure. These findings reinforce the importance of benchmarking under realistic, application-specific conditions&#8212;particularly in industrial contexts where robustness and determinism are key operational requirements.</p><p>Despite these advances, the literature on DRL-based motion planning remains fragmented, with few studies providing direct comparisons to classical planners. Most existing benchmarks focus on either simulation environments or specific robotic platforms, making it difficult to generalize findings across different setups. Furthermore, many studies evaluate DRL policies in isolation, without considering their performance relative to established classical methods or to a limited set of planners&#160;[<xref rid="B15-sensors-25-05282" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05282" ref-type="bibr">16</xref>,<xref rid="B47-sensors-25-05282" ref-type="bibr">47</xref>]. This lack of comparative analysis limits our understanding of the strengths and weaknesses of DRL in practical applications and hinders the development of hybrid approaches that leverage both classical and learning-based techniques.</p><p>To address both the lack of DRL&#8211;classical parity and the limitations of existing benchmarking tools, our dataset construction departs from scene-centric approaches like MotionBenchMaker and Planner Arena in several key ways. First, we employ uniform spherical sampling of the reachable workspace, followed by distance-binned start&#8211;goal pairing to balance problem difficulty across short-, medium-, and long-range motions. Second, we use IKFast to enumerate all possible inverse-kinematics solutions for each pose, filtering out self-collisions and floor collisions to retain only feasible configurations while exposing the robot&#8217;s kinematic redundancy. Third, we ensure strict ROS/MoveIt parity by running both OMPL planners and the DRL planner through identical request adapters and time-parameterization stages (TOTG/TOPPRA), guaranteeing fairness in execution and timing. Finally, our dataset serves a dual purpose: it supports both large-scale benchmarking of classical planners and provides high-quality expert demonstrations for seeding DRL replay buffers, a feature not supported by existing benchmarking frameworks.</p><p>Our work aims to address this gap by constructing a shared benchmark that compares OMPL-based sampling planners and a SAC policy trained with expert demonstrations across over 100,000 motion queries for an industrial robot arm. By standardizing the dataset, evaluation metrics, and planning interfaces, we provide an empirical foundation for assessing how classical and learning-based motion planning approaches perform under operationally relevant conditions.</p></sec></sec><sec id="sec3-sensors-25-05282"><title>3. Materials and Methods</title><p>This section describes the experimental methodology employed to compare classical sampling-based motion planning techniques with DRL approaches for trajectory generation on a UR3e robotic manipulator. The experimentation was structured in two main phases. First, a large-scale dataset of over 100,000 motion planning trajectories was generated by systematically sampling the robot&#8217;s reachable workspace and solving motion queries using OMPL-based planners within the MoveIt framework. These trajectories served both as a benchmark for classical methods and as expert demonstrations to accelerate DRL training. In the second phase, SAC policy was trained in simulation to reproduce the motion planning task, using the dataset to bootstrap learning and improve sample efficiency. Both the OMPL-generated and DRL-generated trajectories were post-processed using time-optimal parameterization algorithms to ensure dynamic feasibility, enabling a fair comparison across methods in terms of planning success rate, trajectory quality, and execution consistency.</p><sec id="sec3dot1-sensors-25-05282"><title>3.1. Dataset Generation</title><p>A dataset of over 100,000 trajectories was generated to evaluate and compare the performance of traditional motion planning techniques and DRL for motion planning in the UR3e robotic manipulator equipped with an OnRobot RG2 gripper. The dataset was constructed in two phases:<list list-type="order"><list-item><p>Uniform sampling of the workspace and filtering of feasible configurations;</p></list-item><list-item><p>Generation of full trajectories using all the available planners in MoveIt&#8217;s OMPL planning pipeline, ensuring only valid and feasible motions were included.</p></list-item></list></p><sec id="sec3dot1dot1-sensors-25-05282"><title>3.1.1. Workspace Definition and Uniform Pose Sampling</title><p>The workspace <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">W</mml:mi></mml:mrow></mml:math></inline-formula> of the UR3e robot is defined as the set of all possible end-effector Cartesian positions and orientations that can be reached while maintaining a valid joint configuration. Mathematically, we express this as: <disp-formula><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">W</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mrow><mml:mo>&#215;</mml:mo><mml:mi>S</mml:mi><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>|</mml:mo><mml:mspace width="0.277778em"/><mml:mo>&#8707;</mml:mo><mml:mi>q</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Cartesian position of the end-effector.</p></list-item><list-item><p><inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents its orientation.</p></list-item><list-item><p><inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a valid joint configuration.</p></list-item><list-item><p><inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="script">W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the forward kinematics function.</p></list-item></list><p>To ensure that the workspace was sampled in a representative manner, a spherical uniform distribution was used to generate feasible end-effector positions. The poses were sampled within a bounded workspace range to avoid collisions with the base and maximize coverage of reachable areas. Furthermore, for each pose, multiple orientation values were sampled to ensure a diverse representation of feasible end-effector configurations. The sampling followed a hemispherical constraint, ensuring no samples were generated in unreachable regions directly below the base. The limits were defined as follows:<list list-type="bullet"><list-item><p>Radial distance: <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.85</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> meters to prevent self-collisions near the base and ensure reachability within the robot&#8217;s range.</p></list-item><list-item><p>Azimuthal angle <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula>: Uniformly sampled in <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, covering all directions in a circular plane.</p></list-item><list-item><p>Elevation angle <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>: Sampled using <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arccos</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#8764;</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>), ensuring uniform distribution in the hemisphere.</p></list-item><list-item><p>Orientation: Random joint configurations were generated for each sample, within the UR3e&#8217;s joint limits.</p></list-item></list></p><p>The mathematical formulation of the sampling is as follows:<disp-formula><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>r</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>&#952;</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>&#981;</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">arccos</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a uniform distribution between <italic toggle="yes">a</italic> and <italic toggle="yes">b</italic>. The sampled Cartesian coordinates of the poses are then computed as:<disp-formula><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>x</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo form="prefix">cos</mml:mo><mml:mi>&#952;</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>&#981;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>y</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>&#952;</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>&#981;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>z</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo form="prefix">cos</mml:mo><mml:mi>&#981;</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>To ensure comprehensive workspace coverage, an iterative process was conducted to determine the optimal number of sampled poses. Initially, multiple pose number sizes were tested by varying the number of uniformly sampled poses. Each variation was graphically analyzed to verify that all regions of the workspace were adequately represented without excessive clustering or gaps. This process was repeated, progressively refining the number of poses, until a balance was achieved as follows:<list list-type="bullet"><list-item><p>The entire reachable workspace was covered.</p></list-item><list-item><p>No regions were overrepresented.</p></list-item><list-item><p>The sampling maintained uniformity in the distribution of poses.</p></list-item></list></p><p><xref rid="sensors-25-05282-f001" ref-type="fig">Figure 1</xref> illustrates the final validated distribution of poses, showing uniform coverage of the reachable workspace. Notably, an inner spherical region remains unoccupied due to proximity to the base, preventing self-collisions. A total of 134,742 poses were sampled, ensuring a diverse representation of the robot&#8217;s workspace.</p></sec><sec id="sec3dot1dot2-sensors-25-05282"><title>3.1.2. Inverse Kinematics Computation and Configuration Space Filtering</title><p>For each sampled workspace pose, all possible Inverse Kinematic (IK) solutions were computed using IKFast&#160;[<xref rid="B64-sensors-25-05282" ref-type="bibr">64</xref>], an analytical solver optimized for efficiency. Given a workspace pose <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the corresponding set of joint-space solutions was defined as:<disp-formula><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">Q</mml:mi></mml:mrow></mml:math></inline-formula> represents all possible joint configurations satisfying the given pose. However, not all computed configurations were viable for execution, as some may lead to infeasible transitions between configurations or kinematic constraints. To ensure that the final set of sampled poses and configurations allows for valid motion planning, the following filtering steps were applied:<list list-type="order"><list-item><p><bold>Self-collision filtering</bold>: Configurations that resulted in self-collisions between any part of the robot&#8217;s links were discarded.</p></list-item><list-item><p><bold>Floor collision filtering</bold>: In configurations where the end-effector or other parts of the robot intersected, the floor was removed.</p></list-item><list-item><p><bold>Unreachable configurations</bold>: If no valid IK solutions existed for a given pose or the configuration was not reachable due to physical constraints, the pose was discarded (see <xref rid="sensors-25-05282-f002" ref-type="fig">Figure 2</xref>).</p></list-item></list></p><p>The purpose of this filtering was to ensure that for any chosen pose&#8217;s initial configuration, it is kinematically possible to transition to a valid goal pose&#8217;s configuration through a feasible trajectory. By removing configurations that are not physically achievable or that create isolated solutions in the configuration space, this step guarantees that motion planning algorithms can consistently generate collision-free and executable trajectories.</p><p><xref rid="sensors-25-05282-f002" ref-type="fig">Figure 2</xref> shows the initial configuration space distribution for all the sampled poses before filtering, where many sampled pose configurations were found to be infeasible due to self-collisions, floor collisions, or unreachable workspace regions. The first pose sampling phase provided a large set of candidate configurations, but a significant portion of them were non-executable due to physical and kinematic limitations of the robot.</p><p>To extract a refined set of valid poses and associated joint configurations, a filtering algorithm was applied. This algorithm systematically iterated through the initially sampled configurations, discarding those that violated reachability, self-collision, or floor constraints. The details of this process are outlined in Algorithm&#160;1.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Filtering of Valid Poses and Associated Joint Configurations</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>1:</label><p><bold>Input:</bold> Robot kinematic model, workspace bounds <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">W</mml:mi></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>2:</label><p><bold>Output:</bold> Set of valid poses and associated configurations <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>3:</label><p>Uniformly sample over 100,000 poses in the robot&#8217;s workspace</p></list-item><list-item><label>4:</label><p>Initialize set of valid configurations <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#8592;</mml:mo><mml:mo>&#8709;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>5:</label><p><bold>for</bold> each sampled pose <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>6:</label><p>&#160;&#160;&#160;Compute all IK solutions <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">Q</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using IKFast</p></list-item><list-item><label>7:</label><p>&#160;&#160;&#160;<bold>for</bold> each joint configuration <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">Q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>8:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>if</bold>&#160;<inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is collision-free and reachable <bold>then</bold></p></list-item><list-item><label>9:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Add <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <italic toggle="yes">C</italic></p></list-item><list-item><label>10:</label><p>&#160;&#160;&#160;&#160;&#160;<bold>end if</bold></p></list-item><list-item><label>11:</label><p>&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>12:</label><p><bold>end for</bold></p></list-item><list-item><label>13:</label><p><bold>Return</bold>&#160;<italic toggle="yes">C</italic></p></list-item></list></td></tr></tbody></array></p><p>After applying the filtering constraints, 104,295 valid initial and goal poses along with their corresponding joint configurations were selected. The final configuration space distribution, displayed in <xref rid="sensors-25-05282-f003" ref-type="fig">Figure 3</xref>, presents a refined set of poses containing only valid and executable joint configurations. This selection ensured the following:<list list-type="bullet"><list-item><p>Diversity of trajectory complexity, including short, medium, and long-distance&#160;movements.</p></list-item><list-item><p>Even coverage of the workspace, ensuring that all regions of <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">W</mml:mi></mml:mrow></mml:math></inline-formula> were represented.</p></list-item><list-item><p>Balanced representation of joint configurations, preserving uniformity in the valid configuration space.</p></list-item></list></p><fig position="anchor" id="sensors-25-05282-f003" orientation="portrait"><label>Figure 3</label><caption><p>Configuration space sample distribution after filtering unreachable regions and collision constraints.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g003.jpg"/></fig></sec><sec id="sec3dot1dot3-sensors-25-05282"><title>3.1.3. OMPL-Based Motion Planning Pipeline</title><p>This subsection describes the internal motion planning pipeline used by MoveIt when interfacing with the OMPL for sampling-based global motion planning. The pipeline transforms a high-level pose goal into a fully time-parameterized, dynamically feasible trajectory in configuration space through several modular stages.</p><p>The process begins with the user-defined input: a target pose in Cartesian space, specified for the robot&#8217;s end-effector. Since OMPL operates in configuration space, this pose goal must first be converted into one or more joint-space configurations. This is accomplished through the kinematics plugin, in this case, IKFast, which provides fast and analytical IK solutions. A valid goal configuration is selected among those returned by the plugin, taking into account feasibility constraints in the configuration space.</p><p>Following this conversion, the motion planning request is internally constructed and processed by a series of pre-processing planning request adapters. These adapters serve to prepare and sanitize the request before it reaches the planner. Common adapters used at this stage include the following:<list list-type="bullet"><list-item><p><monospace>CheckStartStateBounds</monospace>: Ensures that the start state is within joint limits.</p></list-item><list-item><p><monospace>ValidateWorkspaceBounds</monospace>: Confirms that the start and goal are within the declared workspace.</p></list-item><list-item><p><monospace>CheckStartStateCollision</monospace>: Verifies that the initial configuration is collision-free.</p></list-item><list-item><p><monospace>ResolveConstraintFrames</monospace>: Resolves any relative frames used in the constraint specification.</p></list-item></list></p><p>After pre-processing, the validated motion planning request is forwarded to the OMPL motion planner. At this stage, OMPL attempts to solve the motion planning problem in configuration space using the requested planner (e.g., RRT, RRTConnect, PRM, among others). The planner incrementally builds a roadmap or tree by sampling random configurations and connecting them while enforcing kinematic limits, collision avoidance (using FCL), and workspace constraints. The result of this step is a raw path, i.e., a sequence of joint-space waypoints connecting the start and goal configurations.</p><p>Once a valid path is found, MoveIt begins constructing the motion planning response. This response includes the raw trajectory, as well as status information, about the planning attempt. Before finalizing the response, a second sequence of adapters&#8212;known as post-processing request adapters&#8212;is applied to refine the trajectory further. Typical post-processing steps include the following:<list list-type="bullet"><list-item><p><monospace>AddTimeOptimalParameterization</monospace>: Applies time-optimal path parameterization (TOTG)&#160;[<xref rid="B41-sensors-25-05282" ref-type="bibr">41</xref>] to assign velocity and timing profiles consistent with the robot&#8217;s dynamic constraints.</p></list-item><list-item><p><monospace>ValidateSolution</monospace>: Ensures that the final trajectory remains collision-free and satisfies all dynamic and kinematic constraints.</p></list-item></list></p><p>The final output is a fully parameterized, smooth, and executable trajectory in configuration space. This trajectory can be passed directly to the controller for execution or used offline for training and benchmarking, as in the case of this study.</p></sec><sec id="sec3dot1dot4-sensors-25-05282"><title>3.1.4. Trajectory Dataset Construction</title><p>The dataset construction began with the set of valid poses and their associated joint configurations obtained from the configuration space filtering process. To ensure a uniform distribution of motion trajectories across different workspace path lengths, distance bins were defined, allowing pose pairs to be sampled evenly based on their separation in the workspace. <xref rid="sensors-25-05282-f004" ref-type="fig">Figure 4</xref> illustrates the final distribution of sampled start and goal pose distances in the workspace. The distance bins were designed to ensure that the dataset contained a balanced representation of short, medium, and long-distance movements, providing a diverse set of motion planning scenarios.</p><p>For each sampled start and goal pose pair, a random valid configuration was selected for the initial pose, and planning requests for every available OMPL planner using MoveIt&#8217;s motion planning pipeline were made. To ensure a correct trajectory generation, this pipeline was configured as follows:<list list-type="bullet"><list-item><p>The IKFast kinematics plugin was used to compute the IK for the goal pose. The plugin was configured to use the filtered constraints identified during the configuration space&#160;filtering.</p></list-item><list-item><p>OMPL planners were configured to attempt path planning with a maximum of 10&#160;attempts per trajectory and a 5 s timeout per attempt.</p></list-item><list-item><p>The time-optimized trajectory was obtained using MoveIt&#8217;s TOTG module, with a 0.1&#160;scaling factor for the velocity and acceleration limits.</p></list-item></list></p><p>For every pose pair, the pipeline was executed for all available planners in MoveIt&#8217;s OMPL motion planning pipeline. Algorithm 2 outlines the overall process for constructing the trajectory dataset. The successful trajectories were recorded, along with relevant data consisting of full motion trajectories, not just pose pairs. For each successfully planned trajectory, the following data was recorded:<list list-type="bullet"><list-item><p>Planner name: Identifying which OMPL planner successfully computed the trajectory.</p></list-item><list-item><p>Initial pose: The start pose of the motion plan.</p></list-item><list-item><p>Goal pose: The target pose of the motion plan.</p></list-item><list-item><p>Path: The sequence of joint configurations representing the motion plan.</p></list-item><list-item><p>Time parameterization: The time-optimized trajectory obtained using TOTG&#160;[<xref rid="B41-sensors-25-05282" ref-type="bibr">41</xref>].</p></list-item><list-item><p>Planning time: The total computation time required to generate the trajectory.</p></list-item><list-item><p>Configuration space length: The total length of the path in the configuration space.</p></list-item></list></p><p>The initial and final configurations were stored along with the path data to ensure reproducibility and consistency in the dataset.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2</bold> Trajectory Dataset Construction</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label>1:</label><p><bold>Input:</bold> Set of valid poses and associated configurations <italic toggle="yes">C</italic></p></list-item><list-item><label>2:</label><p><bold>Output:</bold> Final dataset <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>,</mml:mo><mml:mi>planner</mml:mi><mml:mo>,</mml:mo><mml:mi>param</mml:mi><mml:mo>,</mml:mo><mml:mi>time</mml:mi><mml:mo>,</mml:mo><mml:mi>length</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>3:</label><p>Initialize dataset <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#8592;</mml:mo><mml:mo>&#8709;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>4:</label><p>Define distance bins for sampling pose pairs</p></list-item><list-item><label>5:</label><p><bold>for</bold> each distance bin <italic toggle="yes">d</italic>&#160;<bold>do</bold></p></list-item><list-item><label>6:</label><p>&#160;&#160;&#160;Uniformly sample pose pairs <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from <italic toggle="yes">C</italic> within distance <italic toggle="yes">d</italic></p></list-item><list-item><label>7:</label><p>&#160;&#160;&#160;<bold>for</bold> each <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>8:</label><p>&#160;&#160;&#160;&#160;&#160;Randomly select a valid configuration <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>start</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">Q</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>9:</label><p>&#160;&#160;&#160;&#160;&#160;Compute IK for <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>goal</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> using IKFast solver with filtered constraints</p></list-item><list-item><label>10:</label><p>&#160;&#160;&#160;&#160;&#160;<bold>for</bold> each OMPL planner <italic toggle="yes">P</italic> in MoveIt <bold>do</bold></p></list-item><list-item><label>11:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>for</bold> attempt <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to 10 <bold>do</bold></p></list-item><list-item><label>12:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Attempt trajectory planning <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a 5-second timeout</p></list-item><list-item><label>13:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>if</bold>&#160;<inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula> is valid <bold>then</bold></p></list-item><list-item><label>14:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;t = TOTG(<inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula>)</p></list-item><list-item><label>15:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Compute configuration space length of <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>16:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Store (<italic toggle="yes">P</italic>, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>start</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>goal</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula>, t, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>planning</mml:mi><mml:mspace width="4.pt"/><mml:mi>time</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>config</mml:mi><mml:mspace width="4.pt"/><mml:mi>length</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) in <italic toggle="yes">D</italic></p></list-item><list-item><label>17:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>end if</bold></p></list-item><list-item><label>18:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>19:</label><p>&#160;&#160;&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>20:</label><p>&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>21:</label><p><bold>end for</bold></p></list-item><list-item><label>22:</label><p><bold>Return</bold>&#160;<italic toggle="yes">D</italic></p></list-item></list></td></tr></tbody></array></p><p>The planning process was conducted for all 104,295 pose pairs, obtaining the same number of trajectories. The pose and configuration pair distribution ensured that the dataset contained a balanced representation of short, medium, and long-distance movements, providing a diverse set of motion planning scenarios.</p><p>The dataset was divided into training, validation, and test sets, with 80%, 10%, and 10% of the trajectories, respectively. The training set was used to train the DRL agent, while the validation and test sets were used to evaluate the performance of both classical and DRL-based motion planning methods.</p></sec></sec><sec id="sec3dot2-sensors-25-05282"><title>3.2. Deep Reinforcement Learning Formulation and Training</title><p>This section describes how the motion planning task was formulated as a Markov Decision Process (MDP) and solved using DRL. The agent was trained in a simulated environment using a hybrid reward function, expert demonstrations, and curriculum learning [<xref rid="B65-sensors-25-05282" ref-type="bibr">65</xref>] to progressively acquire high-precision motion skills across the robot&#8217;s full workspace.</p><sec id="sec3dot2dot1-sensors-25-05282"><title>3.2.1. Problem Formulation</title><p>The motion planning task was modeled as a finite-horizon, episodic MDP defined by the tuple <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-script">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-script">A</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-script">S</mml:mi></mml:mrow></mml:math></inline-formula> denotes the state space, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-script">A</mml:mi></mml:mrow></mml:math></inline-formula> the action space, <italic toggle="yes">P</italic> the transition dynamics, <italic toggle="yes">r</italic> the reward function, and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> the discount factor.</p><p>The agent observes the robot&#8217;s current joint states, the relative pose to the goal, and a collision indicator. It outputs incremental joint motions as actions. The environment transitions deterministically based on the commanded joint updates, with collisions and kinematic constraints modeled through PyBullet [<xref rid="B66-sensors-25-05282" ref-type="bibr">66</xref>].</p></sec><sec id="sec3dot2dot2-sensors-25-05282"><title>3.2.2. Environment Design and State&#8211;Action Spaces</title><p>To train the reinforcement learning agent for motion planning, a custom environment was implemented using the PyBullet physics engine. The simulated setup included a UR3e robotic manipulator equipped with an OnRobot RG2 gripper and a planar floor object restraining the robot&#8217;s workspace. The UR3e robot was modeled using the URDF format, and the gripper was attached to the end-effector link. The simulation environment was designed to closely resemble the real-world setup, including the robot&#8217;s kinematic and dynamic properties, as well as collision detection capabilities. It was configured to model only the kinematics and collision checking of the robot. Dynamics were omitted to speed up the training process, as the trajectory generation problem focuses solely on geometric feasibility and does not require consideration of dynamic constraints. The environment also included a planar floor object to enable detection of potential floor collisions.</p><p>The reinforcement learning agent interacted with the environment through a structured observation and action interface. The observation space <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow></mml:math></inline-formula> was composed of 14 continuous floating-point values, ensuring that the policy has all the necessary information to generalize across the entire robot workspace. This is particularly important because if only the poses were provided, the non-unique mapping between the workspace and configuration space would prevent the policy from generalizing effectively. The observation space includes the following:<list list-type="bullet"><list-item><p>The six current joint values of the robot, <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>The Cartesian difference between the end-effector&#8217;s current pose and the goal pose:<disp-formula><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>current</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="1.em"/><mml:mo>&#916;</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>quatdiff</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>current</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where the quaternion difference quatdiff is computed as:<disp-formula><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>quatdiff</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>current</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>goal</mml:mi></mml:msub><mml:mo>&#8855;</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>current</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, &#8855; denotes the quaternion multiplication operator, and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>current</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the inverse of the current quaternion, defined as:<disp-formula><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>current</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
for a quaternion <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>current</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>A binary collision indicator:<disp-formula><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>in</mml:mi><mml:mspace width="4.pt"/><mml:mi>self</mml:mi><mml:mspace width="4.pt"/><mml:mi>or</mml:mi><mml:mspace width="4.pt"/><mml:mi>floor</mml:mi><mml:mspace width="4.pt"/><mml:mi>collision</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p>Therefore, the full observation vector is:<disp-formula><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>14</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The action space <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-script">A</mml:mi></mml:mrow></mml:math></inline-formula> was defined as a 6-dimensional continuous vector representing joint increments in radians, constrained within a fixed range for each joint:<disp-formula><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, the action space forms a continuous box, where each joint increment <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can vary independently within the interval <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These increments are applied additively to the current joint state at every step and are clipped to ensure they remain within the physical joint limits of the UR3e robot.</p><p>The problem exhibits a high-dimensional and continuous state space, involving six joint angles and seven pose deltas (position and orientation as quaternions), plus a discrete collision state. This yields a 14-dimensional hybrid observation vector with both continuous and categorical components. Furthermore, the state space is highly non-linear due to the forward kinematics and collision constraints imposed by the robot geometry. These properties significantly increase the complexity of the exploration and learning process, making standard RL strategies difficult to converge. As such, careful design of the reward function and additional training strategies (such as expert demonstrations and curriculum learning) were necessary for facilitating effective policy optimization.</p></sec><sec id="sec3dot2dot3-sensors-25-05282"><title>3.2.3. Curriculum Learning</title><p>To facilitate progressive learning and stabilize convergence in this high-dimensional motion planning task, a dual-curriculum learning strategy was implemented. The curriculum controlled the difficulty of the task by gradually tightening the tolerances required for successful completion:<list list-type="bullet"><list-item><p>Position precision <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: distance between the current and goal Cartesian positions in meters.</p></list-item><list-item><p>Orientation precision <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: angular distance between the current and goal orientations (in quaternion form).</p></list-item></list></p><p>Progression to the next curriculum level was triggered after the agent completed a fixed number of consecutive successful episodes (typically 100, to ensure proper progress and reach the expected tolerances) under the current tolerances. This mechanism allowed the policy to gradually master coarse-to-fine motion precision while maintaining high exploration and learning signals.</p><p>Both position and orientation curricula were coupled, requiring the agent to improve simultaneously in translational and rotational dimensions. This approach ensured balanced learning for the UR3e robot, addressing its rotational redundancy and joint constraints while maintaining consistent progress across both position and orientation accuracy. This coupling also prevented the agent from early focusing on reaching high accuracy in one dimension while neglecting the other, which could lead to suboptimal behaviors or local minima.</p></sec><sec id="sec3dot2dot4-sensors-25-05282"><title>3.2.4. Reward Function</title><p>Several reward strategies were initially tested, including sparse and dense formulations. However, neither proved sufficient on its own to ensure stable training convergence or generalization.</p><p>Sparse rewards, which provided a fixed positive reward only when the agent reached the goal, led to sparse credit assignment and poor exploration [<xref rid="B67-sensors-25-05282" ref-type="bibr">67</xref>]. On the other hand, dense rewards based on step-wise penalties proportional to Cartesian or configuration space distances resulted in premature convergence to suboptimal behaviors, such as local oscillations near the goal [<xref rid="B68-sensors-25-05282" ref-type="bibr">68</xref>].</p><p>As a result, a hybrid reward strategy was adopted. The final reward function combined a dense negative step reward with a sparse success reward. At each timestep <italic toggle="yes">t</italic>, the reward was calculated as:<disp-formula><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#966;</mml:mi><mml:mrow><mml:mi>goal</mml:mi><mml:mspace width="4.pt"/><mml:mi>reached</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>success</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where:<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the Euclidean distance between the end-effector position and the target position in meters, computed as:<disp-formula><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>goal</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the orientation error, computed as the quaternion angular distance (rad), computed as:<disp-formula><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">arccos</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>2</mml:mn><mml:msup><mml:mfenced separators="" open="&#x2329;" close="&#x232A;"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>goal</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> are scalar weights for position and orientation penalties.</p></list-item><list-item><p><inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#966;</mml:mi><mml:mrow><mml:mi>goal</mml:mi><mml:mspace width="4.pt"/><mml:mi>reached</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is an indicator function that activates when both <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>success</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the success reward, which scales with the current curriculum level <italic toggle="yes">i</italic>.</p></list-item></list></p><p>The curriculum defines progressively stricter thresholds for success:<disp-formula><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#183;</mml:mo><mml:mi>&#948;</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mi>target</mml:mi></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#183;</mml:mo><mml:mi>&#948;</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mi>target</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the decay factor, and <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#949;</mml:mi><mml:mi>target</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the minimum tolerance. The success reward was also scaled inversely with the curriculum level, using:<disp-formula><mml:math id="mm80" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>success</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mi>&#948;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This encourages the agent to continue improving even as the task becomes more difficult, maintaining a strong learning signal. Multiple experiments were conducted to determine the optimal values for <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>, success reward <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>success</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, the curriculum levels, and decay factor <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula>. The final values were selected based on empirical performance and convergence speed. The final values used in the training process were as follows:<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p><inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p><inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p>Curriculum levels: 100;</p></list-item><list-item><p>Decay factor <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.98</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p>Success thresholds: <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mi>target</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0005</mml:mn><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mi>target</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mspace width="0.166667em"/><mml:mi>rad</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p>Maximum number of consecutive successful episodes to trigger curriculum progression: 100;</p></list-item><list-item><p>Maximum number of steps per episode: 200.</p></list-item></list></p><p>At the start of training, the tolerances were set to relaxed values <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn><mml:mspace width="0.166667em"/><mml:mi>rad</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, making it easier for the agent to succeed.</p><p>This dual curriculum strategy ensured that the agent first learned coarse reaching behaviors and then progressively refined its precision, ultimately enabling it to solve the full task under tight tolerances.</p><p>At curriculum level <italic toggle="yes">i</italic>, the success predicate requires simultaneous satisfaction of position and orientation tolerances:<disp-formula id="FD1-sensors-25-05282"><label>(1)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-script">S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.277778em"/><mml:mo>=</mml:mo><mml:mspace width="0.277778em"/><mml:mfenced separators="" open="{" close="}"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>|</mml:mo><mml:mspace width="0.277778em"/><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mspace width="0.277778em"/><mml:mo>&#8743;</mml:mo><mml:mspace width="0.277778em"/><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>Advancement from level <italic toggle="yes">i</italic> to <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is triggered only after <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>succ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> consecutive successful episodes under <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold-script">S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The sparse success bonus <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>success</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is granted exclusively when <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="bold-script">S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>; partial satisfaction (only one metric within tolerance) yields no bonus and never triggers level advancement. This coupling prevents the policy from optimizing one objective at the expense of the other.</p><p>Although a 6-DoF manipulator tracks a 6-DoF end-effector pose, it exhibits discrete IK redundancy. For each sampled pose, we enumerate all IKFast solutions <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-script">Q</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and retain collision-free ones (<xref rid="sec3dot1dot2-sensors-25-05282" ref-type="sec">Section 3.1.2</xref>). Decoupled curricula allow policies to minimize position error while letting orientation drift, often leading to late-stage wrist flips or branch changes that increase path length and risk joint-limit proximity. By contrast, the coupled predicate <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold-script">S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> regularizes training toward trajectories that respect both objectives throughout the episode, reducing branch changes and orientation drift.</p></sec></sec><sec id="sec3dot3-sensors-25-05282"><title>3.3. Coupled vs. Decoupled Pose Curricula</title><p>To validate the design choice of coupling position and orientation tolerances during curriculum learning, we compared the proposed coupled strategy against a decoupled variant in which position and orientation thresholds were tightened independently. Both agents used identical SAC architectures, hyperparameters, and training conditions. A 2,000,000 step training budget was allocated to each agent, with performance evaluated on a held-out test set.</p><p>The coupled curriculum produced trajectories that maintained orientation accuracy throughout the motion, avoiding late-stage wrist flips and abrupt joint changes observed in the decoupled case. This led to paths that were generally shorter, smoother, and more consistent, with fewer configuration switches between Inverse Kinematic (IK) branches. In contrast, the decoupled variant often achieved good positional accuracy early, but allowed significant orientation drift that required corrective maneuvers near the goal, resulting in longer and less stable motions.</p><p>These observations align with the discrete redundancy characteristics of a 6 DoF manipulator: by enforcing simultaneous satisfaction of position and orientation objectives during training, the coupled approach regularizes the policy toward solutions that respect both constraints throughout the motion, improving execution stability without the need for post-hoc corrections.</p><sec id="sec3dot3dot1-sensors-25-05282"><title>3.3.1. Episode Structure</title><p>Each training episode was initialized by randomly selecting an initial and goal pose from the previously generated dataset of valid configurations. One valid joint configuration was chosen for each pose using the associated IK solutions. The robot was reset to the initial configuration, and the goal pose was provided as part of the observation vector. The episode proceeded with the agent applying incremental joint actions at each step until one of the following termination conditions was met:<list list-type="order"><list-item><p>The end-effector reached the goal within the current curriculum tolerances; this is the success condition defined as follows:<disp-formula><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mspace width="1.em"/><mml:mi>and</mml:mi><mml:mspace width="1.em"/><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>&#949;</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>A collision with either the robot itself or the floor occurred;</p></list-item><list-item><p>A maximum number of steps per episode was reached (typically 200, ensuring that the agent had sufficient time to reach any goal and to explore the environment).</p></list-item></list></p><p>This episode structure ensured diverse trajectory samples while enforcing physical feasibility and encouraging the agent to learn a policy that can generalize across the entire workspace.</p></sec><sec id="sec3dot3dot2-sensors-25-05282"><title>3.3.2. Training Algorithm and Hyperparameter Optimization</title><p>The training process was implemented using the Stable-Baselines3 (SB3) [<xref rid="B69-sensors-25-05282" ref-type="bibr">69</xref>] framework, which provides a robust and modular implementation of state-of-the-art reinforcement learning algorithms. SB3 was chosen for its widespread adoption in the research community, the maturity of its implementation, the extensive validation of its reinforcement learning algorithms, and ease of integration with custom Gymnasium environments [<xref rid="B70-sensors-25-05282" ref-type="bibr">70</xref>].</p><p>Due to the high dimensionality and non-linear constraints of the motion planning problem, selecting an appropriate reinforcement learning algorithm was critical for achieving stable and efficient learning. An iterative evaluation process was carried out, comparing several state-of-the-art off-policy and on-policy algorithms, including PPO, TD3, and SAC.</p><p>Among these, SAC demonstrated the most promising results in terms of convergence speed, stability, and final performance [<xref rid="B56-sensors-25-05282" ref-type="bibr">56</xref>]. SAC is particularly well-suited for this task due to its entropy-regularized objective, which promotes exploration in large and continuous action spaces. Furthermore, its off-policy nature allows more efficient reuse of past experiences, which is especially beneficial given the size and complexity of the state&#8211;action space involved in robotic motion planning.</p><p>The selected actor and critic networks were implemented as Multi-Layer Perceptrons (MLPs) with two hidden layers, each containing 256 units and ReLU activation functions. The output layer of the actor network was configured to produce continuous actions, while the critic network was designed to output Q-values for the state&#8211;action pairs. The policy was trained using the soft Bellman backup operator, which incorporates both the Q-value and entropy terms to encourage exploration.</p><p>To optimize performance, the hyperparameters of SAC were tuned using a combination of the Optuna framework [<xref rid="B71-sensors-25-05282" ref-type="bibr">71</xref>] and empirical results. Optuna, an automatic hyperparameter optimization library based on sequential model-based optimization, was employed to explore the search space for parameters such as learning rates, batch size, entropy coefficient, and target smoothing coefficient. Additionally, empirical testing was conducted to refine the selected hyperparameters further, ensuring robustness and optimal performance across multiple seeds.</p><p>The hyperparameter search was conducted over a range of values, including learning rates in <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, batch sizes in <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mn>1024</mml:mn><mml:mo>,</mml:mo><mml:mn>2048</mml:mn><mml:mo>,</mml:mo><mml:mn>4096</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and entropy coefficients in <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each trial was evaluated over 500k environment steps using the validation set success rate as the objective. Our actor and critic networks use two hidden layers of 256 units each with ReLU activations, matching common practice in continuous-control benchmarks and providing sufficient representational capacity without overfitting [<xref rid="B18-sensors-25-05282" ref-type="bibr">18</xref>,<xref rid="B43-sensors-25-05282" ref-type="bibr">43</xref>].</p><p>Optimal values for <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>, success reward <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>success</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and decay factor <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula> were also tuned using empirical testing and Optuna. The final values were selected based on a combination of Optuna search results, empirical performance, and convergence speed. The values used in the training process are then presented in the previous section.</p><p>The final hyperparameter configuration used for training is summarized in <xref rid="sensors-25-05282-t001" ref-type="table">Table 1</xref>; they were selected for their balance of convergence speed and stability. Training was performed using mini-batch updates sampled from a replay buffer, with target networks for the value function and policy updated using Polyak averaging.</p></sec><sec id="sec3dot3dot3-sensors-25-05282"><title>3.3.3. Expert Demonstrations and Replay Buffer Injection</title><p>Despite the careful design of the environment, reward function, and curriculum, training the SAC agent from scratch remained computationally expensive due to the high complexity and dimensionality of the state space. The exploration phase, in particular, suffered from inefficiency during early stages, where the agent often failed to encounter successful trajectories within reasonable timeframes. To address this challenge, a strategy based on expert demonstrations was introduced to accelerate training and improve learning stability.</p><p>A set of expert trajectories was readily available from the OMPL-based motion planning dataset described in the previous sections, of which 80% (from the total 104,295 samples) was used for training. Each trajectory was composed of a sequence of collision-free joint configurations connecting a start and a goal pose. To leverage this data, we reconstructed episodes from these trajectories and injected them directly into the SAC agent&#8217;s replay buffer.</p><p>Each expert trajectory was transformed into a sequence of transition tuples <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was the observation corresponding to the configuration at time step <italic toggle="yes">t</italic>;</p></list-item><list-item><p><inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was computed as the joint difference <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p><inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was computed using the same hybrid reward function as in online training;</p></list-item><list-item><p><inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was the observation after applying <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p>The injection strategy followed an adaptive conditional rule: after each training episode, the environment evaluated whether the policy successfully reached the goal within the prescribed tolerances. If the episode failed, a complete expert trajectory was sampled from the dataset and injected in its entirety into the replay buffer. This ensured that the agent could observe full sequences of state&#8211;action&#8211;reward transitions, leading from an initial state to a successful goal state, rather than isolated transitions.</p><p>Once injected, expert and online trajectories coexisted in the buffer and were sampled uniformly during training. Because injections only occurred on failures, the proportion of expert data in the buffer was higher during early training (when failures were common) and decreased naturally as the policy improved. This adaptive mechanism allowed the agent to benefit from high-quality demonstrations when most needed, while avoiding excessive reliance on them later, thus maintaining a healthy balance between exploiting expert knowledge and exploring novel strategies.</p></sec><sec id="sec3dot3dot4-sensors-25-05282"><title>3.3.4. Trajectory Post-Processing with TOPPRA</title><p>The configuration space paths generated by the trained DRL policy were post-processed using the Reachability-Analysis-based Time-Optimal Path Parameterization (TOPPRA) [<xref rid="B72-sensors-25-05282" ref-type="bibr">72</xref>] algorithm, allowing for the production of complete time-parameterized trajectories that respect the robot&#8217;s velocity and acceleration limits. TOPPRA is a well-established algorithm for time-optimal trajectory generation, which computes the minimum time required to traverse a given path while satisfying dynamic constraints. The algorithm was applied to the DRL-generated paths to ensure that they were not only kinematically feasible but also dynamically optimal for execution on the UR3e robot. This step enforces dynamic feasibility by respecting joint velocity and acceleration limits.</p><p>Each DRL path, composed of a sequence of joint-space waypoints <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, was first interpolated using a quintic clamped spline <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the normalized path parameter. TOPPRA then solved a reachability-constrained optimization problem to compute a time-parameterized trajectory <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, subject to:<disp-formula><mml:math id="mm120" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>&#729;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>&#168;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
with</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mrow><mml:mi>rad</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>: conservative joint velocity limits for UR3e.</p></list-item><list-item><p><inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:mi>rad</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>: conservative joint acceleration limits.</p></list-item><list-item><p><inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>: scaling factors to reflect slower, safer motion profiles.</p></list-item></list><p>The time parameterization was computed using TOPPRA&#8217;s Seidel solver over a grid of 1000 waypoints. Each trajectory was processed individually, and the time required for parameterization was recorded and added to the original planning time. This enabled a consistent performance evaluation during benchmarking.</p></sec><sec id="sec3dot3dot5-sensors-25-05282"><title>3.3.5. Training Process and Hardware Setup</title><p>The training process was structured into two distinct phases: an initial phase leveraging expert demonstrations to bootstrap learning, and a subsequent phase, where the agent refined its policy through self-guided exploration and reinforcement learning.</p><p>In the initial phase, expert trajectories were incorporated into the agent&#8217;s experience replay buffer for every failed episode, providing high-quality samples to accelerate learning and improve early-stage performance. This phase allowed the agent to quickly grasp fundamental motion planning behaviors by imitating expert solutions.</p><p>The final phase emphasized self-exploration, enabling the agent to autonomously interact with the environment and refine its policy based on its own experiences. This phase was critical for the agent to generalize beyond the expert demonstrations and adapt to a wide range of scenarios.</p><p>Throughout the training process, key performance metrics such as success rate, average reward, and episode length were continuously monitored. Periodic evaluations were conducted to ensure the agent was effectively learning and progressing towards a robust and reliable policy.</p><p>The entire training process spanned 6 million environment steps. The hardware setup details are shown in <xref rid="sensors-25-05282-t002" ref-type="table">Table 2</xref>.</p></sec></sec></sec><sec sec-type="results" id="sec4-sensors-25-05282"><title>4. Results</title><p>This section presents the experimental results obtained from the large-scale benchmarking of the DRL planner against classical sampling-based planners from MoveIt&#8217;s OMPL planning pipeline. First, we analyze the training performance of the DRL agent, evaluating its learning progression through success rates, reward evolution, curriculum advancement, and episode length over 6 million environment interactions. Subsequently, we compare the DRL-based planner to traditional OMPL planners across the common dataset of motion queries.</p><sec id="sec4dot1-sensors-25-05282"><title>4.1. Experimental Setup</title><p>The experiments were conducted in a simulated environment using the PyBullet physics engine. The UR3e robot was modeled using the URDF format, and the simulation environment was designed to closely resemble the real-world setup. The training process was implemented using the Stable-Baselines3 framework, with the SAC algorithm selected for its efficiency and stability in high-dimensional continuous action spaces.</p></sec><sec id="sec4dot2-sensors-25-05282"><title>4.2. Training Performance of the DRL Policy</title><p>In this subsection, we present the training performance of the DRL policy over the course of 6 million steps. The training process was monitored using various metrics, including the average reward, success rate, and episode length.</p><sec id="sec4dot2dot1-sensors-25-05282"><title>4.2.1. Success Rate</title><p><xref rid="sensors-25-05282-f005" ref-type="fig">Figure 5</xref> illustrates the percentage of successful episodes over the total number of episodes during the first 100,000 steps. Initially, the performance is low due to the agent&#8217;s exploration phase, where expert demonstrations are not yet fully integrated into the training process. However, as the agent learns from these trajectories and refines its policy, the percentage of successful episodes improves significantly.</p><p><xref rid="sensors-25-05282-f006" ref-type="fig">Figure 6</xref> shows the success rate over the entire training process. The success rate remains higher than 90% after the first 10,000 steps, but as the curricula progress and the goals become more challenging, the success rate slightly fluctuates. The agent learns to adapt to the new goals and improve its performance over time. The success rate reaches a plateau after approximately 6 million steps, indicating that the agent has learned to reach the goal pose effectively across a wide range of configurations.</p></sec><sec id="sec4dot2dot2-sensors-25-05282"><title>4.2.2. Reward</title><p><xref rid="sensors-25-05282-f007" ref-type="fig">Figure 7</xref> shows the reward progression over the course of the training process. The reward is computed as the mean of the rewards obtained during each episode. It starts at a low value and gradually increases as the agent learns to reach the goal pose more effectively. With the integration of expert demonstrations, the reward increases significantly during the first 10,000 steps. The agent learns to reach the goal pose more effectively, resulting in higher rewards. As the training progresses, the reward continues to improve as the curriculum becomes more challenging, and the reward function is adjusted to reflect the new tolerances. The reward reaches a plateau after approximately 6 million steps, indicating that the agent has learned to reach the goal pose effectively up to the specified tolerances.</p></sec><sec id="sec4dot2dot3-sensors-25-05282"><title>4.2.3. Curriculum Progression</title><p><xref rid="sensors-25-05282-f008" ref-type="fig">Figure 8</xref> shows the goal position and orientation tolerances over the course of the training process. It illustrates the progression of the curriculum levels, with the position and orientation tolerances decreasing as the agent learns to reach the goal poses. The agent starts with relaxed tolerances and gradually tightens them as it learns to reach the goal poses more accurately. Initially, the progress is fast as the agent learns to reach the goal poses with relaxed tolerances. As the training progresses, the agent encounters more challenging tolerances, leading to a slower progression in the curriculum levels. The figures show that the agent successfully adapts to the new tolerances and improves its performance over time. It is important to note that the curriculum levels are coupled for position and orientation, preventing the agent from focusing solely on one aspect of the task. This coupling ensures that the agent learns to reach the goal poses in both position and orientation simultaneously, which is crucial for effective motion planning in robotic applications. The figures show that for the agent, it is harder to reach the position than the orientation. This is evident from the slower progression in the position curriculum levels compared to the orientation curriculum levels. The agent requires more training steps to achieve tighter tolerances in position, indicating that the translational component of the task poses a greater challenge than the rotational component, especially for the higher curriculum levels.</p></sec><sec id="sec4dot2dot4-sensors-25-05282"><title>4.2.4. Episode Length</title><p><xref rid="sensors-25-05282-f009" ref-type="fig">Figure 9</xref> shows the average episode length over the course of the training process. The average episode length is computed as the mean number of steps taken to reach a termination condition. The initial low episode length is due to the agent&#8217;s exploration phase, where the expert demonstrations are not yet fully integrated into the training process. In this phase, the agent provokes collisions and fails to reach the goal pose, leading to shorter episode lengths. As the agent learns from the expert trajectories and refines its policy, the episode length increases until it reaches a plateau. The episode length remains relatively stable after approximately 6 million steps with minor increases, indicating that, as the goals become more challenging, more steps are required to reach the goal pose. The agent learns to adapt to the new goals and improve its performance over time.</p></sec></sec><sec id="sec4dot3-sensors-25-05282"><title>4.3. Comparison Between DRL and OMPL-Based Planners</title><p>To assess the relative performance of the proposed DRL-based planner and classical sampling-based motion planners from MoveIt&#8217;s OMPL pipeline, a systematic comparison was carried out across the common dataset of goal pairs. This comparison uses the test set of 10,429 trajectories, which were not used during training or validation. All planners were tested under identical workspace conditions, and the resulting trajectories were analyzed according to the following key performance metrics:<list list-type="bullet"><list-item><p>Success rate: percentage of planning requests that resulted in valid, collision-free paths.</p></list-item><list-item><p>Planning time: time required to compute a trajectory (DRL includes inference and TOPPRA time parameterization; OMPL includes full MoveIt pipeline).</p></list-item><list-item><p>Trajectory length: total number of waypoints in the path (pre-parameterization).</p></list-item></list></p><sec id="sec4dot3dot1-sensors-25-05282"><title>4.3.1. Overall Performance</title><p><xref rid="sensors-25-05282-t003" ref-type="table">Table 3</xref> presents the success rate and the average and standard deviation of key performance metrics for the evaluated planners across the validation dataset, including success rate, planning time, and trajectory length. These metrics provide a high-level summary of the comparative performance.</p><p>The smoothness of the joint-space trajectories is computed as the same quadratic objective formulation employed in TOPPRA. In this context, the trajectory is represented as <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">n</italic> is the number of joints. The smoothness metric <italic toggle="yes">S</italic> is defined as the integral of the squared joint acceleration norm over the execution time,<disp-formula id="FD2-sensors-25-05282"><label>(2)</label><mml:math id="mm126" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mspace width="0.277778em"/><mml:mo>=</mml:mo><mml:mspace width="0.277778em"/><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:msubsup><mml:msubsup><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>&#168;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> denotes the Euclidean norm across all joints. This metric penalizes rapid changes in velocity, effectively favoring trajectories with lower acceleration magnitudes over time. In discrete form, with sampled accelerations <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>&#168;</mml:mo></mml:mover><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> at times <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the integral is approximated using the trapezoidal rule as<disp-formula id="FD3-sensors-25-05282"><label>(3)</label><mml:math id="mm130" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mspace width="0.277778em"/><mml:mo>&#8776;</mml:mo><mml:mspace width="0.277778em"/><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>&#168;</mml:mo></mml:mover><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mspace width="0.166667em"/><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. This formulation is directly aligned with the convex quadratic cost functions supported by TOPPRA, which allow optimizing not only for traversal time but also for motion smoothness [<xref rid="B72-sensors-25-05282" ref-type="bibr">72</xref>].</p><p>Detailed box plots are provided for each metric. These box plots exclude outliers to focus on the central distribution of results. Only the most relevant planners are shown, as some planners exhibited results with metrics far from meaningful values, making the plots unbalanced for comparison.</p></sec><sec id="sec4dot3dot2-sensors-25-05282"><title>4.3.2. Planning Time and Waypoint Complexity</title><p><xref rid="sensors-25-05282-f010" ref-type="fig">Figure 10</xref> presents the distribution of planning times for each method. As expected, the DRL policy produces deterministic inference results with low variance and significantly reduced planning time. OMPL planners exhibit much higher variance, with some instances exceeding several seconds due to roadmap construction or constraint resolution. It is important to note that for every failed planning attempt, the OMPL planning pipeline exhausted the maximum number of attempts, leading to a planning time of 5 s. This is omitted from the box plot for clarity.</p><p><xref rid="sensors-25-05282-f011" ref-type="fig">Figure 11</xref> shows the number of waypoints per path. The DRL policy produces smoother, more direct paths with fewer discrete steps compared to OMPL-generated trajectories, which tend to be longer due to sampling density and post-processing limitations.</p></sec><sec id="sec4dot3dot3-sensors-25-05282"><title>4.3.3. Configuration Space Path Length</title><p><xref rid="sensors-25-05282-f012" ref-type="fig">Figure 12</xref> shows the distribution of the total path length in configuration space, computed as the cumulative Euclidean distance between consecutive joint-space waypoints. This metric offers a more detailed insight into the overall motion efficiency. The DRL policy consistently produces shorter joint-space paths, likely due to its incremental control strategy and its tendency to avoid redundant motions.</p></sec><sec id="sec4dot3dot4-sensors-25-05282"><title>4.3.4. Failure Modes and Comparison</title><p>Across the 10,429-query test set, our SAC-based planner failed to produce collision-free trajectories in approximately 5.9% of cases. These failures predominantly occur for goal poses located near the extreme boundaries of the UR3e&#8217;s workspace or due to high-precision goal tasks at the tightest curriculum tolerances. In these scenarios, the policy occasionally oscillates or cannot satisfy both position and orientation constraints within the allotted 200 steps.</p><p>The observed failures at the edges of the workspace can be linked to both task-specific and algorithmic factors. From a robotics perspective, extreme poses often lie near the manipulator&#8217;s kinematic limits, where feasible solution regions are narrow and discontinuous.</p><p>From a deep reinforcement learning perspective, such boundary states are typically underrepresented in the training data distribution. Even with uniform workspace sampling, collision filtering and the curriculum&#8217;s gradual tolerance tightening reduce the relative frequency of these extreme cases in early training stages. Consequently, the policy&#8217;s state&#8211;action value function is less accurate in these regions. Furthermore, reward shaping based primarily on Euclidean pose errors can lead to gradient disappearance near boundaries if the agent&#8217;s actions produce negligible improvements due to physical constraints, resulting in weak learning signals. These factors combine to create conditions in which the policy generalizes poorly to the workspace edges, despite strong performance in the interior.</p><p>Mitigation strategies include targeted oversampling of boundary poses, curriculum phases explicitly dedicated to extreme regions, or auxiliary exploration bonuses to increase coverage. Future work may also explore adaptive reward scaling near kinematic limits to maintain informative gradients for policy improvement.</p><p>By contrast, classical sampling-based planners such as RRTConnect (approximately 7.1% failure rate) can sometimes succeed on these boundary cases through exhaustive sampling, though at the cost of planning times up to several seconds. Conversely, the DRL planner successfully handles certain narrow-passage queries&#8212;where OMPL planners fail repeatedly&#8212;by leveraging its learned collision-avoidance priors. This complementary behavior suggests that incorporating fallback classical planning for the DRL&#8217;s rare failure cases (and vice versa) could yield a more robust hybrid system.</p></sec></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05282"><title>5. Discussion</title><sec id="sec5dot1-sensors-25-05282"><title>5.1. Advantages of DRL for Industrial Motion Planning</title><p>The experimental results demonstrate clear advantages of the DRL-based planner over traditional sampling-based motion planning techniques when applied to the task of collision-free goal reaching in the UR3e robot&#8217;s workspace. The most prominent benefit is the significantly reduced planning time: the DRL policy consistently computes valid actions within milliseconds, in stark contrast to OMPL planners that often require hundreds of milliseconds to several seconds. This determinism and low-latency response make the DRL approach suitable for real-time applications, including moving target poses, where responsiveness is critical.</p><p>Another major advantage lies in trajectory compactness. DRL-generated paths exhibit fewer waypoints and shorter configuration space lengths. This behavior can be attributed to the continuous control formulation of the policy and the curriculum-based reward shaping, which encourages direct and minimal joint-space movement. In contrast, OMPL planners, being sampling-based, rely on a fixed sampling density and may produce longer paths with more waypoints due to their exploration strategies. This results in smoother trajectories that are easier to execute on real hardware, reducing wear and tear on the robot&#8217;s joints and improving overall efficiency.</p><p>The DRL planner also achieved a high success rate, slightly outperforming most of the OMPL planners in the same benchmark conditions. This indicates that the agent successfully generalized within the sampled workspace and learned robust collision-free behaviors. The use of curriculum learning contributed to this result by enabling the agent to first solve easy instances and progressively adapt to tighter tolerances. The success rate remained stable above 90% across a wide range of configurations, even when the curriculum posed increasingly strict constraints.</p><p>The deterministic nature of DRL also eliminates the variability observed in OMPL-based approaches. Since both the trained policy and the post-processing step (TOPPRA) are deterministic, the only source of uncertainty is the number of steps needed to reach the goal, which was shown to be consistently bounded across tasks. This provides strong guarantees on worst-case behavior, which is often difficult to ensure in sampling-based planning. The use of expert demonstrations from OMPL planning pipeline trajectories significantly accelerated training and improved sample efficiency. This hybrid approach helped bootstrap the learning process, allowing the agent to benefit from classical planning knowledge without inheriting its runtime constraints. This synergy highlights a promising direction where classical methods can be used as priors to warm-start learning-based planners for high-performance industrial deployment.</p><p>Finally, the use of the same ROS interfaces and MoveIt! pipeline for both the DRL-based planner and OMPL planners allows for seamless integration into existing robotic systems. This compatibility facilitates the deployment of the DRL policy in real-world applications without requiring modifications to the software architecture.</p></sec><sec id="sec5dot2-sensors-25-05282"><title>5.2. Limitations and Failure Cases</title><p>Despite the promising results obtained with the DRL-based planner, several limitations must be acknowledged. Failure cases were also observed during the early phases of training, where the policy frequently failed due to collisions or an inability to make progress toward the goal. While expert demonstrations mitigated this issue, their usefulness depends on the quality and diversity of the dataset, and integrating such demonstrations requires a well-structured replay buffer and careful management of exploration vs. imitation. Moreover, the DRL training process remains computationally expensive, requiring several million environment steps to converge (approximately 22 h). This training burden contrasts sharply with the plug-and-play nature of classical OMPL planners, which require no learning phase and can be deployed immediately.</p><p>Another important consideration is the potential advantage of multi-query planners in static environments. Multi-query planners, such as PRM or LazyPRM, are designed to handle multiple planning queries efficiently by constructing a reusable roadmap of the configuration space. Once the roadmap is built, these planners can quickly find paths for new start and goal configurations by leveraging the precomputed graph structure. This makes them particularly effective in scenarios where the environment remains static and multiple planning requests are expected. However, the upfront computational cost of building the roadmap can be significant, and the quality of the roadmap heavily depends on its density and coverage of the configuration space.</p><p>Transfer learning [<xref rid="B73-sensors-25-05282" ref-type="bibr">73</xref>] emerges as a powerful solution to address both the computational cost of training and the adaptability to new environments. By leveraging knowledge from previously trained policies, transfer learning can significantly reduce the number of training steps required to adapt to unseen environments or tasks. This capability not only mitigates the initial training burden but also ensures that the DRL-based planner remains versatile and scalable, making it a compelling choice for industrial applications where environments may evolve over time or require frequent reconfiguration.</p></sec><sec id="sec5dot3-sensors-25-05282"><title>5.3. Training Overhead and Industrial Deployment Considerations</title><p>While the DRL-based planner demonstrates superior planning speed and trajectory compactness, it requires significant upfront computational resources for training.</p><p>In contrast, classical OMPL planners are plug-and-play, requiring no training and minimal configuration effort, making them highly suitable for rapid deployment and integration.</p><p>To better understand the trade-offs, <xref rid="sensors-25-05282-t004" ref-type="table">Table 4</xref> compares the overall computational costs of both approaches. The DRL planner amortizes its training cost across thousands of inferences, yielding deterministic sub-10 ms planning latency once trained. OMPL planners, on the other hand, exhibit variable planning times across queries, often exceeding hundreds of milliseconds or seconds in complex workspaces.</p><p>While the upfront cost of DRL is non-negligible, this investment can be justified in settings such as the following:<list list-type="bullet"><list-item><p>The same planning task is repeated across many cycles (e.g., 24/7 manufacturing).</p></list-item><list-item><p>Real-time responsiveness is critical.</p></list-item><list-item><p>Motion quality and energy efficiency are prioritized.</p></list-item></list></p><p>Additionally, first experiments indicate that simple retrainings on new datasets can be performed in significantly less time than the initial training, depending on the complexity of the new environment. This makes the DRL approach increasingly attractive for dynamic or reconfigurable industrial applications.</p></sec><sec id="sec5dot4-sensors-25-05282"><title>5.4. Comparison to Sampling-Based Planning</title><p>The comparative analysis between the DRL policy and MoveIt&#8217;s OMPL planners highlights complementary strengths of each approach, offering insights into when one may be favored over the other.</p><p>Classical sampling-based planners, such as RRT, PRM, or KPIECE, exhibit a high degree of generality and are well-suited for environments with changing constraints, varying kinematic chains, or unknown obstacles. Their flexibility and modularity make them ideal for prototyping in diverse robotic systems, and they can operate immediately without any prior data or learning phase. Moreover, OMPL planners natively handle motion constraints, including joint limits and collision avoidance, via the MoveIt planning pipeline, making them robust for a wide range of planning tasks.</p><p>However, these benefits come at the cost of computational overhead and non-deterministic behavior. Planning times are highly variable depending on the specific planner, goal pair, and search space complexity. In contrast, the DRL policy generates motions with near-constant latency and complete determinism once trained. This enables seamless integration into real-time control loops, a requirement in many industrial applications where planning speed and predictability are critical.</p><p>Another key difference lies in trajectory quality. DRL-generated paths are significantly shorter in both the number of waypoints and configuration space length. This suggests more direct and efficient joint-space motion, which is likely a result of policy optimization for task completion rather than exploration. While OMPL planners employ smoothing and shortcutting, they remain constrained by discrete sampling resolution and the inherent randomness of the roadmap or tree construction process.</p><p>The success of the DRL approach depends on prior training on a representative dataset. This makes it less adaptable to novel or unseen constraints without additional learning. In contrast, OMPL planners are stateless and can adapt to arbitrary start and goal poses within the robot&#8217;s kinematic limits, making them more versatile for exploration or dynamic environments.</p></sec><sec id="sec5dot5-sensors-25-05282"><title>5.5. Future Work</title><p>Future work will focus on three main directions. First, we aim to explore the application of transfer learning to adapt the trained DRL policies to new scenarios and environments. In particular, we will study how the planner generalizes to irregular and curved workspace boundaries, as opposed to the planar configurations considered in this work. While the present dataset was limited to the robot&#8217;s reachable region with the floor plane as the main obstacle, future experiments will test adaptability to more complex environments with variable obstacle geometries. This approach will leverage the knowledge acquired during the initial training phase to significantly reduce the computational cost and time required for retraining in novel settings. Transfer learning is particularly promising for dynamic or reconfigurable industrial environments, where the robot&#8217;s workspace or task requirements may change frequently. By fine-tuning the policy on new datasets or using domain adaptation techniques, we aim to extend the generalization capabilities of the DRL planner while maintaining its low-latency and deterministic performance. This will also allow us to evaluate the policy&#8217;s ability to adapt rapidly to new, highly variable environments and task configurations, thereby extending beyond the controlled benchmark presented here.</p><p>Second, we plan to analyze and benchmark the use of DRL policies as a preprocessor for optimization-based motion planning algorithms, such as CHOMP and STOMP. These algorithms excel at refining trajectories to meet additional constraints, such as smoothness, obstacle clearance, and dynamic feasibility, but often require a high-quality initial trajectory to converge efficiently. By providing a near-optimal initialization, the DRL planner can significantly reduce the computational burden of these optimization-based methods, enabling faster convergence and higher-quality solutions. While in this work we did not implement a hybrid planner, preliminary results suggest that this hybrid approach could combine the strengths of both paradigms, offering the speed and determinism of DRL with the precision and constraint-handling capabilities of optimization-based planners. Future work will include systematic benchmarking of this hybrid framework to evaluate its performance across diverse industrial scenarios.</p><p>Lastly, we will pursue hardware validation on the UR3e and generalization to more complex robotic systems. At this stage, we will also incorporate sensor noise and small disturbances in the experimental evaluation, as robustness to such factors is essential for bridging the sim-to-real gap. Although noise does not directly affect execution in our current setup, since complete trajectories are generated offline, its effect on state estimation will be systematically studied in the hardware validation phase.Because our policy runs entirely offline (requiring only the robot&#8217;s kinematic model, an initial joint configuration, and a target end-effector pose), it is expected to transfer to the physical UR3e nearly plug-and-play; we will validate this and apply lightweight calibration or fine-tuning on a small set of real trajectories if needed to correct any residual kinematic offsets. The same dataset generation and training pipeline can be directly extended to dual-arm manipulators by expanding the state/action spaces and encoding coordination constraints, just as OMPL planners do. Multi-agent systems stand to benefit from the planner&#8217;s low-latency inference, enabling real-time replanning for collision avoidance among agents. Furthermore, trajectory execution will follow a two-step workflow: first, validation in simulation to ensure the correctness of the planned path, and second, execution on the real robot through standard ROS interfaces. This approach leverages existing industrial safety mechanisms while providing an additional layer of verification, thereby ensuring safe and reliable operation of the DRL-generated trajectories in real industrial lines.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05282"><title>6. Conclusions</title><p>This work presents a comprehensive comparison between classical MoveIt&#8217;s OMPL sampling-based motion planning pipeline and a DRL approach based on SAC for industrial robot motion planning. The study focused on the UR3e robotic arm equipped with an RG2 gripper in a workspace constrained only by self-collisions and ground contact, allowing for a controlled yet representative evaluation of planning performance.</p><p>To support this comparison, a large-scale dataset was generated using IK sampling and filtering, followed by motion planning using MoveIt-integrated OMPL planners. A DRL policy was trained using curriculum learning and expert demonstrations extracted from the same OMPL-generated trajectories. The trained DRL policy was then integrated with TOPPRA for time-optimal parameterization and compared against traditional planners across several metrics.</p><p>The results demonstrate that the DRL-based planner achieves competitive or superior performance in terms of success rate, planning time, trajectory compactness, and configuration space length. Inference using the trained policy is not only deterministic and fast but also produces smoother and shorter joint-space paths. These characteristics make it particularly well-suited for time-sensitive industrial applications.</p><p>Nonetheless, the DRL policy exhibits limitations when it comes to handling high-precision goals, especially in the translational domain, and its applicability is currently confined to simulation environments. Although the training environment closely matches the UR3e&#8217;s kinematics and collision model, real-world deployment would introduce unmodeled dynamics, sensor noise, and compliance effects. These aspects were not tested in this study, limiting conclusions about physical robustness. Future work will focus on hardware validation and sim-to-real techniques to bridge this gap. OMPL planners, in contrast, retain advantages in adaptability and generality for new tasks and environments, albeit at a higher computational cost.</p><p>The findings suggest that DRL, when augmented with structured training techniques and expert knowledge, can serve as a high-performance alternative to traditional motion planners in deterministic and repetitive industrial contexts. Moreover, DRL demonstrates exceptional potential when combined with classical methods, effectively addressing the cold-start problem in highly complex environments or near-infinite workspaces. This synergy leverages the strengths of both paradigms, enabling robust and efficient motion planning even in challenging scenarios.</p><p>While hardware validation on the UR3e remains as future work, the use of an identical MoveIt! planning stack and conservative TOPPRA parameterization provides strong confidence that the observed simulation-based performance will translate effectively to real industrial settings.</p><p>To summarize, this work presents the first large-scale comparison of DRL (SAC) and sampling-based planners for 6-DoF industrial manipulators, leveraging over 100,000 expert trajectories, curriculum learning, and expert-demonstration bootstrapping to achieve sub-10 ms deterministic planning with &gt;94% success. Open questions remain, including the energy efficiency of DRL-generated trajectories compared to classical planners and the balance between training cost and long-term operational savings.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, I.F.A., G.V.-C., A.T. and J.-I.V.; Methodology, I.F.A., G.V.-C., A.T. and J.-I.V.; Software, I.F.A., G.V.-C., A.T. and J.-I.V.; Validation, I.F.A., G.V.-C., A.T. and J.-I.V.; Formal analysis, I.F.A., G.V.-C., A.T. and J.-I.V.; Investigation, I.F.A., G.V.-C., A.T. and J.-I.V.; Resources, I.F.A., G.V.-C., A.T. and J.-I.V.; Data curation, I.F.A., G.V.-C., A.T. and J.-I.V.; Writing&#8212;original draft, I.F.A., G.V.-C., A.T. and J.-I.V.; Writing&#8212;review &amp; editing, I.F.A., G.V.-C., A.T. and J.-I.V.; Visualization, I.F.A., G.V.-C., A.T. and J.-I.V.; Supervision, I.F.A., G.V.-C., A.T. and J.-I.V.; Project administration, I.F.A., G.V.-C., A.T. and J.-I.V.; Funding acquisition, I.F.A., G.V.-C., A.T. and J.-I.V. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Guillermo Villate-Castillo was employed by the company TECNALIA. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05282"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Larsen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kupke</surname><given-names>M.</given-names></name><name name-style="western"><surname>Schuster</surname><given-names>A.</given-names></name></person-group><article-title>Automatic Path Planning of Industrial Robots Comparing Sampling-based and Computational Intelligence Methods</article-title><source>Procedia Manuf.</source><year>2017</year><volume>11</volume><fpage>241</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1016/j.promfg.2017.07.237</pub-id></element-citation></ref><ref id="B2-sensors-25-05282"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vosniakos</surname><given-names>G.C.</given-names></name><name name-style="western"><surname>Chronopoulos</surname><given-names>A.</given-names></name></person-group><article-title>Industrial robot path planning in a constraint-based computer-aided design and kinematic analysis environment</article-title><source>Proc. Inst. Mech. Eng. Part B J. Eng. Manuf.</source><year>2009</year><volume>223</volume><fpage>523</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1243/09544054JEM1234</pub-id></element-citation></ref><ref id="B3-sensors-25-05282"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Antonelli</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chiaverini</surname><given-names>S.</given-names></name><name name-style="western"><surname>Palladino</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gerio</surname><given-names>G.P.</given-names></name><name name-style="western"><surname>Renga</surname><given-names>G.</given-names></name></person-group><article-title>Cartesian space motion planning for robots. An industrial implementation</article-title><source>Proceedings of the the Fourth International Workshop on Robot Motion and Control, RoMoCo&#8217;0</source><conf-loc>Puszczykowo, Poland</conf-loc><conf-date>20 June 2004</conf-date><fpage>279</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1109/ROMOCO.2004.240741</pub-id></element-citation></ref><ref id="B4-sensors-25-05282"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bernabe</surname><given-names>A.D.S.</given-names></name><name name-style="western"><surname>Lakshminarayanan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>K.C.H.</given-names></name><name name-style="western"><surname>Mutiargo</surname><given-names>B.</given-names></name><name name-style="western"><surname>Suppiah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>S.</given-names></name></person-group><article-title>Robots in manufacturing: Current technology trends</article-title><source>Digital Manufacturing: Key Elements of a Digital Factory</source><publisher-name>Elsevier</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2024</year><fpage>39</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/B978-0-443-13812-6.00008-7</pub-id></element-citation></ref><ref id="B5-sensors-25-05282"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ratliff</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zucker</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bagnell</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Srinivasa</surname><given-names>S.</given-names></name></person-group><article-title>CHOMP: Gradient optimization techniques for efficient motion planning</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Kobe, Japan</conf-loc><conf-date>12&#8211;17 May 2009</conf-date><fpage>489</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1109/ROBOT.2009.5152817</pub-id></element-citation></ref><ref id="B6-sensors-25-05282"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kalakrishnan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chitta</surname><given-names>S.</given-names></name><name name-style="western"><surname>Theodorou</surname><given-names>E.</given-names></name><name name-style="western"><surname>Pastor</surname><given-names>P.</given-names></name><name name-style="western"><surname>Schaal</surname><given-names>S.</given-names></name></person-group><article-title>STOMP: Stochastic trajectory optimization for motion planning</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Shanghai, China</conf-loc><conf-date>9&#8211;13 May 2011</conf-date><fpage>4569</fpage><lpage>4574</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2011.5980280</pub-id></element-citation></ref><ref id="B7-sensors-25-05282"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schulman</surname><given-names>J.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ho</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>A.</given-names></name><name name-style="western"><surname>Awwal</surname><given-names>I.</given-names></name><name name-style="western"><surname>Bradlow</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Patil</surname><given-names>S.</given-names></name><name name-style="western"><surname>Goldberg</surname><given-names>K.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name></person-group><article-title>Motion planning with sequential convex optimization and convex collision checking</article-title><source>Int. J. Robot. Res.</source><year>2014</year><volume>33</volume><fpage>1251</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1177/0278364914528132</pub-id></element-citation></ref><ref id="B8-sensors-25-05282"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>&#350;ucan</surname><given-names>I.A.</given-names></name><name name-style="western"><surname>Moll</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kavraki</surname><given-names>L.</given-names></name></person-group><article-title>The open motion planning library</article-title><source>IEEE Robot. Autom. Mag.</source><year>2012</year><volume>19</volume><fpage>72</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1109/MRA.2012.2205651</pub-id></element-citation></ref><ref id="B9-sensors-25-05282"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Coleman</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Sucan</surname><given-names>I.A.</given-names></name><name name-style="western"><surname>Chitta</surname><given-names>S.</given-names></name><name name-style="western"><surname>Correll</surname><given-names>N.</given-names></name></person-group><article-title>Reducing the Barrier to Entry of Complex Robotic Software: A MoveIt! Case Study</article-title><source>J. Softw. Eng. Robot.</source><year>2014</year><volume>5</volume><fpage>3</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.6092/JOSER_2014_05_01_P3</pub-id></element-citation></ref><ref id="B10-sensors-25-05282"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Quigley</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gerkey</surname><given-names>B.</given-names></name><name name-style="western"><surname>Conley</surname><given-names>K.</given-names></name><name name-style="western"><surname>Faust</surname><given-names>J.</given-names></name><name name-style="western"><surname>Foote</surname><given-names>T.</given-names></name><name name-style="western"><surname>Leibs</surname><given-names>J.</given-names></name><name name-style="western"><surname>Berger</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wheeler</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ng</surname><given-names>A.</given-names></name></person-group><article-title>ROS: An open-source Robot Operating System</article-title><source>Proceedings of the ICRA Workshop on Open Source Software</source><conf-loc>Kobe, Japan</conf-loc><conf-date>12&#8211;17 May 2009</conf-date></element-citation></ref><ref id="B11-sensors-25-05282"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Akgun</surname><given-names>B.</given-names></name><name name-style="western"><surname>Stilman</surname><given-names>M.</given-names></name></person-group><article-title>Sampling heuristics for optimal motion planning in high dimensions</article-title><source>Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>25&#8211;30 September 2011</conf-date><fpage>2640</fpage><lpage>2645</lpage><pub-id pub-id-type="doi">10.1109/IROS.2011.6095077</pub-id></element-citation></ref><ref id="B12-sensors-25-05282"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Elbanhawi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Simic</surname><given-names>M.</given-names></name></person-group><article-title>On the performance of sampling-based optimal motion planners</article-title><source>Proceedings of the UKSim-AMSS 7th European Modelling Symposium on Computer Modelling and Simulation, EMS 2013</source><conf-loc>Manchester, UK</conf-loc><conf-date>20&#8211;22 November 2013</conf-date><fpage>73</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1109/EMS.2013.13</pub-id></element-citation></ref><ref id="B13-sensors-25-05282"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tamizi</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>Yaghoubi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Najjaran</surname><given-names>H.</given-names></name></person-group><article-title>A review of recent trend in motion planning of industrial robots</article-title><source>Int. J. Intell. Robot. Appl.</source><year>2023</year><volume>7</volume><fpage>253</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1007/s41315-023-00274-2</pub-id></element-citation></ref><ref id="B14-sensors-25-05282"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>C.</given-names></name></person-group><article-title>A novel deep convolutional neural network algorithm for surface defect detection</article-title><source>J. Comput. Des. Eng.</source><year>2022</year><volume>9</volume><fpage>1616</fpage><lpage>1632</lpage><pub-id pub-id-type="doi">10.1093/jcde/qwac071</pub-id></element-citation></ref><ref id="B15-sensors-25-05282"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raajan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Srihari</surname><given-names>P.V.</given-names></name><name name-style="western"><surname>Satya</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Bhikkaji</surname><given-names>B.</given-names></name><name name-style="western"><surname>Pasumarthy</surname><given-names>R.</given-names></name></person-group><article-title>Real Time Path Planning of Robot using Deep Reinforcement Learning</article-title><source>IFAC-PapersOnLine</source><year>2020</year><volume>53</volume><fpage>15602</fpage><lpage>15607</lpage><pub-id pub-id-type="doi">10.1016/j.ifacol.2020.12.2494</pub-id></element-citation></ref><ref id="B16-sensors-25-05282"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>S.</given-names></name></person-group><article-title>Multi-Objective Optimal Trajectory Planning for Robotic Arms Using Deep Reinforcement Learning</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>5974</elocation-id><pub-id pub-id-type="doi">10.3390/s23135974</pub-id><pub-id pub-id-type="pmid">37447823</pub-id><pub-id pub-id-type="pmcid">PMC10346668</pub-id></element-citation></ref><ref id="B17-sensors-25-05282"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jurgenson</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tamar</surname><given-names>A.</given-names></name></person-group><article-title>Harnessing Reinforcement Learning for Neural Motion Planning</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1906.00214</pub-id><pub-id pub-id-type="arxiv">1906.00214</pub-id></element-citation></ref><ref id="B18-sensors-25-05282"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Haarnoja</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>A.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name><name name-style="western"><surname>Levine</surname><given-names>S.</given-names></name></person-group><article-title>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</article-title><source>Proceedings of the 35th International Conference on Machine Learning</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>10&#8211;15 July 2018</conf-date><volume>Volume 5</volume><fpage>2976</fpage><lpage>2989</lpage></element-citation></ref><ref id="B19-sensors-25-05282"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>LaValle</surname><given-names>S.M.</given-names></name></person-group><article-title>Rapidly-Exploring Random Trees: A New Tool for Path Planning</article-title><source>Technical Report 98-11</source><publisher-name>Department of Computer Science, Iowa State University</publisher-name><publisher-loc>Ames, IA, USA</publisher-loc><year>1998</year></element-citation></ref><ref id="B20-sensors-25-05282"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kavraki</surname><given-names>L.E.</given-names></name><name name-style="western"><surname>&#352;vestka</surname><given-names>P.</given-names></name><name name-style="western"><surname>Latombe</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Overmars</surname><given-names>M.H.</given-names></name></person-group><article-title>Probabilistic roadmaps for path planning in high-dimensional configuration spaces</article-title><source>IEEE Trans. Robot. Autom.</source><year>1996</year><volume>12</volume><fpage>566</fpage><lpage>580</lpage><pub-id pub-id-type="doi">10.1109/70.508439</pub-id></element-citation></ref><ref id="B21-sensors-25-05282"><label>21.</label><element-citation publication-type="journal"><article-title>A sampling-based tree planner for systems with complex dynamics</article-title><source>IEEE Trans. Robot.</source><year>2012</year><volume>28</volume><fpage>116</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1109/TRO.2011.2160466</pub-id></element-citation></ref><ref id="B22-sensors-25-05282"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Orthey</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chamzas</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kavraki</surname><given-names>L.E.</given-names></name></person-group><article-title>Sampling-Based Motion Planning: A Comparative Review</article-title><source>Annu. Rev. Control. Robot. Auton. Syst.</source><year>2024</year><volume>7</volume><fpage>285</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1146/annurev-control-061623-094742</pub-id></element-citation></ref><ref id="B23-sensors-25-05282"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bohlin</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kavraki</surname><given-names>L.E.</given-names></name></person-group><article-title>Path planning using Lazy PRM</article-title><source>Proceedings of the 2000 ICRA Millennium Conference, IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>24&#8211;28 April 2000</conf-date><volume>Volume 1</volume><fpage>521</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1109/ROBOT.2000.844107</pub-id></element-citation></ref><ref id="B24-sensors-25-05282"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gammell</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Srinivasa</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Barfoot</surname><given-names>T.D.</given-names></name></person-group><article-title>Informed RRT*: Optimal sampling-based path planning focused via direct sampling of an admissible ellipsoidal heuristic</article-title><source>Proceedings of the IEEE International Conference on Intelligent Robots and Systems</source><conf-loc>Chicago, IL, USA</conf-loc><conf-date>14&#8211;18 September 2014</conf-date><fpage>2997</fpage><lpage>3004</lpage><pub-id pub-id-type="doi">10.1109/IROS.2014.6942976</pub-id></element-citation></ref><ref id="B25-sensors-25-05282"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gammell</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Srinivasa</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Barfoot</surname><given-names>T.D.</given-names></name></person-group><article-title>Batch Informed Trees (BIT*): Sampling-based optimal planning via the heuristically guided search of implicit random geometric graphs</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>26&#8211;30 May 2015</conf-date><fpage>3067</fpage><lpage>3074</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2015.7139620</pub-id></element-citation></ref><ref id="B26-sensors-25-05282"><label>26.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Santos</surname><given-names>C.H.</given-names></name><name name-style="western"><surname>Robinson</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Unemyr</surname><given-names>E.</given-names></name></person-group><source>ROS-Industrial: An Open-Source Approach to Revolutionizing Industrial Automation-Webinar</source><publisher-name>ROS-Industrial</publisher-name><publisher-loc>Singapore</publisher-loc><year>2020</year></element-citation></ref><ref id="B27-sensors-25-05282"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fresnillo</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>Vasudevan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mohammed</surname><given-names>W.M.</given-names></name><name name-style="western"><surname>Lastra</surname><given-names>J.L.M.</given-names></name><name name-style="western"><surname>Garcia</surname><given-names>J.A.P.</given-names></name></person-group><article-title>Extending the motion planning framework-MoveIt with advanced manipulation functions for industrial applications</article-title><source>Robot. Comput.-Integr. Manuf.</source><year>2023</year><volume>8</volume><fpage>102559</fpage><pub-id pub-id-type="doi">10.1016/j.rcim.2023.102559</pub-id></element-citation></ref><ref id="B28-sensors-25-05282"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Martinez</surname><given-names>C.</given-names></name><name name-style="western"><surname>Barrero</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hernandez</surname><given-names>W.</given-names></name><name name-style="western"><surname>Monta&#241;o</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mondragon</surname><given-names>I.</given-names></name></person-group><article-title>Setup of the Yaskawa SDA10F Robot for Industrial Applications, Using ROS-Industrial</article-title><source>Lect. Notes Netw. Syst.</source><year>2017</year><volume>13</volume><fpage>186</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-54377-2_16</pub-id></element-citation></ref><ref id="B29-sensors-25-05282"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rahimi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Veeraraghavan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fumagalli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nicho</surname><given-names>J.</given-names></name><name name-style="western"><surname>Meyer</surname><given-names>J.</given-names></name><name name-style="western"><surname>Edwards</surname><given-names>S.</given-names></name><name name-style="western"><surname>Flannigan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Evans</surname><given-names>P.</given-names></name></person-group><article-title>An industrial robotics application with cloud computing and high-speed networking</article-title><source>Proceedings of the 2017 1st IEEE International Conference on Robotic Computing, IRC 2017</source><conf-loc>Taichung, Taiwan</conf-loc><conf-date>10&#8211;12 April 2017</conf-date><fpage>44</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1109/IRC.2017.39</pub-id></element-citation></ref><ref id="B30-sensors-25-05282"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name></person-group><article-title>Development of dual-arm mobile robot platform based on ROS</article-title><source>Cobot</source><year>2022</year><volume>1</volume><fpage>4</fpage><pub-id pub-id-type="doi">10.12688/cobot.17457.1</pub-id></element-citation></ref><ref id="B31-sensors-25-05282"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ripperger</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nicho</surname><given-names>J.</given-names></name><name name-style="western"><surname>Veeraraghavan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fumagalli</surname><given-names>A.</given-names></name></person-group><article-title>Autonomous Object Pick-and-Sort Procedure for Industrial Robotics Application</article-title><source>Int. J. Semant. Comput.</source><year>2019</year><volume>13</volume><fpage>161</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1142/S1793351X19400075</pub-id></element-citation></ref><ref id="B32-sensors-25-05282"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kuffner</surname><given-names>J.J.</given-names></name><name name-style="western"><surname>Valle</surname><given-names>S.M.L.</given-names></name></person-group><article-title>RRT-connect: An efficient approach to single-query path planning</article-title><source>Proceedings of the 2000 ICRA Millennium Conference, IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>24&#8211;28 April 2000</conf-date><volume>Volume 2</volume><fpage>995</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1109/ROBOT.2000.844730</pub-id></element-citation></ref><ref id="B33-sensors-25-05282"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Leu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tomizuka</surname><given-names>M.</given-names></name></person-group><article-title>Efficient Robot Motion Planning via Sampling and Optimization</article-title><source>Proceedings of the American Control Conference</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>25&#8211;28 May 2021</conf-date><fpage>4196</fpage><lpage>4202</lpage><pub-id pub-id-type="doi">10.23919/ACC50511.2021.9483146</pub-id></element-citation></ref><ref id="B34-sensors-25-05282"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>P.</given-names></name></person-group><article-title>Benchmarking and optimization of robot motion planning with motion planning pipeline</article-title><source>Int. J. Adv. Manuf. Technol.</source><year>2022</year><volume>118</volume><fpage>949</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1007/s00170-021-07985-5</pub-id></element-citation></ref><ref id="B35-sensors-25-05282"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>P.</given-names></name></person-group><article-title>Robot motion planning benchmarking and optimization through motion planning pipeline</article-title><source>Proceedings of the IEEE International Conference on Automation Science and Engineering</source><conf-loc>Lyon, France</conf-loc><conf-date>23&#8211;27 August 2021</conf-date><fpage>633</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1109/CASE49439.2021.9551646</pub-id></element-citation></ref><ref id="B36-sensors-25-05282"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kwon</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yoon</surname><given-names>S.E.</given-names></name></person-group><article-title>Dancing PRM*: Simultaneous Planning of Sampling and Optimization with Configuration Free Space Approximation</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Brisbane, QLD, Australia</conf-loc><conf-date>21&#8211;25 May 2018</conf-date><fpage>7071</fpage><lpage>7078</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2018.8463181</pub-id></element-citation></ref><ref id="B37-sensors-25-05282"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Long</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gennert</surname><given-names>M.A.</given-names></name></person-group><article-title>BiRRTOpt: A combined sampling and optimizing motion planner for humanoid robots</article-title><source>Proceedings of the IEEE-RAS International Conference on Humanoid Robots</source><conf-loc>Cancun, Mexico</conf-loc><conf-date>15&#8211;17 November 2016</conf-date><fpage>469</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1109/HUMANOIDS.2016.7803317</pub-id></element-citation></ref><ref id="B38-sensors-25-05282"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marcucci</surname><given-names>T.</given-names></name><name name-style="western"><surname>Petersen</surname><given-names>M.</given-names></name><name name-style="western"><surname>von Wrangel</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tedrake</surname><given-names>R.</given-names></name></person-group><article-title>Motion planning around obstacles with convex optimization</article-title><source>Sci. Robot.</source><year>2023</year><volume>8</volume><elocation-id>eadf7843</elocation-id><pub-id pub-id-type="doi">10.1126/scirobotics.adf7843</pub-id><pub-id pub-id-type="pmid">37967206</pub-id></element-citation></ref><ref id="B39-sensors-25-05282"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ichter</surname><given-names>B.</given-names></name><name name-style="western"><surname>Harrison</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pavone</surname><given-names>M.</given-names></name></person-group><article-title>Learning Sampling Distributions for Robot Motion Planning</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1709.05448</pub-id><pub-id pub-id-type="arxiv">1709.05448</pub-id></element-citation></ref><ref id="B40-sensors-25-05282"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Elbanhawi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Simic</surname><given-names>M.</given-names></name></person-group><article-title>Sampling-based robot motion planning: A review</article-title><source>IEEE Access</source><year>2014</year><volume>2</volume><fpage>56</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2014.2302442</pub-id></element-citation></ref><ref id="B41-sensors-25-05282"><label>41.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kunz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Stilman</surname><given-names>M.</given-names></name></person-group><article-title>Time-Optimal Trajectory Generation for Path Following with Bounded Acceleration and Velocity</article-title><source>Robotics: Science and Systems VIII</source><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2013</year><fpage>209</fpage><lpage>216</lpage></element-citation></ref><ref id="B42-sensors-25-05282"><label>42.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sutton</surname><given-names>R.S.</given-names></name><name name-style="western"><surname>Barto</surname><given-names>A.G.</given-names></name></person-group><source>Reinforcement Learning: An Introduction</source><edition>2nd ed.</edition><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2018</year></element-citation></ref><ref id="B43-sensors-25-05282"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lillicrap</surname><given-names>T.P.</given-names></name><name name-style="western"><surname>Hunt</surname><given-names>J.J.</given-names></name><name name-style="western"><surname>Pritzel</surname><given-names>A.</given-names></name><name name-style="western"><surname>Heess</surname><given-names>N.</given-names></name><name name-style="western"><surname>Erez</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tassa</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Silver</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wierstra</surname><given-names>D.</given-names></name></person-group><article-title>Continuous control with deep reinforcement learning</article-title><source>Proceedings of the 4th International Conference on Learning Representations, ICLR 2016-Conference Track Proceedings</source><conf-loc>San Juan, Puerto Rico</conf-loc><conf-date>2&#8211;4 May 2016</conf-date></element-citation></ref><ref id="B44-sensors-25-05282"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalashnikov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Irpan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pastor</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ibarz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Herzog</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jang</surname><given-names>E.</given-names></name><name name-style="western"><surname>Quillen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Holly</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kalakrishnan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Vanhoucke</surname><given-names>V.</given-names></name><etal/></person-group><article-title>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation</article-title><source>Proc. Mach. Learn. Res.</source><year>2018</year><volume>87</volume><fpage>651</fpage><lpage>673</lpage></element-citation></ref><ref id="B45-sensors-25-05282"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schulman</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wolski</surname><given-names>F.</given-names></name><name name-style="western"><surname>Dhariwal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name><name name-style="western"><surname>Openai</surname><given-names>O.K.</given-names></name></person-group><article-title>Proximal Policy Optimization Algorithms</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1707.06347</pub-id><pub-id pub-id-type="arxiv">1707.06347</pub-id></element-citation></ref><ref id="B46-sensors-25-05282"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fujimoto</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hoof</surname><given-names>H.V.</given-names></name><name name-style="western"><surname>Meger</surname><given-names>D.</given-names></name></person-group><article-title>Addressing Function Approximation Error in Actor-Critic Methods</article-title><source>Proceedings of the 35th International Conference on Machine Learning</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>10&#8211;15 July 2018</conf-date><volume>Volume 4</volume><fpage>2587</fpage><lpage>2601</lpage></element-citation></ref><ref id="B47-sensors-25-05282"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ha</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Song</surname><given-names>S.</given-names></name></person-group><article-title>Learning a Decentralized Multi-arm Motion Planner</article-title><source>Proc. Mach. Learn. Res.</source><year>2020</year><volume>155</volume><fpage>103</fpage><lpage>114</lpage></element-citation></ref><ref id="B48-sensors-25-05282"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Johannink</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bahl</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nair</surname><given-names>A.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Loskyll</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ojea</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Solowjow</surname><given-names>E.</given-names></name><name name-style="western"><surname>Levine</surname><given-names>S.</given-names></name></person-group><article-title>Residual reinforcement learning for robot control</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#8211;24 May 2019</conf-date><fpage>6023</fpage><lpage>6029</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2019.8794127</pub-id></element-citation></ref><ref id="B49-sensors-25-05282"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>X.B.</given-names></name><name name-style="western"><surname>Andrychowicz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zaremba</surname><given-names>W.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name></person-group><article-title>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Brisbane, QLD, Australia</conf-loc><conf-date>21&#8211;25 May 2018</conf-date><fpage>3803</fpage><lpage>3810</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2018.8460528</pub-id></element-citation></ref><ref id="B50-sensors-25-05282"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ying</surname><given-names>F.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Shan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>B.</given-names></name></person-group><article-title>Obstacle-Avoidable Robotic Motion Planning Framework Based on Deep Reinforcement Learning</article-title><source>IEEE/Asme Trans. Mechatronics</source><year>2024</year><volume>29</volume><fpage>4377</fpage><lpage>4388</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2024.3377002</pub-id></element-citation></ref><ref id="B51-sensors-25-05282"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tobin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fong</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ray</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schneider</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zaremba</surname><given-names>W.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name></person-group><article-title>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</article-title><source>Proceedings of the IEEE International Conference on Intelligent Robots and Systems</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>24&#8211;28 September 2017</conf-date><fpage>23</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1109/IROS.2017.8202133</pub-id></element-citation></ref><ref id="B52-sensors-25-05282"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping</article-title><source>Robot. Comput.-Integr. Manuf.</source><year>2022</year><volume>78</volume><fpage>102365</fpage><pub-id pub-id-type="doi">10.1016/j.rcim.2022.102365</pub-id></element-citation></ref><ref id="B53-sensors-25-05282"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gallou&#233;dec</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cazin</surname><given-names>N.</given-names></name><name name-style="western"><surname>Dellandr&#233;a</surname><given-names>E.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>panda-gym: Open-Source Goal-Conditioned Environments for Robotic Learning</article-title><source>Proceedings of the 4th Robot Learning Workshop: Self-Supervised and Lifelong Learning at NeurIPS</source><conf-loc>Virtual Conference</conf-loc><conf-date>13 December 2021</conf-date><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2106.13687" ext-link-type="uri">https://arxiv.org/abs/2106.13687</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-15">(accessed on 15 January 2024)</date-in-citation></element-citation></ref><ref id="B54-sensors-25-05282"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mandlekar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mart&#237;n-Mart&#237;n</surname><given-names>R.</given-names></name><name name-style="western"><surname>Joshi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>K.</given-names></name><name name-style="western"><surname>Maddukuri</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nasiriany</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name></person-group><article-title>robosuite: A Modular Simulation Framework and Benchmark for Robot Learning</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="doi">10.48550/arXiv.2009.12293</pub-id><pub-id pub-id-type="arxiv">2009.12293</pub-id></element-citation></ref><ref id="B55-sensors-25-05282"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>James</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Arrojo</surname><given-names>D.R.</given-names></name><name name-style="western"><surname>Davison</surname><given-names>A.J.</given-names></name></person-group><article-title>RLBench: The Robot Learning Benchmark &amp; Learning Environment</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1909.12271</pub-id><pub-id pub-id-type="arxiv">1909.12271</pub-id></element-citation></ref><ref id="B56-sensors-25-05282"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Abbatematteo</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chandra</surname><given-names>R.</given-names></name><name name-style="western"><surname>Mart&#237;n-Mart&#237;n</surname><given-names>R.</given-names></name><name name-style="western"><surname>Stone</surname><given-names>P.</given-names></name></person-group><article-title>Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2408.03539</pub-id><pub-id pub-id-type="arxiv">2408.03539</pub-id></element-citation></ref><ref id="B57-sensors-25-05282"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Event-triggered reconfigurable reinforcement learning motion-planning approach for mobile robot in unknown dynamic environments</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>123</volume><fpage>106197</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.106197</pub-id></element-citation></ref><ref id="B58-sensors-25-05282"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Di</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name></person-group><article-title>Robotic Arm Trajectory Planning in Dynamic Environments Based on Self-Optimizing Replay Mechanism</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>4681</elocation-id><pub-id pub-id-type="doi">10.3390/s25154681</pub-id><pub-id pub-id-type="pmid">40807846</pub-id><pub-id pub-id-type="pmcid">PMC12349172</pub-id></element-citation></ref><ref id="B59-sensors-25-05282"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>C.</given-names></name></person-group><article-title>Adaptive critic design for safety-optimal FTC of unknown nonlinear systems with asymmetric constrained-input</article-title><source>ISA Trans.</source><year>2024</year><volume>155</volume><fpage>309</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1016/j.isatra.2024.09.018</pub-id><pub-id pub-id-type="pmid">39306561</pub-id></element-citation></ref><ref id="B60-sensors-25-05282"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>Event-triggered H control for unknown constrained nonlinear systems with application to robot arm</article-title><source>Appl. Math. Model.</source><year>2025</year><volume>144</volume><fpage>116089</fpage><pub-id pub-id-type="doi">10.1016/j.apm.2025.116089</pub-id></element-citation></ref><ref id="B61-sensors-25-05282"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chamzas</surname><given-names>C.</given-names></name><name name-style="western"><surname>Quintero-Pe&#241;a</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kingston</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Orthey</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rakita</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gleicher</surname><given-names>M.</given-names></name><name name-style="western"><surname>Toussaint</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kavraki</surname><given-names>L.E.</given-names></name></person-group><article-title>MotionBenchMaker: A Tool to Generate and Benchmark Motion Planning Datasets</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>7</volume><fpage>882</fpage><lpage>889</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3133603</pub-id></element-citation></ref><ref id="B62-sensors-25-05282"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moll</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sucan</surname><given-names>I.A.</given-names></name><name name-style="western"><surname>Kavraki</surname><given-names>L.E.</given-names></name></person-group><article-title>Benchmarking Motion Planning Algorithms: An Extensible Infrastructure for Analysis and Visualization</article-title><source>IEEE Robot. Autom. Mag.</source><year>2015</year><volume>22</volume><fpage>96</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1109/MRA.2015.2448276</pub-id></element-citation></ref><ref id="B63-sensors-25-05282"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kroemer</surname><given-names>O.</given-names></name><name name-style="western"><surname>Niekum</surname><given-names>S.</given-names></name><name name-style="western"><surname>Konidaris</surname><given-names>G.</given-names></name></person-group><article-title>A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms</article-title><source>J. Mach. Learn. Res.</source><year>2019</year><volume>22</volume><fpage>1395</fpage><lpage>1476</lpage></element-citation></ref><ref id="B64-sensors-25-05282"><label>64.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Diankov</surname><given-names>R.</given-names></name></person-group><article-title>Automated Construction of Robotic Manipulation Programs</article-title><source>Ph.D. Thesis</source><publisher-name>Carnegie Mellon University, Robotics Institute</publisher-name><publisher-loc>Pittsburgh, PA, USA</publisher-loc><year>2010</year></element-citation></ref><ref id="B65-sensors-25-05282"><label>65.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Louradour</surname><given-names>J.</given-names></name><name name-style="western"><surname>Collobert</surname><given-names>R.</given-names></name><name name-style="western"><surname>Weston</surname><given-names>J.</given-names></name></person-group><article-title>Curriculum learning</article-title><source>Proceedings of the 26th ICML &#8217;09 Annual International Conference on Machine Learning</source><conf-loc>New York, NY, USA</conf-loc><conf-date>14&#8211;18 June 2009</conf-date><fpage>41</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1145/1553374.1553380</pub-id></element-citation></ref><ref id="B66-sensors-25-05282"><label>66.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Coumans</surname><given-names>E.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name></person-group><article-title>PyBullet, a Python Module for Physics Simulation for Games, Robotics and Machine Learning. 2016&#8211;2021</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://pybullet.org" ext-link-type="uri">http://pybullet.org</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-15">(accessed on 15 January 2024)</date-in-citation></element-citation></ref><ref id="B67-sensors-25-05282"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andrychowicz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wolski</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ray</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schneider</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fong</surname><given-names>R.</given-names></name><name name-style="western"><surname>Welinder</surname><given-names>P.</given-names></name><name name-style="western"><surname>McGrew</surname><given-names>B.</given-names></name><name name-style="western"><surname>Tobin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zaremba</surname><given-names>W.</given-names></name></person-group><article-title>Hindsight Experience Replay</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1707.01495</pub-id><pub-id pub-id-type="arxiv">1707.01495</pub-id></element-citation></ref><ref id="B68-sensors-25-05282"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>OpenAI</collab><name name-style="western"><surname>Andrychowicz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Baker</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chociej</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jozefowicz</surname><given-names>R.</given-names></name><name name-style="western"><surname>McGrew</surname><given-names>B.</given-names></name><name name-style="western"><surname>Pachocki</surname><given-names>J.</given-names></name><name name-style="western"><surname>Petron</surname><given-names>A.</given-names></name><name name-style="western"><surname>Plappert</surname><given-names>M.</given-names></name><name name-style="western"><surname>Powell</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Learning Dexterous In-Hand Manipulation</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1808.00177</pub-id><pub-id pub-id-type="doi">10.1177/0278364919887447</pub-id></element-citation></ref><ref id="B69-sensors-25-05282"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raffin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hill</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gleave</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kanervisto</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ernestus</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dormann</surname><given-names>N.</given-names></name></person-group><article-title>Stable-Baselines3: Reliable Reinforcement Learning Implementations</article-title><source>J. Mach. Learn. Res.</source><year>2021</year><volume>22</volume><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B70-sensors-25-05282"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Towers</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kwiatkowski</surname><given-names>A.</given-names></name><name name-style="western"><surname>Terry</surname><given-names>J.</given-names></name><name name-style="western"><surname>Balis</surname><given-names>J.U.</given-names></name><name name-style="western"><surname>Cola</surname><given-names>G.D.</given-names></name><name name-style="western"><surname>Deleu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Goul&#227;o</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kallinteris</surname><given-names>A.</given-names></name><name name-style="western"><surname>Krimmel</surname><given-names>M.</given-names></name><name name-style="western"><surname>KG</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Gymnasium: A Standard Interface for Reinforcement Learning Environments</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2407.17032</pub-id><pub-id pub-id-type="arxiv">2407.17032</pub-id></element-citation></ref><ref id="B71-sensors-25-05282"><label>71.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Akiba</surname><given-names>T.</given-names></name><name name-style="western"><surname>Sano</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yanase</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ohta</surname><given-names>T.</given-names></name><name name-style="western"><surname>Koyama</surname><given-names>M.</given-names></name></person-group><article-title>Optuna: A Next-generation Hyperparameter Optimization Framework</article-title><source>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>4&#8211;8 August 2019</conf-date></element-citation></ref><ref id="B72-sensors-25-05282"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pham</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pham</surname><given-names>Q.C.</given-names></name></person-group><article-title>A New Approach to Time-Optimal Path Parameterization Based on Reachability Analysis</article-title><source>IEEE Trans. Robot.</source><year>2018</year><volume>34</volume><fpage>645</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1109/TRO.2018.2819195</pub-id></element-citation></ref><ref id="B73-sensors-25-05282"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Finn</surname><given-names>C.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name><name name-style="western"><surname>Levine</surname><given-names>S.</given-names></name></person-group><article-title>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1703.03400</pub-id><pub-id pub-id-type="arxiv">1703.03400</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05282-f001" orientation="portrait"><label>Figure 1</label><caption><p>Workspace distribution of uniformly sampled goal positions for the robot. The figure presents three different views (Top, Front, and Side) to illustrate the uniform coverage of the reachable space.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g001.jpg"/></fig><fig position="float" id="sensors-25-05282-f002" orientation="portrait"><label>Figure 2</label><caption><p>Joint configuration distribution for the initially sampled poses for the UR3e robot before filtering. The sampled space contains unreachable regions (orange) and collision constraints (red).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g002.jpg"/></fig><fig position="float" id="sensors-25-05282-f004" orientation="portrait"><label>Figure 4</label><caption><p>Distribution of sampled start and goal pose distances in the workspace.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g004.jpg"/></fig><fig position="float" id="sensors-25-05282-f005" orientation="portrait"><label>Figure 5</label><caption><p>Training performance of the DRL policy success rate over the initial training steps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g005.jpg"/></fig><fig position="float" id="sensors-25-05282-f006" orientation="portrait"><label>Figure 6</label><caption><p>Training performance of the DRL policy, illustrating the success rate progression over the entire training process, highlighting the agent&#8217;s learning curve and adaptation to increasingly challenging goals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g006.jpg"/></fig><fig position="float" id="sensors-25-05282-f007" orientation="portrait"><label>Figure 7</label><caption><p>Training performance of the DRL policy, showing the success rate progression over training steps, highlighting the agent&#8217;s learning curve and adaptation to the task.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g007.jpg"/></fig><fig position="float" id="sensors-25-05282-f008" orientation="portrait"><label>Figure 8</label><caption><p>Curriculum-level progression for the robot end-effector, showing the gradual tightening of tolerances for both position and orientation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g008.jpg"/></fig><fig position="float" id="sensors-25-05282-f009" orientation="portrait"><label>Figure 9</label><caption><p>Average episode length of the DRL policy during training, illustrating the progression and stabilization of the agent&#8217;s performance over training steps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g009.jpg"/></fig><fig position="float" id="sensors-25-05282-f010" orientation="portrait"><label>Figure 10</label><caption><p>Box plot illustrating the planning time (in seconds) for each method. The DRL-based planner demonstrates significantly faster and more consistent inference times compared to OMPL planners. Note that only successful cases are represented, and outliers (flyers) have been omitted for clarity.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g010.jpg"/></fig><fig position="float" id="sensors-25-05282-f011" orientation="portrait"><label>Figure 11</label><caption><p>Box plot illustrating the number of waypoints per planned trajectory across different methods. DRL-generated paths are notably more compact and efficient compared to those produced by OMPL planners.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g011.jpg"/></fig><fig position="float" id="sensors-25-05282-f012" orientation="portrait"><label>Figure 12</label><caption><p>Box plot illustrating the configuration space path lengths (joint-space distances) for planned trajectories across different methods. Shorter path lengths indicate more efficient joint-space motion.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05282-g012.jpg"/></fig><table-wrap position="float" id="sensors-25-05282-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05282-t001_Table 1</object-id><label>Table 1</label><caption><p>Final SAC training hyperparameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Learning rate</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="left" valign="middle" rowspan="1" colspan="1">4096</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Replay buffer size</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm133" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Discount factor (<inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula>)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05282-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05282-t002_Table 2</object-id><label>Table 2</label><caption><p>Hardware specifications used for experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Component</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specification</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Intel Core i7-10700F @ 2.90 GHz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RAM</td><td align="left" valign="middle" rowspan="1" colspan="1">16 GiB DDR4 @ 4400 MHz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU</td><td align="left" valign="middle" rowspan="1" colspan="1">NVIDIA GeForce RTX 2060</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSD</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Samsung 870 1 TB</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05282-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05282-t003_Table 3</object-id><label>Table 3</label><caption><p>Summary of averaged performance metrics for DRL and OMPL planners. The table highlights key metrics, including planning time, waypoint count, success rate, and maximum planning time. For planning time and waypoint count, the average values along with their standard deviations are provided to reflect variability across trials. The best performance for each metric is highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Planner</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Planning Time (s)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Waypoint Number</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Success Rate (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Maximum Planning Time (s)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Smoothness</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>DRL</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.01322 &#177; 0.003</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>7.20 &#177; 4.25</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>94.12</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.023</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.2286 &#177; 0.4285</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BITSTAR</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03349 &#177; 0.190</td><td align="center" valign="middle" rowspan="1" colspan="1">18.46 &#177; 10.54</td><td align="center" valign="middle" rowspan="1" colspan="1">92.91</td><td align="center" valign="middle" rowspan="1" colspan="1">5.028</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2869 &#177; 0.5108</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PRM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03360 &#177; 0.182</td><td align="center" valign="middle" rowspan="1" colspan="1">18.61 &#177; 11.09</td><td align="center" valign="middle" rowspan="1" colspan="1">92.97</td><td align="center" valign="middle" rowspan="1" colspan="1">5.039</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2987 &#177; 0.5149</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RRTConnect</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03542 &#177; 0.203</td><td align="center" valign="middle" rowspan="1" colspan="1">18.51 &#177; 10.61</td><td align="center" valign="middle" rowspan="1" colspan="1">92.86</td><td align="center" valign="middle" rowspan="1" colspan="1">5.048</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2874 &#177; 0.4983</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PROJEST</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03665 &#177; 0.067</td><td align="center" valign="middle" rowspan="1" colspan="1">19.01 &#177; 12.07</td><td align="center" valign="middle" rowspan="1" colspan="1">92.60</td><td align="center" valign="middle" rowspan="1" colspan="1">5.016</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2941 &#177; 0.5124</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EST</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03992 &#177; 0.065</td><td align="center" valign="middle" rowspan="1" colspan="1">18.87 &#177; 11.92</td><td align="center" valign="middle" rowspan="1" colspan="1">92.86</td><td align="center" valign="middle" rowspan="1" colspan="1">5.013</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2961 &#177; 0.5141</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BIEST</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04051 &#177; 0.185</td><td align="center" valign="middle" rowspan="1" colspan="1">19.78 &#177; 12.45</td><td align="center" valign="middle" rowspan="1" colspan="1">91.36</td><td align="center" valign="middle" rowspan="1" colspan="1">5.042</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3177 &#177; 0.5372</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BITRRT</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04057 &#177; 0.278</td><td align="center" valign="middle" rowspan="1" colspan="1">19.29 &#177; 13.34</td><td align="center" valign="middle" rowspan="1" colspan="1">91.68</td><td align="center" valign="middle" rowspan="1" colspan="1">5.043</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3189 &#177; 0.5353</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RRT</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04578 &#177; 0.236</td><td align="center" valign="middle" rowspan="1" colspan="1">18.56 &#177; 10.83</td><td align="center" valign="middle" rowspan="1" colspan="1">92.92</td><td align="center" valign="middle" rowspan="1" colspan="1">5.052</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2999 &#177; 0.5251</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KPIECE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07106 &#177; 0.220</td><td align="center" valign="middle" rowspan="1" colspan="1">20.56 &#177; 13.03</td><td align="center" valign="middle" rowspan="1" colspan="1">90.59</td><td align="center" valign="middle" rowspan="1" colspan="1">5.233</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3478 &#177; 0.5485</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">STRIDE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07476 &#177; 0.176</td><td align="center" valign="middle" rowspan="1" colspan="1">20.75 &#177; 13.75</td><td align="center" valign="middle" rowspan="1" colspan="1">90.58</td><td align="center" valign="middle" rowspan="1" colspan="1">5.054</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3475 &#177; 0.5691</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PDST</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11133 &#177; 0.339</td><td align="center" valign="middle" rowspan="1" colspan="1">20.32 &#177; 12.33</td><td align="center" valign="middle" rowspan="1" colspan="1">90.49</td><td align="center" valign="middle" rowspan="1" colspan="1">5.067</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3372 &#177; 0.5289</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TRRT</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28084 &#177; 0.963</td><td align="center" valign="middle" rowspan="1" colspan="1">20.33 &#177; 12.04</td><td align="center" valign="middle" rowspan="1" colspan="1">90.62</td><td align="center" valign="middle" rowspan="1" colspan="1">5.085</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3572 &#177; 0.5793</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BKPIECE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38242 &#177; 0.265</td><td align="center" valign="middle" rowspan="1" colspan="1">18.79 &#177; 11.65</td><td align="center" valign="middle" rowspan="1" colspan="1">92.74</td><td align="center" valign="middle" rowspan="1" colspan="1">5.046</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2923 &#177; 0.5306</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SBL</td><td align="center" valign="middle" rowspan="1" colspan="1">0.52241 &#177; 0.296</td><td align="center" valign="middle" rowspan="1" colspan="1">18.85 &#177; 12.04</td><td align="center" valign="middle" rowspan="1" colspan="1">92.95</td><td align="center" valign="middle" rowspan="1" colspan="1">5.055</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3016 &#177; 0.5466</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LBKPIECE</td><td align="center" valign="middle" rowspan="1" colspan="1">1.44631 &#177; 0.534</td><td align="center" valign="middle" rowspan="1" colspan="1">20.87 &#177; 13.79</td><td align="center" valign="middle" rowspan="1" colspan="1">90.47</td><td align="center" valign="middle" rowspan="1" colspan="1">5.121</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3627 &#177; 0.6179</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LAZYPRM*</td><td align="center" valign="middle" rowspan="1" colspan="1">5.00021 &#177; 0.002</td><td align="center" valign="middle" rowspan="1" colspan="1">18.37 &#177; 11.65</td><td align="center" valign="middle" rowspan="1" colspan="1">92.46</td><td align="center" valign="middle" rowspan="1" colspan="1">5.074</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2799 &#177; 0.4776</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RRT*</td><td align="center" valign="middle" rowspan="1" colspan="1">5.003 &#177; 0.006</td><td align="center" valign="middle" rowspan="1" colspan="1">18.08 &#177; 9.38</td><td align="center" valign="middle" rowspan="1" colspan="1">92.99</td><td align="center" valign="middle" rowspan="1" colspan="1">5.008</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2713 &#177; 0.4597</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PRM*</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.00617 &#177; 0.005</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.14 &#177; 9.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.954</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.2880 &#177; 0.4979</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05282-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05282-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of training and inference costs for DRL and OMPL.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Metric</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DRL (SAC)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sampling Based</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Training time</td><td align="left" valign="middle" rowspan="1" colspan="1">22 h (one-time)</td><td align="left" valign="middle" rowspan="1" colspan="1">None</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Hardware required</td><td align="left" valign="middle" rowspan="1" colspan="1">GPU (RTX 2060)</td><td align="left" valign="middle" rowspan="1" colspan="1">CPU-only</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Per-query planning time</td><td align="left" valign="middle" rowspan="1" colspan="1">&#8764;13 ms</td><td align="left" valign="middle" rowspan="1" colspan="1">50 ms&#8211;5 s</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Success rate (test set)</td><td align="left" valign="middle" rowspan="1" colspan="1">94.1%</td><td align="left" valign="middle" rowspan="1" colspan="1">90&#8211;93%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Trajectory compactness</td><td align="left" valign="middle" rowspan="1" colspan="1">7 points avg.</td><td align="left" valign="middle" rowspan="1" colspan="1">18&#8211;20 points avg.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adaptability to new setups</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Retraining needed</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Direct use</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>