<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431139</article-id><article-id pub-id-type="pmcid-ver">PMC12431139.1</article-id><article-id pub-id-type="pmcaid">12431139</article-id><article-id pub-id-type="pmcaiid">12431139</article-id><article-id pub-id-type="doi">10.3390/s25175552</article-id><article-id pub-id-type="publisher-id">sensors-25-05552</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Pool Drowning Detection Model Based on Improved YOLO</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1384-6825</contrib-id><name name-style="western"><surname>Zhang</surname><given-names initials="W">Wenhui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05552" ref-type="aff">1</xref><xref rid="c1-sensors-25-05552" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-5056-4530</contrib-id><name name-style="western"><surname>Chen</surname><given-names initials="L">Lu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05552" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shi</surname><given-names initials="J">Jianchun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-05552" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Wong</surname><given-names initials="A">Alexander</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05552"><label>1</label>School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin 541004, China; <email>23032304041@mails.guet.edu.cn</email></aff><aff id="af2-sensors-25-05552"><label>2</label>Jiangsu Zhaoming Information Technology Co., Ltd., Nantong 213000, China; <email>13961495188@139.com</email></aff><author-notes><corresp id="c1-sensors-25-05552"><label>*</label>Correspondence: <email>zhangwh@guet.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5552</elocation-id><history><date date-type="received"><day>23</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>28</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>04</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>05</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05552.pdf"/><abstract><sec sec-type="highlights"><title>Highlights</title><p>
<bold>What are the main findings?</bold>
<list list-type="bullet"><list-item><p>The proposed YOLO11-LiB achieves a high drowning class mean average precision (DmAP50) of 94.1% while being extremely lightweight (2.02 M parameters, 4.25 MB size).</p></list-item><list-item><p>Key innovations include the LGCBlock for efficient downsampling, the C2PSAiSCSA module for enhanced spatial&#8211;channel feature attention, and the BiFF-Net for improved multi-scale feature fusion.</p></list-item></list>
</p><p>
<bold>What is the implication of the main finding?</bold>
<list list-type="bullet"><list-item><p>Addresses critical limitations in real-time drowning detection: poor edge deployment efficiency, robustness in complex water environments, and multi-scale object challenges.</p></list-item><list-item><p>Provides a high-performance, computationally efficient solution enabling practical real-time surveillance in swimming pool scenarios.</p></list-item></list>
</p></sec><sec><title>Abstract</title><p>Drowning constitutes the leading cause of injury-related fatalities among adolescents. In swimming pool environments, traditional manual surveillance exhibits limitations, while existing technologies suffer from poor adaptability of wearable devices. Vision models based on YOLO still face challenges in edge deployment efficiency, robustness in complex water conditions, and multi-scale object detection. To address these issues, we propose YOLO11-LiB, a drowning object detection model based on YOLO11n, featuring three key enhancements. First, we design the Lightweight Feature Extraction Module (LGCBlock), which integrates the Lightweight Attention Encoding Block (LAE) and effectively combines Ghost Convolution (GhostConv) with dynamic convolution (DynamicConv). This optimizes the downsampling structure and the C3k2 module in the YOLO11n backbone network, significantly reducing model parameters and computational complexity. Second, we introduce the Cross-Channel Position-aware Spatial Attention Inverted Residual with Spatial&#8211;Channel Separate Attention module (C2PSAiSCSA) into the backbone. This module embeds the Spatial&#8211;Channel Separate Attention (SCSA) mechanism within the Inverted Residual Mobile Block (iRMB) framework, enabling more comprehensive and efficient feature extraction. Finally, we redesign the neck structure as the Bidirectional Feature Fusion Network (BiFF-Net), which integrates the Bidirectional Feature Pyramid Network (BiFPN) and Frequency-Aware Feature Fusion (FreqFusion). The enhanced YOLO11-LiB model was validated against mainstream algorithms through comparative experiments, and ablation studies were conducted. Experimental results demonstrate that YOLO11-LiB achieves a drowning class mean average precision (DmAP50) of 94.1%, with merely 2.02 M parameters and a model size of 4.25 MB. This represents an effective balance between accuracy and efficiency, providing a high-performance solution for real-time drowning detection in swimming pool scenarios.</p></sec></abstract><kwd-group><kwd>drowning detection</kwd><kwd>YOLO11</kwd><kwd>lightweight</kwd><kwd>attention</kwd><kwd>feature fusion</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China Regional Project</funding-source><award-id>61966007</award-id></award-group><award-group><funding-source>Guangxi Natural Science Foundation for the surface project</funding-source><award-id>2022GXNSFAA035629</award-id></award-group><funding-statement>This work was supported in part by the National Natural Science Foundation of China Regional Project: 61966007 and Guangxi Natural Science Foundation for the surface project: 2022GXNSFAA035629.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05552"><title>1. Introduction</title><p>Drowning constitutes a major global public health crisis that demands urgent attention, with its severity being impossible to overlook. According to the World Health Organization (WHO), the number of global drowning deaths reached a staggering 300,000 in 2021&#8212;equivalent to over 34 fatalities per hour [<xref rid="B1-sensors-25-05552" ref-type="bibr">1</xref>]. Focusing on China, data from the China Youth Drowning Prevention Big Data Report 2022 [<xref rid="B2-sensors-25-05552" ref-type="bibr">2</xref>] shows that drowning accounts for 33% of all injury-related deaths among Chinese adolescents. The high incidence of drowning incidents in 2022 further emphasizes the urgency of strengthening prevention and control measures.</p><p>Statistics indicate that China currently has a total of 39,700 swimming venues, primarily categorized into outdoor pools, indoor pools, and natural swimming sites. Specifically, outdoor pools (20,600 venues, accounting for 51.95%) and indoor pools (18,200 venues, accounting for 45.68%) together make up 97.63% of the total, while natural swimming sites are far less common (only 940 venues, comprising a mere 2.37%) [<xref rid="B3-sensors-25-05552" ref-type="bibr">3</xref>].</p><p>As the most prevalent types of swimming facilities, indoor and outdoor pools pose particularly acute drowning risks. Drowning is characterized by its quiet and rapid progression, and this process is easily obscured by complex environmental factors&#8212;such as water surface glare and crowd movements&#8212;greatly undermining the effectiveness of manual surveillance [<xref rid="B4-sensors-25-05552" ref-type="bibr">4</xref>]. Conversely, the inherent physical characteristics of these venues (fixed boundaries, clear water quality, and relatively controllable lighting) provide ideal, engineering-friendly conditions for deploying automated detection systems based on AI vision or sensor technology [<xref rid="B5-sensors-25-05552" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05552" ref-type="bibr">6</xref>].</p><p>Therefore, researching drowning detection technology specifically for indoor and outdoor swimming pools&#8212;core scenarios marked by high drowning risks and strong technical feasibility&#8212;is of critical significance. It helps address blind spots in manual supervision, shorten rescue response times, and enhance the overall safety prevention and control capabilities of swimming facilities.</p><p>In recent years, drowning detection technologies have primarily developed along two technical pathways:</p><p>The first category relies on wearable sensing devices. These technologies collect physiological and behavioral data (e.g., swimmers&#8217; heart rate, blood oxygen levels, water pressure, and movement status) to enable real-time behavior monitoring and emergency response [<xref rid="B7-sensors-25-05552" ref-type="bibr">7</xref>]. However, this approach mandates that users wear the devices, which not only may impair the swimming experience but also presents practical challenges&#8212;such as inconvenience in use and poor adaptability for infants and young children&#8212;hindering its large-scale application [<xref rid="B8-sensors-25-05552" ref-type="bibr">8</xref>].</p><p>The second category adopts vision-based deep learning methods, which use cameras to capture image data and leverage object detection algorithms to analyze swimmers&#8217; behaviors [<xref rid="B9-sensors-25-05552" ref-type="bibr">9</xref>]. With the rapid iteration of object detection techniques, single-stage detection models (represented by the YOLO series) have gradually become the mainstream choice for drowning detection. This is attributed to their advantages of strong real-time performance, low deployment costs, and excellent scene generalization capabilities [<xref rid="B10-sensors-25-05552" ref-type="bibr">10</xref>]. Nevertheless, existing solutions still have room for optimization in several key dimensions:</p><p>From an engineering implementation perspective: Even lightweight models like YOLO11n impose significant burdens on the storage and computational resources of edge devices, due to their parameter scale, model size, and computational overhead. This makes them difficult to adapt to resource-constrained scenarios (e.g., pool-embedded terminals) [<xref rid="B11-sensors-25-05552" ref-type="bibr">11</xref>]. From a detection performance perspective: in complex aquatic environments, factors such as water surface glare interference and target occlusion can reduce the model&#8217;s focus on subtle drowning posture features, thereby weakening the robustness of recognition [<xref rid="B12-sensors-25-05552" ref-type="bibr">12</xref>]. From a multi-scale target adaptation perspective: drowning scenarios often involve targets of varying scales (e.g., nearby swimmers vs. distant drowning individuals). Current models&#8217; multi-scale feature fusion strategies are still inadequate, frequently leading to missed detection of small targets or insufficient feature extraction for large targets [<xref rid="B13-sensors-25-05552" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05552" ref-type="bibr">14</xref>].</p><p>To address the aforementioned challenges, this paper proposes the YOLO11-LiB model, which is designed to enhance existing object detection algorithms for improved drowning detection performance and provide a more effective solution to the identified problems. Specifically, the model achieves three key optimizations:</p><p>It reduces computational complexity through lightweight dynamic convolution and optimized downsampling operations, meeting the requirements for real-time early warning; it incorporates a residual-structured separation attention mechanism to adaptively adjust feature weights, suppress irrelevant interference, enhance sensitivity to drowning victim features, and reduce false positives and false negatives caused by complex backgrounds; and it employs advanced multi-scale feature fusion to capture detailed information of targets at different scales, mitigating the degradation of detection accuracy caused by distance variations or pose changes of drowning individuals, and thus improving detection performance in complex scenarios.</p><p>The main contributions of this study are summarized as follows:<list list-type="order"><list-item><p>Lightweight Feature Extraction Design: We propose the Lightweight Feature Extraction Module (LGCBlock), which integrates the Lightweight Attention Encoding Block (LAE) and effectively combines Ghost Convolution (GhostConv) with Dynamic Convolution (DynamicConv). This module optimizes the downsampling structure and the C3k2 module in the YOLO11n backbone network, significantly reducing model parameters and computational complexity while preserving the integrity of key features.</p></list-item><list-item><p>Enhanced Attention Mechanism for Robust Feature Extraction: We introduce the Cross-Channel Position-aware Spatial Attention Inverted Residual with Spatial-Channel Separate Attention module (C2PSAiSCSA) into the backbone network. This module embeds the Spatial&#8211;Channel Separate Attention (SCSA) mechanism within the Inverted Residual Mobile Block (iRMB) framework, enabling more comprehensive and efficient feature extraction. It enhances the perception of subtle drowning postures and effectively suppresses interference from water surface glare and crowd occlusion.</p></list-item><list-item><p>Multi-scale Feature Fusion Architecture: We redesign the neck structure as the Bidirectional Feature Fusion Network (BiFF-Net), which integrates the Bidirectional Feature Pyramid Network (BiFPN) and Frequency-Aware Feature Fusion (FreqFusion). This design optimizes the semantic fusion of multi-scale features through a bidirectional feedback mechanism of dynamic weighting and frequency-domain enhancement, significantly improving detection accuracy for multi-scale targets&#8212;particularly small drowning victims.</p></list-item><list-item><p>Comprehensive Performance Evaluation: We establish a multi-dimensional evaluation system and validate the proposed YOLO11-LiB model through comparative experiments with state-of-the-art algorithms and ablation studies. The results demonstrate that our model achieves a drowning class mean average precision (DmAP50) of 94.% with only 2.02 M parameters and a model size of 4.25 MB, effectively balancing accuracy and efficiency for real-time drowning detection in swimming pool scenarios.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05552"><title>2. Existing Studies</title><sec id="sec2dot1-sensors-25-05552"><title>2.1. Sensor-Based Drowning Detection Technologies</title><p>Early research on drowning detection primarily relied on wearable devices or environmental sensors to collect physiological and motion parameters. For example, Palaniappan et al. [<xref rid="B15-sensors-25-05552" ref-type="bibr">15</xref>] developed a wristband-type wireless sensor system: it triggers alerts by comparing the moving average of accelerometer data with preset thresholds, while simultaneously activating an airbag inflation device to enable active rescue. However, this method depends on single-dimensional motion data, making it prone to false alarms when swimmers perform intense movements.</p><p>Jebamalar et al. [<xref rid="B16-sensors-25-05552" ref-type="bibr">16</xref>] further integrated heart rate oximeters, triaxial accelerometers, and water pressure sensors with Global Positioning System (GPS) positioning technology to achieve multi-parameter fusion detection. Similarly, Gumaei et al. [<xref rid="B17-sensors-25-05552" ref-type="bibr">17</xref>] demonstrated the effectiveness of hybrid deep learning models in fusing multimodal body sensing data for complex activity recognition, which provides a valuable reference for multi-sensor drowning detection systems. Nevertheless, in scenarios involving turbid water or multipath interference, sensor signals are still vulnerable to attenuation, affecting detection reliability.</p><p>Basthikodi et al. [<xref rid="B18-sensors-25-05552" ref-type="bibr">18</xref>] designed a monitoring system based on ESP8266 and Arduino boards, which incorporates environmental parameters such as body temperature and humidity for comprehensive monitoring. However, the system&#8217;s wired connection design severely restricts its practical deployment scenarios, limiting its applicability in large-scale swimming venues.</p><p>As pointed out by Ganesamoorthy et al. [<xref rid="B19-sensors-25-05552" ref-type="bibr">19</xref>], traditional video surveillance methods based on the HSV color space experience a significant decline in detection rates when targets are not fully submerged or in turbid water environments. Although sensor-based approaches can make up for the blind spots of visual detection, they have inherent drawbacks&#8212;such as poor wearing comfort and high risk of detachment&#8212;making them particularly unsuitable for infants and young children [<xref rid="B20-sensors-25-05552" ref-type="bibr">20</xref>].</p></sec><sec id="sec2dot2-sensors-25-05552"><title>2.2. Deep Learning-Based Visual Detection Technologies</title><p>With the iterative advancement of object detection algorithms, vision-based solutions have gradually become a research focus due to their non-contact advantage. Chen et al. [<xref rid="B21-sensors-25-05552" ref-type="bibr">21</xref>] proposed a lightweight network based on forward-looking sonar images, which integrates Bottleneck Transformer and feature pyramids to address the limited imaging range of underwater optical systems. Other sensing modalities like mmWave radar have also been explored for robust perception; for instance, Kosuge et al. [<xref rid="B22-sensors-25-05552" ref-type="bibr">22</xref>] developed a real-time multiclass recognition system based on mmWave imaging radar, showcasing its potential for all-weather applications. However, the low resolution of sonar images or radar point clouds makes it difficult to distinguish between drowning postures and normal diving postures, leading to potential misjudgments.</p><p>YOLO series algorithms have been widely applied in drowning detection, thanks to their favorable balance between real-time performance and detection accuracy. Yang et al. [<xref rid="B23-sensors-25-05552" ref-type="bibr">23</xref>] enhanced the YOLOv5s model by innovatively introducing an ICA module (integrating Coordinate Attention (CA) and Sigmoid Linear Unit (SiLU) activation functions) and a BiFPN feature fusion structure. The work by Sun et al. [<xref rid="B24-sensors-25-05552" ref-type="bibr">24</xref>] on arbitrary-oriented object detection in SAR images also exemplifies the trend of designing specialized network architectures (like their BiFA module) to address challenges in specific visual domains. Their model achieved a detection accuracy of 98.1% in indoor swimming pool scenarios, but it still showed limitations in complex environments with multi-target occlusion. Jiang et al. [<xref rid="B25-sensors-25-05552" ref-type="bibr">25</xref>] proposed the Swimming-YOLO model, which dynamically adjusts the position of sampling points through deformable convolutions and embeds a deformable attention mechanism to enhance the extraction of detailed features. Although this model demonstrates better comprehensive performance than the traditional YOLOv8 on multi-scene datasets, its large parameter size poses challenges for deployment on embedded devices with limited resources.</p><p>For complex outdoor water environments, Liu et al. [<xref rid="B26-sensors-25-05552" ref-type="bibr">26</xref>] proposed the lightweight YOLOv8-REH model. They designed a C2f-RVB-ELA feature extraction module and an ELA-HSFPN fusion structure, and combined them with the Powerful-IoUv2 loss function and layer-adaptive pruning technology. This approach compressed the model size to 1.8 MB and increased the frame rate by 22.9 FPS. However, the model&#8217;s detection accuracy for small targets under backlight or wave interference conditions still needs further optimization. He et al. [<xref rid="B20-sensors-25-05552" ref-type="bibr">20</xref>] compared the performance of Faster R-CNN and YOLOv5 variants, and found that YOLOv5s achieved the best results in infant drowning detection: it reached 89% mAP at a frame rate of 75 FPS, which was significantly superior to Faster R-CNN&#8217;s 6 FPS. This confirms the significant real-time advantage of single-stage detection algorithms in drowning detection tasks.</p><p>Gao et al. [<xref rid="B27-sensors-25-05552" ref-type="bibr">27</xref>] built a pose analysis system based on the YOLOv8-POSE model, which identifies drowning behavior by detecting abnormal changes in human key points. To better model the temporal dynamics of such postural changes, techniques from other domains like Spatio-Temporal Graph Convolutional Networks (ST-GCNs) [<xref rid="B28-sensors-25-05552" ref-type="bibr">28</xref>] could be insightful, as they are specifically designed to capture spatial and temporal dependencies in graph-structured data like human skeletons. However, the system relies on continuous pose sequences for judgment, resulting in a response delay of up to 1.2 s in sudden drowning scenarios&#8212;this delay may miss the optimal rescue window. Jebamalar et al. [<xref rid="B16-sensors-25-05552" ref-type="bibr">16</xref>] noted that current models still lack sufficient robustness in handling multi-scale targets (e.g., coexisting adults and children) and dynamic backgrounds (e.g., water flow fluctuations). Nevertheless, the iterative upgrades of YOLO series algorithms (such as the C3k2 structure and improved loss functions in YOLO11) provide a solid technical foundation for addressing these challenges.</p><p>These studies highlight three core advantages of YOLO series algorithms in drowning detection:<list list-type="bullet"><list-item><p>Real-time performance: The single-stage architecture supports high frame rates, meeting the requirement of rapid response in drowning emergencies;</p></list-item><list-item><p>Environmental adaptability: The integration of attention mechanisms and feature fusion structures optimizes performance in complex scenarios;</p></list-item><list-item><p>Lightweight potential: Compact model designs enable deployment on edge devices.</p></list-item></list></p><p>However, existing improved models based on YOLOv5/v8 still have limitations: low recall rates in multi-target occlusion scenarios, high miss rates for small targets under strong outdoor light, and insufficient modeling of the dynamic evolution process of drowning postures.</p><p>Considering these factors, this study selects YOLO11n as the base model for further enhancement. Its upgraded backbone network and detection head design have inherent potential to address the aforementioned challenges. By introducing dynamic attention mechanisms and multi-scale feature alignment modules, we aim to further improve the detection accuracy and robustness of the model in complex water environments.</p></sec></sec><sec id="sec3-sensors-25-05552"><title>3. Method</title><p>The overall architecture of YOLO11-LiB is illustrated in <xref rid="sensors-25-05552-f001" ref-type="fig">Figure 1</xref>. Based on the Ultralytics YOLO11n framework, our model introduces targeted enhancements for drowning detection in complex aquatic environments, adopting a multi-scale output structure. Key modifications include:</p><p>In the backbone, traditional downsampling is replaced with the lightweight LAE module. Combined with the GhostC3k2 block, this enables efficient feature extraction. An enhanced Inverted Residual module with Spatial&#8211;Channel Separate Attention (C2PSAiSCSA) is added at the backbone&#8217;s end to boost focus on fine-grained details of small targets.</p><p>Within the neck, feature channels from different scales are first standardized. Feature representation is then refined through FreqFusion and multiple C3k2 blocks. Contextual interaction is enhanced using the bidirectionally weighted BiFPN, followed by cross-scale downsampling via strided convolutions. The fused features are finally fed into the detection head for prediction.</p><p>This architecture significantly improves detection accuracy and inference efficiency through multi-level attention and frequency domain fusion, meeting the real-time and robustness requirements for drowning detection.</p><sec id="sec3dot1-sensors-25-05552"><title>3.1. LGCBlock</title><p>To enhance edge inference efficiency, we propose the LGCBlock, integrating a Lightweight Attention-based Extraction (LAE) module and a GhostC3k2 module (<xref rid="sensors-25-05552-f002" ref-type="fig">Figure 2</xref>), addressing computational redundancy and information loss in the original YOLO11n backbone.</p><p>To address downsampling optimization, this paper introduces the Lightweight Attention-based Extraction (LAE) module (Yu et al., 2024, LSM-YOLO [<xref rid="B29-sensors-25-05552" ref-type="bibr">29</xref>]), which achieves efficient feature compression via dual-branch cooperation (<xref rid="sensors-25-05552-f002" ref-type="fig">Figure 2</xref>). The attention branch employs 3 &#215; 3 average pooling and 1 &#215; 1 convolution to generate spatial weights for 2 &#215; 2 regions. The downsampling branch uses grouped convolution for 2&#215; reduction. Features are reshaped to <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, fused via softmax-normalized weights and weighted summation, reducing computation to 12.5% of standard convolution while preserving critical information.</p><p>To reduce C3k2&#8217;s complexity, we design a GhostModule using dynamic convolution. Its primary path applies 1 &#215; 1 DynamicConv (with a routing network combining four experts) for channel compression. The secondary path uses 3 &#215; 3 depthwise separable dynamic convolution. Given output channels <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>out</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the primary branch produces <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="&#x2308;" close="&#x2309;"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mi>r</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> channels (<inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), and the secondary branch <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, halving computation. This extends to a GhostBottleneck with channel expansion, depthwise convolution (for stride &gt; 1), and channel compression, integrated with adaptive shortcuts and DropPath.</p><p>Integrated into the backbone, LAE replaces downsampling layers, while GhostC3k2 modifies C3k2 by dynamically using GhostBottlenecks (c3k = False) or retaining C3k (c3k = True), preserving residual connections.This synergy significantly reduces the computational load of the LGCBlock while maintaining accuracy, enabling efficient edge deployment for drowning detection.</p></sec><sec id="sec3dot2-sensors-25-05552"><title>3.2. C2PSAiSCSA</title><p>The original C2PSA module in YOLO11n lacks effective spatial modeling and dynamic adaptability, limiting its perception of subtle drowning movements in complex aquatic environments. To address this, we design the C2PSAiSCSA module by integrating an inverted Residual Mobile Block (iRMB) for local context and a Spatial&#8211;Channel Separate Attention (SCSA) mechanism for long-range dependencies.</p><p>As illustrated in <xref rid="sensors-25-05552-f003" ref-type="fig">Figure 3</xref>, the core innovation is the iSCSA unit, which embeds SCSA within iRMB&#8217;s residual framework. Input features are first normalized and processed by SCSA to generate spatial-channel attention maps. Depthwise convolution then models local context and performs channel recalibration, followed by a projection layer and residual connection.</p><p>This iSCSA unit forms the PSABlock, which replaces the attention branch in the original C2PSA. The overall structure retains the dual-path design: one path preserves features via a skip connection, while the other undergoes deep enhancement through stacked PSABlocks. Each PSABlock contains an iSCSA module and a Feedforward Network (FFN). The iSCSA module employs an inverted residual structure: it expands channels, performs spatial modeling with depthwise separable convolution (enhanced by the integrated SCSA), and then compresses channels. The embedded SCSA uses multi-scale depthwise convolution for spatial attention and multi-head attention for channel selection, guiding the model to focus on critical regions.</p><p>Finally, features from the enhanced path are concatenated with the skip-connected features and integrated via a <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution. This design significantly improves spatial perception and feature selectivity for small drowning targets under lightweight constraints, enhancing robustness and accuracy in complex backgrounds without substantially increasing computational cost.</p></sec><sec id="sec3dot3-sensors-25-05552"><title>3.3. BiFF-Net</title><p>The original YOLO11n neck structure (FPN + PAN) employs a static fusion mechanism, struggling to balance semantic and detail features for small, low-contrast drowning targets amidst complex aquatic interference. To overcome this, we propose the Bidirectional Feature Fusion Network (BiFF-Net), integrating BiFPN and FreqFusion for dynamic, robust multi-scale fusion (<xref rid="sensors-25-05552-f004" ref-type="fig">Figure 4</xref>).</p><p>The BiFPN module introduces learnable weights, normalized via a Swish activation, to dynamically modulate the contribution of input features from different scales. This adaptive weighting enhances critical features (e.g., target edges) while suppressing redundancies (e.g., water ripple noise), significantly improving fusion flexibility and robustness.</p><p>The FreqFusion module operates in the frequency domain to compensate for detail loss. It generates content-aware adaptive low-pass (ALPF) and high-pass (AHPF) filters. The low-pass branch upsamples and smooths deeper features using CARAFE with ALPF to emphasize semantic contours. The high-pass branch applies AHPF to shallower features to accentuate high-frequency details (e.g., fine edges, textures). The outputs are summed, producing features rich in both global semantics and local details.</p><p>BiFF-Net establishes a synergistic &#8220;Frequency Enhancement &#8594; Dynamic Weighting&#8221; workflow. Multi-scale features are first enhanced by FreqFusion to repair degradation (e.g., blurred contours). BiFPN then dynamically fuses these enhanced features. This mechanism enables dynamic cross-level interaction, effectively suppressing background interference and compensating for detail loss, thereby significantly improving perception and localization accuracy for challenging drowning targets while maintaining efficient inference.</p></sec></sec><sec id="sec4-sensors-25-05552"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05552"><title>4.1. Dataset</title><p>This study constructs a dedicated drowning detection dataset covering both indoor and outdoor swimming pool environments, comprising a total of 2000 original images. Through extensive data augmentation techniques (including rotation, scaling, brightness adjustment, saturation variation, hue shift, zoom, blur, noise injection, and mosaic augmentation), the dataset was expanded to a total of 4800 images for model training. Data was sourced from the public dataset platform Roboflow (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://roboflow.com/">https://roboflow.com/</uri>) and open internet resources (including videos and images), encompassing both above-water and underwater perspectives.</p><p>To maximize diversity and real-world applicability, the dataset was meticulously curated across multiple dimensions:<list list-type="bullet"><list-item><p>Scenario Diversity: Includes indoor heated pools and outdoor open-air pools, with different lane designs and poolside environments.</p></list-item><list-item><p>Environmental Diversity: Covers various lighting conditions (e.g., bright midday sunlight, low evening light, artificial nighttime lighting, and light refraction underwater), different weather conditions (sunny, cloudy), and varying water clarity.</p></list-item><list-item><p>Subject Diversity: Encompasses individuals of different age groups (children, adults, the elderly), body types, skin tones, and those wearing different swimwear (e.g., dark, light, bikinis, swimsuits).</p></list-item><list-item><p>Behavioral Diversity: Data includes not only swimming and drowning subjects but also rich background activities such as walking poolside, diving, and playing in the water, enhancing the model&#8217;s ability to discern targets in complex scenes.</p></list-item><list-item><p>Technical Diversity: Data was obtained from various capture devices (e.g., surveillance cameras, action cameras, smartphones), introducing variations in resolution, viewing angles (overhead, eye-level, oblique), and aspect ratios.</p></list-item></list></p><p>The dataset was partitioned into training, validation, and test sets in a 7:2:1 ratio. All images were meticulously annotated in YOLO format using Roboflow tools. The annotation categories are defined as follows:<list list-type="bullet"><list-item><p>Swimming: Represents normal and controlled swimming postures. Characteristics typically include a streamlined, near-horizontal body position; rhythmic and coordinated limb movements (e.g., regular arm strokes and leg kicks); the head is often raised for breathing or submerged rhythmically; and movement direction is purposeful.</p></list-item><list-item><p>Drowning: Represents active or imminent drowning behavior, characterized fundamentally by a loss of voluntary motor control. Specific manifestations include a vertical or upright, tilted body position with an inability to swim effectively; arms may extend laterally or slap the water involuntarily (Instinctive Drowning Response) without providing propulsion; the head may be submerged for extended periods or tilted back with the mouth seeking air, often with a glassy-eyed stare; leg movement is minimal or absent, often leading to a gradual submersion after struggle.</p></list-item></list></p><p>To mitigate overfitting risks due to the extended training of 500 epochs, the aforementioned data augmentation strategies were rigorously applied. Furthermore, the training process was closely monitored. The loss curves for both training and validation sets demonstrated a convergent trend without significant divergence, indicating that overfitting was effectively controlled. The loss curves were analyzed in detail in the subsequent chapters.</p><p><xref rid="sensors-25-05552-f005" ref-type="fig">Figure 5</xref> showcases representative samples from the dataset, providing a visual comparison of the two behavioral modes. This dataset has been publicly released on the Roboflow platform.</p></sec><sec id="sec4dot2-sensors-25-05552"><title>4.2. Evaluation Metrics</title><p>This study employs multiple evaluation metrics to comprehensively assess model performance, including Drowning Precision (DP), Drowning Recall (DR), Drowning mean Average Precision (DmAP50), Swimming mean Average Precision (SmAP50), Frame Rate (FPS), Floating Point Operations (FLOPs, in G), Number of Parameters (P/M, in millions), and model size (Size/M, in MB).</p><p>Specifically, Drowning Precision measures the accuracy of predictions for the drowning class, representing the proportion of true drowning instances among samples predicted as drowning. Drowning Recall quantifies the detection capability for drowning instances, indicating the proportion of actual drowning cases correctly identified by the model.</p><p>DmAP50 and SmAP50 denote the mean average precision for drowning and swimming classes respectively at an Intersection over Union (IoU) threshold of 0.5, reflecting the comprehensive performance of detection precision and recall capability for both target categories. The Frame Rate (FPS) evaluates the model&#8217;s real-time inference performance, demonstrating its capability for instantaneous detection.</p><p>FLOPs represent the computational complexity of the model, while the Number of Parameters and model size indicate the model&#8217;s scale and storage requirements, respectively, assessing its deployment feasibility on resource-constrained devices. Collectively, these metrics comprehensively characterize the proposed model&#8217;s overall performance across two dimensions: detection effectiveness and model efficiency.</p></sec><sec id="sec4dot3-sensors-25-05552"><title>4.3. Experimental Setup and Convergence Analysis</title><p>All experiments in this study were conducted within the PyCharm 2024.3.3 (Professional Edition) integrated development environment. The hardware configuration includes an NVIDIA GeForce RTX 3090 GPU (manufactured by NVIDIA Corporation, Santa Clara, CA, USA) and an Intel(R) Xeon(R) Platinum 8362 CPU (manufactured by Intel Corporation, Santa Clara, CA, USA) (15 cores, 2.80 GHz base frequency), running on the Windows 11 operating system. The software environment utilizes Python 3.10, with PyTorch 2.0.0 as the deep learning framework, supported by CUDA 11.8 acceleration.</p><p>Experimental hyperparameters were carefully tuned for the drowning detection task, with key parameters detailed in <xref rid="sensors-25-05552-t001" ref-type="table">Table 1</xref>. The configuration follows established practices for YOLO-series models while incorporating adjustments specific to our application scenario. This setup enabled both our proposed model and the baseline models to achieve optimal performance.</p><p>The training process and convergence behavior were meticulously analyzed. Extensive data augmentation was employed to mitigate overfitting risks associated with the limited dataset size. <xref rid="sensors-25-05552-f006" ref-type="fig">Figure 6</xref> presents the trajectories of the training and validation losses alongside key performance metrics over the 300-epoch training course.</p><p>Analysis of the curves reveals a rapid decline in both training and validation losses during the initial 50 epochs, indicating efficient feature learning. The rate of decrease gradually slowed, with losses stabilizing after approximately 200 epochs, suggesting model convergence. Crucially, the validation loss closely tracked the training loss throughout the entire process without significant divergence, demonstrating strong generalization and a lack of severe overfitting. This is further supported by the performance metrics on the validation set. The mAP@50 exhibited a swift increase from near zero, surpassed 0.8 around epoch 100, and eventually plateaued at a high value of 0.941 by epoch 300.</p><p>The learning rate scheduling strategy contributed to this stable convergence. The learning rate was gradually reduced from its initial value of 0.01 to a final value of 0.000133, allowing for precise parameter tuning in the later stages of training without introducing instability.</p><p>In conclusion, the combination of a carefully chosen experimental setup, extensive data augmentation, and a tailored training regimen resulted in a stable and effective learning process. The model converged reliably over 300 epochs, achieving high performance on the validation set without exhibiting signs of overfitting, thereby validating the chosen experimental parameters and overall approach.</p></sec><sec id="sec4dot4-sensors-25-05552"><title>4.4. Comparative Experiments</title><p>To validate the effectiveness of the proposed YOLO11n-based improved model, this study selected multiple mainstream YOLO series versions, advanced object detection models, and representative non-YOLO series models for comparative experiments under identical datasets and experimental conditions. The results are presented in <xref rid="sensors-25-05552-t002" ref-type="table">Table 2</xref>. To ensure the robustness of the conclusions, all metrics are reported as the mean with a 95% confidence interval (CI) from five independent runs (<italic toggle="yes">n</italic> = 5), and statistical significance was assessed using paired <italic toggle="yes">t</italic>-tests (<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.05).</p><p>The improved model demonstrates outstanding performance in drowning detection: Drowning Precision (DP) reaches 88.4% (88.4 &#177; 0.4%), and Drowning Recall (DR) reaches 89.7% (89.7 &#177; 0.5%), significantly outperforming the lightweight models YOLO11n (DP: 85.7 &#177; 0.5%, DR: 88.8 &#177; 0.6%) and YOLO12n (DP: 80.8 &#177; 0.8%, DR: 83.3 &#177; 0.7%) (<italic toggle="yes">p</italic> &lt; 0.05) while matching the performance of the medium-sized models YOLO11s and YOLO11m, and surpassing traditional one-stage detectors like RetinaNet (DP: 84.9 &#177; 0.6%, <italic toggle="yes">p</italic> &lt; 0.05). The drowning mean Average Precision (DmAP50) was 94.1% (94.1 &#177; 0.3%), which is statistically superior to most comparison models, including YOLO11n (91.8 &#177; 0.4%, <italic toggle="yes">p</italic> &lt; 0.01), YOLOv5 (92.9 &#177; 0.3%, <italic toggle="yes">p</italic> &lt; 0.05), and Faster R-CNN (92.8 &#177; 0.5%, <italic toggle="yes">p</italic> &lt; 0.05). It demonstrates highly competitive performance against the state-of-the-art ViTDet (94.5 &#177; 0.6%), with the difference being not statistically significant (<italic toggle="yes">p</italic> &gt; 0.05), which significantly enhances the detection accuracy for critical targets. For swimming detection, the improved model achieves 85.6% (85.6 &#177; 0.4%) SmAP50, outperforming most baseline models (e.g., vs. YOLO11n: 82.4 &#177; 0.5%, <italic toggle="yes">p</italic> &lt; 0.01) and demonstrating robust multi-class recognition capability.</p><p>Although the inference speed (FPS) of the improved model is approximately 80.5 (80.5 &#177; 2.5) frames/second, lower than some models like YOLO11m and YOLO11l, it still meets the real-time requirements for drowning detection scenarios. In terms of model efficiency, the computational complexity (FLOPs) is only 6.2 G (6.2 &#177; 0.1 G), with approximately 2.02 million parameters and a model size of just 4.25MB, significantly lower than RTDERT-I, large-scale YOLO11 models, Faster R-CNN, ViTDet, etc., facilitating deployment in resource-constrained environments.</p><p>Minor declines in certain metrics (such as inference speed and detection accuracy for specific classes) primarily stem from the computational overhead introduced by enhanced feature representation and multi-scale fusion capabilities. Specifically, while multi-layer attention modules, frequency domain feature fusion, and weighted feature pyramid structures significantly enhance recognition of drowning targets in complex scenes, they also introduce additional computational burden, resulting in slightly reduced inference efficiency. Simultaneously, the deepened backbone network and detection head structures, designed to improve detection accuracy, further increase model complexity, leading to trade-offs in some lightweight metrics. Overall, this balance reflects a reasonable compromise between accuracy and efficiency, better aligning with the high precision requirements of practical drowning detection applications.</p><p>In summary, the proposed improved model effectively reduces model complexity and storage requirements while maintaining high detection precision and recall, achieving an optimal balance between detection performance and lightweight design. The statistical analysis confirms that the performance improvements are robust and significant, demonstrating broad applicability and promotion value for practical drowning detection applications.</p></sec><sec id="sec4dot5-sensors-25-05552"><title>4.5. Ablation Study</title><p>To systematically evaluate the impact of each proposed module on the drowning detection performance of YOLO11n, this paper conducts ablation studies: the lightweight module LGCBlock, attention-enhanced module C2PSAiSCSA, and feature fusion module BiFF-Net are incrementally integrated into the baseline model to investigate their individual contributions to detection performance and operational efficiency. The results, presented in <xref rid="sensors-25-05552-t003" ref-type="table">Table 3</xref>, are reported as mean &#177;95% CI from five independent runs, with statistical significance (paired <italic toggle="yes">t</italic>-test, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.05) calculated against the baseline (YOLO11n).</p><p>First, after introducing the lightweight downsampling feature extraction module (denoted as A), which integrates the attention-enhanced properties of LAE and the efficient convolution design of GhostC3k2, the model&#8217;s computational complexity and parameter count are significantly reduced (FLOPs: 5.9 &#177; 0.1 G vs. 6.3 &#177; 0.1 G; Params: 2.21 M vs. 2.58 M). This module effectively decreases FLOPs and model size, enhancing lightweight performance. However, due to a trade-off in feature representation capability, a slight decline in precision and recall for the drowning category is observed (DmAP50: 92.0 &#177; 0.4% vs. 91.8 &#177; 0.4%, <italic toggle="yes">p</italic> &gt; 0.05), illustrating the inherent balance between lightweight design and detection performance.</p><p>Second, when independently incorporating the inverted residual separated attention module C2PSAiSCSA (denoted as B), its spatial&#8211;channel selective attention mechanism strengthens the model&#8217;s ability to capture critical features within drowning target regions. This results in a significant improvement in drowning category detection precision and mAP (DmAP50: 93.0 &#177; 0.3% vs. 91.8 &#177; 0.4%, <italic toggle="yes">p</italic> &lt; 0.01). Simultaneously, while maintaining low computational complexity, this module effectively enhances the model&#8217;s capability to recognize small targets against complex backgrounds, achieving substantial performance gains.</p><p>Third, after independently integrating BiFF-Net (denoted as C), the optimized weighted fusion process for multi-scale features enriches the interaction and propagation of semantic information. This strategy not only further boosts detection recall (DR: 90.1 &#177; 0.6%) but also plays a crucial role in reducing computational resource consumption and model size, thereby enhancing the model&#8217;s practicality and deployment efficiency.</p><p>Finally, modules are progressively combined: The YOLO11n+A+B configuration introduces the attention mechanism on the lightweight foundation, achieving a balance between detection accuracy (DmAP50: 92.3 &#177; 0.4%) and model efficiency. The YOLO11n+A+B+C configuration leverages the synergistic effect of all three modules, yielding optimal performance in drowning category precision, recall, and mAP (DmAP50: 94.1 &#177; 0.3% vs. baseline 91.8 &#177; 0.4%, <italic toggle="yes">p</italic> &lt; 0.001). Although inference speed exhibits minor fluctuations, it remains within application requirements, while computational complexity and parameter size are substantially reduced.</p><p>The ablation studies comprehensively validate the synergistic effects of lightweight design, attention mechanisms, and efficient feature fusion. The statistical analysis confirms the significant contribution of each module, demonstrating the targeted contributions of each module in enhancing performance and optimizing efficiency, ultimately achieving an ideal balance between accuracy and efficiency for the drowning detection task.</p></sec><sec id="sec4dot6-sensors-25-05552"><title>4.6. Visualization Result Analysis</title><p>To intuitively demonstrate the advantages of the proposed algorithm in complex real-world scenarios, particularly its capability to address challenges such as water surface glare, crowd occlusion, and small object detection, this section provides a detailed qualitative analysis, as shown in <xref rid="sensors-25-05552-f007" ref-type="fig">Figure 7</xref>.</p><p>As shown in the second example column, under interference from complex bright backgrounds caused by strong water surface glare, the baseline model YOLO11n was able to detect the drowning person but with relatively low confidence. In contrast, our model&#8212;leveraging the powerful spatial&#8211;channel attention mechanism of the C2PSAiSCSA module&#8212;effectively suppresses glare-induced background noise and focuses on authentic target features. This results in more complete and accurate detections without false positives.</p><p>As illustrated in the third and fourth example columns, in scenarios with dense crowds and splash occlusion between targets, YOLO11n exhibits low confidence in detecting partially occluded targets due to its limited receptive field and insufficient feature extraction capability. In comparison, the C2PSAiSCSA module in our model enhances the ability to focus on key parts of the targets (such as heads and arms), effectively penetrating visual occlusions. It successfully localizes and identifies occluded drowning targets, significantly improving the model&#8217;s robustness.</p><p>As demonstrated in the fifth example column, for distant small-scale targets (with bounding boxes occupying less than 32 &#215; 32 pixels), the detection confidence of all comparative models declines. Notably, YOLO11n and YOLOv8n show high missed-detection rates, while other models remain effective in identifying these targets. By bidirectionally integrating deep semantic information with shallow high-resolution features, the proposed BiFF-Net module supplies the detection head with rich, fine-grained details of small targets. As a result, our method achieves the highest and most stable detection performance for small objects, successfully validating the design objective of this module.</p><p>In summary, the visualization results strongly corroborate, from a qualitative perspective, the superior performance of the proposed model in addressing critical challenges in practical deployment, especially empowered by its core modules, which aligns with and reinforces the conclusions drawn from the previous quantitative analysis.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05552"><title>5. Discussion</title><p>This study enhances the YOLO11n model to address the specific challenges of drowning detection in complex aquatic environments. The discussion below clarifies the novelty of our contributions relative to prior YOLO-based improvements and explores the practical deployment considerations of the proposed model.</p><sec id="sec5dot1-sensors-25-05552"><title>5.1. Novelty and Adaptation Relative to Prior YOLO Improvements</title><p>Our work stands on the shoulders of extensive research that has leveraged the YOLO architecture for drowning detection, as reviewed in <xref rid="sec2-sensors-25-05552" ref-type="sec">Section 2</xref>. We consciously integrate several advanced concepts from the broader object detection domain while introducing targeted innovations to create a more effective and efficient solution.</p><p>The LGCBlock is a composite module designed for extreme lightweightness. Its LAE component is adapted from the work of Yu et al. [<xref rid="B29-sensors-25-05552" ref-type="bibr">29</xref>], repurposing their efficient downsampling technique for our context. However, its integration with our novel GhostC3k2 module, which utilizes dynamic convolution to reduce computational complexity, represents a novel architectural choice not found in standard YOLO implementations or prior drowning detection models like [<xref rid="B23-sensors-25-05552" ref-type="bibr">23</xref>,<xref rid="B26-sensors-25-05552" ref-type="bibr">26</xref>]. This synergy directly addresses the common trade-off between speed and accuracy, a core limitation in deployment-focused studies such as [<xref rid="B26-sensors-25-05552" ref-type="bibr">26</xref>].</p><p>The C2PSAiSCSA module introduces a significant innovation to enhance feature representation. While the idea of enhancing attention is prevalent (e.g., the ICA module in [<xref rid="B23-sensors-25-05552" ref-type="bibr">23</xref>]), our approach is distinct. We novelly embed a Spatial&#8211;Channel Separate Attention (SCSA) mechanism within an inverted Residual Mobile Block (iRMB) framework. This creates a deeply integrated iSCSA unit that simultaneously captures local context and long-range dependencies more effectively than prior adaptations of coordinate or channel attention. This is particularly crucial for discerning subtle drowning postures amidst noisy water surfaces, a challenge highlighted by [<xref rid="B16-sensors-25-05552" ref-type="bibr">16</xref>].</p><p>The BiFF-Net in the neck demonstrates a hybrid adaptation-and-innovation strategy. The use of BiFPN is an adaptation of a known technique for weighted feature fusion, similar to concepts in [<xref rid="B23-sensors-25-05552" ref-type="bibr">23</xref>]. The novelty lies in its combination with our proposed FreqFusion module. This frequency-domain approach to feature enhancement, using adaptive filters to repair blurred contours and accentuate details, is a novel contribution to drowning detection. It directly tackles the problem of detail loss in low-contrast targets and wave interference, which limits the performance of models like [<xref rid="B26-sensors-25-05552" ref-type="bibr">26</xref>].</p><p>In summary, our novelty does not solely lie in inventing entirely new operators but in the strategic integration and customized redesign of existing concepts into a cohesive architecture specifically optimized for the challenges of drowning detection, achieving a balance not present in prior art.</p></sec><sec id="sec5dot2-sensors-25-05552"><title>5.2. Deployment Considerations and Applicability</title><p>The design choices of YOLO11-LiB are intrinsically guided by practical deployment needs, namely handling multi-camera feeds under strict latency constraints.</p><p>Multi-Camera Support: The significant reduction in computational complexity and parameter count, primarily achieved through the LGCBlock, is the key to multi-camera deployment. A lighter model requires less computational resources per video stream, allowing a single edge computing device (e.g., an NVIDIA Jetson module) to process feeds from multiple cameras concurrently. This makes our model suitable for monitoring large pool areas or several zones within a natural water body without a linear increase in hardware cost.</p><p>Latency Constraints: The model&#8217;s architecture is engineered for high inference speed. The LAE module reduces computational overhead during downsampling, and the GhostC3k2 module streamlines feature extraction. These optimizations collectively ensure a high frames-per-second (FPS) rate, which is critical for real-time alerting. Low latency minimizes the time between a potential drowning event and its detection, a factor crucial for the effectiveness of any rescue system, as noted in [<xref rid="B27-sensors-25-05552" ref-type="bibr">27</xref>].</p><p>Edge Deployment: The model&#8217;s lightweight nature makes it a prime candidate for deployment on edge devices. This offers advantages in remote or outdoor aquatic environments where bandwidth is limited and privacy is a concern, as video data can be processed locally without being transmitted to the cloud. The reduced dependency on high-end GPUs also lowers the overall system cost and power consumption, enhancing its practicality for widespread adoption.</p><p>Therefore, YOLO11-LiB is not merely an academic exercise in accuracy improvement but a model designed with tangible deployment scenarios in mind, addressing the critical challenges of scalability, real-time performance, and cost-effectiveness.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05552"><title>6. Conclusions</title><p>This paper proposes the YOLO11-LiB model, an improved version based on YOLO11n, to address the practical requirements of drowning detection in swimming pool scenarios. Through lightweight design, attention mechanism optimization, and multi-scale feature fusion, it effectively resolves bottlenecks in existing technologies regarding edge deployment efficiency, robustness in complex environments, and multi-scale target detection.</p><p>The main research conclusions are as follows:<list list-type="bullet"><list-item><p>Lightweight design significantly enhances deployment adaptability: LGCBlock through synergistic optimization of LAE downsampling and GhostC3k2 structure, reduces downsampling computation by 87.5% and feature extraction overhead by 50% while preserving the integrity of key features. This substantially reduces model parameters and size, meeting deployment requirements in resource-constrained scenarios such as pool-embedded terminals.</p></list-item><list-item><p>Attention mechanism strengthens robustness in complex environments: C2PSAiSCSA enhances perception of subtle drowning postures by combining spatial-channel decoupled attention with local context modeling. It effectively suppresses interference from water surface glare and crowd occlusion, increasing drowning recall rate (DR) to 89.7%.</p></list-item><list-item><p>Fusion improves multi-scale detection accuracy: BiFF-Net optimizes semantic fusion of multi-scale features through its bidirectional feedback mechanism of dynamic weighting and frequency-domain enhancement. It particularly improves detection accuracy for small targets, achieving a DmAP50 of 94.1%, outperforming all comparison models.</p></list-item></list></p><p>Comprehensive performance balance validates solution effectiveness: Comparative and ablation experiments demonstrate that YOLO11-LiB achieves a DmAP50 of 94.1%, representing a 2.3% improvement over the baseline YOLO11n, while reducing model size by 19.2%. This achieves an ideal &#8220;accuracy&#8211;efficiency&#8221; balance, providing reliable technical support for real-time drowning detection.</p><p>Current research on aquatic object detection utilizing YOLO architectures demonstrates notable limitations: The visual datasets commonly employed frequently lack comprehensive coverage of complex, variable real-world aquatic environments, including adverse weather, turbid water, and low-light conditions, while exhibiting inadequate robustness under extreme optical circumstances. More critically, as single-frame detectors, YOLO-based systems fundamentally cannot capture temporal features inherent in continuous dynamic processes such as drowning, substantially elevating risks of false alarms and missed detections.</p><p>Future work must urgently prioritize constructing diversified datasets encompassing more complex and extreme scenarios, alongside exploring integration of multimodal sensory information to enhance perceptual reliability in challenging environments. Concurrently, transcending the single-frame detection paradigm through incorporation of temporal modeling or object tracking techniques is essential to analyze targets&#8217; continuous motion states and behavioral patterns. This paradigm shift will enable more precise identification of progressive events like drowning. Collectively, these advancements will significantly improve system practicality and generalization capability, facilitating deployment of aquatic safety monitoring technologies across broader, more demanding real-world applications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.Z. and L.C.; methodology, W.Z. and L.C.; software, L.C.; validation, J.S.; formal analysis, L.C. and J.S.; investigation, L.C.; resources, W.Z. and J.S.; data curation, L.C. and J.S.; writing&#8212;original draft preparation, L.C.; writing&#8212;review and editing, W.Z.; visualization, L.C.; supervision, W.Z. and J.S.; project administration, W.Z. and J.S.; funding acquisition, W.Z. and J.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Ethical review and approval were waived for this study due to the fact that the research was based solely on publicly available data and video materials without involving human subjects or animals directly in the experiment.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data and codes presented in this study are publicly available on GitHub: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/Mibugi/Drowning-detection">https://github.com/Mibugi/Drowning-detection</uri> (accessed on 3 September 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Jianchun Shi was employed by Jiangsu Zhaoming Information Technology Co., Ltd. The remaining authors, Wenhui Zhang and Lu Chen, declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05552"><label>1.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>World Health Organization</collab></person-group><article-title>World Health Organization. Drowning: Key Facts</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.who.int/news-room/fact-sheets/detail/drowning" ext-link-type="uri">https://www.who.int/news-room/fact-sheets/detail/drowning</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-25">(accessed on 25 March 2025)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-05552"><label>2.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>People&#8217;s Daily Public Opinion Data Center</collab></person-group><article-title>2022 Big Data Report on Drowning Prevention Among Chinese Adolescents. People&#8217;s Daily Online</article-title><year>2022</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://fcy.618cloud.com.cn/news/info?data=48844efbadb140acb17aca7c33c3cb85&amp;active=1" ext-link-type="uri">https://fcy.618cloud.com.cn/news/info?data=48844efbadb140acb17aca7c33c3cb85&amp;active=1</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-10">(accessed on 10 December 2024)</date-in-citation></element-citation></ref><ref id="B3-sensors-25-05552"><label>3.</label><element-citation publication-type="gov"><person-group person-group-type="author"><collab>National Sports Administration of China</collab></person-group><article-title>Statistical Overview of Swimming Venues in China (as of December 31, 2024). Official Release via National Sports Bureau/Xinhua News</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.sport.gov.cn/n315/n329/c28533813/content.html" ext-link-type="uri">https://www.sport.gov.cn/n315/n329/c28533813/content.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-30">(accessed on 30 March 2025)</date-in-citation></element-citation></ref><ref id="B4-sensors-25-05552"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chan</surname><given-names>J.S.E.</given-names></name><name name-style="western"><surname>Ng</surname><given-names>M.X.R.</given-names></name><name name-style="western"><surname>Ng</surname><given-names>Y.Y.</given-names></name></person-group><article-title>Drowning in swimming pools: Clinical features and safety recommendations based on a study of descriptive records by emergency medical services attending to 995 calls</article-title><source>Singap. Med. J.</source><year>2018</year><volume>59</volume><fpage>44</fpage><pub-id pub-id-type="doi">10.11622/smedj.2017021</pub-id><pub-id pub-id-type="pmid">28367581</pub-id><pub-id pub-id-type="pmcid">PMC5778258</pub-id></element-citation></ref><ref id="B5-sensors-25-05552"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kao</surname><given-names>W.C.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Y.L.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>F.R.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>L.D.</given-names></name></person-group><article-title>Next-Generation swimming pool drowning prevention strategy integrating AI and IoT technologies</article-title><source>Heliyon</source><year>2024</year><volume>10</volume><fpage>e35484</fpage><pub-id pub-id-type="doi">10.1016/j.heliyon.2024.e35484</pub-id><pub-id pub-id-type="pmid">39309814</pub-id><pub-id pub-id-type="pmcid">PMC11416264</pub-id></element-citation></ref><ref id="B6-sensors-25-05552"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cepeda-Pacheco</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Domingo</surname><given-names>M.C.</given-names></name></person-group><article-title>Deep learning and 5G and beyond for child drowning prevention in swimming pools</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>7684</elocation-id><pub-id pub-id-type="doi">10.3390/s22197684</pub-id><pub-id pub-id-type="pmid">36236782</pub-id><pub-id pub-id-type="pmcid">PMC9571852</pub-id></element-citation></ref><ref id="B7-sensors-25-05552"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jalalifar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Phan</surname><given-names>H.P.</given-names></name><name name-style="western"><surname>Abbas</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Asadnia</surname><given-names>M.</given-names></name></person-group><article-title>A Smart Multi-Sensor Drowning Detection Device with Real-time Alarm Function</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>25</volume><fpage>7163</fpage><lpage>7170</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3518436</pub-id></element-citation></ref><ref id="B8-sensors-25-05552"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Johnson</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Lawson</surname><given-names>K.A.</given-names></name></person-group><article-title>Evaluation of the WAVE Drowning Detection SystemTM for use with children&#8217;s summer camp groups in swimming pools: A prospective observational study</article-title><source>Int. J. Crit. Illn. Inj. Sci.</source><year>2022</year><volume>12</volume><fpage>184</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.4103/ijciis.ijciis_24_22</pub-id><pub-id pub-id-type="pmid">36779217</pub-id><pub-id pub-id-type="pmcid">PMC9910119</pub-id></element-citation></ref><ref id="B9-sensors-25-05552"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shatnawi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Albreiki</surname><given-names>F.</given-names></name><name name-style="western"><surname>Alkhoori</surname><given-names>A.</given-names></name><name name-style="western"><surname>Alhebshi</surname><given-names>M.</given-names></name></person-group><article-title>Deep learning and vision-based early drowning detection</article-title><source>Information</source><year>2023</year><volume>14</volume><elocation-id>52</elocation-id><pub-id pub-id-type="doi">10.3390/info14010052</pub-id></element-citation></ref><ref id="B10-sensors-25-05552"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alharbi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Aljohani</surname><given-names>L.</given-names></name><name name-style="western"><surname>Alqasir</surname><given-names>A.</given-names></name><name name-style="western"><surname>Alahmadi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Alhasiri</surname><given-names>R.</given-names></name><name name-style="western"><surname>Aldajan</surname><given-names>D.</given-names></name></person-group><article-title>Improved Automatic Drowning Detection Approach with YOLOv8</article-title><source>Eng. Technol. Appl. Sci. Res.</source><year>2024</year><volume>14</volume><fpage>18070</fpage><lpage>18076</lpage><pub-id pub-id-type="doi">10.48084/etasr.8834</pub-id></element-citation></ref><ref id="B11-sensors-25-05552"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mittal</surname><given-names>P.</given-names></name></person-group><article-title>A comprehensive survey of deep learning-based lightweight object detection models for edge devices</article-title><source>Artif. Intell. Rev.</source><year>2024</year><volume>57</volume><fpage>242</fpage><pub-id pub-id-type="doi">10.1007/s10462-024-10877-1</pub-id></element-citation></ref><ref id="B12-sensors-25-05552"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>P.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name></person-group><article-title>Rethinking general underwater object detection: Datasets, challenges, and solutions</article-title><source>Neurocomputing</source><year>2023</year><volume>517</volume><fpage>243</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2022.10.039</pub-id></element-citation></ref><ref id="B13-sensors-25-05552"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>A small-sized object detection oriented multi-scale feature fusion approach with application to defect detection</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2022</year><volume>71</volume><fpage>3507014</fpage><pub-id pub-id-type="doi">10.1109/TIM.2022.3153997</pub-id></element-citation></ref><ref id="B14-sensors-25-05552"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name></person-group><article-title>Feature enhancement for multi-scale object detection</article-title><source>Neural Process. Lett.</source><year>2020</year><volume>51</volume><fpage>1907</fpage><lpage>1919</lpage><pub-id pub-id-type="doi">10.1007/s11063-019-10182-x</pub-id></element-citation></ref><ref id="B15-sensors-25-05552"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Palaniappan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Subramaniam</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kalaiselvi</surname><given-names>V.</given-names></name><name name-style="western"><surname>Subha</surname><given-names>T.</given-names></name></person-group><article-title>Drowning detection and prevention system</article-title><source>Proceedings of the 2022 1st International Conference on Computational Science and Technology (ICCST)</source><conf-loc>Chennai, India</conf-loc><conf-date>9&#8211;10 November 2022</conf-date><fpage>783</fpage><lpage>785</lpage></element-citation></ref><ref id="B16-sensors-25-05552"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jebamalar</surname><given-names>G.B.</given-names></name><name name-style="western"><surname>Layola</surname><given-names>J.A.A.</given-names></name><name name-style="western"><surname>Saranya</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rose</surname><given-names>R.A.M.</given-names></name><name name-style="western"><surname>Nickel</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>M.M.</given-names></name></person-group><article-title>To Detect Active Drowning Using Deep Learning Algorithms</article-title><source>Proceedings of the 2023 9th International Conference on Smart Structures and Systems (ICSSS)</source><conf-loc>Chennai, India</conf-loc><conf-date>23&#8211;24 November 2023</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B17-sensors-25-05552"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gumaei</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hassan</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Alelaiwi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Alsalman</surname><given-names>H.</given-names></name></person-group><article-title>A hybrid deep learning model for human activity recognition using multimodal body sensing data</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>99152</fpage><lpage>99160</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2927134</pub-id></element-citation></ref><ref id="B18-sensors-25-05552"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Basthikodi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Shetty</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Shetty</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Prakruthi</surname><given-names>P.D.</given-names></name></person-group><article-title>Automated Wearable Device to Detect Drowning Incidents in Water Bodies</article-title><source>Proceedings of the 2024 Second International Conference on Advances in Information Technology (ICAIT)</source><conf-loc>Chikkamagaluru, India</conf-loc><conf-date>24&#8211;27 July 2024</conf-date><volume>Volume 1</volume><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B19-sensors-25-05552"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ganesamoorthy</surname><given-names>B.</given-names></name><name name-style="western"><surname>MohanPrasath</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kamalraj</surname><given-names>A.</given-names></name><name name-style="western"><surname>Thirukumaran</surname><given-names>M.</given-names></name></person-group><article-title>Automated drowning detection and security in swimming
pool with iot server</article-title><source>Proceedings of the 2023 International Conference on System, Computation, Automation and Networking
(ICSCAN)</source><conf-loc>Puducherry, India</conf-loc><conf-date>17&#8211;18 November 2023</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B20-sensors-25-05552"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Mei</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name></person-group><article-title>Automatic real-time detection of infant drowning using YOLOv5 and faster R-CNN models based on video surveillance</article-title><source>J. Soc. Comput.</source><year>2023</year><volume>4</volume><fpage>62</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.23919/JSC.2023.0006</pub-id></element-citation></ref><ref id="B21-sensors-25-05552"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mu</surname><given-names>X.</given-names></name></person-group><article-title>Underwater drowning people detection based on bottleneck transformer and feature pyramid network</article-title><source>Proceedings of the 2022 IEEE International Conference on Unmanned Systems (ICUS)</source><conf-loc>Guangzhou, China</conf-loc><conf-date>28&#8211;30 October 2022</conf-date><fpage>1145</fpage><lpage>1150</lpage></element-citation></ref><ref id="B22-sensors-25-05552"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kosuge</surname><given-names>A.</given-names></name><name name-style="western"><surname>Suehiro</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hamada</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kuroda</surname><given-names>T.</given-names></name></person-group><article-title>mmWave-YOLO: A mmWave imaging radar-based real-time multiclass object recognition system for ADAS applications</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2022</year><volume>71</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1109/TIM.2022.3176014</pub-id></element-citation></ref><ref id="B23-sensors-25-05552"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name></person-group><article-title>An improved YOLOv5 algorithm for drowning detection in the indoor swimming pool</article-title><source>Appl. Sci.</source><year>2023</year><volume>14</volume><elocation-id>200</elocation-id><pub-id pub-id-type="doi">10.3390/app14010200</pub-id></element-citation></ref><ref id="B24-sensors-25-05552"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Leng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kuang</surname><given-names>G.</given-names></name></person-group><article-title>BiFA-YOLO: A novel YOLO-based method for arbitrary-oriented ship detection in high-resolution SAR images</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>4209</elocation-id><pub-id pub-id-type="doi">10.3390/rs13214209</pub-id></element-citation></ref><ref id="B25-sensors-25-05552"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name></person-group><article-title>Swimming-YOLO: A drowning detection method in multi-swimming scenarios based on improved YOLO algorithm</article-title><source>Signal Image Video Process.</source><year>2025</year><volume>19</volume><fpage>161</fpage><pub-id pub-id-type="doi">10.1007/s11760-024-03744-7</pub-id></element-citation></ref><ref id="B26-sensors-25-05552"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shuai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name></person-group><article-title>Lightweight outdoor drowning detection based on improved YOLOv8</article-title><source>J. Real-Time Image Process.</source><year>2025</year><volume>22</volume><fpage>59</fpage><pub-id pub-id-type="doi">10.1007/s11554-025-01638-6</pub-id></element-citation></ref><ref id="B27-sensors-25-05552"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Design of drowning detection method based on YOLOv8</article-title><source>Proceedings of the 2024 IEEE 6th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)</source><conf-loc>Hangzhou, China</conf-loc><conf-date>23&#8211;25 October 2024</conf-date><fpage>1635</fpage><lpage>1638</lpage></element-citation></ref><ref id="B28-sensors-25-05552"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Du</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Han</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>W.</given-names></name></person-group><article-title>Deep spatio-temporal graph convolutional network for traffic accident prediction</article-title><source>Neurocomputing</source><year>2021</year><volume>423</volume><fpage>135</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2020.09.043</pub-id></element-citation></ref><ref id="B29-sensors-25-05552"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name></person-group><article-title>Lsm-yolo: A compact and effective roi detector for medical detection</article-title><source>Proceedings of the 31st International Conference on Neural Information Processing</source><conf-loc>Auckland, New Zealand</conf-loc><conf-date>2&#8211;6 December 2024</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2024</year><fpage>30</fpage><lpage>44</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05552-f001" orientation="portrait"><label>Figure 1</label><caption><p>YOLO11-LiB network structure, composed of backbone, neck and head.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05552-g001.jpg"/></fig><fig position="float" id="sensors-25-05552-f002" orientation="portrait"><label>Figure 2</label><caption><p>The structure of LGCBlock comprises LAE and GhostC3k2.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05552-g002.jpg"/></fig><fig position="float" id="sensors-25-05552-f003" orientation="portrait"><label>Figure 3</label><caption><p>The structure of C2PSAiSCSA.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05552-g003.jpg"/></fig><fig position="float" id="sensors-25-05552-f004" orientation="portrait"><label>Figure 4</label><caption><p>The structure of BiFF-Net.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05552-g004.jpg"/></fig><fig position="float" id="sensors-25-05552-f005" orientation="portrait"><label>Figure 5</label><caption><p>Examples of the dataset include scenarios with mixed samples of different perspectives, different environments, multi-targets, and small targets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05552-g005.jpg"/></fig><fig position="float" id="sensors-25-05552-f006" orientation="portrait"><label>Figure 6</label><caption><p>Training and validation metrics over 300 epochs. The top-left panels (<bold>a</bold>&#8211;<bold>c</bold>) show the training losses (box, cls, dfl). The bottom-left panels (<bold>d</bold>&#8211;<bold>f</bold>) show the corresponding validation losses. The right panels (<bold>g</bold>&#8211;<bold>j</bold>) illustrate the performance metrics on the validation set, including precision, recall, mAP@50, and mAP@50&#8211;95.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05552-g006.jpg"/></fig><fig position="float" id="sensors-25-05552-f007" orientation="portrait"><label>Figure 7</label><caption><p>The detection results of different methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05552-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05552-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05552-t001_Table 1</object-id><label>Table 1</label><caption><p>Model parameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Epoch</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch Size</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Image Size</td><td align="center" valign="middle" rowspan="1" colspan="1">640</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learning Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">SGD</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EarlyStop</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td></tr></tbody></table><table-wrap-foot><fn><p>All experiments used the same parameter settings.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05552-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05552-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparative experimental results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DR (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DmAP50 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SmAP50 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size (MB)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO11n</td><td align="center" valign="middle" rowspan="1" colspan="1">85.7</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td><td align="center" valign="middle" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" rowspan="1" colspan="1">82.4</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td><td align="center" valign="middle" rowspan="1" colspan="1">189.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.58</td><td align="center" valign="middle" rowspan="1" colspan="1">5.26</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO11s</td><td align="center" valign="middle" rowspan="1" colspan="1">88.0</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" rowspan="1" colspan="1">84.6</td><td align="center" valign="middle" rowspan="1" colspan="1">21.3</td><td align="center" valign="middle" rowspan="1" colspan="1">182.9</td><td align="center" valign="middle" rowspan="1" colspan="1">9.41</td><td align="center" valign="middle" rowspan="1" colspan="1">18.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO11m</td><td align="center" valign="middle" rowspan="1" colspan="1">88.1</td><td align="center" valign="middle" rowspan="1" colspan="1">89.4</td><td align="center" valign="middle" rowspan="1" colspan="1">92.5</td><td align="center" valign="middle" rowspan="1" colspan="1">84.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.7</td><td align="center" valign="middle" rowspan="1" colspan="1">146.7</td><td align="center" valign="middle" rowspan="1" colspan="1">20.03</td><td align="center" valign="middle" rowspan="1" colspan="1">40.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO11l</td><td align="center" valign="middle" rowspan="1" colspan="1">85.9</td><td align="center" valign="middle" rowspan="1" colspan="1">90.9</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">86.1</td><td align="center" valign="middle" rowspan="1" colspan="1">86.6</td><td align="center" valign="middle" rowspan="1" colspan="1">98.3</td><td align="center" valign="middle" rowspan="1" colspan="1">25.28</td><td align="center" valign="middle" rowspan="1" colspan="1">51.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv5</td><td align="center" valign="middle" rowspan="1" colspan="1">86.0</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">92.9</td><td align="center" valign="middle" rowspan="1" colspan="1">84.8</td><td align="center" valign="middle" rowspan="1" colspan="1">7.1</td><td align="center" valign="middle" rowspan="1" colspan="1">180.0</td><td align="center" valign="middle" rowspan="1" colspan="1">2.50</td><td align="center" valign="middle" rowspan="1" colspan="1">5.07</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8n</td><td align="center" valign="middle" rowspan="1" colspan="1">87.6</td><td align="center" valign="middle" rowspan="1" colspan="1">88.6</td><td align="center" valign="middle" rowspan="1" colspan="1">92.5</td><td align="center" valign="middle" rowspan="1" colspan="1">84.5</td><td align="center" valign="middle" rowspan="1" colspan="1">8.1</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>250.9</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">3.01</td><td align="center" valign="middle" rowspan="1" colspan="1">5.98</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv9t</td><td align="center" valign="middle" rowspan="1" colspan="1">85.2</td><td align="center" valign="middle" rowspan="1" colspan="1">87.1</td><td align="center" valign="middle" rowspan="1" colspan="1">91.9</td><td align="center" valign="middle" rowspan="1" colspan="1">84.4</td><td align="center" valign="middle" rowspan="1" colspan="1">7.6</td><td align="center" valign="middle" rowspan="1" colspan="1">97.9</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.97</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">4.46</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10n</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td><td align="center" valign="middle" rowspan="1" colspan="1">89.0</td><td align="center" valign="middle" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" rowspan="1" colspan="1">83.9</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2</td><td align="center" valign="middle" rowspan="1" colspan="1">164.2</td><td align="center" valign="middle" rowspan="1" colspan="1">2.70</td><td align="center" valign="middle" rowspan="1" colspan="1">5.53</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO12n</td><td align="center" valign="middle" rowspan="1" colspan="1">80.8</td><td align="center" valign="middle" rowspan="1" colspan="1">83.3</td><td align="center" valign="middle" rowspan="1" colspan="1">90.0</td><td align="center" valign="middle" rowspan="1" colspan="1">81.2</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td><td align="center" valign="middle" rowspan="1" colspan="1">126.2</td><td align="center" valign="middle" rowspan="1" colspan="1">2.56</td><td align="center" valign="middle" rowspan="1" colspan="1">5.31</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RTDERT-I</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>90.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">87.8</td><td align="center" valign="middle" rowspan="1" colspan="1">90.0</td><td align="center" valign="middle" rowspan="1" colspan="1">82.3</td><td align="center" valign="middle" rowspan="1" colspan="1">103.4</td><td align="center" valign="middle" rowspan="1" colspan="1">52.8</td><td align="center" valign="middle" rowspan="1" colspan="1">31.99</td><td align="center" valign="middle" rowspan="1" colspan="1">63</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Faster R-CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">86.2</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>91.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.8</td><td align="center" valign="middle" rowspan="1" colspan="1">83.1</td><td align="center" valign="middle" rowspan="1" colspan="1">369.2</td><td align="center" valign="middle" rowspan="1" colspan="1">18.2</td><td align="center" valign="middle" rowspan="1" colspan="1">136.5</td><td align="center" valign="middle" rowspan="1" colspan="1">278</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ConvNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">88.5</td><td align="center" valign="middle" rowspan="1" colspan="1">89.3</td><td align="center" valign="middle" rowspan="1" colspan="1">93.2</td><td align="center" valign="middle" rowspan="1" colspan="1">85.2</td><td align="center" valign="middle" rowspan="1" colspan="1">95.7</td><td align="center" valign="middle" rowspan="1" colspan="1">42.5</td><td align="center" valign="middle" rowspan="1" colspan="1">48.9</td><td align="center" valign="middle" rowspan="1" colspan="1">98</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ViTDet</td><td align="center" valign="middle" rowspan="1" colspan="1">90.2</td><td align="center" valign="middle" rowspan="1" colspan="1">90.1</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>94.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>86.3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1024.0</td><td align="center" valign="middle" rowspan="1" colspan="1">9.8</td><td align="center" valign="middle" rowspan="1" colspan="1">150.2</td><td align="center" valign="middle" rowspan="1" colspan="1">305</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RetinaNet</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">88.5</td><td align="center" valign="middle" rowspan="1" colspan="1">91.0</td><td align="center" valign="middle" rowspan="1" colspan="1">82.0</td><td align="center" valign="middle" rowspan="1" colspan="1">210.5</td><td align="center" valign="middle" rowspan="1" colspan="1">28.1</td><td align="center" valign="middle" rowspan="1" colspan="1">36.8</td><td align="center" valign="middle" rowspan="1" colspan="1">74.5</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>6.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.25</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>All experiments used the same parameter settings; The bold markings in the table are used to highlight the model that has the most outstanding performance under the corresponding evaluation metric.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05552-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05552-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation study results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DR (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DmAP50 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SmAP50 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size (MB)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO11n</td><td align="center" valign="middle" rowspan="1" colspan="1">85.7</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td><td align="center" valign="middle" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" rowspan="1" colspan="1">82.4</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>189.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.58</td><td align="center" valign="middle" rowspan="1" colspan="1">5.26</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">+A</td><td align="center" valign="middle" rowspan="1" colspan="1">86.6</td><td align="center" valign="middle" rowspan="1" colspan="1">87.9</td><td align="center" valign="middle" rowspan="1" colspan="1">92</td><td align="center" valign="middle" rowspan="1" colspan="1">83.8</td><td align="center" valign="middle" rowspan="1" colspan="1">5.9</td><td align="center" valign="middle" rowspan="1" colspan="1">118.8</td><td align="center" valign="middle" rowspan="1" colspan="1">2.21</td><td align="center" valign="middle" rowspan="1" colspan="1">4.58</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">+B</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>89.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>85.8</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td><td align="center" valign="middle" rowspan="1" colspan="1">165.7</td><td align="center" valign="middle" rowspan="1" colspan="1">2.55</td><td align="center" valign="middle" rowspan="1" colspan="1">5.21</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">+C</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>90.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7</td><td align="center" valign="middle" rowspan="1" colspan="1">85.4</td><td align="center" valign="middle" rowspan="1" colspan="1">6.8</td><td align="center" valign="middle" rowspan="1" colspan="1">108.6</td><td align="center" valign="middle" rowspan="1" colspan="1">2.43</td><td align="center" valign="middle" rowspan="1" colspan="1">4.99</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">+A+B</td><td align="center" valign="middle" rowspan="1" colspan="1">87.4</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">92.3</td><td align="center" valign="middle" rowspan="1" colspan="1">83.9</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>5.9</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">109.4</td><td align="center" valign="middle" rowspan="1" colspan="1">2.18</td><td align="center" valign="middle" rowspan="1" colspan="1">4.53</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+A+B+C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>94.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.02</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.25</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>All experiments used the same parameter settings; A: LGCBlock, B: C2PSAiSCSA, C: BiFF-Net; The bold markings in the table are used to highlight the model that has the most outstanding performance under the corresponding evaluation metric.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>