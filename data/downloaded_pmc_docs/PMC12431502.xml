<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431502</article-id><article-id pub-id-type="pmcid-ver">PMC12431502.1</article-id><article-id pub-id-type="pmcaid">12431502</article-id><article-id pub-id-type="pmcaiid">12431502</article-id><article-id pub-id-type="doi">10.3390/s25175275</article-id><article-id pub-id-type="publisher-id">sensors-25-05275</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Lightweight Image Super-Resolution Reconstruction Network Based on Multi-Order Information Optimization</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Gao</surname><given-names initials="S">Shengxuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05275" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-1067-2324</contrib-id><name name-style="western"><surname>Li</surname><given-names initials="L">Long</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05275" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-4516-4313</contrib-id><name name-style="western"><surname>Cui</surname><given-names initials="W">Wen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05275" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3345-9665</contrib-id><name name-style="western"><surname>Jiang</surname><given-names initials="H">He</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05275" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8937-1515</contrib-id><name name-style="western"><surname>Ge</surname><given-names initials="H">Hongwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05275" ref-type="aff">1</xref><xref rid="c1-sensors-25-05275" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Smolka</surname><given-names initials="B">Bogdan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05275"><label>1</label>School of Computer Science and Technology, Dalian University of Technology, Dalian 116024, China; <email>15652093118@mail.dlut.edu.cn</email></aff><aff id="af2-sensors-25-05275"><label>2</label>School of Information and Control Engineering, China University of Mining and Technology, Xuzhou 221116, China; <email>longli@cumt.edu.cn</email> (L.L.); <email>cuiwen@cumt.edu.cn</email> (W.C.); <email>jianghe@cumt.edu.cn</email> (H.J.)</aff><author-notes><corresp id="c1-sensors-25-05275"><label>*</label>Correspondence: <email>hwge@dlut.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5275</elocation-id><history><date date-type="received"><day>23</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>12</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>21</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>25</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05275.pdf"/><abstract><p>Traditional information distillation networks using single-scale convolution and simple feature fusion often result in insufficient information extraction and ineffective restoration of high-frequency details. To address this problem, we propose a lightweight image super-resolution reconstruction network based on multi-order information optimization. The core of this network lies in the enhancement and refinement of high-frequency information. Our method operates through two main stages to fully exploit the high-frequency features in images while eliminating redundant information, thereby enhancing the network&#8217;s detail restoration capability. In the high-frequency information enhancement stage, we design a self-calibration high-frequency information enhancement block. This block generates calibration weights through self-calibration branches to modulate the response strength of each pixel. It then selectively enhances critical high-frequency information. Additionally, we combine an auxiliary branch and a chunked space optimization strategy to extract local details and adaptively reinforce high-frequency features. In the high-frequency information refinement stage, we propose a multi-scale high-frequency information refinement block. First, multi-scale information is captured through multiplicity sampling to enrich the feature hierarchy. Second, the high-frequency information is further refined using a multi-branch structure incorporating wavelet convolution and band convolution, enabling the extraction of diverse detailed features. Experimental results demonstrate that our network achieves an optimal balance between complexity and performance, outperforming popular lightweight networks in both quantitative metrics and visual quality.</p></abstract><kwd-group><kwd>super-resolution</kwd><kwd>lightweight</kwd><kwd>attention mechanism</kwd><kwd>information distillation</kwd><kwd>multi-order information optimization</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>61976034</award-id><award-id>52304182</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China under Grant 61976034, 52304182.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05275"><title>1. Introduction</title><p>Image super-resolution (SR) reconstructs a low-resolution (LR) image into a high-resolution (HR) one. It has made substantial contributions to medical imaging [<xref rid="B1-sensors-25-05275" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05275" ref-type="bibr">2</xref>], smart mining [<xref rid="B3-sensors-25-05275" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05275" ref-type="bibr">4</xref>], and computational photography [<xref rid="B5-sensors-25-05275" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05275" ref-type="bibr">6</xref>]. Traditional SR methods fall into two categories: interpolation [<xref rid="B7-sensors-25-05275" ref-type="bibr">7</xref>] and reconstruction [<xref rid="B8-sensors-25-05275" ref-type="bibr">8</xref>]. However, interpolation often results in blurry, distorted images, while reconstruction is computationally intensive, hindering their practical use.</p><p>In recent years, with the development of deep learning, convolutional neural networks (CNNs) [<xref rid="B9-sensors-25-05275" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05275" ref-type="bibr">10</xref>] have been widely used in image super-resolution (SR). Dong et al. [<xref rid="B11-sensors-25-05275" ref-type="bibr">11</xref>] first introduced CNNs into SR and proposed the super-resolution convolutional neural network (SRCNN), which consists of three convolutional layers. Subsequently, the same team proposed the Fast Super-Resolution Convolutional Neural Network (FSRCNN) [<xref rid="B12-sensors-25-05275" ref-type="bibr">12</xref>], which significantly improves inference speed by incorporating deconvolution layers and a more compact network architecture. Shi et al. [<xref rid="B13-sensors-25-05275" ref-type="bibr">13</xref>] proposed a pixel shuffling strategy and used it to construct the efficient sub-pixel convolutional neural network (ESPCN). However, SRCNN, FSRCNN, and ESPCN have shallow network layers and poor reconstruction effects. To address these limitations, Kim et al. [<xref rid="B14-sensors-25-05275" ref-type="bibr">14</xref>] increased the number of network layers and proposed very deep convolutional networks for single image super-resolution (VDSR), which improved network performance to some extent. Lim et al. [<xref rid="B15-sensors-25-05275" ref-type="bibr">15</xref>] proposed enhanced deep residual networks for single image super-resolution (EDSR), which constructs an even deeper network by removing the batch normalization layer, achieving excellent reconstruction results. Although VDSR and EDSR achieved good reconstruction results, their large number of network parameters and computational overhead make it difficult to deploy them on mobile devices for practical applications. Consequently, researchers have proposed recursive, pruning, and information distillation methods to develop lightweight networks. Among these, the residual feature distillation network (RFDN) [<xref rid="B16-sensors-25-05275" ref-type="bibr">16</xref>], as a typical information distillation network, refines features layer by layer through a flexible feature distillation mechanism, ensuring a lightweight network while demonstrating efficient reconstruction capabilities.</p><p>Currently, most information distillation networks still use the RFDN&#8217;s information distillation mechanism, which completes feature extraction by stacking single-scale convolution in the distillation block, directly fuses the distilled features of different layers, and then unifies all the features. Although this approach meets the lightweight network requirements, it is prone to information singularity during feature extraction and the dilution or loss of high-frequency information due to the simple fusion mechanism. To address these issues, we propose a lightweight image super-resolution reconstruction network based on multi-order information optimization (MOION). The network optimizes image detail restoration through four streamlined stages: high-frequency enhancement, information distillation, frequency refinement, and feature fusion, effectively extracting critical features while eliminating redundancy.</p><p>The core component of MOION is the multi-order information optimization block (MOIOB), which incorporates three dual-branch self-calibrating high-frequency information enhancement block (SCHIEB). These generate wavelet-derived calibration weights to amplify critical features while suppressing noise, complemented by auxiliary branches and spatial optimization for local detail extraction. Following the enhancement stage, we set up four distillation layers. Each layer compresses the enhanced multi-channel information into a small number of key features using low-dimensional convolution. This ensures the lightweight design of network. Subsequently, we construct a multi-scale high-frequency information refinement block (MSHIRB), which further refines the distilled key features through multiplicity sampling and a multi-branch feature extraction strategy, enabling the network to capture diverse image details. Finally, we introduce an enhanced spatial attention block to weight and map the processed features, further strengthening the representation of key regions and enabling full feature fusion and utilization.</p><p>The main contributions of this paper are summarized as follows:<list list-type="bullet"><list-item><p>We propose a self-calibrating high-frequency information enhancement block (SCHIEB). By designing an adaptive high-frequency enhancement mechanism, the network can dynamically adjust feature representation across different regions, addressing the insufficient high-frequency expression in traditional distillation networks.</p></list-item><list-item><p>We design a multi-scale high-frequency information refinement block (MSHIRB). By using a lightweight multiplicity sampling and multi-branch feature extraction method, it fully captures the remaining multi-scale information and high-frequency details, solving the problem of limited feature diversity in traditional distillation networks.</p></list-item><list-item><p>We propose a multi-order information optimization block (MOIOB). Compared to traditional distillation blocks, our architecture establishes a complete information optimization path, enabling better extraction of high-frequency features and removal of redundant information, thus improving detail recovery.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05275"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05275"><title>2.1. Lightweight SR Network</title><p>Deep networks, dependent on vast parameters and computational resources, pose training challenges and hinder practical applications. This has spurred interest in lightweight networks suitable for mobile deployment. Tai et al. [<xref rid="B17-sensors-25-05275" ref-type="bibr">17</xref>] presented the deep recursive residual network (DRRN), which uses parameter sharing and recursive learning to effectively reduce network parameters. Yu et al. [<xref rid="B18-sensors-25-05275" ref-type="bibr">18</xref>] introduced the distillation and iterative pruning network (DIPNet), adopting pruning techniques to remove redundant connections and thus enhance network efficiency and generalization ability. Sun et al. [<xref rid="B19-sensors-25-05275" ref-type="bibr">19</xref>] proposed the spatially-adaptive feature modulation network (SAFMN), designing spatially adaptive feature modulation mechanisms to dynamically select representative features and increase information processing speed. Lu et al. [<xref rid="B20-sensors-25-05275" ref-type="bibr">20</xref>] developed the efficient super-resolution transformer (ESRT), mixing CNN and lightweight Transformer backbone to extract deep features efficiently at a low computational cost. Li et al. [<xref rid="B21-sensors-25-05275" ref-type="bibr">21</xref>] designed the cross-receptive focused inference network (CFIN), which combines cross-scale aggregation blocks with cross-acceptance focusing mechanisms to eliminate redundant features and achieve a good balance between network performance and complexity.</p><p>Due to hardware constraints, super-resolution networks must balance computational efficiency with enhanced detail reconstruction. To overcome this limitation, we redesign traditional information distillation through multi-order information optimization block, establishing comprehensive informa-tion optimization path. Through this optimized architectural design, our network enhances detail recovery while maintaining low computational overhead.</p></sec><sec id="sec2dot2-sensors-25-05275"><title>2.2. Lightweight SR Network Based on Information Distillation</title><p>Currently, researchers propose a variety of efficient information distillation networks to meet the needs of practical applications. Hui et al. [<xref rid="B22-sensors-25-05275" ref-type="bibr">22</xref>] first proposed the information distillation network (IDN), which first performs channel segmentation of the features, processes only some of the features, and finally aggregates them with the retained original features. This approach greatly reduces the computational complexity of the network. Subsequently, Hui et al. [<xref rid="B23-sensors-25-05275" ref-type="bibr">23</xref>] proposed the information multi-distillation network (IMDN), which gradually extracts features by cascading distillation layers and improves the efficiency of network feature extraction. Based on the IMDN architecture, Liu et al. [<xref rid="B16-sensors-25-05275" ref-type="bibr">16</xref>] proposed RFDN, which refines the features layer by layer through a flexible feature distillation mechanism and demonstrates efficient reconstruction capability while ensuring lightweight. Kong et al. [<xref rid="B24-sensors-25-05275" ref-type="bibr">24</xref>] proposed a residual local feature network (RLFN), which removes the feature distillation connections and significantly accelerates the network inference. Li et al. [<xref rid="B25-sensors-25-05275" ref-type="bibr">25</xref>] proposed a blueprint separable residual network (BSRN), which reduces the redundant computation in the feature extraction block by blueprint separable convolution and enhances the distillation feature extraction capability by combining with an efficient attention block.</p><p>However, most of these networks focus on structural simplification and computational efficiency but neglect the balance between performance and complexity. The proposed multi-order information optimization network bridges this research gap. It enhances feature extraction and captures more image details. Additionally, it improves network performance with fewer added parameters, offering a better trade-off than existing information distillation networks.</p></sec></sec><sec id="sec3-sensors-25-05275"><title>3. Multi-Order Information Optimization Network</title><sec id="sec3dot1-sensors-25-05275"><title>3.1. Network Architecture</title><p>The overall structure of a multi-order information optimization network (MOION) is shown in <xref rid="sensors-25-05275-f001" ref-type="fig">Figure 1</xref>. MOION consists of four parts: shallow feature extraction, deep feature extraction, multi-layer feature fusion, and reconstruction. For the input low-resolution image <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, we use a <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution to extract shallow features <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The process is represented as<disp-formula id="FD1-sensors-25-05275"><label>(1)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes shallow feature extraction function and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution. <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is fed into multiple multi-order information optimization block (MOIOB) to extract the deep features step by step, and the process is expressed as<disp-formula id="FD2-sensors-25-05275"><label>(2)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>MOIOB</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>MOIOB</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:mo>&#8230;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>MOIOB</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mfenced></mml:mfenced><mml:mo>&#8230;</mml:mo></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>MOIOB</mml:mi><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <italic toggle="yes">n</italic>th MOIOB function and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the output of the <italic toggle="yes">n</italic>th MOIOB. In order to fully utilize the features of all depths, they are spliced and activated by <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional fusion and GELU, and then the fused features are refined by <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, which is denoted as<disp-formula id="FD3-sensors-25-05275"><label>(3)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Concat</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes feature splicing along the channel dimension, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes fused features. In order to take advantage of residual learning, the <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the fused features are summed and fed into the reconstruction part. This part consists of one <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution and pixel shuffling operation [<xref rid="B13-sensors-25-05275" ref-type="bibr">13</xref>] for up-sampling the image. The process is represented as<disp-formula id="FD4-sensors-25-05275"><label>(4)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#8853;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the reconstruction function and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the output super-resolution image. In this paper, the network is trained by minimizing the <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> loss function, which is denoted by <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi><mml:mo>(</mml:mo><mml:mi>&#952;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD5-sensors-25-05275"><label>(5)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#952;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>MOION</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the learnable parameters of MOION, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>MOION</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the MOION function, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> denotes the number of <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> norm, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denote the <italic toggle="yes">i</italic>th input LR and HR image sample pairs, <italic toggle="yes">N</italic> is the total number of samples, <italic toggle="yes">i</italic> is the number of sample serial numbers.</p></sec><sec id="sec3dot2-sensors-25-05275"><title>3.2. Multi-Order Information Optimization Block</title><p>Traditional information distillation blocks typically use single-scale convolution in their backbone to extract high-frequency information and then directly fuse the distilled results. This leads to insufficiently rich and overly uniform feature information, failing to fully capture the image&#8217;s diverse details. In order to solve the above problems, we propose a multi-order information optimization block (MOIOB), which fully exploits the high-frequency information in the image through four stages: high-frequency information enhancement, information distillation, high-frequency information refinement, and information fusion. The structure of MOIOB is shown in <xref rid="sensors-25-05275-f002" ref-type="fig">Figure 2</xref>.</p><p>The first stage consists of three series-connected self-calibrating high-frequency information enhancement blocks (SCHIEBs) which aim to enhance the high-frequency information of the input image. Furthermore, this stage provides the subsequent information distillation and refinement stages with richer high-frequency features. Taking the first <italic toggle="yes">n</italic>th MOIOB in <xref rid="sensors-25-05275-f001" ref-type="fig">Figure 1</xref> as an example, the input is <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the high-frequency information enhancement stage can be expressed as follows:<disp-formula id="FD6-sensors-25-05275"><label>(6)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>SCHIEB</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>SCHIEB</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the output of the <italic toggle="yes">i</italic>th SCHIEB and <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>H</mml:mi><mml:mi>I</mml:mi><mml:mi>E</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the SCHIEB function.</p><p>The second stage consists of three <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution layers and one <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution layer, which compresses the information of multiple channels into fewer key features through low-dimensional convolution to achieve information distillation. This stage intends to reduce the information redundancy, refine the feature representation via low-dimensional convolution, and ensure the lightweight of the network. The information distillation stage can be expressed as <disp-formula id="FD7-sensors-25-05275"><label>(7)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denote the distilled features and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the information distillation function.</p><p>The third stage consists of two multi-scale high-frequency information refinement blocks (MSHIRBs) whose purpose is to refine the distilled information, further refine the high-frequency features, and optimize the final reconstruction effect. The high-frequency information refinement stage can be expressed as<disp-formula id="FD8-sensors-25-05275"><label>(8)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>MSHIRB</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>MSHIRB</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote the output of two MSHIRBs and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>H</mml:mi><mml:mi>I</mml:mi><mml:mi>R</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the MSHIRB function. The outputs of the MSHIRB are fused in the fourth stage. In this paper, the features are channel rearranged using channel blending to break the isolation of the information between each channel and avoid the singularity of feature information.</p><p>Finally, the features after <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional smoothing are then fed into the enhanced spatial attention block (ESAB) [<xref rid="B26-sensors-25-05275" ref-type="bibr">26</xref>] for weighted combination and feature mapping, which helps the network to focus on more discriminative features in the spatial domain to improve the efficiency of information utilization. The information fusion stage can be represented as<disp-formula id="FD9-sensors-25-05275"><label>(9)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the output of the <italic toggle="yes">n</italic>th MOIOB and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the information fusion function. Through the above four stages, MOIOB can remove redundant information to optimize the information extraction and fusion process, and it improves the network&#8217;s ability to recover the details of the features.</p></sec><sec id="sec3dot3-sensors-25-05275"><title>3.3. Self-Calibrating High-Frequency Information Enhancement Block</title><p>In image super-resolution tasks, edges and textures are crucial for image restoration, and this information is usually embedded in the high-frequency features of the image. However, the operation of stacked convolution of conventional distillation blocks cannot dynamically adjust the feature expression in different regions, resulting in high-frequency information being easily interfered by noise, which affects the quality of image reconstruction. For this reason, we propose the self-calibrating high-frequency information enhancement block (SCHIEB) which adaptively enhances the high-frequency information through dual branching.</p><p>The structure of SCHIEB is shown in <xref rid="sensors-25-05275-f003" ref-type="fig">Figure 3</xref>, which includes self-calibrating branch (SCB) and auxiliary branch (AB). In SCB, the input features are processed in two steps: first, we use <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution to downscale the input features to reduce the computational complexity and then extract the local high-frequency features by <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution after activation by the GELU function; second, we introduce wavelet convolution (WTConv) [<xref rid="B27-sensors-25-05275" ref-type="bibr">27</xref>], which realizes a larger sensory field and helps the network to capture the shape information of the image. The input features are first processed through a WTConv-5 layer and sigmoid activation to generate calibration weights. Then, they are multiplied with the outputs of a 3 &#215; 3 convolutional layer. This operation controls pixel-wise response intensity and suppresses the noise. Finally, it can adaptively enhance the high-frequency information. Taking the first SCHIEB in <xref rid="sensors-25-05275-f002" ref-type="fig">Figure 2</xref> as an example, the processing of features by SCB can be expressed as follows:<disp-formula id="FD10-sensors-25-05275"><label>(10)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>&#957;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the sigmoid function, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>&#957;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the wavelet convolution, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the input of SCHIEB, <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the GELU activation function, and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the output of SCB. In AB, the input features are also activated by dimensionality reduction, then the significant features in the local region are retained by maxpooling, and finally the local details are further refined by <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution accumulation to further refine the local details to assist in enhancing the high-frequency information. The processing of features in AB can be represented as<disp-formula id="FD11-sensors-25-05275"><label>(11)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes maximum pooling and <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the output of AB.</p><p>The features enhanced by SCB and AB are spliced in the channel dimension after GELU activation, respectively. In order to capture more useful features in the space, we input the spliced features into the chunked space optimization block (CSOB) to further optimize the feature representation, the structure of which is shown in <xref rid="sensors-25-05275-f004" ref-type="fig">Figure 4</xref>. The CSOB module builds upon spatially adaptive feature modulation [<xref rid="B19-sensors-25-05275" ref-type="bibr">19</xref>], employing feature partitioning and adaptive max-pooling for multi-scale downsampling. Local contextual information within partitioned regions is captured through depthwise convolutions, followed by upsampling and channel-wise concatenation of processed features. Spatial correlations across blocks are aggregated via efficient blueprint separable convolution [<xref rid="B25-sensors-25-05275" ref-type="bibr">25</xref>]. The optimized features are ultimately obtained through element-wise multiplication. The CSOB effectively implements the cross-scale information interactions and further enhances the diversity of the feature expression. Finally, the CSOB-optimized features are summed with the original features through residual linkage as the output of SCHIEB. The above process can be expressed as<disp-formula id="FD12-sensors-25-05275"><label>(12)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8853;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>CSOB</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the features after two-branch splicing, <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the output of SCHIEB, <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes layer normalization, and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>CSOB</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the CSOB function.</p></sec><sec id="sec3dot4-sensors-25-05275"><title>3.4. Multi-Scale High-Frequency Information Refinement Block</title><p>The traditional information distillation block directly fuses features from different layers and then unifies all the features. In this process, high-frequency information is diluted or lost due to simple weighting and fusion. In order to further refine the distillation information and retain more high-frequency features, we propose the multi-scale high-frequency information refinement block (MSHIRB) as shown in <xref rid="sensors-25-05275-f005" ref-type="fig">Figure 5</xref>. It takes two adjacent post-distilled features as input. First, these features are spliced along the channel dimension. Then, channel blending is applied to enhance cross-level information exchange. Finally, a <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution reduces the dimension, lowering the network&#8217;s computational complexity.</p><p>In MSHIRB, we use multiplicity sampling to accomplish the extraction of high-frequency information. This approach can capture multi-scale high-frequency information with very little overhead. Specifically, the downsampled features are downsampled two, four, and eight times by maxpooling and recovered to the original feature map size by interpolation to obtain the features containing more low-frequency information and subtracted from the original features element by element to extract the features containing multi-scale high-frequency information. Taking the distillation features <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> as an example, the process of multiplicity sampling can be expressed as <disp-formula id="FD13-sensors-25-05275"><label>(13)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes features after dimensionality reduction, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the channel mixing operation, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> are the multiscale high-frequency features obtained by multiplicity sampling, <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicate two-, four- and eight-time downsampling, <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicate two-, four-, and eight-time upsampling, respectively. The multiplicity sampling effectively retains and refines the high-frequency features in the distillation information.</p><p>To further refine these features and enable the network to capture richer texture information, we propose the multi-branch feature extraction block (MBFEB) illustrated in <xref rid="sensors-25-05275-f006" ref-type="fig">Figure 6</xref>. The multi-scale high-frequency information, after channel splicing and dimensionality reduction, is fed into MBFEB. Within MBFEB, channel segmentation divides the information into four parts. While part of the original channel information is retained, the remaining three branches undergo wavelet and band convolutions separately. Wavelet convolution [<xref rid="B27-sensors-25-05275" ref-type="bibr">27</xref>] captures the image&#8217;s shape information, while band convolution extracts its horizontal and vertical texture information. By fusing diverse information from different branches, MBFEB enhances the network&#8217;s ability to recover image details. The refinement process of features by MBFEB can be represented as<disp-formula id="FD14-sensors-25-05275"><label>(14)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>MBFEB</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the multiplicity sampled features spliced in the channel dimension, <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>F</mml:mi><mml:mi>E</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the MBFEB function, and <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the output of MSHIRB.</p></sec></sec><sec id="sec4-sensors-25-05275"><title>4. Experimental Results and Analysis</title><sec id="sec4dot1-sensors-25-05275"><title>4.1. Experimental Setup</title><p>The experiments are conducted in an environment with an Intel i5-13490F processor, a NVIDIA RTX 4070 graphics card, and a Pytorch 10.1 framework. The initial learning rate of the network is set to <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the number of training rounds is halved when the number of training rounds reaches 200 with a total of 1000 rounds of training. The training image cropping block size is <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>64</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> with 16 small blocks input per batch. The optimizer is ADAM [<xref rid="B28-sensors-25-05275" ref-type="bibr">28</xref>] with parameters set to <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The number of input channels to the network is 64 and the number of MOIOB is 6. All networks in the ablation experiment are trained for 300 rounds with 32 input channels and the rest of the training settings are the same as the established configuration.</p></sec><sec id="sec4dot2-sensors-25-05275"><title>4.2. Datasets and Evaluation Indicators</title><p>In this paper, 800 pairs of images from DIV2K [<xref rid="B29-sensors-25-05275" ref-type="bibr">29</xref>] are used as the training set, and four publicly available datasets, Set5 [<xref rid="B30-sensors-25-05275" ref-type="bibr">30</xref>], Set14 [<xref rid="B31-sensors-25-05275" ref-type="bibr">31</xref>], B100 [<xref rid="B32-sensors-25-05275" ref-type="bibr">32</xref>], and Urban100 [<xref rid="B33-sensors-25-05275" ref-type="bibr">33</xref>], are used as the test set. The network measures the complexity by the number of parameters and floating-point operations FLOPs, and the quality of the reconstructed image is measured by peak signal to noise ratio (PSNR) and structural similarity (SSIM) [<xref rid="B34-sensors-25-05275" ref-type="bibr">34</xref>]. The PSNR is measured in dB, and the larger the value, the higher the quality of the reconstructed image. The PSNR is calculated using the following formula:<disp-formula id="FD15-sensors-25-05275"><label>(15)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mi>x</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mi>y</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mfenced></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>PSNR</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo form="prefix">lg</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">x</italic> is the reconstructed image, <italic toggle="yes">y</italic> is the real high resolution image, <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean square error, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the pixel values at the corresponding coordinates, <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> are the height and width of the image, respectively, <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum pixel value in the image. SSIM mainly consists from the brightness, structure, and contrast to consider in the reconstruction of the quality of the image, which take a range from 0 to 1. The closer the value to 1, the higher the quality of the reconstructed image. SSIM is calculated by the following formula:<disp-formula id="FD16-sensors-25-05275"><label>(16)</label><mml:math id="mm97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>SSIM</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mn>2</mml:mn><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfenced><mml:mfenced separators="" open="(" close=")"><mml:mn>2</mml:mn><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfenced><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where the reconstructed image is <italic toggle="yes">x</italic>, the real high-resolution image is <italic toggle="yes">y</italic>, <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the average pixel values of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, respectively, <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> are the variances of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, respectively, <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the covariance of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, and <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are constants.</p></sec><sec id="sec4dot3-sensors-25-05275"><title>4.3. Network Performance Comparison</title><sec id="sec4dot3dot1-sensors-25-05275"><title>4.3.1. Comparison of Objective Quantitative Indicators</title><p>In order to verify the superiority of the networks proposed in this paper, MOION is compared with the current state-of-the-art lightweight networks, including EDSR-baseline [<xref rid="B15-sensors-25-05275" ref-type="bibr">15</xref>], IMDN [<xref rid="B23-sensors-25-05275" ref-type="bibr">23</xref>], RFDN [<xref rid="B16-sensors-25-05275" ref-type="bibr">16</xref>], BSRN [<xref rid="B25-sensors-25-05275" ref-type="bibr">25</xref>], SAFMN [<xref rid="B19-sensors-25-05275" ref-type="bibr">19</xref>], DLSR [<xref rid="B35-sensors-25-05275" ref-type="bibr">35</xref>], DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>], HAFRN [<xref rid="B37-sensors-25-05275" ref-type="bibr">37</xref>], OSFFNet [<xref rid="B38-sensors-25-05275" ref-type="bibr">38</xref>], and HSRNet [<xref rid="B39-sensors-25-05275" ref-type="bibr">39</xref>]. As can be seen from <xref rid="sensors-25-05275-t001" ref-type="table">Table 1</xref>, MOION achieves optimality in all metrics. As the scale factor increases, the more high-frequency information is needed for image reconstruction, the more difficult it is to reconstruct, and MOION has a more obvious advantage in large-scale reconstruction metrics compared with other networks. The Urban100 dataset, with its challenging and complex texture, can better validate the network&#8217;s reconstruction capability. Taking <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> as an example, MOION improves PSNR by <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.27</mml:mn><mml:mspace width="0.166667em"/><mml:mi>dB</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and SSIM by <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0071</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on Urban100 compared to HSRNet with the number of parameters greater than <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, while the number of parameters is only <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>65</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of HSRNet. Compared with IMDN, which has a close number of parameters, MOION improves PSNR by <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.51</mml:mn><mml:mspace width="0.166667em"/><mml:mi>dB</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and SSIM by <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0167</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on Urban100, while the computation is <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>12</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> less than IMDN. This shows that MOION has superior performance and a good trade-off between complexity and performance.</p></sec><sec id="sec4dot3dot2-sensors-25-05275"><title>4.3.2. Comparison of Subjective Visual Effects</title><p>In order to visualize the reconstruction performance of MOION, images with complex texture details in B100 and Urban100 are selected for reconstruction at <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> scale, and the reconstruction results are compared with IMDN, RFDN, BSRN, LatticeNet [<xref rid="B43-sensors-25-05275" ref-type="bibr">43</xref>], ESRT [<xref rid="B20-sensors-25-05275" ref-type="bibr">20</xref>], and NGSwin [<xref rid="B44-sensors-25-05275" ref-type="bibr">44</xref>] in terms of subjective visual effects. The experimental results are shown in <xref rid="sensors-25-05275-f007" ref-type="fig">Figure 7</xref>, <xref rid="sensors-25-05275-f008" ref-type="fig">Figure 8</xref>, <xref rid="sensors-25-05275-f009" ref-type="fig">Figure 9</xref> and <xref rid="sensors-25-05275-f010" ref-type="fig">Figure 10</xref>. In image 86000, MOION reconstructs straighter and clearer grid lines, while the rest of the networks reconstruct distorted and blurred lines. In image 210088, MOION reconstructs the fisheye shape closest to HR, while the rest of the networks reconstruct the fisheye with obvious distortion. In image 058, MOION reconstructs all curves completely, while the remaining networks fail to reconstruct them completely or are illegible. In image 015, MOION reconstructs the lines in the correct orientation, while the remaining networks all reconstruct the wrong orientation. Overall, the comparison of the reconstruction results in <xref rid="sensors-25-05275-f007" ref-type="fig">Figure 7</xref>, <xref rid="sensors-25-05275-f008" ref-type="fig">Figure 8</xref>, <xref rid="sensors-25-05275-f009" ref-type="fig">Figure 9</xref> and <xref rid="sensors-25-05275-f010" ref-type="fig">Figure 10</xref> further demonstrates the advanced performance of MOION.</p></sec><sec id="sec4dot3dot3-sensors-25-05275"><title>4.3.3. Comparison with Transformer-Based Networks</title><p>In recent years, the application of Transformer in SR has greatly improved the reconstruction performance and demonstrated great competitiveness compared with CNN networks. In order to further verify the superiority of MOION, seven Transformer-based lightweight networks are selected for comparison in this paper, including SwinIR-light [<xref rid="B45-sensors-25-05275" ref-type="bibr">45</xref>], LBNet [<xref rid="B46-sensors-25-05275" ref-type="bibr">46</xref>], ESRT, NGSwin, DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>], CFIN [<xref rid="B21-sensors-25-05275" ref-type="bibr">21</xref>], and HCFormer [<xref rid="B47-sensors-25-05275" ref-type="bibr">47</xref>], and their results are shown in <xref rid="sensors-25-05275-t002" ref-type="table">Table 2</xref>. Compared with HCFormer, MOION achieves 16 optimal and 6 suboptimal out of 24 metrics, while HCFormer achieves 7 optimal and 5 suboptimal. The total number of optimal and suboptimal values of MOION is more than that of HCFormer, and the number of parameters required for the network at <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> scales is reduced by 10.4%, 10.6%, and 11% respectively. Compared with SwinIR-light, the total number of most and second best of MOION is still more than SwinIR-light, and the number of parameters and computation required by the network is lower than that of SwinIR-light in all scales. Compared with the rest of the networks, MOION achieves the optimum in most of the metrics, which further validates the superiority of MOION in image reconstruction.</p></sec></sec><sec id="sec4dot4-sensors-25-05275"><title>4.4. Ablation Studies</title><p>In order to investigate the effect of the main blocks of the network on the performance, we conduct ablation experiments on WTConv-5, which provides self-calibrating weights, CSOB, a chunked spatial optimization block, and MSHIRB, a multiscale high-frequency information refinement block, respectively. The test dataset is Urban100 with complex texture and a scale factor of <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The network with the three blocks removed is used as the baseline, and each block is added to the baseline for reconstruction, respectively, and the results are shown in <xref rid="sensors-25-05275-t003" ref-type="table">Table 3</xref>. From the results, it can be seen that when only WTConv-5 is used; compared to the baseline, PSNR is improved by 0.12 dB, SSIM is improved by 0.0036, while the number of parameters and computation are only increased by 38 K and 0.85 G, respectively. When only CSOB and MSHIRB are used, the network also obtains a large performance improvement at the cost of a small increase in overhead. This proves the effectiveness of each block. When only any two blocks are used, the network obtains a greater performance improvement compared to using each block individually, and when all three blocks are used simultaneously, the PSNR improves by 0.21 dB compared to the baseline. The SSIM improves by 0.0076, optimizing the performance with only a small increase in the network complexity. This demonstrates the synergistic effects between the blocks.</p><p>MSHIRB mainly consists of multiplicity sampling and multi-branch feature extraction block MBFEB. In order to investigate the effect of the two blocks on the network performance, the removal of the two blocks is used as the baseline, and the experiments are conducted by adding each block to the baseline, respectively, and the results are shown in <xref rid="sensors-25-05275-t004" ref-type="table">Table 4</xref>. When only multiplicity sampling is used, the network improves by 0.02 dB compared to the baseline PSNR and 0.0008 for SSIM. When only MBFEB is used, the network improves by 0.07 dB compared to the baseline PSNR and 0.0023 for SSIM. When both are used at the same time, the performance is optimized. To visually demonstrate the effects of multiplicity sampling and MBFEB on image reconstruction, we conducted image restoration using three network configurations: (1) employing only multiplicity sampling, (2) utilizing solely MBFEB, and (3) integrating both blocks simultaneously. The reconstructed images and their corresponding high-frequency information are shown in <xref rid="sensors-25-05275-f011" ref-type="fig">Figure 11</xref>a. As demonstrated, both MBFEB and multiplicity sampling contribute to capturing more high-frequency components. However, when used individually, each block still leaves some artifacts in the reconstructed images. The combined use of both blocks enables more accurate high-frequency information recovery, consequently yielding optimal reconstruction quality. This demonstrates the complementary nature of multiplicity sampling for high-frequency information and the ability of MBFEB to refine the features, while their synergistic action is required to better accomplish the function of high-frequency information refinement.</p><p>To validate the rationale of SCHIEB&#8217;s dual-branch architecture, we conducted ablation studies by retaining only the self-calibrating branch (SCB) or auxiliary branch (AB) individually. As shown in <xref rid="sensors-25-05275-t005" ref-type="table">Table 5</xref>, the dual-branch configuration achieves optimal performance, with SCB outperforming AB in single-branch tests. The SCB uses wavelet convolution to capture large-scale pixel relationships. It generates calibration weights that dynamically adjust regional feature representations, emphasizing high-frequency components and significantly enhancing high-frequency information. In contrast, the AB primarily preserves local salient features through max-pooling operations but lacks self-calibrating attention mechanisms for high-frequency compensation. To visually compare the effects of each branch, we performed image reconstruction using individual branches (SCB or AB) and dual branches. The reconstructed images and their high-frequency components are shown in <xref rid="sensors-25-05275-f011" ref-type="fig">Figure 11</xref>b,c. It can be observed that SCB helps the network reconstruct more high-frequency components, while AB compensates for partial high-frequency information. The dual-branch configuration achieves the highest reconstruction quality. This structural comparison shows that the dual-branch design combines their complementary advantages synergistically. It adaptively enhances high-frequency information and improves the ability to recover detailed textures.</p><p>To evaluate the impact of convolution kernel sizes in MBFEB, we tested six configurations combining wavelet convolution (WTConv) and dual strip convolutions (DW), where &#8220;5-7-7&#8221; denotes WTConv-5, DW-1&#215;7, and DW-7&#215;1. As shown in <xref rid="sensors-25-05275-t006" ref-type="table">Table 6</xref>, larger kernels in both wavelet and strip convolutions consistently enhance network performance while maintaining moderate parameter and computational costs. Specifically, expanding the wavelet convolution kernel improves the capture of broad shape features, while larger strip convolution kernels strengthen modeling of long-range horizontal/vertical texture dependencies. Notably, since each MBFEB branch processes only a subset of channel features, the 5&#8211;7&#8211;7 configuration achieves optimal performance with minimal computational overhead.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05275"><title>5. Conclusions</title><p>This paper proposes a lightweight image super-resolution reconstruction network based on multi-order information optimization. The core of the network lies in the enhancement and refinement of high-frequency information. Through multiple stages, it fully extracts high-frequency features and removes redundant information to improve detail restoration. For high-frequency enhancement, we design a SCHIEB that regulates pixel-wise response intensities through learnable calibration weights. This block incorporates an auxiliary branch with chunked space optimization to adaptively enhance high-frequency components while preserving local structural details. In the refinement stage, we propose an MSHIRB. It first captures multi-scale information via multiplicity sampling, and then uses a multi-branch structure with wavelet and band convolutions to extract diverse detail features, further refining high-frequency information. Together, these blocks address the limitations of traditional distillation networks in high-frequency recovery and detail reconstruction. Experimental results show that the proposed network achieves competitive quantitative metrics and visual reconstruction quality while maintaining good balance between complexity and performance.</p></sec></body><back><ack><title>Acknowledgments</title><p>We thank the China University of Mining and Technology and Dalian University of Technology.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, L.L.; formal analysis, S.G.; data curation, S.G., L.L., W.C., H.J. and H.G.; writing&#8212;original draft preparation, S.G., L.L., W.C., H.J. and H.G.; writing&#8212;review and editing, S.G., L.L., W.C., H.J. and H.G. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05275"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lian</surname><given-names>H.</given-names></name></person-group><article-title>Flexible alignment super-resolution network for multi-contrast magnetic resonance imaging</article-title><source>IEEE Trans. Multimed.</source><year>2023</year><volume>26</volume><fpage>5159</fpage><lpage>5169</lpage><pub-id pub-id-type="doi">10.1109/TMM.2023.3330085</pub-id></element-citation></ref><ref id="B2-sensors-25-05275"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>E.</given-names></name></person-group><article-title>Medical image super-resolution based on semantic perception transfer learning</article-title><source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source><year>2022</year><volume>20</volume><fpage>2598</fpage><lpage>2609</lpage><pub-id pub-id-type="doi">10.1109/TCBB.2022.3212343</pub-id><pub-id pub-id-type="pmid">36201418</pub-id></element-citation></ref><ref id="B3-sensors-25-05275"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Super-resolution reconstruction of lightweight mine images by fusing hierarchical features and attention mechanisms</article-title><source>J. Instrum.</source><year>2022</year><volume>43</volume><fpage>73</fpage><lpage>84</lpage></element-citation></ref><ref id="B4-sensors-25-05275"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Lightweight super resolution method based on blueprint separable convolution for mine image</article-title><source>J. China Coal Soc.</source><year>2024</year><volume>49</volume><fpage>4038</fpage><lpage>4050</lpage></element-citation></ref><ref id="B5-sensors-25-05275"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Asad</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>D.</given-names></name></person-group><article-title>Single image detail enhancement via metropolis theorem</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>36329</fpage><lpage>36353</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-16914-5</pub-id></element-citation></ref><ref id="B6-sensors-25-05275"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name></person-group><article-title>Image Super-Resolution Algorithms Based on Deep Feature Differentiation Network</article-title><source>J. Electron. Inf.</source><year>2024</year><volume>46</volume><fpage>1033</fpage><lpage>1042</lpage></element-citation></ref><ref id="B7-sensors-25-05275"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name></person-group><article-title>A novel learnable interpolation approach for scale-arbitrary image super-resolution</article-title><source>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence</source><conf-loc>Macao, China</conf-loc><conf-date>19&#8211;25 August 2023</conf-date><fpage>564</fpage><lpage>572</lpage></element-citation></ref><ref id="B8-sensors-25-05275"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>G.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>P.</given-names></name></person-group><article-title>Adaptive nonnegative sparse representation for hyperspectral image super-resolution</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2021</year><volume>14</volume><fpage>4267</fpage><lpage>4283</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2021.3072044</pub-id></element-citation></ref><ref id="B9-sensors-25-05275"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name></person-group><article-title>Image super-resolution reconstruction algorithm with adaptive aggregation of hierarchical information</article-title><source>J. Comput. Eng. Appl.</source><year>2024</year><volume>60</volume><fpage>221</fpage><lpage>231</lpage></element-citation></ref><ref id="B10-sensors-25-05275"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Du</surname><given-names>J.</given-names></name></person-group><article-title>3D-MRI Super-Resolution Algorithm Fusing Attention and Dilated Encoder-Decoder</article-title><source>J. Comput. Eng. Appl.</source><year>2024</year><volume>60</volume><fpage>228</fpage><lpage>236</lpage></element-citation></ref><ref id="B11-sensors-25-05275"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Learning a deep convolutional network for image super-resolution</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2014: 13th European Conference</source><conf-loc>Zurich, Switzerland</conf-loc><conf-date>6&#8211;12 September 2014</conf-date><comment>Proceedings, Part IV 13</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2014</year><fpage>184</fpage><lpage>199</lpage></element-citation></ref><ref id="B12-sensors-25-05275"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Accelerating the super-resolution convolutional neural network</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#8211;14 October 2016</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2016</year><fpage>391</fpage><lpage>407</lpage></element-citation></ref><ref id="B13-sensors-25-05275"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Caballero</surname><given-names>J.</given-names></name><name name-style="western"><surname>Husz&#225;r</surname><given-names>F.</given-names></name><name name-style="western"><surname>Totz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Aitken</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Bishop</surname><given-names>R.</given-names></name><name name-style="western"><surname>Rueckert</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>1874</fpage><lpage>1883</lpage></element-citation></ref><ref id="B14-sensors-25-05275"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.K.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>K.M.</given-names></name></person-group><article-title>Accurate image super-resolution using very deep convolutional networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>1646</fpage><lpage>1654</lpage></element-citation></ref><ref id="B15-sensors-25-05275"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>B.</given-names></name><name name-style="western"><surname>Son</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name><name name-style="western"><surname>Nah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mu Lee</surname><given-names>K.</given-names></name></person-group><article-title>Enhanced deep residual networks for single image super-resolution</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>136</fpage><lpage>144</lpage></element-citation></ref><ref id="B16-sensors-25-05275"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>G.</given-names></name></person-group><article-title>Residual feature distillation network for lightweight image super-resolution</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2020 Workshops</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><comment>Proceedings, Part III 16</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>41</fpage><lpage>55</lpage></element-citation></ref><ref id="B17-sensors-25-05275"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>Image super-resolution via deep recursive residual network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>3147</fpage><lpage>3155</lpage></element-citation></ref><ref id="B18-sensors-25-05275"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Dipnet: Efficiency distillation and iterative pruning for image super-resolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date><fpage>1692</fpage><lpage>1701</lpage></element-citation></ref><ref id="B19-sensors-25-05275"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name></person-group><article-title>Spatially-adaptive feature modulation for efficient image super-resolution</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>13190</fpage><lpage>13199</lpage></element-citation></ref><ref id="B20-sensors-25-05275"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>T.</given-names></name></person-group><article-title>Transformer for single image super-resolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>457</fpage><lpage>466</lpage></element-citation></ref><ref id="B21-sensors-25-05275"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>G.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>G.J.</given-names></name></person-group><article-title>Cross-receptive focused inference network for lightweight image super-resolution</article-title><source>IEEE Trans. Multimed.</source><year>2023</year><volume>26</volume><fpage>864</fpage><lpage>877</lpage><pub-id pub-id-type="doi">10.1109/TMM.2023.3272474</pub-id></element-citation></ref><ref id="B22-sensors-25-05275"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hui</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name></person-group><article-title>Fast and accurate single image super-resolution via information distillation network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>723</fpage><lpage>731</lpage></element-citation></ref><ref id="B23-sensors-25-05275"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hui</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Lightweight image super-resolution with information multi-distillation network</article-title><source>Proceedings of the 27th ACM International Conference on Multimedia</source><conf-loc>Nice, France</conf-loc><conf-date>21&#8211;25 October 2019</conf-date><fpage>2024</fpage><lpage>2032</lpage></element-citation></ref><ref id="B24-sensors-25-05275"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kong</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>He</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>L.</given-names></name></person-group><article-title>Residual local feature network for efficient super-resolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>766</fpage><lpage>776</lpage></element-citation></ref><ref id="B25-sensors-25-05275"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>C.</given-names></name></person-group><article-title>Blueprint separable residual network for efficient image super-resolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>833</fpage><lpage>843</lpage></element-citation></ref><ref id="B26-sensors-25-05275"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>G.</given-names></name></person-group><article-title>Residual feature aggregation network for image super-resolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>2359</fpage><lpage>2368</lpage></element-citation></ref><ref id="B27-sensors-25-05275"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Finder</surname><given-names>S.</given-names></name><name name-style="western"><surname>Amoyal</surname><given-names>R.</given-names></name><name name-style="western"><surname>Treister</surname><given-names>E.</given-names></name><name name-style="western"><surname>Freifeld</surname><given-names>O.</given-names></name></person-group><article-title>Wavelet Convolutions for Large Receptive Fields</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2407.05848</pub-id><pub-id pub-id-type="arxiv">2407.05848</pub-id></element-citation></ref><ref id="B28-sensors-25-05275"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1412.6980</pub-id></element-citation></ref><ref id="B29-sensors-25-05275"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name><name name-style="western"><surname>Agustsson</surname><given-names>E.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Ntire 2017 challenge on single image super-resolution: Methods and results</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>114</fpage><lpage>125</lpage></element-citation></ref><ref id="B30-sensors-25-05275"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bevilacqua</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roumy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Guillemot</surname><given-names>C.</given-names></name><name name-style="western"><surname>Alberi-Morel</surname><given-names>M.L.</given-names></name></person-group><source>Low-Complexity Single-Image Super-Resolution Based On Nonnegative Neighbor Embedding</source><publisher-name>BMVA Press</publisher-name><publisher-loc>Durham, UK</publisher-loc><year>2012</year></element-citation></ref><ref id="B31-sensors-25-05275"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zeyde</surname><given-names>R.</given-names></name><name name-style="western"><surname>Elad</surname><given-names>M.</given-names></name><name name-style="western"><surname>Protter</surname><given-names>M.</given-names></name></person-group><article-title>On single image scale-up using sparse-representations</article-title><source>Proceedings of the Curves and Surfaces: 7th International Conference</source><conf-loc>Avignon, France</conf-loc><conf-date>24&#8211;30 June 2010</conf-date><comment>Revised Selected Papers 7</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2012</year><fpage>711</fpage><lpage>730</lpage></element-citation></ref><ref id="B32-sensors-25-05275"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Martin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Fowlkes</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tal</surname><given-names>D.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>J.</given-names></name></person-group><article-title>A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</article-title><source>Proceedings of the Eighth IEEE International Conference on Computer Vision. ICCV 2001</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>7&#8211;14 July 2001</conf-date><volume>Volume 2</volume><fpage>416</fpage><lpage>423</lpage></element-citation></ref><ref id="B33-sensors-25-05275"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ahuja</surname><given-names>N.</given-names></name></person-group><article-title>Single image super-resolution from transformed self-exemplars</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>5197</fpage><lpage>5206</lpage></element-citation></ref><ref id="B34-sensors-25-05275"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bovik</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Sheikh</surname><given-names>H.R.</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>E.P.</given-names></name></person-group><article-title>Image quality assessment: From error visibility to structural similarity</article-title><source>IEEE Trans. Image Process.</source><year>2004</year><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="B35-sensors-25-05275"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name></person-group><article-title>Differentiable neural architecture search for extremely lightweight image super-resolution</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2022</year><volume>33</volume><fpage>2672</fpage><lpage>2682</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2022.3230824</pub-id></element-citation></ref><ref id="B36-sensors-25-05275"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>K.</given-names></name><name name-style="western"><surname>Soh</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>N.I.</given-names></name></person-group><article-title>A dynamic residual self-attention network for lightweight single image super-resolution</article-title><source>IEEE Trans. Multimed.</source><year>2021</year><volume>25</volume><fpage>907</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1109/TMM.2021.3134172</pub-id></element-citation></ref><ref id="B37-sensors-25-05275"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>G.</given-names></name></person-group><article-title>Hybrid attention feature refinement network for lightweight image super-resolution in metaverse immersive display</article-title><source>IEEE Trans. Consum. Electron.</source><year>2023</year><volume>70</volume><fpage>3232</fpage><lpage>3244</lpage><pub-id pub-id-type="doi">10.1109/TCE.2023.3329813</pub-id></element-citation></ref><ref id="B38-sensors-25-05275"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name></person-group><article-title>Osffnet: Omni-stage feature fusion network for lightweight image super-resolution</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>26&#8211;27 February 2024</conf-date><volume>Volume 38</volume><fpage>5660</fpage><lpage>5668</lpage></element-citation></ref><ref id="B39-sensors-25-05275"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>W.</given-names></name></person-group><article-title>Hierarchical similarity learning for aliasing suppression image super-resolution</article-title><source>IEEE Trans. Neural Networks Learn. Syst.</source><year>2022</year><volume>35</volume><fpage>2759</fpage><lpage>2771</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2022.3191674</pub-id><pub-id pub-id-type="pmid">35930518</pub-id></element-citation></ref><ref id="B40-sensors-25-05275"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yasir</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ullah</surname><given-names>I.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>C.</given-names></name></person-group><article-title>Depthwise channel attention network (DWCAN): An efficient and lightweight model for single image super-resolution and metaverse gaming</article-title><source>Expert Syst.</source><year>2024</year><volume>41</volume><fpage>e13516</fpage><pub-id pub-id-type="doi">10.1111/exsy.13516</pub-id></element-citation></ref><ref id="B41-sensors-25-05275"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ning</surname><given-names>K.</given-names></name></person-group><article-title>MSWSR: A Lightweight Multi-Scale Feature Selection Network for Single-Image Super-Resolution Methods</article-title><source>Symmetry</source><year>2025</year><volume>17</volume><elocation-id>431</elocation-id><pub-id pub-id-type="doi">10.3390/sym17030431</pub-id></element-citation></ref><ref id="B42-sensors-25-05275"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name><name name-style="western"><surname>Cong</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name></person-group><article-title>Srconvnet: A transformer-style convnet for lightweight image super-resolution</article-title><source>Int. J. Comput. Vis.</source><year>2025</year><volume>133</volume><fpage>173</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1007/s11263-024-02147-y</pub-id></element-citation></ref><ref id="B43-sensors-25-05275"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name></person-group><article-title>Lattice network for lightweight image restoration</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>4826</fpage><lpage>4842</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3194090</pub-id><pub-id pub-id-type="pmid">35914039</pub-id></element-citation></ref><ref id="B44-sensors-25-05275"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Choi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>N-gram in swin transformers for efficient lightweight image super-resolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date><fpage>2071</fpage><lpage>2081</lpage></element-citation></ref><ref id="B45-sensors-25-05275"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name></person-group><article-title>Swinir: Image restoration using swin transformer</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>1833</fpage><lpage>1844</lpage></element-citation></ref><ref id="B46-sensors-25-05275"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>T.</given-names></name></person-group><article-title>Lightweight bimodal network for single-image super-resolution via symmetric CNN and recursive transformer</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2204.13286</pub-id></element-citation></ref><ref id="B47-sensors-25-05275"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ke</surname><given-names>Y.</given-names></name></person-group><article-title>Hybrid convolution-transformer for lightweight single image super-resolution</article-title><source>Proceedings of the ICASSP 2024&#8212;2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>14&#8211;19 April 2024</conf-date><fpage>2395</fpage><lpage>2399</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05275-f001" orientation="portrait"><label>Figure 1</label><caption><p>Multi-order information optimization network structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g001.jpg"/></fig><fig position="float" id="sensors-25-05275-f002" orientation="portrait"><label>Figure 2</label><caption><p>Multi-order information optimization block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g002.jpg"/></fig><fig position="float" id="sensors-25-05275-f003" orientation="portrait"><label>Figure 3</label><caption><p>Self-calibrating high-frequency information enhancement block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g003.jpg"/></fig><fig position="float" id="sensors-25-05275-f004" orientation="portrait"><label>Figure 4</label><caption><p>Chunked space optimization block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g004.jpg"/></fig><fig position="float" id="sensors-25-05275-f005" orientation="portrait"><label>Figure 5</label><caption><p>Multi-scale high-frequency information refinement block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g005.jpg"/></fig><fig position="float" id="sensors-25-05275-f006" orientation="portrait"><label>Figure 6</label><caption><p>Multi-branch feature extraction block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g006.jpg"/></fig><fig position="float" id="sensors-25-05275-f007" orientation="portrait"><label>Figure 7</label><caption><p>Visual comparison of different networks on B100: 86000.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g007.jpg"/></fig><fig position="float" id="sensors-25-05275-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visual comparison of different networks on B100: 210088.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g008.jpg"/></fig><fig position="float" id="sensors-25-05275-f009" orientation="portrait"><label>Figure 9</label><caption><p>Visual comparison of different networks on Urban100: img058.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g009.jpg"/></fig><fig position="float" id="sensors-25-05275-f010" orientation="portrait"><label>Figure 10</label><caption><p>Visual comparison of different networks on Urban100: img015.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g010.jpg"/></fig><fig position="float" id="sensors-25-05275-f011" orientation="portrait"><label>Figure 11</label><caption><p>Reconstructed images and their high-frequency component images.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05275-g011.jpg"/></fig><table-wrap position="float" id="sensors-25-05275-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05275-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of metrics under the baseline dataset when the scale factor is &#215;2, &#215;3, and &#215;4. <bold>Bold</bold> is optimal, <underline>underlined</underline> is sub-optimal, and - indicates that the network was not tested for this condition.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Scale</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Params</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Set5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Set14</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">B100</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Urban100</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th></tr></thead><tbody><tr><td rowspan="14" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;2</td><td align="center" valign="middle" rowspan="1" colspan="1">EDSR-baseline [<xref rid="B15-sensors-25-05275" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1370 K</td><td align="center" valign="middle" rowspan="1" colspan="1">316.3 G</td><td align="center" valign="middle" rowspan="1" colspan="1">37.99/0.9604</td><td align="center" valign="middle" rowspan="1" colspan="1">33.57/0.9175</td><td align="center" valign="middle" rowspan="1" colspan="1">32.16/0.8994</td><td align="center" valign="middle" rowspan="1" colspan="1">31.98/0.9272</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IMDN [<xref rid="B23-sensors-25-05275" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">694 K</td><td align="center" valign="middle" rowspan="1" colspan="1">186.7 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.00/0.9605</td><td align="center" valign="middle" rowspan="1" colspan="1">33.63/0.9177</td><td align="center" valign="middle" rowspan="1" colspan="1">32.19/0.8996</td><td align="center" valign="middle" rowspan="1" colspan="1">32.17/0.9283</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RFDN [<xref rid="B16-sensors-25-05275" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">534 K</td><td align="center" valign="middle" rowspan="1" colspan="1">95.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.05/0.9606</td><td align="center" valign="middle" rowspan="1" colspan="1">33.68/0.9184</td><td align="center" valign="middle" rowspan="1" colspan="1">32.16/0.8994</td><td align="center" valign="middle" rowspan="1" colspan="1">32.12/0.9278</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BSRN [<xref rid="B25-sensors-25-05275" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">332 K</td><td align="center" valign="middle" rowspan="1" colspan="1">73.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.10/<underline>0.9610</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">33.74/0.9193</td><td align="center" valign="middle" rowspan="1" colspan="1">32.24/0.9006</td><td align="center" valign="middle" rowspan="1" colspan="1">32.34/0.9303</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAFMN [<xref rid="B19-sensors-25-05275" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">228 K</td><td align="center" valign="middle" rowspan="1" colspan="1">52.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.00/0.9605</td><td align="center" valign="middle" rowspan="1" colspan="1">33.54/0.9177</td><td align="center" valign="middle" rowspan="1" colspan="1">32.16/0.8995</td><td align="center" valign="middle" rowspan="1" colspan="1">31.84/0.9256</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DLSR [<xref rid="B35-sensors-25-05275" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">322 K</td><td align="center" valign="middle" rowspan="1" colspan="1">68.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.04/0.9606</td><td align="center" valign="middle" rowspan="1" colspan="1">33.67/0.9183</td><td align="center" valign="middle" rowspan="1" colspan="1">32.21/0.9002</td><td align="center" valign="middle" rowspan="1" colspan="1">32.26/0.9297</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1190 K</td><td align="center" valign="middle" rowspan="1" colspan="1">274.6 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>38.14</underline>/<bold>0.9611</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">33.75/0.9188</td><td align="center" valign="middle" rowspan="1" colspan="1">32.25/0.9010</td><td align="center" valign="middle" rowspan="1" colspan="1">32.46/0.9317</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HAFRN [<xref rid="B37-sensors-25-05275" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">496 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">38.05/0.9606</td><td align="center" valign="middle" rowspan="1" colspan="1">33.66/0.9187</td><td align="center" valign="middle" rowspan="1" colspan="1">32.21/0.8999</td><td align="center" valign="middle" rowspan="1" colspan="1">32.20/0.9289</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OSFFNet [<xref rid="B38-sensors-25-05275" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">516 K</td><td align="center" valign="middle" rowspan="1" colspan="1">83.2 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.11/<underline>0.9610</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">33.72/0.9190</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>32.29</underline>/<underline>0.9012</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>32.67</underline>/<underline>0.9331</underline></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HSRNet [<xref rid="B39-sensors-25-05275" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1260 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">38.07/0.9607</td><td align="center" valign="middle" rowspan="1" colspan="1">33.78/0.9197</td><td align="center" valign="middle" rowspan="1" colspan="1">32.26/0.9006</td><td align="center" valign="middle" rowspan="1" colspan="1">32.53/0.9320</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DWCAN [<xref rid="B40-sensors-25-05275" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">401 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">37.60/0.9598</td><td align="center" valign="middle" rowspan="1" colspan="1">33.33/0.9160</td><td align="center" valign="middle" rowspan="1" colspan="1">32.07/0.8987</td><td align="center" valign="middle" rowspan="1" colspan="1">31.95/0.9267</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MSWSR [<xref rid="B41-sensors-25-05275" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">312 K</td><td align="center" valign="middle" rowspan="1" colspan="1">243.3 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.01/<underline>0.9610</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">33.71/0.9193</td><td align="center" valign="middle" rowspan="1" colspan="1">32.22/0.9003</td><td align="center" valign="middle" rowspan="1" colspan="1">32.29/0.9301</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SRConvNet-L [<xref rid="B42-sensors-25-05275" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">885 K</td><td align="center" valign="middle" rowspan="1" colspan="1">160 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>38.14</underline>/<underline>0.9610</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>33.81</underline>/<underline>0.9199</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">32.28/0.9010</td><td align="center" valign="middle" rowspan="1" colspan="1">32.59/0.9321</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOION</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">816 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">163.74 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>38.16</bold>/<bold>0.9611</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>33.92</bold>/<bold>0.9204</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>32.32</bold>/<bold>0.9014</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>32.69</bold>/<bold>0.9339</bold></td></tr><tr><td rowspan="14" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;3</td><td align="center" valign="middle" rowspan="1" colspan="1">EDSR-baseline [<xref rid="B15-sensors-25-05275" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1555 K</td><td align="center" valign="middle" rowspan="1" colspan="1">160.2 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.37/0.9270</td><td align="center" valign="middle" rowspan="1" colspan="1">30.28/0.8417</td><td align="center" valign="middle" rowspan="1" colspan="1">29.09/0.8052</td><td align="center" valign="middle" rowspan="1" colspan="1">28.15/0.8527</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IMDN [<xref rid="B23-sensors-25-05275" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">703 K</td><td align="center" valign="middle" rowspan="1" colspan="1">84.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.36/0.9270</td><td align="center" valign="middle" rowspan="1" colspan="1">30.32/0.8417</td><td align="center" valign="middle" rowspan="1" colspan="1">29.09/0.8046</td><td align="center" valign="middle" rowspan="1" colspan="1">28.17/0.8519</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RFDN [<xref rid="B16-sensors-25-05275" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">541 K</td><td align="center" valign="middle" rowspan="1" colspan="1">42.2 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.41/0.9273</td><td align="center" valign="middle" rowspan="1" colspan="1">30.34/0.8420</td><td align="center" valign="middle" rowspan="1" colspan="1">29.09/0.8050</td><td align="center" valign="middle" rowspan="1" colspan="1">28.21/0.8525</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BSRN [<xref rid="B25-sensors-25-05275" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">340 K</td><td align="center" valign="middle" rowspan="1" colspan="1">33.3 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.46/0.9277</td><td align="center" valign="middle" rowspan="1" colspan="1">30.47/0.8449</td><td align="center" valign="middle" rowspan="1" colspan="1">29.18/0.8068</td><td align="center" valign="middle" rowspan="1" colspan="1">28.39/0.8567</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAFMN [<xref rid="B19-sensors-25-05275" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">233 K</td><td align="center" valign="middle" rowspan="1" colspan="1">23.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.34/0.9267</td><td align="center" valign="middle" rowspan="1" colspan="1">30.33/0.8418</td><td align="center" valign="middle" rowspan="1" colspan="1">29.08/0.8048</td><td align="center" valign="middle" rowspan="1" colspan="1">27.95/0.8474</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DLSR [<xref rid="B35-sensors-25-05275" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">329 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">34.49/0.9279</td><td align="center" valign="middle" rowspan="1" colspan="1">30.39/0.8428</td><td align="center" valign="middle" rowspan="1" colspan="1">29.13/0.8061</td><td align="center" valign="middle" rowspan="1" colspan="1">28.26/0.8548</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1290 K</td><td align="center" valign="middle" rowspan="1" colspan="1">133.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>34.59</underline>/0.9286</td><td align="center" valign="middle" rowspan="1" colspan="1">30.42/0.8443</td><td align="center" valign="middle" rowspan="1" colspan="1">29.18/0.8079</td><td align="center" valign="middle" rowspan="1" colspan="1">28.52/0.8593</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HAFRN [<xref rid="B37-sensors-25-05275" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">505 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">34.45/0.9276</td><td align="center" valign="middle" rowspan="1" colspan="1">30.40/0.8433</td><td align="center" valign="middle" rowspan="1" colspan="1">29.12/0.8058</td><td align="center" valign="middle" rowspan="1" colspan="1">28.16/0.8528</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OSFFNet [<xref rid="B38-sensors-25-05275" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">524 K</td><td align="center" valign="middle" rowspan="1" colspan="1">37.8 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.58/0.9287</td><td align="center" valign="middle" rowspan="1" colspan="1">30.48/0.8450</td><td align="center" valign="middle" rowspan="1" colspan="1">29.21/0.8080</td><td align="center" valign="middle" rowspan="1" colspan="1">28.49/0.8595</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HSRNet [<xref rid="B39-sensors-25-05275" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">34.47/0.9278</td><td align="center" valign="middle" rowspan="1" colspan="1">30.40/0.8435</td><td align="center" valign="middle" rowspan="1" colspan="1">29.15/0.8066</td><td align="center" valign="middle" rowspan="1" colspan="1">28.42/0.8579</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DWCAN [<xref rid="B40-sensors-25-05275" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">401 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">34.29/0.9258</td><td align="center" valign="middle" rowspan="1" colspan="1">30.29/0.8410</td><td align="center" valign="middle" rowspan="1" colspan="1">29.00/0.8027</td><td align="center" valign="middle" rowspan="1" colspan="1">28.18/0.8521</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MSWSR [<xref rid="B41-sensors-25-05275" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">307 K</td><td align="center" valign="middle" rowspan="1" colspan="1">249.6 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.40/0.9277</td><td align="center" valign="middle" rowspan="1" colspan="1">30.35/0.8437</td><td align="center" valign="middle" rowspan="1" colspan="1">29.12/0.8067</td><td align="center" valign="middle" rowspan="1" colspan="1">28.22/0.8548</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SRConvNet-L [<xref rid="B42-sensors-25-05275" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">906 K</td><td align="center" valign="middle" rowspan="1" colspan="1">74 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>34.59</underline>/<underline>0.9288</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>30.50</underline>/<underline>0.8455</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>29.22</underline>/<underline>0.8081</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.56</underline>/<underline>0.8600</underline></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOION</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">825 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.72 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>34.69</bold>/<bold>0.9294</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>30.57</bold>/<bold>0.8467</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>29.24</bold>/<bold>0.8091</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>28.68</bold>/<bold>0.8629</bold></td></tr><tr><td rowspan="14" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;4</td><td align="center" valign="middle" rowspan="1" colspan="1">EDSR-baseline [<xref rid="B15-sensors-25-05275" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1518 K</td><td align="center" valign="middle" rowspan="1" colspan="1">114.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.09/0.8938</td><td align="center" valign="middle" rowspan="1" colspan="1">28.58/0.7813</td><td align="center" valign="middle" rowspan="1" colspan="1">27.57/0.7357</td><td align="center" valign="middle" rowspan="1" colspan="1">26.04/0.7849</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IMDN [<xref rid="B23-sensors-25-05275" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">715 K</td><td align="center" valign="middle" rowspan="1" colspan="1">48.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.21/0.8948</td><td align="center" valign="middle" rowspan="1" colspan="1">28.58/0.7811</td><td align="center" valign="middle" rowspan="1" colspan="1">27.56/0.7353</td><td align="center" valign="middle" rowspan="1" colspan="1">26.04/0.7838</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RFDN [<xref rid="B16-sensors-25-05275" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">550 K</td><td align="center" valign="middle" rowspan="1" colspan="1">23.9 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.24/0.8952</td><td align="center" valign="middle" rowspan="1" colspan="1">28.61/0.7819</td><td align="center" valign="middle" rowspan="1" colspan="1">27.57/0.7360</td><td align="center" valign="middle" rowspan="1" colspan="1">26.11/0.7858</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BSRN [<xref rid="B25-sensors-25-05275" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">352 K</td><td align="center" valign="middle" rowspan="1" colspan="1">19.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.35/0.8966</td><td align="center" valign="middle" rowspan="1" colspan="1">28.73/0.7847</td><td align="center" valign="middle" rowspan="1" colspan="1">27.65/0.7387</td><td align="center" valign="middle" rowspan="1" colspan="1">26.27/0.7908</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAFMN [<xref rid="B19-sensors-25-05275" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">240 K</td><td align="center" valign="middle" rowspan="1" colspan="1">14.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.18/0.8948</td><td align="center" valign="middle" rowspan="1" colspan="1">28.60/0.7813</td><td align="center" valign="middle" rowspan="1" colspan="1">27.58/0.7359</td><td align="center" valign="middle" rowspan="1" colspan="1">25.97/0.7809</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DLSR [<xref rid="B35-sensors-25-05275" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">338 K</td><td align="center" valign="middle" rowspan="1" colspan="1">20 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.33/0.8963</td><td align="center" valign="middle" rowspan="1" colspan="1">28.68/0.7832</td><td align="center" valign="middle" rowspan="1" colspan="1">27.61/0.7374</td><td align="center" valign="middle" rowspan="1" colspan="1">26.19/0.7892</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1270 K</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.34/0.8960</td><td align="center" valign="middle" rowspan="1" colspan="1">28.65/0.7841</td><td align="center" valign="middle" rowspan="1" colspan="1">27.63/0.7390</td><td align="center" valign="middle" rowspan="1" colspan="1">26.33/0.7936</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HAFRN [<xref rid="B37-sensors-25-05275" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">517 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">32.24/0.8953</td><td align="center" valign="middle" rowspan="1" colspan="1">28.60/0.7816</td><td align="center" valign="middle" rowspan="1" colspan="1">27.58/0.7365</td><td align="center" valign="middle" rowspan="1" colspan="1">26.02/0.7849</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OSFFNet [<xref rid="B38-sensors-25-05275" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">537 K</td><td align="center" valign="middle" rowspan="1" colspan="1">22.0 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.39/<underline>0.8976</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">28.75/0.7852</td><td align="center" valign="middle" rowspan="1" colspan="1">27.66/0.7393</td><td align="center" valign="middle" rowspan="1" colspan="1">26.36/0.7950</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HSRNet [<xref rid="B39-sensors-25-05275" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1285 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">32.28/0.8960</td><td align="center" valign="middle" rowspan="1" colspan="1">28.68/0.7840</td><td align="center" valign="middle" rowspan="1" colspan="1">27.64/0.7388</td><td align="center" valign="middle" rowspan="1" colspan="1">26.28/0.7934</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DWCAN [<xref rid="B40-sensors-25-05275" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">401 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">32.20/0.8938</td><td align="center" valign="middle" rowspan="1" colspan="1">28.56/0.2809</td><td align="center" valign="middle" rowspan="1" colspan="1">27.41/0.7339</td><td align="center" valign="middle" rowspan="1" colspan="1">26.06/0.7851</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MSWSR [<xref rid="B41-sensors-25-05275" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">316 K</td><td align="center" valign="middle" rowspan="1" colspan="1">257.6 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.26/0.8966</td><td align="center" valign="middle" rowspan="1" colspan="1">28.67/0.7843</td><td align="center" valign="middle" rowspan="1" colspan="1">27.62/0.7379</td><td align="center" valign="middle" rowspan="1" colspan="1">26.17/0.7896</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SRConvNet-L [<xref rid="B42-sensors-25-05275" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">902 K</td><td align="center" valign="middle" rowspan="1" colspan="1">45 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>32.44</underline>/<underline>0.8976</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.77</underline>/<underline>0.7857</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>27.69</underline>/<underline>0.7402</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>26.47</underline>/<underline>0.7970</underline></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOION</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">837 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.13 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>32.51</bold>/<bold>0.8984</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>28.85</bold>/<bold>0.7874</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>27.72</bold>/<bold>0.7418</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>26.55</bold>/<bold>0.8005</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05275-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05275-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison with Transformer network metrics for scale factors of &#215;2, &#215;3, and &#215;4. <bold>Bold</bold> is optimal, <underline>underlined</underline> is sub-optimal, and - indicates that the network was not tested for this condition.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Scale</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Params</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Set5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Set14</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">B100</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Urban100</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th></tr></thead><tbody><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;2</td><td align="center" valign="middle" rowspan="1" colspan="1">SwinIR-light [<xref rid="B45-sensors-25-05275" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">878 K</td><td align="center" valign="middle" rowspan="1" colspan="1">195.6 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>38.14</underline>/<bold>0.9611</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>33.86</underline>/<underline>0.9206</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">32.31/0.9012</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>32.76</bold>/<underline>0.9340</underline></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LBNet [<xref rid="B46-sensors-25-05275" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ESRT [<xref rid="B20-sensors-25-05275" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">677 K</td><td align="center" valign="middle" rowspan="1" colspan="1">191.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.03/<underline>0.9600</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">33.75/0.9184</td><td align="center" valign="middle" rowspan="1" colspan="1">32.25/0.9001</td><td align="center" valign="middle" rowspan="1" colspan="1">32.58/0.9318</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">NGSwin [<xref rid="B44-sensors-25-05275" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">998 K</td><td align="center" valign="middle" rowspan="1" colspan="1">140.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">38.05/<underline>0.9610</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">33.79/0.9199</td><td align="center" valign="middle" rowspan="1" colspan="1">32.27/0.9008</td><td align="center" valign="middle" rowspan="1" colspan="1">32.53/0.9324</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1190 K</td><td align="center" valign="middle" rowspan="1" colspan="1">274.6 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>38.14</underline>/<bold>0.9611</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">33.75/0.9188</td><td align="center" valign="middle" rowspan="1" colspan="1">32.25/0.9010</td><td align="center" valign="middle" rowspan="1" colspan="1">32.46/0.9317</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CFIN [<xref rid="B21-sensors-25-05275" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">675 K</td><td align="center" valign="middle" rowspan="1" colspan="1">116.9 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>38.14</underline>/<underline>0.9610</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">33.80/0.9199</td><td align="center" valign="middle" rowspan="1" colspan="1">32.26/0.9006</td><td align="center" valign="middle" rowspan="1" colspan="1">32.48/0.9311</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HCFormer [<xref rid="B47-sensors-25-05275" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">911 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">38.06/0.9609</td><td align="center" valign="middle" rowspan="1" colspan="1">34.18/<bold>0.9253</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>32.45</bold>/<bold>0.9051</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">32.67/<bold>0.9359</bold></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOION</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">816 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">163.74 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>38.16</bold>/<bold>0.9611</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>33.92</bold>/0.9204</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>32.32</underline>
<bold>/</bold>
<underline>0.9014</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><underline>32.69</underline>/0.9339</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;3</td><td align="center" valign="middle" rowspan="1" colspan="1">SwinIR-light [<xref rid="B45-sensors-25-05275" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">886 K</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.62/<underline>0.9289</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">30.54/<underline>0.8463</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">29.20/0.8082</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.66</underline>/<underline>0.8624</underline></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LBNet [<xref rid="B46-sensors-25-05275" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">736 K</td><td align="center" valign="middle" rowspan="1" colspan="1">68.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.47/0.9277</td><td align="center" valign="middle" rowspan="1" colspan="1">30.38/0.8417</td><td align="center" valign="middle" rowspan="1" colspan="1">29.13/0.8061</td><td align="center" valign="middle" rowspan="1" colspan="1">28.42/0.8559</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ESRT [<xref rid="B20-sensors-25-05275" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">770 K</td><td align="center" valign="middle" rowspan="1" colspan="1">96.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.42/0.9268</td><td align="center" valign="middle" rowspan="1" colspan="1">30.43/0.8433</td><td align="center" valign="middle" rowspan="1" colspan="1">29.15/0.8063</td><td align="center" valign="middle" rowspan="1" colspan="1">28.46/0.8574</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">NGSwin [<xref rid="B44-sensors-25-05275" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1007 K</td><td align="center" valign="middle" rowspan="1" colspan="1">66.6 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.52/0.9282</td><td align="center" valign="middle" rowspan="1" colspan="1">30.53/0.8456</td><td align="center" valign="middle" rowspan="1" colspan="1">29.19/0.8078</td><td align="center" valign="middle" rowspan="1" colspan="1">28.52/0.8603</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1290 K</td><td align="center" valign="middle" rowspan="1" colspan="1">133.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">34.59/0.9286</td><td align="center" valign="middle" rowspan="1" colspan="1">30.42/0.8443</td><td align="center" valign="middle" rowspan="1" colspan="1">29.18/0.8079</td><td align="center" valign="middle" rowspan="1" colspan="1">28.52/0.8593</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CFIN [<xref rid="B21-sensors-25-05275" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">681 K</td><td align="center" valign="middle" rowspan="1" colspan="1">53.5 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>34.65</underline>/<underline>0.9289</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">30.45/0.8443</td><td align="center" valign="middle" rowspan="1" colspan="1">29.18/0.8071</td><td align="center" valign="middle" rowspan="1" colspan="1">28.49/0.8583</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HCFormer [<xref rid="B47-sensors-25-05275" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">923 K</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">34.51/0.9279</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>30.55</underline>/0.8459</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>29.31</bold>/<bold>0.8104</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">28.56/0.8613</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOION</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">825 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.72 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>34.69</bold>/<bold>0.9294</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>30.57</bold>/<bold>0.8467</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><underline>29.24</underline>/<underline>0.8091</underline></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>28.68</bold>/<bold>0.8629</bold></td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;4</td><td align="center" valign="middle" rowspan="1" colspan="1">SwinIR-light [<xref rid="B45-sensors-25-05275" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">897 K</td><td align="center" valign="middle" rowspan="1" colspan="1">49.6 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.44/0.8976</td><td align="center" valign="middle" rowspan="1" colspan="1">28.77/0.7858</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>27.69</underline>/0.7406</td><td align="center" valign="middle" rowspan="1" colspan="1">26.47/0.7980</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LBNet [<xref rid="B46-sensors-25-05275" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">742 K</td><td align="center" valign="middle" rowspan="1" colspan="1">38.9 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.29/0.8960</td><td align="center" valign="middle" rowspan="1" colspan="1">28.68/0.7832</td><td align="center" valign="middle" rowspan="1" colspan="1">27.62/0.7382</td><td align="center" valign="middle" rowspan="1" colspan="1">26.27/0.7906</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ESRT [<xref rid="B20-sensors-25-05275" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">751 K</td><td align="center" valign="middle" rowspan="1" colspan="1">67.7 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.19/0.8947</td><td align="center" valign="middle" rowspan="1" colspan="1">28.69/0.7833</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>27.69</underline>/0.7379</td><td align="center" valign="middle" rowspan="1" colspan="1">26.39/0.7962</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">NGSwin [<xref rid="B44-sensors-25-05275" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1019 K</td><td align="center" valign="middle" rowspan="1" colspan="1">36.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.33/0.8963</td><td align="center" valign="middle" rowspan="1" colspan="1">28.78/<underline>0.7859</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">27.66/0.7396</td><td align="center" valign="middle" rowspan="1" colspan="1">26.45/0.7963</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRSAN [<xref rid="B36-sensors-25-05275" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1270 K</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.34/0.8960</td><td align="center" valign="middle" rowspan="1" colspan="1">28.65/0.7841</td><td align="center" valign="middle" rowspan="1" colspan="1">27.63/0.7390</td><td align="center" valign="middle" rowspan="1" colspan="1">26.33/0.7936</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CFIN [<xref rid="B21-sensors-25-05275" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">699 K</td><td align="center" valign="middle" rowspan="1" colspan="1">31.2 G</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>32.49</underline>/<bold>0.8985</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">28.74/0.7849</td><td align="center" valign="middle" rowspan="1" colspan="1">27.68/0.7396</td><td align="center" valign="middle" rowspan="1" colspan="1">26.39/0.7946</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HCFormer [<xref rid="B47-sensors-25-05275" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">940 K</td><td align="center" valign="middle" rowspan="1" colspan="1">58.7 G</td><td align="center" valign="middle" rowspan="1" colspan="1">32.41/0.8976</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.84</underline>/<bold>0.7874</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">27.66/<underline>0.7413</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>26.51</underline>/<underline>0.7987</underline></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOION</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">837 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.13 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>32.51</bold>/<underline>0.8984</underline></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>28.85</bold>/<bold>0.7874</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>27.72</bold>/<bold>0.7418</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>26.55</bold>/<bold>0.8005</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05275-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05275-t003_Table 3</object-id><label>Table 3</label><caption><p>Impact of different modules on network performance. <bold>Bold</bold> is optimal, &#215; means adding this block, &#10004; means removing this block.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Scale</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">WTConv-5</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">CSOB</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">MSHIRB</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Params</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Urban100</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th></tr></thead><tbody><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">162 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.79 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.73/0.7734</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">200 K</td><td align="center" valign="middle" rowspan="1" colspan="1">9.64 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.85/0.7770</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">194 K</td><td align="center" valign="middle" rowspan="1" colspan="1">10.30 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.84/0.7771</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">175 K</td><td align="center" valign="middle" rowspan="1" colspan="1">9.43 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.82/0.7758</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">231 K</td><td align="center" valign="middle" rowspan="1" colspan="1">11.14 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.90/0.7797</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">207 K</td><td align="center" valign="middle" rowspan="1" colspan="1">10.93 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.92/0.7806</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">213 K</td><td align="center" valign="middle" rowspan="1" colspan="1">10.27 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.88/0.7789</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">244 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.78 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>25.94</bold>/<bold>0.7810</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05275-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05275-t004_Table 4</object-id><label>Table 4</label><caption><p>Effect of multiplicity sampling and multi-branch feature extraction on performance. <bold>Bold</bold> is optimal, &#215; means adding this block, &#10004; means removing this block.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Scale</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Multiplicity<break/>Sampling (MS)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">MBFEB</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Params</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Urban100</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">162 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.79 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.73/0.7734</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">166 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.89 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.80/0.7757</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">172 K</td><td align="center" valign="middle" rowspan="1" colspan="1">9.34 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.75/0.7742</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10004;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">175 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.43 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>25.82</bold>/<bold>0.7758</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05275-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05275-t005_Table 5</object-id><label>Table 5</label><caption><p>The impact of dual-branch on performance in SCHIEB. <bold>Bold</bold> is optimal.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Scale</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Branch Name</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Params</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Urban100</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;4</td><td align="center" valign="middle" rowspan="1" colspan="1">SCB</td><td align="center" valign="middle" rowspan="1" colspan="1">195 K</td><td align="center" valign="middle" rowspan="1" colspan="1">9.21 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.81/0.7765</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AB</td><td align="center" valign="middle" rowspan="1" colspan="1">121 K</td><td align="center" valign="middle" rowspan="1" colspan="1">6.39 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.61/0.7692</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dual-Branch</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.64 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>25.85</bold>/<bold>0.7770</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05275-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05275-t006_Table 6</object-id><label>Table 6</label><caption><p>The influence of different convolutional kernel sizes on performance in MBFEB. <bold>Bold</bold> is optimal.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Scale</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Combination</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Params</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Urban100</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSNR/SSIM</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#215;4</td><td align="center" valign="middle" rowspan="1" colspan="1">3-3-3</td><td align="center" valign="middle" rowspan="1" colspan="1">164 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.83 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.70/0.7724</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3-5-5</td><td align="center" valign="middle" rowspan="1" colspan="1">164 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.84 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.72/0.7725</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3-7-7</td><td align="center" valign="middle" rowspan="1" colspan="1">164 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.84 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.73/0.7733</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5-3-3</td><td align="center" valign="middle" rowspan="1" colspan="1">166 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.87 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.75/0.7735</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5-5-5</td><td align="center" valign="middle" rowspan="1" colspan="1">166 K</td><td align="center" valign="middle" rowspan="1" colspan="1">8.88 G</td><td align="center" valign="middle" rowspan="1" colspan="1">25.77/0.7747</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5-7-7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">166 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.89 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>25.80</bold>/<bold>0.7757</bold></td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>