<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431280</article-id><article-id pub-id-type="pmcid-ver">PMC12431280.1</article-id><article-id pub-id-type="pmcaid">12431280</article-id><article-id pub-id-type="pmcaiid">12431280</article-id><article-id pub-id-type="doi">10.3390/s25175345</article-id><article-id pub-id-type="publisher-id">sensors-25-05345</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>RaSS: 4D mm-Wave Radar Point Cloud Semantic Segmentation with Cross-Modal Knowledge Distillation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-3428-6813</contrib-id><name name-style="western"><surname>Zhang</surname><given-names initials="C">Chenwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05345" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3329-7037</contrib-id><name name-style="western"><surname>Xiang</surname><given-names initials="Z">Zhiyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05345" ref-type="aff">1</xref><xref rid="af2-sensors-25-05345" ref-type="aff">2</xref><xref rid="c1-sensors-25-05345" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-5171-6223</contrib-id><name name-style="western"><surname>Xu</surname><given-names initials="R">Ruoyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05345" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6264-9858</contrib-id><name name-style="western"><surname>Shan</surname><given-names initials="H">Hangguan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05345" ref-type="aff">1</xref><xref rid="af2-sensors-25-05345" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2090-3648</contrib-id><name name-style="western"><surname>Zhao</surname><given-names initials="X">Xijun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af3-sensors-25-05345" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Dang</surname><given-names initials="R">Ruina</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af3-sensors-25-05345" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>L&#225;zaro</surname><given-names initials="A">Antonio</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05345"><label>1</label>College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; <email>zhangchenwei@zju.edu.cn</email> (C.Z.); <email>xuruoyu@zju.edu.cn</email> (R.X.); <email>hshan@zju.edu.cn</email> (H.S.)</aff><aff id="af2-sensors-25-05345"><label>2</label>Zhejiang Provincial Key Laboratory of Mutil-Modal Communication Networks and Intelligent Information Processing, Zhejiang University, Hangzhou 310027, China</aff><aff id="af3-sensors-25-05345"><label>3</label>ChinaNorth Artificial Intelligence &amp; Innovation Research Institute, Beijing 100072, China; <email>heejunzhao@163.com</email> (X.Z.); <email>ruinadang@163.com</email> (R.D.)</aff><author-notes><corresp id="c1-sensors-25-05345"><label>*</label>Correspondence: <email>xiangzy@zju.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>28</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5345</elocation-id><history><date date-type="received"><day>21</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>15</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>27</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>28</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 12:25:22.357"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05345.pdf"/><abstract><p>Environmental perception is an essential task for autonomous driving, which is typically based on LiDAR or camera sensors. In recent years, 4D mm-Wave radar, which acquires 3D point cloud together with point-wise Doppler velocities, has drawn substantial attention owing to its robust performance under adverse weather conditions. Nonetheless, due to the high sparsity and substantial noise inherent in radar measurements, most radar perception studies are limited to object-level tasks, with point-level tasks such as semantic segmentation remaining largely underexplored. This paper aims to explore the possibility of using 4D radar in semantic segmentation. We set up the ZJUSSet dataset containing accurate point-wise class labels for radar and LiDAR. Then we propose a cross-modal distillation framework RaSS to fulfill the task. An adaptive Doppler compensation module is also designed to facilitate the segmentation. Experimental results on ZJUSSet and VoD dataset demonstrate that our RaSS model significantly outperforms the baselines and competitors. Code and dataset will be available upon paper acceptance.</p></abstract><kwd-group><kwd>radar</kwd><kwd>semantic segmentation</kwd><kwd>knowledge distillation</kwd></kwd-group><funding-group><award-group><funding-source>The Key Research &amp; Development Plan of Zhejiang Province</funding-source><award-id>2024C01017</award-id><award-id>2024C01010</award-id></award-group><award-group><funding-source>Yangtze River Delta Community of Sci-Tech Innovation</funding-source><award-id>2024CSJGG01000</award-id></award-group><funding-statement>This research was funded by the The Key Research &amp; Development Plan of Zhejiang Province (2024C01017 and 2024C01010), and the Joint R&amp;D Program of the Yangtze River Delta Community of Sci-Tech Innovation (2024CSJGG01000).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05345"><title>1. Introduction</title><p>Semantic segmentation stands as a critical task in the field of autonomous driving. Currently, semantic segmentation techniques relying on images or LiDAR point cloud have undergone extensive research and reached a mature stage. Despite significant advancements, image or LiDAR-based segmentation remains challenged under adverse illumination or weather conditions. By contrast, mm-Wave radar can operate under adverse weathers such as snow and fog, and acquire point cloud with Doppler velocities, which makes them a popular sensor for environmental perception.</p><p>Based on the dimensionality of the point cloud, mm-Wave radar can be categorized into 3D and 4D. The former produces 2D point cloud while the latter can produce 3D point cloud similar to 3D LiDAR. However, in contrast to LiDAR, the point cloud generated by 4D radar is extremely sparse, boasting a density merely one-tenth that of LiDAR. Furthermore, owing to the multi-path effect intrinsic to millimeter waves, the range measurements of radar are substantially noisier than those of LiDAR. Therefore, most studies use radar as an auxiliary sensor and focus on fusing radar with other modalities [<xref rid="B1-sensors-25-05345" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05345" ref-type="bibr">2</xref>].</p><p>Majority of current 4D radar studies focus on object-level tasks [<xref rid="B3-sensors-25-05345" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05345" ref-type="bibr">4</xref>], prioritizing object presence and position with bounding boxes. Point-level semantic segmentation, by contrast, labels each point to enable detailed environmental understanding that encompasses objects and backgrounds. This is critical for fine-grained autonomous driving. As a result, current mm-Wave radar datasets primarily focus on 2D or 3D object detection tasks [<xref rid="B5-sensors-25-05345" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05345" ref-type="bibr">6</xref>]. A very small number of them are involved in semantic segmentation task [<xref rid="B7-sensors-25-05345" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05345" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05345" ref-type="bibr">9</xref>], with only 2D point cloud provided. The lack of 4D radar-based semantic segmentation datasets hinders further study and exploration of this point-based classification problem. In addition, the sparse points and large noise contained in the 4D radar data also call for novel methods for semantic segmentation.</p><p>To tackle these challenges, this paper proposes a new semantic segmentation dataset ZJUSSet, which comprises synchronized 4D mm-Wave radar, LiDAR, and camera data acquired in urban environments. The sensors are carefully calibrated to produce spatial aligned data. Up to 10 categories are annotated in order to provide a comprehensive description of the scene. Several sample cases from the dataset are illustrated in <xref rid="sensors-25-05345-f001" ref-type="fig">Figure 1</xref>.</p><p>Building on the ZJUSSet, we propose RaSS, a cross-modal distillation-based semantic segmentation framework, to tackle the challenges stemming from the marked sparsity and noise present in radar point cloud. The framework employs a pre-trained LiDAR-based segmentation model in the role of teacher, with the radar-only model serving as its student counterpart. With much higher density and accuracy, the LiDAR-based teacher can produce features containing rich structural and semantic information for the segmentation task. Knowledge distillation guides radar to learn strong features analogous to LiDAR&#8217;s, thereby improving the feature representation of the Student model. In addition, an Adaptive Doppler Compensation (ADC) module is designed to strengthen the distinguishing capability of static and non-static objects. Results from experiments on ZJUSSet demonstrate that our RaSS model not only yields a 5.57% mIOU improvement over the baseline but also outperforms existing methods with superior performance.</p><p>The main contributions are summarized as follows:<list list-type="bullet"><list-item><p>We present ZJUSSet, a new dataset designed specifically for 4D radar semantic segmentation. To our knowledge, it is the first dataset offering point-level annotations of 4D radar across 10 categories.</p></list-item><list-item><p>We develop a cross-modal knowledge distillation framework for semantic segmentation of 4D radar point cloud. Leveraging spatially aligned feature maps from a LiDAR-based teacher model, the radar-only student model is able to learn and extract more discriminative features optimized for segmentation tasks.</p></list-item><list-item><p>We evaluate our RaSS framework alongside other state-of-the-art approaches on both ZJUSSet and VoD [<xref rid="B6-sensors-25-05345" ref-type="bibr">6</xref>]. The findings confirm that our model outperforms competing methods and highlight the promising prospects of 4D radar-based semantic segmentation.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05345"><title>2. Related&#160;Work</title><sec id="sec2dot1-sensors-25-05345"><title>2.1. Radar Segmentation&#160;Dataset</title><p>Existing mm-Wave radar datasets typically include radar and camera images, most of which are designed for the task of object detection [<xref rid="B5-sensors-25-05345" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05345" ref-type="bibr">6</xref>]. Merely a handful of these apply to the task of semantic segmentation on radar.</p><p>CARRADA [<xref rid="B7-sensors-25-05345" ref-type="bibr">7</xref>] contains synchronized RGB images and 3D radar frames, enabling 2D semantic segmentation via range-Doppler or range-angle representations. In addition to object bounding box annotation, RADIal [<xref rid="B8-sensors-25-05345" ref-type="bibr">8</xref>] also provides free-space annotations on the range-Doppler spectrum. RadarScenes [<xref rid="B9-sensors-25-05345" ref-type="bibr">9</xref>], a large-scale multi-sensor dataset with 2D point-wise annotations, supports tasks such as object detection, clustering, classification, and semantic segmentation.</p><p><xref rid="sensors-25-05345-t001" ref-type="table">Table 1</xref> summarizes the differences between the existing radar segmentation dataset and ours. Our ZJUSSet includes rich data from 4D radar, dense LiDAR, and camera, providing more categories with 3D point-wise annotations, which is more challenging for the segmentation task.</p></sec><sec id="sec2dot2-sensors-25-05345"><title>2.2. Radar-Based Semantic Segmentation&#160;Methods</title><p>Existing radar semantic segmentation methods all focus on 3D radars. According to the form of radar data used, they can be grouped into two types: tensor-based and point-based. The tensor-based methods take the radar RD tensor [<xref rid="B10-sensors-25-05345" ref-type="bibr">10</xref>], RA tensor [<xref rid="B11-sensors-25-05345" ref-type="bibr">11</xref>], or RAD tensor [<xref rid="B8-sensors-25-05345" ref-type="bibr">8</xref>] as the input and segment them into different semantic regions. The point-based methods are more popular since they directly carry out segmentation on the point cloud, which is more efficient and intuitive. Furthermore, these methods can draw upon proven algorithms for LiDAR processing, eliminating the need to develop them entirely from the scratch. According to the feature presentation used, these techniques can be further partitioned into point-based [<xref rid="B12-sensors-25-05345" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05345" ref-type="bibr">13</xref>] and grid-based methodologies [<xref rid="B14-sensors-25-05345" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05345" ref-type="bibr">15</xref>].</p><p>With a radar point cloud as input, Ole [<xref rid="B16-sensors-25-05345" ref-type="bibr">16</xref>] employed PointNet [<xref rid="B17-sensors-25-05345" ref-type="bibr">17</xref>] as the feature extractor. Nobis [<xref rid="B13-sensors-25-05345" ref-type="bibr">13</xref>] introduced a KPConv [<xref rid="B18-sensors-25-05345" ref-type="bibr">18</xref>] based long short-term memory (LSTM) to leverage the in-context formation in the radar sequence. Jakob [<xref rid="B19-sensors-25-05345" ref-type="bibr">19</xref>] divided the radar point cloud into grids and classified the points contained in them. More recently, Gaussian Radar Transformer (GRT) [<xref rid="B20-sensors-25-05345" ref-type="bibr">20</xref>] proposes a self-attention method for radar points, which achieves better 2D point segmentation performance on the sparse radar point cloud.</p><p>Existing algorithms, however, are confined to 3D mm-Wave radar and can only handle 2D semantic segmentation tasks.</p></sec><sec id="sec2dot3-sensors-25-05345"><title>2.3. Knowledge&#160;Distillation</title><p>Knowledge distillation usually refers to training a compact, low-complexity, and inference-deployment-friendly student network by introducing a complex and high-precision teacher network with knowledge transfer [<xref rid="B21-sensors-25-05345" ref-type="bibr">21</xref>]. The student network can acquire implicit knowledge from the teacher network by different levels of distillation, such as response level [<xref rid="B21-sensors-25-05345" ref-type="bibr">21</xref>], feature level [<xref rid="B22-sensors-25-05345" ref-type="bibr">22</xref>], and relation level [<xref rid="B23-sensors-25-05345" ref-type="bibr">23</xref>].</p><p>In recent years, knowledge distillation has become popular in 3D LiDAR point semantic segmentation. They are manifested primarily in feature learning and fusion [<xref rid="B24-sensors-25-05345" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05345" ref-type="bibr">25</xref>], multi-modal learning [<xref rid="B26-sensors-25-05345" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05345" ref-type="bibr">27</xref>], and weakly or unsupervised learning [<xref rid="B28-sensors-25-05345" ref-type="bibr">28</xref>]. Jiang [<xref rid="B25-sensors-25-05345" ref-type="bibr">25</xref>] distills 3D voxel characteristics onto BEV characteristics, allowing the BEV model to perceive more geometric information. Hou [<xref rid="B24-sensors-25-05345" ref-type="bibr">24</xref>] proposes point and voxel affinity distillation to assist the student model in better capturing the features of the structure. Yan [<xref rid="B26-sensors-25-05345" ref-type="bibr">26</xref>] extracts rich semantic features from a multi-modal teacher (camera and LiDAR) and employs cross-modal distillation to strengthen the pure LiDAR-based student networks. Furthermore, Zhang [<xref rid="B27-sensors-25-05345" ref-type="bibr">27</xref>] utilizes foundation models such as SAM [<xref rid="B29-sensors-25-05345" ref-type="bibr">29</xref>] and CLIP [<xref rid="B30-sensors-25-05345" ref-type="bibr">30</xref>] to obtain dense pseudo-labels for training.</p><p>Compared to the existing cross-modal methods, our study focuses on distillation dense Lidar-based teacher to highly sparse and noisy radar-input student and exploring its potential in semantic segmentation.</p></sec></sec><sec id="sec3-sensors-25-05345"><title>3. 4D Radar Semantic Segmentation&#160;Dataset</title><p>The ZJUSSet dataset was collected in Hangzhou city, China, with an experimental car platform configured with multiple sensors, as shown in <xref rid="sensors-25-05345-f002" ref-type="fig">Figure 2</xref>. This section will introduce the sensor setup, calibration, data annotation, and statistical information of the new dataset.</p><sec id="sec3dot1-sensors-25-05345"><title>3.1. Sensor&#160;Setup</title><p>ZJUSSet contains data acquired from an Oculii 4D mm-Wave radar, a Livox Avia LiDAR, and a Realsense Camera. The radar supports a 400 m detection range, achieving 0.86 m distance resolution and 0.27 m/s velocity resolution, respectively. The radar boasts a field of view (FOV) measuring 113&#176; horizontally by 45&#176; vertically. As for the LiDAR, its FOV spans 70.4&#176; (horizontal) by 77.2&#176; (vertical) and the maximum detection distance of 450 m with a measurement accuracy of 2 cm. The resolution of the image is 640 &#215; 480. All data were acquired at a synchronized frame rate of 10 Hz.</p></sec><sec id="sec3dot2-sensors-25-05345"><title>3.2. Calibration</title><p>Accurate calibration of the sensors is crucial. We first use a checkerboard for the intrinsic calibration of the camera, and the joint calibration between the LiDAR and the camera [<xref rid="B31-sensors-25-05345" ref-type="bibr">31</xref>]. Since the mmw-radar and LiDAR themselves are pre-calibrated at the factory, only the extrinsic calibrations among them are required. We employ corner reflectors for radar-LiDAR and radar-camera calibration. We position corner reflectors at various locations in front of the sensors. The radar points with the highest Signal-to-Noise Ratio (SNR) near each reflector are selected to determine their positions in the radar coordinate system. Corresponding reflector positions are also identified in the LiDAR and camera data. Using the Iterative Closest Point (ICP) algorithm [<xref rid="B32-sensors-25-05345" ref-type="bibr">32</xref>] and the Perspective-n-Point (PnP) method [<xref rid="B33-sensors-25-05345" ref-type="bibr">33</xref>], we compute the coordinate transformations for radar-LiDAR and radar-camera pairs, respectively.</p></sec><sec id="sec3dot3-sensors-25-05345"><title>3.3. Data&#160;Annotation</title><p>Compared to LiDAR, the point cloud mm-Wave radar is extremely sparse, making direct annotation challenging and unreliable. Therefore, we facilitate the radar point annotation work by projecting LiDAR points onto the radar coordinates, and refer to the denser LiDAR point for annotation guidance. With the help of the image and the long-range Livox LiDAR point cloud, we can achieve precise radar point annotations within 200 m.</p><p>To relieve the labor of labeling, we first annotate the foreground objects with 3D bounding boxes. All radar and LiDAR points falling within the identical bounding box are subsequently given the same class label as the box itself. At last, we manually annotate points of the background categories such as buildings, vegetation, and fences.</p><p>Finally, the dataset encompasses ten categories: Building, Vegetation, Fence, Car, Cyclist, Pedestrian, Truck, Bus, Tricycle, and Others. Each radar point is an 8-vector, composed of coordinates (<inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), range <italic toggle="yes">r</italic>, the Doppler velocity <italic toggle="yes">v</italic>, the signal to noise ratio <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, Azimuth Angle <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>, and Elevation Angle <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot4-sensors-25-05345"><title>3.4. Statistics&#160;Analysis</title><p>In this work, we assemble the dataset with 190 sequences, each consisting of about 100 frames captured at 10 Hz. Annotations were made every 5 frames, resulting in a total of 3794 annotated frames within 19,000 frames of raw data. The initial 3000 annotated frames serve as the training set, while the rest 794 annotation frames are reserved for validation purpose.</p><p>The distribution of the radar point is shown in <xref rid="sensors-25-05345-f003" ref-type="fig">Figure 3</xref>, where we can see that the average quantity of radar point is less than one-tenth of the LiDAR. In the meantime, the point distributions among the categories are not uniform, with much fewer points in the categories of Pedestrian, Truck, Bus, and Tricycle. All such aspects underscore the distinct characteristic of radar points and highlight the unique challenges in 4D radar semantic segmentation task.</p></sec></sec><sec id="sec4-sensors-25-05345"><title>4. Method</title><sec id="sec4dot1-sensors-25-05345"><title>4.1. Framework</title><p>The primary objective of this study is to investigate radar point cloud for the semantic segmentation task and set up a benchmark for the ZJUSSet. As there is no similar work previously, we employ 3D semantic segmentation network that is originally designed for LiDAR as the baseline for our 4D radar-based segmentation task. To tackle the emerging challenges arising from the high sparsity and noise in 4D radar point cloud, we put forward a cross-modal knowledge distillation-based Radar Semantic Segmentation (RaSS) framework aimed at boosting model performance.</p><p>The overview framework is depicted in <xref rid="sensors-25-05345-f004" ref-type="fig">Figure 4</xref>, where a LiDAR-based semantic segmentation model is employed as the teacher, and a radar-based model acts as the student. In the student branch, prior to feeding into the backbone, the radar point cloud gets preprocessed by an Adaptive Doppler Compensation module. In addition, during the training process, multi-scale feature distillation is performed between the LiDAR features and the Radar features. In inference, only the radar-based student model participates. For distillation facilitation, both teacher and student models utilize the same backbone. Next, we give more details about the Adaptive Doppler Compensation (ADC) and the Radar Feature Knowledge Distillation (RFKD) modules.</p></sec><sec id="sec4dot2-sensors-25-05345"><title>4.2. Adaptive Doppler Compensation (ADC)</title><p>The Doppler speed provided by the radar can be valuable for classifying the points of moving objects. However, the raw Doppler speed obtained from the 4D radar is only the relative radial speed, which is non-zero for the points from static backgrounds when the ego-vehicle is moving. Based on the observation (<xref rid="sensors-25-05345-f003" ref-type="fig">Figure 3</xref>) that most of the radar points belong to the static categories, we propose an Adaptive Doppler Compensation (ADC) algorithm for the radar. The module can work in the situation of unknown ego-vehicle speed. The process is illustrated in <xref rid="sensors-25-05345-f005" ref-type="fig">Figure 5</xref> and described as follows.</p><list list-type="simple"><list-item><label>(1)</label><p>For simplicity, we assume that the ego-vehicle moves only on the <italic toggle="yes">x</italic>-<italic toggle="yes">y</italic> plane and heads along the <italic toggle="yes">x</italic> direction. For each radar point <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> in <italic toggle="yes">j</italic>th frame, we project the raw Doppler <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> onto the <italic toggle="yes">x</italic>-<italic toggle="yes">y</italic> plane, resulting in the projected speed <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>(2)</label><p>Project the <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> to <italic toggle="yes">x</italic>-<italic toggle="yes">axis</italic> to obtain the relative speed <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> along the vehicle&#8217;s motion direction, with <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>/</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>(3)</label><p>Assuming the majority of points in the scene are from the static background, we discretize the <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and define a function <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext>_</mml:mtext><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to count the votes within the frame. The highest count is regarded as the estimated ego-vehicle velocity for the current <italic toggle="yes">j</italic>th frame, i.e., <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext>_</mml:mtext><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>(4)</label><p>For the radar point <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in the <italic toggle="yes">j</italic>th frame, the compensated velocity is obtained by <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, we use <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> instead of the original <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and feed it into the student network as the initial feature.</p></list-item></list><fig position="anchor" id="sensors-25-05345-f005" orientation="portrait"><label>Figure 5</label><caption><p>Illustration of the Adaptive Doppler Compensation. (<bold>a</bold>) For each radar point <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, its <italic toggle="yes">x</italic>-<italic toggle="yes">y</italic> plane velocity can be obtained with <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>b</bold>) For the static point <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is caused by the velocity of ego-vehicle <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. While for the moving point <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> consists of <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the point&#8217;s self-velocity <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>comp</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g005.jpg"/></fig></sec><sec id="sec4dot3-sensors-25-05345"><title>4.3. Radar Feature Knowledge Distillation (RFKD)</title><p>Through the convolution layers within the LiDAR and radar backbones (based on Cylinder3D), we can derive multi-level features for each input modality. Owing to the significant discrepancies in quantity and density between LiDAR and mm-Wave radar point cloud, effectively establishing feature correspondences and facilitating distillation constitutes a critical challenge. To address this issue, we first perform feature aggregation on the LiDAR and radar voxel features extracted by each backbone within the bird&#8217;s-eye-view (BEV) perspective. Guided by spatial relationships, we then select features from both aggregated LiDAR and radar BEV features, yielding two sets of aligned cross-modal features for subsequent feature distillation.</p><p>For each layer, given the LiDAR feature tensor <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and radar feature tensor <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, their corresponding voxel coordinates <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Voxel</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Voxel</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are obtained with all coordinate ranges falling within the interval defined by <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denote the LiDAR and radar feature sets, respectively; <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the sets of LiDAR and radar point indices, with <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicating the number of points in each set; <italic toggle="yes">C</italic> denotes the channel dimension of the features, while <italic toggle="yes">H</italic>, <italic toggle="yes">W</italic>, and <italic toggle="yes">D</italic> represent the voxelization boundaries corresponding to maximum height, maximum width, and maximum depth, respectively.</p><p>Subsequently, we leverage the voxel coordinates <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Voxel</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Voxel</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to perform feature aggregation within the bird&#8217;s-eye view (BEV) perspective, yielding the aggregated features <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the sets of occupied BEV grid indices derived from the LiDAR point set <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and radar point set <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively; <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicate the number of non-empty grid cells after aggregation. Their corresponding grid coordinates <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Grid</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Grid</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are obtained with each entry in <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Grid</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) representing the 2D coordinates of the centroid of the corresponding occupied grid cell.</p><p>Given that the point cloud distributions of radar and LiDAR are different, we aim to perform knowledge distillation only in regions where both LiDAR and radar grids are occupied with points. To this end, guided by the spatial position of <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Grid</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Grid</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we then select and obtain a set of matched features <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>&#8594;</mml:mo><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>&#8594;</mml:mo><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>&#8838;</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>&#8838;</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the subsets of point-occupied and spatially matched grid indices, with <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicating the number of successfully matched pairs.</p><p>Then, we pass the selected radar BEV feature <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>R</mml:mi><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> through a multi-layer perceptron (MLP) based adapter to reduce the domain gap prior to distillation. The adapted radar features are subsequently used to compute the distillation loss <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>distill</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to the corresponding LiDAR features <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>L</mml:mi><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:msubsup></mml:mrow></mml:math></inline-formula>, formulated in Equation (<xref rid="FD1-sensors-25-05345" ref-type="disp-formula">1</xref>):<disp-formula id="FD1-sensors-25-05345"><label>(1)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>distill</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>L</mml:mi><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:msubsup><mml:mo>,</mml:mo><mml:mi>MLP</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>R</mml:mi><mml:msubsup><mml:mi mathvariant="script">G</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> specifically represents the mean squared error (MSE) metric.</p><p>To ensure adequate learning of multi-scale feature extraction in the radar network, we distill features from deeper layers, which are organized according to the feature pyramid structure in the backbone. The details of the RFKD are shown in <xref rid="sensors-25-05345-f006" ref-type="fig">Figure 6</xref>.</p></sec><sec id="sec4dot4-sensors-25-05345"><title>4.4. Loss&#160;Functions</title><p>For the semantic segmentation task, we employ the Lovasz-Softmax loss [<xref rid="B34-sensors-25-05345" ref-type="bibr">34</xref>] and Cross Entropy (CE) loss. Combined with the feature knowledge distillation loss <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>distill</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the overall loss function is formulated as:<disp-formula id="FD2-sensors-25-05345"><label>(2)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>Total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>Seg</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>distill</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>Seg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>Lovasz</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>CE</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the segmentation loss, <italic toggle="yes">K</italic> represents the number of distillation layers, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the corresponding weighting coefficient for each layer <italic toggle="yes">l</italic>, and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>distill</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the distillation loss computed at layer <italic toggle="yes">l</italic> using Equation (<xref rid="FD1-sensors-25-05345" ref-type="disp-formula">1</xref>).</p></sec></sec><sec id="sec5-sensors-25-05345"><title>5. Experiments</title><sec id="sec5dot1-sensors-25-05345"><title>5.1. Experiment&#160;Setups</title><sec id="sec5dot1dot1-sensors-25-05345"><title>5.1.1. Dataset</title><p>We mainly evaluate the RaSS method on our proposed ZJUSSet. The annotated data are sequentially partitioned into training and validation subsets: the former comprises the first 3000 frames, and the latter contains the remaining 794 frames. For both subsets, we focus our evaluation on points within the range of [&#8722;5 m, 5 m] along the <italic toggle="yes">z</italic>-axis and [0, 100 m] along the <italic toggle="yes">x</italic>-axis. On the ZJUSSet dataset, we evaluate performance across 9 categories: building, vegetation, fence, car, cyclist, pedestrian, truck, bus and tricycle. After calibration, the reprojection error (RMSE) between the point clouds of mm-Wave radar and the LiDAR is 7.26 cm.</p><p>Meanwhile, we also use VoD dataset validate the generalizability of our algorithm. As VoD dataset ia originally designed for 4D mm-Wave radar 3D object detection, we generate pseudo-semantic segmentation labels for the annotated objects. Specifically, we leverage ground truth 3D object bounding box (bbox) for auxiliary annotation generation. Each point within the bbox is assigned the class label of the corresponding box, enabling point-level semantic annotation for six movable categories: car, pedestrian, cyclist, motor, truck, and bicycle. Finally, we filtered out frames with fewer than 100 annotated points, resulting in 2701 frames for training and 703 frames for validation. Although this is not a complete semantic segmentation dataset (with labeled points only accounting for approximately 10% of the total points), it provides complementary validation in more diverse scenarios.</p></sec><sec id="sec5dot1dot2-sensors-25-05345"><title>5.1.2. Evaluation&#160;Metrics</title><p>The performance is evaluated using the commonly adopted Intersection over Union (IoU), where the mean IoU (mIoU) across all categories is computed as:<disp-formula id="FD3-sensors-25-05345"><label>(3)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> donates True Positive, False Positive, and False Negative predictions for class <italic toggle="yes">i</italic>, and <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the total number of categories.</p><p>For each category, we also use recall and precision to give a in-depth observation of each category, as shown in Equation (<xref rid="FD4-sensors-25-05345" ref-type="disp-formula">4</xref>) and Equation (<xref rid="FD5-sensors-25-05345" ref-type="disp-formula">5</xref>) respectively.<disp-formula id="FD4-sensors-25-05345"><label>(4)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-05345"><label>(5)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec5dot1dot3-sensors-25-05345"><title>5.1.3. Network&#160;Setups</title><p>We separately choose Cylinder3D [<xref rid="B35-sensors-25-05345" ref-type="bibr">35</xref>] and PT-V2 [<xref rid="B36-sensors-25-05345" ref-type="bibr">36</xref>] as the 3D backbones in our method. In each experiment, the same backbone is employed for the Teacher and the Student. The cylinder voxel size is configured as 1000 &#215; 128 &#215; 64, with each voxel corresponding to a spatial size of [0.1 m, 0.88&#176;, 0.16 m]. For Cylinder3D, which consists of 8 feature layers in total, we apply the RFKD model to layers 2, 4, 6, and 8. While PT-V2, which has 9 feature layers, we select layers 1, 3, 6, and 8. Additionally, since PT-V2 does not include a voxelization step, we perform feature alignment between LiDAR and radar using K-Nearest Neighbors (KNN) with <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec5dot1dot4-sensors-25-05345"><title>5.1.4. Training and Inference&#160;Details</title><p>Regarding the loss function, we configure the weights of the RFKD module as follows: <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for Cylinder3D, and <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for PT-V2. And we applied data augmentation to the training data, including rotating the point cloud within the range of [&#8722;22.5&#176;, 22.5&#176;], randomly flipping the point cloud along the y-axis, randomly scaling the point cloud with scales within [0.95, 1.05], along with the introduction of zero-mean Gaussian noise characterized by a standard deviation of 0.1. The model underwent training for 75 epochs, using a batch size of 32. We utilized the Adamw optimizer and OneCycleLR strategy to dynamically adjust the learning rate with a maximum learning rate of <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for Cylinder3D and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.02</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for PT-V2. All the experiments were performed on a single Nvidia GeForce RTX 4090 GPU. The training time is 3 h for Cylinder3D and 2 h for PT-V2 backbones.</p></sec></sec><sec id="sec5dot2-sensors-25-05345"><title>5.2. Experimental&#160;Results</title><sec sec-type="results" id="sec5dot2dot1-sensors-25-05345"><title>5.2.1. Results on&#160;ZJUSSet</title><p><xref rid="sensors-25-05345-t002" ref-type="table">Table 2</xref> shows the semantic segmentation results on ZJUSSet. Firstly, our RaSS model ranks the top among all of the radar-input models and outperforms the baseline by <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.42</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> mIoU for Cylinder3D and <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.57</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for PT-V2. It should mainly thank to the RFKD module, which can transfer the teacher&#8217;s feature extraction capability to the radar-based student model. Secondly, despite the highly sparse radar input, the RaSS model can still achieve <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>42.35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> mIOU, exhibiting the surprising potential of 4D radar-based semantic segmentation for the autonomous vehicle. Lastly, the RaSS model still performs behind the LiDAR-based baseline model. This can be attributed to the sparser and noisier characteristics of mm-Wave radar point cloud. The qualitative results are presented in <xref rid="sensors-25-05345-f007" ref-type="fig">Figure 7</xref>.</p><table-wrap position="anchor" id="sensors-25-05345-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05345-t002_Table 2</object-id><label>Table 2</label><caption><p>Semantic segmentation results on the ZJUSSet Validation set. L and R separately stand for LiDAR and mm-Wave radar data. RaSS(C) and RaSS(P) represents RaSS using Cylinder3D or PT-V2 as backbone, respectively. The bold and underlined values denote the best and second place within the radar input methods, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Input</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Building</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fence</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Vegetation</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cyclist</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pedestrian</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Truck</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bus</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Tricycle</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cylinder3D&#160;[<xref rid="B35-sensors-25-05345" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">69.02</td><td align="center" valign="middle" rowspan="1" colspan="1">72.36</td><td align="center" valign="middle" rowspan="1" colspan="1">63.17</td><td align="center" valign="middle" rowspan="1" colspan="1">91.95</td><td align="center" valign="middle" rowspan="1" colspan="1">95.52</td><td align="center" valign="middle" rowspan="1" colspan="1">69.39</td><td align="center" valign="middle" rowspan="1" colspan="1">68.30</td><td align="center" valign="middle" rowspan="1" colspan="1">44.41</td><td align="center" valign="middle" rowspan="1" colspan="1">84.71</td><td align="center" valign="middle" rowspan="1" colspan="1">31.33</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PT-V2&#160;[<xref rid="B36-sensors-25-05345" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.95</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KPConv&#160;[<xref rid="B18-sensors-25-05345" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">29.20</td><td align="center" valign="middle" rowspan="1" colspan="1">55.70</td><td align="center" valign="middle" rowspan="1" colspan="1">55.90</td><td align="center" valign="middle" rowspan="1" colspan="1">42.90</td><td align="center" valign="middle" rowspan="1" colspan="1">68.40</td><td align="center" valign="middle" rowspan="1" colspan="1">8.30</td><td align="center" valign="middle" rowspan="1" colspan="1">15.40</td><td align="center" valign="middle" rowspan="1" colspan="1">3.20</td><td align="center" valign="middle" rowspan="1" colspan="1">12.90</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PolarNet&#160;[<xref rid="B37-sensors-25-05345" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">31.57</td><td align="center" valign="middle" rowspan="1" colspan="1">71.56</td><td align="center" valign="middle" rowspan="1" colspan="1">46.08</td><td align="center" valign="middle" rowspan="1" colspan="1">57.04</td><td align="center" valign="middle" rowspan="1" colspan="1">73.41</td><td align="center" valign="middle" rowspan="1" colspan="1">1.73</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68</td><td align="center" valign="middle" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">33.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Minkowski&#160;[<xref rid="B38-sensors-25-05345" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">33.42</td><td align="center" valign="middle" rowspan="1" colspan="1">61.06</td><td align="center" valign="middle" rowspan="1" colspan="1">58.56</td><td align="center" valign="middle" rowspan="1" colspan="1">51.92</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>75.10</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">7.13</td><td align="center" valign="middle" rowspan="1" colspan="1">15.73</td><td align="center" valign="middle" rowspan="1" colspan="1">2.81</td><td align="center" valign="middle" rowspan="1" colspan="1">25.25</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>3.19</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cylinder3D&#160;[<xref rid="B35-sensors-25-05345" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">36.15</td><td align="center" valign="middle" rowspan="1" colspan="1">73.82</td><td align="center" valign="middle" rowspan="1" colspan="1">58.61</td><td align="center" valign="middle" rowspan="1" colspan="1">58.64</td><td align="center" valign="middle" rowspan="1" colspan="1">69.35</td><td align="center" valign="middle" rowspan="1" colspan="1">5.62</td><td align="center" valign="middle" rowspan="1" colspan="1">29.75</td><td align="center" valign="middle" rowspan="1" colspan="1">3.11</td><td align="center" valign="middle" rowspan="1" colspan="1">26.34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.14</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PT-V2&#160;[<xref rid="B36-sensors-25-05345" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.53</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>63.81</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RaSS(C)</td><td align="center" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>41.57</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">73.86</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>63.3</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">61.08</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>75.83</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>16.20</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>34.30</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>9.64</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">38.82</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>1.11</underline>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RaSS(P)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>42.35</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>74.09</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>63.46</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>61.99</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>9.66</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>51.74</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>7.82</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>42.37</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.94</td></tr></tbody></table></table-wrap><p>Considering the practical application of semantic segmentation in autonomous driving, we statistically analyzed the network&#8217;s real-time performance and memory footprint. Experimental results demonstrate that our model fully meets real-time requirements (74 ms with Cylinder3D, 60 ms with PT-V2) and is faster than LiDAR-based counterparts (79 ms and 66 ms for LiDAR with the same backbone, respectively). Moreover, radar exhibits significantly lower memory usage than LiDAR (1.5 GB versus 6.3 GB with Cylinder3D; 0.4 GB versus 1.1 GB with PT-V2).</p><p><xref rid="sensors-25-05345-t003" ref-type="table">Table 3</xref> presents the IoU, precision, and recall for each category in the ZJUSSet. For categories with few points, Pedestrian achieves higher precision and recall than Cyclist, Truck, and Tricycle, possibly because more training samples are available in the training set.</p></sec><sec sec-type="results" id="sec5dot2dot2-sensors-25-05345"><title>5.2.2. Results on VoD&#160;Dataset</title><p>We validated our method on the self-constructed VoD dataset and the results are shown in <xref rid="sensors-25-05345-t004" ref-type="table">Table 4</xref>. With less categories for segmentation, the mIoU becomes higher than those in the ZJUSSet. Our method still outperforms the baselines with a noticeable margin.</p></sec></sec><sec id="sec5dot3-sensors-25-05345"><title>5.3. Failure Cases&#160;Analysis</title><p>Some failure cases on the ZJUSSet validation set are presented in <xref rid="sensors-25-05345-f008" ref-type="fig">Figure 8</xref>. In the top of the <xref rid="sensors-25-05345-f008" ref-type="fig">Figure 8</xref>, the model shows poor segmentation performance on the minority categories, with IoUs as follows: cyclist (10.45%) and tricycle (0%). The bottom of <xref rid="sensors-25-05345-f008" ref-type="fig">Figure 8</xref> illustrates another failure case, where the model produces false semantic labels for some minority categories (truck), small object (pedestrian), and the categories with sparse point cloud (in this case, car). The detailed IoUs are: truck (0%), pedestrian (0%) and car (16.67%).</p></sec><sec id="sec5dot4-sensors-25-05345"><title>5.4. Ablation&#160;Study</title><p>Ablation studies were conducted to validate the effectiveness of key components in our framework. The results are summarized in <xref rid="sensors-25-05345-t005" ref-type="table">Table 5</xref>.</p><p>We first evaluated the impact of the ADC module for motion compensation. As shown in the rows 1&#8211;2 of the <xref rid="sensors-25-05345-t005" ref-type="table">Table 5</xref>, enabling the ADC module improved mIoU by <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> compared to the baseline. This confirms that accurate motion alignment is crucial for enhancing the representation capability of radar features.</p><p>We also compared different knowledge distillation strategies in the rows 3&#8211;4, 6 of the <xref rid="sensors-25-05345-t005" ref-type="table">Table 5</xref>. Logits-based distillation and feature-based distillation each improved mIoU by <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.07</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4.18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, when applied individually. However, their combination not only increased learning complexity but also resulted in a performance drop, with an mIoU that is <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.99</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> lower than that of feature-based distillation alone. This indicates significant redundancy or potential conflict between the two distillation mechanisms, underscoring the need to avoid arbitrary combination of distillation strategies and instead prioritize the selection of a single effective mechanism.</p><p>Regarding the number of distillation layers, a trade-off exists between knowledge quantity and transfer efficiency. As shown in the last 3 rows of the <xref rid="sensors-25-05345-t005" ref-type="table">Table 5</xref>, increasing the number of distillation layers from 2 to 4 improved mIoU by <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.21</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but further increasing to 8 layers led to a <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> drop. This suggests that excessive layers may introduce noisy or redundant information, underscoring the need for optimal layer selection in knowledge distillation.</p><p>Finally, we evaluated the functionality of the feature selection operation for LiDAR and radar features described in <xref rid="sec4dot3-sensors-25-05345" ref-type="sec">Section 4.3</xref>. As detailed in <xref rid="sec4dot3-sensors-25-05345" ref-type="sec">Section 4.3</xref>, we perform selection on the BEV features of both LiDAR and radar based on spatial correspondence between <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Grid</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Grid</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which effectively restricts knowledge distillation to grid regions commonly occupied by both modalities. In contrast, the approach only aligns LiDAR features using the same spatial correspondence criteria, thereby performing distillation across all radar-occupied grid regions. Experimental results in the rows 5&#8211;6 of the <xref rid="sensors-25-05345-t005" ref-type="table">Table 5</xref> demonstrate the effectiveness of our proposed strategy: despite the richer and more discriminative nature of LiDAR features, enforcing alignment between all radar features and their corresponding LiDAR features leads to performance degradation.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05345"><title>6. Conclusions and Future&#160;Works</title><p>Semantic segmentation for 4D mm-Wave radar remains an unexplored and attractive domain in robotics and autonomous driving field. In this paper, we presented a new dataset for 4D radar point cloud semantic segmentation, namely ZJUSSet, and proposed a knowledge distillation-based method to fulfill this task. The ZJUSSet provides data from 4D radar, LiDAR, and camera, and annotates the point cloud with up to 10 categories. To mitigate the difficulties stemming from the sparsity and noise, we introduce a cross-modal distillation mechanism for semantic segmentation. It encourages the radar model to extract similar features like LiDAR, to improve the performance. We also design the Doppler compensation module to provide more explicit motion information for the points. Thanks to all these delicate designs, our PT-V2 based RaSS version achieves <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.57</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> mIOU improvements over the baseline on ZJUSSet. While there still remains a performance gap compared to LiDAR, this undoubtedly marks a promising start, which will hopefully inspire more research into exploring more potential of 4D mm-wave radar. In our future work, we plan to augment the ZJUSSet with more data under adverse weathers, and develop more advanced algorithms to improve the semantic segmentation performance.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Data curation, C.Z. and R.X.; methodology, C.Z. and R.X.; software, C.Z.; validation, C.Z.; writing&#8212;original draft preparation, C.Z.; writing&#8212;review and editing Z.X., H.S., X.Z. and R.D.; funding, Z.X. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">mm-Wave</td><td align="left" valign="middle" rowspan="1" colspan="1">millimeter-wave [<xref rid="B34-sensors-25-05345" ref-type="bibr">34</xref>]</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PT-V2</td><td align="left" valign="middle" rowspan="1" colspan="1">PointTransformer-V2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RaSS</td><td align="left" valign="middle" rowspan="1" colspan="1">Radar Point Cloud Semantic Segmentation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VoD</td><td align="left" valign="middle" rowspan="1" colspan="1">The View of Delft dataset</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ADC</td><td align="left" valign="middle" rowspan="1" colspan="1">Adaptive Doppler Compensation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RFKD</td><td align="left" valign="middle" rowspan="1" colspan="1">Radar Feature Knowledge Distillation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEV</td><td align="left" valign="middle" rowspan="1" colspan="1">bird&#8217;s-eye-view</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05345"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name></person-group><article-title>Rcfusion: Fusing 4-d radar and camera with bird&#8217;s-eye view features for 3-d object detection</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>8503814</fpage><pub-id pub-id-type="doi">10.1109/TIM.2023.3280525</pub-id></element-citation></ref><ref id="B2-sensors-25-05345"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>J.</given-names></name></person-group><article-title>Mvfusion: Multi-view 3d object detection with semantic-aligned radar and camera fusion</article-title><source>Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#8211;2 June 2023</conf-date><fpage>2766</fpage><lpage>2773</lpage></element-citation></ref><ref id="B3-sensors-25-05345"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>E.</given-names></name></person-group><article-title>SCKD: Semi-supervised cross-modality knowledge distillation for 4d radar object detection</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>25&#8211;30 February 2025</conf-date><fpage>8933</fpage><lpage>8941</lpage></element-citation></ref><ref id="B4-sensors-25-05345"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.X.</given-names></name></person-group><article-title>Ratrack: Moving object detection and tracking with 4d radar point cloud</article-title><source>Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>13&#8211;17 May 2024</conf-date><fpage>4480</fpage><lpage>4487</lpage></element-citation></ref><ref id="B5-sensors-25-05345"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Caesar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bankiti</surname><given-names>V.</given-names></name><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liong</surname><given-names>V.E.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Krishnan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Baldan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>nuScenes: A Multimodal Dataset for Autonomous Driving</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11621</fpage><lpage>11631</lpage></element-citation></ref><ref id="B6-sensors-25-05345"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Palffy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pool</surname><given-names>E.</given-names></name><name name-style="western"><surname>Baratam</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kooij</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Gavrila</surname><given-names>D.M.</given-names></name></person-group><article-title>Multi-class road user detection with 3+ 1D radar in the View-of-Delft dataset</article-title><source>IEEE Robot. Autom. Lett.</source><year>2022</year><volume>7</volume><fpage>4961</fpage><lpage>4968</lpage><pub-id pub-id-type="doi">10.1109/LRA.2022.3147324</pub-id></element-citation></ref><ref id="B7-sensors-25-05345"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ouaknine</surname><given-names>A.</given-names></name><name name-style="western"><surname>Newson</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rebut</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tupin</surname><given-names>F.</given-names></name><name name-style="western"><surname>P&#233;rez</surname><given-names>P.</given-names></name></person-group><article-title>Carrada dataset: Camera and automotive radar with range-angle-doppler annotations</article-title><source>Proceedings of the 2020 25th International Conference on Pattern Recognition (ICPR)</source><conf-loc>Milan, Italy</conf-loc><conf-date>10&#8211;15 January 2021</conf-date><fpage>5068</fpage><lpage>5075</lpage></element-citation></ref><ref id="B8-sensors-25-05345"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rebut</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ouaknine</surname><given-names>A.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>W.</given-names></name><name name-style="western"><surname>P&#233;rez</surname><given-names>P.</given-names></name></person-group><article-title>Raw high-definition radar for multi-task learning</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>17021</fpage><lpage>17030</lpage></element-citation></ref><ref id="B9-sensors-25-05345"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Schumann</surname><given-names>O.</given-names></name><name name-style="western"><surname>Hahn</surname><given-names>M.</given-names></name><name name-style="western"><surname>Scheiner</surname><given-names>N.</given-names></name><name name-style="western"><surname>Weishaupt</surname><given-names>F.</given-names></name><name name-style="western"><surname>Tilly</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Dickmann</surname><given-names>J.</given-names></name><name name-style="western"><surname>W&#246;hler</surname><given-names>C.</given-names></name></person-group><article-title>Radarscenes: A real-world radar point cloud data set for automotive applications</article-title><source>Proceedings of the 2021 IEEE 24th International Conference on Information Fusion (FUSION)</source><conf-loc>Sun City, South Africa</conf-loc><conf-date>1&#8211;4 November 2021</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B10-sensors-25-05345"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Orr</surname><given-names>I.</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zalevsky</surname><given-names>Z.</given-names></name></person-group><article-title>High-resolution radar road segmentation using weakly supervised learning</article-title><source>Nat. Mach. Intell.</source><year>2021</year><volume>3</volume><fpage>239</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-00288-6</pub-id></element-citation></ref><ref id="B11-sensors-25-05345"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kaul</surname><given-names>P.</given-names></name><name name-style="western"><surname>De Martini</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gadd</surname><given-names>M.</given-names></name><name name-style="western"><surname>Newman</surname><given-names>P.</given-names></name></person-group><article-title>Rss-net: Weakly-supervised multi-class semantic segmentation with fmcw radar</article-title><source>Proceedings of the 2020 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>19 October&#8211;13 November 2020</conf-date><fpage>431</fpage><lpage>436</lpage></element-citation></ref><ref id="B12-sensors-25-05345"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>B.</given-names></name></person-group><article-title>Deep instance segmentation with automotive radar detection points</article-title><source>IEEE Trans. Intell. Veh.</source><year>2022</year><volume>8</volume><fpage>84</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1109/TIV.2022.3168899</pub-id></element-citation></ref><ref id="B13-sensors-25-05345"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nobis</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fent</surname><given-names>F.</given-names></name><name name-style="western"><surname>Betz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lienkamp</surname><given-names>M.</given-names></name></person-group><article-title>Kernel point convolution LSTM networks for radar point cloud segmentation</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><elocation-id>2599</elocation-id><pub-id pub-id-type="doi">10.3390/app11062599</pub-id></element-citation></ref><ref id="B14-sensors-25-05345"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Prophet</surname><given-names>R.</given-names></name><name name-style="western"><surname>Deligiannis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fuentes-Michel</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Weber</surname><given-names>I.</given-names></name><name name-style="western"><surname>Vossiek</surname><given-names>M.</given-names></name></person-group><article-title>Semantic segmentation on 3D occupancy grids for automotive radar</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>197917</fpage><lpage>197930</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3032034</pub-id></element-citation></ref><ref id="B15-sensors-25-05345"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Prophet</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sturm</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vossiek</surname><given-names>M.</given-names></name></person-group><article-title>Semantic segmentation on automotive radar maps</article-title><source>Proceedings of the 2019 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Paris, France</conf-loc><conf-date>9&#8211;12 June 2019</conf-date><fpage>756</fpage><lpage>763</lpage></element-citation></ref><ref id="B16-sensors-25-05345"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kunert</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wiesbeck</surname><given-names>W.</given-names></name></person-group><article-title>Point cloud segmentation with a high-resolution automotive radar</article-title><source>Proceedings of the AmE 2019-Automotive Meets Electronics; 10th GMM-Symposium</source><conf-loc>Dortmund, Germany</conf-loc><conf-date>12&#8211;13 March 2019</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B17-sensors-25-05345"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Pointnet: Deep learning on point sets for 3d classification and segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>652</fpage><lpage>660</lpage></element-citation></ref><ref id="B18-sensors-25-05345"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Thomas</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Deschaud</surname><given-names>J.E.</given-names></name><name name-style="western"><surname>Marcotegui</surname><given-names>B.</given-names></name><name name-style="western"><surname>Goulette</surname><given-names>F.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Kpconv: Flexible and deformable convolution for point clouds</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>6411</fpage><lpage>6420</lpage></element-citation></ref><ref id="B19-sensors-25-05345"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lombacher</surname><given-names>J.</given-names></name><name name-style="western"><surname>Laudt</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hahn</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dickmann</surname><given-names>J.</given-names></name><name name-style="western"><surname>W&#246;hler</surname><given-names>C.</given-names></name></person-group><article-title>Semantic radar grids</article-title><source>Proceedings of the 2017 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Los Angeles, CA, USA</conf-loc><conf-date>11&#8211;14 June 2017</conf-date><fpage>1170</fpage><lpage>1175</lpage></element-citation></ref><ref id="B20-sensors-25-05345"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeller</surname><given-names>M.</given-names></name><name name-style="western"><surname>Behley</surname><given-names>J.</given-names></name><name name-style="western"><surname>Heidingsfeld</surname><given-names>M.</given-names></name><name name-style="western"><surname>Stachniss</surname><given-names>C.</given-names></name></person-group><article-title>Gaussian radar transformer for semantic segmentation in noisy radar data</article-title><source>IEEE Robot. Autom. Lett.</source><year>2022</year><volume>8</volume><fpage>344</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1109/LRA.2022.3226030</pub-id></element-citation></ref><ref id="B21-sensors-25-05345"><label>21.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name><name name-style="western"><surname>Vinyals</surname><given-names>O.</given-names></name><name name-style="western"><surname>Dean</surname><given-names>J.</given-names></name></person-group><article-title>Distilling the Knowledge in a Neural Network</article-title><year>2015</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1503.02531" ext-link-type="uri">https://arxiv.org/abs/1503.02531</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-15">(accessed on 15 August 2025)</date-in-citation></element-citation></ref><ref id="B22-sensors-25-05345"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mei</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name></person-group><article-title>Cross-layer distillation with semantic calibration</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Virtual</conf-loc><conf-date>2&#8211;9 February 2021</conf-date><fpage>7028</fpage><lpage>7036</lpage></element-citation></ref><ref id="B23-sensors-25-05345"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>M.</given-names></name></person-group><article-title>Relational knowledge distillation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>3967</fpage><lpage>3976</lpage></element-citation></ref><ref id="B24-sensors-25-05345"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Point-to-voxel knowledge distillation for lidar semantic segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>8479</fpage><lpage>8488</lpage></element-citation></ref><ref id="B25-sensors-25-05345"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>J.</given-names></name></person-group><article-title>Knowledge distillation from 3d to bird&#8217;s-eye-view for lidar semantic segmentation</article-title><source>Proceedings of the 2023 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Brisbane, Australia</conf-loc><conf-date>10&#8211;14 July 2023</conf-date><fpage>402</fpage><lpage>407</lpage></element-citation></ref><ref id="B26-sensors-25-05345"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>2dpass: 2d priors assisted semantic segmentation on lidar point clouds</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><fpage>677</fpage><lpage>695</lpage></element-citation></ref><ref id="B27-sensors-25-05345"><label>27.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>C.</given-names></name></person-group><article-title>ELiTe: Efficient Image-to-LiDAR Knowledge Transfer for Semantic Segmentation</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2405.04121" ext-link-type="uri">https://arxiv.org/abs/2405.04121</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-15">(accessed on 15 August 2025)</date-in-citation></element-citation></ref><ref id="B28-sensors-25-05345"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Unal</surname><given-names>O.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hoyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Can</surname><given-names>Y.B.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>2D feature distillation for weakly-and semi-supervised 3D semantic segmentation</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2024</conf-date><fpage>7336</fpage><lpage>7345</lpage></element-citation></ref><ref id="B29-sensors-25-05345"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mintun</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ravi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rolland</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gustafson</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Whitehead</surname><given-names>S.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Lo</surname><given-names>W.Y.</given-names></name><etal/></person-group><article-title>Segment anything</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>4015</fpage><lpage>4026</lpage></element-citation></ref><ref id="B30-sensors-25-05345"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Hallacy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ramesh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Goh</surname><given-names>G.</given-names></name><name name-style="western"><surname>Agarwal</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sastry</surname><given-names>G.</given-names></name><name name-style="western"><surname>Askell</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mishkin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Clark</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Learning transferable visual models from natural language supervision</article-title><source>Proceedings of the International Conference on Machine Learning. PMLR</source><conf-loc>Virtual</conf-loc><conf-date>18&#8211;24 July 2021</conf-date><fpage>8748</fpage><lpage>8763</lpage></element-citation></ref><ref id="B31-sensors-25-05345"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>A flexible new technique for camera calibration</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2000</year><volume>22</volume><fpage>1330</fpage><lpage>1334</lpage><pub-id pub-id-type="doi">10.1109/34.888718</pub-id></element-citation></ref><ref id="B32-sensors-25-05345"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Besl</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>McKay</surname><given-names>N.D.</given-names></name></person-group><article-title>Method for registration of 3-D shapes</article-title><source>Proceedings of the Sensor Fusion IV: Control Paradigms and Data Structures</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>12&#8211;15 November 1991</conf-date><fpage>586</fpage><lpage>606</lpage></element-citation></ref><ref id="B33-sensors-25-05345"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fischler</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Bolles</surname><given-names>R.C.</given-names></name></person-group><article-title>Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</article-title><source>Commun. ACM</source><year>1981</year><volume>24</volume><fpage>381</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1145/358669.358692</pub-id></element-citation></ref><ref id="B34-sensors-25-05345"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Berman</surname><given-names>M.</given-names></name><name name-style="western"><surname>Triki</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Blaschko</surname><given-names>M.B.</given-names></name></person-group><article-title>The lov&#225;sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>4413</fpage><lpage>4421</lpage></element-citation></ref><ref id="B35-sensors-25-05345"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>D.</given-names></name></person-group><article-title>Cylindrical and asymmetrical 3d convolution networks for lidar-based perception</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>6807</fpage><lpage>6822</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3098789</pub-id><pub-id pub-id-type="pmid">34310286</pub-id></element-citation></ref><ref id="B36-sensors-25-05345"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Point transformer v2: Grouped vector attention and partition-based pooling</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>33330</fpage><lpage>33342</lpage></element-citation></ref><ref id="B37-sensors-25-05345"><label>37.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Nowruzi</surname><given-names>F.E.</given-names></name><name name-style="western"><surname>Kolhatkar</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kapoor</surname><given-names>P.</given-names></name><name name-style="western"><surname>Heravi</surname><given-names>E.J.</given-names></name><name name-style="western"><surname>Hassanat</surname><given-names>F.A.</given-names></name><name name-style="western"><surname>Laganiere</surname><given-names>R.</given-names></name><name name-style="western"><surname>Rebut</surname><given-names>J.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>W.</given-names></name></person-group><article-title>Polarnet: Accelerated Deep Open Space Segmentation Using Automotive Radar in Polar Domain</article-title><year>2021</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2103.03387" ext-link-type="uri">https://arxiv.org/abs/2103.03387</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-18">(accessed on 18 August 2025)</date-in-citation></element-citation></ref><ref id="B38-sensors-25-05345"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Choy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gwak</surname><given-names>J.</given-names></name><name name-style="western"><surname>Savarese</surname><given-names>S.</given-names></name></person-group><article-title>4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>3075</fpage><lpage>3084</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05345-f001" orientation="portrait"><label>Figure 1</label><caption><p>A data example of the ZJUSSet. The top-left panel (<bold>a</bold>) displays the camera image. The bottom left (<bold>b</bold>) and right (<bold>c</bold>) show the corresponding 128-line Livox LiDAR and Oculii 4D radar point cloud, with annotated categories marked by different colors.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g001.jpg"/></fig><fig position="float" id="sensors-25-05345-f002" orientation="portrait"><label>Figure 2</label><caption><p>The experimental car with sensors and the coordinate system. The right image shows the enlarged view of the framed area in the left figure, with the details of the mounting sensors and their coordinate systems.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g002.jpg"/></fig><fig position="float" id="sensors-25-05345-f003" orientation="portrait"><label>Figure 3</label><caption><p>Distribution of radar points on ZJUSSet. (<bold>a</bold>) shows the average point number per frame for the data splits with respect to the range, while (<bold>b</bold>) shows the point distribution of each category in the entire train+val set.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g003.jpg"/></fig><fig position="float" id="sensors-25-05345-f004" orientation="portrait"><label>Figure 4</label><caption><p>The framework of our RaSS model. Both the LiDAR (teacher) and the 4D radar (student) branch use the same backbones. The radar point cloud is pre-processed by the Adaptive Doppler Compensation (ADC) module to eliminate the speed produced by the ego motion. Radar Feature Knowledge Distillation (RFKD) module is designed to enhance the radar&#8217;s feature extraction at different scales during training. In inference, only the radar branch is involved.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g004.jpg"/></fig><fig position="float" id="sensors-25-05345-f006" orientation="portrait"><label>Figure 6</label><caption><p>Illustration of the Radar Feature Knowledge Distillation (RFKD) module. For each layer, we separately aggregate the LiDAR and radar point features in the BEV view, apply the selection operation to both modality features, and then use a MLP-based adapter for radar features before calculating the MSE loss.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g006.jpg"/></fig><fig position="float" id="sensors-25-05345-f007" orientation="portrait"><label>Figure 7</label><caption><p>Qualitative results on the ZJUSSet validation set. The results are from the Cylinder3D-based RaSS (<bold>a</bold>). The meaning of colors in (<bold>b</bold>,<bold>c</bold>) are the same as in <xref rid="sensors-25-05345-f001" ref-type="fig">Figure 1</xref>c. For clarity, the correct and incorrect segmentation results are separately marked in black and red in (<bold>d</bold>,<bold>e</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g007.jpg"/></fig><fig position="float" id="sensors-25-05345-f008" orientation="portrait"><label>Figure 8</label><caption><p>Failure cases on the ZJUSSet validation set. The meaning of colors in (<bold>b</bold>,<bold>c</bold>) are the same as in <xref rid="sensors-25-05345-f001" ref-type="fig">Figure 1</xref>c. For clarity, the correct and incorrect segmentation results are separately marked in black and red in (<bold>d</bold>). Different colored circles in (<bold>a</bold>,<bold>d</bold>) denote distinct categories.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05345-g008.jpg"/></fig><table-wrap position="float" id="sensors-25-05345-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05345-t001_Table 1</object-id><label>Table 1</label><caption><p>The semantic segmentation datasets containing mm-Wave radar data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Radar</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Camera</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LiDAR</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Annotation</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Categories</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CARRADA&#160;[<xref rid="B7-sensors-25-05345" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">3D</td><td align="center" valign="middle" rowspan="1" colspan="1">RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">2D Box, 2D Pixel</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RADIal&#160;[<xref rid="B8-sensors-25-05345" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">3D</td><td align="center" valign="middle" rowspan="1" colspan="1">RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">16-line</td><td align="center" valign="middle" rowspan="1" colspan="1">2D Box, 2D point</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RadarScenes&#160;[<xref rid="B9-sensors-25-05345" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2D Point</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ZJUSSet (<bold>Ours</bold>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Livox (&#8764;128-line)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D Point</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05345-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05345-t003_Table 3</object-id><label>Table 3</label><caption><p>IoU, Precision, and Recall Metrics for Each Category in the Validation set of ZJUSSet. The results are from the PT-V2-based RaSS.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Building</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fence</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Vegetation</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cyclist</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pedestrian</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Truck</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bus</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Tricycle</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">IoU</td><td align="center" valign="middle" rowspan="1" colspan="1">74.09</td><td align="center" valign="middle" rowspan="1" colspan="1">63.46</td><td align="center" valign="middle" rowspan="1" colspan="1">61.99</td><td align="center" valign="middle" rowspan="1" colspan="1">69.06</td><td align="center" valign="middle" rowspan="1" colspan="1">9.66</td><td align="center" valign="middle" rowspan="1" colspan="1">51.74</td><td align="center" valign="middle" rowspan="1" colspan="1">7.82</td><td align="center" valign="middle" rowspan="1" colspan="1">42.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">79.20</td><td align="center" valign="middle" rowspan="1" colspan="1">76.82</td><td align="center" valign="middle" rowspan="1" colspan="1">80.90</td><td align="center" valign="middle" rowspan="1" colspan="1">80.95</td><td align="center" valign="middle" rowspan="1" colspan="1">13.44</td><td align="center" valign="middle" rowspan="1" colspan="1">85.79</td><td align="center" valign="middle" rowspan="1" colspan="1">21.56</td><td align="center" valign="middle" rowspan="1" colspan="1">79.10</td><td align="center" valign="middle" rowspan="1" colspan="1">2.57</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.46</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05345-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05345-t004_Table 4</object-id><label>Table 4</label><caption><p>Semantic segmentation results with pseudo-labels on the VoD Validation set. L and R separately stand for LiDAR and mm-Wave radar data. RaSS(C) and RaSS(P) represents RaSS using Cylinder3D and PT-V2 as backbone, respectively. The bold and underlined values denote the best and second place within radar input methods, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Input</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pedestrian</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cyclist</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Motor</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Truck</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bicycle</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cylinder3D</td><td align="center" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">74.82</td><td align="center" valign="middle" rowspan="1" colspan="1">89.93</td><td align="center" valign="middle" rowspan="1" colspan="1">87.19</td><td align="center" valign="middle" rowspan="1" colspan="1">79.62</td><td align="center" valign="middle" rowspan="1" colspan="1">61.51</td><td align="center" valign="middle" rowspan="1" colspan="1">43.80</td><td align="center" valign="middle" rowspan="1" colspan="1">86.89</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PTV2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.05</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cylinder3D</td><td align="center" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">61.22</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>87.81</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">71.97</td><td align="center" valign="middle" rowspan="1" colspan="1">77.02</td><td align="center" valign="middle" rowspan="1" colspan="1">1.50</td><td align="center" valign="middle" rowspan="1" colspan="1">71.10</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>57.92</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PTV2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>72.49</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>80.91</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>84.55</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.44</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RaSS(C)</td><td align="center" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>65.73</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>87.63</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>75.17</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>82.17</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>6.64</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>86.64</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>56.15</underline>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RaSS(P)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>64.49</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>38.81</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.71</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05345-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05345-t005_Table 5</object-id><label>Table 5</label><caption><p>Performance comparison of different configurations on ZJUSSet. Herein, both the baseline and RaSS employ Cylinder3D as their backbone. ADC stands for the Adaptive Doppler Compensation. K represents the number of distillation layers adopted. Selected (marked with ) refers to the Selected feature distillation described in <xref rid="sec4dot3-sensors-25-05345" ref-type="sec">Section 4.3</xref>, whereas an empty entry denotes distillation for all non-empty radar grids, regardless of whether there are corresponding non-empty LiDAR features at that location.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Network</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ADC</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Logits-KD</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feats-KD</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">K</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Selected</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Baseline</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.15</td></tr><tr><td rowspan="7" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">RaSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">37.39</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">38.46</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">40.58</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">40.29</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>41.57</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">38.36</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.77</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>