<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431475</article-id><article-id pub-id-type="pmcid-ver">PMC12431475.1</article-id><article-id pub-id-type="pmcaid">12431475</article-id><article-id pub-id-type="pmcaiid">12431475</article-id><article-id pub-id-type="doi">10.3390/s25175496</article-id><article-id pub-id-type="publisher-id">sensors-25-05496</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dual-Stream Attention-Enhanced Memory Networks for Video Anomaly Detection</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Gao</surname><given-names initials="W">Weishan</given-names></name><xref rid="af1-sensors-25-05496" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="X">Xiaoyin</given-names></name><xref rid="af1-sensors-25-05496" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="Y">Ye</given-names></name><xref rid="af1-sensors-25-05496" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jing</surname><given-names initials="X">Xiaochuan</given-names></name><xref rid="af1-sensors-25-05496" ref-type="aff">1</xref><xref rid="af2-sensors-25-05496" ref-type="aff">2</xref><xref rid="c1-sensors-25-05496" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Yitzhaky</surname><given-names initials="Y">Yitzhak</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05496"><label>1</label>China Aerospace Academy of Systems Science and Engineering, Beijing 100048, China; <email>gaowsh518@163.com</email> (W.G.); <email>wxy690@126.com</email> (X.W.); <email>15066241262@163.com</email> (Y.W.)</aff><aff id="af2-sensors-25-05496"><label>2</label>Aerospace Hongka Intelligent Technology (Beijing) Co., Ltd., Beijing 100048, China</aff><author-notes><corresp id="c1-sensors-25-05496"><label>*</label>Correspondence: <email>ht12hk@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>04</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5496</elocation-id><history><date date-type="received"><day>24</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>28</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 14:25:13.570"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05496.pdf"/><abstract><p>Weakly supervised video anomaly detection (WSVAD) aims to identify unusual events using only video-level labels. However, current methods face several key challenges, including ineffective modelling of complex temporal dependencies, indistinct feature boundaries between visually similar normal and abnormal events, and high false alarm rates caused by an inability to distinguish salient events from complex background noise. This paper proposes a novel method that systematically enhances feature representation and discrimination to address these challenges. The proposed method first builds robust temporal representations by employing a hierarchical multi-scale temporal encoder and a position-aware global relation network to capture both local and long-range dependencies. The core of this method is the dual-stream attention-enhanced memory network, which achieves precise discrimination by learning distinct normal and abnormal patterns via dual memory banks, while utilising bidirectional spatial attention to mitigate background noise and focus on salient events before memory querying. The models underwent a comprehensive evaluation utilising solely RGB features on two demanding public datasets, UCF-Crime and XD-Violence. The experimental findings indicate that the proposed method attains state-of-the-art performance, achieving 87.43% AUC on UCF-Crime and 85.51% AP on XD-Violence. This result demonstrates that the proposed &#8220;attention-guided prototype matching&#8221; paradigm effectively resolves the aforementioned challenges, enabling robust and precise anomaly detection.</p></abstract><kwd-group><kwd>video anomaly detection</kwd><kwd>deep learning</kwd><kwd>video understanding</kwd><kwd>attention mechanism</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05496"><title>1. Introduction</title><p>As urban security demands escalate, video anomaly detection (VAD) is increasingly implemented in intelligent surveillance systems to identify anomalous events in lengthy unedited videos, enhancing system response efficiency while minimising manual surveillance costs [<xref rid="B1-sensors-25-05496" ref-type="bibr">1</xref>]. Current VAD approaches are primarily categorised into four types based on annotation granularity: fully supervised, semi-supervised, unsupervised, and weakly supervised [<xref rid="B2-sensors-25-05496" ref-type="bibr">2</xref>]. While fully supervised techniques can theoretically attain superior accuracy, their dependence on precise frame-level annotations incurs significantly high labour costs. It limits the capacity of the model to generalise across varied real-world situations. Conversely, weakly supervised methods necessitate merely video-level annotations, achieving a commendable equilibrium between labelling efficiency and detection efficacy, so positioning them as a pivotal technological avenue for addressing extensive real-world surveillance challenges.</p><p>Despite its advantages, the prevailing weakly supervised video anomaly detection (WSVAD) paradigm, which typically employs a multi-instance learning (MIL) framework [<xref rid="B3-sensors-25-05496" ref-type="bibr">3</xref>], faces significant challenges that this paper aims to address. These challenges can be categorised into three main areas: high false alarm rates, indistinct feature boundaries stemming from the MIL framework itself, and critical limitations in temporal modelling.</p><p>Weakly supervised video anomaly detection (WSVAD) typically employs a multi-instance learning (MIL) framework [<xref rid="B3-sensors-25-05496" ref-type="bibr">3</xref>], wherein the video is regarded as a &#8220;bag&#8221; comprising many segments, hence converting the anomaly detection problem into the identification of &#8220;key instances&#8221; within the bag. This technique inherently causes the model to concentrate on the highest-scoring anomalous segments, therefore inadequately modelling the numerous normal segments. This inclination to prioritise the abnormal over the normal results in two significant challenges: firstly, in the spatial dimension, the model fails to comprehend normal patterns fully and is susceptible to misclassifying normal movements in intricate contexts as abnormal, such as detecting a sudden change in illumination during nighttime surveillance or a brief presence in a crowd as a false alarm. Secondly, in the feature dimension, the model struggles to establish a distinct decision boundary, causing similar motion characteristics, such as normal walking and abnormal running, to be easily conflated, which results in feature redundancy and diminished discriminative capacity.</p><p>Moreover, current methodologies depend on single-scale convolution or attention mechanisms for temporal modelling, hindering the ability to capture both the immediate onset of events and the long-term progression of the process [<xref rid="B4-sensors-25-05496" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05496" ref-type="bibr">5</xref>]. This is susceptible to feature deterioration in deep networks. To address the aforementioned challenges, this paper offers a dual-stream attention and memory-enhanced network that fundamentally tackles the issue of boundary blurring. To specifically tackle the temporal modelling limitations, this study also develops a hierarchical multi-scale temporal encoder architecture. It ensures seamless information flow via residual connections, leveraging the robust capabilities of temporal convolutional networks [<xref rid="B6-sensors-25-05496" ref-type="bibr">6</xref>] and multi-scale kernels to effectively capture intricate dependencies across various time scales. The core of the proposed method, the dual-stream attention module, employs a dual-channel attention method to assign weights to features in both spatial and channel dimensions, therefore precisely identifying essential regions in the video and mitigating irrelevant information interference. By individually generating normal and abnormal memory banks, the model may independently learn the prototype distributions of both sample types, in contrast to prior memory networks that solely rebuilt the normal pattern [<xref rid="B7-sensors-25-05496" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05496" ref-type="bibr">8</xref>]. The integration of the memory improvement mechanism with the Top-K selection process aligns and refines the features, leading to highly discriminative feature representations and significantly mitigating the probability of misjudgement. This primary research focus and contributions of this paper are outlined as follows:<list list-type="bullet"><list-item><p>A hierarchical multi-scale temporal encoder architecture is developed, incorporating multi-scale modelling capabilities and integrating residual connectivity to increase the representation of intricate temporal aspects.</p></list-item><list-item><p>A dual-stream attention-enhanced memory network is introduced. This module utilises a bidirectional attention mechanism to refine features before interacting with the memory banks, enabling independent modelling of the prototype distributions of normal and abnormal samples. This design enables the model to effectively differentiate between the two feature types under weakly supervised settings.</p></list-item><list-item><p>The suggested method, thoroughly tested on two public datasets, UCF-Crime and XD-Violence, surpasses most contemporary popular methods while maintaining RGB unimodal inputs. It exhibits robust generalisation capability and practical application possibilities.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05496"><title>2. Related Works</title><sec id="sec2dot1-sensors-25-05496"><title>2.1. Weakly Supervised Video Anomaly Detection</title><p>While our work focuses on weakly supervised learning, another prominent paradigm is unsupervised learning, where models are trained without any video-level labels. Many of these methods focus on generating pseudo-supervision to guide the training. For instance, C2FPL [<xref rid="B9-sensors-25-05496" ref-type="bibr">9</xref>] creates coarse-to-fine pseudo-labels via clustering, DvAnNet [<xref rid="B10-sensors-25-05496" ref-type="bibr">10</xref>] refines pseudo anomaly scores using a dual-branch network, and GCL [<xref rid="B11-sensors-25-05496" ref-type="bibr">11</xref>] employs a generative cooperative learning framework for cross-supervision. Other recent approaches, like CLAP [<xref rid="B12-sensors-25-05496" ref-type="bibr">12</xref>], leverage large-scale cross-modal pre-trained models for this task. In contrast, weakly supervised video anomaly detection has evolved along two primary technical avenues: single-stage approaches grounded in multiple instance learning (MIL) and two-stage methodologies that employ pseudo-labelling for self-directed training.</p><p>The single-stage methodology is exemplified by the fundamental MIL framework introduced by Sultani et al. [<xref rid="B3-sensors-25-05496" ref-type="bibr">3</xref>], which categorises movies into positive and negative &#8220;packets&#8221; and enhances the separation between the highest scoring segments in these packets using sorting loss. Nonetheless, the primary issue with this methodology is its excessive dependence on the presumption that &#8220;anomalous clips possess pronounced features,&#8221; with its optimisation objective centred on &#8220;identifying&#8221; anomalies, significantly overlooking the modelling of extensive and varied normal patterns. Subsequent works, including the top-k mechanism by Tian et al. [<xref rid="B13-sensors-25-05496" ref-type="bibr">13</xref>] and the centre-of-mass loss proposed by Wan et al. [<xref rid="B14-sensors-25-05496" ref-type="bibr">14</xref>], aim to enhance feature extraction; however, they remain anchored to this fundamental concept. Another approach, BN-WVAD [<xref rid="B15-sensors-25-05496" ref-type="bibr">15</xref>], is inspired by the statistical insight that features of abnormal events often exhibit outlier characteristics. It leverages the Divergence of Feature from the Mean (DFM) from BatchNorm statistics as an additional, noise-resilient anomaly score to amend the predictions of the primary classifier. Consequently, these methods encounter difficulties in managing false alarms and generalising the feature space, complicating the distinction between normal behaviours in intricate contexts and genuine anomalies.</p><p>Another technical approach is to employ pseudo-labels for self-supervised training. For instance, Feng et al. [<xref rid="B16-sensors-25-05496" ref-type="bibr">16</xref>] and Zhang et al. [<xref rid="B17-sensors-25-05496" ref-type="bibr">17</xref>] improve the quality of pseudo-labels via generators or evaluation methods. Similarly, OE-CTST [<xref rid="B18-sensors-25-05496" ref-type="bibr">18</xref>] develops a temporal transformer framework that uses an Outlier Embedder and a Cross Temporal Scale Transformer to better model the temporal dynamics of both long and short anomalies. While these methods enhance the precision of anomaly detection, their pseudo-labels are predominantly derived from the higher-scoring suspected anomalous segments, neglecting the significance of structural information from normal samples, thereby leaving the distinction between normal and anomalous ambiguous. Furthermore, specific approaches, like the graph convolutional network proposed by Li et al. [<xref rid="B19-sensors-25-05496" ref-type="bibr">19</xref>], attempt to simulate fragment interactions; nevertheless, their static topology poses challenges in adapting to the dynamically evolving temporal dependencies. In contrast, MTFL [<xref rid="B20-sensors-25-05496" ref-type="bibr">20</xref>] proposes a Multi-Timescale Feature Learning method, which enhances feature representation by employing a Video Swin Transformer on short, medium, and long temporal tubelets. The approaches above typically exhibit insufficient residual connectivity in the construction of deep networks, resulting in feature degradation issues and constraining the deep expressive capability of the model.</p><p>In summary, the design of contemporary mainstream technological approaches presents several intrinsic limitations, particularly an inability to precisely target critical spatio-temporal information. To address this, various attention mechanisms have been explored. Some approaches generate attention maps using a hybrid of classic and deep learning methods. For instance, Shoaib et al. [<xref rid="B21-sensors-25-05496" ref-type="bibr">21</xref>] proposes a visual attention mechanism where motion regions are first identified using a background subtraction algorithm before being processed by a 3D CNN. While effective, such methods can be sensitive to complex background dynamics. More recent works have focused on end-to-end learnable attention, often in multi-modal settings. A prominent example is the work by Ghadiya et al. [<xref rid="B22-sensors-25-05496" ref-type="bibr">22</xref>], which introduces a hyperbolic Lorentzian graph attention to capture hierarchical relationships between audio-visual features. Recent trends also leverage large pre-trained models from the language domain to enhance video understanding. AnomalyCLIP [<xref rid="B4-sensors-25-05496" ref-type="bibr">4</xref>], for example, was the first to combine a Vision-Language Model (VLM) like CLIP with the MIL framework, using text prompts about normalcy to learn text-driven feature space directions for identifying anomalies. Similarly, MIL-BERT [<xref rid="B23-sensors-25-05496" ref-type="bibr">23</xref>] adapts the BERT architecture to improve performance via explicit whole-video classification, where it aggregates all snippet features into a single video-level representation.</p><p>To resolve these problems in a unimodal setting, this paper develops a dual-stream attention and memory-enhanced network that directly addresses the issues of feature space border ambiguity and elevated false alarm rates via its dual-attention mechanism and separate normal and abnormal prototype memory banks. The hierarchical multi-scale temporal encoder we developed effectively addresses the limitations of temporal modelling and feature degradation through its multi-scale architecture and residual connections.</p></sec><sec id="sec2dot2-sensors-25-05496"><title>2.2. Memory Networks</title><p>Memory networks are neural networks equipped with external storage that can read and write data, capture long-term dependencies during training, and employ stored memory components to produce outcomes during inference. Memory networks were initially utilised for text-based question&#8211;answer activities to preserve long-term memory by storing memory items related to scene information. This type of model requires supervision at each layer during training, complicating the backpropagation training process. Sukhbaatar et al. [<xref rid="B24-sensors-25-05496" ref-type="bibr">24</xref>] presented a continuous memory network that may be taught end-to-end to address this issue, hence broadening its applicability to other tasks.</p><p>At present, memory networks are primarily utilised for visual anomaly detection in unsupervised learning contexts. Gong et al. [<xref rid="B7-sensors-25-05496" ref-type="bibr">7</xref>] introduced a memory-enhanced self-encoder to address the issue of excessive reconstruction of anomalous events by the self-encoder. Rather than directly supplying the input image to the decoder, the model interprets it as a query, collects the most pertinent memory elements from the memory module for integration, and subsequently, the decoder finalises the image reconstruction. Park et al. [<xref rid="B25-sensors-25-05496" ref-type="bibr">25</xref>] contended that the conventional singular prototypical feature fails to encompass the multimodal attributes of normal data. Consequently, they introduced a novel memory module wherein each memory item embodies a normal modal prototypical feature, regulating the relationship between features and memory items through feature compactness loss and feature separation loss. Nevertheless, the technique continues to inadequately address the alignment issue between memory content and temporal structure, rendering its modelling of extended time-series events somewhat flawed.</p><p>Nonetheless, current memory network-based methodologies have some significant drawbacks. Firstly, a solitary memory module struggles to encapsulate the intricate dynamic information included in a video accurately. Secondly, the absence of precise labelling at the frame level hinders present approaches from effectively capturing the category information of video frames, hence constraining their applicability in weakly supervised learning contexts. Moreover, the majority of approaches employ a predetermined quantity of memory terms, a hyperparameter set manually that remains invariant throughout training, hence constraining the adaptability of the model to varying video contexts. To address the issues above, new research has commenced experimentation with the concept of integrating dual memory modules with uncertainty modelling to enhance adaptability to weakly supervised tasks; for instance, the DMU technique [<xref rid="B26-sensors-25-05496" ref-type="bibr">26</xref>] exemplifies this approach. This work presents a memory information distillation module that integrates dual memory modules and dual attention techniques for both channel and spatial dimensions. The essence of the module is to adaptively discern &#8220;what&#8221; and &#8220;where&#8221; through the synergistic interplay of channel and spatial attention, hence extracting more distinctive traits. Subsequently, these enhanced capabilities are retained and disseminated in standard and atypical prototypes utilising the engineered split dual memory unit. This approach enables the model to reliably differentiate between normal and abnormal patterns and successfully adapt to poorly supervised environments, hence greatly enhancing detection performance and minimising false alarm rates.</p></sec></sec><sec id="sec3-sensors-25-05496"><title>3. Proposed Method</title><p>This paper proposes a weakly supervised video anomaly detection method designed to systematically tackle the fundamental problems of existing methods in temporal modelling and precise classification using a problem-driven, layer-by-layer architecture, as illustrated in <xref rid="sensors-25-05496-f001" ref-type="fig">Figure 1</xref>.</p><p>This paper firstly confronts the primary problem in the weakly supervised context: the proficient modelling of intricate temporal relationships. To tackle this difficulty, we developed the Hierarchical Multi-Scale Temporal Encoder (HMTE) to effectively capture intricate local and global dynamic properties via its multi-scale convolutional architecture (see <xref rid="sec3dot1-sensors-25-05496" ref-type="sec">Section 3.1</xref> for further details).</p><p>Nonetheless, it is challenging to capture non-adjacent yet semantically linked global connections just by depending on the fixed receptive fields of HMTE. Consequently, we present the Position-Aware Global Relation Modelling module (PGRN) to augment the global contextual comprehension of the model by integrating the self-attention mechanism of content and locational data (see <xref rid="sec3dot3-sensors-25-05496" ref-type="sec">Section 3.3</xref> for further details).</p><p>Upon acquiring robust spatio-temporal features, the primary objective is to execute precise and dependable anomaly differentiation. This is where the Dual-Stream Attention-Enhanced Memory Network (DEMN), the principal breakthrough of this study, is introduced. This module transitions from feature characterisation to prototype-level differentiation by creating a prototype memory and integrating it with bi-directional attention (see <xref rid="sec3dot2-sensors-25-05496" ref-type="sec">Section 3.2</xref> for specifics).</p><p>The comprehensive architecture is supported by a dynamic residual module for deep structure (refer to <xref rid="sec3dot1-sensors-25-05496" ref-type="sec">Section 3.1</xref> for specifics). It is concurrently trained using a multi-objective co-optimisation loss function (see <xref rid="sec3dot4-sensors-25-05496" ref-type="sec">Section 3.4</xref> for further details).</p><sec id="sec3dot1-sensors-25-05496"><title>3.1. Hierarchical Multi-Scale Temporal Encoder</title><p>To address the challenge that conventional MIL frameworks struggle to adequately capture intricate temporal patterns and long-range dependencies, this paper develops a Hierarchical Multi-Scale Temporal Encoder (HMTE). The core role of the HMTE is to thoroughly extract both local temporal features and long temporal dependencies from video frame sequences. It achieves this through a multi-layer architecture of stacked dilated convolutions, as illustrated in <xref rid="sensors-25-05496-f002" ref-type="fig">Figure 2</xref>, allowing the model to perceive temporal dynamics across various scales.</p><p>The embedding layer employs hierarchical temporal feature extraction through a multilayer convolutional architecture, wherein each layer transforms the input sequence into a high-dimensional feature space via convolutional operations to elucidate the intricate dependency patterns of the time series. Let the input tensor be denoted as <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> representing the batch size, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> indicating the time step duration, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> signifying the number of features per time step. The objective is to transform the input into an output tensor <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the output feature dimension, via the embedding layer. The output of layer <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> can be articulated as:<disp-formula id="FD1-sensors-25-05496"><label>(1)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8727;</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the equation above, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8727;</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the dilation convolution operation, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>&#183;</mml:mo></mml:mrow></mml:mfenced><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the nonlinear activation function ReLU, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> describe the convolution kernel and the bias term of layer <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. The dilation convolution is defined as follows:<disp-formula id="FD2-sensors-25-05496"><label>(2)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#183;</mml:mo><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>r</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the dimension of the convolution kernel, whereas <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the dilation factor. Dilation convolution enhances the receptive field efficiently without augmenting the parameter count or the convolution kernel size, which is essential for managing extensive time series. By structuring distinct layers to utilise varying sequences of dilation factors (e.g., <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>), the embedding layers can concentrate on diverse scales of temporal information across multiple layers, hence facilitating multi-scale feature extraction. The layer-by-layer integration of multilayer convolution and activation functions enables the model to extract both local and global time-dependent characteristics, thereby considerably improving its capacity to differentiate between complicated anomalous patterns and typical behavioural patterns.</p><p>To ensure that the HMTE can still be effectively trained after stacking multiple layers, this paper incorporates a dynamic residual feature enhancement module, as illustrated in <xref rid="sensors-25-05496-f003" ref-type="fig">Figure 3</xref>. The residual block is engineered to mitigate the vanishing gradient issue in deep networks by incorporating shortcut links, hence facilitating more efficient information flow over multiple layers. The fundamental procedure of the residual block commences with the input signal <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The input signal is initially processed by the first convolutional layer for feature extraction, which typically utilises a small convolutional kernel (e.g., 3 &#215; 3) to collect local features. The convolutional layer alters the spatial configuration of the input signal into a collection of feature mappings, offering a more representative input for further processing. Upon completion of the convolution procedure, the feature mappings are transmitted to a batch normalisation layer. Batch normalisation aims to standardise the inputs of each layer by setting the mean of each feature to zero and the variance to one, hence mitigating internal covariate shift. This procedure not only accelerates convergence but also makes the model more stable. Following batch normalisation, the ReLU activation function is applied to the convolved and normalised feature mapping to incorporate nonlinear characteristics. The nonlinearly triggered feature mappings are subsequently input into a second convolutional layer for further feature extraction. Like the initial convolutional layer, the second convolutional layer utilises a 3 &#215; 3 convolutional kernel and retains the same number of channels to preserve feature dimensionality. The output of this convolutional layer undergoes batch normalisation to help stabilise the learning of the model. At this juncture, the fundamental characteristics of the residual block are activated. Upon the completion of the second convolution and batch normalisation of the feature mapping, the output <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is summed with the original input <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, constituting a crucial element in residual learning. This addition operation enables the model to preserve the original information of the input signal while also facilitating information movement within the deep network. It is articulated expressly as:<disp-formula id="FD3-sensors-25-05496"><label>(3)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mrow><mml:munder accentunder="false"><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#9183;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder accentunder="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#9183;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">short</mml:mi><mml:mo stretchy="false">-</mml:mo><mml:mi mathvariant="italic">circuit</mml:mi><mml:mo>&#160;</mml:mo><mml:mi mathvariant="italic">connections</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2-sensors-25-05496"><title>3.2. Dual-Stream Attention-Enhanced Memory Network</title><p>After acquiring robust spatio-temporal features, the next critical challenge is to perform precise and reliable anomaly differentiation. To solve the problem that initial deep features often contain substantial duplicate and irrelevant information that obstructs final judgement, this paper introduces the Dual-Stream Attention-Enhanced Memory Network (DS-AEMN). The purpose of this module is twofold: first, its attention mechanism refines input features by focusing on essential spatio-temporal information; second, its learnable memory bank acts as prior knowledge to augment the features&#8217; discriminative power.</p><p>The fundamental premise of the approach posits that anomalous events can be viewed as deviations from recognised typical patterns. To formalise this previous knowledge, two parallel, trainable memory banks are introduced: a normal memory bank (<inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) and an abnormal memory bank (<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). Beyond simply improving accuracy, the goal of this dual-memory architecture is to make the model more transparent and interpretable. Each memory item, within these banks is intended to learn a semantically distinct pattern. For instance, different prototypes in <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> might learn to represent common normal events like &#8216;walking pedestrians&#8217; or &#8216;passing cars&#8217;. Conversely, prototypes in <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are expected to specialise in different types of anomalies, such as one prototype capturing &#8216;physical fights&#8217; while another captures &#8216;sudden explosions&#8217;. This approach enables the model to statistically assess real-time inputs against these learned &#8220;standard&#8221; templates, allowing practitioners to gain insight into not just whether an anomaly occurred, but also what kind of event the model identified, thus establishing a definitive and transparent basis for discriminating.</p><p>Before comparing with the memorised templates, the purification of the input features is an essential step. Typically, a designated spatial area (&#8220;where did the event transpire?&#8221;) and a defined feature channel (&#8220;what type of event was it?&#8221;) are critical in a video clip. A bidirectional spatial attention module has been developed to identify and enhance this essential information dynamically. To attain this objective, the module separates the attention challenge into two basic components: localising physically significant locations (&#8220;where) and finding semantically significant channels (&#8220;what&#8221;). Spatial attention produces a spatial weight map using the convolution operation <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which emphasises the areas where events transpire. Simultaneously, channel attention allocates weights to each feature channel via <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, emphasising the most informative feature dimensions.</p><p>The two attention modules do not function separately; instead, they combine synergistically through element-wise multiplication <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to attain enhanced precision in concentration, as shown in <xref rid="sensors-25-05496-f004" ref-type="fig">Figure 4</xref>. The advantage of this fusion strategy is that it assigns a greater final attention weight to a feature only when it occupies a critical spatial location and simultaneously belongs to an essential feature channel. This synergy efficiently mitigates background noise and significantly enhances the signal-to-noise ratio of the feature. Ultimately, we utilise this aggregated attention weight on the input data to produce the improved feature <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#183;</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which establishes the groundwork for ensuing memory searches.</p><p><xref rid="sensors-25-05496-f004" ref-type="fig">Figure 4</xref> illustrates the structure of the dual-stream attention mechanism. The spatial attention module identifies the most informative spatial regions, while the channel attention module highlights the most discriminative feature channels. These two attention maps are then fused through element-wise multiplication to produce a refined attention weight map, which enhances the most relevant spatio-temporal features and suppresses irrelevant background noise. Upon acquiring the meticulously honed qualities, they are utilised as queries to engage with the memory bank. The Query process initially acquires a matrix of similarity scores between the input features and all memory templates by calculating the dot product and applying the softmax activation function <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, here <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the matrix of <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> input features, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the memory bank containing <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> prototype templates, and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the dimension of the features. The softmax function is applied row-wise, converting the similarity scores for each input feature into a probability distribution over the <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> prototypes. This paper additionally utilises the Top-K selection technique to enhance the robustness of query results and mitigate the influence of secondary templates. This technique picks the <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> templates that most closely align with the input and calculates the final query score <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> by averaging their similarity scores, <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the set of indices for the top <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> scores in that row. This final score, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, consistently indicates the degree to which the input corresponds to an entire category of prototype patterns.</p><p>The advantages of the query process manifest in two aspects: the generation of a discriminant score and the acquisition of a memory enhancement feature, <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a reconstructed feature representation for the input, created by computing a weighted sum of all prototype vectors from the memory bank (<inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), using the similarity scores in the matrix <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as the corresponding weights. This enhancement feature efficiently addresses essential information that may be confusing or absent in the original representation by integrating the matched prototype information back into the input features. Specifically, for sparse anomalous occurrences, the input samples often engage just a limited number of templates in the anomaly memory bank, allowing the model to selectively employ the most pertinent prior knowledge for enhanced recognition of certain anomaly kinds.</p></sec><sec id="sec3dot3-sensors-25-05496"><title>3.3. Position-Aware Global Relation Network</title><p>While the HMTE (<xref rid="sec3dot1-sensors-25-05496" ref-type="sec">Section 3.1</xref>) adeptly captures local timing characteristics, its rigid convolutional architecture possesses an intrinsic constraint: it struggles to form dynamic, content-oriented global associations between non-adjacent yet semantically significant keyframes. To overcome this bottleneck, this paper introduces the Position-Aware Global Relation Network (PGRN). The primary role of the PGRN is to enhance the global contextual refinement of the feature sequences produced by HMTE. It achieves this by incorporating a modified self-attention mechanism, which successfully integrates content-driven similarity with temporal proximity by adding a relative distance-based attention bias to the conventional scaled dot-product attention.</p><p>Consequently, with the query (<italic toggle="yes">Q</italic>), key (<italic toggle="yes">K</italic>), and value (<italic toggle="yes">V</italic>) matrices, the resultant attentional weight matrix <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is conclusive and the attentional <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> may be succinctly expressed as:<disp-formula id="FD4-sensors-25-05496"><label>(4)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow><mml:mspace linebreak="newline"/></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the dimension of each attention head, and the bias term <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated from the relative distances <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> via a compact learnable network, integrating essential location-sensing functionalities into the model.</p><p>The suggested location-aware self-attention layer is integrated into conventional Transformer encoder blocks within the overall design. Each encoder block comprises a multi-head self-attention mechanism and a feed-forward neural network (FFN), incorporating residual connections and layer normalisation to facilitate efficient information transfer and training stability. For layer <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the comprehensive computational procedure of layer <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> can be articulated as:<disp-formula id="FD6-sensors-25-05496"><label>(5)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">LN</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="italic">MultiHeadAttn</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">LN</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FFN</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow><mml:mspace linebreak="newline"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, the PGRN module transcends fixed spatio-temporal neighbourhood restrictions, capturing genuine global dependencies that integrate content and location information, so facilitating a comprehensive understanding of the deeper semantics of the entire video sequence.</p></sec><sec id="sec3dot4-sensors-25-05496"><title>3.4. Multi-Objective Collaborative Optimisation Loss Function</title><p>In the anomalous behaviour recognition framework presented in this paper, the ultimate categorisation choice arises from the collaboration of numerous subtasks. A singular loss function, such as Binary Cross Entropy (BCE), while offering fundamental categorisation guidance, fails to enforce nuanced constraints on critical components (e.g., memory banks) inside the model. This paper develops a multi-objective composite loss function aimed at converting domain-specific prior knowledge (such as feature distinctions between normal and abnormal) and the structural advantages of the model (such as the memory bank) into precise, optimisable mathematical constraints that facilitate more efficient and robust learning for the model.</p><p>The design initiates with a principal classification objective. We utilise binary cross-entropy loss (BCE) as the principal supervisory term, which serves as the key source of gradients for comprehensive end-to-end training of the model. We employ the output of the attention mechanism, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, to calculate the anomaly score by averaging the top-k clips with the highest attention scores, thereby representing the anomaly probability of the entire video and effectively concentrating on critical moments while disregarding background noise. The Anomaly Loss of this core is delineated as follows:<disp-formula id="FD8-sensors-25-05496"><label>(6)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To maximise the utility of the normal and anomalous memory pools in the model, we implement a series of prototype memory restrictions aimed at establishing a &#8220;compact within-class, separated between-class&#8221; memory feature structure. The objective is to develop a memory feature structure that is &#8220;intra-class compact and inter-class separated.&#8221; To attain intra-class compactness, we compel the responses of abnormal samples in abnormal memory banks (<inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and normal samples in normal memory banks (<inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) to converge to 1 via the loss functions for abnormal samples, <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and normal samples, <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively: <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. To attain inter-class separation, we propose a &#8220;crossover&#8221; loss <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> that compels the response of abnormal samples within the normal pool <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to approach 0. Additionally, we establish a more robust constraint, the abnormal loss of normal samples <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, to penalise elevated score responses of normal samples within the abnormal pool:<disp-formula id="FD9-sensors-25-05496"><label>(7)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Alongside enhancing the overall configuration of the feature space, we present general structural regularity terms that enforce geometric and distributional restrictions over the entire feature space. We present Triplet Loss, designed to enable the learnt features to establish an effective geometric structure in the metric space, positioning comparable samples in proximity and dissimilar samples at a distance. Simultaneously, we impose distributional and distance constraints: the KL Scatter Loss <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is employed to avert the variance collapse of the latent Gaussian distribution model devised for normal data, thereby ensuring the smoothness of the latent space. At the same time, the Distance Loss (<inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) directly enhances the separability of the final features by explicitly penalising the distances between normal and outlier features.</p><p>The comprehensive loss function is formulated as a weighted summation of the following components:<disp-formula id="FD10-sensors-25-05496"><label>(8)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The weight of the core classification loss, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is set at 1.0, serving as the standard for optimisation. All remaining loss terms are auxiliary or regularisation components, with weights <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> that adjust the contributions of various learning objectives. For instance, <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> governs the level of oversight on the memory bank module, whereas <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> modulate the structural limitations on the feature space and data distribution. These weights seek to establish an equilibrium that stabilises the training process and optimises the performance of the model.</p></sec></sec><sec id="sec4-sensors-25-05496"><title>4. Experiments</title><p>This section intends to assess our suggested method via a series of systematic experiments. We will evaluate the performance against contemporary state-of-the-art methodologies using the public datasets UCF-Crime [<xref rid="B3-sensors-25-05496" ref-type="bibr">3</xref>] and XD-Violence [<xref rid="B27-sensors-25-05496" ref-type="bibr">27</xref>]. Our experimental design will focus intently on the fundamental challenges addressed in the preceding section to systematically evaluate our proposed technical solutions. Firstly, we will assess the foundational timing modelling capabilities by juxtaposing its performance with that of conventional models, and evaluate whether the HMTE module can proficiently capture both long- and short-term interdependence of intricate events. We will determine the background suppression capability of the model via directional tests and performance comparisons in highly disturbed environments, and evaluate the anomalous prototype fitting ability of the bi-directional memory bank based on its performance with the label-ambiguous XD-Violence dataset. Ultimately, we will comprehensively validate the final feature discrimination capability of the model through specialised fine-grained anomaly classification tests to assess its efficacy in differentiating comparable, ambiguous abnormalities. This section seeks to illustrate that each design in our problem-driven validation approach provides a distinct technological solution, hence highlighting its specific advantages over current methodologies.</p><sec id="sec4dot1-sensors-25-05496"><title>4.1. Dataset and Evaluation Criteria</title><p>To assess the efficacy of the proposed strategy, we performed comprehensive evaluations on two prominent anomaly detection datasets, UCF-Crime [<xref rid="B3-sensors-25-05496" ref-type="bibr">3</xref>] and XD-Violence [<xref rid="B27-sensors-25-05496" ref-type="bibr">27</xref>].</p><p>The UCF-Crime dataset comprises 13 categories of genuine anomalous incidents, using films extracted from unprocessed surveillance recordings. The training dataset includes 800 normal videos and 810 anomalous videos, whilst the test dataset consists of 150 normal videos and 140 anomalous videos. The primary problem with this dataset is that the abnormal signals are feeble. Simultaneously, the backdrop dynamics are pronounced: surveillance film frequently includes substantial dynamic background elements (e.g., pedestrians, traffic) that are irrelevant to the event, and numerous anomalous behaviours are visually indistinct from the periphery of normal behaviour. This presents a definitive technical validation case for our dual-stream attention method. In low signal-to-noise situations, the model must concentrate on critical spatial and temporal areas while efficiently attenuating background noise. Our attention module is the ideal solution for this, capable of identifying and extracting the most informative and suspicious portions from intricate situations through autonomous learning, hence validating its background suppression capability.</p><p>XD-Violence is an extensive and heterogeneous dataset of 4754 unedited videos sourced from web content, live sports broadcasts, and surveillance footage. The primary challenge with this dataset is the morphological diversity of the anomalies and the ambiguity of video-level labelling; specifically, we only ascertain that a lengthy film has anomalies without being able to determine their precise timing. This attribute underscores the necessity and benefit of our memory module. The model must learn a stable and generalisable collection of prototype features despite inaccurate labels and highly varied input patterns. Our learnable bi-directional memory module is intended for this objective: by storing and updating typical normal and abnormal patterns, it facilitates stable and reliable feature matching in extremely diverse data, therefore directly proving its capacity to accommodate atypical prototypes.</p><p>For the experimental evaluation, consistent with previous studies, we measure the performance of WSVAD using the area under the receiver operating characteristic curve (AUC) at the frame level on the UCF-Crime dataset. Meanwhile, we use the area under the precision&#8211;recall curve (AP) at the frame level for the XD-Violence dataset as the evaluation metric. Higher AUC and AP values indicate better network performance.</p></sec><sec id="sec4dot2-sensors-25-05496"><title>4.2. Experimental Details</title><p>We extract snippet features from the I3D [<xref rid="B28-sensors-25-05496" ref-type="bibr">28</xref>] model pretrained on the Kinetics-400 dataset. The configuration for both datasets remains consistent. During training, a multi-crop aggregation method is employed to obtain the final anomaly scores, setting the number of crops for UCF-Crime to 10 and for XD-Violence to 5. To ensure robust performance when processing videos with extreme durations (tens of minutes to hours), our framework adopts a hierarchical temporal sampling strategy to manage computational complexity while maintaining anomaly localization accuracy effectively. Specifically, for videos exceeding 10 min, we first apply uniform frame down-sampling to reduce temporal redundancy while preserving essential motion and appearance cues. Next, we divide the entire video into overlapping temporal segments of fixed length, which are processed independently by the feature extractor and memory module. This segmentation enables our model to capture local spatio-temporal dynamics without being affected by the excessive length of the video. Finally, we aggregate the segment-level anomaly score by aligning the predicted scores to the original timeline. This hierarchical design ensures that the model remains computationally efficient and minimises the risk of anomaly localization loss in long-duration videos.</p><p>The optimal values for key hyperparameters were determined via a systematic grid search on a held-out validation set. The model is trained with a learning rate of 0.0001 and a batch size of 64 over 3000 iterations, with <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for both UCF-Crime and XD-Violence. The hyperparameters (<inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) are set to (0.1, 0.1, 0.001, 0.0001) to ensure a balanced total loss after 3000 training iterations for each dataset. The loss value of the model tends to stabilise, as illustrated in <xref rid="sensors-25-05496-f005" ref-type="fig">Figure 5</xref>, and the optimizer used is Adam. Our experiments were conducted on a machine equipped with an Intel (R) Xeon (R) Gold 6430 CPU (Intel Corporation, Santa Clara, CA, USA) and NVIDIA GeForce RTX 4090 GPU (NVIDIA Corporation, Santa Clara, CA, USA), using CUDA 12.1, Python 3.9.21, and Pytorch 2.1.0.</p></sec><sec id="sec4dot3-sensors-25-05496"><title>4.3. Experimental Results</title><sec id="sec4dot3dot1-sensors-25-05496"><title>4.3.1. Performance Comparison with Competing Approaches on UCF-Crime</title><p>In the UCF-Crime dataset, we compared the AUC score of our method with the current mainstream VAD method, as shown in <xref rid="sensors-25-05496-t001" ref-type="table">Table 1</xref>. Our model achieves an AUC score of 87.43%, and this superior performance reflects how well our model structure matches the challenges of this dataset. Unlike methods such as BN-WVAD [<xref rid="B15-sensors-25-05496" ref-type="bibr">15</xref>] that simply optimise the network structure, our model not only optimises the network but also innovates at the level of feature representation and matching. This proves the feature discrimination capability of our method: learning the discriminative normal and abnormal prototypes through bi-directional memory units provides a powerful discriminative prior for the model. Moreover, in contrast to techniques such as UR-DMU [<xref rid="B26-sensors-25-05496" ref-type="bibr">26</xref>] that also use memory networks, UR-DMU lacks feature purification before querying the memory bank. Our core innovation, the &#8220;Attention Before Memory&#8221; mechanism, utilises prototype knowledge more efficiently by using dual-stream attention as an intelligent filter to denoise and focus features before querying. This directly reflects the superior background suppression capability of the model.</p></sec><sec id="sec4dot3dot2-sensors-25-05496"><title>4.3.2. Performance Comparison with Competing Approaches on XD-Violence</title><p>The XD-Violence dataset necessitates that the model exhibits excellent generalisability across various situations and resilience to inaccurate labels. <xref rid="sensors-25-05496-t002" ref-type="table">Table 2</xref> demonstrates that our strategy attains an AP score of 85.51%. This accomplishment is primarily ascribed to the strength of our bidirectional memory bank. Our algorithm can simultaneously learn to identify &#8220;normal&#8221; and &#8220;abnormal&#8221; patterns from very varied and imprecisely labelled inputs, establishing persistent prototype libraries for both categories. This dual modelling capacity allows the model to render more generalised assessments when encountering movies from unfamiliar scenes. This unequivocally confirms the robust capacity of our model to accommodate atypical prototypes.</p></sec><sec id="sec4dot3dot3-sensors-25-05496"><title>4.3.3. Computational Efficiency</title><p>To evaluate the practical applicability of our proposed method, we conducted an analysis of its computational efficiency. We compared the parameter count and inference time of our model against several state-of-the-art methods, with the results presented in <xref rid="sensors-25-05496-t003" ref-type="table">Table 3</xref>.</p><p>In terms of model size, our method is significantly more efficient, requiring only 0.01G parameters. This is half the size of RTFM [<xref rid="B13-sensors-25-05496" ref-type="bibr">13</xref>] and an order of magnitude smaller than VadCLIP [<xref rid="B33-sensors-25-05496" ref-type="bibr">33</xref>]. Regarding inference speed, our model demonstrates a substantial advantage, with a test time of just 0.03 s. This is over 4.5 times faster than the next fastest method, RTFM [<xref rid="B13-sensors-25-05496" ref-type="bibr">13</xref>]. This analysis clearly demonstrates that our proposed model not only achieves high accuracy but also offers superior computational efficiency. Its low parameter count and fast inference speed make it highly suitable for real-world deployment scenarios where computational resources may be limited.</p></sec><sec id="sec4dot3dot4-sensors-25-05496"><title>4.3.4. Analysis of FAR and Background Suppression Capability</title><p>This section conducts a focused evaluation of the discriminative capability of our model, by analysing two critical dimensions: background suppression efficacy via false alarm rate assessment and feature discrimination proficiency through detailed classification analysis.</p><p>Firstly, to evaluate the background suppression efficacy of the model, we employ two complementary methodologies: frame-by-frame and window-based analyses, to thoroughly assess its false alarm rate (FAR), with results presented in <xref rid="sensors-25-05496-t004" ref-type="table">Table 4</xref>. In the traditional frame-by-frame evaluation, the FAR of our model is only 6.98%, which is much lower than the comparison method. Moreover, in the windowed review, which more accurately reflects real-world applications, the FAR of our model is significantly diminished from 6.98% to 5.21%. In contrast, the FAR of the comparison approach increases slightly instead. This comparison clearly illustrates that the erroneous alerts produced by our model primarily result from decentralised transient noise, which can be efficiently mitigated using temporal smoothing. This outcome offers substantial evidence for both the enhanced accuracy and the robust background suppression ability of the model.</p><p>This paper further breaks down the primary error sources contributing to the FAR to provide a deeper insight into the failure modes of the model. We manually analysed and categorised the top 100 false alarm instances from the UCF-Crime dataset, with the results summarised in <xref rid="sensors-25-05496-t005" ref-type="table">Table 5</xref>. The analysis reveals that the majority of false alarms are caused by sudden illumination changes (41%), followed by complex background object motion (34%) and activities in crowded scenes (25%). This finding suggests that while our model is generally robust, future optimizations could focus on improving invariance to these challenging scenarios.</p><p>Secondly, a robust model must not only suppress the background but also accurately differentiate semantically comparable anomalous events. To validate the advantage of our model in feature discrimination capability, we compared it against the AnomalyCLIP model [<xref rid="B4-sensors-25-05496" ref-type="bibr">4</xref>], which also possesses fine-grained recognition abilities. We conducted a fine-grained classification experiment, with the results illustrated in a confusion matrix in <xref rid="sensors-25-05496-f006" ref-type="fig">Figure 6</xref>.</p><p>To allow for a clearer quantitative assessment of where our prototype learning excels or fails, we calculated the per-class Precision, Recall, and F1-Score based on the confusion matrix. These results are presented in <xref rid="sensors-25-05496-t006" ref-type="table">Table 6</xref>. The quantitative metrics reveal that our model achieves an exceptionally high recall of 98.4% for the &#8216;Stealing&#8217; class, indicating its learned prototypes are highly effective at identifying nearly all instances of this event. While its precision for &#8216;Stealing&#8217; (89.8%) is impacted by some confusion with the visually similar &#8216;Shoplifting&#8217; and &#8216;Robbery&#8217; classes, the model demonstrates high precision for &#8216;Robbery&#8217; (97.0%) and a strong balance for &#8216;Shoplifting&#8217;. This detailed analysis provides a deeper insight into the specific strengths and trade-offs of our prototype learning approach.</p><p><xref rid="sensors-25-05496-f006" ref-type="fig">Figure 6</xref> illustrates that the prediction outcomes of our model are predominantly aligned around the diagonal line when addressing the highly ambiguous categories such as &#8220;theft,&#8221; &#8220;shoplifting,&#8221; and &#8220;robbery,&#8221; whilst the non-diagonal values of the comparative model are also significantly concentrated along the diagonal line. The off-diagonal values of the model are markedly elevated, signifying considerable category confusion. This provides clear proof that our bi-directional memory bank can learn more distinct prototypes for various but analogous anomalous events, leading to more accurate decision boundaries in the feature space.</p><p>By synthesising these results, we can directly attribute the enhancement in performance to the structural improvements of the model. The efficacy of UCF-Crime arises from the concentration capabilities of the Attention mechanism and the temporal modelling proficiency of HMTE, which collectively improve background suppression. The efficacy of XD-Violence derives from the implementation of the Memory mechanism, which creates durable prototypes and enhances the capacity for anomalous prototype fitting. This &#8220;attention focusing, then memory matching&#8221; approach is likely to possess strong generalisability. Even with other datasets exhibiting greater incompleteness in modalities or labels, its fundamental mechanism of dynamically filtering information via attention and stabilising prototypes through memory bank learning will uphold performance stability. It may be regarded as a significant structural contribution.</p></sec></sec><sec id="sec4dot4-sensors-25-05496"><title>4.4. Parameter Sensitivity Analysis</title><p>The dual memory unit based on bidirectional spatial attention mechanism achieves optimal performance by simultaneously storing two types of memory patterns. To investigate the appropriate number of memory libraries, we set up nine memory cases to seek the optimal configuration for the number of memory modules. The results are shown in <xref rid="sensors-25-05496-t007" ref-type="table">Table 7</xref>. Here, <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the number of anomalous and normal memories, respectively. When <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, our method achieved optimal performance on both datasets.</p></sec><sec id="sec4dot5-sensors-25-05496"><title>4.5. Ablation Study</title><p>To systematically assess the efficacy of each core component in the model and to distinctly identify performance improvements attributable to a specific mechanism rather than mere structural complexity, we executed a series of ablation experiments adhering closely to the modular design framework. Commencing with a baseline model (Baseline) that comprises solely the foundational backbone network, we sequentially incorporate HMTE, PGRN, DFRL, and DS-AEMN modules, monitoring the performance variations to establish a definitive causal relationship. The experimental findings are presented in <xref rid="sensors-25-05496-t008" ref-type="table">Table 8</xref>.</p><p>Analysis of individual values for baseline and singular modules: The baseline model attained an AUC of 86.26% and an AP of 82.80% on UCF-Crime and XD-Violence, respectively. Consequently, we assessed the distinct worth of each module. Upon the sole introduction of the HMTE module, performance enhances to 86.37% and 84.00%, illustrating the autonomous and prompt advantage of capturing local timing dependencies through temporal convolutional networks. The introduction of the DFRL module as a standalone support structure also led to enhanced performance, validating the efficacy of residual connection in stabilising training and facilitating the acquisition of robust features. The incorporation of the core DS-AEMN module yields the most substantial single-module performance enhancements of 86.60% and 84.35%, thereby affirming that our fundamental principle of &#8220;notice first, remember later&#8221; is an exceptionally effective strategy for differentiating ambiguous anomalous events, exhibiting robust independent discriminative capability. The basic principle of &#8220;attention first, then memory&#8221; is an effective method for identifying ambiguous abnormal events, demonstrating significant independent discriminatory capability.</p><p>Examination of the synergistic impact of multi-module integration: Upon confirming the independent validity of each module, we further investigated their synergistic effect. The integration of HMTE and DFRL elevates the AUC on UCF-Crime to 86.99%, demonstrating that the superior feature backbone and improved temporal modelling capacity collectively establish a more robust feature foundation for subsequent processing. The fundamental DS-AEMN module provides substantial performance improvements when integrated with other modules. The integration of Base + HMTE + DS-AEMN attained an 85.30% average precision on XD-Violence. This illustrates the synergistic relationship between the modules: with access to a practical temporal context (provided by HMTE), our DS-AEMN module can execute attentional focusing and memory matching with greater precision, demonstrating that its efficacy is enhanced not only in isolation but also through the collaborative framework.</p><p>The comprehensive model, incorporating all three modules, attained optimal performance with an AUC of 87.43% and an AP of 85.51%. To specifically isolate the contribution of the PGRN, we conducted an additional experiment where only the PGRN module was removed from the full model. As shown in <xref rid="sensors-25-05496-t008" ref-type="table">Table 8</xref>, this led to a notable performance drop to 87.18% AUC on UCF-Crime and 85.12% AP on XD-Violence. This result quantitatively confirms the significant impact of PGRN in capturing long-range, content-driven global dependencies, which complements the local features from HMTE. This outcome illustrates the synergistic interaction among the modules: the HMTE captures the temporal context, the PGRN establishes global relations, the DFRL ensures the depth and stability of feature extraction, and the DS-AEMN effectively identifies anomalies through an accurate prototype matching task within this framework. Each component addresses the difficulty from a distinct perspective, and their combination becomes a synergistic partnership rather than a mere performance overlay, culminating in enhanced model performance.</p><p>To further verify the contribution of each component within our dual-stream attention mechanism, we performed a fine-grained ablation study, as shown in <xref rid="sensors-25-05496-t009" ref-type="table">Table 9</xref>. The results indicate that incorporating either the spatial or the channel attention branch individually yields a performance gain over the baseline, confirming the effectiveness of each component. Critically, the synergistic use of both branches achieves the best performance on both datasets. This strongly demonstrates that our dual-stream design is more than a simple stacking of components; it effectively captures complementary &#8216;where&#8217; (spatial) and &#8216;what&#8217; (channel) information to achieve efficient feature purification, thereby validating the rationale and necessity of our approach.</p><p>The efficacy of the composite loss function (Equation (8)) is somewhat contingent upon the hyperparameters <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> associated with each loss weight. A systematic sensitivity analysis is conducted on both the UCF-Crime and XD-Violence datasets to demonstrate that the selected hyperparameters (<inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) are not overfitting to a single dataset but represent a generalised choice that performs robustly across varying data characteristics. The tests utilised the control variable method, assessing one parameter while maintaining all other parameters at their final selected values.</p><p>Impact of memory loss weight <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: <xref rid="sensors-25-05496-f007" ref-type="fig">Figure 7</xref> illustrates that the effect of <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> exhibits a very similar pattern across both datasets. The performance on both UCF-Crime and XD-Violence reaches its zenith at <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. When the weights are very low (0.01), the supervised limitations on the memory module are inadequate, resulting in suboptimal utilisation of its capacity. Simultaneously, when the weights exceed 0.5, the high auxiliary loss disrupts the primary classification job, resulting in a concurrent decline in performance across both datasets. The uniformity across datasets compellingly illustrates that <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is a robust and optimal parameter, irrespective of data distribution.</p><p>Effect of the Triplet loss weight <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: Likewise, the examination of <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> demonstrates a comparable trend. Eliminating this factor (<inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) results in a notable decline in performance across both datasets, thus affirming the critical necessity of explicitly enforcing the structure of the eigenspace metric to enhance model generalisation. Optimal performance is reliably achieved at <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, but increased weights detrimentally affect performance due to overregularization.</p><p>Combined impact of KL divergence and distance loss (<inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>): <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> function as more refined regularisation terms, which we jointly ablate. The table indicates that both datasets exhibit a minor decline in performance from the ideal position when both losses are simultaneously eliminated. Despite the lesser magnitude of the effect compared to <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, it suggests that these two regularisation terms, employed to stabilise the latent space and explicitly separate features, are generally advantageous for subsequent fine-tuning and enhancing model performance.</p><p>This series of ablation experiments over two distinct characterisation datasets establishes a robust empirical foundation for our ultimate hyperparameter design. The consistently reliable performance patterns in the results suggest that the selected weights are not mere &#8220;fortunate values&#8221; that overfit a particular data distribution, but relatively resilient selections that equilibrate the loss terms and enhance the generalisation capacity of the model.</p></sec><sec id="sec4dot6-sensors-25-05496"><title>4.6. Qualitative Results</title><p>To enhance the visualisation of the efficacy of our method and to substantiate the preceding quantitative analysis, we present the findings of our qualitative investigation on the UCF-Crime and XD-Violence datasets.</p><p>The anomaly score curves depicted in <xref rid="sensors-25-05496-f008" ref-type="fig">Figure 8</xref> provide a visual confirmation of the proficiency of our model in time-series modelling. Specifically, the figure provides qualitative evidence of the ability of the model to handle anomalies of varying durations. For instance, the model successfully localises both a long-duration event with a gradually rising score (left plot) and a series of brief, explosive anomalies with sharp, spiky scores (right plot). This demonstrates that our HMTE module, designed to capture both long- and short-term dependencies, is effective in practice. The overall alignment of the scores of the model with the actual abnormal periods (shown by the pink shaded areas) shows that, even in a poorly guided context without precise frame-level annotations, our approach can successfully acquire dependable spatio-temporal patterns and attain accurate anomaly localisation. A detailed quantitative analysis on this topic is a valuable direction for future work.</p><p><xref rid="sensors-25-05496-f009" ref-type="fig">Figure 9</xref> illustrates the distinct performance of this technique on Fighting028_x264 and Vandalism004_x264 within the UCF-Crime dataset, which offers the most explicit visual demonstration of the background suppression capacity of our model. The graphic illustrates that, following the implementation of the dual-stream attention mechanism (from c to d), the focus of the network progressively shifts towards the primary topics (e.g., the combatants) in the image, away from the dispersed background areas. This incremental purification approach successfully mitigates the influence of extraneous background information, hence augmenting the accuracy of the model in learning video features and ultimately enhancing overall recognition performance.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05496"><title>5. Conclusions</title><p>This study addresses key challenges in WSVAD, namely ambiguous anomaly characteristics, significant background interference, and the inadequate modelling of intricate temporal dependencies. To tackle these issues, a dual-stream attention and memory-enhanced network is proposed, transforming the conventional implicit feature learning approach into an innovative framework. This framework integrates multi-scale temporal modelling with explicit prototype discrimination to address the inherent uncertainty in weakly supervised settings systematically. At the temporal modelling level, an enhanced time-series encoder is developed by incorporating multi-scale dilated convolutions with residual connections, which effectively addresses feature degradation in deep networks and enhances the capacity to capture long- and short-term dependencies. More centrally, at the feature discrimination level, a dual-stream attention and memory-enhanced network is introduced. This design converts implicit feature learning into explicit prototype matching via independent memory for normal and abnormal prototypes, fundamentally addressing the issue of indistinct boundaries. Comprehensive studies on the UCF-Crime and XD-Violence datasets demonstrate that the proposed approach surpasses existing mainstream methods across multiple critical measures.</p><p>In conclusion, this research presents a high-performance VAD model and, more significantly, an effective paradigm of &#8220;attention focus and prototype matching&#8221; for resolving uncertainty in weakly supervised learning. Future work will advance in two directions: first, to enhance the cross-domain generalisation capability of the model, we will investigate adaptive, unsupervised updating mechanisms for memory prototypes to reduce reliance on pseudo-labels. The effectiveness of this new approach will be validated by extending our evaluation to additional datasets such as ShanghaiTech and Avenue. Second, while the current framework only utilises RGB inputs, developing a cross-modal attention method utilising multimodal information, such as optical flow or audio data, could improve the discriminative ability of the model in more intricate scenarios, particularly for events with weak visual cues.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.G. and X.W.; methodology, Y.W.; software, X.J.; validation, X.W.; formal analysis, Y.W.; investigation, X.W.; resources, X.J.; data curation, W.G.; writing&#8212;original draft preparation, W.G. and Y.W.; writing&#8212;review and editing, X.W.; visualisation, W.G. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Xiaochuan Jing was employed by the Aerospace Hongka Intelligent Technology (Beijing) Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05496"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Senapati</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Pani</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Baliarsingh</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Dev</surname><given-names>P.P.</given-names></name><name name-style="western"><surname>Tripathy</surname><given-names>H.K.</given-names></name></person-group><article-title>CA-VAD: Caption Aware Video Anomaly Detection in surveillance videos</article-title><source>J. Vis. Commun. Image Represent.</source><year>2025</year><volume>111</volume><elocation-id>104521</elocation-id><pub-id pub-id-type="doi">10.1016/j.jvcir.2025.104521</pub-id></element-citation></ref><ref id="B2-sensors-25-05496"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Duong</surname><given-names>H.-T.</given-names></name><name name-style="western"><surname>Le</surname><given-names>V.-T.</given-names></name><name name-style="western"><surname>Hoang</surname><given-names>V.T.</given-names></name></person-group><article-title>Deep Learning-Based Anomaly Detection in Video Surveillance: A Survey</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>5024</elocation-id><pub-id pub-id-type="doi">10.3390/s23115024</pub-id><pub-id pub-id-type="pmid">37299751</pub-id><pub-id pub-id-type="pmcid">PMC10255829</pub-id></element-citation></ref><ref id="B3-sensors-25-05496"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sultani</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shah</surname><given-names>M.</given-names></name></person-group><article-title>Real-world anomaly detection in surveillance videos</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>6479</fpage><lpage>6488</lpage></element-citation></ref><ref id="B4-sensors-25-05496"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zanella</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liberatori</surname><given-names>B.</given-names></name><name name-style="western"><surname>Menapace</surname><given-names>W.</given-names></name><name name-style="western"><surname>Poiesi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ricci</surname><given-names>E.</given-names></name></person-group><article-title>Delving into CLIP latent space for Video Anomaly Recognition</article-title><source>Comput. Vis. Image Underst.</source><year>2024</year><volume>249</volume><elocation-id>104163</elocation-id><pub-id pub-id-type="doi">10.1016/j.cviu.2024.104163</pub-id></element-citation></ref><ref id="B5-sensors-25-05496"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>Y.</given-names></name></person-group><article-title>Probabilistic Vision-Language Representation for Weakly Supervised Temporal Action Localization</article-title><source>Proceedings of the ACM International Conference on Multimedia</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>28 October&#8211;1 November 2024</conf-date><fpage>5507</fpage><lpage>5516</lpage></element-citation></ref><ref id="B6-sensors-25-05496"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lea</surname><given-names>C.</given-names></name><name name-style="western"><surname>Flynn</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Vidal</surname><given-names>R.</given-names></name><name name-style="western"><surname>Reiter</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hager</surname><given-names>G.D.</given-names></name></person-group><article-title>Temporal Convolutional Networks for Action Segmentation and Detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>1003</fpage><lpage>1012</lpage></element-citation></ref><ref id="B7-sensors-25-05496"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Le</surname><given-names>V.</given-names></name><name name-style="western"><surname>Saha</surname><given-names>B.</given-names></name><name name-style="western"><surname>Mansour</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Venkatesh</surname><given-names>S.</given-names></name><name name-style="western"><surname>van den Hengel</surname><given-names>A.</given-names></name></person-group><article-title>Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>1705</fpage><lpage>1714</lpage></element-citation></ref><ref id="B8-sensors-25-05496"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zaheer</surname><given-names>M.Z.</given-names></name><name name-style="western"><surname>Mahmood</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.-I.</given-names></name></person-group><article-title>A Self-Reasoning Framework for Anomaly Detection Using Video-Level Labels</article-title><source>Ieee Signal Process. Lett.</source><year>2020</year><volume>27</volume><fpage>1705</fpage><lpage>1709</lpage><pub-id pub-id-type="doi">10.1109/LSP.2020.3025688</pub-id></element-citation></ref><ref id="B9-sensors-25-05496"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Al-Lahham</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tastan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zaheer</surname><given-names>M.Z.</given-names></name><name name-style="western"><surname>Nandakumar</surname><given-names>K.</given-names></name><name name-style="western"><surname>Soc</surname><given-names>I.C.</given-names></name></person-group><article-title>A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>4&#8211;8 January 2024</conf-date><fpage>6779</fpage><lpage>6788</lpage></element-citation></ref><ref id="B10-sensors-25-05496"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Thakare</surname><given-names>K.V.</given-names></name><name name-style="western"><surname>Raghuwanshi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dogra</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>I.-J.</given-names></name></person-group><article-title>DyAnNet: A Scene Dynamicity Guided Self-Trained Video Anomaly Detection Network</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;7 January 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>5530</fpage><lpage>5539</lpage></element-citation></ref><ref id="B11-sensors-25-05496"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zaheer</surname><given-names>M.Z.</given-names></name><name name-style="western"><surname>Mahmood</surname><given-names>A.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Segu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.-I.</given-names></name></person-group><article-title>Generative cooperative learning for unsupervised video anomaly detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;23 June 2022</conf-date><fpage>14744</fpage><lpage>14754</lpage></element-citation></ref><ref id="B12-sensors-25-05496"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Al-lahham</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zaheer</surname><given-names>M.Z.</given-names></name><name name-style="western"><surname>Tastan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Nandakumar</surname><given-names>K.</given-names></name></person-group><article-title>Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>12416</fpage><lpage>12425</lpage></element-citation></ref><ref id="B13-sensors-25-05496"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>R.</given-names></name><name name-style="western"><surname>Verjans</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Carneiro</surname><given-names>G.</given-names></name></person-group><article-title>Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>4955</fpage><lpage>4966</lpage></element-citation></ref><ref id="B14-sensors-25-05496"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>X.</given-names></name><name name-style="western"><surname>Mei</surname><given-names>J.</given-names></name></person-group><article-title>Weakly supervised video anomaly detection via center-guided discriminative learning</article-title><source>Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>London, UK</conf-loc><conf-date>6&#8211;10 July 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc></element-citation></ref><ref id="B15-sensors-25-05496"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>F.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>H.T.</given-names></name></person-group><article-title>BatchNorm-Based Weakly Supervised Video Anomaly Detection</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>34</volume><fpage>13642</fpage><lpage>13654</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2024.3450734</pub-id></element-citation></ref><ref id="B16-sensors-25-05496"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>J.-C.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>F.-T.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>W.-S.</given-names></name><name name-style="western"><surname>Ieee Comp</surname><given-names>S.O.C.</given-names></name></person-group><article-title>MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>19&#8211;25 June 2021</conf-date><fpage>14004</fpage><lpage>14013</lpage></element-citation></ref><ref id="B17-sensors-25-05496"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qing</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.-H.</given-names></name></person-group><article-title>Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>16271</fpage><lpage>16280</lpage></element-citation></ref><ref id="B18-sensors-25-05496"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Majhi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Garattoni</surname><given-names>L.</given-names></name><name name-style="western"><surname>Francesca</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bremond</surname><given-names>F.</given-names></name><name name-style="western"><surname>Soc</surname><given-names>I.C.</given-names></name></person-group><article-title>OE-CTST: Outlier-Embedded Cross Temporal Scale Transformer for Weakly-supervised Video Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>4&#8211;8 January 2024</conf-date><fpage>8559</fpage><lpage>8568</lpage></element-citation></ref><ref id="B19-sensors-25-05496"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>J.-X.</given-names></name><name name-style="western"><surname>Shu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>H.</given-names></name></person-group><article-title>Weakly-supervised anomaly detection in video surveillance via graph convolutional label noise cleaning</article-title><source>Neurocomputing</source><year>2022</year><volume>481</volume><fpage>154</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2022.01.026</pub-id></element-citation></ref><ref id="B20-sensors-25-05496"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Akdag</surname><given-names>E.</given-names></name><name name-style="western"><surname>Bondarev</surname><given-names>E.</given-names></name><name name-style="western"><surname>De With</surname><given-names>P.H.N.</given-names></name></person-group><article-title>MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2410.05900</pub-id></element-citation></ref><ref id="B21-sensors-25-05496"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shoaib</surname><given-names>M.</given-names></name><name name-style="western"><surname>Shah</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ullah</surname><given-names>A.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>F.</given-names></name></person-group><article-title>A deep learning-assisted visual attention mechanism for anomaly detection in videos</article-title><source>Multimed. Tools Appl.</source><year>2023</year><volume>83</volume><fpage>73363</fpage><lpage>73390</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-17770-z</pub-id></element-citation></ref><ref id="B22-sensors-25-05496"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ghadiya</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kar</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chudasama</surname><given-names>V.</given-names></name><name name-style="western"><surname>Wasnik</surname><given-names>P.</given-names></name></person-group><article-title>Cross-Modal Fusion and Attention Mechanism forWeakly Supervised Video Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>1965</fpage><lpage>1974</lpage></element-citation></ref><ref id="B23-sensors-25-05496"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Soc</surname><given-names>I.C.</given-names></name></person-group><article-title>Overlooked Video Classification in Weakly Supervised Video Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>4&#8211;8 January 2024</conf-date><fpage>212</fpage><lpage>220</lpage></element-citation></ref><ref id="B24-sensors-25-05496"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sukhbaatar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Szlam</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weston</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fergus</surname><given-names>R.</given-names></name></person-group><article-title>End-To-End Memory Networks</article-title><source>Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>7&#8211;12 December 2015</conf-date></element-citation></ref><ref id="B25-sensors-25-05496"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>H.</given-names></name><name name-style="western"><surname>Noh</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ham</surname><given-names>B.</given-names></name></person-group><article-title>Learning Memory-guided Normality for Anomaly Detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>14360</fpage><lpage>14369</lpage></element-citation></ref><ref id="B26-sensors-25-05496"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name></person-group><article-title>Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>7&#8211;14 February 2023</conf-date><fpage>3769</fpage><lpage>3777</lpage></element-citation></ref><ref id="B27-sensors-25-05496"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name></person-group><article-title>Not only look, but also listen: Learning multimodal violence detection under weak supervision</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><fpage>322</fpage><lpage>339</lpage></element-citation></ref><ref id="B28-sensors-25-05496"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Carreira</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>4724</fpage><lpage>4733</lpage></element-citation></ref><ref id="B29-sensors-25-05496"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Fok</surname><given-names>W.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.-C.</given-names></name></person-group><article-title>MGFN: Magnitude-Contrastive Glance-and-Focus Network for Weakly-Supervised Video Anomaly Detection</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>7&#8211;14 February 2023</conf-date><fpage>387</fpage><lpage>395</lpage></element-citation></ref><ref id="B30-sensors-25-05496"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name><name name-style="western"><surname>Park</surname><given-names>E.</given-names></name></person-group><article-title>ViCap-AD: Video caption-based weakly supervised video anomaly detection</article-title><source>Mach. Vision Appl.</source><year>2025</year><volume>36</volume><elocation-id>61</elocation-id><pub-id pub-id-type="doi">10.1007/s00138-025-01676-x</pub-id></element-citation></ref><ref id="B31-sensors-25-05496"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hasan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Neumann</surname><given-names>J.</given-names></name><name name-style="western"><surname>Roy-Chowdhury</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Davis</surname><given-names>L.S.</given-names></name></person-group><article-title>Learning Temporal Regularity in Video Sequences</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>733</fpage><lpage>742</lpage></element-citation></ref><ref id="B32-sensors-25-05496"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Joo</surname><given-names>H.K.</given-names></name><name name-style="western"><surname>Khoa</surname><given-names>V.</given-names></name><name name-style="western"><surname>Yamazaki</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ngan</surname><given-names>L.</given-names></name></person-group><article-title>Clip-tsa: Clip-assisted temporal self-attention for weakly-supervised video anomaly detection</article-title><source>Proceedings of the IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Kuala Lumpur, Malaysia</conf-loc><conf-date>8&#8211;11 October 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>3230</fpage><lpage>3234</lpage></element-citation></ref><ref id="B33-sensors-25-05496"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection</article-title><source>Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI)/36th Conference on Innovative Applications of Artificial Intelligence/14th Symposium on Educational Advances in Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>20&#8211;27 February 2024</conf-date><fpage>6074</fpage><lpage>6082</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05496-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall architecture of the proposed method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g001.jpg"/></fig><fig position="float" id="sensors-25-05496-f002" orientation="portrait"><label>Figure 2</label><caption><p>Hierarchical multi-scale temporal encoder architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g002.jpg"/></fig><fig position="float" id="sensors-25-05496-f003" orientation="portrait"><label>Figure 3</label><caption><p>The dynamic residual feature enhancement module. This module is represented as the &#8216;ResidualBlock&#8217; in the overall architecture in <xref rid="sensors-25-05496-f001" ref-type="fig">Figure 1</xref>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g003.jpg"/></fig><fig position="float" id="sensors-25-05496-f004" orientation="portrait"><label>Figure 4</label><caption><p>Detailed structure of the dual-stream attention component. This component is part of the DS-AEMN module shown in <xref rid="sensors-25-05496-f001" ref-type="fig">Figure 1</xref>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g004.jpg"/></fig><fig position="float" id="sensors-25-05496-f005" orientation="portrait"><label>Figure 5</label><caption><p>The change in training loss value over training time.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g005.jpg"/></fig><fig position="float" id="sensors-25-05496-f006" orientation="portrait"><label>Figure 6</label><caption><p>Fine-grained anomaly discrimination performance.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g006.jpg"/></fig><fig position="float" id="sensors-25-05496-f007" orientation="portrait"><label>Figure 7</label><caption><p>Impact of <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
on model performance.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g007.jpg"/></fig><fig position="float" id="sensors-25-05496-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visual analysis of the results of the proposed method on the UCF-Crime and XD-Violence datasets. The anomaly score of the model is plotted with a green curve. The light pink shaded regions denote ground-truth data for anomalous frames. Representative snippets of abnormal and benign events are displayed within red and green boxes for comparison.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g008.jpg"/></fig><fig position="float" id="sensors-25-05496-f009" orientation="portrait"><label>Figure 9</label><caption><p>Illustration of the role of the dual-stream attention mechanism in detection via heatmaps. (<bold>a</bold>) Original video frame; (<bold>b</bold>) before adding attention mechanism; (<bold>c</bold>) adding channel attention; (<bold>d</bold>) adding channel and spatial dual-stream attention mechanism.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05496-g009.jpg"/></fig><table-wrap position="float" id="sensors-25-05496-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t001_Table 1</object-id><label>Table 1</label><caption><p>Evaluating the effectiveness of different methods on UCF-Crime.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Supervision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUC(%)</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Unsupervised</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">GCL [<xref rid="B11-sensors-25-05496" ref-type="bibr">11</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNext</td><td align="center" valign="middle" rowspan="1" colspan="1">71.04</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">DyAnNet [<xref rid="B10-sensors-25-05496" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">79.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">C2FPL [<xref rid="B9-sensors-25-05496" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">80.65</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CLAP [<xref rid="B12-sensors-25-05496" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I3D RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.90</td></tr><tr><td rowspan="14" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Weakly supervised</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">GCN [<xref rid="B19-sensors-25-05496" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C3D RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">81.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">GCN [<xref rid="B19-sensors-25-05496" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">TSN RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">82.12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">GCN [<xref rid="B19-sensors-25-05496" ref-type="bibr">19</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">TSN FLOW</td><td align="center" valign="middle" rowspan="1" colspan="1">78.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">MGFN [<xref rid="B29-sensors-25-05496" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">86.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">MGFN [<xref rid="B29-sensors-25-05496" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VideoSwin-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">86.67</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">UR-DMU [<xref rid="B26-sensors-25-05496" ref-type="bibr">26</xref>] </td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">86.34 *</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">AnomalyCLIP [<xref rid="B4-sensors-25-05496" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ViT-B/16</td><td align="center" valign="middle" rowspan="1" colspan="1">86.36</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">BN-WVAD [<xref rid="B15-sensors-25-05496" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">87.24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">OE-CTST [<xref rid="B18-sensors-25-05496" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">86.37</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">OE-CTST [<xref rid="B18-sensors-25-05496" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VideoSwin-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">86.92</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">MIL-BERT [<xref rid="B23-sensors-25-05496" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB + Flow</td><td align="center" valign="middle" rowspan="1" colspan="1">86.71</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">MTFL [<xref rid="B20-sensors-25-05496" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VST-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">87.16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">ViCap-AD [<xref rid="B30-sensors-25-05496" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">87.20</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.43</td></tr></tbody></table><table-wrap-foot><fn><p>* Indicates that the method is re-trained.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05496-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t002_Table 2</object-id><label>Table 2</label><caption><p>Evaluating the effectiveness of different methods on XD-Violence.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Supervision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AP(%)</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Semi-supervised</td><td align="center" valign="middle" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" rowspan="1" colspan="1">Hasan et al. [<xref rid="B31-sensors-25-05496" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">30.77</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CoMo [<xref rid="B7-sensors-25-05496" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.30</td></tr><tr><td rowspan="16" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Weakly supervised</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">RTFM [<xref rid="B13-sensors-25-05496" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">C3D</td><td align="center" valign="middle" rowspan="1" colspan="1">77.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">MGFN [<xref rid="B29-sensors-25-05496" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VedioSwin-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">80.11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">Cu-Net [<xref rid="B17-sensors-25-05496" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">78.74</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">Cu-Net [<xref rid="B17-sensors-25-05496" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB+VGGish</td><td align="center" valign="middle" rowspan="1" colspan="1">81.43</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">CLIP-TSA [<xref rid="B32-sensors-25-05496" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Clip Vit</td><td align="center" valign="middle" rowspan="1" colspan="1">82.17</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">UR-DMU [<xref rid="B26-sensors-25-05496" ref-type="bibr">26</xref>] </td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">82.30 *</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">MGFN [<xref rid="B29-sensors-25-05496" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">79.19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">MGFN [<xref rid="B29-sensors-25-05496" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VideoSwin-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">80.11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">AnomalyCLIP [<xref rid="B4-sensors-25-05496" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ViT-B/16</td><td align="center" valign="middle" rowspan="1" colspan="1">78.51</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">OE-CTST [<xref rid="B18-sensors-25-05496" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">80.56</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">OE-CTST [<xref rid="B18-sensors-25-05496" ref-type="bibr">18</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VideoSwin-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">81.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">BN-WVAD [<xref rid="B15-sensors-25-05496" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">84.09 *</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">MIL-BERT [<xref rid="B23-sensors-25-05496" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB + Flow</td><td align="center" valign="middle" rowspan="1" colspan="1">82.10</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">MTFL [<xref rid="B20-sensors-25-05496" ref-type="bibr">20</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VST-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">84.57</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">ViCap-AD [<xref rid="B30-sensors-25-05496" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">85.02</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I3D-RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.51</td></tr></tbody></table><table-wrap-foot><fn><p>* Indicates that the method is re-trained.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05496-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t003_Table 3</object-id><label>Table 3</label><caption><p>Computational efficiency comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params (G)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Time (s)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">RTFM [<xref rid="B13-sensors-25-05496" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.14</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MIST [<xref rid="B16-sensors-25-05496" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VadCLIP [<xref rid="B33-sensors-25-05496" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">0.27</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.03</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05496-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t004_Table 4</object-id><label>Table 4</label><caption><p>False Alarm Rate (FAR) comparison under different evaluation protocols.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">FAR (Abnormal)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Original (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Window (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UR-DMU [<xref rid="B26-sensors-25-05496" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">11.60</td><td align="center" valign="middle" rowspan="1" colspan="1">12.10</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BN-WVAD [<xref rid="B15-sensors-25-05496" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">21.40</td><td align="center" valign="middle" rowspan="1" colspan="1">21.67</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.21</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05496-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t005_Table 5</object-id><label>Table 5</label><caption><p>Breakdown of False Alarm Error Sources on UCF-Crime.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Error Source</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Examples</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">False Alarm Proportion</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Illumination Changes</td><td align="center" valign="middle" rowspan="1" colspan="1">Shadows, headlights, day-night transitions</td><td align="center" valign="middle" rowspan="1" colspan="1">41%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Background Object Motion</td><td align="center" valign="middle" rowspan="1" colspan="1">Moving trees, camera shake, passing cars</td><td align="center" valign="middle" rowspan="1" colspan="1">34%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Crowed Scenes</td><td align="center" valign="middle" rowspan="1" colspan="1">Group activities, dense human motions</td><td align="center" valign="middle" rowspan="1" colspan="1">25%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100 analysed false alarms</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05496-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t006_Table 6</object-id><label>Table 6</label><caption><p>Per-class fine-grained performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Anomaly Class</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Stealing</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">98.4</td><td align="center" valign="middle" rowspan="1" colspan="1">93.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Shoplifting</td><td align="center" valign="middle" rowspan="1" colspan="1">95.3</td><td align="center" valign="middle" rowspan="1" colspan="1">92.1</td><td align="center" valign="middle" rowspan="1" colspan="1">93.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Robbery</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.9</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05496-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of the results for <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
and <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> with different amounts of memory.
</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">(<inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>)</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin" rowspan="1">DataSet</th></tr><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">UCF (%)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">XD (%)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">(20, 20)</td><td align="center" valign="middle" rowspan="1" colspan="1">87.35</td><td align="center" valign="middle" rowspan="1" colspan="1">35.32</td><td align="center" valign="middle" rowspan="1" colspan="1">93.77</td><td align="center" valign="middle" rowspan="1" colspan="1">83.48</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(30, 30)</td><td align="center" valign="middle" rowspan="1" colspan="1">86.13</td><td align="center" valign="middle" rowspan="1" colspan="1">30.02</td><td align="center" valign="middle" rowspan="1" colspan="1">93.44</td><td align="center" valign="middle" rowspan="1" colspan="1">82.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(40, 40)</td><td align="center" valign="middle" rowspan="1" colspan="1">85.56</td><td align="center" valign="middle" rowspan="1" colspan="1">29.30</td><td align="center" valign="middle" rowspan="1" colspan="1">93.31</td><td align="center" valign="middle" rowspan="1" colspan="1">82.85</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(50, 50)</td><td align="center" valign="middle" rowspan="1" colspan="1">85.95</td><td align="center" valign="middle" rowspan="1" colspan="1">30.82</td><td align="center" valign="middle" rowspan="1" colspan="1">93.74</td><td align="center" valign="middle" rowspan="1" colspan="1">84.15</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(60, 60)</td><td align="center" valign="middle" rowspan="1" colspan="1">86.60</td><td align="center" valign="middle" rowspan="1" colspan="1">33.84</td><td align="center" valign="middle" rowspan="1" colspan="1">93.70</td><td align="center" valign="middle" rowspan="1" colspan="1">84.70</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(70, 70)</td><td align="center" valign="middle" rowspan="1" colspan="1">87.43</td><td align="center" valign="middle" rowspan="1" colspan="1">35.67</td><td align="center" valign="middle" rowspan="1" colspan="1">93.82</td><td align="center" valign="middle" rowspan="1" colspan="1">85.51</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(80, 80)</td><td align="center" valign="middle" rowspan="1" colspan="1">85.38</td><td align="center" valign="middle" rowspan="1" colspan="1">31.29</td><td align="center" valign="middle" rowspan="1" colspan="1">93.03</td><td align="center" valign="middle" rowspan="1" colspan="1">83.32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">(90, 90)</td><td align="center" valign="middle" rowspan="1" colspan="1">86.10</td><td align="center" valign="middle" rowspan="1" colspan="1">31.75</td><td align="center" valign="middle" rowspan="1" colspan="1">93.63</td><td align="center" valign="middle" rowspan="1" colspan="1">83.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(100, 100)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.30</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05496-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t008_Table 8</object-id><label>Table 8</label><caption><p>Comparison of ablation results. &#10003; and &#10007; represent our backbone network with or without specific modules, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Modules</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Results</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HMTE</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PGRN</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DFRL</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DS-AEMN</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UCF AUC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">XD AP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.26</td><td align="center" valign="middle" rowspan="1" colspan="1">82.80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.37</td><td align="center" valign="middle" rowspan="1" colspan="1">84.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.33</td><td align="center" valign="middle" rowspan="1" colspan="1">83.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.60</td><td align="center" valign="middle" rowspan="1" colspan="1">84.35</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.99</td><td align="center" valign="middle" rowspan="1" colspan="1">84.95</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">87.35</td><td align="center" valign="middle" rowspan="1" colspan="1">85.30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.69</td><td align="center" valign="middle" rowspan="1" colspan="1">84.70</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">87.18</td><td align="center" valign="middle" rowspan="1" colspan="1">85.12</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.51</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05496-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05496-t009_Table 9</object-id><label>Table 9</label><caption><p>Ablation study on the dual-stream attention mechanism. &#10003; and &#10007; denote the model with or without the corresponding attention branch.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spatial Attention</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Channel Attention</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UCF AUC (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">XD AP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.79</td><td align="center" valign="middle" rowspan="1" colspan="1">84.65</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">87.23</td><td align="center" valign="middle" rowspan="1" colspan="1">85.26</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">87.14</td><td align="center" valign="middle" rowspan="1" colspan="1">85.15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.51</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>