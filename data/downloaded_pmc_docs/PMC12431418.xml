<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="review-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431418</article-id><article-id pub-id-type="pmcid-ver">PMC12431418.1</article-id><article-id pub-id-type="pmcaid">12431418</article-id><article-id pub-id-type="pmcaiid">12431418</article-id><article-id pub-id-type="doi">10.3390/s25175359</article-id><article-id pub-id-type="publisher-id">sensors-25-05359</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Systematic Review</subject></subj-group></article-categories><title-group><article-title>Buzzing with Intelligence: A Systematic Review of Smart Beehive Technologies</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-3531-4514</contrib-id><name name-style="western"><surname>&#352;abi&#263;</surname><given-names initials="J">Josip</given-names></name><xref rid="c1-sensors-25-05359" ref-type="corresp">*</xref><xref rid="fn1-sensors-25-05359" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8826-4905</contrib-id><name name-style="western"><surname>Perkovi&#263;</surname><given-names initials="T">Toni</given-names></name><xref rid="c1-sensors-25-05359" ref-type="corresp">*</xref><xref rid="fn1-sensors-25-05359" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3468-133X</contrib-id><name name-style="western"><surname>&#352;oli&#263;</surname><given-names initials="P">Petar</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6390-1899</contrib-id><name name-style="western"><surname>&#352;eri&#263;</surname><given-names initials="L">Ljiljana</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Chietera</surname><given-names initials="FP">Francesco Paolo</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Ria</surname><given-names initials="A">Andrea</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Tang</surname><given-names initials="X">Xiao</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05359">Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture in Split, University of Split, 21000 Split, Croatia; <email>psolic@fesb.hr</email> (P.&#352;.); <email>ljiljana@fesb.hr</email> (L.&#352;.)</aff><author-notes><corresp id="c1-sensors-25-05359"><label>*</label>Correspondence: <email>jsabic01@fesb.hr</email> (J.&#352;.); <email>toperkov@fesb.hr</email> (T.P.)</corresp><fn id="fn1-sensors-25-05359"><label>&#8224;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>29</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5359</elocation-id><history><date date-type="received"><day>24</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>26</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>29</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05359.pdf"/><abstract><p>Smart-beehive technologies represent a paradigm shift in beekeeping, transitioning from traditional, reactive methods toward proactive, data-driven management. This systematic literature review investigates the current landscape of intelligent systems applied to beehives, focusing on the integration of IoT-based monitoring, sensor modalities, machine learning techniques, and their applications in precision apiculture. The review adheres to PRISMA guidelines and analyzes 135 peer-reviewed publications identified through searches of Web of Science, IEEE Xplore, and Scopus between 1990 and 2025. It addresses key research questions related to the role of intelligent systems in early problem detection, hive condition monitoring, and predictive intervention. Common sensor types include environmental, acoustic, visual, and structural modalities, each supporting diverse functional goals such as health assessment, behavior analysis, and forecasting. A notable trend toward deep learning, computer vision, and multimodal sensor fusion is evident, particularly in applications involving disease detection and colony behavior modeling. Furthermore, the review highlights a growing corpus of publicly available datasets critical for the training and evaluation of machine learning models. Despite the promising developments, challenges remain in system integration, dataset standardization, and large-scale deployment. This review offers a comprehensive foundation for the advancement of smart apiculture technologies, aiming to improve colony health, productivity, and resilience in increasingly complex environmental conditions.</p></abstract><kwd-group><kwd>smart beehives</kwd><kwd>precision apiculture</kwd><kwd>hive monitoring</kwd><kwd>intelligent systems</kwd><kwd>Internet of Things</kwd><kwd>datasets</kwd></kwd-group><funding-group><award-group><funding-source>&#8220;Fact-checking in the specialised fields of energy and computing&#8221; project, funded by the National Recovery and Resilience Plan of the Republic of Croatia</funding-source><award-id>NPOO C1.1.1. R6-I2</award-id></award-group><award-group><funding-source>Digital Plan co-funded by the European Union through the Interreg Italy-Croatia 2021&#8211;2027 Programme</funding-source></award-group><funding-statement>This paper was supported by the following projects: &#8220;Fact-checking in the specialised fields of energy and computing&#8221; project, funded by the National Recovery and Resilience Plan of the Republic of Croatia, NPOO C1.1.1. R6-I2 and by Digital Plan co-funded by the European Union through the Interreg Italy-Croatia 2021&#8211;2027 Programme.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05359"><title>1. Introduction</title><p>Honeybees (<italic toggle="yes">Apis mellifera</italic>) are essential pollinators in ecosystems and agricultural systems worldwide. However, their populations have been declining due to multiple stressors including climate change, pesticide exposure, habitat loss, and pathogens&#160;[<xref rid="B1-sensors-25-05359" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05359" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05359" ref-type="bibr">3</xref>]. This decline threatens global food security and biodiversity and has motivated researchers to develop more efficient and non-invasive hive monitoring strategies&#160;[<xref rid="B4-sensors-25-05359" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05359" ref-type="bibr">5</xref>].</p><p>Precision apiculture, or smart beekeeping, uses embedded sensors, wireless communications, and AI algorithms to monitor hives in real-time and support timely interventions&#160;[<xref rid="B6-sensors-25-05359" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05359" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05359" ref-type="bibr">8</xref>]. These systems can capture acoustic signals, environmental conditions, hive weight, and visual cues to detect anomalies such as swarming, queen loss, or disease presence&#160;[<xref rid="B9-sensors-25-05359" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05359" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05359" ref-type="bibr">11</xref>].</p><p>Technological advancements include the integration of wireless sensor networks (WSNs) and low-power communication protocols such as ZigBee, LoRa, and NB-IoT&#160;[<xref rid="B12-sensors-25-05359" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05359" ref-type="bibr">13</xref>]. In parallel, artificial intelligence techniques, including convolutional neural networks (CNNs), support vector machines (SVMs), and random forests, are increasingly used to analyze hive soundscapes, image data, and temporal patterns&#160;[<xref rid="B14-sensors-25-05359" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05359" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05359" ref-type="bibr">16</xref>].</p><p>Despite the potential of these systems, the literature remains fragmented. Most studies focus on proof-of-concept deployments with limited duration and controlled environments, lacking robustness and generalizability&#160;[<xref rid="B2-sensors-25-05359" ref-type="bibr">2</xref>,<xref rid="B8-sensors-25-05359" ref-type="bibr">8</xref>]. Moreover, there is limited cross-comparison of sensing approaches and a lack of publicly available datasets for benchmarking&#160;[<xref rid="B17-sensors-25-05359" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05359" ref-type="bibr">18</xref>]. These issues persist despite growing interest and publication volume: heterogeneous application goals, diverse sensing modalities, and a variety of communication protocols and data analysis methods all hinder the synthesis of best practices and limit cross-study generalizability. Many systems are still prototypes that have not been validated over long time periods or in real-world apiaries, making it difficult to assess their robustness&#160;[<xref rid="B19-sensors-25-05359" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05359" ref-type="bibr">20</xref>].</p><p>To address these gaps, this systematic literature review analyzes 135 peer-reviewed publications between 1990 and 2025. We categorize the reviewed works across six dimensions: sensor modality, communication method, data storage approach, data processing algorithm, and application objective, which collectively serve to inform the sixth dimension&#8212;hive state classification. To ensure transparency and reproducibility, this systematic review follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. PRISMA provides a structured framework for conducting and reporting systematic reviews.The aim of this systematic review is to answer the following research questions&#160;(RQ):<list list-type="bullet"><list-item><p><bold>RQ1:</bold> What types of sensing modalities are most commonly used in smart beehive systems?</p></list-item><list-item><p><bold>RQ2:</bold> In which application domains are smart technologies for beehives being deployed, and how have these focal areas evolved over time?</p></list-item><list-item><p><bold>RQ3:</bold> Which data analysis and machine learning methods have been applied, and how prevalent are advanced techniques in comparison to classical approaches?</p></list-item><list-item><p><bold>RQ4:</bold> What technical and practical limitations are reported across these studies?</p></list-item><list-item><p><bold>RQ5:</bold> What publicly available datasets exist for smart-beehive research, what data modalities do they include, and how are these datasets labeled and used to develop or evaluate machine learning models?</p></list-item></list></p><p>Through this analysis, we identify prevailing trends, methodological limitations, and areas for future research in smart apiculture systems.</p></sec><sec id="sec2-sensors-25-05359"><title>2. Materials and Methods</title><p>This systematic review was conducted in accordance with the PRISMA 2020 guidelines&#160;[<xref rid="B21-sensors-25-05359" ref-type="bibr">21</xref>], including development of a review protocol, a comprehensive multi-database search, and documentation of the screening process in a PRISMA flow diagram in <xref rid="sec3-sensors-25-05359" ref-type="sec">Section 3</xref>.</p><sec id="sec2dot1-sensors-25-05359"><title>2.1. Eligibility Criteria</title><p>The primary eligibility criteria for inclusion in this systematic literature review were defined to identify studies that implement intelligent systems within the scope of precision apiculture, particularly involving smart beehive technologies. The broader term &#8217;smart beehive&#8217; was utilized to encompass studies that apply various smart technologies such as sensors, IoT (Internet of Things), artificial intelligence (AI), machine learning (ML), and data analytics specifically targeted at beekeeping and hive management. To enhance study quality, we further restricted the corpus to peer-reviewed publications reporting empirical data and excluded editorials, abstracts, theses, and articles lacking reproducible methods or data. Two authors independently screened titles and abstracts and assessed full texts against these criteria; disagreements were resolved by discussion or, when necessary, by consulting a third reviewer. In addition, a rudimentary quality assessment was applied during full-text screening: we required each study to describe its sensor configuration, provide at least one quantitative metric (e.g., accuracy, precision, recall or error) and report experimental conditions (sample size, location or duration). Papers lacking these methodological details were excluded to ensure reproducibility of the extracted information.</p><p>Studies that focused solely on traditional beekeeping practices without technological integration, or those dealing only with biological or ecological aspects without applying sensor technology or data-driven analytical methods, were excluded.</p><p>Only peer-reviewed articles and conference papers published in English between January 1990 and April 2025 were considered. The detailed eligibility criteria utilized for this systematic review are summarized in <xref rid="sensors-25-05359-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec2dot2-sensors-25-05359"><title>2.2. Information Sources</title><p>A search of the Web of Science, IEEE Xplore, and Scopus databases was conducted on 7 April 2025, to identify relevant scientific publications related to intelligent systems in beekeeping. These databases were chosen for their broad coverage of peer-reviewed literature in engineering, agriculture, and computer science. The search included all publications available up to 7 April 2025.</p></sec><sec id="sec2dot3-sensors-25-05359"><title>2.3. Search Strategy</title><p>A comprehensive literature search was conducted on 7 April 2025, using three major scientific databases: Web of Science, IEEE&#160;Xplore, and Scopus. These databases were selected for their extensive coverage of peer-reviewed scientific and engineering literature relevant to intelligent systems and applied technologies. The search strategy was developed collaboratively by two reviewers and was piloted and iteratively refined to balance recall and&#160;specificity.</p><p>The search strategy was carefully adapted to the syntax and filtering capabilities of each database, with a temporal range covering publications from 1990 to 2025. In all cases, we combined general technology keywords (precision, smart, intelligent, automated) with beekeeping-specific terms (beekeeping, beehive, apiculture, apiary) using Boolean operators. Additional phrase searches (&#8220;precision beekeeping&#8221;, &#8220;smart beehive&#8221;) were included to capture variations. The aim was to identify studies focused on the application of intelligent systems in beekeeping, particularly involving smart beehive technologies. In the Web of Science database, the search query combined general terms such as precision, smart, intelligent, and automated with domain-specific keywords like beekeeping, beehive, apiculture, and apiary. These were searched within all fields using the Boolean operator <monospace>OR</monospace> to ensure inclusivity. Filters were applied to restrict the document type to journal articles and conference proceedings, and the date range was set from 1990 to 2025.</p><p>For IEEE Xplore, the query was adapted to search across all metadata fields using the same combinations of general and domain-specific terms. The results were further refined to include only journal and conference publications. Across all databases we restricted the language to English and excluded document types such as editorial notes, letters and abstracts. Where available, we applied citation indexing filters to prioritize peer-reviewed literature.</p><p>In Scopus, the search was conducted within titles, abstracts, and keywords using an equivalent Boolean logic structure. Publications were filtered to include only articles and conference papers written in English and published after 1990.</p><p>An overview of the search queries applied to each database, including Boolean logic and filtering criteria, is provided in <xref rid="sensors-25-05359-t002" ref-type="table">Table 2</xref>.</p></sec><sec id="sec2dot4-sensors-25-05359"><title>2.4. Data Extraction and Categorization</title><p>To systematically analyze and compare smart beehive systems, we conducted a structured data extraction process. Each reviewed study was annotated across six dimensions: Bibliographic Info, Sensor Type, Communication Type, Method/Technique Type, Goal Category, and Key Aspects. These dimensions enable uniform representation of heterogeneous systems and serve as the foundation for subsequent visualizations and synthesis.</p><p><xref rid="sensors-25-05359-f001" ref-type="fig">Figure 1</xref> illustrates the structure of the extraction matrix. For each publication, binary encoding (0/1) was applied to indicate the presence of a given feature or method. Additionally, descriptive metadata was manually extracted and paraphrased from each study to provide contextual insight.</p><p>To ensure clarity and reproducibility, the coding schema was supported by a formal taxonomy of categories, as shown in <xref rid="sensors-25-05359-f002" ref-type="fig">Figure 2</xref>. This taxonomy was used to organize the various sensor types, communication technologies, analytical methods, and goal categories. In addition to enumerating the building blocks of our review, the figure also depicts the typical flow of information through a smart-beehive system. Readings from the sensors used in a given publication or project may be logged to local or cloud storage (forming a dataset) and/or transmitted via wired, short-range or long-range wireless links to a remote computer. On the remote side, selected analytical methods are applied to these measurements in order to derive the desired goal about the hive or bee state. For completeness we note that some investigations do not collect new data but instead start from previously curated datasets&#8212;this alternative path is indicated by the blue arrow in the figure.</p><p>Each of the main categorical dimensions is described in detail below:</p><p><bold>Sensor Type</bold>: This dimension captures which physical sensing technologies were used in each system. A total of 41 binary-coded sensor types were grouped into six high-level categories:<list list-type="bullet"><list-item><p>Environmental/Weather Sensors (e.g., temperature, humidity, air pressure; these include sensors monitoring conditions both inside the hive (internal climate) and in its surroundings).</p></list-item><list-item><p>Acoustic/Vibration Sensors (e.g., microphones, piezoelectric sensors).</p></list-item><list-item><p>Imaging Sensors (e.g., cameras, optical counters, thermal imaging).</p></list-item><list-item><p>Hive Structural Sensors (e.g., weight/load cells, strain gauges).</p></list-item><list-item><p>Motion/Orientation Sensors (e.g., accelerometers, gyroscopes).</p></list-item><list-item><p>Air Composition Sensors (e.g., CO<sub>2</sub>, VOC, O<sub>2</sub>).</p></list-item><list-item><p>Bee Activity Counters (e.g., infrared gates, tags).</p></list-item></list></p><p><bold>Communication Type</bold>: This category encodes the technologies used to transmit data from the hive. The classification includes:<list list-type="bullet"><list-item><p>Short-Range Wireless (e.g., Zigbee, Wi-Fi, Bluetooth).</p></list-item><list-item><p>Long-Range Wireless (e.g., LoRa, NB-IoT, Sigfox, GSM).</p></list-item><list-item><p>Wired Communication (e.g., Ethernet, PowerLine).</p></list-item></list></p><p><bold>Method/Technique Type</bold>: This is the most granular dimension, with over 180 tags, grouped into ten parent categories:<list list-type="bullet"><list-item><p>Statistical and Time-Series Analysis (e.g., regression, correlation, ARIMA, VAR).</p></list-item><list-item><p>Feature Extraction and Signal Processing (e.g., FFT, MFCC, DWT).</p></list-item><list-item><p>Classical Machine Learning (e.g., SVM, Random Forest, k-NN, Naive Bayes).</p></list-item><list-item><p>Deep Learning and Neural Networks (e.g., CNN, LSTM, Transformer-based models).</p></list-item><list-item><p>Computer Vision and Image Analysis (e.g., contour detection, image segmentation)</p></list-item><list-item><p>Unsupervised Learning and Anomaly Detection (e.g., clustering, outlier detection).</p></list-item><list-item><p>Rule-Based Systems and Thresholding (e.g., thresholding (T1, T2, T3, T*), Custom swarming algorithm).</p></list-item><list-item><p>Data Fusion and Ensemble Methods (e.g., weighted multi-criteria aggregation algorithm, Majority voting).</p></list-item><list-item><p>Expert Systems and Fuzzy Logic (e.g., Fuzzy-stranded-NN, fuzzy logic model (FLM)).</p></list-item><list-item><p>Sensor Analysis/Domain-Specific (e.g., BFCI formula: <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (weather scoring)).</p></list-item></list></p><p><bold>Goal Category</bold>: Each system was also classified by its intended application, which helps contextualize the chosen sensors and methods. The following seven goal categories were used:<list list-type="bullet"><list-item><p>Monitoring: Real-time reporting of hive metrics.</p></list-item><list-item><p>Behavior Detection: Recognizing bee behaviors.</p></list-item><list-item><p>Health Assessment: Detecting disease or colony vitality issues.</p></list-item><list-item><p>Prediction/Forecasting: Estimating future events like swarming or yield.</p></list-item><list-item><p>Optimization/Decision Support: Guiding interventions and hive management.</p></list-item><list-item><p>System/IoT Development: Engineering and infrastructure for sensing platforms.</p></list-item><list-item><p>Threat Detection: Identifying predators, theft, or environmental hazards.</p></list-item></list></p><p><bold>Key Aspects</bold>: This field contains a brief textual summary of the study&#8217;s technical contribution, extracted from the abstract or discussion. It often includes insights into the system&#8217;s novelty, testing conditions, or dataset characteristics. Although not used for quantitative analysis, it adds interpretive richness to the dataset.</p><p><bold>Bibliographic Info</bold>: For traceability, each entry includes citation metadata (author, title, year, source type) alongside classification. This enables filtering by publication date, venue type (conference/journal), or other bibliometric properties.</p><p>The result of this data extraction process is a multidimensional matrix that enables consistent, reproducible analysis across the reviewed literature. All subsequent quantitative results and visualizations&#8212;including heatmaps, frequency distributions, and co-occurrence charts&#8212;are derived from this underlying structure.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05359"><title>3. Results and Discussion</title><sec id="sec3dot1-sensors-25-05359"><title>3.1. Corpus and Structured Summary of Included Studies</title><p>The selection of studies was conducted following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines. A total of 917 records were identified through database searches on 7 April 2025, across three major scientific databases: Web of Science (<italic toggle="yes">n</italic> = 409), Scopus (<italic toggle="yes">n</italic> = 377), and IEEE Xplore (<italic toggle="yes">n</italic> = 131). After automatic removal of duplicates, 532 unique records remained.</p><p>Title screening excluded 246 records based on relevance. The remaining 286 articles were subjected to abstract screening and full-text assessment, during which 151 were excluded for not meeting the inclusion criteria (e.g., lacking empirical data, not involving intelligent systems, or focusing solely on ecological aspects). Ultimately, 135 studies were retained for full analysis.</p><p>The complete selection workflow is illustrated in <xref rid="sensors-25-05359-f003" ref-type="fig">Figure 3</xref>.</p><p>The selected studies were systematically coded into a structured extraction matrix, as described in <xref rid="sec2dot4-sensors-25-05359" ref-type="sec">Section 2.4</xref>. This matrix forms the empirical foundation for all subsequent analyses of technological patterns, analytical techniques, and system goals. A condensed version of the data is presented in <xref rid="app1-sensors-25-05359" ref-type="app">Table A1</xref> in <xref rid="app1-sensors-25-05359" ref-type="app">Appendix A</xref>.</p></sec><sec id="sec3dot2-sensors-25-05359"><title>3.2. Overview</title><p>This organizational framework enables structured comparison of methodological approaches, sensor configurations, and system goals. By categorizing studies based on their intended purpose, the review highlights prevailing trends, emerging directions, and underexplored topics within the domain of smart beekeeping systems.</p><p><xref rid="sensors-25-05359-f004" ref-type="fig">Figure 4</xref> illustrates the distribution of the reviewed works by publication type and research objective. Among the 135 included studies, 93 were journal articles (68.9%) and 42 were conference papers (31.1%), indicating a preference for peer-reviewed journal publication within the research community.</p><p><xref rid="sensors-25-05359-f005" ref-type="fig">Figure 5</xref> presents a heatmap illustrating the distribution of reviewed journal publications by year. The data reveal a steady increase in research activity related to smart beehive technologies, with a notable surge beginning around 2020. Computers and Electronics in Agriculture is the most prominent journal, publishing 15 of the reviewed studies. This reflects the strong alignment between agricultural engineering and the development of digital monitoring systems.</p><p>Other leading publication venues include Sensors, Applied Sciences, and Ecological Informatics, all of which support interdisciplinary research at the intersection of sensing technologies, environmental monitoring, and applied sciences. The &#8220;Others&#8221; category consolidates several journals with smaller contributions, indicating broader, though more dispersed, interest in the topic across additional academic platforms.</p><p><xref rid="sensors-25-05359-f006" ref-type="fig">Figure 6</xref> presents the distribution of reviewed conference publications by year. Although conferences represent a smaller share of the overall scholarly output compared to journals, they remain an essential channel for disseminating technical innovations in smart-beehive systems. Among them, IEEE SOUTHEASTCON contributes the highest number of papers, consistently serving as a venue for research on sensor systems, communication protocols, and embedded platforms relevant to apiculture.</p><p>Other conferences, including Engineering Veracruz, CSCITA, and the Internet of Sounds Symposium, each contributed a single publication, highlighting the growing interdisciplinary interest in applying domain-specific technologies to beekeeping. The &#8220;Others&#8221; category similarly comprises venues with one publication each.</p><p>Overall, the trend lines across both publication types reflect an emerging, but rapidly professionalizing research landscape. While journal publications dominate the discourse, conferences continue to provide a dynamic space for the presentation of nascent research and the cultivation of scholarly dialogue. The consistent appearance of certain venues over multiple years affirms the establishment of recurring academic communities interested in precision apiculture. Notably, our search did not retrieve contributions from major agricultural engineering conferences (e.g., ASABE or European Federation of Agricultural Engineers); this absence underscores the need for greater cross-fertilization between apiculture and mainstream agricultural technology forums. Furthermore, we observed that much of the research in this domain has been published in engineering-oriented journals and conferences, with relatively few contributions in apiculture or agriculture-specific venues. This suggests a need for better cross-disciplinary dissemination and collaboration to ensure smart beehive technologies address practical beekeeping challenges and scientific questions in apiculture.</p><p>In terms of research focus, the most prevalent functional goals across the reviewed literature were Health Assessment (36 papers) and Behavior Detection (35 papers), which together comprise over half of the corpus. Health Assessment studies typically aimed to evaluate colony condition or detect signs of disease and stress, while Behavior Detection focused on identifying bee activities such as foraging, swarming, or in-hive movement patterns through visual, acoustic, or motion-based cues. The Monitoring category included 27 studies centered on real-time tracking of environmental or hive-level parameters. Prediction/Forecasting, found in 17 papers, involved anticipating future hive states such as swarming events or honey yield. Optimization/Decision Support appeared in 8 studies and focused on data-driven recommendations for hive management. System/IoT Development, present in 7 works, primarily addressed sensor integration, platform engineering, or hardware optimization. Finally, Threat Detection was the least represented category, with only 5 studies focused on identifying risks such as predators, theft, or environmental anomalies. These distributions are illustrated in <xref rid="sensors-25-05359-f007" ref-type="fig">Figure 7</xref>, while a complete mapping of publications by goal category is provided in <xref rid="sensors-25-05359-t003" ref-type="table">Table 3</xref>.</p><p>Beyond static category counts, <xref rid="sensors-25-05359-f008" ref-type="fig">Figure 8</xref> illustrates the temporal progression of the four most prevalent research goals from 2020 to 2024 (<xref rid="sensors-25-05359-f008" ref-type="fig">Figure 8</xref>a), along with a snapshot of their current distribution in early 2025 (<xref rid="sensors-25-05359-f008" ref-type="fig">Figure 8</xref>b).</p><p>Health Assessment consistently remained the dominant category throughout this period, with a notable peak in 2024. This sustained trend highlights the continued importance of diagnosing colony conditions and detecting signs of disease using sensor-derived data such as audio, weight, or thermal profiles.</p><p>Behavior Detection showed a steady increase in interest, peaking in 2024 with 11 publications&#8212;making it the second most active category that year after Health Assessment. This underscores growing research attention to behavioral analysis using video, acoustic, and motion sensing, often paired with computer vision and machine learning techniques.</p><p>Monitoring was highly active in 2020 but declined in later years, with minor resurgence in 2023 and 2024. These early peaks likely reflect the foundational role of IoT-based data collection systems that support more advanced analytics downstream.</p><p>Prediction/Forecasting showed a clear upward trajectory, with activity growing steadily from 2020 (1 paper) to a peak in 2024 (6 papers). This progression indicates a shift toward model-based, anticipatory systems that leverage historical and real-time data for proactive decision-making.</p><p>Together, these trends point to a maturing research landscape&#8212;moving from sensor infrastructure and basic data reporting toward complex behavioral inference and predictive analytics in smart apiculture.</p><p><xref rid="sensors-25-05359-f009" ref-type="fig">Figure 9</xref> presents a word cloud generated from the abstracts of the reviewed studies, highlighting the most frequently occurring terms in smart beehive literature. Dominant keywords such as system, monitoring, data, honey, and colony reflect the field&#8217;s central focus on automated hive management and data-centric decision-making. The prominence of terms like monitoring and data reinforces the foundational role of sensor networks and continuous observation, while frequent mentions of honey and colony emphasize biological productivity and colony-level welfare as key research drivers.</p><sec id="sec3dot2dot1-sensors-25-05359"><title>3.2.1. Sensors in Smart-Beehive Systems</title><p>The following subsections explore sensor modalities used in smart-beehive systems. A wide range of sensor types have been employed across the reviewed smart beehive. <xref rid="sensors-25-05359-f010" ref-type="fig">Figure 10</xref> summarizes the distribution of sensor modalities by indicating how many of the 135 studies involved each sensor type. It is important to note that the presence of a sensor in a study does not necessarily imply that the sensor was physically deployed by the authors. In many cases, the researchers either collected their own data using such sensors or utilized publicly available datasets obtained from hives instrumented with the corresponding sensor types. Consequently, multiple sensors may be counted per study, and the totals in <xref rid="sensors-25-05359-f010" ref-type="fig">Figure 10</xref> exceed the number of studies reviewed.</p><p>Several clear patterns emerge from <xref rid="sensors-25-05359-f010" ref-type="fig">Figure 10</xref>. Environmental sensors are the most commonly used, featured in 66 studies, which accounts for approximately 49% of the publications. This prevalence reflects the foundational importance of internal/external climate monitoring for assessing hive health and the relative ease of collecting such data [<xref rid="B7-sensors-25-05359" ref-type="bibr">7</xref>,<xref rid="B40-sensors-25-05359" ref-type="bibr">40</xref>,<xref rid="B42-sensors-25-05359" ref-type="bibr">42</xref>,<xref rid="B132-sensors-25-05359" ref-type="bibr">132</xref>].</p><p>Close behind are acoustic/vibration sensors, used in 52 studies, representing about 39% of the total. Their popularity underscores the value of hive sound patterns&#8212;acoustic signals provide insights into colony behavior, queen status, swarming tendencies, and stress indicators [<xref rid="B43-sensors-25-05359" ref-type="bibr">43</xref>,<xref rid="B48-sensors-25-05359" ref-type="bibr">48</xref>,<xref rid="B53-sensors-25-05359" ref-type="bibr">53</xref>,<xref rid="B84-sensors-25-05359" ref-type="bibr">84</xref>,<xref rid="B90-sensors-25-05359" ref-type="bibr">90</xref>].</p><p>Imaging sensors appear in 49 studies, making up roughly 36% of the reviewed publications. The growth in computer vision methods, driven by increasingly accessible hardware and advances in image processing, has made visual analysis a leading choice for bee traffic monitoring and pathogen detection [<xref rid="B10-sensors-25-05359" ref-type="bibr">10</xref>,<xref rid="B81-sensors-25-05359" ref-type="bibr">81</xref>,<xref rid="B82-sensors-25-05359" ref-type="bibr">82</xref>,<xref rid="B110-sensors-25-05359" ref-type="bibr">110</xref>].</p><p>Hive weight/structural sensors are used in 34 studies, which corresponds to approximately 25%. This highlights the utility of weight measurements for tracking honey accumulation, food reserves, and colony strength with load cells or strain gauges [<xref rid="B4-sensors-25-05359" ref-type="bibr">4</xref>,<xref rid="B25-sensors-25-05359" ref-type="bibr">25</xref>,<xref rid="B30-sensors-25-05359" ref-type="bibr">30</xref>,<xref rid="B86-sensors-25-05359" ref-type="bibr">86</xref>].</p><p>More specialized sensor types are used less frequently. Bee activity counters appear in 13 studies, amounting to about 10%, and are often implemented through tags and cameras, infrared gates, or other entryway counters for quantifying foraging activity or ingress/egress patterns [<xref rid="B47-sensors-25-05359" ref-type="bibr">47</xref>,<xref rid="B49-sensors-25-05359" ref-type="bibr">49</xref>,<xref rid="B54-sensors-25-05359" ref-type="bibr">54</xref>,<xref rid="B109-sensors-25-05359" ref-type="bibr">109</xref>].</p><p>Air composition sensors are used in 10 studies, which represents roughly 7%. While potentially valuable for correlating gas levels with colony metabolism or health, their cost and limited interpretability may explain the relatively low adoption [<xref rid="B76-sensors-25-05359" ref-type="bibr">76</xref>,<xref rid="B88-sensors-25-05359" ref-type="bibr">88</xref>,<xref rid="B96-sensors-25-05359" ref-type="bibr">96</xref>,<xref rid="B133-sensors-25-05359" ref-type="bibr">133</xref>].</p><p>Finally, motion/orientation sensors are the least utilized, appearing in just 6 studies&#8212;approximately 4% of the total. Their use is typically limited to detecting external disturbances such as hive displacement due to wind, physical impact, or theft. As these events are relatively rare or peripheral to core hive monitoring objectives, such sensors are less commonly integrated into smart beehive systems [<xref rid="B16-sensors-25-05359" ref-type="bibr">16</xref>,<xref rid="B31-sensors-25-05359" ref-type="bibr">31</xref>].</p><p>It is worth noting that the average smart hive study employs multiple sensor types to gain a more holistic view of the colony. In our dataset, systems used on average about 1.7 distinct sensor modalities each. As shown in <xref rid="sensors-25-05359-f011" ref-type="fig">Figure 11</xref>, about 55% of studies (74 out of 135) used exactly one sensor type&#8212;often these were single-modality systems such as purely acoustic or image monitoring setups. In contrast, approximately 45% of the studies employed a combination of two or more sensor types.</p></sec><sec id="sec3dot2dot2-sensors-25-05359"><title>3.2.2. Analytical Techniques and Algorithm Performance</title><p>After acquisition, sensor data must be processed and interpreted. This subsection summarizes the analytical methods and machine-learning algorithms used in smart beehive research, highlighting adoption trends and exemplary performance metrics.</p><p>A significant portion (26%) of studies integrated two sensor types, while 13% used three distinct modalities. Only a small fraction (6%) of projects incorporated four sensor types in a single system like [<xref rid="B25-sensors-25-05359" ref-type="bibr">25</xref>,<xref rid="B27-sensors-25-05359" ref-type="bibr">27</xref>,<xref rid="B109-sensors-25-05359" ref-type="bibr">109</xref>]. This multi-sensor approach reflects the principle of sensor fusion: by combining complementary data sources, researchers can cross-validate findings and detect complex colony conditions that may not be evident through a single modality alone.</p><p>However, adding more sensors inevitably increases system complexity, cost, and power requirements&#8212;likely explaining why very few projects go beyond three or four sensing modalities.</p><p>The intelligence in smart beehive systems comes from the data analysis algorithms that process sensor inputs into meaningful predictions, detections, or decisions. The reviewed studies span a broad spectrum of analytical approaches, from simple statistical thresholding to cutting-edge deep learning models. <xref rid="sensors-25-05359-f012" ref-type="fig">Figure 12</xref> shows the distribution of major classes of AI/analysis methods used across the 135 studies (many studies employ more than one type of analysis, so counts are overlapping).</p><p>Deep learning and neural networks have become the most widely used analytical approach, with 44 studies accounting for about 33% of the corpus employing such techniques. This category includes the use of convolutional neural networks (CNNs) for image classification (e.g., identifying pests or classifying bee species/health from images) [<xref rid="B58-sensors-25-05359" ref-type="bibr">58</xref>,<xref rid="B87-sensors-25-05359" ref-type="bibr">87</xref>], as well as other neural network architectures for analyzing audio spectrograms or multivariate sensor time-series [<xref rid="B98-sensors-25-05359" ref-type="bibr">98</xref>]. The prevalence of deep learning indicates that many researchers have started leveraging large datasets and powerful computing to improve detection accuracy in complex tasks like image-based mite detection or audio-based behavior classification. Indeed, deep learning often outperforms classical methods given sufficient data, which aligns with its adoption in about one-third of the studies.</p><p>Almost equally common are feature extraction and signal processing techniques, recorded in 42 studies, representing approximately 31% of the total. This category represents the foundational step in many analyses, for example, calculating the Fourier transform of audio to obtain frequency features [<xref rid="B59-sensors-25-05359" ref-type="bibr">59</xref>] or performing image preprocessing and segmentation to isolate regions of interest (like bee or mite shapes in an image) [<xref rid="B83-sensors-25-05359" ref-type="bibr">83</xref>]. Such techniques often precede other analyses and are pervasive as a component of the methodology, hence their high count.</p><p>Basic statistical and time-series analysis methods were employed in 41 studies, accounting for around 30%. These include approaches like correlation analysis [<xref rid="B122-sensors-25-05359" ref-type="bibr">122</xref>], ARIMA models for time-series forecasting of hive parameters [<xref rid="B121-sensors-25-05359" ref-type="bibr">121</xref>], or simple regression models [<xref rid="B86-sensors-25-05359" ref-type="bibr">86</xref>]. Many papers rely on statistical analysis to interpret trends (for example, to see daily patterns in weight or temperature) or to set adaptive thresholds (like control charts for abnormal sensor readings). The substantial usage of statistical methods reflects that not all smart hive research relies on complex AI&#8212;sometimes straightforward statistical modeling suffices to derive insights from sensor data [<xref rid="B80-sensors-25-05359" ref-type="bibr">80</xref>,<xref rid="B126-sensors-25-05359" ref-type="bibr">126</xref>].</p><p>Turning to classical machine learning, supervised ML techniques appear in 25 studies, making up roughly 18% of the corpus. These methods include algorithms such as support vector machines, random forests, k-nearest neighbors, or naive Bayes classifiers. They have been applied, for instance, to classify sound patterns (using features like Mel-frequency cepstral coefficients from audio) [<xref rid="B53-sensors-25-05359" ref-type="bibr">53</xref>], to distinguish normal vs. abnormal hive states [<xref rid="B84-sensors-25-05359" ref-type="bibr">84</xref>], or to predict outcomes like swarm occurrence based on multivariate sensor inputs [<xref rid="B13-sensors-25-05359" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05359" ref-type="bibr">14</xref>]. While classical ML is less dominant than deep learning in recent literature, it remains relevant, especially for moderate-sized datasets or where model interpretability is valued.</p><p>Applications of computer vision and image analysis techniques were noted in 22 studies, corresponding to approximately 16%. This category overlaps partially with deep learning because many vision tasks now use CNNs; however, it also covers conventional image processing (background subtraction, contour detection, etc.) and classical vision algorithms. The presence of computer vision methods reflects the significant subset of works dealing with camera data&#8212;counting bees at the entrance [<xref rid="B10-sensors-25-05359" ref-type="bibr">10</xref>], detecting mite specks on bees [<xref rid="B106-sensors-25-05359" ref-type="bibr">106</xref>], tracking bee motion in video [<xref rid="B46-sensors-25-05359" ref-type="bibr">46</xref>], etc. Some studies combined traditional vision algorithms with newer ones (e.g., using feature detectors alongside CNNs to improve robustness [<xref rid="B77-sensors-25-05359" ref-type="bibr">77</xref>]).</p><p>We also observed rule-based methods in 17 studies, which amounts to about 13% of the total. These are systems using predefined rules or logic, such as if-then rules triggered by sensor thresholds or expert system approaches. They tend to appear in early or simpler systems, for example, a system that sends an alert if weight drops by more than X in a day (indicating a swarm) or if temperature falls outside a band [<xref rid="B3-sensors-25-05359" ref-type="bibr">3</xref>,<xref rid="B34-sensors-25-05359" ref-type="bibr">34</xref>]. While conceptually straightforward and easy to implement, pure rule-based systems are less adaptive and may not handle complex patterns, which is likely why their relative usage has diminished over time in favor of learning-based methods.</p><p>A smaller portion of studies applied unsupervised learning or anomaly detection techniques, with 14 studies representing around 10%. These include clustering algorithms to group similar hive conditions, or outlier detection methods to flag unusual sensor patterns without pre-labeled examples. Such approaches are valuable when trying to detect novel or unexpected events (for instance, an unknown type of anomaly in hive sound or climate that was not specifically trained for) [<xref rid="B15-sensors-25-05359" ref-type="bibr">15</xref>,<xref rid="B124-sensors-25-05359" ref-type="bibr">124</xref>]. However, unsupervised methods require careful interpretation and have seen limited use, often complementing other analyses rather than being standalone solutions.</p><p>A smaller subset of studies employed other, less common analytical strategies. These include domain-specific methods, data fusion, ensemble methods, and fuzzy logic systems, collectively appearing in only a handful of cases. Such approaches often aim to integrate multiple data sources or model outputs, or to handle uncertainty through rule-based reasoning. While conceptually valuable&#8212;particularly for combining sensor modalities or managing ambiguity in biological systems&#8212;their limited presence likely reflects practical constraints such as small datasets, system complexity, or the dominance of more scalable data-driven techniques in the field [<xref rid="B6-sensors-25-05359" ref-type="bibr">6</xref>,<xref rid="B78-sensors-25-05359" ref-type="bibr">78</xref>,<xref rid="B88-sensors-25-05359" ref-type="bibr">88</xref>,<xref rid="B91-sensors-25-05359" ref-type="bibr">91</xref>,<xref rid="B139-sensors-25-05359" ref-type="bibr">139</xref>].</p><p>To understand the progression of analytical approaches in smart beehive research, <xref rid="sensors-25-05359-f013" ref-type="fig">Figure 13</xref> displays the distribution of data analysis methods across three distinct periods: 2015&#8211;2018, 2019&#8211;2022, and 2023&#8211;2025. <xref rid="sensors-25-05359-t004" ref-type="table">Table 4</xref> complements this by summarizing the overall adoption rate of analytical techniques, including the period prior to 2015.</p><p>Before 2015, no studies employed analytical methods, reflecting the field&#8217;s early focus on hardware prototyping and sensor integration rather than data interpretation.</p><p>Between 2015 and 2018, 68.8% of studies utilized at least one analytical technique. This period was marked by methodological diversity rather than dominance, with various approaches each appearing in only a few studies. These early adopters explored a broad range of techniques, indicating a formative stage of experimentation without clear methodological convergence.</p><p>In the 2019&#8211;2022 period, the use of analytics increased to 81.2% of studies. Deep Learning and Neural Networks emerged as the most frequently used method, featuring in 12 studies&#8212;quadrupling its earlier usage. Statistical and Time Series Analysis and Feature Extraction and Signal Processing also gained traction, used in 9 and 7 studies, respectively. This indicates a shift toward more robust modeling strategies, particularly suited for capturing temporal patterns and supporting health diagnostics or forecasting applications.</p><p>By 2023&#8211;2025, analytical methods were nearly universal, applied in 95.3% of studies. Feature Extraction and Signal Processing led with 31 studies, followed closely by Deep Learning and Neural Networks (29) and Statistical and Time Series Analysis (28). Notably, Computer Vision and Imaging methods rose sharply to 20 studies, reflecting increased emphasis on visual behavior tracking and swarm detection. Meanwhile, Classical ML (Supervised) maintained stable adoption (16 studies), suggesting it is being complemented or gradually supplanted by more advanced architectures. This period marks the transition toward integrated, multimodal, and predictive systems in smart apiculture.</p><p>Together, these trends highlight the maturation of smart beehive research&#8212;from early-stage experimentation to a data-driven discipline where neural networks, signal processing, and vision-based analytics play a central role in colony monitoring and decision-making.</p></sec><sec id="sec3dot2dot3-sensors-25-05359"><title>3.2.3. Comparative Assessment of Sensors and Algorithms</title><p>We synthesized quantitative comparisons of commonly used sensors and machine learning approaches. <xref rid="sensors-25-05359-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-05359-t006" ref-type="table">Table 6</xref> summarize representative sensor characteristics (accuracy and approximate cost) and published performance metrics for exemplary algorithms.</p></sec></sec><sec id="sec3dot3-sensors-25-05359"><title>3.3. Meta-Analysis of Publications</title><p>An interesting question is how the choice of sensor modalities correlates with the research goal of a study. Different application domains might favor certain sensors. To explore this, we present several heatmaps and discuss the patterns they reveal.</p><sec id="sec3dot3dot1-sensors-25-05359"><title>3.3.1. Sensor Usage by Research Objectives</title><p><xref rid="sensors-25-05359-f014" ref-type="fig">Figure 14</xref> shows the co-occurrence of sensor types with each main goal category, highlighting patterns of association between sensing modalities and research objectives.</p><p>Several clear patterns can be observed in <xref rid="sensors-25-05359-f014" ref-type="fig">Figure 14</xref>: Behavior Detection studies heavily rely on imaging and acoustic sensors. This category includes the highest concentration of works utilizing cameras or visual tracking, along with a strong presence of studies using acoustic sensing. This aligns with expectations&#8212;analyzing bee behaviors such as foraging or in-hive activity often depends on direct observation through visual or auditory cues.</p><p>Health Assessment studies make substantial use of acoustic and imaging sensors, as many health diagnoses involve detecting anomalies in either sound&#8212;such as changes in buzzing from sick colonies&#8212;or visual patterns, like images of bees or brood used to identify mites or disease symptoms. These studies also frequently incorporate environmental sensors, since temperature shifts may signal brood issues or colony decline, and certain diseases can manifest through subtle changes in microclimate. A small subset of health-focused research has explored the use of air composition sensors to detect chemical markers of illness or elevated CO<sub>2</sub> resulting from poor ventilation in weakened hives. On the other hand, structural sensors such as those measuring hive weight appear less commonly in this context, as weight is generally more associated with food reserves than with disease&#8212;although significant weight loss can still indicate potential problems. Overall, health-monitoring systems tend to be multimodal, designed to detect a broad spectrum of physiological and behavioral symptoms.</p><p>Monitoring studies, as expected, prioritize the use of environmental and hive structural sensors. These general-purpose platforms often focus on recording vital hive parameters such as internal and external temperature and humidity, air pressure, wind speed, and hive weight. Their goal is usually to provide a comprehensive overview of hive status by capturing key physical indicators. Some studies also incorporate acoustic sensors to detect sound-related anomalies, such as sudden silence following colony collapse or excessive noise indicating disturbance.</p><p>Prediction and Forecasting studies frequently rely on environmental data, along with a notable presence of structural and acoustic inputs. For example, models predicting honey yield often use weather conditions and historical weight trends, while forecasts of swarming behavior might incorporate temperature and sound cues. The strong emphasis on environmental parameters is logical, given that many colony events&#8212;like nectar flow or swarm triggers&#8212;are closely tied to seasonal and weather-related patterns.</p><p>The remaining categories&#8212;Optimization and Decision Support, System and IoT Development, and Threat Detection&#8212;exhibit more specific sensor usage patterns, reflecting their narrower focus or earlier stage of maturity. Optimization-focused studies, although limited in number, typically employ environmental sensors to support yield improvement or management recommendations. System development papers consistently incorporate environmental, and frequently also structural and acoustic, sensors&#8212;suggesting that a standard sensing suite (e.g., temperature, humidity, weight, sound) is considered essential when building general-purpose platforms. In contrast, imaging sensors are less common in this group due to power and complexity constraints. Meanwhile, threat detection studies rely almost exclusively on imaging, sometimes complemented by acoustic sensing, to visually identify external aggressors such as hornets. The absence of environmental and weight sensors in these works highlights the highly targeted nature of this application domain.</p><p>These correlations underscore that the selection of sensors in a smart beehive project is closely aligned with its objectives. If the goal is to monitor or predict general hive status, environmental and weight sensors are the go-to choices for tracking broad trends. For researchers and engineers, this insight can guide the design of future systems: depending on the primary application, one can prioritize certain sensor modalities to maximize the likelihood of success.</p></sec><sec id="sec3dot3dot2-sensors-25-05359"><title>3.3.2. Sensor Co-Occurrence Patterns</title><p>The sensor co-occurrence heatmap shown in <xref rid="sensors-25-05359-f015" ref-type="fig">Figure 15</xref> provides a quantitative overview of how different sensor modalities are jointly used in smart beehive systems.</p><p>The most prominent co-occurrence is observed between environmental and hive structural sensors. This pairing reflects a foundational design in beekeeping technology, where ambient conditions such as temperature and humidity are monitored in parallel with hive weight to assess colony growth, honey production, or seasonal changes. In many cases, the weight sensor serves as a proxy for biomass or food reserves, while temperature provides thermal context&#8212;although the two data streams are often analyzed independently.</p><p>Another commonly observed combination involves environmental sensors and acoustic/vibration sensors. These systems typically use microphones or accelerometers to monitor hive activity, agitation, or swarming behavior, while simultaneously recording internal/external temperature. However, the data are frequently interpreted in isolation: acoustic features are used for classification or anomaly detection, whereas temperature readings are either passively logged or used to validate overall hive conditions.</p><p>A moderately common pattern includes the co-occurrence of hive structural and acoustic/vibration sensors, a setup that theoretically enables the study of behavioral states in relation to hive mass or movement. Similarly, the combination of environmental sensors with imaging systems is often found in platforms that monitor entrance traffic or thermal vision, though true fusion of visual and thermal features remains rare.</p><p>Other co-occurrences&#8212;such as those involving bee activity counters, motion/orientation sensors, or air composition sensors&#8212;are less frequently encountered and tend to serve more specialized roles. For instance, gas sensors like CO<sub>2</sub> or O<sub>2</sub> are typically used to investigate hive respiration or ventilation, while accelerometers may be included for structural monitoring or theft detection. These modalities are rarely integrated with others in a unified analytical framework, although there are notable exceptions. Newton et al. [<xref rid="B96-sensors-25-05359" ref-type="bibr">96</xref>], for example, combine CO<sub>2</sub>, temperature, vibration, and weight data to infer colony behavior during overwintering. Similarly, Robustillo et al. [<xref rid="B111-sensors-25-05359" ref-type="bibr">111</xref>] apply vector autoregressive models to jointly analyze temperature, humidity, weight, and meteorological variables for predicting internal hive conditions. Henry et al. [<xref rid="B1-sensors-25-05359" ref-type="bibr">1</xref>] also examine variability in acoustic and environmental data to assess colony stress under electromagnetic exposure, suggesting potential for integrated interpretations. In another example, the b+WSN platform [<xref rid="B14-sensors-25-05359" ref-type="bibr">14</xref>] incorporates gas, temperature, and weight sensors into a rule-based decision model that triggers alerts at the hive level.</p><p>A key insight drawn from the literature is that while sensor integration at the hardware level is common, true multimodal analysis or data fusion remains largely absent. Even in more complex systems that include multiple sensor types, data from each source are typically processed in isolation. As such, the co-occurrence heatmap primarily reflects design choices and hardware configurations rather than analytical integration.</p></sec><sec id="sec3dot3dot3-sensors-25-05359"><title>3.3.3. Sensor&#8211;Model Co-Occurrence Patterns</title><p>The sensor&#8211;model co-occurrence heatmap, shown in <xref rid="sensors-25-05359-f016" ref-type="fig">Figure 16</xref>, reveals several dominant patterns in the design of smart beehive systems, each reflecting how particular sensor modalities are suited to specific types of analysis as dictated by the intended application goals.</p><p>One of the strongest associations is between Acoustic/Vibration Sensors and a diverse range of analytical methods, including Feature Extraction and Signal Processing, Statistical and Time Series Analysis, Classical ML (Supervised), and Deep Learning and Neural Networks. This pattern reflects a substantial body of research focused on monitoring colony behavior, detecting stress responses, and identifying anomalies through audio-based cues. These systems typically rely on time&#8211;frequency representations of hive sound, such as spectrograms, which are then processed using machine learning or deep learning models. The focus in these works is often on non-invasive, real-time diagnostics aimed at swarm prediction or general hive health assessment.</p><p>Environmental/Weather Sensors are frequently paired with Statistical and Time Series Analysis. These studies generally seek to model colony microclimate, identify seasonal or daily patterns, and detect deviations from thermal norms that may indicate brood disturbance or weakening colonies. Given the scalar and temporal nature of the data, statistical methods such as trend analysis or control charts are both practical and interpretable, especially for low-cost, field-deployable systems.</p><p>Imaging Sensors show strong co-occurrence with Deep Learning and Neural Networks and Computer Vision and Imaging. These combinations are common in studies targeting automation of visual tasks such as bee counting, motion tracking, or disease detection based on visual symptoms. Convolutional neural networks (CNNs) dominate this space due to their ability to learn hierarchical visual features. Such systems are typically high-precision and are designed for monitoring hive entrance activity, external threats, or internal brood conditions.</p><p>Hive Structural Sensors, such as those used to measure weight, are most often linked with Statistical and Time Series Analysis. These systems often aim to infer honey production rates, colony strength, or feeding patterns by analyzing trends in weight data. Because weight is a cumulative and slowly varying signal, it naturally lends itself to forecasting models such as regression or ARIMA.</p><p>Other sensors&#8212;including Air Composition Sensors, Motion/Orientation Sensors, and Bee Activity/Counter Sensors&#8212;appear much less frequently and are mostly found in exploratory or proof-of-concept studies. Their limited analytical pairing reflects either the novelty of their application or the challenges of integrating their outputs into broader data pipelines. While some of these modalities show promise, their adoption remains sparse.</p><p>Overall, the co-occurrence analysis reveals that sensor selection and model choice are closely aligned with the nature of the data and the functional goals of the system. Acoustic and imaging data, being high-dimensional and temporally dynamic, are typically matched with learning-based models capable of complex pattern recognition. In contrast, environmental and structural data, which are scalar and trend-oriented, are more often analyzed using statistical or threshold-based techniques.</p></sec><sec id="sec3dot3dot4-sensors-25-05359"><title>3.3.4. Sensor&#8211;Communication Co-Occurrence Patterns</title><p>Understanding how different sensor modalities are implemented and transmitted is crucial for designing efficient and scalable smart beehive systems. Various sensor types require distinct communication strategies depending on factors such as data rate, energy consumption, and deployment context. <xref rid="sensors-25-05359-f017" ref-type="fig">Figure 17</xref> provides an overview of the co-occurrence patterns between sensor categories and communication technologies used in the reviewed literature. This visual summary helps illustrate which sensor types are commonly paired with wired, short-range, or long-range wireless communication, highlighting both standard practices and emerging trends in smart hive design.</p><p>Environmental/Weather Sensors, Hive Structural Sensors, and Acoustic/Vibration Sensors are most frequently paired with Short-Range Wireless and Long-Range Wireless communication technologies. This common pairing reflects their central role in smart beehive systems, where real-time or continuous data&#8212;such as temperature, humidity, hive weight, or sound&#8212;must be transmitted efficiently from remote or outdoor environments. The low data rate and power requirements of these sensors make them particularly suitable for low-energy wireless protocols. As a result, they are widely adopted in both experimental setups and field-deployable platforms, offering a balance between communication efficiency, scalability, and monitoring reliability.</p><p>Imaging Sensors are more selectively paired, predominantly with Short-Range Wireless communication. This likely reflects their high data bandwidth requirements, which are better handled with nearby base stations or local edge processing units. These sensors often appear in vision-based systems focused on tasks such as bee counting, foraging activity monitoring, or intrusion detection at the hive entrance.</p><p>Across all sensor types, Wired Communication is used infrequently, primarily in early-stage prototypes or laboratory settings where simplicity and data stability are prioritized over deployment flexibility. Most modern implementations prefer wireless connectivity, reinforcing the field&#8217;s emphasis on modularity, scalability, and field-readiness.</p><p>In summary, the communication strategy in smart beehive systems is closely aligned with sensor function, data rate requirements, and deployment context. Foundational sensors like environmental and structural ones appear across a wide range of systems and communication protocols, while high-bandwidth or specialized sensors show more constrained and deliberate communication pairings.</p></sec></sec><sec id="sec3dot4-sensors-25-05359"><title>3.4. Practical and Technical Limitations</title><p>A cross-study analysis of recent literature reveals a variety of practical and technical limitations that hinder the deployment, reliability, and scalability of smart beehive monitoring systems. These challenges arise across multiple layers&#8212;from data collection and algorithm design to hardware constraints and environmental conditions.</p><p>One major limitation is the availability and quality of data. Many studies report small dataset sizes and a lack of environmental diversity, making models vulnerable to overfitting and poor generalization. Edwards-Murphy et al. [<xref rid="B14-sensors-25-05359" ref-type="bibr">14</xref>] and Braga et al. [<xref rid="B79-sensors-25-05359" ref-type="bibr">79</xref>] highlight issues such as the absence of representative samples for different hive states and the geographic confinement of data collection. &#381;gank [<xref rid="B52-sensors-25-05359" ref-type="bibr">52</xref>] and Campell et al. [<xref rid="B124-sensors-25-05359" ref-type="bibr">124</xref>] emphasize that insufficient variability in training data can severely impact model performance&#8212;for instance, by causing convergence to trivial identity matrices in swarm detection methods based on matrix factorization. In addition, Gil-Lebrero et al. [<xref rid="B8-sensors-25-05359" ref-type="bibr">8</xref>] point out that the inherent biological variability in beehive activity introduces further inconsistency into datasets.</p><p>On the algorithmic side, many models rely on computationally intensive methods such as deep learning or spectral decomposition, which often exceed the capabilities of resource-constrained edge devices used in field settings. Kulyukin et al. [<xref rid="B44-sensors-25-05359" ref-type="bibr">44</xref>] note that deep models demand significant processing power, limiting their real-time deployment potential. Campell et al. [<xref rid="B124-sensors-25-05359" ref-type="bibr">124</xref>] raise additional concerns about convergence behavior in spectral methods, while Kulyukin et al. [<xref rid="B121-sensors-25-05359" ref-type="bibr">121</xref>] describe how sensor faults and environmental disruptions can create discontinuities in time-series data, degrading the reliability of forecasting models. Cecchi et al. [<xref rid="B27-sensors-25-05359" ref-type="bibr">27</xref>] also report performance limitations in vision-based systems due to segmentation errors.</p><p>Hardware, energy, and communication constraints present further obstacles. Solar-powered hives often fail to harvest sufficient energy for continuous monitoring, as observed by Edwards-Murphy et al. [<xref rid="B22-sensors-25-05359" ref-type="bibr">22</xref>]. Scalability is another concern&#8212;Kviesis et al. [<xref rid="B37-sensors-25-05359" ref-type="bibr">37</xref>] report that their system could securely support only ten IoT nodes. High costs and lack of flexibility in commercial platforms limit their adaptability in field conditions, as pointed out by Hamza et al. [<xref rid="B39-sensors-25-05359" ref-type="bibr">39</xref>]. Other authors [<xref rid="B28-sensors-25-05359" ref-type="bibr">28</xref>,<xref rid="B135-sensors-25-05359" ref-type="bibr">135</xref>] note that general-purpose computing platforms are often unsuitable due to their energy inefficiency and lack of durability. Multiple studies [<xref rid="B13-sensors-25-05359" ref-type="bibr">13</xref>,<xref rid="B22-sensors-25-05359" ref-type="bibr">22</xref>] independently report the inadequacy of solar energy harvesting, indicating this is a widespread challenge.</p><p>Environmental sensitivity adds another layer of complexity. Sensor placement within the hive can significantly affect measurement accuracy&#8212;Catania and Vallone [<xref rid="B29-sensors-25-05359" ref-type="bibr">29</xref>] demonstrate that temperature readings vary depending on probe location. Lighting conditions, occlusion, and hive structure all affect the reliability of visual data [<xref rid="B2-sensors-25-05359" ref-type="bibr">2</xref>,<xref rid="B27-sensors-25-05359" ref-type="bibr">27</xref>], illustrating how fragile sensor performance can be in uncontrolled environments.</p><p>Lastly, the maturity of many systems remains limited. Numerous solutions are still in early-stage or prototype phases. Kulyukin and Mukherjee [<xref rid="B44-sensors-25-05359" ref-type="bibr">44</xref>] provide only preliminary evaluations of their models, and Szczurek et al. [<xref rid="B76-sensors-25-05359" ref-type="bibr">76</xref>] explicitly call for further validation of gas-based detection techniques. The absence of long-term, multi-seasonal field testing makes it difficult to assess whether these systems can maintain reliability under natural variability and operational stress.</p><p>Taken together, these limitations reflect the growing pains of a research field still transitioning from proof-of-concept studies to practical, field-ready technologies. Addressing them will require robust datasets, computational efficiency, resilient hardware, and sustained validation efforts.</p></sec></sec><sec id="sec4-sensors-25-05359"><title>4. Publicly-Available Datasets for Smart-Beehive Research</title><p>Effective machine learning models for monitoring honey bee colonies rely on access to structured, labeled datasets that reflect the complexity of hive dynamics across multiple sensing modalities. Accordingly, to address our final research question (RQ5) on data resources, we survey the publicly available datasets that have been used to support smart beehive studies. Over the past several years, a number of high-quality public datasets have emerged, capturing audio signals, visual observations, and environmental telemetry relevant to colony health and behavior. These resources support tasks such as swarm prediction, parasite detection, behavior classification, and vitality forecasting. A summary of the most prominent datasets, categorized by modality and typical machine learning application, is provided in <xref rid="sensors-25-05359-t007" ref-type="table">Table 7</xref>.</p><sec id="sec4dot1-sensors-25-05359"><title>4.1. Acoustic Datasets</title><p>Acoustic monitoring offers a non-invasive method for assessing beehive conditions, providing valuable insights into colony behavior and health.</p><p>The &#8220;To bee or not to bee: An annotated dataset for beehive sound recognition&#8221; dataset, created by In&#234;s Nolasco and Emmanouil Benetos from Queen Mary University of London [<xref rid="B141-sensors-25-05359" ref-type="bibr">141</xref>], focuses on the automatic recognition of beehive sounds. This dataset is composed of 78 recordings, totaling approximately 12 h of audio, sourced from the Open Source Beehive (OSBH) project and the NU-Hive project. The audio segments are primarily labeled into two classes: &#8220;Bee&#8221; (pure beehive sounds) and &#8220;noBee&#8221; (periods where external sounds are perceived, superimposed on bee sounds). Annotation was performed by volunteers using Sonic Visualiser, leveraging both auditory perception and visual analysis of log-mel frequency spectrums. This dataset is explicitly designed for investigating machine learning approaches to beehive sound recognition and evaluating developed methods. A related dataset, &#8220;Audio-Based identification of Beehive states: The dataset&#8221; created by Ines Nolasco, Alessandro Terenzi, Stefania Cecchi, Simone Orcioni, Helen L. Bear, and Emmanouil Benetos [<xref rid="B142-sensors-25-05359" ref-type="bibr">142</xref>], also contains audio files and a state_labels.csv for the audio-based identification of beehive states. Another publicly available beehive audio dataset contains 10,000 audio files, each 8.203125 s long, sampled at 8000 Hz in WAV format, identified by date, time, and hive ID [<xref rid="B152-sensors-25-05359" ref-type="bibr">152</xref>]. Another relevant source is the &#8220;Smart bee colony monitor: Clips of beehive sounds&#8221; dataset published on Kaggle by Anna Jyang [<xref rid="B143-sensors-25-05359" ref-type="bibr">143</xref>]. It includes multiple recordings of beehive audio categorized by labels such as &#8220;healthy&#8221;, &#8220;distressed&#8221;, and &#8220;empty&#8221;. The dataset serves as a foundation for machine learning applications focused on recognizing bee colony states through sound analysis, and complements existing audio datasets in offering class-labeled audio in various beehive conditions.</p><p>The &#8220;Dataset for honey bee audio detection,&#8221; by Pawel Biernacki from the University of Science and Technology Wroclaw [<xref rid="B18-sensors-25-05359" ref-type="bibr">18</xref>], provides 10,000 one-second recordings of bees and 1700 one-second recordings of drones. All recordings are in WAV format without compression, sampled at 44.1 kHz. The specific labeling of &#8220;bees&#8221; and &#8220;drones&#8221; makes this dataset directly applicable for developing and evaluating ML models for audio detection and classification of different honey bee types.</p><p>The &#8220;Queenless honeybee acoustic patterns&#8221; dataset, contributed by Antonio Robles-Guerrero [<xref rid="B144-sensors-25-05359" ref-type="bibr">144</xref>], contains acoustic patterns from five Carniola honeybee colonies in Zacatecas, Mexico. The dataset includes recordings from healthy queenright colonies (with huge and moderate populations) and queenless colonies (with low populations), established by removing queens from two colonies. Each sample is 30 s long, recorded at a sampling frequency of 4 kHz with 12-bit resolution. The explicit hypothesis is that the queenless state can be identified by comparing acoustic patterns with healthy colonies using machine learning techniques and feature extraction methodologies.</p></sec><sec id="sec4dot2-sensors-25-05359"><title>4.2. Visual Datasets</title><p>Visual data provides direct observational insights into bee activity, health, and interactions within the hive environment.</p><p>The &#8220;Labeled dataset for bee detection and direction estimation on beehive landing boards,&#8221; contributed by Tomyslav Sledevic [<xref rid="B145-sensors-25-05359" ref-type="bibr">145</xref>], includes several visual datasets:<list list-type="bullet"><list-item><p>A detection dataset with 7200 frames (1920 &#215; 1080 resolution) for bee detection/ segmentation.</p></list-item><list-item><p>A segmentation dataset with 2300 cropped bee images labeled with a triangle shape for direction vector estimation.</p></list-item><list-item><p>A pose directory containing 400 frames from eight beehive entrances, where annotations include two points (head and stinger, or front and back if partially visible) for bee direction estimation.</p></list-item><list-item><p>A ramp detection dataset with 156 images, annotated with bounding box coordinates and four keypoints.</p></list-item><list-item><p>Tracking and behavior datasets consist of annotated MP4 files with bee tracks during foraging, defense, fanning, and washboarding activities within the entrance zone.</p></list-item></list></p><p>All annotations are in YOLO format, supporting the development of ML models for object detection, segmentation, pose estimation, tracking, and classification of specific bee behaviors.</p><p>The &#8220;Dataset for varroa mite detection on sticky boards,&#8221; created by Jose Divas&#243;n et al. [<xref rid="B146-sensors-25-05359" ref-type="bibr">146</xref>], provides 64 high-resolution images (8064 &#215; 6048) of sticky boards with <italic toggle="yes">Varroa mites</italic>, along with their labels. The dataset also includes a version of these images after deblurGAN techniques have been applied. This dataset is intended for use with deep learning techniques to analyze Varroa mite colony infestation levels and includes predefined training and validation splits, as well as developed deep learning models.</p><p>The &#8220;VarroaDataset&#8221; developed by Schurischuster Stefan and Martin Kampel [<xref rid="B17-sensors-25-05359" ref-type="bibr">17</xref>], offers high-resolution images (160 &#215; 280 px) of honeybees, specifically focusing on the presence of the <italic toggle="yes">Varroa destructor</italic> parasite. The dataset contains 13,509 samples, with approximately 3947 manually annotated as infected (class 1) and 9562 as healthy (class 0). It includes predefined dataset splits for training, testing, and validation. Bounding box coordinates are provided for the annotations. This dataset is explicitly designed for detecting parasites on honeybees using machine learning, particularly for image classification and object detection tasks.</p><p>The &#8220;VnPollenBee Dataset&#8221; is specifically built for detecting pollen-bearing bees from videos captured at hive entrances [<xref rid="B147-sensors-25-05359" ref-type="bibr">147</xref>]. It comprises over 2000 images, manually annotated, containing 1758 pollen-bearing bees and 59,068 non-pollen-bearing bees. The images were extracted from 1920 &#215; 1080 resolution videos recorded at 60 frames per second under varying natural light conditions. Annotations were initially manual using Labelme Annotation tools and refined with an object detection model. The dataset is pre-divided into training, validation, and test sets (70:20:10 ratio) to facilitate comparative studies of deep learning models for pollen bee detection. The &#8220;The BeeImage dataset: Annotated honey bee images&#8221; dataset on Kaggle, created by Jenny Yang [<xref rid="B148-sensors-25-05359" ref-type="bibr">148</xref>], provides a collection of annotated bee images aimed at object detection tasks. The dataset consists of over 1000 labeled images with bounding boxes around honey bees, intended to facilitate training and evaluation of deep learning models for detection and classification tasks. It is particularly useful for preliminary experimentation in object detection pipelines.</p></sec><sec id="sec4dot3-sensors-25-05359"><title>4.3. Environmental and Multimodal Datasets</title><p>These datasets combine various sensor measurements to provide a comprehensive understanding of beehive dynamics and their external influences.</p><p>A bee colony monitoring system, detailed in the study &#8220;Bee colony remote monitoring based on IoT using ESP-NOW protocol&#8221; collected real-time environmental data [<xref rid="B37-sensors-25-05359" ref-type="bibr">37</xref>]. The study makes available the &#8220;Measurements for the experimental period&#8221; dataset as supplemental information. This dataset includes battery discharge rates, temperature measurements (inside and outside the hive), and weight measurements of bee colonies. This data, collected from five colonies in Latvia from June to August 2022, was used to evaluate the efficiency of the IoT system and to analyze colony weight dynamics for active foraging periods, as well as in-hive temperature for colony state assessment. Such data is fundamental for developing ML models for predictive monitoring of colony health, activity levels, and resource availability.</p><p>The &#8220;Research project on field data collection for honey bee colony model evaluation&#8212;datasets&#8221; (also known as the MUSTB field data collection), created by Dupont Yoko L. et al. [<xref rid="B149-sensors-25-05359" ref-type="bibr">149</xref>], provides a comprehensive set of data for evaluating honey bee colony models [<xref rid="B149-sensors-25-05359" ref-type="bibr">149</xref>]. It includes various data modalities:<list list-type="bullet"><list-item><p>Environmental/Physiological data, such as hive weight obtained from automatic logging by a hive scale, and adult bee strength from weight assessment of combs.</p></list-item><list-item><p>Visual data, including data on brood development and food provision from image analysis of combs, and forager activity from automatic video recordings and image analysis by a bee counter.</p></list-item><list-item><p>Observational and management logs, detailing colony management actions (e.g., input/output of materials, queen loss, swarming, clinical signs) and observations of honey bee waggle dances (orientation and direction).</p></list-item><list-item><p>Chemical and biological analysis results, including laboratory analyses of pollen, pesticide residues, and parasites/pathogens.</p></list-item><list-item><p>Geographical information for sites and polygons, including UTM coordinates.</p></list-item></list></p><p>The data is reported according to a specific data model and stored in a relational database, providing a rich, multimodal resource for developing and evaluating diverse machine learning models related to bee colony health, behavior, and environmental interactions.</p><p>The &#8220;Winter carbon dioxide measurements in UK honeybee hives 2022/2023&#8221; dataset, contributed by Michael Newton [<xref rid="B150-sensors-25-05359" ref-type="bibr">150</xref>], specifically reports carbon dioxide measurements in wintering beehives in the UK. This data is also compared with hive scale and vibration sensor measurements. While not explicitly detailing ML model use within the source, CO<sub>2</sub> levels, hive mass, and vibration patterns are crucial environmental indicators that can be used to train and evaluate ML models for assessing colony vitality, wintering success, and overall health without manual inspection. This data supports &#8220;Giving Beekeeping Guidance by computational-assisted decision making,&#8221; implying its relevance for ML-driven decision support systems.</p><p>Although not directly a beehive-specific dataset, NASA POWER (Prediction Of Worldwide Energy Resources) provides publicly accessible solar and meteorological datasets [<xref rid="B151-sensors-25-05359" ref-type="bibr">151</xref>]. The &#8220;Agroclimatology Archive&#8221; specifically targets agricultural needs and provides parameters formatted for input to crop models. While it does not contain bee-specific labels, this external environmental data, such as temperature, solar radiation, and other meteorological parameters, is highly relevant for smart-beehive research. It can be integrated into ML models to contextualize bee behavior, foraging patterns, and colony health responses to broader environmental conditions.</p></sec><sec id="sec4dot4-sensors-25-05359"><title>4.4. Summary</title><p>In summary, a growing number of publicly available datasets are instrumental in advancing smart-beehive research. These datasets offer diverse data modalities, including acoustic signals for sound recognition and queen state detection, visual imagery for bee and parasite detection, and a range of environmental and physiological measurements for comprehensive colony monitoring. The detailed labeling and structured organization of these datasets directly support the development, training, and evaluation of various machine learning models for tasks such as classification, object detection, pose estimation, tracking, and behavioral analysis, ultimately contributing to more effective precision apiculture.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05359"><title>5. Discussion and Future Work</title><p>The findings from this review illustrate the remarkable progress made in integrating sensing, communication, and AI technologies into beekeeping. However, a closer analysis reveals a number of systematic limitations that, if addressed, could lead to significantly more robust, scalable, and intelligent smart hive systems.</p><sec id="sec5dot1-sensors-25-05359"><title>5.1. Sensor Modalities and Deployment Gaps</title><p>Environmental sensors&#8212;particularly those for temperature, humidity, and hive weight&#8212;remain the most commonly deployed modalities due to their affordability and ease of integration [<xref rid="B2-sensors-25-05359" ref-type="bibr">2</xref>,<xref rid="B4-sensors-25-05359" ref-type="bibr">4</xref>,<xref rid="B8-sensors-25-05359" ref-type="bibr">8</xref>]. Acoustic sensors rank second and are widely used for non-invasive detection of hive events such as queen loss or swarming [<xref rid="B1-sensors-25-05359" ref-type="bibr">1</xref>,<xref rid="B144-sensors-25-05359" ref-type="bibr">144</xref>,<xref rid="B152-sensors-25-05359" ref-type="bibr">152</xref>]. These audio-based methods have proven particularly effective for identifying critical changes in colony behavior.</p><p>However, other sensor types remain underutilized despite their potential value. Gas sensors such as CO<sub>2</sub> and NO<sub>2</sub> sensors can offer insight into hive respiration and ventilation patterns [<xref rid="B153-sensors-25-05359" ref-type="bibr">153</xref>], while infrared imaging and tag-based bee counters can provide information on thermal dynamics and foraging rates [<xref rid="B141-sensors-25-05359" ref-type="bibr">141</xref>,<xref rid="B148-sensors-25-05359" ref-type="bibr">148</xref>]. Despite their promise, few reviewed systems integrated these additional modalities, indicating a narrow focus in current experimental designs.</p><p>Moreover, most systems use only a single sensor modality, which limits resilience in noisy or uncertain environments. Multimodal sensor fusion&#8212;where acoustic, environmental, and even image-based signals are combined&#8212;remains rare in practical deployments despite its proven benefit in robustness and accuracy [<xref rid="B3-sensors-25-05359" ref-type="bibr">3</xref>,<xref rid="B145-sensors-25-05359" ref-type="bibr">145</xref>]. Furthermore, calibration protocols, sensor placement standards, and long-term durability studies are seldom reported, which impairs reproducibility.</p></sec><sec id="sec5dot2-sensors-25-05359"><title>5.2. Novel Opportunity: Signal-Layer Metrics as Passive Sensors</title><p>A promising research direction for smart beehive systems is the use of wireless communication signal metrics&#8212;such as Received Signal Strength Indicator (RSSI) and Signal-to-Noise Ratio (SNR)&#8212;as low-cost, passive sensing modalities. These metrics, inherent to radio communication technologies like LoRaWAN, can exhibit environmental sensitivity and offer insights without requiring dedicated physical sensors.</p><p>Recent studies have demonstrated that fluctuations in signal strength can be correlated with changes in environmental parameters, such as soil moisture [<xref rid="B154-sensors-25-05359" ref-type="bibr">154</xref>], occupancy and shadowing effects [<xref rid="B155-sensors-25-05359" ref-type="bibr">155</xref>], or spatial positioning [<xref rid="B156-sensors-25-05359" ref-type="bibr">156</xref>]. In these applications, RSSI and SNR patterns are interpreted using classical and machine learning techniques to infer states that would traditionally require more expensive and power-consuming sensors.</p><p>Applied to apiculture, similar principles could be exploited. For instance, changes in hive weight, bee clustering behavior, or humidity buildup may impact the wireless signal propagation between nodes. This opens the door to designing low-power, low-cost hives that leverage communication signals not just for data transmission, but also as sensing elements. Given that many smart beehive platforms already include long-range communication modules, signal-layer analysis could yield significant savings in hardware complexity and power consumption.</p><p>To our knowledge, no reviewed papers apply such differential signal-based sensing in beekeeping. However, the approach is promising due to its low power requirements, passive nature, and ability to integrate with existing LoRaWAN deployments. This form of &#8220;virtual sensing&#8221; could be especially valuable in constrained deployments and represents a novel research opportunity with wide applicability. We recommend future studies examine RSSI/SNR sensitivity to key beehive conditions, explore training ML models on such features for anomaly detection, and benchmark their accuracy against conventional sensors.</p></sec><sec id="sec5dot3-sensors-25-05359"><title>5.3. Data Processing and Machine Learning Approaches</title><p>The review also shows a clear evolution from rule-based alert systems to ML-powered classification and prediction. Traditional ML models like decision trees, support vector machines, and random forests are widely adopted for swarm prediction and audio classification tasks [<xref rid="B6-sensors-25-05359" ref-type="bibr">6</xref>,<xref rid="B15-sensors-25-05359" ref-type="bibr">15</xref>]. Deep learning models&#8212;including CNNs and LSTMs&#8212;are increasingly being used for vision and time-series inference [<xref rid="B14-sensors-25-05359" ref-type="bibr">14</xref>,<xref rid="B16-sensors-25-05359" ref-type="bibr">16</xref>,<xref rid="B148-sensors-25-05359" ref-type="bibr">148</xref>].</p><p>Nonetheless, several methodological shortcomings were identified. First, the majority of models are trained on small or private datasets, reducing reproducibility and generalizability [<xref rid="B142-sensors-25-05359" ref-type="bibr">142</xref>,<xref rid="B147-sensors-25-05359" ref-type="bibr">147</xref>]. Second, comparative model evaluation using standard metrics is rare, making it difficult to benchmark performance. Finally, and most notably, very few systems implement TinyML&#8212;machine learning designed to run on microcontrollers&#8212;for real-time inference on edge devices.</p><p>This lack of on-device inference is a missed opportunity, particularly for remote apiaries with limited connectivity. TinyML models can process acoustic signals, environmental data, and even signal-layer features like changes in signal strength locally, enabling real-time decisions without requiring constant uplink.</p></sec><sec id="sec5dot4-sensors-25-05359"><title>5.4. Deployment and Reproducibility Challenges</title><p>Despite their technical promise, many reviewed systems were only validated in laboratory settings or over short time intervals [<xref rid="B2-sensors-25-05359" ref-type="bibr">2</xref>,<xref rid="B4-sensors-25-05359" ref-type="bibr">4</xref>]. Very few reported long-term, in-situ deployments that accounted for seasonal or geographic variation [<xref rid="B149-sensors-25-05359" ref-type="bibr">149</xref>,<xref rid="B157-sensors-25-05359" ref-type="bibr">157</xref>]. As a result, many systems remain proof-of-concept rather than field-ready solutions. Concrete integration challenges reported in the literature include (i) load-cell weight scales requiring rigid, level platforms and weatherproof housings to maintain &#177;0.1 kg accuracy; humidity and temperature fluctuations can drift calibration over time and necessitate periodic recalibration; (ii) microcontroller and sensor enclosures suffering condensation and propolis buildup in harsh beehive environments, leading to sensor failure; (iii) limited power budgets for wireless modules&#8212;long-range radios such as LoRa require careful duty-cycling or solar power to avoid battery depletion; and (iv) network latency and packet loss when multiple hives share a gateway, complicating real-time anomaly detection. These examples highlight that system engineering constraints, not just algorithmic performance, often limit the robustness and scalability of smart hive prototypes.</p><p>Reproducibility is another critical issue. Fewer than 10% of the reviewed papers provide access to source code or raw datasets [<xref rid="B17-sensors-25-05359" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05359" ref-type="bibr">18</xref>]. Even when data is shared, it is often poorly labeled or lacks critical metadata, preventing meaningful reuse or comparison. The absence of benchmark datasets impedes progress and creates artificial barriers to entry for new researchers, as well as broader scientific insights into honey bee colony dynamics.</p></sec><sec id="sec5dot5-sensors-25-05359"><title>5.5. Future Research Directions</title><p>To advance the field, we recommend the following concrete actions:<list list-type="bullet"><list-item><p>Design and deploy multimodal sensing platforms that combine multiple sensor types, and communication-layer signals (RSSI, SNR) for holistic hive monitoring.</p></list-item><list-item><p>Explore fluctuations in signal strength using internal vs. external LoRaWAN nodes as a novel passive anomaly detection method.</p></list-item><list-item><p>Develop lightweight, interpretable TinyML models capable of real-time inference on embedded microcontrollers using features like sound patterns, temperature, and RSSI fluctuations.</p></list-item><list-item><p>Standardize data annotation, sharing, and benchmarking protocols through the creation of open-access, multi-season, multi-location datasets.</p></list-item><list-item><p>Investigate privacy-preserving distributed learning techniques such as federated learning to enable collaborative model training across apiaries.</p></list-item><list-item><p>Foster stronger collaboration with domain experts (experienced beekeepers and entomologists) to ensure smart beehive solutions address practical beekeeping needs and scientific knowledge gaps. This includes emphasizing user-friendly designs, cost-effectiveness, and validation of technologies in real-world apiary conditions.</p></list-item></list></p><p>By addressing these gaps, the community can transition from fragmented, lab-scale studies to robust, reproducible, and scalable smart hive systems capable of supporting both commercial and ecological beekeeping practices, as well as advancing scientific understanding of honey bee colonies.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05359"><title>6. Conclusions</title><p>This review systematically analyzed 135 peer-reviewed papers on smart beehive systems, identifying major technological trends, challenges, and research opportunities in the domain of precision apiculture. Environmental and acoustic sensors were found to be the most frequently used, while visual and gas sensing remain underexplored. Communication architectures favor short-range wireless protocols, though long-range low-power options like LoRa and NB-IoT are increasingly adopted. Methodologically, a transition is underway from rule-based systems to machine learning, though deep learning remains limited by data availability and deployment complexity.</p><p>The study reveals key gaps in sensor fusion, data transparency, and longitudinal validation. Addressing these will be crucial for the development of robust, scalable, and reproducible smart hive platforms. Future systems must emphasize multimodal sensing, edge intelligence, and open science principles. By consolidating existing work and outlining clear directions for research, this paper contributes a foundational synthesis for scientists, engineers, and beekeepers seeking to harness technology for sustainable apiculture.</p></sec></body><back><ack><title>Acknowledgments</title><p>During the preparation of this manuscript, the author(s) used ChatGPT 4o for the purposes of generating graphs from tables that are originally author materials, as well as to modify and improve the English language. The authors have reviewed and edited the output and take full responsibility for the content of this publication.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.&#352;. and T.P.; methodology, J.&#352;. and T.P.; software, J.&#352;.; validation, T.P., J.&#352;., P.&#352;., and L.&#352;.; formal analysis, J.&#352;. and T.P.; investigation, J.&#352;., T.P., P.&#352;., and L.&#352;.; resources T.P. and P.&#352;.; data curation, T.P. and J.&#352;.; writing&#8212;original draft preparation, T.P. and J.&#352;.; visualization, T.P. and J.&#352;.; supervision, L.&#352;. and P.&#352;.; project administration, P.&#352;. and L.&#352;.; funding acquisition, T.P. and P.&#352;. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Not applicable.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><app-group><app id="app1-sensors-25-05359"><title>Appendix A. Detailed Summary of Included Studies</title><p>This appendix provides the full table summarizing the 135 included studies. The table lists each publication along with its deployed sensor modalities, communication technologies, and analytical methods. In the main text we present aggregated analyses and condensed tables for readability.</p><table-wrap position="anchor" id="sensors-25-05359-t0A1" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t0A1_Table A1</object-id><label>Table A1</label><caption><p>Summary of included studies (N = 135), categorized by sensor modality, communication protocol, and analytical technique.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" style="background:#FFFFFF;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
PUBLICATION</th><th align="left" valign="top" style="background:#FFFFFF;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensor/Data Type</th><th align="left" valign="top" style="background:#FFFFFF;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Communication Type</th><th align="left" valign="top" style="background:#FFFFFF;border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method/Algorithm</th></tr></thead><tbody><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Henry et al.&#160;[<xref rid="B1-sensors-25-05359" ref-type="bibr">1</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi, Ethernet</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Ochoa et&#160;al.&#160;[<xref rid="B3-sensors-25-05359" ref-type="bibr">3</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Khairul et&#160;al.&#160;[<xref rid="B4-sensors-25-05359" ref-type="bibr">4</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Zabasta et&#160;al.&#160;[<xref rid="B5-sensors-25-05359" ref-type="bibr">5</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi, GSM/GPRS, RF</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Komasilovs et al.&#160;[<xref rid="B6-sensors-25-05359" ref-type="bibr">6</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Weight scale, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi, GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Fast Fourier Transform (FFT), Data aggregation techniques (AVG,&#160;COUNT)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Zacepins et&#160;al.&#160;[<xref rid="B2-sensors-25-05359" ref-type="bibr">2</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Custom swarming algorithm</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
S&#225;nchez et&#160;al.&#160;[<xref rid="B7-sensors-25-05359" ref-type="bibr">7</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Li et&#160;al.&#160;[<xref rid="B9-sensors-25-05359" ref-type="bibr">9</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">ANOVA</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Kale et&#160;al.&#160;[<xref rid="B10-sensors-25-05359" ref-type="bibr">10</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Gaussian Mixture Models (GMM), Cascade classification, Optical&#160;flow</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Gil-Lebrero et&#160;al.&#160;[<xref rid="B8-sensors-25-05359" ref-type="bibr">8</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">ZigBee</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Kviesis et&#160;al.&#160;[<xref rid="B11-sensors-25-05359" ref-type="bibr">11</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Neural network</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Rybin et&#160;al.&#160;[<xref rid="B12-sensors-25-05359" ref-type="bibr">12</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">RF</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Wavelet transformation, Neural network</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Edwards-Murphy et&#160;al.&#160;[<xref rid="B13-sensors-25-05359" ref-type="bibr">13</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, CO<sub>2</sub>, O<sub>2</sub>, NO<sub>2</sub>, Pollutant levels, Accelerometer</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS, ZigBee</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Custom temperature and humidity algorithm and CO<sub>2</sub></td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Edwards-Murphy et&#160;al.&#160;[<xref rid="B14-sensors-25-05359" ref-type="bibr">14</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, CO<sub>2</sub>, O<sub>2</sub>, NO<sub>2</sub>, Pollutant levels, Accelerometer</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">GSM/GPRS, ZigBee</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Decision Trees (C4.5), Custom temperature and humidity algorithm and CO<sub>2</sub></td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Kridi et&#160;al.&#160;[<xref rid="B15-sensors-25-05359" ref-type="bibr">15</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">ZigBee</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">k-means clustering</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Edwards-Murphy et&#160;al.&#160;[<xref rid="B16-sensors-25-05359" ref-type="bibr">16</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone, Accelerometer, Infrared camera, Thermal camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">GSM/GPRS, ZigBee</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Edwards-Murphy et&#160;al.&#160;[<xref rid="B22-sensors-25-05359" ref-type="bibr">22</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, CO<sub>2</sub>, O<sub>2</sub>, NO<sub>2</sub>, Pollutant levels, Accelerometer</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS, ZigBee</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
&#381;gank&#160;[<xref rid="B43-sensors-25-05359" ref-type="bibr">43</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi, GSM/GPRS</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Hidden Markov Models (HMM), Mel-Frequency Cepstral Coefficients (MFCC)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Marstaller et al.&#160;[<xref rid="B75-sensors-25-05359" ref-type="bibr">75</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Neural network</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kulyukin, et&#160;al.&#160;[<xref rid="B45-sensors-25-05359" ref-type="bibr">45</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">k-means clustering, Non-Uniform Fast Fourier Transform(NFFT), Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), Neural network, Support vector machine (SVM), Logistic Regression, Random Forest</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Kulyukin, et&#160;al.&#160;[<xref rid="B44-sensors-25-05359" ref-type="bibr">44</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Microphone, Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Neural network, Support vector machine (SVM), Random Forest</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Liu, et al.&#160;[<xref rid="B23-sensors-25-05359" ref-type="bibr">23</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Solar radiation, Wind speed and direction, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Tu, et&#160;al.&#160;[<xref rid="B46-sensors-25-05359" ref-type="bibr">46</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">k-means clustering, Linear regression</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Szczurek, et&#160;al.&#160;[<xref rid="B76-sensors-25-05359" ref-type="bibr">76</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Gas sensor</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">ANOVA, Tukey&#8217;s test</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
STRUYE, et&#160;al.&#160;[<xref rid="B47-sensors-25-05359" ref-type="bibr">47</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Counter</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">asynchronous sequential algorithm</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Ramsey, et&#160;al.&#160;[<xref rid="B48-sensors-25-05359" ref-type="bibr">48</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Accelerometer</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Andrijevi&#263; et al.&#160;[<xref rid="B109-sensors-25-05359" ref-type="bibr">109</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Gas sensors (TGS serise from Figaro Eng), Solar radiation, UV index, IR inensity, Rain detection, Wind speed and direction, Humidity, Microphone, Air Quality, Counter</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">LSTM neural networks, Facebook Prophet, ARIMA</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Voudiotis et&#160;al.&#160;[<xref rid="B110-sensors-25-05359" ref-type="bibr">110</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">LoRaWAN, WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">CNN</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Mrozek et&#160;al.&#160;[<xref rid="B77-sensors-25-05359" ref-type="bibr">77</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">CNN</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Aydin et&#160;al.&#160;[<xref rid="B24-sensors-25-05359" ref-type="bibr">24</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Air Pressure, Gas sensors (TGS serise from Figaro Eng), Humidity, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi, ZigBee</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Robustillo et&#160;al.&#160;[<xref rid="B111-sensors-25-05359" ref-type="bibr">111</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Air Pressure, Solar radiation, Rain detection, Wind speed and direction, Humidity, Pollutant&#160;levels</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Vector Autoregressive (VAR), Dynamic Linear Model (DLM), Generalized Additive Model (GAM)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Hong et&#160;al.&#160;[<xref rid="B25-sensors-25-05359" ref-type="bibr">25</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Microphone, Counter</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Kviesis et&#160;al.&#160;[<xref rid="B78-sensors-25-05359" ref-type="bibr">78</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">fuzzy logic model (FLM)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Braga et&#160;al.&#160;[<xref rid="B79-sensors-25-05359" ref-type="bibr">79</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Dew point, Solar radiation, Rain detection, Wind speed and direction, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Bluetooth</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Neural network, Random Forest, k-nearest neighbors (KNN)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Li et&#160;al.&#160;[<xref rid="B80-sensors-25-05359" ref-type="bibr">80</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Microphone, Counter</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi, GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Data correlation</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Imoize et&#160;al.&#160;[<xref rid="B26-sensors-25-05359" ref-type="bibr">26</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi, GSM/GPRS</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Signal patterns</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Cecchi et&#160;al.&#160;[<xref rid="B27-sensors-25-05359" ref-type="bibr">27</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, CO2, Weight scale, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi, Ethernet</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Signal patterns</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kaplan et&#160;al.&#160;[<xref rid="B81-sensors-25-05359" ref-type="bibr">81</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">VGG19, GoogLeNet</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Zacepins et&#160;al.&#160;[<xref rid="B28-sensors-25-05359" ref-type="bibr">28</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Bermig et&#160;al.&#160;[<xref rid="B49-sensors-25-05359" ref-type="bibr">49</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Camera, Counter</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Manual video inspection and Robber&#8217;s test</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Braga et&#160;al.&#160;[<xref rid="B112-sensors-25-05359" ref-type="bibr">112</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">k-means clustering, Random Forest, k-nearest neighbors (KNN)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Alves et&#160;al.&#160;[<xref rid="B82-sensors-25-05359" ref-type="bibr">82</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">CNNs (MobileNet, DenseNet, Inception, ResNet, etc.), U-Net for segmentation, CHT for detection, Naive Bayes (NB)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Ngo et&#160;al.&#160;[<xref rid="B50-sensors-25-05359" ref-type="bibr">50</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Light illuminance, Rain detection, Wind speed and direction, Humidity, Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Yolov3-tiny, Majority voting, Object tracking</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Sevin et&#160;al.&#160;[<xref rid="B83-sensors-25-05359" ref-type="bibr">83</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Shape and color-based image filtering (bee and mite templates), 3-stage detection process</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Kim et&#160;al.&#160;[<xref rid="B84-sensors-25-05359" ref-type="bibr">84</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Mel-Frequency Cepstral Coefficients (MFCC), Support vector machine (SVM), Random Forest, XGBoost (gradient boosting), VGG19, Shallow CNN, Grad- CAM, CQT (Constant Q transform)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Catania et&#160;al.&#160;[<xref rid="B29-sensors-25-05359" ref-type="bibr">29</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Wind speed and direction, Humidity, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Bluetooth</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">statistical correlation + environmental trend analysis</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Braga et&#160;al.&#160;[<xref rid="B113-sensors-25-05359" ref-type="bibr">113</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">LSTM neural networks, AdamX optimizer</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Schurischuster et&#160;al.&#160;[<xref rid="B85-sensors-25-05359" ref-type="bibr">85</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">AlexNet, ResNet, Deeplabv3 (semantic segmentation)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Williams, et&#160;al.&#160;[<xref rid="B51-sensors-25-05359" ref-type="bibr">51</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera, Thermal camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Gaussian Mixture Models (GMM), Neural network, Support vector machine (SVM), Random Forest, k-nearest neighbors (KNN)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
&#381;gank&#160;[<xref rid="B52-sensors-25-05359" ref-type="bibr">52</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi, GSM/GPRS</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Hidden Markov Models (HMM), Gaussian Mixture Models (GMM), Linear Predictive Coding (LPC), Mel-Frequency Cepstral Coefficients (MFCC)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Rodias et&#160;al.&#160;[<xref rid="B125-sensors-25-05359" ref-type="bibr">125</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, GPS module, LIDAR</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">BFCI formula: <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (weather scoring);</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Libal et&#160;al.&#160;[<xref rid="B53-sensors-25-05359" ref-type="bibr">53</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Mel-Frequency Cepstral Coefficients (MFCC), Support vector machine (SVM), Linear Discriminant Analysis (LDA), Random Forest, k-nearest neighbors (KNN)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Chien et&#160;al.&#160;[<xref rid="B136-sensors-25-05359" ref-type="bibr">136</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv7</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Penaloza-Aponte et&#160;al.&#160;[<xref rid="B54-sensors-25-05359" ref-type="bibr">54</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Tags, Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Degenfellner et&#160;al.&#160;[<xref rid="B86-sensors-25-05359" ref-type="bibr">86</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Weight scale, Enviromental data</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Facebook Prophet, Similar Trend Monitoring (STM), Similar Trend Monitoring (STM).1, Principal Component Analysis (PCA), MM-Regression</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kongsilp et&#160;al.&#160;[<xref rid="B55-sensors-25-05359" ref-type="bibr">55</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Kalman filter, Hungarian algorithm, Mask R-CNN</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Divas&#243;n et&#160;al.&#160;[<xref rid="B87-sensors-25-05359" ref-type="bibr">87</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Faster R-CNN with ResNet18/50/152 + FPN backbones, DeblurGAN</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Chowdhury et&#160;al.&#160;[<xref rid="B56-sensors-25-05359" ref-type="bibr">56</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">YOLOv8</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Libal et&#160;al.&#160;[<xref rid="B114-sensors-25-05359" ref-type="bibr">114</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Mel-Frequency Cepstral Coefficients (MFCC), LASSO regression, Autoencoder neural networks</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Bairo et&#160;al.&#160;[<xref rid="B30-sensors-25-05359" ref-type="bibr">30</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Custom calibration model using linear regression on resistance-voltage-weight relationship</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Camayo et&#160;al.&#160;[<xref rid="B88-sensors-25-05359" ref-type="bibr">88</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, CO2, TVOC</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Neural network, Random Forest, Decision Trees (C4.5), Weighted multi-criteria aggregation algorithm, Data aggregation techniques (AVG, COUNT), XGBoost (gradient boosting)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Liyanage et&#160;al.&#160;[<xref rid="B126-sensors-25-05359" ref-type="bibr">126</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Rain detection, Humidity, Air Quality</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Narcia-Macias et&#160;al.&#160;[<xref rid="B89-sensors-25-05359" ref-type="bibr">89</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv7</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Minaud et&#160;al.&#160;[<xref rid="B115-sensors-25-05359" ref-type="bibr">115</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Generalized Additive Model (GAM), event detection via thresholds and time-interval-based rules, RP_median thermal index, GLM validations</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Garcao et&#160;al.&#160;[<xref rid="B90-sensors-25-05359" ref-type="bibr">90</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">CNN, Logistic Regression, k-nearest neighbors (KNN), Principal Component Analysis (PCA), YAMNET, VGGish, Feedforward neural network (FNN), Kendall&#8217;s tau</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
P&#233;rez-Delgado et&#160;al.&#160;[<xref rid="B140-sensors-25-05359" ref-type="bibr">140</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">CNN</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Kamga et&#160;al.&#160;[<xref rid="B116-sensors-25-05359" ref-type="bibr">116</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Enviromental data, Local land cover quality index (LLCQI),</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">ANFIS-SC (Adaptive Neuro-Fuzzy Inference System + Subtractive Clustering)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kontogiannis et&#160;al.&#160;[<xref rid="B91-sensors-25-05359" ref-type="bibr">91</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">CNNs (VGG-16/19, ResNet-18/50, WideResNet, Inception), Fuzzy-stranded-NN</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Smerkol et&#160;al.&#160;[<xref rid="B127-sensors-25-05359" ref-type="bibr">127</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Air Pressure, Rain detection, Humidity, Weight scale</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">NBIoT</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Support vector machine (SVM), Random Forest, Decision Trees (C4.5), ADABOOST, Gradient Boost</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Lei et al.&#160;[<xref rid="B57-sensors-25-05359" ref-type="bibr">57</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">YOLOv8m, OC-SORT, BOX-METHOD</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Nguyen et&#160;al.&#160;[<xref rid="B58-sensors-25-05359" ref-type="bibr">58</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">CNN, YOLOv5, Faster RCNN, Focal Loss, Overlap Sampler</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Robles-Guerrero et&#160;al.&#160;[<xref rid="B92-sensors-25-05359" ref-type="bibr">92</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">CNNs: EfficientNet, ConvNeXt, MobileNet, ShuffleNet, ResNet18,&#160;etc.</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Hall et&#160;al.&#160;[<xref rid="B137-sensors-25-05359" ref-type="bibr">137</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone, Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Principal Component Analysis (PCA), Discriminant Function Analysis (DFA), 2D Fourier Transform (2DFT), classification via DF-space projection</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Bono et&#160;al.&#160;[<xref rid="B117-sensors-25-05359" ref-type="bibr">117</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, UV index, Rain detection, Wind speed and direction, Humidity, Weight scale, Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Vector Autoregressive (VAR), impulse response functions (IRF), Granger causality tests</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Ramirez-Diaz et&#160;al.&#160;[<xref rid="B118-sensors-25-05359" ref-type="bibr">118</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Enviromental data</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Random Forest, Decision Trees (C4.5), XGBoost (gradient boosting), Boruta FS</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Ho et&#160;al.&#160;[<xref rid="B59-sensors-25-05359" ref-type="bibr">59</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Fast Fourier Transform (FFT), Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), Support vector machine (SVM), Logistic Regression, Random Forest, Extra Trees (ET), k-nearest neighbors (KNN), CQT (Constant Q transform), Spectral Contrast</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
V&#225;rkonyi et&#160;al.&#160;[<xref rid="B119-sensors-25-05359" ref-type="bibr">119</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), Spectral Centroid, Zero Crossing Rate, Histogram-based Gradient Boosting + GA-based feature selection (regression)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Karan et&#160;al.&#160;[<xref rid="B31-sensors-25-05359" ref-type="bibr">31</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Light illuminance, Humidity, Weight scale, Microphone, Accelerometer</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Lee et&#160;al.&#160;[<xref rid="B133-sensors-25-05359" ref-type="bibr">133</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, CO<sub>2</sub>, O<sub>2</sub>, Weight scale, Counter</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">IR, power line communication (PLC)</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Robustillo et&#160;al.&#160;[<xref rid="B120-sensors-25-05359" ref-type="bibr">120</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Air Pressure, Light illuminance, Rain detection, Wind speed and direction, Humidity, particulate matter, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Vector Autoregressive (VAR), Dynamic Factor Analysis (DFA), ombining data from
multiple time series (CMTS), eneral multivariate auto-regressive state-space (MARSG), Vector Error Correction (VEC)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Sledevi&#269; et&#160;al.&#160;[<xref rid="B64-sensors-25-05359" ref-type="bibr">64</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv8-pose (nano, medium, large)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Otesbelgue et&#160;al.&#160;[<xref rid="B93-sensors-25-05359" ref-type="bibr">93</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Support vector machine (SVM), Random Forest, k-nearest neighbors (KNN), multilayer perceptron (MLP), extreme learning machine&#160;(ELM)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Libal et&#160;al.&#160;[<xref rid="B61-sensors-25-05359" ref-type="bibr">61</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Mel-Frequency Cepstral Coefficients (MFCC), gammatone cepstral coefficients (GTCC), BURG algorithm, MUltiple SIgnal Classification (MUSIC), Autoencoder, thresholding (T1, T2, T3, T*), empirical Bayes classifier (ML thresholding)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kulyukin et&#160;al.&#160;[<xref rid="B121-sensors-25-05359" ref-type="bibr">121</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Weight scale, Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">ANN, CNN, LSTM neural networks, ARIMA</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Micheli et&#160;al.&#160;[<xref rid="B62-sensors-25-05359" ref-type="bibr">62</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Gaussian derivative (GDER), Gray-level local variance (GLLV), Steerable filters (SFIL), Tenengrad (TENG), and Tenengrad variance (TENV), t-distributed Stochastic Neighbor Embedding (t-SNE)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Dickson et&#160;al.&#160;[<xref rid="B63-sensors-25-05359" ref-type="bibr">63</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Kalman filter, YOLOv8, Optical Flow + polynomial regression</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Gaikwad et&#160;al.&#160;[<xref rid="B32-sensors-25-05359" ref-type="bibr">32</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Sledevi&#269; et&#160;al.&#160;[<xref rid="B60-sensors-25-05359" ref-type="bibr">60</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">YOLOv8m + YOLOv8n-seg for detection &amp; direction, rule-based behavior detection for 4 patterns (foraging, fanning, defense, washboarding), BoT-SORT, ByteTrack, StrongSORT, DeepOC-SORT, OC-SORT tracking algorithms</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Luz et&#160;al.&#160;[<xref rid="B94-sensors-25-05359" ref-type="bibr">94</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Mel-Frequency Cepstral Coefficients (MFCC), Support vector machine (SVM), Random Forest, multilayer perceptron (MLP), VGG16/ResNet50/MobileNet/YOLO, Mel spectrograms</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Iqbal et&#160;al.&#160;[<xref rid="B65-sensors-25-05359" ref-type="bibr">65</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Mel-Frequency Cepstral Coefficients (MFCC), CNN, LSTM neural networks, Support vector machine (SVM), k-nearest neighbors (KNN), Naive Bayes (NB), Mel spectrograms, transformer mode</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
De Simone et&#160;al.&#160;[<xref rid="B95-sensors-25-05359" ref-type="bibr">95</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">LoRaWAN</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), TinyML neural network (3-layer NN)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Newton et&#160;al.&#160;[<xref rid="B96-sensors-25-05359" ref-type="bibr">96</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, CO<sub>2</sub>, Weight scale, Vibration</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">analysis based on signal tracking, vibration spectrograms, and time-series trends</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Zheng et&#160;al.&#160;[<xref rid="B33-sensors-25-05359" ref-type="bibr">33</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Microphone, Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv5, DeepSORT, rule-based bee entry/exit/count logic</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Alifieris et&#160;al.&#160;[<xref rid="B134-sensors-25-05359" ref-type="bibr">134</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Air Pressure, Humidity, Weight scale, Enviromental data, Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">LoRaWAN, WiFi, GSM/GPRS</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">rule-based journaling, checklist mapping, data stream aggregation</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Janetzky et&#160;al.&#160;[<xref rid="B66-sensors-25-05359" ref-type="bibr">66</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Random Forest, Isolation Forrest, Principal Component Analysis (PCA), Autoencoder neural networks, Spectrograms</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Rathore et&#160;al.&#160;[<xref rid="B97-sensors-25-05359" ref-type="bibr">97</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">CLAHE (contrast enhancement), Bilateral filter, Hough Circle Transform</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Borgianni et&#160;al.&#160;[<xref rid="B98-sensors-25-05359" ref-type="bibr">98</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">DenseNet121, ResNet50, InceptionV3, VGG16; Federated Averaging (FedAvg), CNN-based DNNs (spectrogram input)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kulyukin et&#160;al.&#160;[<xref rid="B122-sensors-25-05359" ref-type="bibr">122</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Electromgnetic radiation (EMR), Air Pressure, Solar radiation, Rain detection, Wind speed and direction, Humidity, Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Support vector machine (SVM), Linear regression, Random Forest</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
V&#225;rkonyi et&#160;al.&#160;[<xref rid="B67-sensors-25-05359" ref-type="bibr">67</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), MFCC differential coefficients (MFCC delta), CNN, LSTM neural networks, Spectral Centroid, Zero Crossing Rate, DANi NF method, Chroma</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Williams et&#160;al.&#160;[<xref rid="B68-sensors-25-05359" ref-type="bibr">68</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera, Doppler radar counter</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Linear Predictive Coding (LPC), Support vector machine (SVM), DenseNet, Log Area Ratios (LAR)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Mahajan et&#160;al.&#160;[<xref rid="B99-sensors-25-05359" ref-type="bibr">99</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone, Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Mel-Frequency Cepstral Coefficients (MFCC), YOLOv7, YOLOv8, Mel spectrograms, Single Shot Multibox Detector (SSD), Detection Transformer (DETR), Dense NN (2-layer MLP on MFCC+Mel features)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Cota et&#160;al.&#160;[<xref rid="B34-sensors-25-05359" ref-type="bibr">34</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Lid microswitch, Humidity, Weight scale, Microphone, GPS module</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Vallone et&#160;al.&#160;[<xref rid="B132-sensors-25-05359" ref-type="bibr">132</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Rule-based trend evaluation for honey production and swarm behavior prediction</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Abdollahi et&#160;al.&#160;[<xref rid="B35-sensors-25-05359" ref-type="bibr">35</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Short-Time Energy, WebRTC VAD, CRDNN</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Vit et&#160;al.&#160;[<xref rid="B128-sensors-25-05359" ref-type="bibr">128</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">CNNs (VGG19, DenseNet121, EfficientNetV2S, ResNet50, InceptionV3)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kulyukin et&#160;al.&#160;[<xref rid="B69-sensors-25-05359" ref-type="bibr">69</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">YOLOv3, YOLOv4-tiny, YOLOv7-tiny, OmniBeeM</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Jeon et&#160;al.&#160;[<xref rid="B138-sensors-25-05359" ref-type="bibr">138</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv5s</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Wu et&#160;al.&#160;[<xref rid="B123-sensors-25-05359" ref-type="bibr">123</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Tags, Dew point, Air Pressure, Solar radiation, UV index, Rain detection, Wind speed and direction, Humidity</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">LSTM neural networks, gated recurrent unit (GRU)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Safie et&#160;al.&#160;[<xref rid="B70-sensors-25-05359" ref-type="bibr">70</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv3, SqueezeNet (18-layer CNN), DarkNet-53 (53-layer CNN)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Milovanovic et&#160;al.&#160;[<xref rid="B36-sensors-25-05359" ref-type="bibr">36</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">64 IR opto-reflective sensors</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">WiFi</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Phan et&#160;al.&#160;[<xref rid="B129-sensors-25-05359" ref-type="bibr">129</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Logistic Regression, Random Forest, Decision Trees (C4.5), Extra Trees (ET), XGBoost (gradient boosting), k-nearest neighbors (KNN)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Kviesis et&#160;al.&#160;[<xref rid="B37-sensors-25-05359" ref-type="bibr">37</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Weight scale</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules, CNN-based DNNs (spectrogram input)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Abdollahi et&#160;al.&#160;[<xref rid="B100-sensors-25-05359" ref-type="bibr">100</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">discrete wavelet transform (DWT), Mel-Frequency Cepstral Coefficients (MFCC), Spectrograms</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Campell et&#160;al.&#160;[<xref rid="B124-sensors-25-05359" ref-type="bibr">124</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Microphone, Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Short term Fourier transform (STFT), Non-Negative Matrix Factorization (NMF), Masked NMF, Minimum Covariance Determinant (MCD), ANN</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Grammalidis et&#160;al.&#160;[<xref rid="B130-sensors-25-05359" ref-type="bibr">130</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Camera, Microscope images, Satellite images</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">CNN, Mask R-CNN, U-TAE (Transformer + U-Net), YOLOv6</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Florea et&#160;al.&#160;[<xref rid="B131-sensors-25-05359" ref-type="bibr">131</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Air Pressure, Humidity, Microphone, ultrasonic distance</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Libal et&#160;al.&#160;[<xref rid="B71-sensors-25-05359" ref-type="bibr">71</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Fast Fourier Transform (FFT), BURG algorithm, Autoencoder neural networks, Blackman-Tukey</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Chen et&#160;al.&#160;[<xref rid="B38-sensors-25-05359" ref-type="bibr">38</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Counter</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">LoRaWAN</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Sledevi&#269; et&#160;al.&#160;[<xref rid="B72-sensors-25-05359" ref-type="bibr">72</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv8m</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Sharma et&#160;al.&#160;[<xref rid="B101-sensors-25-05359" ref-type="bibr">101</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">CLAHE (contrast enhancement), CNN (ResNet-50, Inception V3)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Sledevi&#269; et&#160;al.&#160;[<xref rid="B73-sensors-25-05359" ref-type="bibr">73</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">YOLOv8m</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Barbisan et&#160;al.&#160;[<xref rid="B102-sensors-25-05359" ref-type="bibr">102</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), Support vector machine (SVM), multilayer perceptron (MLP)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Durga et&#160;al.&#160;[<xref rid="B103-sensors-25-05359" ref-type="bibr">103</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Vision Transformer (ViT14, ViT16, ViT32)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
De Simone et&#160;al.&#160;[<xref rid="B104-sensors-25-05359" ref-type="bibr">104</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), 2-layer NN</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Hamza, et&#160;al.&#160;[<xref rid="B39-sensors-25-05359" ref-type="bibr">39</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Temperature, Humidity, Weight scale, Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">event detection via thresholds and time-interval-based rules</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Sanz, et&#160;al.&#160;[<xref rid="B40-sensors-25-05359" ref-type="bibr">40</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Temperature, Air Pressure, Humidity</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">LoRaWAN</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Statistical analysis (ANOVA + Fisher LSD)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Ruvinga, et&#160;al.&#160;[<xref rid="B105-sensors-25-05359" ref-type="bibr">105</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Short term Fourier transform (STFT), Mel-Frequency Cepstral Coefficients (MFCC), CNN, LSTM neural networks, Logistic Regression, multilayer perceptron (MLP)</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Thi, et&#160;al.&#160;[<xref rid="B74-sensors-25-05359" ref-type="bibr">74</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Genetic Programming (GP)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Lee, et&#160;al.&#160;[<xref rid="B106-sensors-25-05359" ref-type="bibr">106</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">ORB (Oriented FAST and Rotated BRIEF), Contrast-Limited Adaptive Histogram Equalization (CLAHE), RGB/HSV/Lab/Gray/YCrCb color models, Histogram Equalization</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Capela, et&#160;al.&#160;[<xref rid="B41-sensors-25-05359" ref-type="bibr">41</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Weight scale, Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">DeepBee# (custom-trained CNN)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Dokukin, et&#160;al.&#160;[<xref rid="B107-sensors-25-05359" ref-type="bibr">107</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Microphone</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Support vector machine (SVM), Logistic Regression, Random Forest, XGBoost (gradient boosting), Statistically Weighted Syndrome (SWS), OVP method</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Nasir et&#160;al.&#160;[<xref rid="B139-sensors-25-05359" ref-type="bibr">139</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera, Infrared camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Xception, GoogLeNet, Ensemble Bagged Trees, Multi-evidence fusion via weighted voting</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">
Milovanovi&#263; et&#160;al.&#160;[<xref rid="B42-sensors-25-05359" ref-type="bibr">42</xref>]</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">900 IR photo reflectors</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF" rowspan="1" colspan="1">Reflectivity-based classification (voltage thresholds), Absorption spectroscopy</td></tr><tr><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">
Divas&#243;n et&#160;al.&#160;[<xref rid="B108-sensors-25-05359" ref-type="bibr">108</xref>]</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Camera</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">&#8211;</td><td align="left" valign="top" style="background:#FFFFFF" rowspan="1" colspan="1">Faster R-CNN + ResNet50-FPN backbone, Enhanced Deep Super-Resolution (EDSR), Stochasticgradientdescent(SGD)</td></tr><tr><td align="left" valign="top" style="background:#E6E6FF;border-bottom:solid thin" rowspan="1" colspan="1">
Braga et&#160;al.&#160;[<xref rid="B135-sensors-25-05359" ref-type="bibr">135</xref>]</td><td align="left" valign="top" style="background:#E6E6FF;border-bottom:solid thin" rowspan="1" colspan="1">Vibration, GPS module</td><td align="left" valign="top" style="background:#E6E6FF;border-bottom:solid thin" rowspan="1" colspan="1">GSM/GPRS</td><td align="left" valign="top" style="background:#E6E6FF;border-bottom:solid thin" rowspan="1" colspan="1">Rule-based detection: vibration triggers + GPS tracking + notification logic</td></tr></tbody></table></table-wrap></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-05359"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Henry</surname><given-names>E.</given-names></name><name name-style="western"><surname>Adamchuk</surname><given-names>V.</given-names></name><name name-style="western"><surname>Stanhope</surname><given-names>T.</given-names></name><name name-style="western"><surname>Buddle</surname><given-names>C.</given-names></name><name name-style="western"><surname>Rindlaub</surname><given-names>N.</given-names></name></person-group><article-title>Precision apiculture: Development of a wireless sensor network for honeybee hives</article-title><source>Comput. Electron. Agric.</source><year>2019</year><volume>156</volume><fpage>138</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2018.11.001</pub-id></element-citation></ref><ref id="B2-sensors-25-05359"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zacepins</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kviesis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Stalidzans</surname><given-names>E.</given-names></name><name name-style="western"><surname>Liepniece</surname><given-names>M.</given-names></name><name name-style="western"><surname>Meitalovs</surname><given-names>J.</given-names></name></person-group><article-title>Remote detection of the swarming of honey bee colonies by single-point temperature monitoring</article-title><source>Biosyst. Eng.</source><year>2016</year><volume>148</volume><fpage>76</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2016.05.012</pub-id></element-citation></ref><ref id="B3-sensors-25-05359"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ochoa</surname><given-names>I.Z.</given-names></name><name name-style="western"><surname>Gutierrez</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rodriguez</surname><given-names>F.</given-names></name></person-group><article-title>Internet of things: Low cost monitoring BeeHive system using wireless sensor network</article-title><source>Proceedings of the 2019 IEEE International Conference on Engineering Veracruz (ICEV)</source><conf-loc>Boca del Rio, Mexico</conf-loc><conf-date>14&#8211;17 October 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2019</year></element-citation></ref><ref id="B4-sensors-25-05359"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Khairul Anuar</surname><given-names>N.H.</given-names></name><name name-style="western"><surname>Amri Md Yunus</surname><given-names>M.</given-names></name><name name-style="western"><surname>Baharuddin</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Sahlan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Abid</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ramli</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Razzi Abu Amin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mohd Lotpi</surname><given-names>Z.F.</given-names></name></person-group><article-title>IoT platform for precision stingless bee farming</article-title><source>Proceedings of the 2019 IEEE International Conference on Automatic Control and Intelligent Systems (I2CACIS)</source><conf-loc>Selangor, Malaysia</conf-loc><conf-date>29 June 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2019</year></element-citation></ref><ref id="B5-sensors-25-05359"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zabasta</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kunicina</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kondratjevs</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ribickis</surname><given-names>L.</given-names></name></person-group><article-title>IoT approach application for development of autonomous beekeeping system</article-title><source>Proceedings of the 2019 International Conference in Engineering Applications (ICEA)</source><conf-loc>Sao Miguel, Portugal</conf-loc><conf-date>8&#8211;11 July 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2019</year></element-citation></ref><ref id="B6-sensors-25-05359"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Komasilovs</surname><given-names>V.</given-names></name><name name-style="western"><surname>Zacepins</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kviesis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fiedler</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kirchner</surname><given-names>S.</given-names></name></person-group><article-title>Modular sensory hardware and data processing solution for implementation of the precision beekeeping</article-title><source>Agron. Res.</source><year>2019</year><volume>17</volume><fpage>509</fpage><lpage>517</lpage></element-citation></ref><ref id="B7-sensors-25-05359"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>S&#225;nchez</surname><given-names>V.</given-names></name><name name-style="western"><surname>Gil</surname><given-names>S.</given-names></name><name name-style="western"><surname>Flores</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Quiles</surname><given-names>F.J.</given-names></name><name name-style="western"><surname>Ortiz</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Luna</surname><given-names>J.J.</given-names></name></person-group><article-title>Implementation of an electronic system to monitor the thermoregulatory capacity of honeybee colonies in hives with open-screened bottom boards</article-title><source>Comput. Electron. Agric.</source><year>2015</year><volume>119</volume><fpage>209</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2015.10.018</pub-id></element-citation></ref><ref id="B8-sensors-25-05359"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gil-Lebrero</surname><given-names>S.</given-names></name><name name-style="western"><surname>Quiles-Latorre</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ortiz-L&#243;pez</surname><given-names>M.</given-names></name><name name-style="western"><surname>S&#225;nchez-Ruiz</surname><given-names>V.</given-names></name><name name-style="western"><surname>G&#225;miz-L&#243;pez</surname><given-names>V.</given-names></name><name name-style="western"><surname>Luna-Rodr&#237;guez</surname><given-names>J.</given-names></name></person-group><article-title>Honey bee colonies remote monitoring system</article-title><source>Sensors</source><year>2016</year><volume>17</volume><elocation-id>55</elocation-id><pub-id pub-id-type="doi">10.3390/s17010055</pub-id><pub-id pub-id-type="pmid">28036061</pub-id><pub-id pub-id-type="pmcid">PMC5298628</pub-id></element-citation></ref><ref id="B9-sensors-25-05359"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.Y.</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>D.B.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>B.</given-names></name></person-group><article-title>Drone and worker brood microclimates are regulated differentially in honey bees, <italic toggle="yes">Apis mellifera</italic></article-title><source>PLoS ONE</source><year>2016</year><volume>11</volume><elocation-id>e0148740</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0148740</pub-id><pub-id pub-id-type="pmid">26882104</pub-id><pub-id pub-id-type="pmcid">PMC4755576</pub-id></element-citation></ref><ref id="B10-sensors-25-05359"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kale</surname><given-names>D.J.</given-names></name><name name-style="western"><surname>Tashakkori</surname><given-names>R.</given-names></name><name name-style="western"><surname>Parry</surname><given-names>R.M.</given-names></name></person-group><article-title>Automated beehive surveillance using computer vision</article-title><source>Proceedings of the SoutheastCon 2015</source><conf-loc>Fort Lauderdale, FL, USA</conf-loc><conf-date>9&#8211;12 April 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2015</year></element-citation></ref><ref id="B11-sensors-25-05359"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kviesis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zacepins</surname><given-names>A.</given-names></name></person-group><article-title>Application of neural networks for honey bee colony state identification</article-title><source>Proceedings of the 2016 17th International Carpathian Control Conference (ICCC)</source><conf-loc>High Tatras, Slovakia</conf-loc><conf-date>29 May&#8211;1 June 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2016</year></element-citation></ref><ref id="B12-sensors-25-05359"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rybin</surname><given-names>V.G.</given-names></name><name name-style="western"><surname>Butusov</surname><given-names>D.N.</given-names></name><name name-style="western"><surname>Karimov</surname><given-names>T.I.</given-names></name><name name-style="western"><surname>Belkin</surname><given-names>D.A.</given-names></name><name name-style="western"><surname>Kozak</surname><given-names>M.N.</given-names></name></person-group><article-title>Embedded data acquisition system for beehive monitoring</article-title><source>Proceedings of the 2017 IEEE II International Conference on Control in Technical Systems (CTS)</source><conf-loc>St. Petersburg, Russia</conf-loc><conf-date>25&#8211;27 October 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2017</year></element-citation></ref><ref id="B13-sensors-25-05359"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Edwards Murphy</surname><given-names>F.</given-names></name><name name-style="western"><surname>Magno</surname><given-names>M.</given-names></name><name name-style="western"><surname>Whelan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Vici</surname><given-names>E.P.</given-names></name></person-group><article-title>b+WSN: Smart beehive for agriculture, environmental, and honey bee health monitoring&#8212;Preliminary results and analysis</article-title><source>Proceedings of the 2015 IEEE Sensors Applications Symposium (SAS)</source><conf-loc>Zadar, Croatia</conf-loc><conf-date>13&#8211;15 April 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2015</year></element-citation></ref><ref id="B14-sensors-25-05359"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Edwards-Murphy</surname><given-names>F.</given-names></name><name name-style="western"><surname>Magno</surname><given-names>M.</given-names></name><name name-style="western"><surname>Whelan</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>O&#8217;Halloran</surname><given-names>J.</given-names></name><name name-style="western"><surname>Popovici</surname><given-names>E.M.</given-names></name></person-group><article-title>b+WSN: Smart beehive with preliminary decision tree analysis for agriculture and honey bee health monitoring</article-title><source>Comput. Electron. Agric.</source><year>2016</year><volume>124</volume><fpage>211</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2016.04.008</pub-id></element-citation></ref><ref id="B15-sensors-25-05359"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kridi</surname><given-names>D.S.</given-names></name><name name-style="western"><surname>de Carvalho</surname><given-names>C.G.N.</given-names></name><name name-style="western"><surname>Gomes</surname><given-names>D.G.</given-names></name></person-group><article-title>Application of wireless sensor networks for beehive monitoring and in-hive thermal patterns detection</article-title><source>Comput. Electron. Agric.</source><year>2016</year><volume>127</volume><fpage>221</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2016.05.013</pub-id></element-citation></ref><ref id="B16-sensors-25-05359"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Murphy</surname><given-names>F.E.</given-names></name><name name-style="western"><surname>Magno</surname><given-names>M.</given-names></name><name name-style="western"><surname>O&#8217;Leary</surname><given-names>L.</given-names></name><name name-style="western"><surname>Troy</surname><given-names>K.</given-names></name><name name-style="western"><surname>Whelan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Popovici</surname><given-names>E.M.</given-names></name></person-group><article-title>Big brother for bees (3B)&#8212;Energy neutral platform for remote monitoring of beehive imagery and sound</article-title><source>Proceedings of the 2015 6th International Workshop on Advances in Sensors and Interfaces (IWASI)</source><conf-loc>Gallipoli, Italy</conf-loc><conf-date>18&#8211;19 June 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2015</year></element-citation></ref><ref id="B17-sensors-25-05359"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Schurischuster</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kampel</surname><given-names>M.</given-names></name></person-group><source>VarroaDataset</source><publisher-name>Zenodo Dataset</publisher-name><publisher-loc>Gen&#232;ve, Switzerland</publisher-loc><year>2020</year><pub-id pub-id-type="doi">10.5281/zenodo.4085044</pub-id></element-citation></ref><ref id="B18-sensors-25-05359"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Biernacki</surname><given-names>P.</given-names></name></person-group><source>Dataset for Honey Bee Audio Detection</source><publisher-name>Zenodo Dataset</publisher-name><publisher-loc>Gen&#232;ve, Switzerland</publisher-loc><year>2023</year><pub-id pub-id-type="doi">10.5281/zenodo.10359686</pub-id></element-citation></ref><ref id="B19-sensors-25-05359"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zacepins</surname><given-names>A.</given-names></name><name name-style="western"><surname>Brusbardis</surname><given-names>K.</given-names></name><name name-style="western"><surname>Meitalovs</surname><given-names>J.</given-names></name><name name-style="western"><surname>Stalidzans</surname><given-names>E.</given-names></name></person-group><article-title>Challenges in the Development of Precision Beekeeping</article-title><source>Biosyst. Eng.</source><year>2017</year><volume>153</volume><fpage>35</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2014.12.001</pub-id></element-citation></ref><ref id="B20-sensors-25-05359"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cecchi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bencini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Rocchi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>R.S.</given-names></name><name name-style="western"><surname>Manes</surname><given-names>G.</given-names></name></person-group><article-title>Development and Field Testing of a Smart Beehive Monitoring System</article-title><source>IEEE Internet Things J.</source><year>2020</year><volume>7</volume><fpage>5824</fpage><lpage>5833</lpage></element-citation></ref><ref id="B21-sensors-25-05359"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Page</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>McKenzie</surname><given-names>J.E.</given-names></name><name name-style="western"><surname>Bossuyt</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>Boutron</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hoffmann</surname><given-names>T.C.</given-names></name><name name-style="western"><surname>Mulrow</surname><given-names>C.D.</given-names></name><name name-style="western"><surname>Shamseer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tetzlaff</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Akl</surname><given-names>E.A.</given-names></name><name name-style="western"><surname>Brennan</surname><given-names>S.E.</given-names></name><etal/></person-group><article-title>The PRISMA 2020 statement: An updated guideline for reporting systematic reviews</article-title><source>BMJ</source><year>2021</year><volume>372</volume><fpage>n71</fpage><pub-id pub-id-type="doi">10.1136/bmj.n71</pub-id><pub-id pub-id-type="pmid">33782057</pub-id><pub-id pub-id-type="pmcid">PMC8005924</pub-id></element-citation></ref><ref id="B22-sensors-25-05359"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Edwards Murphy</surname><given-names>F.</given-names></name><name name-style="western"><surname>Popovici</surname><given-names>E.</given-names></name><name name-style="western"><surname>Whelan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Magno</surname><given-names>M.</given-names></name></person-group><article-title>Development of an heterogeneous wireless sensor network for instrumentation and analysis of beehives</article-title><source>Proceedings of the 2015 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) Proceedings</source><conf-loc>Pisa, Italy</conf-loc><conf-date>11&#8211;14 May 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2015</year></element-citation></ref><ref id="B23-sensors-25-05359"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Leonard</surname><given-names>J.J.</given-names></name><name name-style="western"><surname>Feddes</surname><given-names>J.J.</given-names></name></person-group><article-title>Automated monitoring of flight activity at a beehive entrance using infrared light sensors</article-title><source>J. Apic. Res.</source><year>1990</year><volume>29</volume><fpage>20</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1080/00218839.1990.11101193</pub-id></element-citation></ref><ref id="B24-sensors-25-05359"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aydin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nafiz Aydin</surname><given-names>M.</given-names></name></person-group><article-title>Design and implementation of a smart beehive and its monitoring system using microservices in the context of IoT and open data</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>196</volume><fpage>106897</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.106897</pub-id></element-citation></ref><ref id="B25-sensors-25-05359"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name></person-group><article-title>Long-term and extensive monitoring for bee colonies based on internet of things</article-title><source>IEEE Internet Things J.</source><year>2020</year><volume>7</volume><fpage>7148</fpage><lpage>7155</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2020.2981681</pub-id></element-citation></ref><ref id="B26-sensors-25-05359"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Imoize</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Odeyemi</surname><given-names>S.D.</given-names></name><name name-style="western"><surname>Adebisi</surname><given-names>J.A.</given-names></name></person-group><article-title>Development of a low-cost wireless bee-hive temperature and sound monitoring system</article-title><source>Indones. J. Electr. Eng. Inform.</source><year>2020</year><volume>8</volume><fpage>476</fpage><lpage>485</lpage></element-citation></ref><ref id="B27-sensors-25-05359"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cecchi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Spinsante</surname><given-names>S.</given-names></name><name name-style="western"><surname>Terenzi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Orcioni</surname><given-names>S.</given-names></name></person-group><article-title>A smart sensor-based measurement system for advanced bee hive monitoring</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>2726</elocation-id><pub-id pub-id-type="doi">10.3390/s20092726</pub-id><pub-id pub-id-type="pmid">32397686</pub-id><pub-id pub-id-type="pmcid">PMC7248914</pub-id></element-citation></ref><ref id="B28-sensors-25-05359"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zacepins</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kviesis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Komasilovs</surname><given-names>V.</given-names></name><name name-style="western"><surname>Rido Muhammad</surname><given-names>F.</given-names></name></person-group><article-title>Monitoring system for remote bee colony state detection</article-title><source>Balt. J. Mod. Comput.</source><year>2020</year><volume>8</volume><fpage>461</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.22364/bjmc.2020.8.3.05</pub-id></element-citation></ref><ref id="B29-sensors-25-05359"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Catania</surname><given-names>P.</given-names></name><name name-style="western"><surname>Vallone</surname><given-names>M.</given-names></name></person-group><article-title>Application of A precision apiculture system to monitor honey daily production</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>2012</elocation-id><pub-id pub-id-type="doi">10.3390/s20072012</pub-id><pub-id pub-id-type="pmid">32260116</pub-id><pub-id pub-id-type="pmcid">PMC7181046</pub-id></element-citation></ref><ref id="B30-sensors-25-05359"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bairo</surname><given-names>A.</given-names></name><name name-style="western"><surname>Elisadiki</surname><given-names>J.</given-names></name></person-group><article-title>Development of a digital spring-based weight sensor for monitoring beehive weight</article-title><source>EAJSTI</source><year>2024</year><volume>6</volume><pub-id pub-id-type="doi">10.37425/kzwvyf75</pub-id></element-citation></ref><ref id="B31-sensors-25-05359"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Karan</surname><given-names>I.</given-names></name><name name-style="western"><surname>Leelipushpam Paulraj</surname><given-names>G.J.</given-names></name><name name-style="western"><surname>Johnraja Jebadurai</surname><given-names>I.</given-names></name><name name-style="western"><surname>Allwin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sharan</surname></name><name name-style="western"><surname>Peace</surname><given-names>S.J.</given-names></name></person-group><article-title>BeeSense-A Smart Beehive Monitoring System for Sustainable Apiculture</article-title><source>Proceedings of the 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)</source><conf-loc>Namakkal, India</conf-loc><conf-date>15&#8211;16 March 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year></element-citation></ref><ref id="B32-sensors-25-05359"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gaikwad</surname><given-names>V.</given-names></name><name name-style="western"><surname>Amune</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rajput</surname><given-names>V.</given-names></name><name name-style="western"><surname>Musale</surname><given-names>V.</given-names></name><name name-style="western"><surname>Rajas</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kakade</surname><given-names>S.</given-names></name></person-group><article-title>Smart Beehive Monitoring System using IoT</article-title><source>Proceedings of the 2024 Second International Conference on Advances in Information Technology (ICAIT)</source><conf-loc>Chikkamagaluru, India</conf-loc><conf-date>24&#8211;27 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="B33-sensors-25-05359"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Idrus</surname><given-names>Z.</given-names></name><etal/></person-group><article-title>Intelligent beehive monitoring system based on internet of things and colony state analysis</article-title><source>Smart Agric. Technol.</source><year>2024</year><volume>9</volume><fpage>100584</fpage><pub-id pub-id-type="doi">10.1016/j.atech.2024.100584</pub-id></element-citation></ref><ref id="B34-sensors-25-05359"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cota</surname><given-names>D.</given-names></name><name name-style="western"><surname>Martins</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mamede</surname><given-names>H.</given-names></name><name name-style="western"><surname>Branco</surname><given-names>F.</given-names></name></person-group><article-title>BHiveSense: An integrated information system architecture for sustainable remote monitoring and management of apiaries based on IoT and microservices</article-title><source>J. Open Innov.</source><year>2023</year><volume>9</volume><fpage>100110</fpage><pub-id pub-id-type="doi">10.1016/j.joitmc.2023.100110</pub-id></element-citation></ref><ref id="B35-sensors-25-05359"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Abdollahi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Coallier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Giovenazzo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Falk</surname><given-names>T.H.</given-names></name></person-group><article-title>Performance comparison of voice activity detectors for acoustic beehive monitoring</article-title><source>Proceedings of the 2023 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)</source><conf-loc>Regina, SK, Canada</conf-loc><conf-date>24&#8211;27 September 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B36-sensors-25-05359"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Milovanovi&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Peji&#263;</surname><given-names>J.</given-names></name><name name-style="western"><surname>Peji&#263;</surname><given-names>P.</given-names></name></person-group><article-title>Development of smart beehive frame for multi-parameter monitoring</article-title><source>Proceedings of the 2023 IEEE 33rd International Conference on Microelectronics (MIEL)</source><conf-loc>Nis, Serbia</conf-loc><conf-date>16&#8211;18 October 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B37-sensors-25-05359"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kviesis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Komasilovs</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ozols</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zacepins</surname><given-names>A.</given-names></name></person-group><article-title>Bee colony remote monitoring based on IoT using ESP-NOW protocol</article-title><source>PeerJ Comput. Sci.</source><year>2023</year><volume>9</volume><fpage>e1363</fpage><pub-id pub-id-type="doi">10.7717/peerj-cs.1363</pub-id><pub-id pub-id-type="pmid">37346518</pub-id><pub-id pub-id-type="pmcid">PMC10280558</pub-id></element-citation></ref><ref id="B38-sensors-25-05359"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>J.</given-names></name></person-group><article-title>Design of an unmanned bee breeding box control system based on stm32</article-title><source>Proceedings of the 2024 10th International Conference on Control, Automation and Robotics (ICCAR)</source><conf-loc>Singapore</conf-loc><conf-date>27&#8211;29 April 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year></element-citation></ref><ref id="B39-sensors-25-05359"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamza</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Tashakkori</surname><given-names>R.</given-names></name><name name-style="western"><surname>Underwood</surname><given-names>B.</given-names></name><name name-style="western"><surname>O&#8217;Brien</surname><given-names>W.</given-names></name><name name-style="western"><surname>Campell</surname><given-names>C.</given-names></name></person-group><article-title>BeeLive: The IoT platform of Beemon monitoring and alerting system for beehives</article-title><source>Smart Agric. Technol.</source><year>2023</year><volume>6</volume><fpage>100331</fpage><pub-id pub-id-type="doi">10.1016/j.atech.2023.100331</pub-id></element-citation></ref><ref id="B40-sensors-25-05359"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sanz</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Prado-Jimeno</surname><given-names>R.</given-names></name><name name-style="western"><surname>Fuentes-P&#233;rez</surname><given-names>J.F.</given-names></name></person-group><article-title>Comparative study of natural fibres to improve insulation in wooden beehives using sensor networks</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>5760</elocation-id><pub-id pub-id-type="doi">10.3390/app14135760</pub-id></element-citation></ref><ref id="B41-sensors-25-05359"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Capela</surname><given-names>N.</given-names></name><name name-style="western"><surname>Dupont</surname><given-names>Y.L.</given-names></name><name name-style="western"><surname>Rortais</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sarmento</surname><given-names>A.</given-names></name><name name-style="western"><surname>Papanikolaou</surname><given-names>A.</given-names></name><name name-style="western"><surname>Topping</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Arnold</surname><given-names>G.</given-names></name><name name-style="western"><surname>Pinto</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Rodrigues</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>More</surname><given-names>S.J.</given-names></name><etal/></person-group><article-title>High accuracy monitoring of honey bee colony development by a quantitative method</article-title><source>J. Apic. Res.</source><year>2022</year><volume>62</volume><fpage>741</fpage><lpage>750</lpage><pub-id pub-id-type="doi">10.1080/00218839.2022.2098899</pub-id></element-citation></ref><ref id="B42-sensors-25-05359"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Milovanovi&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Peji&#263;</surname><given-names>J.</given-names></name><name name-style="western"><surname>Peji&#263;</surname><given-names>P.</given-names></name></person-group><article-title>Advanced sensors for noninvasive bee colony inspection</article-title><source>Comput. Electron. Agric.</source><year>2025</year><volume>231</volume><fpage>109945</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2025.109945</pub-id></element-citation></ref><ref id="B43-sensors-25-05359"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zgank</surname><given-names>A.</given-names></name></person-group><article-title>Acoustic monitoring and classification of bee swarm activity using MFCC feature extraction and HMM acoustic modeling</article-title><source>Proceedings of the 2018 ELEKTRO</source><conf-loc>Mikulov, Czech Republic</conf-loc><conf-date>21&#8211;23 May 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2018</year></element-citation></ref><ref id="B44-sensors-25-05359"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulyukin</surname><given-names>V.</given-names></name><name name-style="western"><surname>Mukherjee</surname><given-names>S.</given-names></name></person-group><article-title>On video analysis of omnidirectional bee traffic: Counting bee motions with motion detection and image classification</article-title><source>Appl. Sci.</source><year>2019</year><volume>9</volume><elocation-id>3743</elocation-id><pub-id pub-id-type="doi">10.3390/app9183743</pub-id></element-citation></ref><ref id="B45-sensors-25-05359"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulyukin</surname><given-names>V.</given-names></name><name name-style="western"><surname>Mukherjee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Amlathe</surname><given-names>P.</given-names></name></person-group><article-title>Toward audio beehive monitoring: Deep learning vs. Standard machine learning in classifying beehive audio samples</article-title><source>Appl. Sci.</source><year>2018</year><volume>8</volume><elocation-id>1573</elocation-id><pub-id pub-id-type="doi">10.3390/app8091573</pub-id></element-citation></ref><ref id="B46-sensors-25-05359"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tu</surname><given-names>G.J.</given-names></name><name name-style="western"><surname>Hansen</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Kryger</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ahrendt</surname><given-names>P.</given-names></name></person-group><article-title>Automatic behaviour analysis system for honeybees using computer vision</article-title><source>Comput. Electron. Agric.</source><year>2016</year><volume>122</volume><fpage>10</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2016.01.011</pub-id></element-citation></ref><ref id="B47-sensors-25-05359"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Struye</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Mortier</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Arnold</surname><given-names>G.</given-names></name><name name-style="western"><surname>Miniggio</surname><given-names>C.</given-names></name><name name-style="western"><surname>Borneck</surname><given-names>R.</given-names></name></person-group><article-title>Microprocessor-controlled monitoring of honeybee flight activity at the hive entrance</article-title><source>Apidologie</source><year>1994</year><volume>25</volume><fpage>384</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1051/apido:19940405</pub-id></element-citation></ref><ref id="B48-sensors-25-05359"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramsey</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bencsik</surname><given-names>M.</given-names></name><name name-style="western"><surname>Newton</surname><given-names>M.I.</given-names></name></person-group><article-title>Extensive vibrational characterisation and long-term monitoring of honeybee dorso-ventral abdominal vibration signals</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><elocation-id>14571</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-32931-z</pub-id><pub-id pub-id-type="pmid">30275492</pub-id><pub-id pub-id-type="pmcid">PMC6167329</pub-id></element-citation></ref><ref id="B49-sensors-25-05359"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bermig</surname><given-names>S.</given-names></name><name name-style="western"><surname>Odemer</surname><given-names>R.</given-names></name><name name-style="western"><surname>Gombert</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Frommberger</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rosenquist</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pistorius</surname><given-names>J.</given-names></name></person-group><article-title>Experimental validation of an electronic counting device to determine flight activity of honey bees (<italic toggle="yes">Apis mellifera</italic> L.)</article-title><source>J. F&#252;r Kult.</source><year>2020</year><volume>72</volume><fpage>132</fpage><lpage>140</lpage></element-citation></ref><ref id="B50-sensors-25-05359"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ngo</surname><given-names>T.N.</given-names></name><name name-style="western"><surname>Rustia</surname><given-names>D.J.A.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>E.C.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>T.T.</given-names></name></person-group><article-title>Automated monitoring and analyses of honey bee pollen foraging behavior using a deep learning-based imaging system</article-title><source>Comput. Electron. Agric.</source><year>2021</year><volume>187</volume><fpage>106239</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2021.106239</pub-id></element-citation></ref><ref id="B51-sensors-25-05359"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Bariselli</surname><given-names>S.</given-names></name><name name-style="western"><surname>Palego</surname><given-names>C.</given-names></name><name name-style="western"><surname>Holland</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cross</surname><given-names>P.</given-names></name></person-group><article-title>A comparison of machine-learning assisted optical and thermal camera systems for beehive activity counting</article-title><source>Smart Agric. Technol.</source><year>2022</year><volume>2</volume><fpage>100038</fpage><pub-id pub-id-type="doi">10.1016/j.atech.2022.100038</pub-id></element-citation></ref><ref id="B52-sensors-25-05359"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zgank</surname><given-names>A.</given-names></name></person-group><article-title>Bee swarm activity acoustic classification for an IoT-based farm service</article-title><source>Sensors</source><year>2019</year><volume>20</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.3390/s20010021</pub-id><pub-id pub-id-type="pmid">31861505</pub-id><pub-id pub-id-type="pmcid">PMC6982799</pub-id></element-citation></ref><ref id="B53-sensors-25-05359"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Libal</surname><given-names>U.</given-names></name><name name-style="western"><surname>Biernacki</surname><given-names>P.</given-names></name></person-group><article-title>MFCC-based sound classification of honey bees</article-title><source>Int. J. Electron. Telecommun.</source><year>2024</year><volume>70</volume><fpage>849</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.24425/ijet.2024.152069</pub-id></element-citation></ref><ref id="B54-sensors-25-05359"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Penaloza-Aponte</surname><given-names>D.</given-names></name><name name-style="western"><surname>Brandt</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dent</surname><given-names>E.</given-names></name><name name-style="western"><surname>Underwood</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>DeMoras</surname><given-names>B.</given-names></name><name name-style="western"><surname>Bruckner</surname><given-names>S.</given-names></name><name name-style="western"><surname>L&#243;pez-Uribe</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Urbina</surname><given-names>J.V.</given-names></name></person-group><article-title>Automated entrance monitoring to investigate honey bee foraging trips using open-source wireless platform and fiducial tags</article-title><source>HardwareX</source><year>2024</year><volume>20</volume><fpage>e00609</fpage><pub-id pub-id-type="doi">10.1016/j.ohx.2024.e00609</pub-id><pub-id pub-id-type="pmid">39669441</pub-id><pub-id pub-id-type="pmcid">PMC11636203</pub-id></element-citation></ref><ref id="B55-sensors-25-05359"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kongsilp</surname><given-names>P.</given-names></name><name name-style="western"><surname>Taetragool</surname><given-names>U.</given-names></name><name name-style="western"><surname>Duangphakdee</surname><given-names>O.</given-names></name></person-group><article-title>Individual honey bee tracking in a beehive environment using deep learning and Kalman filter</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>1061</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-44718-y</pub-id><pub-id pub-id-type="pmid">38212336</pub-id><pub-id pub-id-type="pmcid">PMC10784501</pub-id></element-citation></ref><ref id="B56-sensors-25-05359"><label>56.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chowdhury</surname><given-names>M.T.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sumon</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>Hossain</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Reza</surname><given-names>A.W.</given-names></name><name name-style="western"><surname>Emon</surname><given-names>M.Y.</given-names></name></person-group><article-title>Estimation on beehive landing boards using machine learning algorithm</article-title><source>Proceedings of the 2024 IEEE International Conference on Smart Power Control and Renewable Energy (ICSPCRE)</source><conf-loc>Rourkela, India</conf-loc><conf-date>19&#8211;21 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B57-sensors-25-05359"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lei</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>A honey bee in-and-out counting method based on multiple object tracking algorithm</article-title><source>Insects</source><year>2024</year><volume>15</volume><elocation-id>974</elocation-id><pub-id pub-id-type="doi">10.3390/insects15120974</pub-id><pub-id pub-id-type="pmid">39769576</pub-id><pub-id pub-id-type="pmcid">PMC11677362</pub-id></element-citation></ref><ref id="B58-sensors-25-05359"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Le</surname><given-names>T.N.</given-names></name><name name-style="western"><surname>Phung</surname><given-names>T.H.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>D.M.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>H.Q.</given-names></name><name name-style="western"><surname>Pham</surname><given-names>H.T.</given-names></name><name name-style="western"><surname>Phan</surname><given-names>T.T.H.</given-names></name><name name-style="western"><surname>Vu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Le</surname><given-names>T.L.</given-names></name></person-group><article-title>Improving pollen-bearing honey bee detection from videos captured at hive entrance by combining deep learning and handling imbalance techniques</article-title><source>Ecol. Inform.</source><year>2024</year><volume>82</volume><fpage>102744</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2024.102744</pub-id></element-citation></ref><ref id="B59-sensors-25-05359"><label>59.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>H.T.</given-names></name><name name-style="western"><surname>Pham</surname><given-names>M.T.</given-names></name><name name-style="western"><surname>Tran</surname><given-names>Q.D.</given-names></name><name name-style="western"><surname>Pham</surname><given-names>Q.H.</given-names></name><name name-style="western"><surname>Phan</surname><given-names>T.T.H.</given-names></name></person-group><article-title>Evaluating audio feature extraction methods for identifying bee queen presence</article-title><source>Proceedings of the 12th International Symposium on Information and Communication Technology</source><conf-loc>New York, NY, USA</conf-loc><conf-date>7&#8211;8 December 2023</conf-date><fpage>93</fpage><lpage>100</lpage></element-citation></ref><ref id="B60-sensors-25-05359"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sledevi&#269;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Serackis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Matuzevi&#269;ius</surname><given-names>D.</given-names></name><name name-style="western"><surname>Plonis</surname><given-names>D.</given-names></name><name name-style="western"><surname>Vdoviak</surname><given-names>G.</given-names></name></person-group><article-title>Visual recognition of honeybee behavior patterns at the hive entrance</article-title><source>PLoS ONE</source><year>2025</year><volume>20</volume><elocation-id>e0318401</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0318401</pub-id><pub-id pub-id-type="pmid">39999093</pub-id><pub-id pub-id-type="pmcid">PMC11856287</pub-id></element-citation></ref><ref id="B61-sensors-25-05359"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Libal</surname><given-names>U.</given-names></name><name name-style="western"><surname>Biernacki</surname><given-names>P.</given-names></name></person-group><article-title>Non-intrusive system for honeybee recognition based on audio signals and maximum likelihood classification by autoencoder</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5389</elocation-id><pub-id pub-id-type="doi">10.3390/s24165389</pub-id><pub-id pub-id-type="pmid">39205082</pub-id><pub-id pub-id-type="pmcid">PMC11359591</pub-id></element-citation></ref><ref id="B62-sensors-25-05359"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Micheli</surname><given-names>M.</given-names></name><name name-style="western"><surname>Papa</surname><given-names>G.</given-names></name><name name-style="western"><surname>Negri</surname><given-names>I.</given-names></name><name name-style="western"><surname>Lancini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nuzzi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Pasinetti</surname><given-names>S.</given-names></name></person-group><article-title>Sensorizing a beehive: A study on potential embedded solutions for internal contactless monitoring of bees activity</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5270</elocation-id><pub-id pub-id-type="doi">10.3390/s24165270</pub-id><pub-id pub-id-type="pmid">39204965</pub-id><pub-id pub-id-type="pmcid">PMC11360817</pub-id></element-citation></ref><ref id="B63-sensors-25-05359"><label>63.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dickson</surname><given-names>R.T.</given-names></name><name name-style="western"><surname>Parry</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Campell</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tashakkori</surname><given-names>R.</given-names></name></person-group><article-title>Bee traffic estimation with YOLO and optical flow</article-title><source>Proceedings of the SoutheastCon 2024</source><conf-loc>Atlanta, GA, USA</conf-loc><conf-date>15&#8211;24 March 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>928</fpage><lpage>933</lpage></element-citation></ref><ref id="B64-sensors-25-05359"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sledevi&#269;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Serackis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Matuzevi&#269;ius</surname><given-names>D.</given-names></name><name name-style="western"><surname>Plonis</surname><given-names>D.</given-names></name><name name-style="western"><surname>Andriukaitis</surname><given-names>D.</given-names></name></person-group><article-title>Keypoint-based bee orientation estimation and ramp detection at the hive entrance for bee behavior identification system</article-title><source>Agriculture</source><year>2024</year><volume>14</volume><elocation-id>1890</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture14111890</pub-id></element-citation></ref><ref id="B65-sensors-25-05359"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Iqbal</surname><given-names>K.</given-names></name><name name-style="western"><surname>Alabdullah</surname><given-names>B.</given-names></name><name name-style="western"><surname>Al Mudawi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Algarni</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jalal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name></person-group><article-title>Empirical analysis of honeybees acoustics as biosensors signals for swarm prediction in beehives</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>148405</fpage><lpage>148421</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3471895</pub-id></element-citation></ref><ref id="B66-sensors-25-05359"><label>66.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Janetzky</surname><given-names>P.</given-names></name><name name-style="western"><surname>Schaller</surname><given-names>M.</given-names></name><name name-style="western"><surname>Krause</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hotho</surname><given-names>A.</given-names></name></person-group><article-title>Swarming detection in smart beehives using auto encoders for audio data</article-title><source>Proceedings of the 2023 30th International Conference on Systems, Signals and Image Processing (IWSSIP)</source><conf-loc>Ohrid, North Macedonia</conf-loc><conf-date>27&#8211;29 June 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B67-sensors-25-05359"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>V&#225;rkonyi</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Seixas</surname><given-names>J.L.</given-names><suffix>Jr.</suffix></name><name name-style="western"><surname>Horv&#225;th</surname><given-names>T.</given-names></name></person-group><article-title>Dynamic noise filtering for multi-class classification of beehive audio data</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>213</volume><fpage>118850</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2022.118850</pub-id></element-citation></ref><ref id="B68-sensors-25-05359"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Aldabashi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Cross</surname><given-names>P.</given-names></name><name name-style="western"><surname>Palego</surname><given-names>C.</given-names></name></person-group><article-title>Challenges in developing a real-time bee-counting radar</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>5250</elocation-id><pub-id pub-id-type="doi">10.3390/s23115250</pub-id><pub-id pub-id-type="pmid">37299977</pub-id><pub-id pub-id-type="pmcid">PMC10256090</pub-id></element-citation></ref><ref id="B69-sensors-25-05359"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulyukin</surname><given-names>V.A.</given-names></name><name name-style="western"><surname>Kulyukin</surname><given-names>A.V.</given-names></name></person-group><article-title>Accuracy vs. Energy: An assessment of bee object inference in videos from on-hive video loggers with YOLOv3, YOLOv4-tiny, and YOLOv7-tiny</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6791</elocation-id><pub-id pub-id-type="doi">10.3390/s23156791</pub-id><pub-id pub-id-type="pmid">37571576</pub-id><pub-id pub-id-type="pmcid">PMC10422429</pub-id></element-citation></ref><ref id="B70-sensors-25-05359"><label>70.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Safie</surname><given-names>S.I.</given-names></name><name name-style="western"><surname>Kamal</surname><given-names>N.S.A.</given-names></name><name name-style="western"><surname>Yusof</surname><given-names>E.M.M.</given-names></name><name name-style="western"><surname>Tohid</surname><given-names>M.Z.W.M.</given-names></name><name name-style="western"><surname>Jaafar</surname><given-names>N.H.</given-names></name></person-group><article-title>Comparison of SqueezeNet and DarkNet-53 based YOLO-V3 performance for beehive intelligent monitoring system</article-title><source>Proceedings of the 2023 IEEE 13th Symposium on Computer Applications &amp; Industrial Electronics (ISCAIE)</source><conf-loc>Penang, Malaysia</conf-loc><conf-date>20&#8211;21 May 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>62</fpage><lpage>65</lpage></element-citation></ref><ref id="B71-sensors-25-05359"><label>71.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Libal</surname><given-names>U.</given-names></name><name name-style="western"><surname>Biernacki</surname><given-names>P.</given-names></name></person-group><article-title>Detecting drones at an entrance to a beehive based on audio signals and autoencoder neural networks</article-title><source>Proceedings of the 2023 Signal Processing Symposium (SPSympo)</source><conf-loc>Karpacz, Poland</conf-loc><conf-date>26&#8211;28 September 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B72-sensors-25-05359"><label>72.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sledevi&#263;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Plonis</surname><given-names>D.</given-names></name></person-group><article-title>Toward bee behavioral pattern recognition on hive entrance using YOLOv8</article-title><source>Proceedings of the 2023 IEEE 10th Jubilee Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE)</source><conf-loc>Vilnius, Lithuania</conf-loc><conf-date>27&#8211;29 April 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B73-sensors-25-05359"><label>73.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sledevi&#269;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Abromavi&#269;ius</surname><given-names>V.</given-names></name></person-group><article-title>Toward bee motion pattern identification on hive landing board</article-title><source>Proceedings of the 2023 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)</source><conf-loc>Vilnius, Lithuania</conf-loc><conf-date>27 April 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B74-sensors-25-05359"><label>74.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen Thi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Phan</surname><given-names>T.T.H.</given-names></name><name name-style="western"><surname>Tran</surname><given-names>C.T.</given-names></name></person-group><article-title>Genetic programming for bee audio classification</article-title><source>Proceedings of the 2023 8th International Conference on Intelligent Information Technology</source><conf-loc>New York, NY, USA</conf-loc><conf-date>24&#8211;26 February 2023</conf-date><fpage>246</fpage><lpage>250</lpage></element-citation></ref><ref id="B75-sensors-25-05359"><label>75.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Marstaller</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tausch</surname><given-names>F.</given-names></name><name name-style="western"><surname>Stock</surname><given-names>S.</given-names></name></person-group><article-title>DeepBees&#8212;Building and scaling convolutional neuronal nets for fast and large-scale visual monitoring of bee hives</article-title><source>Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27&#8211;28 October 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2019</year></element-citation></ref><ref id="B76-sensors-25-05359"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Szczurek</surname><given-names>A.</given-names></name><name name-style="western"><surname>Maciejewska</surname><given-names>M.</given-names></name><name name-style="western"><surname>B&#261;k</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wilde</surname><given-names>J.</given-names></name><name name-style="western"><surname>Siuda</surname><given-names>M.</given-names></name></person-group><article-title>Semiconductor gas sensor as a detector of Varroa destructor infestation of honey bee colonies&#8212;Statistical evaluation</article-title><source>Comput. Electron. Agric.</source><year>2019</year><volume>162</volume><fpage>405</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2019.04.033</pub-id></element-citation></ref><ref id="B77-sensors-25-05359"><label>77.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mrozek</surname><given-names>D.</given-names></name><name name-style="western"><surname>G&#559;rny</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wachowicz</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ma&#322;ysiak-Mrozek</surname><given-names>B.</given-names></name></person-group><article-title>Edge-based detection of varroosis in beehives with IoT devices with embedded and TPU-accelerated machine learning</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><elocation-id>11078</elocation-id><pub-id pub-id-type="doi">10.3390/app112211078</pub-id></element-citation></ref><ref id="B78-sensors-25-05359"><label>78.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kviesis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Komasilovs</surname><given-names>V.</given-names></name><name name-style="western"><surname>Komasilova</surname><given-names>O.</given-names></name><name name-style="western"><surname>Zacepins</surname><given-names>A.</given-names></name></person-group><article-title>Application of fuzzy logic for honey bee colony state detection based on temperature data</article-title><source>Biosyst. Eng.</source><year>2020</year><volume>193</volume><fpage>90</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2020.02.010</pub-id></element-citation></ref><ref id="B79-sensors-25-05359"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rafael Braga</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gomes</surname><given-names>D.G.</given-names></name><name name-style="western"><surname>Rogers</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hassler</surname><given-names>E.E.</given-names></name><name name-style="western"><surname>Freitas</surname><given-names>B.M.</given-names></name><name name-style="western"><surname>Cazier</surname><given-names>J.A.</given-names></name></person-group><article-title>A method for mining combined data from in-hive sensors, weather and apiary inspections to forecast the health status of honey bee colonies</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>169</volume><fpage>105161</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2019.105161</pub-id></element-citation></ref><ref id="B80-sensors-25-05359"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Analysis of temperature characteristics for overwintering bee colonies based on long-term monitoring data</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>198</volume><fpage>107104</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.107104</pub-id></element-citation></ref><ref id="B81-sensors-25-05359"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaplan Berkaya</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sora Gunal</surname><given-names>E.</given-names></name><name name-style="western"><surname>Gunal</surname><given-names>S.</given-names></name></person-group><article-title>Deep learning-based classification models for beehive monitoring</article-title><source>Ecol. Inform.</source><year>2021</year><volume>64</volume><fpage>101353</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2021.101353</pub-id></element-citation></ref><ref id="B82-sensors-25-05359"><label>82.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alves</surname><given-names>T.S.</given-names></name><name name-style="western"><surname>Pinto</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Ventura</surname><given-names>P.</given-names></name><name name-style="western"><surname>Neves</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Biron</surname><given-names>D.G.</given-names></name><name name-style="western"><surname>Junior</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>De Paula Filho</surname><given-names>P.L.</given-names></name><name name-style="western"><surname>Rodrigues</surname><given-names>P.J.</given-names></name></person-group><article-title>Automatic detection and classification of honey bee comb cells using deep learning</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>170</volume><fpage>105244</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2020.105244</pub-id></element-citation></ref><ref id="B83-sensors-25-05359"><label>83.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sevin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tutun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mutlu</surname><given-names>S.</given-names></name></person-group><article-title>Detection of Varroa mites from honey bee hives by smart technology Var-Gor: A hive monitoring and image processing device</article-title><source>Turk. J. Vet. Anim. Sci.</source><year>2021</year><volume>45</volume><fpage>487</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.3906/vet-2005-89</pub-id></element-citation></ref><ref id="B84-sensors-25-05359"><label>84.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Oh</surname><given-names>J.</given-names></name><name name-style="western"><surname>Heo</surname><given-names>T.Y.</given-names></name></person-group><article-title>Acoustic scene classification and visualization of beehive sounds using machine learning algorithms and Grad-CAM</article-title><source>Math. Probl. Eng.</source><year>2021</year><volume>2021</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1155/2021/5594498</pub-id></element-citation></ref><ref id="B85-sensors-25-05359"><label>85.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Schurischuster</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kampel</surname><given-names>M.</given-names></name></person-group><article-title>Image-based classification of honeybees</article-title><source>Proceedings of the 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)</source><conf-loc>Paris, France</conf-loc><conf-date>9&#8211;12 November 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year></element-citation></ref><ref id="B86-sensors-25-05359"><label>86.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Degenfellner</surname><given-names>J.</given-names></name><name name-style="western"><surname>Templ</surname><given-names>M.</given-names></name></person-group><article-title>Modeling bee hive dynamics: Assessing colony health using hive weight and environmental parameters</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>218</volume><fpage>108742</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.108742</pub-id></element-citation></ref><ref id="B87-sensors-25-05359"><label>87.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Divas&#243;n</surname><given-names>J.</given-names></name><name name-style="western"><surname>Romero</surname><given-names>A.</given-names></name><name name-style="western"><surname>Martinez-de Pison</surname><given-names>F.J.</given-names></name><name name-style="western"><surname>Casalongue</surname><given-names>M.</given-names></name><name name-style="western"><surname>Silvestre</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Santolaria</surname><given-names>P.</given-names></name><name name-style="western"><surname>Y&#225;niz</surname><given-names>J.L.</given-names></name></person-group><article-title>Analysis of Varroa mite colony infestation level using new open software based on deep learning techniques</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3828</elocation-id><pub-id pub-id-type="doi">10.3390/s24123828</pub-id><pub-id pub-id-type="pmid">38931612</pub-id><pub-id pub-id-type="pmcid">PMC11207890</pub-id></element-citation></ref><ref id="B88-sensors-25-05359"><label>88.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Camayo</surname><given-names>A.I.C.</given-names></name><name name-style="western"><surname>Mu&#241;oz</surname><given-names>M.A.C.</given-names></name><name name-style="western"><surname>Corrales</surname><given-names>J.C.</given-names></name></person-group><article-title>ApIsoT: An IoT function aggregation mechanism for detecting Varroa infestation in <italic toggle="yes">Apis mellifera</italic> species</article-title><source>Agriculture</source><year>2024</year><volume>14</volume><elocation-id>846</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture14060846</pub-id></element-citation></ref><ref id="B89-sensors-25-05359"><label>89.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Narcia-Macias</surname><given-names>C.I.</given-names></name><name name-style="western"><surname>Guardado</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rodriguez</surname><given-names>J.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rampersad-Ammons</surname><given-names>J.</given-names></name><name name-style="western"><surname>Enriquez</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>D.C.</given-names></name></person-group><article-title>IntelliBeeHive: An automated honey bee, pollen, and Varroa destructor monitoring system</article-title><source>Proceedings of the 2024 International Conference on Machine Learning and Applications (ICMLA)</source><conf-loc>Miami, FL, USA</conf-loc><conf-date>18&#8211;20 December 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>845</fpage><lpage>850</lpage></element-citation></ref><ref id="B90-sensors-25-05359"><label>90.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gar&#231;&#227;o</surname><given-names>T.</given-names></name><name name-style="western"><surname>Sousa</surname><given-names>J.</given-names></name><name name-style="western"><surname>Andr&#233;</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ferreira</surname><given-names>J.</given-names></name></person-group><article-title>BEE-YOND BUZZ: Exploring deep learning techniques for beehive audio classification</article-title><source>Proceedings of the 2024 4th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)</source><conf-loc>Male, Maldives</conf-loc><conf-date>4&#8211;6 November 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B91-sensors-25-05359"><label>91.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kontogiannis</surname><given-names>S.</given-names></name></person-group><article-title>Beehive smart detector device for the detection of critical conditions that utilize edge device computations and deep learning inferences</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5444</elocation-id><pub-id pub-id-type="doi">10.3390/s24165444</pub-id><pub-id pub-id-type="pmid">39205138</pub-id><pub-id pub-id-type="pmcid">PMC11359104</pub-id></element-citation></ref><ref id="B92-sensors-25-05359"><label>92.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Robles-Guerrero</surname><given-names>A.</given-names></name><name name-style="western"><surname>G&#243;mez-Jim&#233;nez</surname><given-names>S.</given-names></name><name name-style="western"><surname>Saucedo-Anaya</surname><given-names>T.</given-names></name><name name-style="western"><surname>L&#243;pez-Betancur</surname><given-names>D.</given-names></name><name name-style="western"><surname>Navarro-Sol&#237;s</surname><given-names>D.</given-names></name><name name-style="western"><surname>Guerrero-M&#233;ndez</surname><given-names>C.</given-names></name></person-group><article-title>Convolutional neural networks for real time classification of beehive acoustic patterns on constrained devices</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6384</elocation-id><pub-id pub-id-type="doi">10.3390/s24196384</pub-id><pub-id pub-id-type="pmid">39409424</pub-id><pub-id pub-id-type="pmcid">PMC11479142</pub-id></element-citation></ref><ref id="B93-sensors-25-05359"><label>93.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Otesbelgue</surname><given-names>A.</given-names></name><name name-style="western"><surname>de Lima Rodrigues</surname><given-names>&#205;.</given-names></name><name name-style="western"><surname>dos Santos</surname><given-names>C.F.</given-names></name><name name-style="western"><surname>Gomes</surname><given-names>D.G.</given-names></name><name name-style="western"><surname>Blochtein</surname><given-names>B.</given-names></name></person-group><article-title>The missing queen: A non-invasive method to identify queenless stingless bee hives</article-title><source>Apidologie</source><year>2025</year><volume>56</volume><pub-id pub-id-type="doi">10.1007/s13592-025-01148-1</pub-id></element-citation></ref><ref id="B94-sensors-25-05359"><label>94.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luz</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>De Oliveira</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Pereira</surname><given-names>F.d.M.</given-names></name><name name-style="western"><surname>De Ara&#250;jo</surname><given-names>F.H.D.</given-names></name><name name-style="western"><surname>Magalh&#227;es</surname><given-names>D.M.V.</given-names></name></person-group><article-title>Cepstral and Deep Features for <italic toggle="yes">Apis mellifera</italic> hive strength classification</article-title><source>J. Internet Serv. Appl.</source><year>2024</year><volume>15</volume><fpage>548</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.5753/jisa.2024.4015</pub-id></element-citation></ref><ref id="B95-sensors-25-05359"><label>95.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>De Simone</surname><given-names>A.</given-names></name><name name-style="western"><surname>Barbisan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Turvani</surname><given-names>G.</given-names></name><name name-style="western"><surname>Riente</surname><given-names>F.</given-names></name></person-group><article-title>Advancing beekeeping: IoT and TinyML for queen bee monitoring using audio signals</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2024</year><volume>73</volume><fpage>2527309</fpage><pub-id pub-id-type="doi">10.1109/TIM.2024.3449981</pub-id></element-citation></ref><ref id="B96-sensors-25-05359"><label>96.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Newton</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>Chamberlain</surname><given-names>L.</given-names></name><name name-style="western"><surname>McVeigh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bencsik</surname><given-names>M.</given-names></name></person-group><article-title>Winter carbon dioxide measurement in honeybee hives</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>1679</elocation-id><pub-id pub-id-type="doi">10.3390/app14041679</pub-id></element-citation></ref><ref id="B97-sensors-25-05359"><label>97.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rathore</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tyagi</surname><given-names>P.K.</given-names></name><name name-style="western"><surname>Agrawal</surname><given-names>D.</given-names></name></person-group><article-title>Semi-automatic Analysis of cells in honeybee comb images</article-title><source>Proceedings of the 2023 IEEE International Students&#8217; Conference on Electrical, Electronics and Computer Science (SCEECS)</source><conf-loc>Bhopal, India</conf-loc><conf-date>18&#8211;19 February 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B98-sensors-25-05359"><label>98.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Borgianni</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Adami</surname><given-names>D.</given-names></name><name name-style="western"><surname>Giordano</surname><given-names>S.</given-names></name></person-group><article-title>Spectrogram Based Bee Sound Analysis with DNNs: A step toward Federated Learning approach</article-title><source>Proceedings of the 2023 4th International Symposium on the Internet of Sounds</source><conf-loc>Pisa, Italy</conf-loc><conf-date>26&#8211;27 October 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B99-sensors-25-05359"><label>99.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mahajan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>D.</given-names></name><name name-style="western"><surname>Miranda</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pinto</surname><given-names>R.</given-names></name><name name-style="western"><surname>Patil</surname><given-names>V.</given-names></name></person-group><article-title>NeuralBee&#8212;A beehive health monitoring system</article-title><source>Proceedings of the 2023 International Conference on Communication System, Computing and IT Applications (CSCITA)</source><conf-loc>Mumbai, India</conf-loc><conf-date>31 March&#8211;1 April 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>84</fpage><lpage>89</lpage></element-citation></ref><ref id="B100-sensors-25-05359"><label>100.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdollahi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Henry</surname><given-names>E.</given-names></name><name name-style="western"><surname>Giovenazzo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Falk</surname><given-names>T.H.</given-names></name></person-group><article-title>The importance of context awareness in acoustics-based automated beehive monitoring</article-title><source>Appl. Sci.</source><year>2022</year><volume>13</volume><elocation-id>195</elocation-id><pub-id pub-id-type="doi">10.3390/app13010195</pub-id></element-citation></ref><ref id="B101-sensors-25-05359"><label>101.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>A.</given-names></name><name name-style="western"><surname>Varastehpour</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ardekani</surname><given-names>I.</given-names></name><name name-style="western"><surname>Sharifzadeh</surname><given-names>H.</given-names></name></person-group><article-title>Bee disease Varroa prediction: Utilizing convolutional neural networks with augmentation for robust detection and identification of honeybee infection</article-title><source>Proceedings of the 2024 1st International Conference on Innovative Engineering Sciences and Technological Research (ICIESTR)</source><conf-loc>Muscat, Oman</conf-loc><conf-date>14&#8211;15 May 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B102-sensors-25-05359"><label>102.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barbisan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Turvani</surname><given-names>G.</given-names></name><name name-style="western"><surname>Riente</surname><given-names>F.</given-names></name></person-group><article-title>A machine learning approach for queen bee detection through remote audio sensing to safeguard honeybee colonies</article-title><source>IEEE Trans. Agri. Elect.</source><year>2024</year><volume>2</volume><fpage>236</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1109/TAFE.2024.3406648</pub-id></element-citation></ref><ref id="B103-sensors-25-05359"><label>103.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Durga</surname></name><name name-style="western"><surname>Ahmad</surname><given-names>N.</given-names></name></person-group><article-title>Enhancing honeybee hive health monitoring: Vision transformer-based non-invasive classification</article-title><source>Proceedings of the 2025 International Conference on Ambient Intelligence in Health Care (ICAIHC)</source><conf-loc>Raipur Chattisgarh, India</conf-loc><conf-date>10&#8211;11 January 2025</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2025</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B104-sensors-25-05359"><label>104.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>De Simone</surname><given-names>A.</given-names></name><name name-style="western"><surname>Barbisan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Turvani</surname><given-names>G.</given-names></name><name name-style="western"><surname>Riente</surname><given-names>F.</given-names></name></person-group><article-title>IoT-based bee colony health monitoring: A focus on energy impact and audio feature extraction</article-title><source>Proceedings of the 2024 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)</source><conf-loc>Padua, Italy</conf-loc><conf-date>29&#8211;31 October 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>289</fpage><lpage>294</lpage></element-citation></ref><ref id="B105-sensors-25-05359"><label>105.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ruvinga</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hunter</surname><given-names>G.</given-names></name><name name-style="western"><surname>Duran</surname><given-names>O.</given-names></name><name name-style="western"><surname>Nebel</surname><given-names>J.C.</given-names></name></person-group><article-title>Identifying queenlessness in honeybee hives from audio signals using machine learning</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>1627</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12071627</pub-id></element-citation></ref><ref id="B106-sensors-25-05359"><label>106.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.G.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.B.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sin</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>C.</given-names></name></person-group><article-title>Identifying an image-processing method for detection of bee mite in honey bee based on keypoint analysis</article-title><source>Agriculture</source><year>2023</year><volume>13</volume><elocation-id>1511</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture13081511</pub-id></element-citation></ref><ref id="B107-sensors-25-05359"><label>107.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dokukin</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Kuznetsova</surname><given-names>A.V.</given-names></name><name name-style="western"><surname>Okulov</surname><given-names>N.V.</given-names></name><name name-style="western"><surname>Senko</surname><given-names>O.V.</given-names></name><name name-style="western"><surname>Chuchupal</surname><given-names>V.Y.</given-names></name></person-group><article-title>Methods of intelligent data analysis in hive state assessment problem</article-title><source>Pattern Recognit. Image Anal.</source><year>2024</year><volume>34</volume><fpage>1271</fpage><lpage>1280</lpage><pub-id pub-id-type="doi">10.1134/S1054661824701347</pub-id></element-citation></ref><ref id="B108-sensors-25-05359"><label>108.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Divas&#243;n</surname><given-names>J.</given-names></name><name name-style="western"><surname>Martinez-de Pison</surname><given-names>F.J.</given-names></name><name name-style="western"><surname>Romero</surname><given-names>A.</given-names></name><name name-style="western"><surname>Santolaria</surname><given-names>P.</given-names></name><name name-style="western"><surname>Y&#225;niz</surname><given-names>J.L.</given-names></name></person-group><article-title>Varroa mite detection using deep learning techniques</article-title><source>Hybrid Artificial Intelligent Systems</source><comment>Lecture Notes in Computer Science</comment><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2023</year><fpage>326</fpage><lpage>337</lpage></element-citation></ref><ref id="B109-sensors-25-05359"><label>109.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andrijevi&#263;</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uro&#353;evi&#263;</surname><given-names>V.</given-names></name><name name-style="western"><surname>Arsi&#263;</surname><given-names>B.</given-names></name><name name-style="western"><surname>Herceg</surname><given-names>D.</given-names></name><name name-style="western"><surname>Savi&#263;</surname><given-names>B.</given-names></name></person-group><article-title>IoT monitoring and prediction modeling of honeybee activity with alarm</article-title><source>Electronics</source><year>2022</year><volume>11</volume><elocation-id>783</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11050783</pub-id></element-citation></ref><ref id="B110-sensors-25-05359"><label>110.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Voudiotis</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kontogiannis</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pikridas</surname><given-names>C.</given-names></name></person-group><article-title>Proposed smart monitoring system for the detection of bee swarming</article-title><source>Inventions</source><year>2021</year><volume>6</volume><elocation-id>87</elocation-id><pub-id pub-id-type="doi">10.3390/inventions6040087</pub-id></element-citation></ref><ref id="B111-sensors-25-05359"><label>111.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Robustillo</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>P&#233;rez</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Parra</surname><given-names>M.I.</given-names></name></person-group><article-title>Predicting internal conditions of beehives using precision beekeeping</article-title><source>Biosyst. Eng.</source><year>2022</year><volume>221</volume><fpage>19</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2022.06.006</pub-id></element-citation></ref><ref id="B112-sensors-25-05359"><label>112.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rafael Braga</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gomes</surname><given-names>D.G.</given-names></name><name name-style="western"><surname>Freitas</surname><given-names>B.M.</given-names></name><name name-style="western"><surname>Cazier</surname><given-names>J.A.</given-names></name></person-group><article-title>A cluster-classification method for accurate mining of seasonal honey bee patterns</article-title><source>Ecol. Inform.</source><year>2020</year><volume>59</volume><fpage>101107</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2020.101107</pub-id></element-citation></ref><ref id="B113-sensors-25-05359"><label>113.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Braga</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Freitas</surname><given-names>B.M.</given-names></name><name name-style="western"><surname>Gomes</surname><given-names>D.G.</given-names></name><name name-style="western"><surname>Bezerra</surname><given-names>A.D.M.</given-names></name><name name-style="western"><surname>Cazier</surname><given-names>J.A.</given-names></name></person-group><article-title>Forecasting sudden drops of temperature in pre-overwintering honeybee colonies</article-title><source>Biosyst. Eng.</source><year>2021</year><volume>209</volume><fpage>315</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2021.07.009</pub-id></element-citation></ref><ref id="B114-sensors-25-05359"><label>114.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Libal</surname><given-names>U.</given-names></name><name name-style="western"><surname>Biernacki</surname><given-names>P.</given-names></name></person-group><article-title>MFCC selection by LASSO for honey bee classification</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>913</elocation-id><pub-id pub-id-type="doi">10.3390/app14020913</pub-id></element-citation></ref><ref id="B115-sensors-25-05359"><label>115.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Minaud</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rebaudo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Mainardi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Vardakas</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hatjina</surname><given-names>F.</given-names></name><name name-style="western"><surname>Steffan-Dewenter</surname><given-names>I.</given-names></name><name name-style="western"><surname>Requier</surname><given-names>F.</given-names></name></person-group><article-title>Temperature in overwintering honey bee colonies reveals brood status and predicts colony mortality</article-title><source>Ecol. Indic.</source><year>2024</year><volume>169</volume><fpage>112961</fpage><pub-id pub-id-type="doi">10.1016/j.ecolind.2024.112961</pub-id></element-citation></ref><ref id="B116-sensors-25-05359"><label>116.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kamga</surname><given-names>G.A.F.</given-names></name><name name-style="western"><surname>Bouroubi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Germain</surname><given-names>M.</given-names></name><name name-style="western"><surname>Martin</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bitjoka</surname><given-names>L.</given-names></name></person-group><article-title>Beekeeping suitability prediction based on an adaptive neuro-fuzzy inference system and apiary level data</article-title><source>Ecol. Inform.</source><year>2025</year><volume>86</volume><fpage>103015</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2025.103015</pub-id></element-citation></ref><ref id="B117-sensors-25-05359"><label>117.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bono</surname><given-names>F.</given-names></name><name name-style="western"><surname>Vallone</surname><given-names>M.</given-names></name><name name-style="western"><surname>Alleri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lo Verde</surname><given-names>G.</given-names></name><name name-style="western"><surname>Orlando</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ragusa</surname><given-names>E.</given-names></name><name name-style="western"><surname>Catania</surname><given-names>P.</given-names></name></person-group><article-title>Hive behaviour assessment through vector autoregressive model by a smart apiculture system in the Mediterranean area</article-title><source>Smart Agric. Technol.</source><year>2024</year><volume>9</volume><fpage>100676</fpage><pub-id pub-id-type="doi">10.1016/j.atech.2024.100676</pub-id></element-citation></ref><ref id="B118-sensors-25-05359"><label>118.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramirez-Diaz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Manunza</surname><given-names>A.</given-names></name><name name-style="western"><surname>de Oliveira</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Bobbo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nutini</surname><given-names>F.</given-names></name><name name-style="western"><surname>Boschetti</surname><given-names>M.</given-names></name><name name-style="western"><surname>De Iorio</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>Pagnacco</surname><given-names>G.</given-names></name><name name-style="western"><surname>Polli</surname><given-names>M.</given-names></name><name name-style="western"><surname>Stella</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Combining environmental variables and machine learning methods to determine the most significant factors influencing honey production</article-title><source>Insects</source><year>2025</year><volume>16</volume><elocation-id>278</elocation-id><pub-id pub-id-type="doi">10.3390/insects16030278</pub-id><pub-id pub-id-type="pmid">40266781</pub-id><pub-id pub-id-type="pmcid">PMC11943014</pub-id></element-citation></ref><ref id="B119-sensors-25-05359"><label>119.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>V&#225;rkonyi</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>B&#225;nyai</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>V&#225;rkonyi-K&#243;czy</surname><given-names>A.R.</given-names></name></person-group><article-title>Investigating traditional machine learning models and the utility of audio features for lightweight swarming prediction in beehives</article-title><source>Acta Polytech. Hung.</source><year>2024</year><volume>21</volume><fpage>283</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.12700/APH.21.10.2024.10.18</pub-id></element-citation></ref><ref id="B120-sensors-25-05359"><label>120.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Robustillo</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Naranjo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Parra</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>P&#233;rez</surname><given-names>C.J.</given-names></name></person-group><article-title>Addressing multidimensional highly correlated data for forecasting in precision beekeeping</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>226</volume><fpage>109390</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109390</pub-id></element-citation></ref><ref id="B121-sensors-25-05359"><label>121.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulyukin</surname><given-names>V.A.</given-names></name><name name-style="western"><surname>Coster</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kulyukin</surname><given-names>A.V.</given-names></name><name name-style="western"><surname>Meikle</surname><given-names>W.</given-names></name><name name-style="western"><surname>Weiss</surname><given-names>M.</given-names></name></person-group><article-title>Discrete time series forecasting of hive weight, in-hive temperature, and hive entrance traffic in non-invasive monitoring of managed honey bee colonies: Part I</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>6433</elocation-id><pub-id pub-id-type="doi">10.3390/s24196433</pub-id><pub-id pub-id-type="pmid">39409473</pub-id><pub-id pub-id-type="pmcid">PMC11479372</pub-id></element-citation></ref><ref id="B122-sensors-25-05359"><label>122.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulyukin</surname><given-names>V.A.</given-names></name><name name-style="western"><surname>Coster</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tkachenko</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hornberger</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kulyukin</surname><given-names>A.V.</given-names></name></person-group><article-title>Ambient electromagnetic radiation as a predictor of honey bee (Apis mellifera) traffic in linear and non-linear regression: Numerical stability, physical time and energy efficiency</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>2584</elocation-id><pub-id pub-id-type="doi">10.3390/s23052584</pub-id><pub-id pub-id-type="pmid">36904786</pub-id><pub-id pub-id-type="pmcid">PMC10007012</pub-id></element-citation></ref><ref id="B123-sensors-25-05359"><label>123.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>V.</given-names></name></person-group><article-title>Development of a predictive model of honey bee foraging activity under different climate conditions</article-title><source>Proceedings of the 2023 11th International Conference on Agro-Geoinformatics (Agro-Geoinformatics)</source><conf-loc>Wuhan, China</conf-loc><conf-date>25&#8211;28 July 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B124-sensors-25-05359"><label>124.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Campell</surname><given-names>C.</given-names></name><name name-style="western"><surname>Parry</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Tashakkori</surname><given-names>R.</given-names></name></person-group><article-title>Non-invasive spectral-based swarm detection</article-title><source>Proceedings of the SoutheastCon 2023</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>1&#8211;16 April 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>253</fpage><lpage>260</lpage></element-citation></ref><ref id="B125-sensors-25-05359"><label>125.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rodias</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kilimpas</surname><given-names>V.</given-names></name></person-group><article-title>Remote monitoring of bee apiaries as a tool for crisis management</article-title><source>AgriEngineering</source><year>2024</year><volume>6</volume><fpage>2269</fpage><lpage>2282</lpage><pub-id pub-id-type="doi">10.3390/agriengineering6030133</pub-id></element-citation></ref><ref id="B126-sensors-25-05359"><label>126.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liyanage</surname><given-names>N.</given-names></name><name name-style="western"><surname>Attanayaka</surname><given-names>C.</given-names></name><name name-style="western"><surname>Perera</surname><given-names>T.</given-names></name><name name-style="western"><surname>Neilkumara</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bandara</surname><given-names>I.S.</given-names></name><name name-style="western"><surname>Chandrasiri</surname><given-names>L.</given-names></name></person-group><article-title>IoT-based smart beehive monitoring system</article-title><source>Proceedings of the 2024 6th International Conference on Advancements in Computing (ICAC)</source><conf-loc>Colombo, Sri Lanka</conf-loc><conf-date>12&#8211;13 December 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>247</fpage><lpage>252</lpage></element-citation></ref><ref id="B127-sensors-25-05359"><label>127.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Smerkol</surname><given-names>M.</given-names></name><name name-style="western"><surname>&#352;e&#353;et</surname><given-names>&#381;.</given-names></name><name name-style="western"><surname>Bregant</surname><given-names>B.</given-names></name><name name-style="western"><surname>Simon&#269;i&#269;</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fin&#382;gar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gradi&#353;ek</surname><given-names>A.</given-names></name></person-group><article-title>Smart beehive monitoring system for identification of relevant beehive events</article-title><source>Proceedings of the 2024 International Conference on Intelligent Environments (IE)</source><conf-loc>Ljubljana, Slovenia</conf-loc><conf-date>17&#8211;20 June 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year></element-citation></ref><ref id="B128-sensors-25-05359"><label>128.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vit</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Aronson</surname><given-names>Y.</given-names></name></person-group><article-title>Automatic detection of honey in hive frames using deep learning</article-title><source>Proceedings of the World Congress on Electrical Engineering and Computer Systems and Science</source><conf-loc>London, UK</conf-loc><conf-date>3&#8211;5 August 2023</conf-date><publisher-name>Avestia Publishing</publisher-name><publisher-loc>Orleans, ON, Canada</publisher-loc><year>2023</year></element-citation></ref><ref id="B129-sensors-25-05359"><label>129.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Phan</surname><given-names>T.T.H.</given-names></name><name name-style="western"><surname>Nguyen-Doan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nguyen-Huu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nguyen-Van</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pham-Hong</surname><given-names>T.</given-names></name></person-group><article-title>Investigation on new Mel frequency cepstral coefficients features and hyper-parameters tuning technique for bee sound recognition</article-title><source>Soft Comput.</source><year>2023</year><volume>27</volume><fpage>5873</fpage><lpage>5892</lpage><pub-id pub-id-type="doi">10.1007/s00500-022-07596-6</pub-id></element-citation></ref><ref id="B130-sensors-25-05359"><label>130.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Grammalidis</surname><given-names>N.</given-names></name><name name-style="western"><surname>Stergioulas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Avramidis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Karystinakis</surname><given-names>K.</given-names></name><name name-style="western"><surname>Partozis</surname><given-names>T.</given-names></name><name name-style="western"><surname>Topaloudis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kalantzi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Tananaki</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kanelis</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liolios</surname><given-names>V.</given-names></name><etal/></person-group><article-title>A smart beekeeping platform based on remote sensing and artificial intelligence</article-title><source>Proceedings of the Ninth International Conference on Remote Sensing and Geoinformation of the Environment (RSCy2023)</source><conf-loc>Ayia Napa, Cyprus</conf-loc><conf-date>3&#8211;5 April 2023</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Themistocleous</surname><given-names>K.</given-names></name><name name-style="western"><surname>Michaelides</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hadjimitsis</surname><given-names>D.G.</given-names></name><name name-style="western"><surname>Papadavid</surname><given-names>G.</given-names></name></person-group><publisher-name>SPIE</publisher-name><publisher-loc>Bellingham, WA, USA</publisher-loc><year>2023</year><fpage>47</fpage></element-citation></ref><ref id="B131-sensors-25-05359"><label>131.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Florea</surname><given-names>G.</given-names></name><name name-style="western"><surname>Codreanu</surname><given-names>N.</given-names></name></person-group><article-title>Sensor-driven motorized solution for beehive entrance control</article-title><source>Proceedings of the 2024 9th International Conference on Energy Efficiency and Agricultural Engineering (EE&amp;AE)</source><conf-loc>Ruse, Bulgaria</conf-loc><conf-date>27&#8211;29 June 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year></element-citation></ref><ref id="B132-sensors-25-05359"><label>132.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vallone</surname><given-names>M.</given-names></name><name name-style="western"><surname>Orlando</surname><given-names>S.</given-names></name><name name-style="western"><surname>Alleri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ferro</surname><given-names>M.V.</given-names></name><name name-style="western"><surname>Catania</surname><given-names>P.</given-names></name></person-group><article-title>Honey Production with Remote Smart Monitoring System</article-title><source>Chem. Eng. Trans.</source><year>2023</year><volume>102</volume><fpage>169</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.3303/CET23102029</pub-id></element-citation></ref><ref id="B133-sensors-25-05359"><label>133.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>D.H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>W.W.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y.L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>T.Y.</given-names></name></person-group><article-title>New paradigm for beehive monitoring system using infrared and power line communication</article-title><source>IEEE Photonics J.</source><year>2025</year><volume>17</volume><fpage>7300709</fpage><pub-id pub-id-type="doi">10.1109/JPHOT.2025.3552600</pub-id></element-citation></ref><ref id="B134-sensors-25-05359"><label>134.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Alifieris</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chamaidi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Malisova</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mamalis</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nomikos</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rigakis</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vlachogiannis</surname><given-names>E.</given-names></name><name name-style="western"><surname>Stavrakis</surname><given-names>M.</given-names></name></person-group><article-title>IOHIVE: Architecture and infrastructure of an IOT system for beehive monitoring and an interactive journaling wearable device for beekeepers</article-title><source>Computational Science and Its Applications</source><publisher-name>Lecture Notes in Computer Science; Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2023</year><fpage>133</fpage><lpage>149</lpage></element-citation></ref><ref id="B135-sensors-25-05359"><label>135.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rafael Braga</surname><given-names>A.</given-names></name><name name-style="western"><surname>Arruda Fontenele</surname><given-names>T.</given-names></name><name name-style="western"><surname>Guimar&#227;es Al-Alam</surname><given-names>W.</given-names></name><name name-style="western"><surname>de Carvalho Silva</surname><given-names>J.</given-names></name></person-group><article-title>Prototyping a system for detection and notification of damage or theft in beehives</article-title><source>Ecol. Inform.</source><year>2023</year><volume>75</volume><fpage>102015</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2023.102015</pub-id></element-citation></ref><ref id="B136-sensors-25-05359"><label>136.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chien</surname><given-names>H.Y.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>T.W.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.Y.</given-names></name><name name-style="western"><surname>Tsai</surname><given-names>P.J.</given-names></name></person-group><article-title>YOLO-based bee-hornet real-time notification</article-title><source>Proceedings of the 2024 IEEE International Conference on E-health Networking, Application &amp; Services (HealthCom)</source><conf-loc>Nara, Japan</conf-loc><conf-date>18&#8211;20 November 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage></element-citation></ref><ref id="B137-sensors-25-05359"><label>137.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hall</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bencsik</surname><given-names>M.</given-names></name><name name-style="western"><surname>Capela</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sousa</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>de Graaf</surname><given-names>D.C.</given-names></name></person-group><article-title>Remote and automated detection of Asian hornets (Vespa velutina nigrithorax) at an apiary, using spectral features of their hovering flight sounds</article-title><source>Comput. Electron. Agric.</source><year>2025</year><volume>235</volume><fpage>110307</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2025.110307</pub-id></element-citation></ref><ref id="B138-sensors-25-05359"><label>138.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeon</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.B.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>I.</given-names></name></person-group><article-title>Deep learning-based portable image analysis system for real-time detection of Vespa velutina</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>7414</elocation-id><pub-id pub-id-type="doi">10.3390/app13137414</pub-id></element-citation></ref><ref id="B139-sensors-25-05359"><label>139.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nasir</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ullah</surname><given-names>M.O.</given-names></name><name name-style="western"><surname>Yousaf</surname><given-names>M.H.</given-names></name></person-group><article-title>AI in apiculture: A novel framework for recognition of invasive insects under unconstrained flying conditions for smart beehives</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>119</volume><fpage>105784</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2022.105784</pub-id></element-citation></ref><ref id="B140-sensors-25-05359"><label>140.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mar&#237;a-Luisa</surname><given-names>P.D.</given-names></name><name name-style="western"><surname>Jes&#250;s-&#193;ngel</surname><given-names>R.G.</given-names></name></person-group><article-title>Deep Learning for Vespa Velutina Detection</article-title><source>Proceedings of the 2024 2nd International Conference on Machine Vision, Image Processing &amp; Imaging Technology (MVIPIT)</source><conf-loc>Zhangjiakou, China</conf-loc><conf-date>13&#8211;15 September 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>216</fpage><lpage>221</lpage></element-citation></ref><ref id="B141-sensors-25-05359"><label>141.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nolasco</surname><given-names>I.</given-names></name><name name-style="western"><surname>Benetos</surname><given-names>E.</given-names></name></person-group><source>To Bee or not to Bee: An Annotated Dataset for Beehive Sound Recognition</source><publisher-name>Zenodo Dataset</publisher-name><publisher-loc>Gen&#232;ve, Switzerland</publisher-loc><year>2018</year><pub-id pub-id-type="doi">10.5281/zenodo.1321278</pub-id></element-citation></ref><ref id="B142-sensors-25-05359"><label>142.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nolasco</surname><given-names>I.</given-names></name><name name-style="western"><surname>Terenzi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cecchi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Orcioni</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bear</surname><given-names>H.L.</given-names></name><name name-style="western"><surname>Benetos</surname><given-names>E.</given-names></name></person-group><source>Audio-Based Identification of Beehive States: The Dataset</source><publisher-name>Zenodo Dataset</publisher-name><publisher-loc>Gen&#232;ve, Switzerland</publisher-loc><year>2019</year><pub-id pub-id-type="doi">10.5281/zenodo.2667806</pub-id></element-citation></ref><ref id="B143-sensors-25-05359"><label>143.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jyang</surname><given-names>A.</given-names></name></person-group><source>Smart Bee Colony Monitor: Clips of Beehive Sounds</source><publisher-name>Kaggle</publisher-name><publisher-loc>San Francisco, CA, USA</publisher-loc><year>2021</year></element-citation></ref><ref id="B144-sensors-25-05359"><label>144.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Robles-Guerrero</surname><given-names>A.</given-names></name><name name-style="western"><surname>Saucedo-Anaya</surname><given-names>T.</given-names></name><name name-style="western"><surname>Gonzalez</surname><given-names>E.</given-names></name><name name-style="western"><surname>de la Rosa</surname><given-names>J.I.</given-names></name></person-group><source>Queenless Honeybee Acoustic Patterns</source><publisher-name>Elsevier Inc.</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2022</year><pub-id pub-id-type="doi">10.17632/t9prmbmdfn.1</pub-id></element-citation></ref><ref id="B145-sensors-25-05359"><label>145.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sledevic</surname><given-names>T.</given-names></name></person-group><source>Labeled Dataset for Bee Detection and Direction Estimation on Beehive Landing Boards</source><publisher-name>Elsevier Inc.</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2024</year><pub-id pub-id-type="doi">10.17632/8gb9r2yhfc.6</pub-id><pub-id pub-id-type="pmcid">PMC10831503</pub-id><pub-id pub-id-type="pmid">38304387</pub-id></element-citation></ref><ref id="B146-sensors-25-05359"><label>146.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Divas&#243;n</surname><given-names>J.</given-names></name><name name-style="western"><surname>Romero</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mart&#237;nez de Pis&#243;n</surname><given-names>F.J.</given-names></name><name name-style="western"><surname>Silvestre</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Santolaria</surname><given-names>P.</given-names></name><name name-style="western"><surname>Y&#225;niz</surname><given-names>J.L.</given-names></name></person-group><source>Dataset for Varroa Mite Detection on Sticky Boards</source><publisher-name>Zenodo Dataset</publisher-name><publisher-loc>Gen&#232;ve, Switzerland</publisher-loc><year>2023</year><pub-id pub-id-type="doi">10.5281/zenodo.10231845</pub-id></element-citation></ref><ref id="B147-sensors-25-05359"><label>147.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Vietnam Agricultural Academy</collab></person-group><article-title>The VnPollenBee Dataset. Dataset</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://comvis-hust.github.io/datasets/pollenbee.html" ext-link-type="uri">https://comvis-hust.github.io/datasets/pollenbee.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-12">(accessed on 12 June 2025)</date-in-citation></element-citation></ref><ref id="B148-sensors-25-05359"><label>148.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><source>The BeeImage Dataset: Annotated Honey Bee Images</source><publisher-name>Kaggle</publisher-name><publisher-loc>San Francisco, CA, USA</publisher-loc><year>2020</year></element-citation></ref><ref id="B149-sensors-25-05359"><label>149.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dupont</surname><given-names>Y.L.</given-names></name><name name-style="western"><surname>Capela</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kryger</surname><given-names>P.</given-names></name><name name-style="western"><surname>Alves</surname><given-names>J.</given-names></name><name name-style="western"><surname>Axelsen</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Greve</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Bruus</surname><given-names>M.</given-names></name><name name-style="western"><surname>Castro</surname><given-names>S.</given-names></name><name name-style="western"><surname>Frederiksen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Groom</surname><given-names>G.B.</given-names></name><etal/></person-group><source>Research Project on Field Data Collection for Honey Bee Colony Model Evaluation&#8212;Datasets</source><publisher-name>Zenodo Dataset</publisher-name><publisher-loc>Gen&#232;ve, Switzerland</publisher-loc><year>2021</year><pub-id pub-id-type="doi">10.5281/zenodo.5148318</pub-id></element-citation></ref><ref id="B150-sensors-25-05359"><label>150.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Newton</surname><given-names>M.</given-names></name></person-group><article-title>Winter Carbon Dioxide Measurements in UK Honeybee Hives 2022/2023</article-title><comment>Dataset</comment><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://figshare.com/articles/dataset/Winter_carbon_dioxide_measurements_in_UK_honeybee_hives_2022_2023/24411595" ext-link-type="uri">https://figshare.com/articles/dataset/Winter_carbon_dioxide_measurements_in_UK_honeybee_hives_2022_2023/24411595</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-12">(accessed on 12 June 2025)</date-in-citation></element-citation></ref><ref id="B151-sensors-25-05359"><label>151.</label><element-citation publication-type="gov"><person-group person-group-type="author"><collab>NASA Langley Research Center</collab></person-group><article-title>NASA POWER: Prediction of Worldwide Energy Resources</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://power.larc.nasa.gov/" ext-link-type="uri">https://power.larc.nasa.gov/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-12">(accessed on 12 June 2025)</date-in-citation></element-citation></ref><ref id="B152-sensors-25-05359"><label>152.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Naturami</surname></name><name name-style="western"><surname>Varkonyi</surname><given-names>D.T.</given-names></name></person-group><source>Beehive Audio Recordings</source><publisher-name>Zenodo Dataset</publisher-name><publisher-loc>Gen&#232;ve, Switzerland</publisher-loc><year>2022</year><pub-id pub-id-type="doi">10.5281/zenodo.7052981</pub-id></element-citation></ref><ref id="B153-sensors-25-05359"><label>153.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Newton</surname><given-names>M.</given-names></name></person-group><source>Winter CO<sub>2</sub> Measurements in UK Honeybee Hives (2022&#8211;2023)</source><publisher-name>Figshare Dataset</publisher-name><publisher-loc>London, UK</publisher-loc><year>2023</year></element-citation></ref><ref id="B154-sensors-25-05359"><label>154.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rodi&#263;</surname><given-names>L.D.</given-names></name><name name-style="western"><surname>&#381;upanovi&#263;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Perkovi&#263;</surname><given-names>T.</given-names></name><name name-style="western"><surname>&#352;oli&#263;</surname><given-names>P.</given-names></name><name name-style="western"><surname>Rodrigues</surname><given-names>J.J.P.C.</given-names></name></person-group><article-title>Machine Learning and Soil Humidity Sensing: Signal Strength Approach</article-title><source>ACM Trans. Internet Technol.</source><year>2021</year><volume>22</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3418207</pub-id></element-citation></ref><ref id="B155-sensors-25-05359"><label>155.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Duji&#263; Rodi&#263;</surname><given-names>L.</given-names></name><name name-style="western"><surname>Perkovi&#263;</surname><given-names>T.</given-names></name><name name-style="western"><surname>&#352;kiljo</surname><given-names>M.</given-names></name><name name-style="western"><surname>&#352;oli&#263;</surname><given-names>P.</given-names></name></person-group><article-title>Privacy leakage of LoRaWAN smart parking occupancy sensors</article-title><source>Future Gener. Comput. Syst.</source><year>2023</year><volume>138</volume><fpage>142</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/j.future.2022.08.007</pub-id></element-citation></ref><ref id="B156-sensors-25-05359"><label>156.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Perkovi&#263;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Duji&#263; Rodi&#263;</surname><given-names>L.</given-names></name><name name-style="western"><surname>&#352;abi&#263;</surname><given-names>J.</given-names></name><name name-style="western"><surname>&#352;oli&#263;</surname><given-names>P.</given-names></name></person-group><article-title>Machine Learning Approach towards LoRaWAN Indoor Localization</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>457</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12020457</pub-id></element-citation></ref><ref id="B157-sensors-25-05359"><label>157.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>NASA Langley POWER Project</collab></person-group><source>Prediction of Worldwide Energy Resources (POWER) v10</source><publisher-name>NASA POWER Platform</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2024</year></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05359-f001" orientation="portrait"><label>Figure 1</label><caption><p>Structure of the data extraction matrix used to encode publications.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g001.jpg"/></fig><fig position="float" id="sensors-25-05359-f002" orientation="portrait"><label>Figure 2</label><caption><p>Overview of the classification taxonomy used during the data extraction process. Orange arrows indicate the typical flow starting from sensor measurements; the blue arrow denotes studies that bypass data acquisition and start from existing datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g002.jpg"/></fig><fig position="float" id="sensors-25-05359-f003" orientation="portrait"><label>Figure 3</label><caption><p>Flow diagram illustrating the publication identification and screening process following PRISMA guidelines (template adapted from Page et al. [<xref rid="B21-sensors-25-05359" ref-type="bibr">21</xref>], CC BY 4.0).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g003.jpg"/></fig><fig position="float" id="sensors-25-05359-f004" orientation="portrait"><label>Figure 4</label><caption><p>Distribution of publication types.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g004.jpg"/></fig><fig position="float" id="sensors-25-05359-f005" orientation="portrait"><label>Figure 5</label><caption><p>Number of reviewed publications by journal and year.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g005.jpg"/></fig><fig position="float" id="sensors-25-05359-f006" orientation="portrait"><label>Figure 6</label><caption><p>Number of reviewed publications by conference and year.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g006.jpg"/></fig><fig position="float" id="sensors-25-05359-f007" orientation="portrait"><label>Figure 7</label><caption><p>Distribution of primary research goals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g007.jpg"/></fig><fig position="float" id="sensors-25-05359-f008" orientation="portrait"><label>Figure 8</label><caption><p>Combined visualization of publication trends for the four most common research goal categories. (<bold>a</bold>) shows the yearly evolution from 2020 to 2024, while (<bold>b</bold>) highlights current progress in 2025.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g008.jpg"/></fig><fig position="float" id="sensors-25-05359-f009" orientation="portrait"><label>Figure 9</label><caption><p>Word cloud of the most frequent terms appearing in abstracts of publications.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g009.jpg"/></fig><fig position="float" id="sensors-25-05359-f010" orientation="portrait"><label>Figure 10</label><caption><p>Prevalence of different sensor types in smart beehive studies.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g010.jpg"/></fig><fig position="float" id="sensors-25-05359-f011" orientation="portrait"><label>Figure 11</label><caption><p>Distribution of smart hive studies by number of distinct sensor types used.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g011.jpg"/></fig><fig position="float" id="sensors-25-05359-f012" orientation="portrait"><label>Figure 12</label><caption><p>Frequency of various data analysis and machine learning method categories in the literature.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g012.jpg"/></fig><fig position="float" id="sensors-25-05359-f013" orientation="portrait"><label>Figure 13</label><caption><p>Temporal evolution of key data analysis methods used in smart beehive studies.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g013.jpg"/></fig><fig position="float" id="sensors-25-05359-f014" orientation="portrait"><label>Figure 14</label><caption><p>Heatmap showing the usage frequency of each sensor modality within each main goal category of studies.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g014.jpg"/></fig><fig position="float" id="sensors-25-05359-f015" orientation="portrait"><label>Figure 15</label><caption><p>Sensor modality co-occurrence matrix across smart beehive studies.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g015.jpg"/></fig><fig position="float" id="sensors-25-05359-f016" orientation="portrait"><label>Figure 16</label><caption><p>Heatmap showing the co-occurrence between sensor modalities and machine learning (ML) model categories across surveyed smart-beehive systems.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g016.jpg"/></fig><fig position="float" id="sensors-25-05359-f017" orientation="portrait"><label>Figure 17</label><caption><p>Co-occurrence between sensor categories and communication types across reviewed smart beehive systems.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05359-g017.jpg"/></fig><table-wrap position="float" id="sensors-25-05359-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t001_Table 1</object-id><label>Table 1</label><caption><p>Eligibility criteria for systematic review of smart beehive technologies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Criteria</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Type of Data</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Studies must report on environmental, acoustic, visual, or multisensory data collected from within or around beehives, supporting sensor-based monitoring or data-driven analysis.</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Algorithms or Techniques</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">While not a mandatory component, the adoption of data-driven approaches is widely considered advantageous for deriving structured insights from sensor observations and facilitating evidence-based interpretations in smart beekeeping research.</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Comparator</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">RQ1: Types of sensor modalities used. <break/>RQ2: Application domains. <break/>RQ3: Categories of ML and analytical methods used and trends in their adoption over time. <break/>RQ4: Reported technical and practical limitations, including system cost, data quality, power consumption, and deployment challenges. <break/>RQ5: Usage of publicly available datasets, categorized by data modality, labeling approach, and their role in model training or evaluation.</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Outcome</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Detailed characterization of smart beehive systems, including sensor setups, communication methods, ML/AI techniques, goals and reported limitations.</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Timing</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Articles published from January 1990 to April 2025.</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Environmental or Geographical Context</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">No restrictions; studies from any geographic region are&#160;considered.</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Publication Type</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Peer-reviewed journal articles and conference papers published in English.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05359-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t002_Table 2</object-id><label>Table 2</label><caption><p>Search strategy and number of retrieved records per database.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Database</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Search Query</th></tr></thead><tbody><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Web of Science</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ALL = (( (precision OR smart OR intelligent OR automated) AND (beekeeping OR beehive OR apiculture OR apiary) ) OR &#8220;precision beekeeping&#8221; OR &#8220;smart beehive&#8221;) AND DT==(&#8220;ARTICLE&#8221; OR &#8220;PROCEEDINGS PAPER&#8221;) AND DOP=1990-01-01/2025-04-07</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">IEEE Xplore</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(&#8220;All Metadata&#8221;:&#8220;precision beekeeping&#8221; OR &#8220;All Metadata&#8221;:&#8220;smart beehive&#8221; OR ( (&#8220;All Metadata&#8221;:&#8220;precision&#8221; OR &#8220;smart&#8221; OR &#8220;intelligent&#8221; OR &#8220;automated&#8221;) AND (&#8220;All Metadata&#8221;:&#8220;beekeeping&#8221; OR &#8220;beehive&#8221; OR &#8220;apiculture&#8221; OR &#8220;apiary&#8221;) ) ) AND (&#8220;ContentType&#8221;:&#8220;Journals&#8221; OR &#8220;ContentType&#8221;:&#8220;Conferences&#8221;)</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Scopus</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TITLE-ABS-KEY( ( ( precision OR smart OR intelligent OR automated ) AND ( beekeeping OR beehive OR apiculture OR apiary ) ) OR &#8220;precision beekeeping&#8221; OR &#8220;smart beehive&#8221; ) AND PUBYEAR &gt; 1990 AND ( LIMIT-TO ( DOCTYPE,&#8220;ar&#8221; ) OR LIMIT-TO ( DOCTYPE,&#8220;cp&#8221; ) ) AND ( LIMIT-TO ( LANGUAGE,&#8220;English&#8221; ) )</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05359-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t003_Table 3</object-id><label>Table 3</label><caption><p>Mapping of included studies to their primary research goal categories.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Main Goal Category</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Publications</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Monitoring</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;[<xref rid="B4-sensors-25-05359" ref-type="bibr">4</xref>,<xref rid="B7-sensors-25-05359" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05359" ref-type="bibr">8</xref>,<xref rid="B13-sensors-25-05359" ref-type="bibr">13</xref>,<xref rid="B15-sensors-25-05359" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05359" ref-type="bibr">16</xref>,<xref rid="B22-sensors-25-05359" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05359" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05359" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05359" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05359" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05359" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05359" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05359" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05359" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05359" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05359" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05359" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05359" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05359" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05359" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05359" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05359" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05359" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05359" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05359" ref-type="bibr">41</xref>,<xref rid="B42-sensors-25-05359" ref-type="bibr">42</xref>]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Behavior Detection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;[<xref rid="B9-sensors-25-05359" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05359" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-05359" ref-type="bibr">12</xref>,<xref rid="B43-sensors-25-05359" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05359" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05359" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05359" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05359" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-05359" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05359" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-05359" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-05359" ref-type="bibr">51</xref>,<xref rid="B52-sensors-25-05359" ref-type="bibr">52</xref>,<xref rid="B53-sensors-25-05359" ref-type="bibr">53</xref>,<xref rid="B54-sensors-25-05359" ref-type="bibr">54</xref>,<xref rid="B55-sensors-25-05359" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-05359" ref-type="bibr">56</xref>,<xref rid="B57-sensors-25-05359" ref-type="bibr">57</xref>,<xref rid="B58-sensors-25-05359" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-05359" ref-type="bibr">59</xref>,<xref rid="B60-sensors-25-05359" ref-type="bibr">60</xref>,<xref rid="B61-sensors-25-05359" ref-type="bibr">61</xref>,<xref rid="B62-sensors-25-05359" ref-type="bibr">62</xref>,<xref rid="B63-sensors-25-05359" ref-type="bibr">63</xref>,<xref rid="B64-sensors-25-05359" ref-type="bibr">64</xref>,<xref rid="B65-sensors-25-05359" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-05359" ref-type="bibr">66</xref>,<xref rid="B67-sensors-25-05359" ref-type="bibr">67</xref>,<xref rid="B68-sensors-25-05359" ref-type="bibr">68</xref>,<xref rid="B69-sensors-25-05359" ref-type="bibr">69</xref>,<xref rid="B70-sensors-25-05359" ref-type="bibr">70</xref>,<xref rid="B71-sensors-25-05359" ref-type="bibr">71</xref>,<xref rid="B72-sensors-25-05359" ref-type="bibr">72</xref>,<xref rid="B73-sensors-25-05359" ref-type="bibr">73</xref>,<xref rid="B74-sensors-25-05359" ref-type="bibr">74</xref>]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Health Assessment</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;[<xref rid="B11-sensors-25-05359" ref-type="bibr">11</xref>,<xref rid="B14-sensors-25-05359" ref-type="bibr">14</xref>,<xref rid="B75-sensors-25-05359" ref-type="bibr">75</xref>,<xref rid="B76-sensors-25-05359" ref-type="bibr">76</xref>,<xref rid="B77-sensors-25-05359" ref-type="bibr">77</xref>,<xref rid="B78-sensors-25-05359" ref-type="bibr">78</xref>,<xref rid="B79-sensors-25-05359" ref-type="bibr">79</xref>,<xref rid="B80-sensors-25-05359" ref-type="bibr">80</xref>,<xref rid="B81-sensors-25-05359" ref-type="bibr">81</xref>,<xref rid="B82-sensors-25-05359" ref-type="bibr">82</xref>,<xref rid="B83-sensors-25-05359" ref-type="bibr">83</xref>,<xref rid="B84-sensors-25-05359" ref-type="bibr">84</xref>,<xref rid="B85-sensors-25-05359" ref-type="bibr">85</xref>,<xref rid="B86-sensors-25-05359" ref-type="bibr">86</xref>,<xref rid="B87-sensors-25-05359" ref-type="bibr">87</xref>,<xref rid="B88-sensors-25-05359" ref-type="bibr">88</xref>,<xref rid="B89-sensors-25-05359" ref-type="bibr">89</xref>,<xref rid="B90-sensors-25-05359" ref-type="bibr">90</xref>,<xref rid="B91-sensors-25-05359" ref-type="bibr">91</xref>,<xref rid="B92-sensors-25-05359" ref-type="bibr">92</xref>,<xref rid="B93-sensors-25-05359" ref-type="bibr">93</xref>,<xref rid="B94-sensors-25-05359" ref-type="bibr">94</xref>,<xref rid="B95-sensors-25-05359" ref-type="bibr">95</xref>,<xref rid="B96-sensors-25-05359" ref-type="bibr">96</xref>,<xref rid="B97-sensors-25-05359" ref-type="bibr">97</xref>,<xref rid="B98-sensors-25-05359" ref-type="bibr">98</xref>,<xref rid="B99-sensors-25-05359" ref-type="bibr">99</xref>,<xref rid="B100-sensors-25-05359" ref-type="bibr">100</xref>,<xref rid="B101-sensors-25-05359" ref-type="bibr">101</xref>,<xref rid="B102-sensors-25-05359" ref-type="bibr">102</xref>,<xref rid="B103-sensors-25-05359" ref-type="bibr">103</xref>,<xref rid="B104-sensors-25-05359" ref-type="bibr">104</xref>,<xref rid="B105-sensors-25-05359" ref-type="bibr">105</xref>,<xref rid="B106-sensors-25-05359" ref-type="bibr">106</xref>,<xref rid="B107-sensors-25-05359" ref-type="bibr">107</xref>,<xref rid="B108-sensors-25-05359" ref-type="bibr">108</xref>]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prediction/Forecasting</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;[<xref rid="B2-sensors-25-05359" ref-type="bibr">2</xref>,<xref rid="B109-sensors-25-05359" ref-type="bibr">109</xref>,<xref rid="B110-sensors-25-05359" ref-type="bibr">110</xref>,<xref rid="B111-sensors-25-05359" ref-type="bibr">111</xref>,<xref rid="B112-sensors-25-05359" ref-type="bibr">112</xref>,<xref rid="B113-sensors-25-05359" ref-type="bibr">113</xref>,<xref rid="B114-sensors-25-05359" ref-type="bibr">114</xref>,<xref rid="B115-sensors-25-05359" ref-type="bibr">115</xref>,<xref rid="B116-sensors-25-05359" ref-type="bibr">116</xref>,<xref rid="B117-sensors-25-05359" ref-type="bibr">117</xref>,<xref rid="B118-sensors-25-05359" ref-type="bibr">118</xref>,<xref rid="B119-sensors-25-05359" ref-type="bibr">119</xref>,<xref rid="B120-sensors-25-05359" ref-type="bibr">120</xref>,<xref rid="B121-sensors-25-05359" ref-type="bibr">121</xref>,<xref rid="B122-sensors-25-05359" ref-type="bibr">122</xref>,<xref rid="B123-sensors-25-05359" ref-type="bibr">123</xref>,<xref rid="B124-sensors-25-05359" ref-type="bibr">124</xref>]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimization/Decision Support</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;[<xref rid="B125-sensors-25-05359" ref-type="bibr">125</xref>,<xref rid="B126-sensors-25-05359" ref-type="bibr">126</xref>,<xref rid="B127-sensors-25-05359" ref-type="bibr">127</xref>,<xref rid="B128-sensors-25-05359" ref-type="bibr">128</xref>,<xref rid="B129-sensors-25-05359" ref-type="bibr">129</xref>,<xref rid="B130-sensors-25-05359" ref-type="bibr">130</xref>,<xref rid="B131-sensors-25-05359" ref-type="bibr">131</xref>,<xref rid="B132-sensors-25-05359" ref-type="bibr">132</xref>]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">System/IoT Development</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;[<xref rid="B1-sensors-25-05359" ref-type="bibr">1</xref>,<xref rid="B3-sensors-25-05359" ref-type="bibr">3</xref>,<xref rid="B5-sensors-25-05359" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05359" ref-type="bibr">6</xref>,<xref rid="B133-sensors-25-05359" ref-type="bibr">133</xref>,<xref rid="B134-sensors-25-05359" ref-type="bibr">134</xref>,<xref rid="B135-sensors-25-05359" ref-type="bibr">135</xref>]</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Threat Detection</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#160;[<xref rid="B136-sensors-25-05359" ref-type="bibr">136</xref>,<xref rid="B137-sensors-25-05359" ref-type="bibr">137</xref>,<xref rid="B138-sensors-25-05359" ref-type="bibr">138</xref>,<xref rid="B139-sensors-25-05359" ref-type="bibr">139</xref>,<xref rid="B140-sensors-25-05359" ref-type="bibr">140</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05359-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t004_Table 4</object-id><label>Table 4</label><caption><p>Adoption of analytical methods across publication periods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Publication Period</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total Publications</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Used Analytical Methods</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">% with Methods</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Before 2015</td><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">0</td><td align="left" valign="middle" rowspan="1" colspan="1">0.0%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2015&#8211;2018</td><td align="left" valign="middle" rowspan="1" colspan="1">16</td><td align="left" valign="middle" rowspan="1" colspan="1">11</td><td align="left" valign="middle" rowspan="1" colspan="1">68.8%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2019&#8211;2022</td><td align="left" valign="middle" rowspan="1" colspan="1">32</td><td align="left" valign="middle" rowspan="1" colspan="1">26</td><td align="left" valign="middle" rowspan="1" colspan="1">81.2%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023&#8211;2025</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.3%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05359-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t005_Table 5</object-id><label>Table 5</label><caption><p>Representative sensor types used in smart beehive studies, including typical accuracy and approximate cost.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensor Category</th><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Example Device/Modality</th><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Typical Accuracy</th><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Approx. Cost <italic toggle="yes"><sup>a</sup></italic></th></tr></thead><tbody><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Temperature (internal or external)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">DS18B20 digital probe</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#177;0.5 &#176;C</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">$2&#8211;5 per sensor</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Weight/load sensing</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Four strain-gauge load cells with HX711 ADC</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#177;</mml:mo><mml:mn>0.1</mml:mn><mml:mspace width="0.166667em"/><mml:mi>kg</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (approx. 0.02% full scale)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">$20&#8211;30 for four sensors</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic/vibration</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Electret microphone (audio sampling for soundscape)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency response 20&#160;Hz&#8211;20&#160;kHz; no intrinsic accuracy but sensitivity of <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>44</mml:mn><mml:mspace width="0.166667em"/><mml:mi>dB</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">$5&#8211;10 per sensor</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Imaging</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Raspberry Pi camera V2 (8 MP) or USB webcam</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">1080p resolution; shutter speeds down to 30 &#181;s</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">$25&#8211;35 per camera</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Air composition</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">MQ-135 CO<sub>2</sub> sensor or Figaro TGS series</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#177;(100 ppm + 5% of reading) for CO<sub>2</sub> concentration</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">$10&#8211;20 per sensor</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Bee activity counters</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrared gate or RFID tag</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Counting accuracy 90&#8211;95% (dependent on traffic)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">$15&#8211;25 per channel</td></tr></tbody></table><table-wrap-foot><fn><p><italic toggle="yes"><sup>a</sup></italic> Retail prices in USD, as of 2025.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05359-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t006_Table 6</object-id><label>Table 6</label><caption><p>Reported performance metrics for exemplar smart-beehive algorithms. Metrics correspond to the best models reported in the cited studies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Application Task</th><th align="left" valign="top" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reported Performance</th></tr></thead><tbody><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Queen absence/presence detection<break/>(microclimate or audio)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Achieved &gt;97% accuracy using MFCC features in int16;&#160;93% with STFT in int32&#160;[<xref rid="B95-sensors-25-05359" ref-type="bibr">95</xref>]; Microclimate dataset: KNN, MLP, SVM: 100% accuracy; Bioacustic dataset: MLP: 98.2% accuracy&#160;[<xref rid="B93-sensors-25-05359" ref-type="bibr">93</xref>]; CNNs (e.g., ResNet-50) achieved up to 99% accuracy&#160;[<xref rid="B91-sensors-25-05359" ref-type="bibr">91</xref>].</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Drone vs. worker beeclassification (audio)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">99.88% accuracy using Random Forest and 99.68% using KNN&#160;[<xref rid="B53-sensors-25-05359" ref-type="bibr">53</xref>]; MUSIC&#160;+&#160;NN3&#160;+&#160;T*:&#160;99.97%, GTCC + NN3 + T*: 99.94%, Burg/MFCC + NN4 + T*:&#160;&#8805;99.85%&#160;[<xref rid="B61-sensors-25-05359" ref-type="bibr">61</xref>]; Burg method (parametric PSC): Accuracy = 95.9%, Blackman-Tukey method: Accuracy = 94.79%&#160;[<xref rid="B71-sensors-25-05359" ref-type="bibr">71</xref>].</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Swarm prediction/weight forecasting</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Best LSTM performance was achieved with a 2-h prediction window, using a 4-hour input window, where RMSE ranged from 0.042 &#176;C to 0.217 &#176;C across hives&#160;[<xref rid="B113-sensors-25-05359" ref-type="bibr">113</xref>]; Vector Error Correction Model (VEC) outperformed other models in most cases, showing: 1-day ahead MAEs: Temperature: 0.6&#8211;2.4 &#176;C, Humidity: 2.4&#8211;10.9%, Weight: 63&#8211;178 g (3-day ahead predictions remained within similar error margins.)&#160;[<xref rid="B120-sensors-25-05359" ref-type="bibr">120</xref>]; All model types (ANN, CNN, LSTM, ARIMA) were able to predict short-term and long-term trends of thethree&#160;variables [<xref rid="B121-sensors-25-05359" ref-type="bibr">121</xref>].</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Bee counting in images</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">This study used a dataset of 2300 annotated images and 7200 frames, training YOLOv8 to detect bees with high accuracy and robustness under variable lighting. The best model achieved a mean Average Precision (mAP@0.5) of 0.948, an F1-score of 0.91, and precision of 1.00 at a confidence threshold of 0.838.&#160;[<xref rid="B56-sensors-25-05359" ref-type="bibr">56</xref>]; Best pipeline: YOLOv8m + OC-SORT + Box Method, achieving F1-in = 91.49%, F1-out = 89.08%, and FPS = 21.99&#160;[<xref rid="B57-sensors-25-05359" ref-type="bibr">57</xref>].</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Mite detection on bee images</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Bee detection had F1 &#8776; 0.8 and precision up to 1.0, while Varroa detection showed TPR = 0.94, TNR = 0.92, F1 &#8776; 0.8, and precision &#8776; 0.7. Camera resolution strongly impacted detection effectiveness&#8212;5&#160;MP required for reliable results,&#160;[<xref rid="B77-sensors-25-05359" ref-type="bibr">77</xref>]; The authors developed and validated a deep learning model (Faster R-CNN + ResNet-FPN backbone): mAP (mean Average Precision): 0.907, mAR (mean Average Recall): 0.967. These scores were reached using ResNet50-FPN, confidence threshold of 0.5, refinement, and DeblurGAN&#160;[<xref rid="B87-sensors-25-05359" ref-type="bibr">87</xref>]; YOLOv5s achieved best Varroa mite detection: mAP@0.5 = 0.974, Precision&#160;=&#160;0.962, Recall = 0.967. YOLOv5n was fastest: 4.5&#160;ms/image&#160;[<xref rid="B99-sensors-25-05359" ref-type="bibr">99</xref>].</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Activity anomalydetection (multimodal)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Achieved 99.7% accuracy and 87% F1 score on swarm detection using AE trained on spectrograms. Pre-swarming detection was more difficult: AE reached only&#160;60% accuracy, 22&#8211;24% F1, vs. 76.4% accuracy with RF&#160;[<xref rid="B66-sensors-25-05359" ref-type="bibr">66</xref>]; The fuzzy logic model achieved&#160;98% accuracy, 100% precision, 97% recall, and&#160;98% F1-score in colony state detection. It successfully identified events like swarming, colony death, and temperature anomalies based solely on hive temperature profiles&#160;[<xref rid="B78-sensors-25-05359" ref-type="bibr">78</xref>]; Robust regression had R<sup>2</sup> &#8776; 0.95&#8211;0.997, and alarms could be triggered when observed values fall outside prediction intervals&#160;[<xref rid="B86-sensors-25-05359" ref-type="bibr">86</xref>].</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05359-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05359-t007_Table 7</object-id><label>Table 7</label><caption><p>Summary of publicly available smart-beehive datasets by modality and ML application.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset Title</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Modality</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Typical ML Purpose</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">To bee or not to bee: An annotated dataset for beehive sound recognition&#160;[<xref rid="B141-sensors-25-05359" ref-type="bibr">141</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Binary sound classification (Bee vs. noBee)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Audio-Based identification of Beehive states: The dataset&#160;[<xref rid="B142-sensors-25-05359" ref-type="bibr">142</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-class classification of calm/pre-swarm/swarm hive states</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Beehive Sounds&#160;[<xref rid="B143-sensors-25-05359" ref-type="bibr">143</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">State classification (healthy, distressed, empty); anomaly detection</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset for honey bee audio detection&#160;[<xref rid="B18-sensors-25-05359" ref-type="bibr">18</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Species classification (bee vs. drone) using spectrograms</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Queenless honeybee acoustic patterns&#160;[<xref rid="B144-sensors-25-05359" ref-type="bibr">144</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Queen state detection</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Labeled dataset for bee detection and direction estimation on beehive landingboards&#160;[<xref rid="B145-sensors-25-05359" ref-type="bibr">145</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Object detection, pose estimation, and behavior tracking from video</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset for varroa mite detection on stickyboards&#160;[<xref rid="B146-sensors-25-05359" ref-type="bibr">146</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Varroa mite detection</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VarroaDataset&#160;[<xref rid="B17-sensors-25-05359" ref-type="bibr">17</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Parasite detection (Varroa destructor); object detection with bounding boxes</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VnPollenBee Dataset&#160;[<xref rid="B147-sensors-25-05359" ref-type="bibr">147</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pollen-bee classification</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Honey Bee Annotated Images&#160;[<xref rid="B148-sensors-25-05359" ref-type="bibr">148</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bee detection and classification</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Research project on field data collection for honey bee colony model evaluation&#160;[<xref rid="B149-sensors-25-05359" ref-type="bibr">149</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multimodal</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Colony behavior/risk modeling; multi-source integration</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bee colony remote monitoring based on IoT using ESP-NOW protocol&#160;[<xref rid="B37-sensors-25-05359" ref-type="bibr">37</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Environmental</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Colony state monitoring using temperature, weight, battery data for predictive modeling</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Winter carbon dioxide measurements in UK honeybee hives 2022/2023&#160;[<xref rid="B150-sensors-25-05359" ref-type="bibr">150</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Environmental</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Winter vitality prediction</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NASA POWER&#160;[<xref rid="B151-sensors-25-05359" ref-type="bibr">151</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Environmental</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">External environmental feature augmentation for beehive activity modeling</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>