<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431463</article-id><article-id pub-id-type="pmcid-ver">PMC12431463.1</article-id><article-id pub-id-type="pmcaid">12431463</article-id><article-id pub-id-type="pmcaiid">12431463</article-id><article-id pub-id-type="doi">10.3390/s25175497</article-id><article-id pub-id-type="publisher-id">sensors-25-05497</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Fusing Residual and Cascade Attention Mechanisms in Voxel&#8211;RCNN for 3D Object Detection</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Lu</surname><given-names initials="Y">You</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05497" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="Y">Yuwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05497" ref-type="aff">1</xref><xref rid="c1-sensors-25-05497" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1685-4989</contrib-id><name name-style="western"><surname>Fan</surname><given-names initials="X">Xiangsuo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05497" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6980-3579</contrib-id><name name-style="western"><surname>Cai</surname><given-names initials="D">Dengsheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af2-sensors-25-05497" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Gong</surname><given-names initials="R">Rui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05497" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Previtali</surname><given-names initials="M">Mattia</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05497"><label>1</label>School of Automation, Guangxi University of Science and Technology, Liuzhou 545000, China; <email>20230203009@stdmail.gxust.edu.cn</email> (Y.L.); <email>100002085@gxust.edu.cn</email> (X.F.); <email>20230201003@stdmail.gxust.edu.cn</email> (R.G.)</aff><aff id="af2-sensors-25-05497"><label>2</label>Liugong Machinery Co., Ltd., Liuzhou 545000, China; <email>caids@liugong.com</email></aff><author-notes><corresp id="c1-sensors-25-05497"><label>*</label>Correspondence: <email>100000335@gxust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>04</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5497</elocation-id><history><date date-type="received"><day>06</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>7</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05497.pdf"/><abstract><p>In this paper, a high-precision 3D object detector&#8212;Voxel&#8211;RCNN&#8212;is adopted as the baseline detector, and an improved detector named RCAVoxel-RCNN is proposed. To address various issues present in current mainstream 3D point cloud voxelisation methods, such as the suboptimal performance of Region Proposal Networks (RPNs) in generating candidate regions and the inadequate detection of small-scale objects caused by overly deep convolutional layers in both 3D and 2D backbone networks, this paper proposes a Cascade Attention Network (CAN). The CAN is designed to progressively refine and enhance the proposed regions, thereby producing more accurate detection results. Furthermore, a 3D Residual Network is introduced, which improves the representation of small objects by reducing the number of convolutional layers while incorporating residual connections. In the Bird&#8217;s-Eye View (BEV) feature extraction network, a Residual Attention Network (RAN) is developed. This follows a similar approach to the aforementioned 3D backbone network, leveraging the spatial awareness capabilities of the BEV. Additionally, the Squeeze-and-Excitation (SE) attention mechanism is incorporated to assign dynamic weights to features, allowing the network to focus more effectively on informative features. Experimental results on the KITTI validation dataset demonstrate the effectiveness of the proposed method, with detection accuracy for cars, pedestrians, and bicycles improving by 3.34%, 10.75%, and 4.61%, respectively, under the KITTI hard level. The primary evaluation metric adopted is the 3D Average Precision (AP), computed over 40 recall positions (R40). The Intersection over IoU thresholds used are 0.7 for cars and 0.5 for both pedestrians and bicycles.</p></abstract><kwd-group><kwd>residual connection</kwd><kwd>cascaded attention network</kwd><kwd>attention mechanism</kwd><kwd>3D object detection</kwd></kwd-group><funding-group><award-group><funding-source>Guangxi Science and Technology Major Project</funding-source><award-id>AA2023062091</award-id></award-group><award-group><funding-source>Guangxi Outstanding Youth Science Foundation Project</funding-source><award-id>2025GXNSFFA069014</award-id></award-group><funding-statement>This work was supported by the Guangxi Science and Technology Major Project (AA2023062091) and Guangxi Outstanding Youth Science Foundation Project (2025GXNSFFA069014).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05497"><title>1. Introduction</title><p>In recent years, with the rapid advancement of autonomous driving technologies, LiDAR sensors have become an essential component in the development of self-driving vehicles. The point cloud data they produce contain rich spatial information and offer reliable input for environmental perception, thereby attracting significant academic and industrial interest. Object detection is generally classified into two categories: two-dimensional (2D) and three-dimensional (3D). 3D object detection offers a more intuitive representation of the real world, incorporating size, orientation, and distance information [<xref rid="B1-sensors-25-05497" ref-type="bibr">1</xref>]. As LiDAR technology matures and becomes more cost-effective, 3D point cloud-based object detection has emerged as a mainstream approach in autonomous driving. However, LiDAR-generated point clouds are unordered, uneven in density, and highly data-intensive, rendering direct application of 2D object detectors unsuitable for 3D data [<xref rid="B2-sensors-25-05497" ref-type="bibr">2</xref>].</p><p>Research on point cloud-based object detection can be categorised into four main approaches according to the method of point cloud processing: point-based, voxel-based, projection-based, and hybrid point&#8211;voxel approaches. Point-based methods directly operate on raw point cloud data for feature extraction, effectively preserving geometric structures and spatial details. However, they are less suited to large-scale point cloud data. Representative works include PointNet, PointNet++, PointRCNN, and 3DSSD [<xref rid="B3-sensors-25-05497" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05497" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05497" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05497" ref-type="bibr">6</xref>]. Voxel-based methods divide the point cloud into regular 3D voxel grids and model the spatial structure using voxel representations, which facilitates the extraction of local features in 3D space. The choice of voxel size significantly affects detection accuracy. Notable methods in this category include SECOND, VoxelNet, and Voxel&#8211;RCNN [<xref rid="B7-sensors-25-05497" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05497" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05497" ref-type="bibr">9</xref>]. Projection-based approaches convert 3D point clouds into 2D images or pseudo-images, enabling the use of conventional 2D convolutional neural networks. This transformation simplifies the data while preserving the overall outline of objects. Representative methods include PointPillars and CenterPoint [<xref rid="B10-sensors-25-05497" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05497" ref-type="bibr">11</xref>]. Hybrid point&#8211;voxel methods combine point-wise and voxel-wise representations to leverage both the fine-grained features of point clouds and the global structural information offered by voxels. These methods demonstrate strong object recognition capabilities across a variety of scenarios. Representative works include PV-RCNN, SASSD, and CT3D [<xref rid="B12-sensors-25-05497" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05497" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05497" ref-type="bibr">14</xref>].</p><p>The baseline adopted in this study, Voxel&#8211;RCNN, is a classic and effective two-stage detector that not only extracts voxel-based features but also integrates BEV features [<xref rid="B9-sensors-25-05497" ref-type="bibr">9</xref>]. Furthermore, the spatial awareness advantage provided by the BEV perspective enables the detector to identify smaller objects more effectively. Region proposals are first generated through the RPN, followed by refinement and classification to produce the final detection results. However, considering the limitations of this approach&#8212;namely, the suboptimal performance of the RPN and the excessive depth of both the 3D and 2D backbone networks&#8212;this paper proposes a CAN to iteratively refine the predictions. Additionally, residual connections are incorporated into both the 3D and 2D backbone networks. The 2D backbone further integrates a Squeeze-and-Excitation (SE) self-attention mechanism to dynamically reweight features, thereby enhancing the detection capability for small-scale objects [<xref rid="B15-sensors-25-05497" ref-type="bibr">15</xref>].</p><p>Extensive experiments and rigorous ablation studies conducted on the KITTI dataset demonstrate the effectiveness of the proposed method. Specifically, the contributions of this work can be summarised as follows:<list list-type="simple"><list-item><label>(1)</label><p>To address the limited performance of the RPN, a Cascaded Attention Network is designed to iteratively refine region proposals and produce high-quality predictions.</p></list-item><list-item><label>(2)</label><p>To mitigate the degradation in feature representation caused by excessively deep convolutional layers in both 3D and 2D backbones, a 3D Residual Network is introduced to enhance feature transmission across layers. Furthermore, a 2D Residual Attention Network is proposed to improve the model&#8217;s sensitivity to small-scale objects by incorporating attention-based feature weighting.</p></list-item><list-item><label>(3)</label><p>The proposed method is validated on the KITTI dataset, with the results demonstrating its effectiveness. Under the hard setting of the 3D Average Precision (AP) metric using R40 evaluation, detection performance for cars, pedestrians, and cyclists improves by 3.34%, 10.75%, and 4.61%, respectively.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05497"><title>2. Related&#160;Work</title><p>Currently, 3D object detection methods can be broadly categorised into single-stage and two-stage detectors. The baseline adopted in this study is Voxel&#8211;RCNN, a two-stage detector proposed by Jiajun Deng et al. In the first stage, the point cloud is divided into a fixed-size voxel grid, and the point-wise information within each voxel is encoded to form voxel-wise feature representations. These features are then processed by a 3D convolutional backbone network to extract high-level voxel features, and the final-layer features are compressed along the height axis to generate a BEV representation [<xref rid="B16-sensors-25-05497" ref-type="bibr">16</xref>]. Subsequently, a RPN applies a 2D backbone network to the BEV features for further extraction, and uses an anchor-based approach to generate candidate object regions [<xref rid="B17-sensors-25-05497" ref-type="bibr">17</xref>]. In the second stage, in order to refine the proposals, Voxel&#8211;RCNN employs a Multilayer Perceptron (MLP) to directly extract fine-grained geometric features from the raw point cloud within each candidate region [<xref rid="B9-sensors-25-05497" ref-type="bibr">9</xref>]. Finally, the Detection Head further refines these proposals and performs classification and 3D bounding box regression to produce the final detection results.</p><p>For single-stage detectors, object categories and bounding boxes are typically predicted directly from point cloud data. These methods feature a streamlined architecture that facilitates end-to-end modelling. PointNet and PointNet++, inspired by 2D object detection techniques, operate directly on raw point clouds, thereby effectively preserving geometric details effectively [<xref rid="B3-sensors-25-05497" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05497" ref-type="bibr">4</xref>]. VoxelNet introduces a voxelisation approach, dividing the point cloud into fixed-size 3D grids and employing a Voxel Feature Encoding (VFE) module to extract voxel-level features, enabling an end-to-end learning framework from raw data to detection output [<xref rid="B8-sensors-25-05497" ref-type="bibr">8</xref>]. Building upon voxelisation, SECOND addresses the sparse nature of point clouds by incorporating sparse 3D convolutions and applying max pooling to aggregate features, thereby enhancing representational capacity [<xref rid="B7-sensors-25-05497" ref-type="bibr">7</xref>]. PointPillars partitions the point cloud into vertical columnar structures (pillars) and uses the Pillar Feature Net (PFN) to extract features from points within each pillar [<xref rid="B10-sensors-25-05497" ref-type="bibr">10</xref>]. These features are then projected into a BEV representation and processed using 2D convolutional networks. This approach integrates the advantages of voxel partitioning and BEV representation in structural modelling. However, the construction of pillars may result in the loss of some fine-grained point cloud information, potentially affecting the accuracy of boundary detection. CenterPoint directly projects point clouds into the BEV space and extracts spatial features using 2D convolution, simplifying the processing pipeline while preserving the overall object shape [<xref rid="B11-sensors-25-05497" ref-type="bibr">11</xref>]. Nevertheless, the flattening of the 3D structure inherent in BEV representations can lead to the weakening of certain spatial geometric cues. RangeDet projects the point cloud into a depth map based on a radar-view polar coordinate system, maintaining the original spatial structure of the data [<xref rid="B18-sensors-25-05497" ref-type="bibr">18</xref>]. It leverages 2D convolutional networks for feature extraction while preserving spatial relationships. However, the geometric distortion and variation in object scales introduced by the range view projection can present challenges, particularly in detecting small or edge-located objects. DCGNN proposes a distinctive point set optimisation method based on density clustering and incorporates both local and global graph neural network modules, which are designed to model intra-set and inter-set relationships, respectively [<xref rid="B19-sensors-25-05497" ref-type="bibr">19</xref>]. This approach facilitates a more comprehensive learning of spatial features and offers a novel perspective for single-stage detection networks.</p><p>Single-stage detectors lack a region proposal process, which limits their effectiveness in detecting smaller objects&#8212;especially when these objects are subject to geometric occlusion. Inspired by the Faster R-CNN architecture in 2D object detection, researchers have developed two-stage frameworks for 3D object detection [<xref rid="B20-sensors-25-05497" ref-type="bibr">20</xref>]. In such frameworks, the first stage utilises an RPN to generate candidate regions from point cloud data, while the second stage performs refined feature extraction and bounding box regression on these proposals. This approach enhances detection accuracy, making it well-suited for high-precision 3D object detection tasks. PointRCNN directly extracts features from raw point clouds using PointNet++ and generates 3D object proposals. It then applies RoI pooling to further extract point-level features within each candidate region for object classification and bounding box refinement [<xref rid="B5-sensors-25-05497" ref-type="bibr">5</xref>,<xref rid="B21-sensors-25-05497" ref-type="bibr">21</xref>]. Although this method effectively leverages the raw geometric information of the point cloud and achieves high detection accuracy, its computational cost is relatively high, limiting its practicality in real-time applications. PV-RCNN processes point clouds through voxelisation and employs sparse voxel convolution to extract features and generate 3D object proposals [<xref rid="B12-sensors-25-05497" ref-type="bibr">12</xref>]. It then further refines these proposals by fusing local voxel features with fine-grained raw point features, thereby improving detection accuracy [<xref rid="B22-sensors-25-05497" ref-type="bibr">22</xref>]. However, the repeated computation of both voxel-based and point-based features leads to high computational overhead. To address this, PV-RCNN++ introduces improvements to the Voxel Set Abstraction (VSA) module [<xref rid="B23-sensors-25-05497" ref-type="bibr">23</xref>], reducing unnecessary computation while maintaining high accuracy. Voxel Transformer combines the advantages of Transformer architectures and voxel-based representations [<xref rid="B16-sensors-25-05497" ref-type="bibr">16</xref>], leveraging self-attention mechanisms for global feature extraction and thereby enhancing detection performance, particularly for distant objects. Point2Seq builds upon this by modelling object detection as a sequence generation problem [<xref rid="B24-sensors-25-05497" ref-type="bibr">24</xref>]. It adopts an auto-regressive decoding scheme to progressively predict the bounding boxes, making it more effective for detecting small objects and handling sparse point clouds.</p><p>In summary, within the field of object detection, single-stage detectors are generally characterised by higher speed [<xref rid="B25-sensors-25-05497" ref-type="bibr">25</xref>], whereas two-stage detectors often demonstrate superior accuracy, particularly for small objects. Direct processing of raw LiDAR point cloud data can yield high detection accuracy, but it inevitably poses significant computational challenges. Conversely, projecting point clouds into two-dimensional representations can reduce computational complexity, but often results in information loss and limits the ability to fully exploit the spatial structure of the data&#8212;especially when detecting small-scale objects [<xref rid="B26-sensors-25-05497" ref-type="bibr">26</xref>]. The Voxel&#8211;RCNN algorithm combines voxel-based processing with well-established 2D convolutional techniques, effectively extracting both voxel-wise and BEV features [<xref rid="B9-sensors-25-05497" ref-type="bibr">9</xref>]. However, as the depth of convolutional networks increases, the ability to detect small objects may deteriorate. To address this issue, the present study incorporates residual connections and the Squeeze-and-Excitation (SE) attention mechanism to enhance the network&#8217;s sensitivity to small objects. In addition, a CAN is introduced to enable multi-level refinement of detection results, thereby significantly improving overall detection accuracy. These improvements not only enhance the performance of the detection algorithm but also further unlock the potential of point cloud data in 3D object detection tasks.</p></sec><sec sec-type="methods" id="sec3-sensors-25-05497"><title>3. Methods</title><p>In this study, Voxel&#8211;RCNN is adopted as the baseline, with targeted enhancements introduced to address its existing limitations [<xref rid="B9-sensors-25-05497" ref-type="bibr">9</xref>]. The proposed improvements primarily focus on three components: the 3D backbone network, the 2D backbone network, and the cascaded attention network applied after the RPN in the first stage. The overall framework of the proposed approach is illustrated in <xref rid="sensors-25-05497-f001" ref-type="fig">Figure 1</xref>.</p><sec id="sec3dot1-sensors-25-05497"><title>3.1. Residual&#160;Backbone</title><p>In Voxel&#8211;RCNN, the 3D backbone network is employed following voxel encoding [<xref rid="B13-sensors-25-05497" ref-type="bibr">13</xref>], with the objective of extracting spatial features from non-empty voxels. This network primarily comprises Submanifold Convolution Blocks and Sparse Convolutions, which substantially reduce computational complexity while retaining rich feature representations for use in the subsequent RPN. As illustrated in <xref rid="sensors-25-05497-f002" ref-type="fig">Figure 2</xref>a, the 3D backbone network of Voxel&#8211;RCNN receives a four-dimensional voxel input containing the X, Y, and Z coordinates, as well as reflectance intensity. Initially, a Submanifold Convolution Block is applied to perform 1&#215; downsampling [<xref rid="B27-sensors-25-05497" ref-type="bibr">27</xref>], increasing the number of channels from 4 to 16. This is followed by an additional Submanifold Convolution Block for further feature extraction. Subsequently, sparse convolutions are employed to perform 2&#215;, 4&#215;, and 8&#215; downsampling operations using a 3 &#215; 3 &#215; 3 kernel, with the number of channels gradually increased from 16 to 64 [<xref rid="B28-sensors-25-05497" ref-type="bibr">28</xref>]. After each sparse convolution, two Submanifold Convolution Blocks are applied to extract features at multiple scales. The structure of a Submanifold Convolution Block is depicted in <xref rid="sensors-25-05497-f002" ref-type="fig">Figure 2</xref>b and consists of a submanifold convolution layer, normalisation layers, and activation functions. The original 3D backbone network adopts a linear structure, which means that feature extraction is carried out in a sequential manner. This approach results in insufficient multi-scale feature fusion, making it difficult for high-level abstract features to retain low-level detail information. Consequently, when features are subsequently compressed into the BEV representation, more low-level details are lost. To address this issue, residual connections have been integrated into the existing 3D backbone network. These connections facilitate the fusion and interaction of features across different levels, thereby enhancing the effectiveness of feature extraction in subsequent stages [<xref rid="B29-sensors-25-05497" ref-type="bibr">29</xref>]. Specifically, the proposed method modifies the final two Submanifold Convolution Blocks, which are responsible for increasing the number of feature channels, by replacing them with residual Submanifold Convolution Blocks, as illustrated in <xref rid="sensors-25-05497-f002" ref-type="fig">Figure 2</xref>d. This design ensures that the features of small-scale objects are better preserved during the channel expansion process. Let <italic toggle="yes">x</italic> denote the original input, <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the output of the intermediate network layer, and <italic toggle="yes">y</italic> the output of the residual connection. The corresponding computation is defined as follows: <disp-formula id="FD1-sensors-25-05497"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Meanwhile, during the 8&#215; downsampling process, the number of input feature channels is maintained at 64, while the number of output feature channels is increased to 128 in order to extract more expressive features [<xref rid="B30-sensors-25-05497" ref-type="bibr">30</xref>]. This enhancement facilitates spatial transformations and aggregation operations on the 3D feature map, enabling compression along the height dimension. As a result, the voxelised three-dimensional representation can be effectively projected onto a two-dimensional BEV. The modified network structure is illustrated in <xref rid="sensors-25-05497-f002" ref-type="fig">Figure 2</xref>c.</p></sec><sec id="sec3dot2-sensors-25-05497"><title>3.2. Residual Attention&#160;Network</title><p>This study builds upon the final feature map extracted from the 3D backbone network, which is compressed along the height dimension (Z-axis) to generate a BEV feature map [<xref rid="B31-sensors-25-05497" ref-type="bibr">31</xref>]. This transformation effectively simplifies the data structure, reduces computational complexity, and enhances the delineation of object boundaries, thereby benefiting subsequent feature extraction and region proposal generation. The existing 2D backbone network primarily comprises two stages&#8212;downsampling and upsampling&#8212;as illustrated in <xref rid="sensors-25-05497-f003" ref-type="fig">Figure 3</xref>a. Initially, during standard 2D CNN operations, the number of feature channels is reduced from 256 to 64, followed by five consecutive convolutional layers to extract deep features. Subsequently, a 2D convolutional layer is employed to perform 2&#215; downsampling, while increasing the number of feature channels to 128. After downsampling, five additional convolutional layers are applied for further feature extraction. Finally, an upsampling convolution is used to restore the spatial resolution of the original feature map. To fully exploit multi-scale information, the number of feature channels is increased from 64 to 128 through a convolutional operation prior to downsampling, and the resulting features are fused with the upsampled feature map to produce a final fused feature map with 256 channels. However, during the feature extraction phase, the aforementioned framework suffers from a gradual weakening of feature representation for small-scale objects. This is primarily due to the sequential stacking of a large number of convolutional layers in the deeper stages of the network, which tends to dilute fine-grained information.</p><p>To address this issue, we propose an RAN. This network enhances the representation of key information by introducing residual connections and incorporating the SE attention mechanism [<xref rid="B32-sensors-25-05497" ref-type="bibr">32</xref>]. The SE module suppresses redundant features and adaptively adjusts channel-wise feature weights, thereby improving the feature representation of small-scale objects. As a result, it provides higher-quality feature inputs to the RPN. As illustrated in <xref rid="sensors-25-05497-f003" ref-type="fig">Figure 3</xref>b. In this paper, a convolutional module based on residual learning is proposed, comprising two standard convolutional blocks arranged in series. Within this structure, an identity mapping of the input features is retained and combined with the output of the convolutional path via element-wise addition prior to the application of the activation function in the second convolutional block. This design, as illustrated in <xref rid="sensors-25-05497-f003" ref-type="fig">Figure 3</xref>c, facilitates direct information flow and enables efficient gradient propagation. The Squeeze-and-Excitation (SE) attention module employed in this work is capable of automatically learning the relative importance of feature channels, thereby enhancing the representational capacity of 3D point cloud features. The core idea is to increase the network&#8217;s sensitivity to salient features through a channel attention mechanism, while simultaneously suppressing redundant information. The squeeze phase compresses a feature map of dimensions W &#215; H &#215; C into a 1 &#215; 1 &#215; C feature vector by performing global average pooling across the spatial dimensions. This operation captures global contextual information and mitigates inter-channel dependency. Specifically, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is conducted by generating a statistic by reducing <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over the spatial dimensions H &#215; W, with the following calculation formula:<disp-formula id="FD2-sensors-25-05497"><label>(2)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>sq</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The Excitation stage employs an adaptive channel recalibration mechanism comprising two fully connected (FC) layers. The first FC layer reduces the number of channels from C to <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, thereby decreasing computational overhead and introducing a non-linear transformation to enhance the feature representation capability. The second FC layer then restores the number of channels back to C and applies a Sigmoid activation function to generate normalised channel-wise attention weights. <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mi>r</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mi>r</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The calculation formula is as follows:<disp-formula id="FD3-sensors-25-05497"><label>(3)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>s</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>ex</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Finally, the resulting channel attention weights are multiplied element-wise with the input features along the channel dimension, thereby adaptively modulating the representational capacity of each channel. The corresponding flowchart is presented in <xref rid="sensors-25-05497-f002" ref-type="fig">Figure 2</xref>d.</p></sec><sec id="sec3dot3-sensors-25-05497"><title>3.3. Cascade Attention&#160;Network</title><p>Inspired by the remarkable performance of the 2D Cascade Attention Network in object detection tasks, this study extends its underlying principles to the domain of 3D object detection, and proposes a 3D Cascade Attention Network to improve detection accuracy and feature representation capability [<xref rid="B33-sensors-25-05497" ref-type="bibr">33</xref>]. The overall process is illustrated in <xref rid="sensors-25-05497-f004" ref-type="fig">Figure 4</xref>. Initially, pooling operations are applied to the candidate regions generated by the RPN in order to extract features from within these regions. The primary aim of this step is to standardise Region of Interest (RoI) features of varying scales into a fixed size, thereby ensuring consistent input for subsequent processing via a Multi-Layer Perceptron (MLP). During the cascading phase, the features are iteratively refined and updated at each stage. To maintain the flow of global information and avoid redundant feature learning at individual stages, a Shared Feature Connection layer is introduced. Furthermore, to strengthen the network&#8217;s representational capacity, Cross Attention is incorporated into the cascade framework [<xref rid="B34-sensors-25-05497" ref-type="bibr">34</xref>]. <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denotes the encoded features from the previous stage, whereas <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> represents the encoded features at the current stage. Here, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, corresponding to the query, key, and value, respectively. The computation is formulated as follows:<disp-formula id="FD4-sensors-25-05497"><label>(4)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msup><mml:mi>c</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>By learning the interactions between these features, the network is able to effectively integrate cross-stage information, thereby enhancing the robustness of object recognition. Ultimately, the optimised features are passed to the classification branch and regression branch, which are responsible for object category prediction and 3D bounding box regression, respectively, to produce the final detection results. The detailed process is illustrated in <xref rid="sensors-25-05497-f004" ref-type="fig">Figure 4</xref>.</p></sec><sec id="sec3dot4-sensors-25-05497"><title>3.4. Loss&#160;Function</title><p>The loss function in this article combines the RPN loss and the CAN loss with equal weights, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The specific calculation formula is as follows:<disp-formula id="FD5-sensors-25-05497"><label>(5)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>L</mml:mi><mml:mi>RPN</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="[" close="]"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="double-struck">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>reg</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#948;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05497"><label>(6)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>L</mml:mi><mml:mi>CAN</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="[" close="]"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>cls</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="double-struck">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:msubsup><mml:mi>U</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>reg</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>&#948;</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>&#948;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the number of anchors. <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the outputs for classification and bounding box regression, respectively, while <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#948;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> correspond to the classification labels and regression targets. <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:math></inline-formula> refers to the regions where the loss is computed, specifically those with an IoU greater than a predefined threshold. For the loss function of <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#948;</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the predicted results and ground truth targets, respectively, whereas <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#948;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denote the predicted offsets and target offsets.</p></sec></sec><sec id="sec4-sensors-25-05497"><title>4. Experiment</title><sec id="sec4dot1-sensors-25-05497"><title>4.1. Dataset and Evaluation&#160;Metrics</title><p>All model training and validation were conducted on the KITTI dataset, which consists of 7481 LiDAR point cloud samples for training and 7518 samples for testing. Following recent related works, the training set was further divided into 3712 samples for training and 3769 samples for validation. All network models were trained using the training split and evaluated on the validation split. For data augmentation, a mixed strategy was adopted. Specifically, random rotations and flipping were applied to prevent overfitting and to improve the generalisation capability of the models. The primary evaluation metric adopted is the 3D Average Precision (AP), computed over 40 recall positions (R40). The Intersection over IoU thresholds used are 0.7 for cars and 0.5 for both pedestrians and bicycles [<xref rid="B35-sensors-25-05497" ref-type="bibr">35</xref>].</p></sec><sec id="sec4dot2-sensors-25-05497"><title>4.2. Experiment&#160;Details</title><p>All experiments were conducted on an NVIDIA GeForce GTX 3060 GPU with 12 GB of memory. The system was running Ubuntu 20.04, and the experiments were implemented using the PyTorch 1.10 deep learning framework, with CUDA 11.3 employed for model acceleration. The programming language used was Python 3.6.9. The point cloud input was restricted to the following spatial range: X-axis [0, 70.4] m, Y-axis [&#8722;40, 40] m, and Z-axis [&#8722;3, 1] m [<xref rid="B35-sensors-25-05497" ref-type="bibr">35</xref>]. During the training phase, the number of training epochs was set to 80, and the batch size was fixed at 2. Other parameters, such as voxel size and initial learning rate, remained consistent with those provided in the publicly available code of the original paper.</p></sec><sec id="sec4dot3-sensors-25-05497"><title>4.3. Comparison with Other&#160;Algorithms</title><p>To evaluate the effectiveness of the proposed method, we conducted comparative experiments with several state-of-the-art algorithms on the KITTI validation dataset. The experimental results demonstrate that our approach achieves superior overall detection accuracy, with particularly notable improvements in the detection of small-scale objects. The detailed results are presented in <xref rid="sensors-25-05497-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec4dot4-sensors-25-05497"><title>4.4. Ablation&#160;Experiment</title><p>To thoroughly validate the effectiveness of the proposed approach, a series of systematic ablation studies were conducted to evaluate the individual contributions of each module to the overall model performance. Starting from the baseline model, the CAN, RAN, and 3D Residual module were introduced in succession. The experimental results demonstrate that the incorporation of these modules led to significant improvements in detection performance. Specifically, the 3D bounding box detection accuracy for cars, pedestrians, and bicycles under the three difficulty levels was improved by 0.47%, 3.09%, 3.34%, 10.71%, 11.81%, 10.75%, 5.61%, 4.67%, and 4.61%, respectively. The performance metrics for Voxel&#8211;RCNN were obtained using publicly available open-source code. Detailed results are provided in <xref rid="sensors-25-05497-t002" ref-type="table">Table 2</xref>.</p></sec><sec id="sec4dot5-sensors-25-05497"><title>4.5. Visualisation of the&#160;Results</title><p>To provide a more intuitive understanding of the experimental results, visualisations were carried out using the KITTI dataset. During this process, raw point cloud data from multiple randomly selected scenes was input into the proposed model for inference. As illustrated in <xref rid="sensors-25-05497-f005" ref-type="fig">Figure 5</xref>, the 3D bounding box detection results across various scenes can be clearly observed, offering a visual representation of the model&#8217;s performance in diverse environments.</p><p>The proposed model was compared with the baseline model across a range of scenarios. As illustrated in <xref rid="sensors-25-05497-f006" ref-type="fig">Figure 6</xref>, the model presented in this paper exhibits clear superiority in detecting small objects at long distances, thereby providing strong evidence of the effectiveness of the proposed modules.</p><p>Several scenarios were randomly selected for further comparison, and the results indicate that the model proposed in this study achieves a lower false detection rate compared to the baseline model. The detailed visual comparisons are presented in <xref rid="sensors-25-05497-f007" ref-type="fig">Figure 7</xref>.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05497"><title>5. Discussion</title><sec id="sec5dot1-sensors-25-05497"><title>5.1. Limitations</title><p>Although the model proposed in this study has achieved notable improvements in overall detection accuracy, certain limitations remain. Firstly, the incorporation of the cascade attention network inevitably increases the computational complexity of the model. Secondly, in complex scenes, particularly where multiple small-scale objects are densely clustered, the model remains prone to missed detections.</p></sec><sec id="sec5dot2-sensors-25-05497"><title>5.2. Improvement&#160;Methods</title><p>In future research, we will focus on optimising the computational efficiency of the proposed algorithm. To address the issue of region redundancy in the three-stage region proposal mechanism of the CAN network, we plan to adopt a feature caching and reuse strategy [<xref rid="B36-sensors-25-05497" ref-type="bibr">36</xref>]. Specifically, a feature memory bank will be constructed for overlapping regions, and during iterative processing, feature pooling operations will be performed only on newly generated regions. This approach is expected to significantly reduce computational complexity.</p><p>The issue of missed detections in scenarios involving dense clusters of small objects may be attributed to the use of overly large voxel sizes during the voxelisation process. In such cases, occluded small-object point clouds may become merged within a single voxel, making them difficult to distinguish. To address this, we intend to investigate the relationship between voxel size and detection accuracy in greater depth, with the aim of identifying an optimal voxelisation parameter configuration. Additionally, we plan to introduce a multi-scale feature fusion strategy to ensure that small objects across different scales receive adequate feature representation [<xref rid="B37-sensors-25-05497" ref-type="bibr">37</xref>]. By enhancing the sensitivity and accuracy of the detection algorithm to small objects, this approach aims to improve the model&#8217;s capability in complex scenarios characterised by dense small-object distributions.</p></sec></sec><sec id="sec6-sensors-25-05497"><title>6. Summary</title><p>In this study, we designed three modules, namely, the CAN, RAN and 3D Residual modules, to enhance the detection performance of the model. The Cascade Attention Network (CAN) aggregates features from different stages using a cross-attention mechanism, which progressively improves and refines the region proposals generated by the Region Proposal Network (RPN). To address the problem of feature degradation caused by overly deep convolutional layers in the original 3D and 2D backbone networks, we reduced the number of convolutional layers and introduced residual connections. This design facilitates the transmission of information across different levels and improves the capability of the network to represent features effectively. The 3D Residual module successfully mitigates the limitations in the 3D backbone network. However, it is not suitable for the 2D backbone network used in the BEV representation, since the BEV is produced by compressing 3D feature maps along the height axis. This compression leads to weaker features for small objects. To resolve this issue, we developed the RAN, which incorporates the SE attention mechanism into the residual connection process. This mechanism adaptively adjusts the weights of different channels and enhances the feature representation of small-scale targets. Experimental results confirm that the algorithm proposed in this study achieves a clear improvement in overall detection accuracy, particularly in the detection of small objects.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Y.L.: methodology, writing&#8212;reviewing and editing, software. R.G., Y.Z. and D.C.: writing&#8212;review and editing, conceptualisation. X.F.: investigation, writing&#8212;review and editing, project administration, supervision. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors upon request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Dengsheng Cai was employed by the company Liugong Machinery Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">RCAVoxel-RCNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Residual and Cascade Attention Mechanisms in Voxel&#8211;RCNN</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RPN</td><td align="left" valign="middle" rowspan="1" colspan="1">Region Proposal Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CAN</td><td align="left" valign="middle" rowspan="1" colspan="1">Cascade Attention Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RAN</td><td align="left" valign="middle" rowspan="1" colspan="1">Residual Attention Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SE</td><td align="left" valign="middle" rowspan="1" colspan="1">Squeeze-and-Excitation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEV</td><td align="left" valign="middle" rowspan="1" colspan="1">Bird&#8217;s-Eye View</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3D</td><td align="left" valign="middle" rowspan="1" colspan="1">Three-Dimensional</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2D</td><td align="left" valign="middle" rowspan="1" colspan="1">Two-Dimensional</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VFE</td><td align="left" valign="middle" rowspan="1" colspan="1">Voxel Feature Encoding</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PFN</td><td align="left" valign="middle" rowspan="1" colspan="1">Pillar Feature Net PFN</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VSA</td><td align="left" valign="middle" rowspan="1" colspan="1">Voxel Set Abstraction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FC</td><td align="left" valign="middle" rowspan="1" colspan="1">Fully Connected</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05497"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>3D object detection for autonomous driving: A survey</article-title><source>Pattern Recognit.</source><year>2022</year><volume>130</volume><fpage>108796</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2022.108796</pub-id></element-citation></ref><ref id="B2-sensors-25-05497"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ogai</surname><given-names>H.</given-names></name></person-group><article-title>Deep 3D object detection networks using LiDAR data: A review</article-title><source>IEEE Sens. J.</source><year>2020</year><volume>21</volume><fpage>1152</fpage><lpage>1171</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2020.3020626</pub-id></element-citation></ref><ref id="B3-sensors-25-05497"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Pointnet: Deep learning on point sets for 3d classification and segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>652</fpage><lpage>660</lpage></element-citation></ref><ref id="B4-sensors-25-05497"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Pointnet++: Deep hierarchical feature learning on point sets in a metric space</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>NIPS Foundation</publisher-name><publisher-loc>La Jolla, CA, USA</publisher-loc><year>2017</year><volume>Volume 30</volume></element-citation></ref><ref id="B5-sensors-25-05497"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>PointRCNN: 3D object proposal generation and detection from point cloud</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>770</fpage><lpage>779</lpage></element-citation></ref><ref id="B6-sensors-25-05497"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>3DSSD: Point-based 3D single stage object detector</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11040</fpage><lpage>11048</lpage></element-citation></ref><ref id="B7-sensors-25-05497"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>Second: Sparsely embedded convolutional detection</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3337</elocation-id><pub-id pub-id-type="doi">10.3390/s18103337</pub-id><pub-id pub-id-type="pmid">30301196</pub-id><pub-id pub-id-type="pmcid">PMC6210968</pub-id></element-citation></ref><ref id="B8-sensors-25-05497"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tuzel</surname><given-names>O.</given-names></name></person-group><article-title>VoxelNet: End-to-End learning for point cloud based 3d object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>4490</fpage><lpage>4499</lpage></element-citation></ref><ref id="B9-sensors-25-05497"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Voxel r-cnn: Towards high performance voxel-based 3d object detection</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Virtual Event</conf-loc><conf-date>2&#8211;9 February 2021</conf-date><volume>Volume 35</volume><fpage>1201</fpage><lpage>1209</lpage></element-citation></ref><ref id="B10-sensors-25-05497"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Caesar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>Pointpillars: Fast encoders for object detection from point clouds</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>12697</fpage><lpage>12705</lpage></element-citation></ref><ref id="B11-sensors-25-05497"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Krahenbuhl</surname><given-names>P.</given-names></name></person-group><article-title>Center-based 3D object detection and tracking</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>11784</fpage><lpage>11793</lpage></element-citation></ref><ref id="B12-sensors-25-05497"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>10529</fpage><lpage>10538</lpage></element-citation></ref><ref id="B13-sensors-25-05497"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>X.S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Structure aware single-stage 3d object detection from point cloud</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11873</fpage><lpage>11882</lpage></element-citation></ref><ref id="B14-sensors-25-05497"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>B.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>X.S.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>M.J.</given-names></name></person-group><article-title>Improving 3d object detection with channel-wise transformer</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>2743</fpage><lpage>2752</lpage></element-citation></ref><ref id="B15-sensors-25-05497"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage></element-citation></ref><ref id="B16-sensors-25-05497"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>Voxel transformer for 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>3164</fpage><lpage>3173</lpage></element-citation></ref><ref id="B17-sensors-25-05497"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kong</surname><given-names>T.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>F.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name></person-group><article-title>Foveabox: Beyound anchor-based object detection</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>7389</fpage><lpage>7398</lpage><pub-id pub-id-type="doi">10.1109/TIP.2020.3002345</pub-id></element-citation></ref><ref id="B18-sensors-25-05497"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>RangeDet: In defense of range view for lidar-based 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>2918</fpage><lpage>2927</lpage></element-citation></ref><ref id="B19-sensors-25-05497"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name></person-group><article-title>DCGNN: A single-stage 3D object detection network based on density clustering and graph neural network</article-title><source>Complex Intell. Syst.</source><year>2023</year><volume>9</volume><fpage>3399</fpage><lpage>3408</lpage><pub-id pub-id-type="doi">10.1007/s40747-022-00926-z</pub-id></element-citation></ref><ref id="B20-sensors-25-05497"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster r-cnn: Towards real-time object detection with region proposal networks</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>NIPS Foundation</publisher-name><publisher-loc>La Jolla, CA, USA</publisher-loc><year>2015</year><volume>Volume 28</volume></element-citation></ref><ref id="B21-sensors-25-05497"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gholamalinezhad</surname><given-names>H.</given-names></name><name name-style="western"><surname>Khosravi</surname><given-names>H.</given-names></name></person-group><article-title>Pooling methods in deep neural networks, a review</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2009.07485</pub-id><pub-id pub-id-type="arxiv">2009.07485</pub-id></element-citation></ref><ref id="B22-sensors-25-05497"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Han</surname><given-names>S.</given-names></name></person-group><article-title>Searching efficient 3d architectures with sparse point-voxel convolution</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><fpage>685</fpage><lpage>702</lpage></element-citation></ref><ref id="B23-sensors-25-05497"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>PV-RCNN++: Point-voxel feature set abstraction with local vector representation for 3D object detection</article-title><source>Int. J. Comput. Vis.</source><year>2023</year><volume>131</volume><fpage>531</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1007/s11263-022-01710-9</pub-id></element-citation></ref><ref id="B24-sensors-25-05497"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mi</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Point2seq: Detecting 3d objects as sequences</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>8521</fpage><lpage>8530</lpage></element-citation></ref><ref id="B25-sensors-25-05497"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Soft-weighted-average ensemble vehicle detection method based on single-stage and two-stage deep learning models</article-title><source>IEEE Trans. Intell. Veh.</source><year>2020</year><volume>6</volume><fpage>100</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1109/TIV.2020.3010832</pub-id></element-citation></ref><ref id="B26-sensors-25-05497"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Peethambaran</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>D.</given-names></name></person-group><article-title>Lidar point clouds to 3-D urban models: A review</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2018</year><volume>11</volume><fpage>606</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2017.2781132</pub-id></element-citation></ref><ref id="B27-sensors-25-05497"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Graham</surname><given-names>B.</given-names></name><name name-style="western"><surname>Van der Maaten</surname><given-names>L.</given-names></name></person-group><article-title>Submanifold sparse convolutional networks</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.01307</pub-id></element-citation></ref><ref id="B28-sensors-25-05497"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sotelo</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Voxel-RCNN-complex: An effective 3-D point cloud object detector for complex traffic conditions</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2022</year><volume>71</volume><fpage>2507112</fpage><pub-id pub-id-type="doi">10.1109/TIM.2022.3165251</pub-id></element-citation></ref><ref id="B29-sensors-25-05497"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B30-sensors-25-05497"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Appel</surname><given-names>R.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P.</given-names></name></person-group><article-title>Fast feature pyramids for object detection</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2014</year><volume>36</volume><fpage>1532</fpage><lpage>1545</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2014.2300479</pub-id><pub-id pub-id-type="pmid">26353336</pub-id></element-citation></ref><ref id="B31-sensors-25-05497"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>B.</given-names></name></person-group><article-title>Multi-view joint learning and bev feature-fusion network for 3d object detection</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>5274</elocation-id><pub-id pub-id-type="doi">10.3390/app13095274</pub-id></element-citation></ref><ref id="B32-sensors-25-05497"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Residual attention network for image classification</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>3156</fpage><lpage>3164</lpage></element-citation></ref><ref id="B33-sensors-25-05497"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>CasA: A cascade attention network for 3-D object detection from LiDAR point clouds</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>5704511</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3203163</pub-id></element-citation></ref><ref id="B34-sensors-25-05497"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.F.R.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Panda</surname><given-names>R.</given-names></name></person-group><article-title>Crossvit: Cross-attention multi-scale vision transformer for image classification</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>357</fpage><lpage>366</lpage></element-citation></ref><ref id="B35-sensors-25-05497"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Geiger</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lenz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Stiller</surname><given-names>C.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Vision meets robotics: The kitti dataset</article-title><source>Int. J. Robot. Res.</source><year>2013</year><volume>32</volume><fpage>1231</fpage><lpage>1237</lpage><pub-id pub-id-type="doi">10.1177/0278364913491297</pub-id></element-citation></ref><ref id="B36-sensors-25-05497"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name><name name-style="western"><surname>Alsumeri</surname><given-names>A.</given-names></name></person-group><article-title>Distance Awared: Adaptive Voxel Resolution to help 3D Object Detection Networks See Farther</article-title><source>Proceedings of the 2023 42nd Chinese Control Conference (CCC)</source><conf-loc>Tianjin, China</conf-loc><conf-date>24&#8211;26 July 2023</conf-date><fpage>7995</fpage><lpage>8000</lpage></element-citation></ref><ref id="B37-sensors-25-05497"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>B.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Alsaadi</surname><given-names>F.E.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>N.</given-names></name></person-group><article-title>AGGN: Attention-based glioma grading network with multi-scale feature extraction and multi-modal information fusion</article-title><source>Comput. Biol. Med.</source><year>2023</year><volume>152</volume><elocation-id>106457</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.106457</pub-id><pub-id pub-id-type="pmid">36571937</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05497-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall framework diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05497-g001.jpg"/></fig><fig position="float" id="sensors-25-05497-f002" orientation="portrait"><label>Figure 2</label><caption><p>(<bold>a</bold>) Original network structure, (<bold>b</bold>) Subm block, (<bold>c</bold>) modified networkstructure, (<bold>d</bold>) Subm block with residual connections.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05497-g002.jpg"/></fig><fig position="float" id="sensors-25-05497-f003" orientation="portrait"><label>Figure 3</label><caption><p>(<bold>a</bold>) Original 2D backbone network, (<bold>b</bold>) modified 2D backbone network, (<bold>c</bold>) residual convolutional block, (<bold>d</bold>) SE attention mechanism.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05497-g003.jpg"/></fig><fig position="float" id="sensors-25-05497-f004" orientation="portrait"><label>Figure 4</label><caption><p>Cascade Attention Network structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05497-g004.jpg"/></fig><fig position="float" id="sensors-25-05497-f005" orientation="portrait"><label>Figure 5</label><caption><p>An example of results on the KITT validation dataset, featuring four different scenes, where cyclists, pedestrians, and bicycles are displayed in pink, green, and purple, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05497-g005.jpg"/></fig><fig position="float" id="sensors-25-05497-f006" orientation="portrait"><label>Figure 6</label><caption><p>The detection results of distant targets in two different scenes, with the differences marked with circles.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05497-g006.jpg"/></fig><fig position="float" id="sensors-25-05497-f007" orientation="portrait"><label>Figure 7</label><caption><p>The detection results of distant targets in two different scenes, with the differences marked with circles.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05497-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05497-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05497-t001_Table 1</object-id><label>Table 1</label><caption><p>In the comparison of algorithms, the best results are indicated in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Methods</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Car.3D (APR40)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Ped.3D (APR40)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cyc.3D (APR40)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Easy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mod</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hard</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">PV-RCNN (CVPR)</td><td align="center" valign="middle" rowspan="1" colspan="1">92.57</td><td align="center" valign="middle" rowspan="1" colspan="1">84.83</td><td align="center" valign="middle" rowspan="1" colspan="1">82.69</td><td align="center" valign="middle" rowspan="1" colspan="1">64.26</td><td align="center" valign="middle" rowspan="1" colspan="1">56.67</td><td align="center" valign="middle" rowspan="1" colspan="1">51.91</td><td align="center" valign="middle" rowspan="1" colspan="1">88.65</td><td align="center" valign="middle" rowspan="1" colspan="1">71.95</td><td align="center" valign="middle" rowspan="1" colspan="1">66.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PV-RCNN++</td><td align="center" valign="middle" rowspan="1" colspan="1">92.11</td><td align="center" valign="middle" rowspan="1" colspan="1">85.55</td><td align="center" valign="middle" rowspan="1" colspan="1">82.27</td><td align="center" valign="middle" rowspan="1" colspan="1">67.74</td><td align="center" valign="middle" rowspan="1" colspan="1">60.55</td><td align="center" valign="middle" rowspan="1" colspan="1">55.92</td><td align="center" valign="middle" rowspan="1" colspan="1">88.73</td><td align="center" valign="middle" rowspan="1" colspan="1">73.58</td><td align="center" valign="middle" rowspan="1" colspan="1">69.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointPillars (CVPR)</td><td align="center" valign="middle" rowspan="1" colspan="1">86.42</td><td align="center" valign="middle" rowspan="1" colspan="1">77.29</td><td align="center" valign="middle" rowspan="1" colspan="1">75.60</td><td align="center" valign="middle" rowspan="1" colspan="1">53.6</td><td align="center" valign="middle" rowspan="1" colspan="1">48.36</td><td align="center" valign="middle" rowspan="1" colspan="1">45.22</td><td align="center" valign="middle" rowspan="1" colspan="1">82.83</td><td align="center" valign="middle" rowspan="1" colspan="1">64.24</td><td align="center" valign="middle" rowspan="1" colspan="1">60.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SECOND (SENSOR)</td><td align="center" valign="middle" rowspan="1" colspan="1">90.55</td><td align="center" valign="middle" rowspan="1" colspan="1">81.86</td><td align="center" valign="middle" rowspan="1" colspan="1">78.61</td><td align="center" valign="middle" rowspan="1" colspan="1">55.94</td><td align="center" valign="middle" rowspan="1" colspan="1">51.14</td><td align="center" valign="middle" rowspan="1" colspan="1">46.17</td><td align="center" valign="middle" rowspan="1" colspan="1">82.96</td><td align="center" valign="middle" rowspan="1" colspan="1">66.74</td><td align="center" valign="middle" rowspan="1" colspan="1">65.34</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CT3D (ICCV)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>92.85</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>85.82</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">82.86</td><td align="center" valign="middle" rowspan="1" colspan="1">65.73</td><td align="center" valign="middle" rowspan="1" colspan="1">58.56</td><td align="center" valign="middle" rowspan="1" colspan="1">53.04</td><td align="center" valign="middle" rowspan="1" colspan="1">91.99</td><td align="center" valign="middle" rowspan="1" colspan="1">71.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.34</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VoxelNet (CVPR)</td><td align="center" valign="middle" rowspan="1" colspan="1">87.88</td><td align="center" valign="middle" rowspan="1" colspan="1">75.58</td><td align="center" valign="middle" rowspan="1" colspan="1">72.77</td><td align="center" valign="middle" rowspan="1" colspan="1">56.46</td><td align="center" valign="middle" rowspan="1" colspan="1">50.97</td><td align="center" valign="middle" rowspan="1" colspan="1">45.65</td><td align="center" valign="middle" rowspan="1" colspan="1">78.18</td><td align="center" valign="middle" rowspan="1" colspan="1">61.74</td><td align="center" valign="middle" rowspan="1" colspan="1">54.68</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mtext>-</mml:mtext><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (TPAMI)</td><td align="center" valign="middle" rowspan="1" colspan="1">89.36</td><td align="center" valign="middle" rowspan="1" colspan="1">80.23</td><td align="center" valign="middle" rowspan="1" colspan="1">78.88</td><td align="center" valign="middle" rowspan="1" colspan="1">65.7</td><td align="center" valign="middle" rowspan="1" colspan="1">61.32</td><td align="center" valign="middle" rowspan="1" colspan="1">55.4</td><td align="center" valign="middle" rowspan="1" colspan="1">85.6</td><td align="center" valign="middle" rowspan="1" colspan="1">69.24</td><td align="center" valign="middle" rowspan="1" colspan="1">65.63</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointRCNN (CVPR)</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">78.65</td><td align="center" valign="middle" rowspan="1" colspan="1">78.05</td><td align="center" valign="middle" rowspan="1" colspan="1">62.69</td><td align="center" valign="middle" rowspan="1" colspan="1">55.77</td><td align="center" valign="middle" rowspan="1" colspan="1">52.65</td><td align="center" valign="middle" rowspan="1" colspan="1">84.48</td><td align="center" valign="middle" rowspan="1" colspan="1">66.37</td><td align="center" valign="middle" rowspan="1" colspan="1">60.83</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.81</td><td align="center" valign="middle" rowspan="1" colspan="1">85.78</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>83.51</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.46</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>64.74</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>58.72</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>92.92</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>76.61</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.62</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+0.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+5.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+3.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+3.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+0.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+4.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+4.28</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05497-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05497-t002_Table 2</object-id><label>Table 2</label><caption><p>Improvement in performance through different modules, the best results of this paper&#8217;s algorithm are indicated in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Module</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Car.3D (APR40)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Ped (APR40)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cyc.3D (APR40)</th></tr><tr><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
CAN
</th><th colspan="2" align="center" valign="middle" rowspan="1">
RAN
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
3D Residual
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Easy
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Mod
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Hard
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Easy
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Mod
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Hard
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Easy
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Mod
</th><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
Hard
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RN</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RAN</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Voxel&#8211;RCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.34</td><td align="center" valign="middle" rowspan="1" colspan="1">82.71</td><td align="center" valign="middle" rowspan="1" colspan="1">80.17</td><td align="center" valign="middle" rowspan="1" colspan="1">60.75</td><td align="center" valign="middle" rowspan="1" colspan="1">52.93</td><td align="center" valign="middle" rowspan="1" colspan="1">47.97</td><td align="center" valign="middle" rowspan="1" colspan="1">87.31</td><td align="center" valign="middle" rowspan="1" colspan="1">71.94</td><td align="center" valign="middle" rowspan="1" colspan="1">67.43</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.47</td><td align="center" valign="middle" rowspan="1" colspan="1">83.85</td><td align="center" valign="middle" rowspan="1" colspan="1">82.18</td><td align="center" valign="middle" rowspan="1" colspan="1">69.80</td><td align="center" valign="middle" rowspan="1" colspan="1">61.53</td><td align="center" valign="middle" rowspan="1" colspan="1">56.88</td><td align="center" valign="middle" rowspan="1" colspan="1">90.82</td><td align="center" valign="middle" rowspan="1" colspan="1">73.53</td><td align="center" valign="middle" rowspan="1" colspan="1">68.88</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">B</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.58</td><td align="center" valign="middle" rowspan="1" colspan="1">83.86</td><td align="center" valign="middle" rowspan="1" colspan="1">82.33</td><td align="center" valign="middle" rowspan="1" colspan="1">69.9</td><td align="center" valign="middle" rowspan="1" colspan="1">61.54</td><td align="center" valign="middle" rowspan="1" colspan="1">56.95</td><td align="center" valign="middle" rowspan="1" colspan="1">91.22</td><td align="center" valign="middle" rowspan="1" colspan="1">73.46</td><td align="center" valign="middle" rowspan="1" colspan="1">69.91</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">92.80</td><td align="center" valign="middle" rowspan="1" colspan="1">84.93</td><td align="center" valign="middle" rowspan="1" colspan="1">82.99</td><td align="center" valign="middle" rowspan="1" colspan="1">70.13</td><td align="center" valign="middle" rowspan="1" colspan="1">62.52</td><td align="center" valign="middle" rowspan="1" colspan="1">57.31</td><td align="center" valign="middle" rowspan="1" colspan="1">91.85</td><td align="center" valign="middle" rowspan="1" colspan="1">74.05</td><td align="center" valign="middle" rowspan="1" colspan="1">69.92</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>92.81</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>85.78</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>83.51</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.46</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>64.74</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>58.72</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>92.92</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>76.61</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.62</bold>
</td></tr><tr><td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Improvement</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+0.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+3.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+3.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+10.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+11.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+10.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+5.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+4.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+4.61</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>