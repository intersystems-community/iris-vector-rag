<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430914</article-id><article-id pub-id-type="pmcid-ver">PMC12430914.1</article-id><article-id pub-id-type="pmcaid">12430914</article-id><article-id pub-id-type="pmcaiid">12430914</article-id><article-id pub-id-type="doi">10.3390/s25175581</article-id><article-id pub-id-type="publisher-id">sensors-25-05581</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-Temporal Remote Sensing Image Matching Based on Multi-Perception and Enhanced Feature Descriptors</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="J">Jinming</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05581" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zang</surname><given-names initials="W">Wenqian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><xref rid="af2-sensors-25-05581" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-4935-9450</contrib-id><name name-style="western"><surname>Tian</surname><given-names initials="X">Xiaomin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05581" ref-type="aff">1</xref><xref rid="c1-sensors-25-05581" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Yuan</surname><given-names initials="Q">Qiangqiang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05581"><label>1</label>School of Remote Sensing and Information Engineering, North China Institute of Aerospace Engineering, Langfang 065000, China; <email>zhangjm1129@163.com</email></aff><aff id="af2-sensors-25-05581"><label>2</label>Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China; <email>zangwq@aircas.ac.cn</email></aff><author-notes><corresp id="c1-sensors-25-05581"><label>*</label>Correspondence: <email>xm_tian@yeah.net</email></corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5581</elocation-id><history><date date-type="received"><day>30</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>03</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>05</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>07</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05581.pdf"/><abstract><p>Multi-temporal remote sensing image matching plays a crucial role in tasks such as detecting changes in urban buildings, monitoring agriculture, and assessing ecological dynamics. Due to temporal variations in images, significant changes in land features can lead to low accuracy or even failure when matching results. To address these challenges, in this study, a remote sensing image matching framework is proposed based on multi-perception and enhanced feature description. Specifically, the framework consists of two core components: a feature extraction network that integrates multiple perceptions and a feature descriptor enhancement module. The designed feature extraction network effectively focuses on key regions while leveraging depthwise separable convolutions to capture local features at different scales, thereby improving the detection capabilities of feature points. Furthermore, the feature descriptor enhancement module optimizes feature point descriptors through self-enhancement and cross-enhancement phases. The enhanced descriptors not only extract the geometric information of the feature points but also integrate global contextual information. Experimental results demonstrate that, compared to existing remote sensing image matching methods, our approach maintains a strong matching performance under conditions of angular and scale variation.</p></abstract><kwd-group><kwd>image matching</kwd><kwd>multi-temporal remote sensing images</kwd><kwd>deep learning</kwd><kwd>descriptor enhancement</kwd></kwd-group><funding-group><award-group><funding-source>Typical Ground Object Target Optical Characteristics Library for the Common Application Support Platform Project of the National Civil Space Infrastructure &#8220;13th Five Year Plan&#8221; Land Observation Satellite</funding-source><award-id>YG202102H</award-id></award-group><award-group><funding-source>Research Fund Project of North China Institute of Aerospace Engineering</funding-source><award-id>BKY202134</award-id></award-group><funding-statement>This research was funded by the Typical Ground Object Target Optical Characteristics Library for the Common Application Support Platform Project of the National Civil Space Infrastructure &#8220;13th Five Year Plan&#8221; Land Observation Satellite, grant number YG202102H and Research Fund Project of North China Institute of Aerospace Engineering, grant number BKY202134.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05581"><title>1. Introduction</title><p>With the rapid advancement of remote sensing technology, multi-temporal remote sensing images are playing an increasingly important role in areas such as ecological environment monitoring, land cover change detection, and disaster assessment [<xref rid="B1-sensors-25-05581" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05581" ref-type="bibr">2</xref>]. However, significant geometric and radiometric inconsistencies exist between remote sensing images captured at various imaging times, sensor parameters, atmospheric conditions, and changes in the features themselves [<xref rid="B3-sensors-25-05581" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05581" ref-type="bibr">4</xref>]. These inconsistencies pose significant challenges to achieving high-precision image matching. Reliable image matching is fundamental to multi-temporal analysis, as its accuracy directly impacts the precision of subsequent applications such as change detection and temporal modeling [<xref rid="B5-sensors-25-05581" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05581" ref-type="bibr">6</xref>].</p><p>Traditional image matching methods face numerous challenges during multi-temporal image matching, including insufficient feature point counts, high rates of mismatch, and inadequate adaptability to nonlinear deformations [<xref rid="B7-sensors-25-05581" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05581" ref-type="bibr">8</xref>]. In recent years, deep learning approaches, such as convolutional neural networks and attention mechanisms, have demonstrated significant potential in the field of image matching, enhancing matching robustness by learning high-level semantic features [<xref rid="B9-sensors-25-05581" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05581" ref-type="bibr">10</xref>]. However, existing methods still require optimization regarding feature representation and matching stability, particularly the complex changing characteristics of multi-temporal remote sensing images. On the one hand, the two remote sensing images to be matched are acquired at different times, resulting in potential changes in the features they depict. This results in a reduction in the areas that can be effectively matched, making it challenging to achieve the desired quantity and accuracy of extracted feature points. On the other hand, multi-temporal remote sensing images are influenced by factors such as lighting conditions and imaging angles, often exhibiting significant radiometric differences and geometric distortions. These variations complicate the process of achieving stable and accurate matches when image feature points undergo descriptor matching.</p><p>To address these limitations, inspired by the efficient feature point detection paradigm of SuperPoint [<xref rid="B11-sensors-25-05581" ref-type="bibr">11</xref>] and the core idea of feature enhancement in FeatureBooster [<xref rid="B12-sensors-25-05581" ref-type="bibr">12</xref>], we propose a multi-temporal remote sensing image matching framework based on multi-perception and feature description enhancement. Unlike SuperPoint, which relies on a single visual texture to generate feature points, our framework&#8217;s feature extraction network is designed with multiple perception layers that focus on multidimensional remote sensing features. Furthermore, although numerous descriptors have been proposed in the field of image matching to meet the demands of multi-temporal scenarios, little attention has been given to enhancing existing descriptors, particularly through learning-based methods. Therefore, our approach innovatively optimizes the remote sensing image matching process from two aspects: Feature point extraction and feature descriptor enhancement. This method includes a feature extraction network that integrates multiple perception layers and a descriptor enhancement module designed to improve the performance of feature descriptors. The feature extraction network incorporates multiple perception layers, allowing the model to increase its focus on critical feature regions and enhance the distinctiveness of features. Additionally, multi-perception layers combine attention mechanisms with depthwise separable convolutions, allowing for independent feature extraction across channels and inter-channel feature fusion. This approach preserves local feature information, generates richer feature representations, and can reduce computational load while improving overall model performance to some extent. Furthermore, the descriptor enhancement module optimizes the feature descriptors through self-enhancement and cross-enhancement stages to improve their matching performance. The self-enhancement stage involves a multilayer perceptron (<italic toggle="yes">MLP</italic>) that encodes the geometric attributes of key points and combines this information with new descriptors. The cross-enhancement stage leverages a lightweight Transformer to capture spatial contextual cues, further enhancing the discriminative power and robustness of the descriptors.</p><p>The main contributions of this paper are as follows:<list list-type="order"><list-item><p>We designed a feature extraction network based on multi-perception, which increases the number of effective feature points extracted in multi-temporal remote sensing image matching tasks and improves the positional accuracy of feature points to some extent.</p></list-item><list-item><p>We introduced a feature descriptor enhancement module for multi-temporal remote sensing image matching tasks. This module optimizes feature descriptors through self-enhancement and cross-enhancement phases to improve their matching performance.</p></list-item><list-item><p>To demonstrate the superiority of our method, we conducted comparative experiments against existing remote sensing image matching methods on multi-temporal remote sensing datasets, validating the accuracy and robustness of our approach.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05581"><title>2. Related Works</title><p>Existing remote sensing image matching methods can be categorized into two main types: region-based and feature-based approaches [<xref rid="B13-sensors-25-05581" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05581" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05581" ref-type="bibr">15</xref>]. Region-based methods assess similarity using shallow information such as image intensity or phase [<xref rid="B16-sensors-25-05581" ref-type="bibr">16</xref>]. However, the application of these methods is limited due to the susceptibility of pixel values to noise interference in remote sensing images. By contrast, feature-based matching methods focus on prominent features within the images, such as points, lines, or surfaces [<xref rid="B17-sensors-25-05581" ref-type="bibr">17</xref>]. Matching based on image features is more effective at resisting the effects of noise and intensity variations, as well as adapting to deformations and changes in lighting, making it more suitable for remote sensing image matching in multi-temporal scenarios.</p><p>Traditional feature matching algorithms, such as SIFT [<xref rid="B18-sensors-25-05581" ref-type="bibr">18</xref>] and ORB [<xref rid="B19-sensors-25-05581" ref-type="bibr">19</xref>], are widely utilized to match various types of remote sensing images. However, these conventional methods face several limitations in multi-temporal remote sensing image matching. These include deficiencies in feature stability under dynamic object interference, as well as insufficient sensitivity to radiometric changes and geometric deformations [<xref rid="B20-sensors-25-05581" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05581" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05581" ref-type="bibr">22</xref>]. Consequently, the accuracy and robustness of matching are significantly compromised.</p><p>With the rapid advancement of deep learning technologies, numerous deep learning-based feature matching methods have been applied to multi-temporal remote sensing image matching. Among these, SuperPoint [<xref rid="B11-sensors-25-05581" ref-type="bibr">11</xref>] is a self-supervised dual-branch network model that simultaneously extracts the locations of feature points and generates descriptors. LoFTR [<xref rid="B23-sensors-25-05581" ref-type="bibr">23</xref>] is a Transformer-based model for local feature matching that enhances matching accuracy in weak texture scenarios through a detector-free design. However, in complex multi-temporal environments, remote sensing image matching may encounter issues such as low matching accuracy or failure. To address these challenges, we propose a novel framework for multi-temporal remote sensing image matching that optimizes the matching process through feature point extraction and descriptor enhancement.</p></sec><sec id="sec3-sensors-25-05581"><title>3. Method</title><sec id="sec3dot1-sensors-25-05581"><title>3.1. Overview of the Proposed Method</title><p>Our framework incorporates a feature extraction network that integrates multiple modalities of perception and a descriptor enhancement module designed to improve the performance of feature point descriptors. The main workflow and details are illustrated in <xref rid="sensors-25-05581-f001" ref-type="fig">Figure 1</xref>.</p><p>Initially, the encoder of the feature extraction network extracts high-level feature representations from the remote sensing imagery input, producing a feature tensor. Subsequently, the multi-modal perception layer further processes the output feature tensor from the encoder to enhance the focus on significant features (<xref rid="sensors-25-05581-f001" ref-type="fig">Figure 1</xref>a). Next, the feature decoder, comprising both the feature point detection decoder and the descriptor decoder (<xref rid="sensors-25-05581-f001" ref-type="fig">Figure 1</xref>b), identifies the locations of image feature points based on the input feature tensor while generating descriptors for each detected feature point. Finally, the extracted feature point descriptors are fed into the descriptor enhancement module (<xref rid="sensors-25-05581-f001" ref-type="fig">Figure 1</xref>c), which enhances the performance of these descriptors through self-enhancement and cross-enhancement phases. This leads to the matching of enhanced feature point descriptors, resulting in the final image matching outcomes.</p></sec><sec id="sec3dot2-sensors-25-05581"><title>3.2. Feature Extraction Network</title><p>(1) Shared Encoder: The purpose of the shared encoder is to reduce the dimensionality of the input images and extract low-dimensional yet information-rich feature representations. This encoder takes the form of a VGG-style architecture, consisting of convolutional layers, max pooling layers, and nonlinear activation functions. The input image first passes through a 3 &#215; 3 convolutional layer, mapping it into a 64-dimensional feature space, followed by the application of the ReLU activation function to introduce nonlinearity. Subsequently, the image undergoes three convolutional stages, each comprising two 3 &#215; 3 convolutional layers, with a ReLU activation function applied after each convolutional layer. After the convolutional layers are passed through in each stage, a 2 &#215; 2 max pooling layer is employed for downsampling, effectively halving the size of the feature maps. Specifically, the first stage maintains 64 channels, extracting basic features before pooling to reduce the dimensions. The second stage increases the number of channels to 128, capturing mid-level features before further pooling. Finally, the third stage increases the channels to 256, focusing on high-level features before completing the third pooling operation. Following these three downsampling operations, an additional 3 &#215; 3 convolutional layer reduces the number of channels from 256 to 128, serving as a shared feature input for subsequent key point detection and descriptor generation. Ultimately, the spatial dimensions of the images are reduced to one-eighth of their original size. If the input image has dimensions H &#215; W, the output feature map will measure H/8 &#215; W/8.</p><p>(2) Multi-Perspective Layer: To enhance the number of feature points in multi-temporal images and achieve higher positional accuracy, a multi-perspective layer is introduced into the feature extraction network, which combines attention mechanisms [<xref rid="B24-sensors-25-05581" ref-type="bibr">24</xref>] with depthwise separable convolutions [<xref rid="B25-sensors-25-05581" ref-type="bibr">25</xref>]. The main structure is illustrated in <xref rid="sensors-25-05581-f002" ref-type="fig">Figure 2</xref>.</p><p>Initially, the input feature map <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> passes through a channel attention module, where global average pooling and global max pooling compress the spatial information of each channel into two vectors. These vectors are then fed into a shared multi-layer perceptron (<italic toggle="yes">MLP</italic>). The <italic toggle="yes">MLP</italic> employs a two-layer fully connected network structure, using the ReLU activation function after the first fully connected layer, with the hidden layer dimension set to 1/16 of the feature map&#8217;s channel count. After processing using the <italic toggle="yes">MLP</italic>, the two vectors are summed and passed through a sigmoid activation function to generate channel attention weights <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (Equation (1)). Subsequently, this weight is multiplied with the original feature map <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channel-wise to obtain the channel-weighted feature map <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (Equation (2)).<disp-formula id="FD1-sensors-25-05581"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05581"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8855;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the sigmoid function; <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8855;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication. During multiplication, the attention values are broadcast accordingly: channel attention values are broadcast along the spatial dimension and vice versa.</p><p>Subsequently, the feature map <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is processed by a spatial attention module. First, maximum pooling and average pooling are performed along the channel dimension to obtain two single-channel feature maps. These maps are concatenated and passed through a convolutional layer to reduce them to a single-channel feature map. After this, the sigmoid activation function generates the spatial attention weights <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (Equation (3)). Finally, this weight is multiplied pixel-wise with the channel-weighted feature map <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to produce the final enhanced feature map <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8243;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (Equation (4)).<disp-formula id="FD3-sensors-25-05581"><label>(3)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05581"><label>(4)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>&#8243;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mfenced><mml:mo>&#8855;</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the sigmoid function; <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the convolution operation; and&#160;<inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8855;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication. During multiplication, the attention values are broadcast accordingly: channel attention values are broadcast along the spatial dimension and vice versa.</p><p>Moreover, the spatial attention module employs depthwise separable convolutions to aggregate the resulting feature representations. Specifically, a depthwise convolution is first applied to each channel along the spatial dimension, allowing for the retention of more local features within the channels. This approach is particularly effective for processing feature maps with complex scenes, as depthwise convolution can better capture characteristics such as the edges and textures of objects [<xref rid="B26-sensors-25-05581" ref-type="bibr">26</xref>]. Subsequently, a 1 &#215; 1 pointwise convolution is used to fuse the results of the depthwise convolutions. Through pointwise convolution, features from different channels can be effectively combined, enhancing the expressiveness of the features while reducing both the number of parameters and computational complexity, thereby improving the overall performance of the model [<xref rid="B27-sensors-25-05581" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05581" ref-type="bibr">28</xref>].</p><p>(3) Key Point Detection Decoder: The feature point detection decoder first generates a feature tensor, where 64 channels represent the probabilities of feature points, and the 65th channel serves as an invalid channel (dustbin). Specifically, the first 64 channels correspond to the 64 possible sub-pixel locations within each 8 &#215; 8-pixel block of the input image, indicating the candidate probabilities for each position to become a feature point. The 65th channel serves as an invalid channel, denoting that there are no valid feature points within the 8 &#215; 8-pixel block. Subsequently, the Softmax function is applied to compute the probability distribution over the 65 channel values for each 8 &#215; 8-pixel block, resulting in the probabilities for each sub-pixel location (first 64 channels) and the dustbin (65th channel). The probability distribution for each position can be formulated as follows:<disp-formula id="FD5-sensors-25-05581"><label>(5)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>65</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability value of the <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channel at the feature map position (<inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>); <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the value of the input tensor at position (<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) for channel <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>;&#160;<inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>65</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the summation over all channels <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from 1 to 65; and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the exponential function, which is used to convert scores into positive values in preparation for normalization.</p><p>For each 8 &#215; 8-pixel block, the sub-pixel location with the highest probability among the first 64 channels is selected as a candidate point. If the probability of this location exceeds 0.01 and is greater than the probability of the dustbin channel, the candidate point is retained; otherwise, it is considered that the block contains no feature points. For the filtered candidate points, a non-maximum suppression (NMS) algorithm is performed with a radius of 3 pixels at the original image scale, examining all candidate points. For each candidate point, if its probability is the maximum within a 3 &#215; 3 neighborhood, it is retained; otherwise, it is discarded to eliminate redundant points that are too spatially close.</p><p>(4) Feature Descriptor Decoder: The feature descriptor decoder generates a descriptor tensor, which is subject to L2 normalization for enhanced stability. Subsequently, bilinear interpolation is employed to sample the descriptor tensor at the locations of the key points, yielding a descriptor for each identified key point.</p></sec><sec id="sec3dot3-sensors-25-05581"><title>3.3. Descriptor Enhancement Module</title><p>To address the issue of poor stability for the feature point descriptors in multi-temporal remote sensing image matching, we introduced a descriptor enhancer. The model structure is illustrated in <xref rid="sensors-25-05581-f003" ref-type="fig">Figure 3</xref> and primarily consists of two components.</p><list list-type="simple"><list-item><label>(1)</label><p>Self-Enhancement Phase: For each feature point detected in the image, a multilayer perceptron (<italic toggle="yes">MLP</italic>) network is applied to project its original descriptor; it is then mapped into a new space to obtain a preliminary enhanced descriptor. The <italic toggle="yes">MLP</italic> serves as a universal function approximator, as demonstrated by the Cybenko theorem [<xref rid="B29-sensors-25-05581" ref-type="bibr">29</xref>]. We utilized <italic toggle="yes">MLP</italic> to approximate the projection function, denoting it as <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The architecture consists of two fully connected layers, with an ReLU activation function applied between them. Additionally, layer normalization and dropout were utilized as regularization techniques to prevent overfitting. The transformed descriptor <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> for key point <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> was defined as the nonlinear projection of the extracted descriptor <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:</p></list-item></list><p>
<disp-formula id="FD6-sensors-25-05581"><label>(6)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#8592;</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>To leverage the geometric information of feature points valuable for image matching, we employed another <italic toggle="yes">MLP</italic>&#160;<inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to embed the geometric information into a high-dimensional vector, further enhancing the descriptor. We not only encoded the 2D position of the key point <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but also included other relevant information such as scale <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, orientation <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and detection score <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The high-dimensional embedding of the geometric information was added to the transformed descriptor as follows:<disp-formula id="FD7-sensors-25-05581"><label>(7)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#8592;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents all available geometric information as aforementioned.</p><p>From this, the descriptors processed through the <italic toggle="yes">MLP</italic> incorporate geometric information of the key points, resulting in enhanced discriminative power and robustness.</p><list list-type="simple"><list-item><label>(2)</label><p>Cross-Enhancement Phase: The self-enhancement phase does not consider the potential correlations among different feature points, enhancing each descriptor independently. For instance, it fails to leverage the spatial relationships between these feature points; however, spatial contextual cues can significantly improve feature matching capabilities. Consequently, the descriptors obtained after the self-enhancement phase perform poorly in scenarios involving variations in perspective and complex terrain. To address this issue, we further processed these descriptors through the cross-enhancement phase. We took the descriptors generated in the self-enhancement phase as input and utilized the attention mechanism of a Transformer to capture the spatial context relationships among the key points. We denote the Transformer as &#8220;Trans,&#8221; and the projection is described as follows:</p></list-item></list><p><disp-formula id="FD8-sensors-25-05581"><label>(8)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>&#8592;</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where the input of the Transformer is <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> local features within the same image, and the output comprises enhanced feature descriptors.</p><p>Furthermore, we employed an efficient Attention-Free Transformer (AFT) [<xref rid="B30-sensors-25-05581" ref-type="bibr">30</xref>] to replace the multi-head self-attention (MHA) operation with the Vanilla Transformer. Unlike the traditional MHA architecture, AFT does not utilize or approximate dot-product attention. Specifically, AFT rearranges the computation order of <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (Query), <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (Key), and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (Value), similar to linear attention, but multiplies the elements of K and V instead of using matrix multiplication. The no-attention Transformer for key point <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> can be formulated as follows:<disp-formula id="FD9-sensors-25-05581"><label>(9)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8855;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mo>&#8855;</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a sigmoid function; <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th row of <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th rows of <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8855;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication; and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the exponential function.</p><p>The Transformer can integrate global contextual information [<xref rid="B31-sensors-25-05581" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05581" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05581" ref-type="bibr">33</xref>], allowing each descriptor to be adjusted based on its neighboring information, thereby further enhancing its discriminative power and robustness. The ultimately enhanced descriptors not only incorporate geometric information of the key points themselves but also fuse global contextual information, enabling them to handle challenging situations better, such as variations in perspective and local deformations.</p><list list-type="simple"><list-item><label>(3)</label><p>Loss Function: We used the descriptor matching problem as a nearest neighbor retrieval task and Average Precision (<inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) to train the descriptors. Given the transformed local feature descriptor <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, our objective was to maximize the <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B34-sensors-25-05581" ref-type="bibr">34</xref>] of all descriptors. Therefore, the training goal was to minimize the following cost function:</p></list-item></list><p>
<disp-formula id="FD10-sensors-25-05581"><label>(10)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>To ensure that the original descriptors were enhanced, we proposed using an alternative loss function to compel the performance of the transformed descriptors to surpass that of the original descriptors:<disp-formula id="FD11-sensors-25-05581"><label>(11)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The final loss was the sum of the two aforementioned losses:<disp-formula id="FD12-sensors-25-05581"><label>(12)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#955; is a weight to regulate the second term. We used a differentiable approach [<xref rid="B35-sensors-25-05581" ref-type="bibr">35</xref>] to compute the Average Precision (<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) for each descriptor.</p></sec></sec><sec id="sec4-sensors-25-05581"><title>4. Experimental Results and Discussion</title><sec id="sec4dot1-sensors-25-05581"><title>4.1. Experimental Data and Parameter Settings</title><p>In this study, we utilized two datasets: the LEVIR-CD dataset [<xref rid="B36-sensors-25-05581" ref-type="bibr">36</xref>] and the CLCD dataset [<xref rid="B37-sensors-25-05581" ref-type="bibr">37</xref>]. The LEVIR-CD dataset comprises 637 pairs of dual-temporal images spanning 5 to 14 years, with a spatial resolution of 0.5 m. It draws on information from various types of buildings, such as villas, high-rise apartments, small garages, and large warehouses. From the image pairs in the LEVIR-CD dataset, we selected 445 pairs for training, 128 pairs for validation, and 64 pairs for testing. The CLCD dataset comprises 600 pairs of cultivated land change sample images, captured by the GaoFen-2 satellite in Guangdong Province, China, during 2017 and 2019. The spatial resolution of these images ranges from 0.5 to 2.0 m and incorporates cultivated land, forest, shrubland, and grassland. For the CLCD dataset, we selected 360 pairs for training, 120 pairs for validation, and 120 pairs for testing.</p><p>All algorithms in this study were implemented using the PyTorch 1.8.0 framework on an Nvidia GeForce RTX 3090 GPU (Nvidia, Santa Clara, CA, USA). All experiments were conducted with a random seed set to 42. The optimizer used was Adam, with an initial learning rate of 10<sup>&#8722;3</sup> and a minimum learning rate set to 10<sup>&#8722;6</sup>. The <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-neighborhood was set to three pixels. During training, the batch size was set to two, and the maximum number of training epochs was set to ninety. The learning rate was decayed every 10 epochs by multiplying it by a decay factor of gamma = 0.5, and the decay stopped when the learning rate reached the minimum threshold.</p></sec><sec id="sec4dot2-sensors-25-05581"><title>4.2. Performance Metrics</title><p>In the matching experiments, we used the following evaluation metrics for the algorithms: the number of correctly matched points (NCM), matching success rate (SR), root mean square error of matched points (RMSE), and runtime (RT).
<list list-type="simple"><list-item><label>(1)</label><p>NCM: The NCM represents the total number of correctly matched points. One image in the pair is designated as the reference image, while the other serves as the target image. The feature point positions obtained from the matching algorithm on the target image are denoted as <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and the corresponding ground truth positions on the reference image are denoted as <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Using a matching accuracy threshold <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (in this experiment, points within a 3-pixel error margin were considered correctly matched), the determination of correctly matched points is shown in the following equation:<disp-formula id="FD13-sensors-25-05581"><label>(13)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">&#8741;</mml:mo><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#8741;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#8804;</mml:mo><mml:mi>&#949;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the mapping matrix between the reference image and the matched image.</p></list-item><list-item><label>(2)</label><p>SR: The SR is defined as the ratio of the number of correctly matched points (NCMs) to the total number of matched points provided by the algorithm (<inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). The calculation formula is as follows: <disp-formula id="FD14-sensors-25-05581"><label>(14)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the NCM.</p></list-item><list-item><label>(3)</label><p>RMSE: The RMSE is a commonly used metric for evaluating the performance of matching algorithms. It measures the degree of deviation between the predicted positions of matched points and their corresponding ground truth values. Specifically, the RMSE represents the average deviation of the matching points, reflecting the magnitude of the error at each matched point coordinate. The calculation formula is as follows:<disp-formula id="FD15-sensors-25-05581"><label>(15)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">&#8741;</mml:mo><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">&#8741;</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the NCM and <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the mapping matrix between the reference image and the matched image.</p></list-item></list></p></sec><sec id="sec4dot3-sensors-25-05581"><title>4.3. Matching Performance Evaluation</title><p>To comprehensively evaluate the image matching performance of our proposed method, we conducted a comparative analysis with several state-of-the-art image matching algorithms, including SIFT [<xref rid="B18-sensors-25-05581" ref-type="bibr">18</xref>], SOSNet [<xref rid="B38-sensors-25-05581" ref-type="bibr">38</xref>], D2-Net [<xref rid="B39-sensors-25-05581" ref-type="bibr">39</xref>], SuperPoint [<xref rid="B11-sensors-25-05581" ref-type="bibr">11</xref>], and LoFTR [<xref rid="B23-sensors-25-05581" ref-type="bibr">23</xref>]. All methods were assessed using a publicly available code and the optimal parameter settings provided by the respective authors. The datasets and training configurations were consistent with those utilized in this research. We evaluated our model using the test sets selected from the LEVIR-CD and CLCD datasets, which comprise three groups of image pairs:<list list-type="simple"><list-item><label>(1)</label><p>The original dual-temporal remote sensing image pairs;</p></list-item><list-item><label>(2)</label><p>Dual-temporal remote sensing image pairs subjected to angular transformations;</p></list-item><list-item><label>(3)</label><p>Dual-temporal remote sensing image pairs subjected to scale transformations.</p></list-item></list></p><p>Specifically, we conducted tests under multiple parameters, including rotation angles ranging from 10&#176; to 40&#176; and scaling factors from 0.5 to 0.9. For demonstration and analysis, we selected image pairs corresponding to a rotation angle of 30&#176; and a scaling factor of 0.7 as representative examples. For each test, image matching was performed using a matching error threshold of 3 pixels. The results obtained from experiments conducted on the test set were averaged. <xref rid="sensors-25-05581-f004" ref-type="fig">Figure 4</xref> presents representative image pairs demonstrating the matching results of different algorithms, while <xref rid="sensors-25-05581-t001" ref-type="table">Table 1</xref> summarizes the average quantitative evaluation results of six methods across three groups of test datasets.</p><p>In the first group, the LEVIR-CD dataset serves as a high-resolution building change detection dataset, with challenges primarily arising from occlusion and repetitive textures. As a representative of traditional algorithms, SIFT performs poorly compared to other learning-based algorithms across various metrics, mainly due to its reliance on handcrafted feature extraction processes, which often fail to match or result in inaccuracies when changes in land cover exceed expected ranges. SOSNet optimizes local descriptor learning through second-order similarity regularization, demonstrating good performance in simple building regions; however, it suffers from mis-matching due to similar textures and weak feature discrimination. Both D2-Net and SuperPoint algorithms exhibit overall strong performance, with success rates (SRs) exceeding 90%. The matching time of the SuperPoint algorithm is notably the shortest at 0.76 s, which is attributable to its end-to-end output descriptors generated by a fully convolutional network, leveraging high GPU parallel computing efficiency. LoFTR effectively addresses challenges related to occlusion and fragmented boundary matching via global attention and Transformers, and achieves excellent performance across all metrics, with a root mean square error (RMSE) of just 1.58. Our method achieved the highest number of correctly matched points (NCM) and SR, reaching 263 and 99.25%, respectively. This indicates that our method provides a rich quantity of feature points while maintaining high accuracy among the successfully matched feature points. Although our RMSE is higher than that of LoFTR, our method outperforms it in inference speed. For the CLCD land cover dataset, the primary challenges include mixed pixels and seasonal spectral differences. All algorithms performed worse on the CLCD dataset compared to the LEVIR-CD dataset, primarily because the LEVIR-CD dataset contains predominantly well-formed building areas and has a higher resolution than the CLCD dataset. Our method outperformed other algorithms in terms of NCM, SR, and RMSE on the CLCD dataset, although the matching time was longer than that of the SuperPoint algorithm. This can be attributed to our method&#8217;s design, which is suitable for images that are multi-temporal or that have temporal differences by utilizing a multi-sensing fusion feature extraction network to enhance feature discrimination capability, while also optimizing the performance of feature point descriptors through a descriptor enhancement module, albeit at the cost of increased network complexity. Comparisons on the LEVIR-CD and CLCD datasets demonstrate that our method provides accurate matching in scenarios involving land cover changes and seasonal lighting differences, while maintaining a high level of reliability.</p><p>In the second and third groups, we performed certain degrees of rotation and scaling transformations on the initial images to evaluate the algorithms&#8217; capability for image matching under different viewpoints and resolutions. The results indicate that both SIFT and SOSNet algorithms exhibit a significant decline in performance during the image matching process after rotation and scaling transformations, further highlighting their limitations. While D2-Net and SuperPoint algorithms also experienced a decrease in metrics, both maintained good overall performance with minimal differences between them. The multi-scale convolution aspect of the D2-Net algorithm provides advantages in scenarios involving scale variations. Additionally, the training process of the SuperPoint algorithm includes random rotation augmentation, which contributes to its robustness against angular rotations. LoFTR demonstrates good overall matching performance; however, it tends to lose features in scenarios with scale variations and incurs a higher computational cost. In contrast, our method achieves the best performance in terms of the number of correctly matched points (NCMs), success rate (SR), and root mean square error (RMSE), while also demonstrating excellent inference speed. Experimental results demonstrate that after descriptor enhancement, our method continues to deliver accurate and stable matching results in multi-temporal images with perspective and scale variations.</p></sec><sec id="sec4dot4-sensors-25-05581"><title>4.4. Ablation Study</title><p>To demonstrate the effectiveness of each module in the proposed method, we conducted ablation experiments to quantify the impact of each module on matching performance. With the pixel error threshold set to 3, and with other structures and processes kept constant, we randomly selected 10% of the experimental data from the initial image pairs in the test set for testing. This setup aims to enhance efficiency while validating the stability of the model design with fewer experimental data. We tested different combinations of these modules and compared whether the image matching framework utilized multiple perception layers and descriptor enhancement modules. The average quantitative results of these experiments are presented in <xref rid="sensors-25-05581-t002" ref-type="table">Table 2</xref>.</p><p>It is evident from <xref rid="sensors-25-05581-t002" ref-type="table">Table 2</xref> that each module, when applied individually, demonstrated significant improvements relative to the baseline model. Specifically, removing the multiple perception layer led to a decline in the model&#8217;s feature point extraction performance, with a noticeable decrease in the NCM metric, as well as declines in SR and increases in RMSE. This indicates that the multiple perception layer can focus more effectively on prominent features while preserving important information in the images, thereby improving the quantity and positional accuracy of feature points. After removing the descriptor enhancement module, the model&#8217;s descriptor matching performance deteriorated significantly, with notable decreases in SR and increases in RMSE, alongside a slight decline in NCM. This is attributed to the descriptor enhancement module&#8217;s ability to enhance the geometric information of feature point descriptors while integrating global contextual information, which improves the model&#8217;s descriptor matching performance. Furthermore, the addition of both the multiple perception layer and the descriptor enhancement module increased the model&#8217;s network complexity to some extent, resulting in longer run times.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05581"><title>5. Conclusions</title><p>To address the challenge of matching remote sensing images in multi-temporal scenarios, in this paper, a method is proposed that enhances multi-temporal remote sensing image matching through multiple perception and descriptor augmentation. This approach integrates a feature extraction network informed by multiple perceptions to mitigate the poor matching outcomes caused by land cover changes over time and improve the extraction capability of effective feature points. Additionally, to improve matching performance for multi-temporal remote sensing images with varying angles and scales, a descriptor enhancement module comprising both self-enhancement and cross-enhancement stages is employed to optimize feature descriptors. The experimental results demonstrate that our method outperforms existing remote sensing image matching techniques in terms of accuracy and robustness on multi-temporal datasets.</p><p>However, the challenges of remote sensing image matching in multi-temporal contexts extend beyond land cover changes and variations in angle and scale; they also include sensor discrepancies and cross-modal image matching issues. The proposed method was primarily evaluated using homogeneous optical data in this study; thus, the effectiveness of its performance on highly heterogeneous or cross-modal data remains open to further investigation. Additionally, our experiments were conducted mainly on datasets covering specific geographic regions. Although the selected benchmarks are standard and widely utilized, our method&#8217;s performance requires comprehensive validation across different geographic areas and images with varying sensor characteristics. Therefore, future research should focus on these challenges to further enhance the practicality of the algorithm.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.Z.; methodology, W.Z. and J.Z.; software, J.Z.; validation, J.Z.; formal analysis, W.Z. and X.T.; investigation, J.Z.; resources, X.T. and J.Z.; data curation, J.Z.; writing&#8212;original draft preparation, J.Z.; writing&#8212;review and editing, W.Z. and J.Z.; visualization, X.T.; supervision, J.Z.; funding acquisition, X.T. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The LEVIR-CD dataset is available for download at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://justchenhao.github.io/LEVIR/">https://justchenhao.github.io/LEVIR/</uri>. The CLCD dataset is available for download at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/liumency/CropLand-CD">https://github.com/liumency/CropLand-CD</uri>.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05581"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bovolo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bruzzone</surname><given-names>L.</given-names></name><name name-style="western"><surname>King</surname><given-names>R.L.</given-names></name></person-group><article-title>Introduction to the special issue on analysis of multitemporal remote sensing data</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2013</year><volume>51</volume><fpage>1867</fpage><lpage>1869</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2013.2251802</pub-id></element-citation></ref><ref id="B2-sensors-25-05581"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name></person-group><article-title>Spatio&#8211;temporal&#8211;spectral collaborative learning for spatio&#8211;temporal fusion with land cover changes</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2022.3185459</pub-id></element-citation></ref><ref id="B3-sensors-25-05581"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>B.H.</given-names></name></person-group><article-title>Multi-Temporal Snow-Covered Remote Sensing Image Matching via Image Transformation and Multi-Level Feature Extraction</article-title><source>Optics</source><year>2024</year><volume>5</volume><fpage>392</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.3390/opt5040029</pub-id></element-citation></ref><ref id="B4-sensors-25-05581"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mohammadi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sedaghat</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jodeiri Rad</surname><given-names>M.</given-names></name></person-group><article-title>Rotation-invariant self-similarity descriptor for multi-temporal remote sensing image registration</article-title><source>Photogramm. Rec.</source><year>2022</year><volume>37</volume><fpage>6</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1111/phor.12402</pub-id></element-citation></ref><ref id="B5-sensors-25-05581"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Habeeb</surname><given-names>H.N.</given-names></name><name name-style="western"><surname>Mustafa</surname><given-names>Y.T.</given-names></name></person-group><article-title>Deep Learning-Based Prediction of Forest Cover Change in Duhok, Iraq: Past and Future</article-title><source>Forestist</source><year>2025</year><volume>75</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.5152/forestist.2025.24068</pub-id></element-citation></ref><ref id="B6-sensors-25-05581"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>C.</given-names></name></person-group><article-title>A change detection method based on multi-scale adaptive convolution kernel network and multimodal conditional random field for multi-temporal multispectral images</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>5368</elocation-id><pub-id pub-id-type="doi">10.3390/rs14215368</pub-id></element-citation></ref><ref id="B7-sensors-25-05581"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name></person-group><article-title>Image matching from handcrafted to deep features: A survey</article-title><source>Int. J. Comput. Vis.</source><year>2021</year><volume>129</volume><fpage>23</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1007/s11263-020-01359-2</pub-id></element-citation></ref><ref id="B8-sensors-25-05581"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>J.</given-names></name></person-group><article-title>Robust feature matching for remote sensing image registration via locally linear transforming</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2015</year><volume>53</volume><fpage>6469</fpage><lpage>6481</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2015.2441954</pub-id></element-citation></ref><ref id="B9-sensors-25-05581"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Rottensteiner</surname><given-names>F.</given-names></name><name name-style="western"><surname>Heipke</surname><given-names>C.</given-names></name></person-group><article-title>Feature detection and description for image matching: From hand-crafted design to deep learning</article-title><source>Geo-Spat. Inf. Sci.</source><year>2021</year><volume>24</volume><fpage>58</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1080/10095020.2020.1843376</pub-id></element-citation></ref><ref id="B10-sensors-25-05581"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name></person-group><article-title>Image feature matching based on deep learning</article-title><source>Proceedings of the 2018 IEEE 4th International Conference on Computer and Communications (ICCC)</source><conf-loc>Chengdu, China</conf-loc><conf-date>7&#8211;10 December 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2018</year><fpage>1752</fpage><lpage>1756</lpage></element-citation></ref><ref id="B11-sensors-25-05581"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>DeTone</surname><given-names>D.</given-names></name><name name-style="western"><surname>Malisiewicz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group><article-title>Superpoint: Self-supervised interest point detection and description</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>224</fpage><lpage>236</lpage></element-citation></ref><ref id="B12-sensors-25-05581"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>D.</given-names></name></person-group><article-title>Featurebooster: Boosting feature descriptors with a lightweight neural network</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>7630</fpage><lpage>7639</lpage></element-citation></ref><ref id="B13-sensors-25-05581"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Du</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>A novel region-based image registration method for multisource remote sensing images via CNN</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2020</year><volume>14</volume><fpage>1821</fpage><lpage>1831</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2020.3047656</pub-id></element-citation></ref><ref id="B14-sensors-25-05581"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Hui</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>M.</given-names></name></person-group><article-title>A Method Combining Discrete Cosine Transform with Attention for Multi-Temporal Remote Sensing Image Matching</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>1345</elocation-id><pub-id pub-id-type="doi">10.3390/s25051345</pub-id><pub-id pub-id-type="pmid">40096159</pub-id><pub-id pub-id-type="pmcid">PMC11902387</pub-id></element-citation></ref><ref id="B15-sensors-25-05581"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>B.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>C.-I.</given-names></name></person-group><article-title>Iterative scale-invariant feature transform for remote sensing image registration</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2020</year><volume>59</volume><fpage>3244</fpage><lpage>3265</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2020.3008609</pub-id></element-citation></ref><ref id="B16-sensors-25-05581"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Okorie</surname><given-names>A.</given-names></name><name name-style="western"><surname>Makrogiannis</surname><given-names>S.</given-names></name></person-group><article-title>Region-based image registration for remote sensing imagery</article-title><source>Comput. Vis. Image Underst.</source><year>2019</year><volume>189</volume><fpage>102825</fpage><pub-id pub-id-type="doi">10.1016/j.cviu.2019.102825</pub-id><pub-id pub-id-type="pmid">37622168</pub-id><pub-id pub-id-type="pmcid">PMC10448482</pub-id></element-citation></ref><ref id="B17-sensors-25-05581"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Dan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name></person-group><article-title>Multi-temporal remote sensing image registration using deep convolutional features</article-title><source>IEEE Access</source><year>2018</year><volume>6</volume><fpage>38544</fpage><lpage>38555</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2853100</pub-id></element-citation></ref><ref id="B18-sensors-25-05581"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lowe</surname><given-names>D.G.</given-names></name></person-group><article-title>Distinctive image features from scale-invariant keypoints</article-title><source>Int. J. Comput. Vis.</source><year>2004</year><volume>60</volume><fpage>91</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000029664.99615.94</pub-id></element-citation></ref><ref id="B19-sensors-25-05581"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rublee</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rabaud</surname><given-names>V.</given-names></name><name name-style="western"><surname>Konolige</surname><given-names>K.</given-names></name><name name-style="western"><surname>Bradski</surname><given-names>G.</given-names></name></person-group><article-title>ORB: An efficient alternative to SIFT or SURF</article-title><source>Proceedings of the 2011 International Conference on Computer Vision</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>6&#8211;13 November 2011</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2011</year><fpage>2564</fpage><lpage>2571</lpage></element-citation></ref><ref id="B20-sensors-25-05581"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>Y.</given-names></name></person-group><article-title>Weak texture remote sensing image matching based on hybrid domain features and adaptive description method</article-title><source>Photogramm. Rec.</source><year>2023</year><volume>38</volume><fpage>537</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1111/phor.12464</pub-id></element-citation></ref><ref id="B21-sensors-25-05581"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name></person-group><article-title>Feature matching for remote-sensing image registration via neighborhood topological and affine consistency</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>2606</elocation-id><pub-id pub-id-type="doi">10.3390/rs14112606</pub-id></element-citation></ref><ref id="B22-sensors-25-05581"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>R.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Advances and opportunities in remote sensing image geometric registration: A systematic review of state-of-the-art approaches and future research directions</article-title><source>IEEE Geosci. Remote Sens. Mag.</source><year>2021</year><volume>9</volume><fpage>120</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1109/MGRS.2021.3081763</pub-id></element-citation></ref><ref id="B23-sensors-25-05581"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name></person-group><article-title>LoFTR: Detector-free local feature matching with transformers</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>8922</fpage><lpage>8931</lpage></element-citation></ref><ref id="B24-sensors-25-05581"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Woo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>Convolutional block attention module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>3</fpage><lpage>19</lpage></element-citation></ref><ref id="B25-sensors-25-05581"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><article-title>Xception: Deep learning with depthwise separable convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>1251</fpage><lpage>1258</lpage></element-citation></ref><ref id="B26-sensors-25-05581"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>Z.Y.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>Z.</given-names></name></person-group><article-title>CNN with depthwise separable convolutions and combined kernels for rating prediction</article-title><source>Expert Syst. Appl.</source><year>2021</year><volume>170</volume><fpage>114528</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2020.114528</pub-id></element-citation></ref><ref id="B27-sensors-25-05581"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.</given-names></name></person-group><article-title>Deep learning-based technique for remote sensing image enhancement using multiscale feature fusion</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>673</elocation-id><pub-id pub-id-type="doi">10.3390/s24020673</pub-id><pub-id pub-id-type="pmcid">PMC11154389</pub-id><pub-id pub-id-type="pmid">38276366</pub-id></element-citation></ref><ref id="B28-sensors-25-05581"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gui</surname><given-names>G.</given-names></name></person-group><article-title>Attention mechanism and depthwise separable convolution aided 3DCNN for hyperspectral remote sensing image classification</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>2215</elocation-id><pub-id pub-id-type="doi">10.3390/rs14092215</pub-id></element-citation></ref><ref id="B29-sensors-25-05581"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dusmanu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Miksik</surname><given-names>O.</given-names></name><name name-style="western"><surname>Sch&#246;nberger</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Pollefeys</surname><given-names>M.</given-names></name></person-group><article-title>Cross-descriptor visual localization and mapping</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>6058</fpage><lpage>6067</lpage></element-citation></ref><ref id="B30-sensors-25-05581"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Talbott</surname><given-names>W.</given-names></name><name name-style="western"><surname>Srivastava</surname><given-names>N.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Goh</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Susskind</surname><given-names>J.</given-names></name></person-group><article-title>An attention free transformer</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2105.14103</pub-id><pub-id pub-id-type="arxiv">2105.14103</pub-id></element-citation></ref><ref id="B31-sensors-25-05581"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>L.</given-names></name><name name-style="western"><surname>He</surname><given-names>G.</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>B.A.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Y.</given-names></name></person-group><article-title>Transformers for remote sensing: A systematic review and analysis</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3495</elocation-id><pub-id pub-id-type="doi">10.3390/s24113495</pub-id><pub-id pub-id-type="pmid">38894286</pub-id><pub-id pub-id-type="pmcid">PMC11175147</pub-id></element-citation></ref><ref id="B32-sensors-25-05581"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name></person-group><article-title>Remote sensing image change detection with transformers</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2021</year><volume>60</volume><fpage>5607514</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3095166</pub-id></element-citation></ref><ref id="B33-sensors-25-05581"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Ward</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Multimodal image fusion via self-supervised transformer</article-title><source>IEEE Sens. J.</source><year>2023</year><volume>23</volume><fpage>9796</fpage><lpage>9807</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2023.3263336</pub-id></element-citation></ref><ref id="B34-sensors-25-05581"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Boyd</surname><given-names>K.</given-names></name><name name-style="western"><surname>Eng</surname><given-names>K.H.</given-names></name><name name-style="western"><surname>Page</surname><given-names>C.D.</given-names></name></person-group><article-title>Area under the precision-recall curve: Point estimates and confidence intervals</article-title><source>Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases</source><conf-loc>Prague, Czech Republic</conf-loc><conf-date>23&#8211;27 September 2013</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2013</year><fpage>451</fpage><lpage>466</lpage></element-citation></ref><ref id="B35-sensors-25-05581"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cakir</surname><given-names>F.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kulis</surname><given-names>B.</given-names></name><name name-style="western"><surname>Sclaroff</surname><given-names>S.</given-names></name></person-group><article-title>Deep metric learning to rank</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>1861</fpage><lpage>1870</lpage></element-citation></ref><ref id="B36-sensors-25-05581"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name></person-group><article-title>A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><elocation-id>1662</elocation-id><pub-id pub-id-type="doi">10.3390/rs12101662</pub-id></element-citation></ref><ref id="B37-sensors-25-05581"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name></person-group><article-title>A CNN-transformer network with multiscale context aggregation for fine-grained cropland change detection</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2022</year><volume>15</volume><fpage>4297</fpage><lpage>4306</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2022.3177235</pub-id></element-citation></ref><ref id="B38-sensors-25-05581"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Heijnen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Balntas</surname><given-names>V.</given-names></name></person-group><article-title>Sosnet: Second order similarity regularization for local descriptor learning</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>11016</fpage><lpage>11025</lpage></element-citation></ref><ref id="B39-sensors-25-05581"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dusmanu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rocco</surname><given-names>I.</given-names></name><name name-style="western"><surname>Pajdla</surname><given-names>T.</given-names></name><name name-style="western"><surname>Pollefeys</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sivic</surname><given-names>J.</given-names></name><name name-style="western"><surname>Torii</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sattler</surname><given-names>T.</given-names></name></person-group><article-title>D2-net: A trainable cnn for joint description and detection of local features</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>8092</fpage><lpage>8101</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05581-f001" orientation="portrait"><label>Figure 1</label><caption><p>Workflow diagram of the image matching network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05581-g001.jpg"/></fig><fig position="float" id="sensors-25-05581-f002" orientation="portrait"><label>Figure 2</label><caption><p>Structure diagram of multiple perception layers.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05581-g002.jpg"/></fig><fig position="float" id="sensors-25-05581-f003" orientation="portrait"><label>Figure 3</label><caption><p>Structure diagram of the descriptor enhancement module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05581-g003.jpg"/></fig><fig position="float" id="sensors-25-05581-f004" orientation="portrait"><label>Figure 4</label><caption><p>Visualization of partial matching results from the LEVIR-CD and CLCD datasets: (<bold>a</bold>) matching results of initial multi-temporal remote sensing image pairs; (<bold>b</bold>) matching results of multi-temporal remote sensing image pairs after 30&#176; rotation; and (<bold>c</bold>) matching results of multi-temporal remote sensing image pairs after 0.7 scale reduction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05581-g004a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05581-g004b.jpg"/></fig><table-wrap position="float" id="sensors-25-05581-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05581-t001_Table 1</object-id><label>Table 1</label><caption><p>Average matching results for all test data across various algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Group</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LEVIR-CD</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CLCD</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NCM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SR</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RT</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NCM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SR</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RT</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">(a)</td><td align="center" valign="middle" rowspan="1" colspan="1">SIFT</td><td align="center" valign="middle" rowspan="1" colspan="1">82</td><td align="center" valign="middle" rowspan="1" colspan="1">86.74</td><td align="center" valign="middle" rowspan="1" colspan="1">2.34</td><td align="center" valign="middle" rowspan="1" colspan="1">2.53</td><td align="center" valign="middle" rowspan="1" colspan="1">76</td><td align="center" valign="middle" rowspan="1" colspan="1">86.53</td><td align="center" valign="middle" rowspan="1" colspan="1">2.52</td><td align="center" valign="middle" rowspan="1" colspan="1">2.61</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SOSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">121</td><td align="center" valign="middle" rowspan="1" colspan="1">89.83</td><td align="center" valign="middle" rowspan="1" colspan="1">2.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">108</td><td align="center" valign="middle" rowspan="1" colspan="1">88.25</td><td align="center" valign="middle" rowspan="1" colspan="1">2.37</td><td align="center" valign="middle" rowspan="1" colspan="1">1.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D2-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td><td align="center" valign="middle" rowspan="1" colspan="1">91.42</td><td align="center" valign="middle" rowspan="1" colspan="1">2.17</td><td align="center" valign="middle" rowspan="1" colspan="1">1.47</td><td align="center" valign="middle" rowspan="1" colspan="1">177</td><td align="center" valign="middle" rowspan="1" colspan="1">93.36</td><td align="center" valign="middle" rowspan="1" colspan="1">2.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.37</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SuperPoint</td><td align="center" valign="middle" rowspan="1" colspan="1">173</td><td align="center" valign="middle" rowspan="1" colspan="1">92.46</td><td align="center" valign="middle" rowspan="1" colspan="1">1.88</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" rowspan="1" colspan="1">156</td><td align="center" valign="middle" rowspan="1" colspan="1">90.14</td><td align="center" valign="middle" rowspan="1" colspan="1">2.25</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LoFTR</td><td align="center" valign="middle" rowspan="1" colspan="1">256</td><td align="center" valign="middle" rowspan="1" colspan="1">97.82</td><td align="center" valign="middle" rowspan="1" colspan="1">1.58</td><td align="center" valign="middle" rowspan="1" colspan="1">2.32</td><td align="center" valign="middle" rowspan="1" colspan="1">212</td><td align="center" valign="middle" rowspan="1" colspan="1">96.24</td><td align="center" valign="middle" rowspan="1" colspan="1">1.82</td><td align="center" valign="middle" rowspan="1" colspan="1">2.15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">263</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">246</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.30</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">(b)</td><td align="center" valign="middle" rowspan="1" colspan="1">SIFT</td><td align="center" valign="middle" rowspan="1" colspan="1">46</td><td align="center" valign="middle" rowspan="1" colspan="1">48.73</td><td align="center" valign="middle" rowspan="1" colspan="1">2.62</td><td align="center" valign="middle" rowspan="1" colspan="1">2.44</td><td align="center" valign="middle" rowspan="1" colspan="1">38</td><td align="center" valign="middle" rowspan="1" colspan="1">41.93</td><td align="center" valign="middle" rowspan="1" colspan="1">2.77</td><td align="center" valign="middle" rowspan="1" colspan="1">2.47</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SOSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">81</td><td align="center" valign="middle" rowspan="1" colspan="1">55.16</td><td align="center" valign="middle" rowspan="1" colspan="1">2.58</td><td align="center" valign="middle" rowspan="1" colspan="1">1.02</td><td align="center" valign="middle" rowspan="1" colspan="1">66</td><td align="center" valign="middle" rowspan="1" colspan="1">45.00</td><td align="center" valign="middle" rowspan="1" colspan="1">2.61</td><td align="center" valign="middle" rowspan="1" colspan="1">1.18</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D2-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">117</td><td align="center" valign="middle" rowspan="1" colspan="1">88.56</td><td align="center" valign="middle" rowspan="1" colspan="1">2.36</td><td align="center" valign="middle" rowspan="1" colspan="1">1.38</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td><td align="center" valign="middle" rowspan="1" colspan="1">83.27</td><td align="center" valign="middle" rowspan="1" colspan="1">2.44</td><td align="center" valign="middle" rowspan="1" colspan="1">1.42</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SuperPoint</td><td align="center" valign="middle" rowspan="1" colspan="1">146</td><td align="center" valign="middle" rowspan="1" colspan="1">92.50</td><td align="center" valign="middle" rowspan="1" colspan="1">2.20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82</td><td align="center" valign="middle" rowspan="1" colspan="1">122</td><td align="center" valign="middle" rowspan="1" colspan="1">87.57</td><td align="center" valign="middle" rowspan="1" colspan="1">2.32</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LoFTR</td><td align="center" valign="middle" rowspan="1" colspan="1">187</td><td align="center" valign="middle" rowspan="1" colspan="1">93.38</td><td align="center" valign="middle" rowspan="1" colspan="1">1.98</td><td align="center" valign="middle" rowspan="1" colspan="1">2.17</td><td align="center" valign="middle" rowspan="1" colspan="1">165</td><td align="center" valign="middle" rowspan="1" colspan="1">90.29</td><td align="center" valign="middle" rowspan="1" colspan="1">2.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.24</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">232</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">218</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.35</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">(c)</td><td align="center" valign="middle" rowspan="1" colspan="1">SIFT</td><td align="center" valign="middle" rowspan="1" colspan="1">52</td><td align="center" valign="middle" rowspan="1" colspan="1">53.60</td><td align="center" valign="middle" rowspan="1" colspan="1">2.50</td><td align="center" valign="middle" rowspan="1" colspan="1">2.23</td><td align="center" valign="middle" rowspan="1" colspan="1">43</td><td align="center" valign="middle" rowspan="1" colspan="1">47.81</td><td align="center" valign="middle" rowspan="1" colspan="1">2.95</td><td align="center" valign="middle" rowspan="1" colspan="1">2.55</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SOSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">64.71</td><td align="center" valign="middle" rowspan="1" colspan="1">2.46</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">55</td><td align="center" valign="middle" rowspan="1" colspan="1">58.25</td><td align="center" valign="middle" rowspan="1" colspan="1">2.87</td><td align="center" valign="middle" rowspan="1" colspan="1">1.16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D2-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">133</td><td align="center" valign="middle" rowspan="1" colspan="1">91.54</td><td align="center" valign="middle" rowspan="1" colspan="1">2.28</td><td align="center" valign="middle" rowspan="1" colspan="1">1.31</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">88.05</td><td align="center" valign="middle" rowspan="1" colspan="1">2.39</td><td align="center" valign="middle" rowspan="1" colspan="1">1.33</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SuperPoint</td><td align="center" valign="middle" rowspan="1" colspan="1">110</td><td align="center" valign="middle" rowspan="1" colspan="1">86.21</td><td align="center" valign="middle" rowspan="1" colspan="1">2.41</td><td align="center" valign="middle" rowspan="1" colspan="1">0.85</td><td align="center" valign="middle" rowspan="1" colspan="1">102</td><td align="center" valign="middle" rowspan="1" colspan="1">82.34</td><td align="center" valign="middle" rowspan="1" colspan="1">2.65</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LoFTR</td><td align="center" valign="middle" rowspan="1" colspan="1">202</td><td align="center" valign="middle" rowspan="1" colspan="1">94.67</td><td align="center" valign="middle" rowspan="1" colspan="1">2.17</td><td align="center" valign="middle" rowspan="1" colspan="1">2.08</td><td align="center" valign="middle" rowspan="1" colspan="1">198</td><td align="center" valign="middle" rowspan="1" colspan="1">92.15</td><td align="center" valign="middle" rowspan="1" colspan="1">2.23</td><td align="center" valign="middle" rowspan="1" colspan="1">2.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">218</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">203</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.28</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05581-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05581-t002_Table 2</object-id><label>Table 2</label><caption><p>Ablation studies utilizing the test dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Multi-Perspective Layer</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Descriptor Enhancement Module</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NCM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SR</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RT</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">202</td><td align="center" valign="middle" rowspan="1" colspan="1">91.56</td><td align="center" valign="middle" rowspan="1" colspan="1">2.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">243</td><td align="center" valign="middle" rowspan="1" colspan="1">94.83</td><td align="center" valign="middle" rowspan="1" colspan="1">1.97</td><td align="center" valign="middle" rowspan="1" colspan="1">1.14</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">217</td><td align="center" valign="middle" rowspan="1" colspan="1">96.71</td><td align="center" valign="middle" rowspan="1" colspan="1">1.72</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.27</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>