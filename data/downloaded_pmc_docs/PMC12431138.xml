<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431138</article-id><article-id pub-id-type="pmcid-ver">PMC12431138.1</article-id><article-id pub-id-type="pmcaid">12431138</article-id><article-id pub-id-type="pmcaiid">12431138</article-id><article-id pub-id-type="doi">10.3390/s25175276</article-id><article-id pub-id-type="publisher-id">sensors-25-05276</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Development of Machine-Learning-Based Facial Thermal Image Analysis for Dynamic Emotion Sensing</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Tang</surname><given-names initials="B">Budu</given-names></name><xref rid="af1-sensors-25-05276" ref-type="aff">1</xref><xref rid="af2-sensors-25-05276" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5335-1272</contrib-id><name name-style="western"><surname>Sato</surname><given-names initials="W">Wataru</given-names></name><xref rid="af1-sensors-25-05276" ref-type="aff">1</xref><xref rid="af2-sensors-25-05276" ref-type="aff">2</xref><xref rid="c1-sensors-25-05276" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3799-4550</contrib-id><name name-style="western"><surname>Kawanishi</surname><given-names initials="Y">Yasutomo</given-names></name><xref rid="af3-sensors-25-05276" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Doulamis</surname><given-names initials="A">Anastasios</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Posada-Quintero</surname><given-names initials="HF">Hugo F.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Rojas</surname><given-names initials="RF">Raul Fernandez</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Veeranki</surname><given-names initials="YR">Yedukondala Rao</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Kong</surname><given-names initials="Y">Youngsun</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05276"><label>1</label>Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo, Kyoto 606-8507, Japan; <email>tang.budu.53s@st.kyoto-u.ac.jp</email></aff><aff id="af2-sensors-25-05276"><label>2</label>Psychological Process Research Team, Guardian Robot Project, RIKEN, 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan</aff><aff id="af3-sensors-25-05276"><label>3</label>Multimodal Data Recognition Research Team, Guardian Robot Project, RIKEN, 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan; <email>yasutomo.kawanishi@riken.jp</email></aff><author-notes><corresp id="c1-sensors-25-05276"><label>*</label>Correspondence: <email>wataru.sato.ya@riken.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5276</elocation-id><history><date date-type="received"><day>07</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>19</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>23</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>25</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05276.pdf"/><abstract><p>Information on the relationship between facial thermal responses and emotional state is valuable for sensing emotion. Yet, previous research has typically relied on linear methods of analysis based on regions of interest (ROIs), which may overlook nonlinear pixel-wise information across the face. To address this limitation, we investigated the use of machine learning (ML) for pixel-level analysis of facial thermal images to estimate dynamic emotional arousal ratings. We collected facial thermal data from 20 participants who viewed five emotion-eliciting films and assessed their dynamic emotional self-reports. Our ML models, including random forest regression, support vector regression, ResNet-18, and ResNet-34, consistently demonstrated superior estimation performance compared to traditional simple or multiple linear regression models for the ROIs. To interpret the nonlinear relationships between facial temperature changes and arousal, saliency maps and integrated gradients were used for the ResNet-34 model. The results show nonlinear associations of arousal ratings in nose = tip, forehead, and cheek temperature changes. These findings imply that ML-based analysis of facial thermal images can estimate emotional arousal more effectively, pointing to potential applications of non-invasive emotion sensing for mental health, education, and human&#8211;computer interaction.</p></abstract><kwd-group><kwd>emotional arousal</kwd><kwd>deep learning</kwd><kwd>facial thermal imaging</kwd><kwd>machine learning</kwd><kwd>pixel-level analysis</kwd></kwd-group><funding-group><award-group><funding-source>JST SPRING</funding-source><award-id>JPMJSP2110</award-id></award-group><funding-statement>Our study was supported by funds from JST SPRING (JPMJSP2110).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05276"><title>1. Introduction</title><p>Thermal imaging technology has emerged as a powerful tool for analyzing physiological and emotional states, offering significant advantages as a non-invasive, contact-free method that is particularly valued in medicine and human&#8211;computer interactions [<xref rid="B1-sensors-25-05276" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05276" ref-type="bibr">2</xref>]. Unlike traditional visible-light-based imaging that can be hindered by inadequate or excessive lighting, thermal imaging performs consistently regardless of the illumination conditions, capturing infrared radiation from the body to analyze physiological changes [<xref rid="B3-sensors-25-05276" ref-type="bibr">3</xref>]. Facial emotion recognition using visible spectrum data faces challenges due to lighting variations and captures only superficial features; thermal imaging bypasses these limitations by detecting temperature variations that are indicative of underlying physiological responses to emotional stimuli [<xref rid="B4-sensors-25-05276" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05276" ref-type="bibr">5</xref>]. Temperature changes in facial skin are related to autonomic nervous system activity, particularly in the sympathetic branch that is activated during emotional arousal and causes vasoconstriction and reduced blood flow to the skin surface, leading to measurable decreases in temperature [<xref rid="B6-sensors-25-05276" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05276" ref-type="bibr">7</xref>]. These temperature changes across different facial regions are valuable markers of emotional states [<xref rid="B8-sensors-25-05276" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05276" ref-type="bibr">9</xref>], making thermal imaging particularly effective for applications such as mental health monitoring, education, and advanced human&#8211;computer interaction [<xref rid="B2-sensors-25-05276" ref-type="bibr">2</xref>].</p><p>Extensive research has explored the association between facial thermal signals and subjective emotional states, using linear analyses (e.g., correlation and linear regression) based on regions of interest (ROIs) [<xref rid="B10-sensors-25-05276" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05276" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05276" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05276" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05276" ref-type="bibr">15</xref>]. ROI analysis typically focuses on specific areas of the face, such as the nose tip, to determine how temperature fluctuations correspond to emotional arousal and valence. Many studies have found a consistent pattern in which the temperature of the nose tip inversely correlates with arousal levels [<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05276" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05276" ref-type="bibr">13</xref>]. In these studies, emotional states were assessed using the two-dimensional affective model proposed by Russell [<xref rid="B16-sensors-25-05276" ref-type="bibr">16</xref>], in which arousal refers to the level of emotional intensity and valence to the degree of pleasantness. For example, Sato et al. found that, when participants viewed emotionally charged films, nose tip temperature dropped as subjective arousal increased, implying a physiological response to the emotional stimuli [<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>]. Other regions of the face that have also been studied include the forehead, cheeks, and periorbital area, although the findings have sometimes been mixed or inconclusive. Some studies have linked a reduction in forehead temperature to heightened emotional arousal [<xref rid="B14-sensors-25-05276" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05276" ref-type="bibr">15</xref>], whereas another found divergent temperature changes in the cheeks and periorbital area in response to startling sounds, reflecting complex emotional processing [<xref rid="B15-sensors-25-05276" ref-type="bibr">15</xref>]. Overall, these studies have shown that temperature shifts in different facial regions, such as the nose tip, provide valuable insights into emotional experiences.</p><p>Although previous analyses have provided valuable insights into the relationship between facial thermal signals and emotional states, they have had inherent limitations, with two major issues restricting their ability to capture detailed variation across the entire face. First, many studies have used linear analysis methods, which may not be sufficient to capture the complex, nonlinear relationships inherent in emotional processes. Linear approaches can fail to detect the subjective&#8211;physiological associations adequately, potentially failing to model the complicated neurovascular activity underlying facial temperature changes. Second, ROI-based methods rely on analyzing pre-defined facial areas, such as the nose tip or forehead, which may lead to subtle yet important thermal changes in other parts of the face being overlooked. These methods also depend heavily on manual feature extraction, which can introduce subjectivity and variability into analysis.</p><p>To address these limitations, we used pixel-level machine learning (ML) modeling, enabling examination of nonlinear associations between thermal changes across the entire face, and dynamic emotional arousal. ML techniques, particularly those capable of pixel-level analysis, offer a promising approach for assessing complex thermal features across the entire face automatically, including nonlinear relationships that traditional linear methods might miss. No study has used ML-based pixel-level dynamic emotion analysis of facial thermal images. Studies applying ML to thermal recognition of facial emotions remain relatively scarce, with most having focused on classifying discrete emotions such as happiness or fear [<xref rid="B17-sensors-25-05276" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05276" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05276" ref-type="bibr">19</xref>]. While ML models have shown benefits in related tasks, such as thermal-to-visible image translation [<xref rid="B20-sensors-25-05276" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05276" ref-type="bibr">21</xref>], their application to the continuous tracking of dynamic emotional states using pixel-level analysis of whole-face thermal images remains largely unexplored. Because there are several different ML models, we explored two conventional models&#8212;random forest [<xref rid="B22-sensors-25-05276" ref-type="bibr">22</xref>] and support vector regression (SVR) [<xref rid="B23-sensors-25-05276" ref-type="bibr">23</xref>]&#8212;and two deep-learning models, ResNet-18 and ResNet-34 [<xref rid="B24-sensors-25-05276" ref-type="bibr">24</xref>].</p><p>In this study, we used ML techniques ranging from simple linear models to advanced deep-learning architectures to establish a robust framework for thermal facial emotion estimation. We recorded thermal data of participants&#8217; faces and obtained dynamic valence and arousal ratings while they observed five emotional film clips. To assess the feasibility of predicting emotional arousal from thermal facial data, as a baseline, we began with linear regression analysis of the nose tip ROI. To perform pixel-level analysis across the entire face and address the limitations of ROI methods, we explored deep-learning models capable of capturing detailed thermal patterns (<xref rid="sensors-25-05276-f001" ref-type="fig">Figure 1</xref>) after image preprocessing (<xref rid="sensors-25-05276-f002" ref-type="fig">Figure 2</xref>). We applied leave-one-person-out cross validation (LOPOCV) to evaluate predictive performance rigorously and address concerns related to overfitting. To overcome the black-box nature of the ML models and interpret our results, we first used saliency maps to identify which facial regions had the most significant impact on the model predictions by highlighting the areas where temperature changes contributed most to emotional state estimation. Subsequently, we applied the integrated gradients method to these significant regions to analyze patterns of temperature changes in relation to emotional states. Based on ample evidence showing an association between facial thermal changes and subjective emotional arousal [<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05276" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05276" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05276" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05276" ref-type="bibr">15</xref>], together with the rationale that facial thermal changes reflect sympathetic nervous system activity, which is theoretically coupled with arousal [<xref rid="B6-sensors-25-05276" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05276" ref-type="bibr">7</xref>], we analyzed the arousal ratings. This comprehensive approach helped identify influential facial regions and enhanced our understanding of how ML methods can be leveraged for pixel-level thermal emotion analysis.</p></sec><sec id="sec2-sensors-25-05276"><title>2. Materials and Methods</title><sec sec-type="subjects" id="sec2dot1-sensors-25-05276"><title>2.1. Participants</title><p>This study recruited 20 healthy Japanese adults (10 women, 10 men; mean &#177; standard deviation [<italic toggle="yes">SD</italic>] age, 22.0 &#177; 2.6 years). Although an 11th woman participated, her data were not analyzed due to substantial motion artifacts. The sample size was determined based on previous research examining the relationship between facial thermal responses and emotional states [<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>]. All participants had normal or corrected-to-normal vision, without the use of glasses, and were native speakers of Japanese. The inclusion criteria included willingness to participate in subjective and physiological measurements, no neurological or psychiatric disorders, and no prior experience with the emotional film clips used in the study. All participants consented after being fully informed about the experimental procedures. The study was approved by the RIKEN Ethics Committee and was conducted in accordance with institutional ethical standards and the Declaration of Helsinki.</p></sec><sec id="sec2dot2-sensors-25-05276"><title>2.2. Apparatus</title><p>The experimental setup consisted of a Windows-based HP Z200 SFF computer (Hewlett-Packard Japan, Tokyo, Japan) running Presentation software ver. 14.9 (Neurobehavioral Systems, Berkeley, CA, USA) to control stimulus delivery. Visual stimuli were presented on a 19-inch monitor (model HM903D-A; Iiyama, Tokyo, Japan) with 1024 &#215; 768-pixel resolution. Participants used a wired optical mouse (model MS116; Dell, Round Rock, TX, USA) connected to an additional Windows laptop (model CF-SV8; Panasonic, Tokyo, Japan) to collect dynamic ratings. Thermal imaging was conducted using an A655sc infrared thermal camera (FLIR Systems, Wilsonville, OR, USA) and the Research IR Max software (ver. 4.40). Positioned adjacent to the monitor, the camera captured full-face thermal images at a spatial resolution of 640 &#215; 480 pixels with a frame rate of 50 Hz. Although additional thermal data of the profiles and full-face RGB data were acquired as part of the experiment, these data are not reported in this paper.</p></sec><sec id="sec2dot3-sensors-25-05276"><title>2.3. Stimuli</title><p>A set of five film clips was selected to elicit a range of emotional responses: &#8220;Cry Freedom&#8221; (very negative, anger), &#8220;The Champ&#8221; (moderately negative, sadness), &#8220;Abstract Shapes&#8221; (neutral), &#8220;Wild Birds of Japan&#8221; (moderately positive, contentment), and &#8220;M-1 Grand Prix The Best 2007&#8211;2009&#8221; (very positive, amusement). Several previous studies used and validated these films as a means of eliciting emotion [<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>,<xref rid="B25-sensors-25-05276" ref-type="bibr">25</xref>]. Specifically, anger, sadness, neutral, contentment, and amusement films reportedly showed linear and quadratic relationships with subjective valence and arousal ratings, respectively [<xref rid="B25-sensors-25-05276" ref-type="bibr">25</xref>]. The mean &#177; <italic toggle="yes">SD</italic> duration of these films was 175.8 &#177; 22.2 s, with individual durations of 157 s for anger, 172 s for sadness, 206 s for neutral, 148 s for contentment, and 196 s for amusement. Two additional films, depicting scenes from &#8220;The Silence of the Lambs&#8221; and &#8220;Colour Bars&#8221; from Gross and Levenson, were used for practice trials. The stimuli were presented at a 640 &#215; 480-pixel resolution, corresponding to visual angles of approximately 25.5&#176; horizontally and 11.0&#176; vertically.</p></sec><sec id="sec2dot4-sensors-25-05276"><title>2.4. Procedure</title><p>The experiments were conducted in a soundproof, electrically shielded chamber. The ambient temperature was maintained at between 23.5 &#176;C and 24.5 &#176;C and was monitored using a TR-76Ui data logger (T&amp;D Corp., Matsumoto, Japan). Participants were informed that the purpose of the study was to obtain subjective emotional ratings and record physiological responses while they viewed a series of films. The subjects were provided with approximately 10 min to acclimate to the room conditions before the experiment commenced.</p><p>Participants were seated comfortably on a chair positioned approximately 0.77 m from the monitor. The thermal imaging camera was placed adjacent to the monitor to capture full-face thermal images continuously throughout the experiment. Following two practice films to familiarize participants with the procedure, the five test films were presented in a pseudorandom order.</p><p>Each trial began with a 1-second fixation point displayed at the center of the screen, followed by a 10-second white screen that served as a pre-stimulus baseline. The film clip was then presented. The onset of thermal image acquisition was synchronized with the film presentation, and data were acquired during the pre- and post-stimulus periods. After the film concluded, another white screen was displayed for 10 s as a post-trial baseline. Participants were then asked to rate the overall subjective experience of emotional valence and arousal using an affect grid [<xref rid="B16-sensors-25-05276" ref-type="bibr">16</xref>], which provided a nine-point scale for each dimension. The ratings were entered by pressing the number keys 1&#8211;9 on the keyboard, corresponding to the participant&#8217;s perceived level of valence and arousal. Each participant was instructed to fixate on the central point, watch each film attentively, and subsequently rate the subjective experience by pressing the appropriate number keys. After the rating was provided, the screen turned black during the inter-trial interval, which varied randomly between 24 and 30 s. Thermal imaging data were recorded continuously throughout all trials, along with digital markers indicating the onset of each film.</p><p>Upon completion of all trials, participants engaged in a dynamic rating session. All film stimuli were presented again on the monitor. Participants were instructed to recall their subjective emotional experiences during the initial viewing and continuously rate their experiences in terms of valence and arousal by moving the mouse. The mouse coordinates were sampled at a rate of 10 Hz. The onset of rating data acquisition was synchronized with film presentation. This cued recall procedure was used to obtain continuous ratings of valence and arousal, which were challenging to collect simultaneously during the initial viewing. Previous studies have indicated that cued recall continuous ratings are strongly correlated with real-time continuous ratings for emotional films [<xref rid="B26-sensors-25-05276" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05276" ref-type="bibr">27</xref>].</p></sec><sec id="sec2dot5-sensors-25-05276"><title>2.5. Workflow</title><p>The workflow used for analyzing facial thermal images to estimate emotional responses with the ResNet-34 model is illustrated in <xref rid="sensors-25-05276-f001" ref-type="fig">Figure 1</xref>. The process involved acquiring thermal images, followed by facial registration using UV mapping for consistent alignment. Preprocessing included random transformations and normalization, and the data were analyzed using LOPOCV, which evaluates both linear regression and deep learning models. Saliency maps and integrated gradients were used to interpret the model results, highlighting important facial regions and their contributions to emotional states.</p></sec><sec id="sec2dot6-sensors-25-05276"><title>2.6. Preprocessing</title><p>The first thermal frame was imported into the Blender 3.6 software; its mesh was then manually rotated and translated until it matched a standardized three-dimensional (3D) facial template. Blender then generated a UV map that defined a fixed correspondence between image pixels and template vertices (<xref rid="sensors-25-05276-f002" ref-type="fig">Figure 2</xref>). This reference UV map was exported and applied to every subsequent frame. To propagate the alignment automatically, we used our landmark-tracking script implemented in MATLAB R2020a (Mathworks, Natick, MA, USA) as follows: facial landmarks were detected in each frame, and their coordinates were used to estimate a rigid (rotation + translation) transform that brought the frame into the reference coordinate system before re-sampling onto the template surface. This two-step procedure combined manual adjustment with automatic landmark-based alignment, allowing the method to compensate for small head movements and changes in facial orientation, thereby reducing motion-related variability across the entire sequence.</p><p>After registration, the facial region was cropped to remove extraneous areas that did not contribute to the analysis, such as the background or neck. Cropping focuses the analysis on the facial regions that are most relevant for emotional detection, reducing computational requirements and improving model accuracy by eliminating irrelevant information.</p><p>Normalization was then performed to ensure consistent pixel intensity values across the dataset. This step reduced the influence of varying thermal conditions that might arise from differences in ambient temperature or individual physiological variation. By scaling the intensity values to a consistent range, the models can focus on meaningful differences in temperature across facial regions, which are indicative of emotional responses.</p><p>A smoothing filter was also applied to reduce noise in the thermal images. Given that thermal cameras can sometimes produce noisy images, particularly in low-resolution settings, smoothing helps to enhance the signal-to-noise ratio. A Gaussian filter was commonly used for this purpose, providing a balance between reducing noise and preserving important features in facial regions.</p><p>These preprocessing steps collectively ensure that the thermal images are of high quality and consistently aligned, providing a solid foundation for subsequent analysis and modeling.</p></sec><sec id="sec2dot7-sensors-25-05276"><title>2.7. Model Training and Validation</title><p>The analysis of facial thermal images to predict emotional responses involves training and validating several models, ranging from simple linear models to those with more complex deep learning architectures.</p><p>To establish a baseline for predicting emotional arousal, linear regression was used as a straightforward yet effective starting point. Two types of linear models were implemented: one using only the nose tip ROI and another using multiple ROIs, including the nose tip, forehead, and bilateral cheeks.</p><p>Following the baseline, two conventional ML models&#8212;random forest regression and SVR&#8212;were evaluated. For the random forest model, we used the RandomForestRegressor from scikit-learn with 300 decision trees, a maximum depth of 15, and a minimum leaf size of 2. The SVR model was implemented with a radial basis function kernel, and its hyperparameters were optimized using grid search. For these analyses, thermal images were resized to 64 &#215; 64 pixels, converted to grayscale, and flattened into 4096-dimensional vectors before being processed. To reduce noise and improve efficiency, a principal component analysis was conducted to reduce the dimensionality to 200 before training.</p><p>We then explored various deep learning architectures, including lightweight models such as MobileNet, but their performance differences were marginal. Based on its consistently superior accuracy, we ultimately selected a regression model with a ResNet-34 backbone for the main analyses. A smaller ResNet-18 model was also included for comparison, serving as a lightweight convolutional neural network alternative. Both models were implemented using the torchvision library with ImageNet-pretrained weights. Their final fully connected layers were replaced with a single output node to support continuous arousal prediction. The input images were converted to RGB, resized to 224 &#215; 224 pixels, and normalized using the standard ImageNet mean and standard deviation. The models were trained using the Adam optimizer (learning rate = 1 &#215; 10<sup>&#8722;4</sup>; weight decay = 1 &#215; 10<sup>&#8722;4</sup>), a batch size of 32, and the mean squared error (MSE) loss for 10 epochs. The process began with image acquisition, followed by preprocessing steps including facial registration to ensure consistent alignment across frames. Within the ResNet backbone, the thermal images passed through multiple residual blocks to extract discriminative features associated with emotional arousal. Each residual block typically comprises a convolutional layer, a batch normalization (BN) layer, and a rectified linear unit (ReLU) activation function. The convolutional layer uses learnable filters to scan local receptive fields, capturing the spatial patterns that are crucial for downstream predictions. Next, the BN layer standardizes the per-channel outputs within each mini-batch to attain near-zero mean and unit variance, thereby mitigating internal covariate shift and stabilizing gradient flow. The ReLU activation function then zeroes out negative values while preserving positive ones, which introduces nonlinearity and helps to prevent vanishing gradients. Meanwhile, each residual block integrates a skip connection that sums its input directly with its output, improving gradient propagation and enabling deeper architectures. After the final residual block, a pooling layer reduces the spatial dimensions of the resulting feature maps, which are then flattened into a one-dimensional vector. To reduce overfitting further, a dropout layer randomly deactivates a fraction of neurons during each training iteration, thereby promoting more robust feature learning. Finally, an output layer produces a continuous value that reflects the predicted emotional arousal from the facial thermal image.</p><p>To evaluate the models, LOPOCV was applied. In this approach, each participant&#8217;s data were left out once as a test set, while the remaining participants&#8217; data were used for training. This enables a robust evaluation of model generalizability, ensuring that the trained model is not overly reliant on the data of any single participant. Both the linear regression and lightweight deep-learning models were evaluated, with metrics such as the correlation coefficients and MSE used to compare their predictive accuracy.</p></sec><sec id="sec2dot8-sensors-25-05276"><title>2.8. Statistical Analysis</title><p>The mean dynamic ratings during film presentation (five ratings for each participant) were subjected to repeated-measures trend analyses (two-tailed). The linear and quadratic natures of the valence and arousal ratings were assessed across five films (anger, sadness, neutral, contentment, and amusement) to confirm the previous finding [<xref rid="B25-sensors-25-05276" ref-type="bibr">25</xref>].</p><p>To validate the effectiveness of the models, we performed statistical analyses to compare the predictive accuracy of the linear regression and deep learning models. Using the LOPOCV approach, a separate model was trained for each participant, and the correlation between the predicted and actual arousal ratings was computed for each individual. To evaluate the prediction performance, the correlation coefficient produced by each model was analyzed using a one-sample <italic toggle="yes">t</italic>-test contrasting with zero (two-tailed). To determine whether there were significant differences in the correlations obtained by the models, one-way repeated-measure analyses of variance (ANOVAs) with model as a factor, followed by multiple comparisons using the Bonferroni method (two-tailed), were performed. We also evaluated the MSE using these tests.</p><p>The &#945;-level for all analyses was set to 0.05.</p></sec><sec sec-type="discussion" id="sec2dot9-sensors-25-05276"><title>2.9. Model Interpretation</title><p>To gain insights into the key facial regions contributing to emotional state predictions and the nonlinear relationships between facial temperature changes and arousal, we used a two-step interpretability approach for the ML models with saliency maps [<xref rid="B28-sensors-25-05276" ref-type="bibr">28</xref>], followed by integrated gradients [<xref rid="B29-sensors-25-05276" ref-type="bibr">29</xref>]. We analyzed the ResNet-34 model, which showed the highest estimation performance in the statistical tests.</p><p>First, saliency maps were generated to identify the areas of the face that had the greatest impact on the model predictions. By highlighting the pixels contributing most to the output, these maps provided an overview of important facial regions, such as the nose tip, forehead, and cheeks, which were identified as critical indicators of emotional arousal.</p><p>After identifying these key regions using saliency maps, we applied integrated gradients to conduct a more detailed analysis of these specific areas. The integrated gradients allowed a quantitative assessment of the contribution of each pixel by computing the path integral of gradients as the input transitioned from baseline to its actual value. This allowed us to measure the magnitude of the effect that temperature changes in the identified facial regions had on the predicted arousal values, offering a more precise understanding of the nonlinear relationships captured by the deep learning models. To evaluate nonlinear relationships between temperature and arousal ratings, polynomial regression analysis was performed using first-degree (linear), second-degree (quadratic), third-degree (cubic), and fourth-degree (quartic) models. The optimal model was selected based on the adjusted R<sup>2</sup>, root mean squared error (RMSE), Akaike&#8217;s information criterion (AIC), and Bayesian information criterion (BIC).</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05276"><title>3. Results</title><sec id="sec3dot1-sensors-25-05276"><title>3.1. Subjective Ratings</title><p><xref rid="sensors-25-05276-f003" ref-type="fig">Figure 3</xref> shows the group mean time courses of the dynamic valence and arousal ratings. The figures indicate that the emotional film clips elicited dynamic changes in subjective valence and arousal. For example, the valence curve for the angry film showed a slight increase followed by a sharp decline, reflecting the film&#8217;s content: a pleasant group gathering scene followed by an anger-inducing massacre. Planned contrasts confirmed that the mean dynamic ratings acquired during film presentation reflected the expected linear and quadratic patterns of the valence and arousal ratings across films, respectively (valence: <italic toggle="yes">t</italic>[76] = 13.45, <italic toggle="yes">p</italic> &lt; 0.001; arousal: <italic toggle="yes">t</italic>[76] = 9.35, <italic toggle="yes">p</italic> &lt; 0.001).</p></sec><sec id="sec3dot2-sensors-25-05276"><title>3.2. Estimation Performance</title><p><xref rid="sensors-25-05276-f004" ref-type="fig">Figure 4</xref> presents a representative arousal trajectory for a held-out participant in the LOPOCV test, comparing the ResNet-34 model&#8217;s predictions with the participant&#8217;s self-reported ratings and illustrating their temporal correspondence.</p><p><xref rid="sensors-25-05276-f005" ref-type="fig">Figure 5</xref> summarizes the mean &#177; standard error of the MSE and the Pearson correlation coefficients between the predicted and actual arousal ratings obtained using six arousal prediction models: two baseline ROI-based linear regression models (simple and multiple regression models), two conventional ML models (random forest regression and SVR), and two deep-learning models (ResNet-18 and ResNet-34). The evaluation was based on LOPOCV, in which the data from one participant served as the test set and the rest were used for training.</p><p>One-sample <italic toggle="yes">t</italic>-tests verified that the correlation coefficients of all models were significantly greater than zero (t[19] &gt; 18.9, <italic toggle="yes">p</italic> &lt; 0.001, <italic toggle="yes">d</italic> &gt; 1.0), confirming that each model captured meaningful arousal information.</p><p>A one-way repeated-measures ANOVA with model as a factor on the correlation coefficients revealed a significant main effect of model (<italic toggle="yes">F</italic>[5, 95] = 61.34, <italic toggle="yes">p</italic> &lt; 0.001, &#951;<sup>2</sup><sub>p</sub> = 0.76). Bonferroni-corrected multiple comparisons showed that the four ML models were significantly more correlated with the ground-truth arousal ratings than the two linear baseline models (<italic toggle="yes">p</italic> &lt; 0.001). No significant difference was observed among the ML models (<italic toggle="yes">p</italic> &gt; 0.05).</p><p>A parallel ANOVA on the MSE likewise yielded a significant main effect of model (<italic toggle="yes">F</italic>[5, 95] = 21.01, <italic toggle="yes">p</italic> &lt; 0.001, &#951;<sup>2</sup><sub>p</sub> = 0.53). Post hoc tests indicated that both deep learning models achieved lower MSEs than the other four models, and both conventional ML models achieved lower MSEs than the two linear regression models (<italic toggle="yes">p</italic> &lt; 0.001).</p></sec><sec sec-type="discussion" id="sec3dot3-sensors-25-05276"><title>3.3. Model Interpretation</title><p>To interpret the nonlinear relationships between facial temperature changes and arousal, we applied model interpretation tools. We analyzed the ResNet-34 model, which had the highest estimation performance in the above analyses.</p><p>We first used saliency maps to visualize the facial regions that had the greatest influence on the ML model predictions. <xref rid="sensors-25-05276-f006" ref-type="fig">Figure 6</xref> shows the saliency map alongside a corresponding original thermal image of a representative participant. The saliency map highlights the key facial regions that had the most significant impact on predicting emotional arousal, such as the nose tip, forehead, and both cheeks.</p><p>To interpret the model predictions further, integrated gradients were used to analyze the contributions of representative facial regions, including the nose tip, forehead, and both cheeks. Integrated gradients are an attribution method used to understand the influence of each input feature on model predictions by calculating the path integral of the gradients from a baseline input to the actual input. This enables a more nuanced understanding of how individual pixel changes affect the output of the model, thus providing insights into which areas of the input contribute most significantly to the decision of the model. To quantify the visually observed patterns and rule out the possibility that apparent nonlinearities were merely a consequence of an over-parameterized model, we fitted first- to fourth-degree polynomials to the temperature&#8211;attribution pairs of each ROI and compared them with adjusted R<sup>2</sup>, RMSE, AIC, and BIC values (<xref rid="sensors-25-05276-t001" ref-type="table">Table 1</xref>). For the nose tip, the adjusted R<sup>2</sup> increased from 0.64 for the linear fit to 0.71 for the cubic fit, while the quartic term produced only a negligible gain; because AIC and BIC bottomed out at the cubic&#8211;quartic transition and the cubic model is more parsimonious, the cubic degree was selected as optimal for this ROI. The forehead showed a modest linear fit but improved sharply only when a quartic term was added, raising the adjusted R<sup>2</sup> to 0.11 and lowering AIC by about 270 points relative to the cubic fit; therefore, a quartic model was selected as optimal for the forehead. Both cheeks displayed much weaker effects. The left cheek rose monotonically to an adjusted R<sup>2</sup> of 0.07 at the quartic degree, with the AIC and BIC values continuing to decline; despite the small effect size, the quartic fit was selected for completeness. The right cheek reached an adjusted R<sup>2</sup> of 0.10 under the cubic model and showed virtually no further improvement at the quartic degree; in this case, the cubic fit offered the best balance between fit and simplicity, and was chosen as the best model. <xref rid="sensors-25-05276-f007" ref-type="fig">Figure 7</xref> illustrates these final choices by overlaying the selected polynomial curve on the frame-level scatter for each ROI.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05276"><title>4. Discussion</title><p>Our model comparisons using correlation coefficients and MSE (<xref rid="sensors-25-05276-f004" ref-type="fig">Figure 4</xref>) demonstrated that the ML model significantly outperformed the ROI-based linear regression model in predicting emotional arousal from facial thermal data. Although the linear regression model for the nose tip ROI showed a significant correlation between the predicted and actual arousal values, consistent with previous findings [<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05276" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05276" ref-type="bibr">13</xref>], the ML model had superior predictive performance for both measures. These results imply that the relationship between facial thermal signals and emotional arousal is not purely linear, instead involving complex, nonlinear dynamics that are better captured by ML models. Regarding the prediction performance of the different ML models, correlation coefficients between the predicted and actual arousal ratings were comparable between the conventional (i.e., random forest and SVR) and deep-learning (i.e., ResNet-18 and ResNet-34) models, although the MSE values of the latter models were better. These results are consistent with the findings of some previous studies that reported a comparable estimation performance between conventional and deep-learning models [<xref rid="B30-sensors-25-05276" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05276" ref-type="bibr">31</xref>], although other studies reported better estimation performance by deep-learning models than by conventional models [<xref rid="B32-sensors-25-05276" ref-type="bibr">32</xref>]. We postulate that one factor contributing to this inconsistency may be the limited sample size and relatively uncomplicated model structure employed in this study [<xref rid="B30-sensors-25-05276" ref-type="bibr">30</xref>]. Taken together, our results demonstrate that an ML model can predict emotional arousal from thermal facial imaging accurately, underscoring its potential for emotion detection tasks.</p><p>To interpret our ML model, we used saliency maps [<xref rid="B28-sensors-25-05276" ref-type="bibr">28</xref>] to visualize the facial regions that influenced the model predictions the most. Saliency maps help identify which areas of the input data have the greatest impact on the model output, thereby providing insights into the underlying decision-making process of an ML model. Our results show that the forehead, nose tip, and both cheeks had the highest brightness on the saliency maps, indicating that these regions contributed most significantly to the prediction of emotional arousal.</p><p>To explore the importance of these high-contribution regions further, we used integrated gradients [<xref rid="B29-sensors-25-05276" ref-type="bibr">29</xref>], an interpretability technique that quantifies the contribution of each input feature to the model output. The average attribution value curve for the nose tip region had negative and approximately linear associations with the subjective arousal ratings, which aligns well with previous linear analyses&#8217; results [<xref rid="B8-sensors-25-05276" ref-type="bibr">8</xref>,<xref rid="B11-sensors-25-05276" ref-type="bibr">11</xref>]. The temperature decrease at the nose tip and in other regions during heightened emotional arousal can be attributed to increased sympathetic nervous system activity, which induces vasoconstriction and reduces blood flow [<xref rid="B7-sensors-25-05276" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05276" ref-type="bibr">8</xref>]. The absence of underlying muscles in the nose tip, unlike other facial regions, may minimize nonlinear thermal interference from muscle contractions [<xref rid="B7-sensors-25-05276" ref-type="bibr">7</xref>,<xref rid="B33-sensors-25-05276" ref-type="bibr">33</xref>]. The average attribution curve for the forehead showed distinct peaks and valleys across different temperature ranges, indicating significant fluctuations in its contribution to the model. Our analysis revealed a complex, nonlinear relationship between forehead temperature and emotional arousal, which has not been captured by previous studies, which primarily reported a straightforward negative correlation [<xref rid="B8-sensors-25-05276" ref-type="bibr">8</xref>,<xref rid="B34-sensors-25-05276" ref-type="bibr">34</xref>]. This implies that the forehead exhibits dynamic changes beyond a simple linear trend, further emphasizing its role in reflecting emotional states. Similarly, both cheeks exhibited nonlinear relationships between temperature and integrated gradients attribution. Although the left- and right-cheek trajectories shared comparable shapes, the right cheek showed somewhat larger attribution magnitudes (<xref rid="sensors-25-05276-f007" ref-type="fig">Figure 7</xref>). This pattern implies a broadly bilateral consistency in autonomic thermal responses during emotional arousal, with minor side-specific amplitude differences.</p><p>Our findings demonstrate the potential real-world applications of ML-based facial thermal analysis, particularly mental health monitoring and intelligent interventions. By providing a non-invasive, automated, continuous approach to assess emotional arousal, our research enables more effective and precise tracking of emotional states. Traditional methods for assessing emotion-related mental health, such as questionnaires and structural interviews, often present challenges due to their subjective nature, difficulty in acquiring quantitative measures, and continuous recording that may fail to capture the dynamic nonlinear relationships inherent in emotional and physiological data. By contrast, ML models applied to facial thermal data can continuously track emotional changes by analyzing subtle, pixel-level variation across the entire face, providing more precise and objective insights into an individual&#8217;s mental state. This high granularity in emotional detection is crucial for identifying early signs of mental health conditions, such as anxiety or depression, which may manifest via nuanced nonlinear changes in emotional arousal. Unlike conventional linear ROI-based methods that focus on specific facial areas, like the nose or forehead, ML models can learn dynamically from an entire thermal imaging dataset, allowing them to capture intricate interactions between different facial regions and their thermal signatures. This capability is particularly valuable for tailoring personalized interventions, as the system can detect and respond to minor emotional shifts, potentially alerting users or healthcare professionals before emotional dysregulation becomes pronounced.</p><p>However, several limitations of this study must be acknowledged. First, the sample size of the present study was modest (<italic toggle="yes">n</italic> = 20) and culturally homogeneous, comprising young Japanese adults recruited from a single site. This limitation may restrict the generalizability of the findings. Future work should employ stratified, multisite recruitment to assemble larger and demographically diverse cohorts across age, gender, and ethnicity, thereby enabling more precise estimates of interindividual variability and stronger external validity. Second, only five film clips were used as stimuli; so, the generalizability of the present findings remains unproven. Future research using additional film stimuli is warranted to develop a robust understanding of the association between facial thermal changes and subjective arousal. Third, our analysis relied exclusively on facial temperature and lacked multimodal validation. Future studies should collect additional physiological signals (e.g., electrodermal activity, electrocardiography, and electroencephalography) to provide independent benchmarks of autonomic nervous system activity and arousal beyond self-reports. Fourth, although ambient temperature was held at 23.5&#8211;24.5 &#176;C, residual variation in room climate, baseline skin temperature, respiration patterns, and subtle facial muscle activity were not explicitly modelled and could have introduced noise. Incorporating these covariates or adopting adaptive compensation schemes in future protocols would further enhance predictive accuracy. Finally, while ResNet-34 offered the best performance among the tested models, exploring alternative architectures&#8212;ranging from detection-style networks like Transformer [<xref rid="B35-sensors-25-05276" ref-type="bibr">35</xref>] to lightweight attention-based frameworks&#8212;and re-engineering them with regression heads tailored to continuous arousal estimation should help determine whether even greater accuracy and interpretability can be achieved.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05276"><title>5. Conclusions</title><p>Our study demonstrated that an ML approach significantly outperformed linear regression in predicting emotional arousal from facial thermal data, highlighting its ability to model the complex, nonlinear relationships inherent in emotional responses. By leveraging pixel-level analysis across the entire face, our ML model captured subtle thermal variation that correlated with emotional arousal, thereby achieving greater predictive accuracy as evidenced by higher correlation coefficients and lower MSE values. The use of saliency maps, along with integrated gradients, provided insights into the key facial regions in arousal prediction, emphasizing the value of ML for enhancing the interpretability and precision of emotion detection systems. Despite some limitations, such as a relatively small dataset and the focus on temperature as the sole physiological measure, our findings establish an important foundation for future research. ML shows great promise for real-world applications in emotion detection, offering a more reliable, non-invasive means of assessing emotional states; this has potential implications for mental health monitoring and adaptive interventions.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank Junyao Zhang and Masaru Usami for technical support and Professors Shin&#8217;ya Nishida and Michio Nomura for helpful advice.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-05276"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mdpi.com/article/10.3390/s25175276/s1">https://www.mdpi.com/article/10.3390/s25175276/s1</uri>. Supplementary Table S1: Correlation coefficient and mean squared error value between the actual and predicted values for each model of each participant. Supplementary Code S1: Codes used in the present study.</p><supplementary-material id="sensors-25-05276-s001" position="float" content-type="local-data" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05276-s001.zip" position="float" orientation="portrait"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceived and designed the experiments: B.T., W.S. and Y.K. Performed the experiments: B.T. and W.S. Analyzed the data: B.T. Wrote the paper: B.T., W.S. and Y.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of RIKEN (Wako3 2020-21; 22 July 2020).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all participants involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data statistically analyzed during this study are included in this published article and its <xref rid="app1-sensors-25-05276" ref-type="app">Supplementary Information</xref> Files. The facial thermal images are not publicly available due to privacy or ethical restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05276"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lahiri</surname><given-names>B.B.</given-names></name><name name-style="western"><surname>Bagavathiappan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jayakumar</surname><given-names>T.</given-names></name><name name-style="western"><surname>Philip</surname><given-names>J.</given-names></name></person-group><article-title>Medical applications of infrared thermography: A review</article-title><source>Infrared Phys. Technol.</source><year>2012</year><volume>55</volume><fpage>221</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.infrared.2012.03.007</pub-id><pub-id pub-id-type="pmid">32288544</pub-id><pub-id pub-id-type="pmcid">PMC7110787</pub-id></element-citation></ref><ref id="B2-sensors-25-05276"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hossain</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Assiri</surname><given-names>B.</given-names></name></person-group><article-title>Facial emotion verification by infrared image</article-title><source>Proceedings of the 2020 International Conference on Emerging Smart Computing and Informatics (ESCI)</source><conf-loc>Pune, India</conf-loc><conf-date>12&#8211;14 March 2020</conf-date><fpage>330</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1109/ESCI48226.2020.9167616</pub-id></element-citation></ref><ref id="B3-sensors-25-05276"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kotani</surname><given-names>K.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name><name name-style="western"><surname>Le</surname><given-names>B.</given-names></name></person-group><article-title>A thermal facial emotion database and its analysis</article-title><source>Proceedings of the Image Video Technology: 6th Pacific-Rim Symposium PSIVT 2013</source><conf-loc>Guanajuato, Mexico</conf-loc><conf-date>28 October&#8211;1 November 2013</conf-date><volume>Volume 6</volume><fpage>397</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-53842-1_34</pub-id></element-citation></ref><ref id="B4-sensors-25-05276"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kosonogov</surname><given-names>V.</given-names></name><name name-style="western"><surname>De Zorzi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Honor&#233;</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mart&#237;nez-Vel&#225;zquez</surname><given-names>E.S.</given-names></name><name name-style="western"><surname>Nandrino</surname><given-names>J.-L.</given-names></name><name name-style="western"><surname>Martinez-Selva</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Sequeira</surname><given-names>H.</given-names></name><name name-style="western"><surname>Urgesi</surname><given-names>C.</given-names></name></person-group><article-title>Facial thermal variations: A new marker of emotional arousal</article-title><source>PLoS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0183592</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0183592</pub-id><pub-id pub-id-type="pmid">28922392</pub-id><pub-id pub-id-type="pmcid">PMC5603162</pub-id></element-citation></ref><ref id="B5-sensors-25-05276"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ordun</surname><given-names>C.</given-names></name><name name-style="western"><surname>Raff</surname><given-names>E.</given-names></name><name name-style="western"><surname>Purushotham</surname><given-names>S.</given-names></name></person-group><article-title>The use of AI for thermal emotion recognition: A review of problems and limitations in standard design and data</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2009.10589</pub-id></element-citation></ref><ref id="B6-sensors-25-05276"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.A.</given-names></name><name name-style="western"><surname>Baird</surname><given-names>T.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Coutinho</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Brien</surname><given-names>D.C.</given-names></name><name name-style="western"><surname>Munoz</surname><given-names>D.P.</given-names></name></person-group><article-title>Arousal effects on pupil size, heart rate, and skin conductance in an emotional face task</article-title><source>Front. Neurol.</source><year>2018</year><volume>9</volume><elocation-id>1029</elocation-id><pub-id pub-id-type="doi">10.3389/fneur.2018.01029</pub-id><pub-id pub-id-type="pmid">30559707</pub-id><pub-id pub-id-type="pmcid">PMC6287044</pub-id></element-citation></ref><ref id="B7-sensors-25-05276"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alba</surname><given-names>B.K.</given-names></name><name name-style="western"><surname>Castellani</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Charkoudian</surname><given-names>N.</given-names></name></person-group><article-title>Cold-induced cutaneous vasoconstriction in humans: Function, dysfunction and the distinctly counterproductive</article-title><source>Exp. Physiol.</source><year>2019</year><volume>104</volume><fpage>1202</fpage><lpage>1214</lpage><pub-id pub-id-type="doi">10.1113/EP087718</pub-id><pub-id pub-id-type="pmid">31045297</pub-id></element-citation></ref><ref id="B8-sensors-25-05276"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Ward</surname><given-names>R.D.</given-names></name><name name-style="western"><surname>Ingleby</surname><given-names>M.</given-names></name></person-group><article-title>Classifying pretended and evoked facial expression of positive and negative affective states using infrared measurement of skin temperature</article-title><source>Trans. Appl. Percept.</source><year>2009</year><volume>6</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1145/1462055.1462061</pub-id></element-citation></ref><ref id="B9-sensors-25-05276"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nakanishi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Matsumura</surname><given-names>K.I.</given-names></name></person-group><article-title>Facial skin temperature decreases in infants with joyful expression</article-title><source>Infant Behav. Dev.</source><year>2008</year><volume>31</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.infbeh.2007.09.001</pub-id><pub-id pub-id-type="pmid">17983661</pub-id></element-citation></ref><ref id="B10-sensors-25-05276"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salazar-L&#243;pez</surname><given-names>E.</given-names></name><name name-style="western"><surname>Dom&#237;nguez</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ju&#225;rez Ramos</surname><given-names>V.</given-names></name><name name-style="western"><surname>de la Fuente</surname><given-names>J.</given-names></name><name name-style="western"><surname>Meins</surname><given-names>A.</given-names></name><name name-style="western"><surname>Iborra</surname><given-names>O.</given-names></name><name name-style="western"><surname>G&#225;lvez</surname><given-names>G.</given-names></name><name name-style="western"><surname>Rodr&#237;guez-Artacho</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>G&#243;mez-Mil&#225;n</surname><given-names>E.</given-names></name></person-group><article-title>The mental and subjective skin: Emotion, empathy, feelings and thermography</article-title><source>Conscious. Cogn.</source><year>2015</year><volume>34</volume><fpage>149</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2015.04.003</pub-id><pub-id pub-id-type="pmid">25955182</pub-id></element-citation></ref><ref id="B11-sensors-25-05276"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sato</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kochiyama</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yoshikawa</surname><given-names>S.</given-names></name></person-group><article-title>Physiological correlates of subjective emotional valence and arousal dynamics while viewing films</article-title><source>Biol. Psychol.</source><year>2020</year><volume>157</volume><elocation-id>107974</elocation-id><pub-id pub-id-type="doi">10.1016/j.biopsycho.2020.107974</pub-id><pub-id pub-id-type="pmid">33086090</pub-id></element-citation></ref><ref id="B12-sensors-25-05276"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eom</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Sohn</surname><given-names>J.H.</given-names></name></person-group><article-title>Emotion recognition using facial thermal images</article-title><source>J. Ergon. Soc. Korea</source><year>2012</year><volume>31</volume><fpage>427</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.5143/JESK.2012.31.3.427</pub-id></element-citation></ref><ref id="B13-sensors-25-05276"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jian</surname><given-names>B.L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>M.W.</given-names></name><name name-style="western"><surname>Yau</surname><given-names>H.T.</given-names></name></person-group><article-title>Emotion-specific facial activation maps based on infrared thermal image sequences</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>48046</fpage><lpage>48052</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2908819</pub-id></element-citation></ref><ref id="B14-sensors-25-05276"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mostafa</surname><given-names>E.</given-names></name><name name-style="western"><surname>Farag</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shalaby</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gault</surname><given-names>T.</given-names></name><name name-style="western"><surname>Mahmoud</surname><given-names>A.</given-names></name></person-group><article-title>Long term facial parts tracking in thermal imaging for uncooperative emotion recognition</article-title><source>Proceedings of the 2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</source><conf-loc>Arlington, VA, USA</conf-loc><conf-date>29 September&#8211;2 October 2013</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/BTAS.2013.6712739</pub-id></element-citation></ref><ref id="B15-sensors-25-05276"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pavlidis</surname><given-names>I.</given-names></name><name name-style="western"><surname>Levine</surname><given-names>J.</given-names></name><name name-style="western"><surname>Baukol</surname><given-names>P.</given-names></name></person-group><article-title>Thermal imaging for anxiety detection</article-title><source>Proceedings of the 2000 IEEE Workshop on Computer Vision Beyond the Visible Spectrum: Methods and Applications (Cat. No.PR00640)</source><conf-loc>Hilton Head, SC, USA</conf-loc><conf-date>16 June 2000</conf-date><volume>Volume 2</volume><fpage>315</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1109/CVBVS.2000.855255</pub-id></element-citation></ref><ref id="B16-sensors-25-05276"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Russell</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Weiss</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mendelsohn</surname><given-names>G.A.</given-names></name></person-group><article-title>Affect grid: A single-item scale of pleasure and arousal</article-title><source>J. Pers. Soc. Psychol.</source><year>1989</year><volume>57</volume><fpage>493</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.57.3.493</pub-id></element-citation></ref><ref id="B17-sensors-25-05276"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yoshitomi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.-I.</given-names></name><name name-style="western"><surname>Kawano</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kilazoe</surname><given-names>T.</given-names></name></person-group><article-title>Effect of sensor fusion for recognition of emotional states using voice, face image and thermal image of face</article-title><source>Proceedings of the 2000 9th IEEE International Workshop on Robot and Human Interactive Communication (RO-MAN 2000)</source><conf-loc>Osaka, Japan</conf-loc><conf-date>27&#8211;29 September 2000</conf-date><fpage>85</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1109/ROMAN.2000.892491</pub-id></element-citation></ref><ref id="B18-sensors-25-05276"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Ingleby</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ward</surname><given-names>R.D.</given-names></name></person-group><article-title>Automated facial expression classification and affect interpretation using infrared measurement of facial skin temperature variations</article-title><source>ACM Trans. Auton. Adapt. Syst.</source><year>2006</year><volume>1</volume><fpage>91</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1145/1152934.1152939</pub-id></element-citation></ref><ref id="B19-sensors-25-05276"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Q.</given-names></name></person-group><article-title>Emotion recognition from thermal infrared images using deep boltzmann machine</article-title><source>Front. Comput. Sci.</source><year>2014</year><volume>8</volume><fpage>609</fpage><lpage>618</lpage><pub-id pub-id-type="doi">10.1007/s11704-014-3295-3</pub-id></element-citation></ref><ref id="B20-sensors-25-05276"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mallat</surname><given-names>K.</given-names></name><name name-style="western"><surname>Dugelay</surname><given-names>J.-L.</given-names></name></person-group><article-title>A benchmark database of visible and thermal paired face images across multiple variations</article-title><source>Proceedings of the 2018 BIOSIG</source><conf-loc>Darmstadt, Germany</conf-loc><conf-date>26&#8211;28 September 2018</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.23919/BIOSIG.2018.8553431</pub-id></element-citation></ref><ref id="B21-sensors-25-05276"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kniaz</surname><given-names>V.V.</given-names></name><name name-style="western"><surname>Knyaz</surname><given-names>V.A.</given-names></name><name name-style="western"><surname>Hladuvka</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kropatsch</surname><given-names>W.G.</given-names></name><name name-style="western"><surname>Mizginov</surname><given-names>V.</given-names></name></person-group><article-title>Thermalgan: Multimodal color-to-thermal image translation for person re-identification in multispectral dataset</article-title><source>Proceedings of the ECCV</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><pub-id pub-id-type="doi">10.1007/978-3-030-11024-6_46</pub-id></element-citation></ref><ref id="B22-sensors-25-05276"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Breiman</surname><given-names>L.</given-names></name></person-group><article-title>Random forests</article-title><source>Machine Learning</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation></ref><ref id="B23-sensors-25-05276"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Drucker</surname><given-names>H.</given-names></name><name name-style="western"><surname>Burges</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Kaufman</surname><given-names>L.</given-names></name><name name-style="western"><surname>Smola</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vapnik</surname><given-names>V.</given-names></name></person-group><article-title>Support vector regression machines</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>1996</year><volume>9</volume><fpage>155</fpage><lpage>161</lpage></element-citation></ref><ref id="B24-sensors-25-05276"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="B25-sensors-25-05276"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sato</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kochiyama</surname><given-names>T.</given-names></name></person-group><article-title>Exploration of emotion dynamics sensing using trapezius EMG and fingertip temperature</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>6553</elocation-id><pub-id pub-id-type="doi">10.3390/s22176553</pub-id><pub-id pub-id-type="pmid">36081011</pub-id><pub-id pub-id-type="pmcid">PMC9460856</pub-id></element-citation></ref><ref id="B26-sensors-25-05276"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>W.</given-names></name></person-group><article-title>Deep facial expression recognition: A survey</article-title><source>IEEE Trans. Affect. Comput.</source><year>2020</year><volume>13</volume><fpage>1195</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2020.2981446</pub-id></element-citation></ref><ref id="B27-sensors-25-05276"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goulart</surname><given-names>C.</given-names></name><name name-style="western"><surname>Valadao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Delisle-Rodriguez</surname><given-names>D.</given-names></name><name name-style="western"><surname>Caldeira</surname><given-names>E.</given-names></name><name name-style="western"><surname>Bastos</surname><given-names>T.</given-names></name></person-group><article-title>Emotion analysis in children through facial emissivity of infrared thermal imaging</article-title><source>PLoS ONE</source><year>2019</year><volume>14</volume><elocation-id>e0212928</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0212928</pub-id><pub-id pub-id-type="pmid">30893343</pub-id><pub-id pub-id-type="pmcid">PMC6426206</pub-id></element-citation></ref><ref id="B28-sensors-25-05276"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Vedaldi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Deep inside convolutional networks: Visualising image classification models and saliency maps</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1312.6034</pub-id></element-citation></ref><ref id="B29-sensors-25-05276"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sundararajan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Taly</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Q.</given-names></name></person-group><article-title>Axiomatic attribution for deep networks</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1703.01365</pub-id></element-citation></ref><ref id="B30-sensors-25-05276"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Song</surname><given-names>R.</given-names></name><name name-style="western"><surname>Jiao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>T.</given-names></name></person-group><article-title>Bus Single-Trip Time Prediction Based on Ensemble Learning</article-title><source>Comput. Intell. Neurosci.</source><year>2022</year><volume>2022</volume><fpage>6831167</fpage><pub-id pub-id-type="doi">10.1155/2022/6831167</pub-id><pub-id pub-id-type="pmid">35990123</pub-id><pub-id pub-id-type="pmcid">PMC9388288</pub-id></element-citation></ref><ref id="B31-sensors-25-05276"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Oyeleye</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Titarenko</surname><given-names>S.</given-names></name><name name-style="western"><surname>Antoniou</surname><given-names>G.</given-names></name></person-group><article-title>A predictive analysis of heart rates using machine learning techniques</article-title><source>Int. J. Environ. Res. Public Health</source><year>2022</year><volume>19</volume><elocation-id>2417</elocation-id><pub-id pub-id-type="doi">10.3390/ijerph19042417</pub-id><pub-id pub-id-type="pmid">35206603</pub-id><pub-id pub-id-type="pmcid">PMC8872524</pub-id></element-citation></ref><ref id="B32-sensors-25-05276"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siirtola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Tamminen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chandra</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ihalapathirana</surname><given-names>A.</given-names></name><name name-style="western"><surname>R&#246;ning</surname><given-names>J.</given-names></name></person-group><article-title>Predicting emotion with biosignals: A comparison of classification and regression models for estimating valence and arousal level using wearable sensors</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>1598</elocation-id><pub-id pub-id-type="doi">10.3390/s23031598</pub-id><pub-id pub-id-type="pmid">36772638</pub-id><pub-id pub-id-type="pmcid">PMC9920941</pub-id></element-citation></ref><ref id="B33-sensors-25-05276"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Craig</surname><given-names>A.D.</given-names></name></person-group><article-title>Forebrain emotional asymmetry: A neuroanatomical basis?</article-title><source>Trends Cogn. Sci.</source><year>2005</year><volume>9</volume><fpage>566</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.10.005</pub-id><pub-id pub-id-type="pmid">16275155</pub-id></element-citation></ref><ref id="B34-sensors-25-05276"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Borod</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Koff</surname><given-names>E.</given-names></name><name name-style="western"><surname>White</surname><given-names>B.</given-names></name></person-group><article-title>Facial asymmetry in posed and spontaneous expressions of emotion</article-title><source>Brain Cogn.</source><year>1983</year><volume>2</volume><fpage>165</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0278-2626(83)90006-4</pub-id><pub-id pub-id-type="pmid">6546020</pub-id></element-citation></ref><ref id="B35-sensors-25-05276"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention Is All You Need</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><fpage>5998</fpage><lpage>6008</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05276-f001" orientation="portrait"><label>Figure 1</label><caption><p>Workflow used for thermal-image-based emotion analysis with the ResNet-34 model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05276-g001.jpg"/></fig><fig position="float" id="sensors-25-05276-f002" orientation="portrait"><label>Figure 2</label><caption><p>The image maps the original thermal data onto a standardized three-dimensional facial model using ultraviolet mapping.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05276-g002.jpg"/></fig><fig position="float" id="sensors-25-05276-f003" orientation="portrait"><label>Figure 3</label><caption><p>Group mean ratings of the second-by-second dynamic valence (<bold>left</bold>) and arousal (<bold>right</bold>) elicited by the emotional film clips. The plots illustrate the temporal fluctuations in subjective emotional responses, showing how valence and arousal ratings vary across film clips.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05276-g003.jpg"/></fig><fig position="float" id="sensors-25-05276-f004" orientation="portrait"><label>Figure 4</label><caption><p>A representative example of actual and predicted arousal ratings (<italic toggle="yes">r</italic> = 0.57) obtained from the ResNet-34 model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05276-g004.jpg"/></fig><fig position="float" id="sensors-25-05276-f005" orientation="portrait"><label>Figure 5</label><caption><p>Mean &#177; standard error of Pearson correlation coefficients (<bold>left</bold>) and mean squared error (<bold>right</bold>) for six arousal prediction models: nose region of interest (ROI) linear regression, multi-ROI linear regression, random forest regression, support vector regression (SVR), ResNet-18, and ResNet-34.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05276-g005.jpg"/></fig><fig position="float" id="sensors-25-05276-f006" orientation="portrait"><label>Figure 6</label><caption><p>The saliency map. The brighter areas made greater contributions to the model predictions. The map is depicted using the standardized three-dimensional facial model coordinate system in <xref rid="sensors-25-05276-f002" ref-type="fig">Figure 2</xref>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05276-g006.jpg"/></fig><fig position="float" id="sensors-25-05276-f007" orientation="portrait"><label>Figure 7</label><caption><p>The scatterplots and polynomial regression lines of integrated gradients (IGs) for the representative facial regions, including the nose tip, forehead, and left and right cheeks, showing the nonlinear relationships between the temperature and arousal ratings in these regions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05276-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05276-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05276-t001_Table 1</object-id><label>Table 1</label><caption><p>Fit indices (adjusted R<sup>2</sup>, root mean squared error [RMSE], Akaike&#8217;s information criterion [AIC], and Bayesian information criterion [BIC]) values for polynomial models (degrees 1&#8211;4) applied to each facial region of interest (ROI).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ROI</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Degree</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Adjusted R<sup>2</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AIC</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BIC</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Nose</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.639401</td><td align="center" valign="middle" rowspan="1" colspan="1">0.003378</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;74,632.8</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;74,619.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.699779</td><td align="center" valign="middle" rowspan="1" colspan="1">0.003082</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;75,833.5</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;75,813.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.706937</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.003045</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;75,990.8</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;75,963.7</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.707854</td><td align="center" valign="middle" rowspan="1" colspan="1">0.00304</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;76,010.4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;75,976.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Forehead</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.058802</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000361</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;103,975</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;103,961</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.059797</td><td align="center" valign="middle" rowspan="1" colspan="1">0.00036</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;103,981</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;103,960</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.073815</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000358</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;104,078</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;104,051</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.111072</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.00035</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;104,347</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;104,313</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Left cheek</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.005772</td><td align="center" valign="middle" rowspan="1" colspan="1">8.08 &#215; 10<sup>&#8722;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;123,590</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;123,577</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.009857</td><td align="center" valign="middle" rowspan="1" colspan="1">8.08 &#215; 10<sup>&#8722;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;123,592</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;123,571</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.058239</td><td align="center" valign="middle" rowspan="1" colspan="1">8.06 &#215; 10<sup>&#8722;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;123,623</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;123,596</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.06942</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>8.06 &#215; 10<sup>&#8722;5</sup></bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;123,629</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;123,595</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Right cheek</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.074671</td><td align="center" valign="middle" rowspan="1" colspan="1">7.73 &#215; 10<sup>&#8722;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;124,177</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;124,164</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.097298</td><td align="center" valign="middle" rowspan="1" colspan="1">7.63 &#215; 10<sup>&#8722;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;124,339</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;124,318</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.102118</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>7.61 &#215; 10<sup>&#8722;5</sup></bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;124,373</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#8722;124,346</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.102143</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.61 &#215; 10<sup>&#8722;5</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;124,372</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;124,338</td></tr></tbody></table><table-wrap-foot><fn><p>The optimal models are in bold.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>