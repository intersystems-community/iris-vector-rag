<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431000</article-id><article-id pub-id-type="pmcid-ver">PMC12431000.1</article-id><article-id pub-id-type="pmcaid">12431000</article-id><article-id pub-id-type="pmcaiid">12431000</article-id><article-id pub-id-type="doi">10.3390/s25175337</article-id><article-id pub-id-type="publisher-id">sensors-25-05337</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>TopoTempNet: A High-Accuracy and Interpretable Decoding Method for fNIRS-Based Motor Imagery</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-6984-3901</contrib-id><name name-style="western"><surname>Han</surname><given-names initials="Q">Qiulei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05337" ref-type="aff">1</xref><xref rid="af2-sensors-25-05337" ref-type="aff">2</xref><xref rid="af3-sensors-25-05337" ref-type="aff">3</xref><xref rid="af4-sensors-25-05337" ref-type="aff">4</xref><xref rid="c1-sensors-25-05337" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ye</surname><given-names initials="H">Hongbiao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-05337" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Sun</surname><given-names initials="Y">Yan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05337" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Song</surname><given-names initials="Z">Ze</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-05337" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3265-6461</contrib-id><name name-style="western"><surname>Zhao</surname><given-names initials="J">Jian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05337" ref-type="aff">1</xref><xref rid="af2-sensors-25-05337" ref-type="aff">2</xref><xref rid="af3-sensors-25-05337" ref-type="aff">3</xref><xref rid="af4-sensors-25-05337" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shi</surname><given-names initials="L">Lijuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af2-sensors-25-05337" ref-type="aff">2</xref><xref rid="af3-sensors-25-05337" ref-type="aff">3</xref><xref rid="af4-sensors-25-05337" ref-type="aff">4</xref><xref rid="af5-sensors-25-05337" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5632-7596</contrib-id><name name-style="western"><surname>Kuang</surname><given-names initials="Z">Zhejun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05337" ref-type="aff">1</xref><xref rid="af2-sensors-25-05337" ref-type="aff">2</xref><xref rid="af3-sensors-25-05337" ref-type="aff">3</xref><xref rid="af4-sensors-25-05337" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Shleev</surname><given-names initials="S">Sergey</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Magnus</surname><given-names initials="F">Falk</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05337"><label>1</label>College of Computer Science and Technology, Changchun University, Changchun 130022, China; <email>ye_hb1109@163.com</email> (H.Y.); <email>sunyan.ee@outlook.com</email> (Y.S.); <email>szfsy520@gmail.com</email> (Z.S.); <email>zhaojian@ccu.edu.cn</email> (J.Z.); <email>kuangzhejun@ccu.edu.cn</email> (Z.K.)</aff><aff id="af2-sensors-25-05337"><label>2</label>Key Laboratory of Intelligent Rehabilitation and Barrier-Free Access for the Disabled, Ministry of Education, Changchun 130022, China; <email>shilj@ccu.edu.cn</email></aff><aff id="af3-sensors-25-05337"><label>3</label>Jilin Provincial Key Laboratory of Human Health State Identification and Function Enhancement, Changchun 130022, China</aff><aff id="af4-sensors-25-05337"><label>4</label>Jilin Rehabilitation Equipment and Technology Engineering Research Center for the Disabled, Changchun 130022, China</aff><aff id="af5-sensors-25-05337"><label>5</label>College of Electronic Information Engineering, Changchun University, Changchun 130022, China</aff><author-notes><corresp id="c1-sensors-25-05337"><label>*</label>Correspondence: <email>hanql@ccu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>28</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5337</elocation-id><history><date date-type="received"><day>17</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>23</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>27</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>28</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05337.pdf"/><abstract><p>Functional near-infrared spectroscopy (fNIRS) offers a safe and portable signal source for brain&#8211;computer interface (BCI) applications, particularly in motor imagery (MI) decoding. However, its low sampling rate and hemodynamic delay pose challenges for temporal modeling and dynamic brain network analysis. To address these limitations in temporal dynamics, static graph modeling, and feature fusion interpretability, we propose TopoTempNet, an innovative topology-enhanced temporal network for biomedical signal decoding. TopoTempNet integrates multi-level graph features with temporal modeling through three key innovations: (1) multi-level topological feature construction using local and global functional connectivity metrics (e.g., connection strength, density, global efficiency); (2) a graph-modulated attention mechanism combining Transformer and Bi-LSTM to dynamically model key connections; and (3) a multimodal fusion strategy uniting raw signals, graph structures, and temporal representations into a high-dimensional discriminative space. Evaluated on three public fNIRS datasets (MA, WG, UFFT), TopoTempNet achieves superior accuracy (up to 90.04% &#177; 3.53%) and Kappa scores compared to state-of-the-art models. The ROC curves and t-SNE visualizations confirm its excellent feature discrimination and structural clarity. Furthermore, the statistical analysis of graph features reveals the model&#8217;s ability to capture task-specific functional connectivity patterns, enhancing the interpretability of decoding outcomes. TopoTempNet provides a novel pathway for building interpretable and high-performance BCI systems based on fNIRS.</p></abstract><kwd-group><kwd>functional near-infrared spectroscopy (fNIRS)</kwd><kwd>brain&#8211;computer interface (BCI)</kwd><kwd>biomedical signal decoding</kwd><kwd>topological graph features</kwd></kwd-group><funding-group><award-group><funding-source>Science and Technology Department of Jilin Province, China</funding-source><award-id>YDZJ202201ZYTS684</award-id></award-group><funding-statement>This work was supported in part by the development program project of the Science and Technology Department of Jilin Province, China, under Grant YDZJ202201ZYTS684.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05337"><title>1. Introduction</title><p>Brain&#8211;computer interface (BCI), as a cutting-edge technology enabling direct information exchange between the brain and external devices, is particularly valuable for assisting individuals with motor impairments in achieving non-muscle-dependent intent expression. Among BCI modalities, functional near-infrared spectroscopy (fNIRS) has garnered significant attention in recent research due to its non-invasiveness, portability, and excellent user adaptability [<xref rid="B1-sensors-25-05337" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05337" ref-type="bibr">2</xref>]. By detecting absorption changes in near-infrared light (650&#8211;950 nm wavelength) [<xref rid="B3-sensors-25-05337" ref-type="bibr">3</xref>], fNIRS records dynamic concentration variations in oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR) in the cerebral cortex, thereby indirectly reflecting hemodynamic responses evoked by neural activity [<xref rid="B4-sensors-25-05337" ref-type="bibr">4</xref>]. Combined with the motor imagery (MI) paradigm, fNIRS-based BCI systems offer a safe and efficient neural signal decoding solution for patients with motor disabilities [<xref rid="B5-sensors-25-05337" ref-type="bibr">5</xref>].</p><p>To further enhance decoding performance in MI tasks, researchers have progressively explored the multimodal fusion of electroencephalography (EEG) and fNIRS [<xref rid="B6-sensors-25-05337" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05337" ref-type="bibr">7</xref>]. EEG provides millisecond-level temporal resolution [<xref rid="B8-sensors-25-05337" ref-type="bibr">8</xref>], enabling real-time monitoring of neuronal population electrical activity [<xref rid="B9-sensors-25-05337" ref-type="bibr">9</xref>], while fNIRS delivers spatial information on neurovascular coupling [<xref rid="B10-sensors-25-05337" ref-type="bibr">10</xref>], rendering the two modalities highly complementary in spatiotemporal dimensions. For instance, Cooney et al. proposed an EEG-fNIRS hybrid BCI using dual convolutional networks for speech decoding, where concatenated EEG and fNIRS features were fed into a gated recurrent unit (GRU) layer [<xref rid="B11-sensors-25-05337" ref-type="bibr">11</xref>]. Similarly, Arif et al. achieved cross-subject mental state recognition by fusing spatiotemporal features of EEG and fNIRS [<xref rid="B7-sensors-25-05337" ref-type="bibr">7</xref>].</p><p>However, despite EEG&#8217;s inherent advantage in temporal resolution, its practical application faces multiple challenges. On the one hand, EEG is highly susceptible to physiological and environmental noise such as ocular/muscular artifacts and powerline interference [<xref rid="B12-sensors-25-05337" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05337" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05337" ref-type="bibr">14</xref>], with noise control becoming particularly difficult during natural or movement conditions [<xref rid="B15-sensors-25-05337" ref-type="bibr">15</xref>]. Wang et al. systematically verified the anti-interference advantages of fNIRS: quantitative comparisons showed superior resistance to motion and electromagnetic artifacts compared with fMRI, PET, and EEG; in practice, it is suitable for children and multi-electromagnetic environments; and in principle, it relies on near-infrared light to detect hemoglobin changes, inherently providing strong noise resistance [<xref rid="B16-sensors-25-05337" ref-type="bibr">16</xref>].</p><p>Current fNIRS-based signal decoding methods predominantly adopt a four-stage pipeline: signal acquisition, preprocessing, feature extraction, and state classification. Feature extraction typically relies on manually designed statistical metrics including mean, variance, and slope coupled with traditional classifiers like SVM, LDA, or RF [<xref rid="B17-sensors-25-05337" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05337" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05337" ref-type="bibr">19</xref>]. Such approaches heavily depend on handcrafted feature selection and domain expertise, constraining their generalization capability. Recently, deep learning has emerged as a mainstream technical pathway for fNIRS decoding. For example, Rojas et al. demonstrated superior performance of deep learning models such as CNN and LSTM over machine learning in pain type identification [<xref rid="B20-sensors-25-05337" ref-type="bibr">20</xref>]. In neuroscience research, graph-based methods leverage connectivity metrics such as Pearson correlation coefficient (PCC), phase lag index (PLI), and phase locking value (PLV) to holistically analyze brain networks [<xref rid="B21-sensors-25-05337" ref-type="bibr">21</xref>]. Representative works include Wang et al.&#8217;s fNIRSNet incorporating hemodynamic delay characteristics [<xref rid="B22-sensors-25-05337" ref-type="bibr">22</xref>] and Cheng et al.&#8217;s approach for transforming fNIRS time series into 2D images via Gramian Angular Field fused with cognitive scale data at the decision level to improve diagnostic accuracy for amnestic mild cognitive impairment (aMCI) [<xref rid="B23-sensors-25-05337" ref-type="bibr">23</xref>].</p><p>The inherent functional connectivity information embedded in fNIRS signals has propelled graph-based modeling into a research hotspot. To precisely capture inter-regional brain collaboration patterns, researchers have introduced learnable and dynamically updated graph structures. For instance, Seo et al. constructed PCC-based graphs with channels as nodes and inter-channel connections as edges, and then extracted functional connectivity patterns using graph convolutional networks (GCNs) [<xref rid="B24-sensors-25-05337" ref-type="bibr">24</xref>], while Yu et al. employed coherence-based graphs with GCNs for spatiotemporal feature extraction in depression recognition [<xref rid="B25-sensors-25-05337" ref-type="bibr">25</xref>], achieving notable improvements in decoding performance and interpretability.</p><p>Furthermore, to mitigate overfitting in fNIRS deep learning models, strategies including multimodal fusion, data augmentation, and adaptive receptive field design have been proposed [<xref rid="B26-sensors-25-05337" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05337" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05337" ref-type="bibr">28</xref>]. Examples include the GCN-CA-CapsNet model for EEG-fNIRS emotion recognition, which fused features via Pearson correlation graphs and optimized accuracy using capsule attention mechanisms [<xref rid="B29-sensors-25-05337" ref-type="bibr">29</xref>]. Graph theory, as a tool for modeling system elements and interactions via nodes and edges, exhibits unique advantages in brain network analysis [<xref rid="B30-sensors-25-05337" ref-type="bibr">30</xref>]. For fNIRS channel connectivity, metrics like average degree and global efficiency have been applied to quantify functional connectivity in the dorsolateral prefrontal cortex (DLPFC) [<xref rid="B31-sensors-25-05337" ref-type="bibr">31</xref>].</p><p>Despite these advances, critical challenges that remain are as follows: 1. Inadequate temporal modeling: most models fail to adapt to fNIRS&#8217;s low sampling rate and delayed response characteristics, limiting dynamic feature extraction. 2. Static graph structures: graph construction often relies on fixed or manual designs, unable to reflect dynamic functional connectivity during cognitive processes. 3. Weak feature fusion and interpretability: insufficient multi-scale feature integration and neural network interpretability hinder further performance gains.</p><p>To address these issues, this paper proposes TopoTempNet, a novel fNIRS-MI classification method integrating graph theory and temporal dynamic modeling, with three key contributions: 1. Multi-level graph feature modeling: constructs local channel pair and global whole-network functional connectivity graphs, extracting metrics including connection strength, density, Reciprocal of Functional Signal Mean Difference (RFSMD), and global efficiency to unify local&#8211;global brain network relationships. 2. Graph-enhanced temporal architecture: designs a hybrid temporal network combining Transformer and Bi-LSTM with graph attention to highlight critical channel pairs, capturing spatiotemporal dynamics and structural dependencies. 3. Multi-source fusion mechanism: combines raw signals, graph features, and temporal representations into a high-dimensional fusion space, boosting decoding accuracy and cross-subject generalization.</p><p>Experimental results demonstrate significant performance improvements over state-of-the-art methods on multiple fNIRS datasets, providing robust support for dynamic modeling of complex brain networks and differential region identification.</p></sec><sec id="sec2-sensors-25-05337"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-05337"><title>2.1. Selected Dataset</title><p>To evaluate the effectiveness of the proposed TopoTempNet, experiments were conducted on three publicly available datasets.</p><sec id="sec2dot1dot1-sensors-25-05337"><title>2.1.1. MA Dataset [<xref rid="B32-sensors-25-05337" ref-type="bibr">32</xref>]</title><p>This dataset includes 29 participants (mean age: 28.5 &#177; 3.7 years). During experiments, participants performed mental arithmetic (MA) tasks and baseline (BL) tasks. In MA tasks, participants memorized arithmetic expressions displayed on-screen and performed consecutive addition/subtraction operations. BL tasks required participants to remain motionless without additional instructions. Each trial began with a 2 s cue period, followed by a 10 s task execution phase, with 15&#8211;17 s of rest between trials. Each participant completed three experimental blocks, with 10 MA and 10 BL trials per block.</p></sec><sec id="sec2dot1dot2-sensors-25-05337"><title>2.1.2. WG Dataset [<xref rid="B33-sensors-25-05337" ref-type="bibr">33</xref>]</title><p>Comprising 26 subjects (mean age: 26.1 &#177; 3.5 years), this dataset involved word generation (WG) and baseline (BL) tasks. During WG tasks, subjects generated as many words as possible starting with a displayed letter within a time limit, while BL tasks required quiet rest. Each task type was repeated 30 times, with 10 s task durations and 13&#8211;15 s of rest intervals.</p></sec><sec id="sec2dot1dot3-sensors-25-05337"><title>2.1.3. UFFT Dataset [<xref rid="B34-sensors-25-05337" ref-type="bibr">34</xref>]</title><p>This dataset contains fNIRS signals from 30 volunteers (17 males) performing three motor tasks: right-hand tapping (RHT), left-hand tapping (LHT), and foot tapping (FT), with 25 trials per task. Each trial consisted of a 2 s cue phase, a 10 s execution phase, and 17&#8211;19 s of rest.</p></sec></sec><sec id="sec2dot2-sensors-25-05337"><title>2.2. Data Preprocessing</title><p>The near-infrared spectroscopy (NIRS) acquisition system records raw optical density changes (<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">&#916;</mml:mo><mml:mi>O</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), which are converted to oxygenated hemoglobin <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo mathvariant="normal">&#916;</mml:mo><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and deoxygenated hemoglobin <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo mathvariant="normal">&#916;</mml:mo><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) concentration changes using the modified Beer&#8211;Lambert law [<xref rid="B35-sensors-25-05337" ref-type="bibr">35</xref>]. The mathematical formulation is as follows:<disp-formula id="FD1-sensors-25-05337"><label>(1)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo mathvariant="normal">&#916;</mml:mo><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo mathvariant="normal">&#916;</mml:mo><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#949;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>O</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:msub><mml:mi>&#949;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>R</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#949;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>O</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:msub><mml:mi>&#949;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>R</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo mathvariant="normal">&#916;</mml:mo><mml:mi>O</mml:mi><mml:mi>D</mml:mi><mml:mfenced><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo mathvariant="normal">&#916;</mml:mo><mml:mi>O</mml:mi><mml:mi>D</mml:mi><mml:mfenced><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#949;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>O</mml:mi><mml:mfenced><mml:mo mathvariant="normal">&#183;</mml:mo></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#949;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>R</mml:mi><mml:mfenced><mml:mo mathvariant="normal">&#183;</mml:mo></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the extinction coefficients of HbO and HbR at wavelength <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the differential pathlength factor, and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> represents the source&#8211;detector separation distance.</p><p>Following the protocols described in the respective dataset references, the NIRS signals for MA and WG were downsampled to 10 Hz, while UTTF maintained its original 13.3 Hz sampling rate. Since raw fNIRS signals contain instrument noise, physiological noise, and motion artifacts, task-specific filtering strategies were applied, i.e., MA and UTTF: bandpass filter (0.01&#8211;0.1 Hz); WG: lowpass filter (cutoff: 0.2 Hz).</p><p>Subsequently, signals were segmented into non-overlapping 1 s epochs. To account for hemodynamic response delays, each epoch included pre- and post-stimulus periods to ensure the complete capture of task-related dynamics. Baseline correction was performed by subtracting the mean value of the pre-stimulus reference interval from each fNIRS epoch to mitigate drift.</p><p>As illustrated in <xref rid="sensors-25-05337-f001" ref-type="fig">Figure 1</xref>, the proposed TopoTempNet framework is designed for multi-class time series signal classification. Its core innovation lies in a multi-level graph-theoretic feature extraction framework that quantifies inter-channel functional connectivity patterns (local associations and global network properties), integrated with a hybrid GAM-Bi-LSTM architecture for precise temporal pattern classification.</p></sec><sec id="sec2dot3-sensors-25-05337"><title>2.3. Data Structure Definition</title><p>The model input data are represented as a multichannel temporal signal matrix:<disp-formula id="FD2-sensors-25-05337"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> denotes the total number of channels, and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> represents the number of time points in each signal segment. To further model the pairing relationships between channels, let <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, meaning that all channels are grouped into <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> pairs, with each pair corresponding to a potential functional connection.</p><p>To construct the initial feature representation, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> is stacked column-wise and reshaped into a one-dimensional vector as follows:<disp-formula id="FD3-sensors-25-05337"><label>(3)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">&#183;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This vector serves as the foundational feature representation, preserving the joint information across channels and time dimensions from the original temporal signals, thereby providing the input basis for subsequent graph-structured and temporal modeling.</p></sec><sec id="sec2dot4-sensors-25-05337"><title>2.4. Channel Pair Construction</title><p>To more effectively model the functional connectivity between brain regions, this study converts the raw optical density change signals using the modified Beer&#8211;Lambert law to extract the concentration changes in oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR). Since each measurement channel contains both HbO and HbR hemodynamic signals, this study treats them as the fundamental unit of functional coupling&#8212;referred to as a channel pair&#8212;with a total of <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> pairs.</p></sec><sec id="sec2dot5-sensors-25-05337"><title>2.5. Graph-Theoretic Feature Extraction Module</title><p>This module aims to extract two types of key features from fNIRS signals through graph-theoretic analysis: local connectivity features (based on channel pairs) and global topological features (based on the overall network structure), incorporating both raw signal characteristics and functional connectivity features.</p><sec id="sec2dot5dot1-sensors-25-05337"><title>2.5.1. Local Feature Extraction of Homologous Channel Pairs</title><p>For each channel pair (the <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>-th pair), its local functional connectivity characteristics are quantified from the following three aspects:<list list-type="bullet"><list-item><p>Connection Strength: it is defined as the normalized Pearson correlation coefficient, measuring the linear association between the channels in the pair.</p></list-item></list><disp-formula id="FD4-sensors-25-05337"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced><mml:mrow><mml:mi>&#961;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>&#160;</mml:mo><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represent the HbO and HbR signals of the <italic toggle="yes">i</italic>-th channel pair, respectively, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mfenced><mml:mo>&#183;</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Pearson correlation coefficient:<disp-formula id="FD5-sensors-25-05337"><label>(5)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mstyle mathsize="70%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#175;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#175;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mstyle mathsize="70%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#175;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:msubsup><mml:mstyle mathsize="70%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#175;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle><mml:msubsup><mml:mstyle mathsize="70%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean of <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the value at the <italic toggle="yes">t</italic>-th time point. After normalization, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> ranges within (0, 1), with larger values indicating stronger connectivity between the two channels.
<list list-type="bullet"><list-item><p>Connection Density: it is defined as the proportion of significant correlations within a sliding window, measuring the stability of the connection.</p></list-item></list>
<disp-formula id="FD6-sensors-25-05337"><label>(6)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mn>1</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>&#961;</mml:mi><mml:mfenced close="|" open="|"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mn>0.3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">L</italic> is the length of the sliding window, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mfenced><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>L</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the signal segment in the <italic toggle="yes">k</italic>-th sliding window, 0.3 is the significance threshold, and 1(&#8901;) is the indicator function (equals 1 if the condition is true, otherwise 0). A larger value of <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> indicates more frequent effective connectivity between the two channels.
<list list-type="bullet"><list-item><p>Reciprocal of Functional Signal Mean Difference (RFSMD): it is defined as the reciprocal of the mean difference between two channel signals, used to quantify the efficiency of functional information transfer between channel pairs.</p></list-item></list>
<disp-formula id="FD7-sensors-25-05337"><label>(7)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:msubsup><mml:mstyle mathsize="70%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#949;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#949;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a small constant added to avoid division by zero. A smaller value indicates greater similarity between the two channel signals and higher information transfer efficiency.</p></sec><sec id="sec2dot5dot2-sensors-25-05337"><title>2.5.2. Global Network Feature Extraction</title><p>A functional connectivity graph is constructed at the full-channel scale, and topological metrics of the overall brain network are extracted.</p><list list-type="bullet"><list-item><p>Construction of the Functional Connectivity Network</p></list-item></list><p>An adjacency matrix <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is constructed to indicate whether a significant functional connection exists between channels:<disp-formula id="FD8-sensors-25-05337"><label>(8)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>&#961;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mn>0.3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8201;</mml:mo><mml:mo>&#8201;</mml:mo><mml:mo>&#8201;</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#961;(&#183;) denotes the Pearson correlation coefficient, and 0.3 is the global connection threshold. <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> = 1 indicates that a significant functional connection exists between the two corresponding channels.</p><list list-type="bullet"><list-item><p>Global Efficiency</p></list-item></list><p>It is defined as the average of the inverse shortest path lengths between all pairs of nodes in the network, and global efficiency measures the average efficiency of information transfer across the entire network. It is formally defined as follows:<disp-formula id="FD9-sensors-25-05337"><label>(9)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mfenced><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8800;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mi>C</mml:mi></mml:msubsup><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the shortest path length between nodes <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, calculated based on the adjacency matrix A. The global efficiency <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>[0, 1], with higher values indicating more efficient global information transfer within the network.</p></sec></sec><sec id="sec2dot6-sensors-25-05337"><title>2.6. Feature Fusion Module</title><p>To integrate information from multiple sources, this study designs a feature fusion mechanism that concatenates the raw temporal signal features, graph-theoretic local connectivity features, and global topological metrics into a unified representation. The details are as follows:<disp-formula id="FD10-sensors-25-05337"><label>(10)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mfenced><mml:mn>1</mml:mn></mml:mfenced><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mn>1</mml:mn></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mfenced><mml:mn>1</mml:mn></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector formed by sequentially stacking the local features of all channel pairs. The dimension of the fused feature vector is <italic toggle="yes">D</italic> = <italic toggle="yes">C</italic>
<inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#183;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec2dot7-sensors-25-05337"><title>2.7. Temporal Modeling Module</title><sec id="sec2dot7dot1-sensors-25-05337"><title>2.7.1. Transformer Temporal Encoding [<xref rid="B36-sensors-25-05337" ref-type="bibr">36</xref>]</title><p>To model the temporal dynamics of fNIRS signals, this study introduces a Transformer module to encode the channel pair signals. The specific steps are as follows:<list list-type="order"><list-item><p>Input Projection: the dual-channel signals (HbO and HbR) of each channel pair are mapped into a high-dimensional feature space:
<disp-formula id="FD11-sensors-25-05337"><label>(11)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, with <italic toggle="yes">d</italic> representing the feature dimension; <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the projection weight matrix, and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias vector.</p></list-item><list-item><p>Transformer Encoding: A standard Transformer layer is applied to model the temporal dependencies within the time series of each channel pair, capturing relationships across different time points:
<disp-formula id="FD12-sensors-25-05337"><label>(12)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Temporal Aggregation: average pooling is performed along the temporal dimension to obtain an overall temporal representation for each channel pair
<disp-formula id="FD13-sensors-25-05337"><label>(13)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>Z</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>D</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the features of <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> at the <italic toggle="yes">t</italic>-th time point.</p></list-item></list></p></sec><sec id="sec2dot7dot2-sensors-25-05337"><title>2.7.2. Feature Projection and Fusion</title><p>To achieve the unified alignment of features from different sources, the fused feature <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is projected into the same feature space dimension as the Transformer output:<disp-formula id="FD14-sensors-25-05337"><label>(14)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">proj</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the projected fused feature, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the projection weight matrix, and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias vector. This is then further fused with the temporal features <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> extracted by the Transformer to construct a unified input representation:<disp-formula id="FD15-sensors-25-05337"><label>(15)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This process jointly represents structural (graph-theoretic) and dynamic (temporal) features on a unified scale, providing the foundation for subsequent attention modulation and decoding.</p></sec><sec id="sec2dot7dot3-sensors-25-05337"><title>2.7.3. Graph-Theoretic Temporal Association Network Module</title><p>This module integrates the topological characteristics of the functional connectivity network with temporal dynamics through graph-theory-guided gated sequence modeling. The core lies in the design of a graph attention mechanism (GAM), which enables the precise capture of dynamic relationships between channel pairs by fusing the output features of the subsequent Bi-LSTM. Specifically, GAM utilizes weights <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> derived from graph-theoretic metrics (such as connection strength) to modulate the input sequence to LSTM. By introducing graph structural priors into the gating layer, it achieves topology-aware temporal attention modeling.</p><p>Graph Attention Mechanism (GAM): It assigns differentiated semantic weights to different channel pairs. For the <italic toggle="yes">i</italic>-th channel pair, the attention weight <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as follows:<disp-formula id="FD16-sensors-25-05337"><label>(16)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>&#945;</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>&#945;</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the learnable attention weight matrix, producing the attention weights <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the channel pairs.</p></sec><sec id="sec2dot7dot4-sensors-25-05337"><title>2.7.4. Bidirectional Temporal Association Modeling (Bi-LSTM)</title><p>The Bidirectional Temporal Association Modeling (Bi-LSTM) module aims to capture bidirectional temporal dependencies in the fused feature sequences using forward and backward LSTM units (as shown in <xref rid="sensors-25-05337-f002" ref-type="fig">Figure 2</xref>), and enhances attention to dynamic patterns of critical channel pairs by incorporating graph-theoretic attention weights.</p><p>Input Data: The input consists of the fused feature sequence E (formed by concatenating temporal features from the Transformer and projected graph-theoretic features, <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) and the graph attention weights <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> (from the GAM module, <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). Here, <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is used to modulate the LSTM&#8217;s focus on channel pairs with strong functional connectivity.</p><p>Gating Mechanism and Equations: each LSTM unit updates its state dynamically through the forget gate, input gate, and output gate, where all gating operations incorporate the attention weights <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>:</p><p>Forget Gate: It determines the proportion of the historical hidden state to retain. The formula is as follows:<disp-formula id="FD17-sensors-25-05337"><label>(17)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo mathvariant="normal">&#183;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight matrix of the forget gate, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias term, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> is the sigmoid activation function, and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the forward hidden state from the previous time step.</p><p>Input Gate: it regulates the contribution of new features to the current cell state.<disp-formula id="FD18-sensors-25-05337"><label>(18)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo mathvariant="normal">&#183;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the input gate weight matrix, and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias term.</p><p>Cell State Update:<disp-formula id="FD19-sensors-25-05337"><label>(19)</label><mml:math id="mm79" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#732;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo mathvariant="normal">&#183;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD20-sensors-25-05337"><label>(20)</label><mml:math id="mm80" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#732;</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight matrix for the cell state, <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias term, and &#8857; denotes element-wise multiplication.</p><p>Output Gate: it generates the current hidden state.<disp-formula id="FD21-sensors-25-05337"><label>(21)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo mathvariant="normal">&#183;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD22-sensors-25-05337"><label>(22)</label><mml:math id="mm84" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:mrow><mml:mi>tan</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the output gate weight matrix, <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias term, and <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the current forward hidden state.</p><p>Bidirectional Modeling Output:<disp-formula id="FD23-sensors-25-05337"><label>(23)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#8594;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#8592;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The final output is obtained by concatenating the forward and backward hidden states: <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">h</italic> is the dimension of the hidden layer.</p></sec></sec><sec id="sec2dot8-sensors-25-05337"><title>2.8. Classification Module</title><p>The classification layer takes the fused features and temporal association features as inputs and outputs the classification results through a multilayer perceptron (MLP).</p><sec id="sec2dot8dot1-sensors-25-05337"><title>2.8.1. Input Feature Definition</title><p>The fused features and temporal association features are concatenated and integrated through a linear transformation to form the input representation:<disp-formula id="FD24-sensors-25-05337"><label>(24)</label><mml:math id="mm90" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mfenced><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>h</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the learnable weight matrix (with mm denoting the dimension of the intermediate layer), <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias vector, and <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the intermediate classification feature.</p></sec><sec id="sec2dot8dot2-sensors-25-05337"><title>2.8.2. Nonlinear Activation</title><p>The ReLU function is applied to enhance the nonlinear representation capability of the features:<disp-formula id="FD25-sensors-25-05337"><label>(25)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">f</mml:mi><mml:mrow><mml:mi>act</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi mathvariant="normal">f</mml:mi><mml:mrow><mml:mi>cls</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot8dot3-sensors-25-05337"><title>2.8.3. Classification Output</title><p>The final classification probability distribution is produced via a fully connected layer followed by a Softmax function:<disp-formula id="FD26-sensors-25-05337"><label>(26)</label><mml:math id="mm95" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (with KK denoting the number of classes), <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the output bias, and <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability distribution.</p></sec></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05337"><title>3. Experimental</title><sec id="sec3dot1-sensors-25-05337"><title>3.1. Experimental Setup</title><p>All experiments were conducted on a GPU server with CUDA-compatible drivers using the PyTorch (v2.7.1) framework. The model was trained with the Adam optimizer [<xref rid="B37-sensors-25-05337" ref-type="bibr">37</xref>] (learning rate = 1 &#215; 10<sup>&#8722;3</sup>, weight decay = 1 &#215; 10<sup>&#8722;4</sup>). A ReduceLROnPlateau scheduler [<xref rid="B38-sensors-25-05337" ref-type="bibr">38</xref>] adjusted the learning rate based on validation accuracy. </p><p>Training used 5-fold cross-validation per subject, with a batch size of 5 and a maximum of 200 epochs. To prevent overfitting, dropout was applied in two modules:<list list-type="bullet"><list-item><p>Bi-LSTM Module: a 0.1 dropout rate after temporal feature projection.</p></list-item><list-item><p>Classifier Module: a 0.2 dropout rate in the intermediate MLP layer to enhance robustness and generalization.</p></list-item></list></p></sec><sec id="sec3dot2-sensors-25-05337"><title>3.2. Evaluation Metrics</title><p>Model performance was evaluated using standard classification metrics:</p><p><italic toggle="yes">Accuracy</italic>: it measures overall prediction correctness.<disp-formula id="FD27-sensors-25-05337"><label>(27)</label><mml:math id="mm99" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>ROC and AUC: the ROC curve evaluates classifier performance, with AUC indicating the model&#8217;s overall effectiveness (closer to 1 is better).<disp-formula id="FD28-sensors-25-05337"><label>(28)</label><mml:math id="mm100" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD29-sensors-25-05337"><label>(29)</label><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Cohen&#8217;s <italic toggle="yes">Kappa</italic>: it assesses agreement beyond chance, especially under class imbalance.<disp-formula id="FD30-sensors-25-05337"><label>(30)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where<disp-formula id="FD31-sensors-25-05337"><label>(31)</label><mml:math id="mm103" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD32-sensors-25-05337"><label>(32)</label><mml:math id="mm104" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced><mml:mfenced><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mfenced><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Higher <italic toggle="yes">Kappa</italic> values indicate more reliable and consistent classification.</p></sec></sec><sec sec-type="results" id="sec4-sensors-25-05337"><title>4. Results and Discussion</title><p><xref rid="sensors-25-05337-f003" ref-type="fig">Figure 3</xref> presents the average subject-wise confusion matrices under three tasks: two binary classification tasks (mental arithmetic versus baseline, word generation versus baseline) and one three-class motor imagery task (right-hand imagery, left-hand imagery, foot imagery). In the mental arithmetic versus baseline task, an average of 90 percent of real MA samples were correctly predicted as MA, while 10 percent were misclassified as BL. Similarly, 90 percent of real BL samples were correctly recognized, with 10 percent misclassified as MA, demonstrating good classification performance for this task. In the word generation versus baseline task, 83.33 percent of real WG samples were correctly identified, while 16.66 percent were misclassified as BL. Meanwhile, 80 percent of BL samples were correctly classified, and 20 percent were misclassified as WG. For the three-class motor imagery task, 85 percent of RHF samples were correctly classified, 10 percent misclassified as LHF, and 5 percent as FT. Among the LHF samples, 80 percent were correctly identified, 5 percent misclassified as RHF, and 15 percent as FT. For FT samples, 70 percent were correctly classified, while 15 percent were misclassified as RHF and another 15 percent as LHF. These matrices intuitively illustrate the model&#8217;s average classification performance across different tasks by presenting the proportions of correct and incorrect classifications for each class.</p><sec id="sec4dot1-sensors-25-05337"><title>4.1. Comparative Experiments</title><sec id="sec4dot1dot1-sensors-25-05337"><title>4.1.1. Subject-Specific Evaluation</title><p>In this study, the proposed TopoTempNet model is systematically benchmarked against a series of structurally comparable baseline methods. To ensure rigor and reproducibility, all baseline models were faithfully re-implemented according to the hyperparameter configurations reported in their original publications. Furthermore, the entire preprocessing pipeline, input representation, and training and evaluation protocols were standardized across all models for a fair comparison.</p><p>The comparative results, encompassing both classification accuracy (Acc) and inference time (IT), are summarized in <xref rid="sensors-25-05337-t001" ref-type="table">Table 1</xref>. Here, Acc denotes the predictive accuracy of the model, while IT quantifies the computational cost incurred during the inference stage. The baseline methods under evaluation are detailed as follows:<list list-type="bullet"><list-item><p>CNN [<xref rid="B39-sensors-25-05337" ref-type="bibr">39</xref>]: It consists of three convolutional layers and two dense layers. The input is the &#916;[HbO<sub>2</sub>] and &#916;HbR data stacked along a new dimension after separation.</p></list-item><list-item><p>LSTM [<xref rid="B39-sensors-25-05337" ref-type="bibr">39</xref>]: A basic three-layer Long Short-Term Memory model with a hidden size of 20. The input includes the unseparated &#916;[HbO<sub>2</sub>] and &#916;HbR signals [<xref rid="B37-sensors-25-05337" ref-type="bibr">37</xref>].</p></list-item><list-item><p>PVTv2-B0 [<xref rid="B40-sensors-25-05337" ref-type="bibr">40</xref>]: A vision Transformer model using 4 &#215; 4 patches and an eight-layer structure. Multichannel GADF-transformed virtual images are used as input.</p></list-item><list-item><p>MLP-Mixer [<xref rid="B40-sensors-25-05337" ref-type="bibr">40</xref>]: An eight-layer MLP-Mixer model based on 4 &#215; 4 patches, which captures features through channel-mixing and spatial-mixing MLPs. It also takes multichannel GADF-transformed virtual images as input.</p></list-item><list-item><p>fNIRS-T [<xref rid="B41-sensors-25-05337" ref-type="bibr">41</xref>]: A Transformer-based classification model designed to capture long-range dependencies in fNIRS signals.</p></list-item><list-item><p>FCS-TPNet [<xref rid="B42-sensors-25-05337" ref-type="bibr">42</xref>]: It learns delayed hemodynamic features and captures the correlation between HbO and HbR through a dual-signal interaction module. A dynamic graph convolution module is employed to extract complex topological patterns between channels for fNIRS signal classification.</p></list-item></list>
<table-wrap position="anchor" id="sensors-25-05337-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05337-t001_Table 1</object-id><label>Table 1</label><caption><p>Performance comparison of different models on three public datasets (bold indicates the best performance). Acc denotes model accuracy, and IT denotes inference time.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MA</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">WG</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">UFFT</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IT</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IT</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IT</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">72.23 &#177; 3.36</td><td align="center" valign="middle" rowspan="1" colspan="1">0.52</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.70 &#177; 0.11</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">69.73 &#177; 2.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.66 &#177; 0.12</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">70.02 &#177; 5.22</td><td align="center" valign="middle" rowspan="1" colspan="1">0.58</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.69 &#177; 0.23</bold></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">73.50 &#177; 10.74</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" rowspan="1" colspan="1">1.84 &#177; 0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">71.20 &#177; 7.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41</td><td align="center" valign="middle" rowspan="1" colspan="1">1.71 &#177; 0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">70.28 &#177; 4.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.58</td><td align="center" valign="middle" rowspan="1" colspan="1">1.70 &#177; 0.16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PVTv2-B0</td><td align="center" valign="middle" rowspan="1" colspan="1">78.26 &#177; 6.59</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">2.12 &#177; 0.44</td><td align="center" valign="middle" rowspan="1" colspan="1">72.88 &#177; 6.22</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44</td><td align="center" valign="middle" rowspan="1" colspan="1">1.85 &#177; 0.23</td><td align="center" valign="middle" rowspan="1" colspan="1">72.08 &#177; 4.98</td><td align="center" valign="middle" rowspan="1" colspan="1">0.59</td><td align="center" valign="middle" rowspan="1" colspan="1">1.92 &#177; 0.53</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MLP-MIXer</td><td align="center" valign="middle" rowspan="1" colspan="1">78.73 &#177; 6.76</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">3.96 &#177; 0.24</td><td align="center" valign="middle" rowspan="1" colspan="1">76.44 &#177; 4.36</td><td align="center" valign="middle" rowspan="1" colspan="1">0.58</td><td align="center" valign="middle" rowspan="1" colspan="1">6.54 &#177; 0.68</td><td align="center" valign="middle" rowspan="1" colspan="1">72.51 &#177; 3.46</td><td align="center" valign="middle" rowspan="1" colspan="1">0.53</td><td align="center" valign="middle" rowspan="1" colspan="1">2.12 &#177; 0.44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">fNIRS-T</td><td align="center" valign="middle" rowspan="1" colspan="1">78.96 &#177; 4.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.65</td><td align="center" valign="middle" rowspan="1" colspan="1">1.90 &#177; 0.44</td><td align="center" valign="middle" rowspan="1" colspan="1">76.50 &#177; 4.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">1.75 &#177; 0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">79.21 &#177; 3.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">1.95 &#177; 0.43</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCS-TPNet</td><td align="center" valign="middle" rowspan="1" colspan="1">80.02 &#177; 4.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.60</td><td align="center" valign="middle" rowspan="1" colspan="1">2.03 &#177; 0.53</td><td align="center" valign="middle" rowspan="1" colspan="1">76.36 &#177; 3.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">8.99 &#177; 0.45</td><td align="center" valign="middle" rowspan="1" colspan="1">79.26 &#177; 3.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" rowspan="1" colspan="1">3.68 &#177; 0.82</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>TopoTempNet (Ours)</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>90.04 &#177; 3.53</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.77</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.91 &#177; 0.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>78.33 &#177; 5.42</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.62</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.76 &#177; 0.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>81.66 &#177; 3.23</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.68</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.91 &#177; 0.44</td></tr></tbody></table></table-wrap></p></sec><sec id="sec4dot1dot2-sensors-25-05337"><title>4.1.2. ROC Curve Analysis of Different Models in Multi-Task fNIRS Classification</title><p>In this study, we further analyzed the performance of the TopoTempNet model on three different task datasets. The horizontal axis represents the false positive rate (FPR), and the vertical axis represents the true positive rate (TPR). Different colored curves correspond to different models. The corresponding ROC curves are shown in <xref rid="sensors-25-05337-f004" ref-type="fig">Figure 4</xref>.</p><p><xref rid="sensors-25-05337-f004" ref-type="fig">Figure 4</xref>a: The horizontal axis is the false positive rate (FPR), and the vertical axis is the true positive rate (TPR), with different colors representing various models. TopoTempNet (AUC = 0.817) stands out with its curve closer to the top-left corner, indicating stronger ability to correctly identify true positives while avoiding false positives in distinguishing brain signals related to the mental arithmetic task. Compared to other models, such as FCS-TPNet (AUC = 0.727) and fNIRS-T (AUC = 0.702), whose curves lie relatively lower with smaller AUC values, their performance in recognizing the mental arithmetic state is weaker than TopoTempNet, highlighting TopoTempNet&#8217;s advantage in classifying brain signals for this task.</p><p><xref rid="sensors-25-05337-f004" ref-type="fig">Figure 4</xref>b: TopoTempNet (AUC = 0.769) again shows a curve closer to the top-left corner, with a higher AUC than other models. Models like FCS-TPNet (AUC = 0.764) and fNIRS-T (AUC = 0.734) have curves and AUC values indicating that their balance between correct and incorrect classifications in distinguishing brain signals during the word generation imagination task is inferior to TopoTempNet. This demonstrates TopoTempNet&#8217;s strong performance in classifying language imagination-related brain signals.</p><p><xref rid="sensors-25-05337-f004" ref-type="fig">Figure 4</xref>c: TopoTempNet&#8217;s curve (AUC = 0.731) remains closer to the ideal top-left region across most intervals. Compared to models like fNIRS-T (AUC = 0.714) and FCS-TPNet (AUC = 0.671), their curves and AUC values show that, when handling multi-class motor imagery classification (distinguishing different motor intentions), TopoTempNet better captures complex motor imagery features and achieves more accurate classification, demonstrating a performance advantage in this complex scenario.</p><p>In summary, across all three figures, TopoTempNet&#8217;s ROC curves and AUC values lead the comparisons, indicating that for fNIRS signal classification in MA (mental arithmetic), WG (word generation imagination), and UFFT (motor imagery) tasks, it can more precisely identify target states and deliver superior classification performance compared to other models.</p></sec><sec id="sec4dot1dot3-sensors-25-05337"><title>4.1.3. Subject-Independent Evaluation</title><p>To assess the generalization ability of the TopoTempNet model on unseen subjects, this study adopts the Leave-One-Subject-Out Cross-Validation (LOSO-CV) strategy across all three datasets. In this method, during each iteration, the data from one subject are used as the test set, while the remaining data from all other subjects are used for training. This approach provides a comprehensive evaluation of the model&#8217;s adaptability and stability in handling inter-subject variability.</p><p>The process is repeated until every subject has served once as the test case. The final reported performance metrics are the average results across all subjects, aiming to objectively reflect the decoding performance and generalization ability of the model in cross-subject scenarios. The comparison results are summarized in <xref rid="sensors-25-05337-t002" ref-type="table">Table 2</xref>.</p></sec></sec><sec id="sec4dot2-sensors-25-05337"><title>4.2. Validation of TopoTempNet&#8217;s Capability in Extracting Brain Dynamic Patterns</title><sec id="sec4dot2dot1-sensors-25-05337"><title>4.2.1. t-SNE Visualization of Learned Feature Distributions</title><p>t-SNE (t-distributed Stochastic Neighbor Embedding) [<xref rid="B43-sensors-25-05337" ref-type="bibr">43</xref>] is a widely used nonlinear dimensionality reduction method for visualizing high-dimensional features. It preserves local structures while mapping complex features into a two-dimensional space, effectively illustrating class separability.</p><p><xref rid="sensors-25-05337-f005" ref-type="fig">Figure 5</xref> shows the low-dimensional representations of the MA, WG, and UFFT tasks in both the original feature space and after encoding by TopoTempNet. The results indicate that, in the original feature space, task distributions are scattered with significant class overlap and unclear boundaries. After TopoTempNet encoding, distinct clusters corresponding to different task states emerge clearly in the low-dimensional space, strongly demonstrating TopoTempNet&#8217;s effectiveness in enhancing feature discriminability and extracting key brain dynamic patterns.</p></sec><sec id="sec4dot2dot2-sensors-25-05337"><title>4.2.2. Graph Theory Feature Analysis</title><sec><title>Connectivity Strength Interpretability Analysis</title><p>As shown in the violin plot of connectivity strength (<xref rid="sensors-25-05337-f006" ref-type="fig">Figure 6</xref>), distinct distribution patterns are observed across significant channel pairs in the MA (mental arithmetic), WG (word generation imagination), and UFFT (motor imagery) datasets. In certain channel pairs, the connectivity strength of the MA group is concentrated in the mid-to-high range, whereas the WG group shows a greater spread in the lower range. The UFFT group, on the other hand, exhibits unique distribution patterns in some channel pairs, differing from the other two. These findings suggest that the degree of inter-channel connectivity varies in a task-specific manner, reflecting potentially distinct neural cooperation mechanisms underlying different cognitive processes such as calculation, language generation, and motor imagery.</p></sec><sec><title>Connectivity Density Interpretability Analysis</title><p>The boxplot of connectivity density (<xref rid="sensors-25-05337-f007" ref-type="fig">Figure 7</xref>) reveals the dispersion and central tendency of significant channel pairs across the MA (mental arithmetic), WG (word generation imagination), and UFFT (motor imagery) groups. In certain channel pairs, the MA group shows notable differences in median and interquartile range compared to the WG and UFFT groups, indicating a task-specific pattern of &#8220;connectivity activity frequency&#8221; in the corresponding brain regions during mental arithmetic. Additionally, the distribution of outliers varies among groups&#8212;MA exhibits more extreme high values in some channel pairs, which may suggest that the mental arithmetic task elicits statistically significant, high-frequency atypical connections in specific brain regions. Meanwhile, the density characteristics observed in the WG and UFFT groups also align with their respective task-related neural mechanisms.</p></sec><sec><title>Reciprocal of Functional Signal Mean Difference Interpretability Analysis</title><p>The bar chart with error bars (<xref rid="sensors-25-05337-f008" ref-type="fig">Figure 8</xref>), representing the mean and standard deviation, illustrates clear differences in Reciprocal of Functional Signal Mean Difference (RFSMD) across significant channel pairs for the MA (mental arithmetic), WG (word generation imagination), and UFFT (motor imagery) groups. In certain channel pairs, the MA group shows higher average RFSMD values compared to the others, suggesting more efficient signal transmission and faster neural information integration in the corresponding brain regions during mental arithmetic. The WG group, on the other hand, exhibits lower average RFSMD values in some channel pairs, indicating that information flow may be less efficient under the word generation task, reflecting a distinct neural routing pattern. For the UFFT group, the observed RFSMD features align with the hypothesis that motor imagery requires rapid integration into motor-related regions (evidenced by relatively high RFSMD values in motor cortex-associated channel pairs), but may involve varying transmission efficiencies across different areas (reflected by uneven RFSMD distributions). Additionally, differences in the lengths of error bars between groups reflect variability in neural activity&#8212;shorter error bars in some MA group channel pairs suggest more consistent RFSMD values across individuals, indicating a relatively stable neural activation pattern during mental arithmetic.</p></sec></sec><sec id="sec4dot2dot3-sensors-25-05337"><title>4.2.3. Neurophysiological Plausibility of TopoTempNet Features</title><p>As shown in <xref rid="sensors-25-05337-f009" ref-type="fig">Figure 9</xref>, in the MA dataset (mental arithmetic task), significant channels were primarily distributed across the bilateral prefrontal cortex, encompassing both ventromedial and dorsolateral prefrontal regions (DLPFC). This distribution reflects the reliance of mental arithmetic on working memory, executive control, and numerical cognition. In contrast, in the WG dataset (word generation task), significant channels were concentrated in the right prefrontal cortex, underscoring its role in letter-guided lexical retrieval and semantic selection in coordination with left-hemisphere language areas. Overall, the observed &#8220;prefrontal&#8211;parietal&#8221; co-activation pattern captured by the model is consistent with previously reported task-related activation mechanisms [<xref rid="B44-sensors-25-05337" ref-type="bibr">44</xref>], thereby supporting the neurophysiological plausibility of the topological features extracted by TopoTempNet. The UFFT dataset was not analyzed due to unavailable montage location information.</p></sec></sec><sec id="sec4dot3-sensors-25-05337"><title>4.3. Ablation Analysis</title><sec id="sec4dot3dot1-sensors-25-05337"><title>4.3.1. Module Ablation</title><p>To evaluate the effectiveness of each core module in the TopoTempNet model, ablation experiments were conducted on the three fNIRS datasets (MA, WG, UFFT) to analyze the impact of different submodules on overall performance. The ablation results are shown in <xref rid="sensors-25-05337-t003" ref-type="table">Table 3</xref>, indicating that the graph theory feature module, graph attention mechanism, and bidirectional temporal modeling module each play unique yet indispensable roles across all three fNIRS datasets.</p><p>The fully integrated model combining all three modules achieved the highest accuracies of 90.04 &#177; 3.53, 78.33 &#177; 5.42, and 81.66 &#177; 3.23 on the MA, WG, and UFFT datasets, respectively, highlighting the synergistic benefits among these modules and providing robust support for task recognition and neural dynamic modeling.</p></sec><sec id="sec4dot3dot2-sensors-25-05337"><title>4.3.2. Ablation Study on the Effects of Global Connectivity Thresholds</title><p>The ablation experiments on global connection thresholds revealed task-specific sensitivities in both average efficiency (AE) and classification accuracy (Acc). For the mental arithmetic task, the optimal threshold was 0.3 (AE = 96.98, Acc = 90.04 &#177; 3.53%), with lower thresholds introducing noise (0.1: Acc = 88.43 &#177; 1.62%) and higher thresholds filtering out key links (0.5: Acc = 85.44 &#177; 4.37%). In the motor imagery task, accuracy was stable at low thresholds (0.1&#8211;0.2: 81.23&#8211;81.57%), while AE decreased as thresholds rose, with the best trade-off again at 0.3 (AE = 98.99, Acc = 81.66 &#177; 3.23%). The word generation task showed the highest sensitivity, with 0.3 yielding AE = 76.10 and Acc = 78.33 &#177; 5.42%, but performance fluctuated at lower thresholds (0.2: Acc = 83.33 &#177; 2.63%) and sharply declined at higher ones (0.5: Acc = 66.67 &#177; 8.33%). Overall, the results indicate that mental arithmetic requires precise threshold tuning, motor imagery is more robust, and word generation is highly sensitive, offering task-specific guidance for optimizing connectivity thresholds in BCI applications. The results are summarized in <xref rid="sensors-25-05337-t004" ref-type="table">Table 4</xref>.</p></sec><sec id="sec4dot3dot3-sensors-25-05337"><title>4.3.3. Window Length Sensitivity Analysis</title><p>Although our framework is described as &#8220;dynamic functional connectivity,&#8221; it should be clarified that the adopted features (e.g., Pearson correlation coefficients, global efficiency) are computed within fixed sliding windows, and therefore represent a segmented quasi-static approximation of temporal variations rather than a fully continuous dynamic model as achieved by methods such as sliding-window Hidden Markov Models or time-varying state-space approaches. Our aim is not to infer latent state transitions, but to capture task-relevant fluctuations in functional connectivity within physiologically meaningful temporal segments. To delineate the reasonable boundary of this quasi-dynamic characterization, we conducted a sensitivity analysis on epoch length (<inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and sliding window length (L). The results show that overly short epochs (e.g., 0.5 s) are noise-sensitive, whereas overly long ones (e.g., 3 s) smooth out critical temporal details; an intermediate epoch of 1 s provided the most stable performance across MA (fast-response), WG (variable cognitive-load), and UFFT (sustained-activation) tasks. Similarly, the optimal L range was 0.5&#8211;1 s, as shorter windows (0.2 s) were noise-dominated while longer windows (&gt;1 s) excessively smoothed connectivity features. These findings collectively define the practical dynamic boundary of the proposed model as <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> = 0.5&#8211;2 s and L = 0.5&#8211;1 s, with task-specific sensitivities such that fast-response tasks benefit from shorter segments, whereas sustained-activation tasks tolerate longer ones (see <xref rid="sensors-25-05337-t005" ref-type="table">Table 5</xref>).</p></sec><sec id="sec4dot3dot4-sensors-25-05337"><title>4.3.4. Ablation of Feature Fusion Strategies</title><p>To verify the impact of scale differences in the feature fusion stage on the model&#8217;s classification performance, an ablation study was conducted to compare multiple strategies. <xref rid="sensors-25-05337-t006" ref-type="table">Table 6</xref> presents the classification accuracies (mean &#177; standard deviation) of different fusion methods on the MA, WG, and UFFT datasets. The results show that basic feature concatenation demonstrates a significant advantage: on the MA (90.04 &#177; 3.53), WG (78.33 &#177; 5.42), and UFFT (81.66 &#177; 3.23) datasets, its accuracy consistently outperforms Z-score normalized concatenation, min&#8211;max normalized concatenation, and fixed-weight concatenation. The latter three approaches, due to their forced intervention on the original feature distribution or weights, led to performance degradation on certain datasets (e.g., WG and UFFT). This indicates that the simple concatenation scheme adopted in this study, when combined with the model&#8217;s subsequent dynamic modulation mechanism, can effectively mitigate optimization bias caused by scale differences. By enabling a more natural feature integration, this strategy provides a reference for the design of feature fusion methods&#8212;namely, &#8220;lightweight intervention with reliance on module collaboration.&#8221;</p></sec></sec><sec id="sec4dot4-sensors-25-05337"><title>4.4. Limitations and Future Work</title><p>Although the proposed TopoTempNet model achieved promising decoding performance on multiple public fNIRS datasets (MA, WG, UFFT), several limitations remain. The model&#8217;s applicability is currently restricted to these three task datasets, which provide only objective information (&#916;HbO/&#916;HbR signals, task duration, demographic data) but lack subjective assessments such as NASA-TLX scores, making it difficult to disentangle model adaptability from confounding effects of cognitive workload. Moreover, its effectiveness in more complex motor imagery scenarios or pathological populations (e.g., stroke, ADHD, ASD) has not been validated, limiting its generalizability to clinical rehabilitation. The graph-theoretic feature extraction module also involves high computational complexity, especially for global network measures, which may hinder deployment on low-power portable devices. Finally, interpretability analysis remains at the feature-visualization level without integrating neuroscientific priors to construct a refined brain network interpretation framework. Future research should therefore broaden validation to diverse tasks and clinical populations with both objective and subjective measures, adopt lightweight strategies such as knowledge distillation or sparse graph structures to reduce computational cost and enhance real-time applicability, and establish a multi-scale interpretability system linking features, brain regions, and functions to improve clinical reliability and translational potential.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05337"><title>5. Conclusions</title><p>The TopoTempNet model integrates graph-theoretic topology with hybrid temporal modeling, achieving leading decoding accuracy and strong interpretability across multiple fNIRS tasks. Experiments show that TopoTempNet significantly outperforms mainstream models such as CNN and LSTM on the MA, WG, and UFFT datasets, with subject-dependent accuracies of 90.04%, 78.33%, and 81.66%, and subject-independent accuracies of 81.06%, 78.68%, and 80.18%, demonstrating good generalization ability and stability. Kappa coefficients and AUC further confirm the model&#8217;s discriminative power and robustness.</p><p>Interpretability analysis reveals that TopoTempNet precisely captures dynamic brain features. Graph theory features significantly highlight task-specific functional connectivity patterns: increased connectivity strength in calculation-related brain regions during the MA task, enhanced connectivity density in language-related regions during the WG task, and shortened RFSMD in motor cortex during the UFFT task, reflecting task-induced brain network cooperation and optimized information transmission efficiency. These differences provide neural mechanism support for the model&#8217;s performance.</p><p>In summary, TopoTempNet excels in decoding accuracy, feature discriminability, and neural mechanism interpretation, offering strong technical support for fNIRS-BCI applications in cognitive modeling, disease diagnosis, and rehabilitation training.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, H.Y.; Formal analysis, Y.S. and L.S.; Investigation, Z.S.; Writing&#8212;review &amp; editing, Q.H. and J.Z.; Supervision, Z.K. All authors have read and agreed to the published version of the manuscript. </p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets used in this study are publicly available in multiple repositories. These include the Open access dataset for simultaneous EEG and NIRS brain-computerinterface (BCI) (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doc.ml.tu-berlin.de/hBCI/">https://doc.ml.tu-berlin.de/hBCI/</uri>). Open access NIRS dataset for classification of the unilateral finger and foot-tapping <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://figshare.com/articles/dataset/Open_accessfNIRS_dataset_for_classification_of_the_unilateral_finger_and_foottapping/9783755/1">https://figshare.com/articles/dataset/Open_accessfNIRS_dataset_for_classification_of_the_unilateral_finger_and_foottapping/9783755/1</uri> and Simultaneous acquisition of EEG and NIRS during cognitive tasks for an Open access dataset (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doc.ml.tu-berlin.de/simultaneous_EEG_NIRS/">https://doc.ml.tu-berlin.de/simultaneous_EEG_NIRS/</uri>).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05337"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>T.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>A.S.</given-names></name></person-group><article-title>fNIRS as a biomarker for individuals with subjective memory complaints and MCI</article-title><source>Alzheimer&#8217;s Dement.</source><year>2024</year><volume>20</volume><fpage>5170</fpage><lpage>5182</lpage><pub-id pub-id-type="doi">10.1002/alz.13897</pub-id><pub-id pub-id-type="pmid">38837656</pub-id><pub-id pub-id-type="pmcid">PMC11350052</pub-id></element-citation></ref><ref id="B2-sensors-25-05337"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Si</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ming</surname><given-names>D.</given-names></name></person-group><article-title>The cortical spatial responses and decoding of emotion imagery towards a novel fNIRS-based affective BCI</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2025</year><volume>33</volume><fpage>2577</fpage><lpage>2586</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2025.3584765</pub-id><pub-id pub-id-type="pmid">40601441</pub-id></element-citation></ref><ref id="B3-sensors-25-05337"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name></person-group><article-title>Brain Activation During Motor Preparation and Execution in Patients with Mild Cognitive Impairment: An fNIRS Study</article-title><source>Brain Sci.</source><year>2025</year><volume>15</volume><elocation-id>333</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci15040333</pub-id><pub-id pub-id-type="pmid">40338241</pub-id><pub-id pub-id-type="pmcid">PMC12025269</pub-id></element-citation></ref><ref id="B4-sensors-25-05337"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>A.W.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.B.</given-names></name><name name-style="western"><surname>Kan</surname><given-names>R.L.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>T.T.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>P.P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chau</surname><given-names>W.M.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>N.M.</given-names></name><name name-style="western"><surname>Kannan</surname><given-names>P.</given-names></name><etal/></person-group><article-title>Investigating the hemodynamic response to iTBS of the left DLPFC: A concurrent iTBS/fNIRS study</article-title><source>Brain Stimul.</source><year>2025</year><volume>18</volume><fpage>235</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.brs.2025.02.008</pub-id><pub-id pub-id-type="pmid">39955026</pub-id></element-citation></ref><ref id="B5-sensors-25-05337"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>X.</given-names></name></person-group><article-title>STA-Net: Spatial&#8211;temporal alignment network for hybrid EEG-fNIRS decoding</article-title><source>Inf. Fusion</source><year>2025</year><volume>119</volume><fpage>103023</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2025.103023</pub-id></element-citation></ref><ref id="B6-sensors-25-05337"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name></person-group><article-title>Emotion Recognition Based on a EEG&#8211;fNIRS Hybrid Brain Network in the Source Space</article-title><source>Brain Sci.</source><year>2024</year><volume>14</volume><elocation-id>1166</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci14121166</pub-id><pub-id pub-id-type="pmid">39766365</pub-id><pub-id pub-id-type="pmcid">PMC11674611</pub-id></element-citation></ref><ref id="B7-sensors-25-05337"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arif</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Helmy</surname><given-names>A.</given-names></name></person-group><article-title>EF-Net: Mental state recognition by analyzing multimodal EEG-fNIRS via CNN</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>1889</elocation-id><pub-id pub-id-type="doi">10.3390/s24061889</pub-id><pub-id pub-id-type="pmid">38544152</pub-id><pub-id pub-id-type="pmcid">PMC10974548</pub-id></element-citation></ref><ref id="B8-sensors-25-05337"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Al Fahoum</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zyout</surname><given-names>A.</given-names></name></person-group><article-title>Wavelet transform, reconstructed phase space, and deep learning neural networks for EEG-based schizophrenia detection</article-title><source>Int. J. Neural Syst.</source><year>2024</year><volume>34</volume><fpage>2450046</fpage><pub-id pub-id-type="doi">10.1142/S0129065724500461</pub-id><pub-id pub-id-type="pmid">39010724</pub-id></element-citation></ref><ref id="B9-sensors-25-05337"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dash</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dash</surname><given-names>D.K.</given-names></name><name name-style="western"><surname>Tripathy</surname><given-names>R.K.</given-names></name><name name-style="western"><surname>Pachori</surname><given-names>R.B.</given-names></name></person-group><article-title>Time&#8211;frequency domain machine learning for detection of epilepsy using wearable EEG sensor signals recorded during physical activities</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>100</volume><elocation-id>107041</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2024.107041</pub-id></element-citation></ref><ref id="B10-sensors-25-05337"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>R.</given-names></name><name name-style="western"><surname>She</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Neurovascular coupling analysis based on multivariate variational Gaussian process convergent cross-mapping</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2024</year><volume>32</volume><fpage>1873</fpage><lpage>1883</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2024.3398662</pub-id><pub-id pub-id-type="pmid">38717876</pub-id></element-citation></ref><ref id="B11-sensors-25-05337"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cooney</surname><given-names>C.</given-names></name><name name-style="western"><surname>Folli</surname><given-names>R.</given-names></name><name name-style="western"><surname>Coyle</surname><given-names>D.</given-names></name></person-group><article-title>A bimodal deep learning architecture for EEG-fNIRS decoding of overt and imagined speech</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2021</year><volume>69</volume><fpage>1983</fpage><lpage>1994</lpage><pub-id pub-id-type="doi">10.1109/TBME.2021.3132861</pub-id><pub-id pub-id-type="pmid">34874850</pub-id></element-citation></ref><ref id="B12-sensors-25-05337"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rui</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>Z.</given-names></name></person-group><article-title>EEG, EOG, Likert scale, and interview approaches for assessing stressful hazard perception scenarios</article-title><source>Int. J. Hum.-Comput. Interact.</source><year>2025</year><volume>41</volume><fpage>5199</fpage><lpage>5224</lpage><pub-id pub-id-type="doi">10.1080/10447318.2024.2358461</pub-id></element-citation></ref><ref id="B13-sensors-25-05337"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>L&#233;vi-Strauss</surname><given-names>J.</given-names></name><name name-style="western"><surname>Marois</surname><given-names>C.</given-names></name><name name-style="western"><surname>Worbe</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bedoucha</surname><given-names>L.</given-names></name><name name-style="western"><surname>Benchikh Lehocine</surname><given-names>R.</given-names></name><name name-style="western"><surname>Rohaut</surname><given-names>B.</given-names></name><name name-style="western"><surname>Weiss</surname><given-names>N.</given-names></name><name name-style="western"><surname>Demeret</surname><given-names>S.</given-names></name><name name-style="western"><surname>Apartis</surname><given-names>E.</given-names></name><name name-style="western"><surname>Lambrecq</surname><given-names>V.</given-names></name></person-group><article-title>Utility and value of movement recording with combined EEG-EMG monitoring in the intensive care unit</article-title><source>Neurocrit. Care</source><year>2025</year><volume>43</volume><fpage>333</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1007/s12028-025-02230-3</pub-id><pub-id pub-id-type="pmid">40032771</pub-id></element-citation></ref><ref id="B14-sensors-25-05337"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baghdadi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hadaeghi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Kamarajan</surname><given-names>C.</given-names></name></person-group><article-title>Multimodal approaches to investigating neural dynamics in cognition and related clinical conditions: Integrating EEG, MEG, and fMRI data</article-title><source>Front. Syst. Neurosci.</source><year>2025</year><volume>19</volume><elocation-id>1495018</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2025.1495018</pub-id><pub-id pub-id-type="pmid">40012906</pub-id><pub-id pub-id-type="pmcid">PMC11850518</pub-id></element-citation></ref><ref id="B15-sensors-25-05337"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Electroencephalography-based assessment of worker vigilance for evaluating safety interventions in construction</article-title><source>Adv. Eng. Inform.</source><year>2025</year><volume>64</volume><fpage>102973</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2024.102973</pub-id></element-citation></ref><ref id="B16-sensors-25-05337"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>The functional near infrared spectroscopy applications in children with developmental diseases: A review</article-title><source>Front. Neurol.</source><year>2025</year><volume>16</volume><elocation-id>1495138</elocation-id><pub-id pub-id-type="doi">10.3389/fneur.2025.1495138</pub-id><pub-id pub-id-type="pmid">40599736</pub-id><pub-id pub-id-type="pmcid">PMC12209276</pub-id></element-citation></ref><ref id="B17-sensors-25-05337"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hui</surname><given-names>P.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qie</surname><given-names>S.</given-names></name></person-group><article-title>Exploring the application and challenges of fNIRS technology in early detection of Parkinson&#8217;s disease</article-title><source>Front. Aging Neurosci.</source><year>2024</year><volume>16</volume><elocation-id>1354147</elocation-id><pub-id pub-id-type="doi">10.3389/fnagi.2024.1354147</pub-id><pub-id pub-id-type="pmid">38524116</pub-id><pub-id pub-id-type="pmcid">PMC10957543</pub-id></element-citation></ref><ref id="B18-sensors-25-05337"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Khadka</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sultan</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Yazidi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ombao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mirtaheri</surname><given-names>P.</given-names></name></person-group><article-title>Unleashing the potential of fNIRS with machine learning: Classification of fine anatomical movements to empower future brain-computer interface</article-title><source>Front. Hum. Neurosci.</source><year>2024</year><volume>18</volume><elocation-id>1354143</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2024.1354143</pub-id><pub-id pub-id-type="pmid">38435744</pub-id><pub-id pub-id-type="pmcid">PMC10904609</pub-id></element-citation></ref><ref id="B19-sensors-25-05337"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maniruzzaman</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hirooka</surname><given-names>K.</given-names></name><name name-style="western"><surname>Tomioka</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hasan</surname><given-names>M.A.M.</given-names></name><name name-style="western"><surname>Hwang</surname><given-names>Y.S.</given-names></name><name name-style="western"><surname>Megumi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yasumura</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shin</surname><given-names>J.</given-names></name></person-group><article-title>Machine Learning-Based ADHD Detection From fNIRs Signal During Reverse Stroop Tasks</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>82984</fpage><lpage>82995</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3411558</pub-id></element-citation></ref><ref id="B20-sensors-25-05337"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fernandez Rojas</surname><given-names>R.</given-names></name><name name-style="western"><surname>Joseph</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bargshady</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ou</surname><given-names>K.-L.</given-names></name></person-group><article-title>Empirical comparison of deep learning models for fNIRS pain decoding</article-title><source>Front. Neuroinform.</source><year>2024</year><volume>18</volume><elocation-id>1320189</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2024.1320189</pub-id><pub-id pub-id-type="pmid">38420133</pub-id><pub-id pub-id-type="pmcid">PMC10899478</pub-id></element-citation></ref><ref id="B21-sensors-25-05337"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Akila</surname><given-names>V.</given-names></name><name name-style="western"><surname>Johnvictor</surname><given-names>A.C.</given-names></name></person-group><article-title>Functional near infrared spectroscopy for brain functional connectivity analysis: A graph theoretic approach</article-title><source>Heliyon</source><year>2023</year><volume>9</volume><fpage>e15002</fpage><pub-id pub-id-type="doi">10.1016/j.heliyon.2023.e15002</pub-id><pub-id pub-id-type="pmid">37082646</pub-id><pub-id pub-id-type="pmcid">PMC10112026</pub-id></element-citation></ref><ref id="B22-sensors-25-05337"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Rethinking delayed hemodynamic responses for fNIRS classification</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2023</year><volume>31</volume><fpage>4528</fpage><lpage>4538</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2023.3330911</pub-id><pub-id pub-id-type="pmid">37934649</pub-id></element-citation></ref><ref id="B23-sensors-25-05337"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name></person-group><article-title>An fNIRS representation and fNIRS-scales multimodal fusion method for auxiliary diagnosis of amnestic mild cognitive impairment</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>96</volume><elocation-id>106646</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2024.106646</pub-id></element-citation></ref><ref id="B24-sensors-25-05337"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Seo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>K.-S.</given-names></name></person-group><article-title>Multi-Class fNIRS Classification Using an Ensemble of GNN-Based Models</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>137606</fpage><lpage>137620</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3339647</pub-id></element-citation></ref><ref id="B25-sensors-25-05337"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name></person-group><article-title>GNN-based depression recognition using spatio-temporal information: A fNIRS study</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2022</year><volume>26</volume><fpage>4925</fpage><lpage>4935</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2022.3195066</pub-id><pub-id pub-id-type="pmid">35905061</pub-id></element-citation></ref><ref id="B26-sensors-25-05337"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name></person-group><article-title>fNIRS-Driven Depression Recognition Based on Cross-Modal Data Augmentation</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2024</year><volume>32</volume><fpage>2688</fpage><lpage>2698</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2024.3429337</pub-id><pub-id pub-id-type="pmid">39012734</pub-id></element-citation></ref><ref id="B27-sensors-25-05337"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Zahour</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tariq</surname><given-names>U.</given-names></name><name name-style="western"><surname>Masri</surname><given-names>G.</given-names></name><name name-style="western"><surname>Almadani</surname><given-names>I.F.</given-names></name><name name-style="western"><surname>Al-Nashah</surname><given-names>H.</given-names></name></person-group><article-title>Exploring Effects of Mental Stress with Data Augmentation and Classification Using fNIRS</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>428</elocation-id><pub-id pub-id-type="doi">10.3390/s25020428</pub-id><pub-id pub-id-type="pmid">39860797</pub-id><pub-id pub-id-type="pmcid">PMC11768738</pub-id></element-citation></ref><ref id="B28-sensors-25-05337"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hirshfield</surname><given-names>L.M.</given-names></name><name name-style="western"><surname>Wickens</surname><given-names>C.</given-names></name><name name-style="western"><surname>Doherty</surname><given-names>E.</given-names></name><name name-style="western"><surname>Spencer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Williams</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hayne</surname><given-names>L.</given-names></name></person-group><article-title>Toward workload-based adaptive automation: The utility of fNIRS for measuring load in multiple resources in the brain</article-title><source>Int. J. Hum.-Comput. Interact.</source><year>2024</year><volume>40</volume><fpage>7404</fpage><lpage>7430</lpage><pub-id pub-id-type="doi">10.1080/10447318.2023.2266242</pub-id></element-citation></ref><ref id="B29-sensors-25-05337"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>EEG&#8211;fNIRS-Based emotion recognition using graph convolution and capsule attention network</article-title><source>Brain Sci.</source><year>2024</year><volume>14</volume><elocation-id>820</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci14080820</pub-id><pub-id pub-id-type="pmid">39199511</pub-id><pub-id pub-id-type="pmcid">PMC11352237</pub-id></element-citation></ref><ref id="B30-sensors-25-05337"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name></person-group><article-title>Graph theory methods: Applications in brain networks</article-title><source>Dialogues Clin. Neurosci.</source><year>2018</year><volume>20</volume><fpage>111</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.31887/DCNS.2018.20.2/osporns</pub-id><pub-id pub-id-type="pmid">30250388</pub-id><pub-id pub-id-type="pmcid">PMC6136126</pub-id></element-citation></ref><ref id="B31-sensors-25-05337"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>S.</given-names></name></person-group><article-title>Exploring Graph Theory Mechanisms of Fluid Intelligence in the DLPFC: Insights From Resting-State fNIRS Across Various Time Windows</article-title><source>Brain Behav.</source><year>2025</year><volume>15</volume><fpage>e70386</fpage><pub-id pub-id-type="doi">10.1002/brb3.70386</pub-id><pub-id pub-id-type="pmid">40022279</pub-id><pub-id pub-id-type="pmcid">PMC11870832</pub-id></element-citation></ref><ref id="B32-sensors-25-05337"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shin</surname><given-names>J.</given-names></name><name name-style="western"><surname>von L&#252;hmann</surname><given-names>A.</given-names></name><name name-style="western"><surname>Blankertz</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>D.-W.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hwang</surname><given-names>H.-J.</given-names></name><name name-style="western"><surname>M&#252;ller</surname><given-names>K.-R.</given-names></name></person-group><article-title>Open access dataset for EEG+ NIRS single-trial classification</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2016</year><volume>25</volume><fpage>1735</fpage><lpage>1745</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2016.2628057</pub-id><pub-id pub-id-type="pmid">27849545</pub-id></element-citation></ref><ref id="B33-sensors-25-05337"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Von L&#252;hmann</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>D.-W.</given-names></name><name name-style="western"><surname>Mehnert</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hwang</surname><given-names>H.-J.</given-names></name><name name-style="western"><surname>M&#252;ller</surname><given-names>K.-R.</given-names></name></person-group><article-title>Simultaneous acquisition of EEG and NIRS during cognitive tasks for an open access dataset</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>180003</fpage><pub-id pub-id-type="doi">10.1038/sdata.2018.3</pub-id><pub-id pub-id-type="pmid">29437166</pub-id><pub-id pub-id-type="pmcid">PMC5810421</pub-id></element-citation></ref><ref id="B34-sensors-25-05337"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bak</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>J.</given-names></name></person-group><article-title>Open-access fNIRS dataset for classification of unilateral finger-and foot-tapping</article-title><source>Electronics</source><year>2019</year><volume>8</volume><elocation-id>1486</elocation-id><pub-id pub-id-type="doi">10.3390/electronics8121486</pub-id></element-citation></ref><ref id="B35-sensors-25-05337"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cope</surname><given-names>M.</given-names></name><name name-style="western"><surname>Delpy</surname><given-names>D.T.</given-names></name></person-group><article-title>System for long-term measurement of cerebral blood and tissue oxygenation on newborn infants by near infra-red transillumination</article-title><source>Med. Biol. Eng. Comput.</source><year>1988</year><volume>26</volume><fpage>289</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1007/BF02447083</pub-id><pub-id pub-id-type="pmid">2855531</pub-id></element-citation></ref><ref id="B36-sensors-25-05337"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.03762</pub-id></element-citation></ref><ref id="B37-sensors-25-05337"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Loshchilov</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hutter</surname><given-names>F.</given-names></name></person-group><article-title>Decoupled weight decay regularization</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1711.05101</pub-id></element-citation></ref><ref id="B38-sensors-25-05337"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thakur</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sinha</surname><given-names>D.K.</given-names></name><name name-style="western"><surname>Mishra</surname><given-names>K.K.</given-names></name><name name-style="western"><surname>Venkatesan</surname><given-names>V.K.</given-names></name><name name-style="western"><surname>Guluwadi</surname><given-names>S.</given-names></name></person-group><article-title>Transformative breast Cancer diagnosis using CNNs with optimized ReduceLROnPlateau and Early stopping Enhancements</article-title><source>Int. J. Comput. Intell. Syst.</source><year>2024</year><volume>17</volume><fpage>14</fpage></element-citation></ref><ref id="B39-sensors-25-05337"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lyu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Pham</surname><given-names>T.</given-names></name><name name-style="western"><surname>Blaney</surname><given-names>G.</given-names></name><name name-style="western"><surname>Haga</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sassaroli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fantini</surname><given-names>S.</given-names></name><name name-style="western"><surname>Aeron</surname><given-names>S.</given-names></name></person-group><article-title>Domain adaptation for robust workload level alignment between sessions and subjects using fNIRS</article-title><source>J. Biomed. Opt.</source><year>2021</year><volume>26</volume><fpage>022908</fpage><pub-id pub-id-type="doi">10.1117/1.JBO.26.2.022908</pub-id><pub-id pub-id-type="pmid">33415849</pub-id><pub-id pub-id-type="pmcid">PMC7790507</pub-id></element-citation></ref><ref id="B40-sensors-25-05337"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name></person-group><article-title>A general and scalable vision framework for functional near-infrared spectroscopy classification</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2022</year><volume>30</volume><fpage>1982</fpage><lpage>1991</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2022.3190431</pub-id><pub-id pub-id-type="pmid">35830404</pub-id></element-citation></ref><ref id="B41-sensors-25-05337"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name></person-group><article-title>Transformer model for functional near-infrared spectroscopy classification</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2022</year><volume>26</volume><fpage>2559</fpage><lpage>2569</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2022.3140531</pub-id><pub-id pub-id-type="pmid">34986110</pub-id></element-citation></ref><ref id="B42-sensors-25-05337"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name></person-group><article-title>FCS-TPNet: Fusion of fNIRS chromophore signals to construct temporal-spatial graph representation for topological networks</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>104</volume><elocation-id>107528</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2025.107528</pub-id></element-citation></ref><ref id="B43-sensors-25-05337"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van der Maaten</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Visualizing data using t-SNE</article-title><source>J. Mach. Learn. Res.</source><year>2008</year><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="B44-sensors-25-05337"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bauernfeind</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wriessnegger</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Neuper</surname><given-names>C.</given-names></name></person-group><article-title>Focal frontal (de) oxyhemoglobin responses during simple arithmetic</article-title><source>Int. J. Psychophysiol.</source><year>2010</year><volume>76</volume><fpage>186</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2010.03.013</pub-id><pub-id pub-id-type="pmid">20381546</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05337-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall architecture of the model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g001.jpg"/></fig><fig position="float" id="sensors-25-05337-f002" orientation="portrait"><label>Figure 2</label><caption><p>LSTM module structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g002.jpg"/></fig><fig position="float" id="sensors-25-05337-f003" orientation="portrait"><label>Figure 3</label><caption><p>Average confusion matrices of the proposed TopoTempNet model: (<bold>a</bold>) MA dataset, (<bold>b</bold>) WG dataset, and (<bold>c</bold>) UFFT dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g003.jpg"/></fig><fig position="float" id="sensors-25-05337-f004" orientation="portrait"><label>Figure 4</label><caption><p>Average ROC curves and corresponding AUC values of different models across the three datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g004.jpg"/></fig><fig position="float" id="sensors-25-05337-f005" orientation="portrait"><label>Figure 5</label><caption><p>t-SNE visualizations of different datasets: (<bold>a</bold>) MA dataset, (<bold>b</bold>) WG dataset, and (<bold>c</bold>) UFFT dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g005.jpg"/></fig><fig position="float" id="sensors-25-05337-f006" orientation="portrait"><label>Figure 6</label><caption><p>Statistical significance analysis of local channel connection strength on different datasets: (<bold>a</bold>) MA dataset, (<bold>b</bold>) WG dataset, and (<bold>c</bold>) UFFT dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g006.jpg"/></fig><fig position="float" id="sensors-25-05337-f007" orientation="portrait"><label>Figure 7</label><caption><p>Statistical significance analysis of local channel connection density on different datasets, Circles represent outlier data points, defined as values exceeding 1.5 times the interquartile range (IQR) from the box edges in boxplot visualization.: (<bold>a</bold>) MA dataset, (<bold>b</bold>) WG dataset, and (<bold>c</bold>) UFFT dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g007.jpg"/></fig><fig position="float" id="sensors-25-05337-f008" orientation="portrait"><label>Figure 8</label><caption><p>Statistical significance analysis of local channel characteristic RFSMD on different datasets: (<bold>a</bold>) MA dataset, (<bold>b</bold>) WG dataset, and (<bold>c</bold>) UFFT dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g008.jpg"/></fig><fig position="float" id="sensors-25-05337-f009" orientation="portrait"><label>Figure 9</label><caption><p>MA (<bold>a</bold>) and WG (<bold>b</bold>) dataset heatmaps, with red channels indicating significant connections (<italic toggle="yes">p</italic> &lt; 0.05).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05337-g009.jpg"/></fig><table-wrap position="float" id="sensors-25-05337-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05337-t002_Table 2</object-id><label>Table 2</label><caption><p>LOSO-CV results for the three datasets. Bold values indicate the best performance for each evaluation metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MA</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">WG</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">UFFT</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy%</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy%</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy%</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">70.14 &#177; 6.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">63.22 &#177; 7.30</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32</td><td align="center" valign="middle" rowspan="1" colspan="1">66.16 &#177; 4.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">72.17 &#177; 4.94</td><td align="center" valign="middle" rowspan="1" colspan="1">0.51</td><td align="center" valign="middle" rowspan="1" colspan="1">67.06 &#177; 4.98</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">66.67 &#177; 3.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PVTv2-B0</td><td align="center" valign="middle" rowspan="1" colspan="1">76.43 &#177; 4.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.55</td><td align="center" valign="middle" rowspan="1" colspan="1">68.88 &#177; 1.92</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">67.02 &#177; 4.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MLP-MIXer</td><td align="center" valign="middle" rowspan="1" colspan="1">74.28 &#177; 4.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.56</td><td align="center" valign="middle" rowspan="1" colspan="1">69.90 &#177; 7.20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">68.66 &#177; 3.19</td><td align="center" valign="middle" rowspan="1" colspan="1">0.48</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">fNIRS-T</td><td align="center" valign="middle" rowspan="1" colspan="1">72.46 &#177; 6.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.51</td><td align="center" valign="middle" rowspan="1" colspan="1">71.26 &#177; 4.78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">72.38 &#177; 4.96</td><td align="center" valign="middle" rowspan="1" colspan="1">0.53</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCS-TPNet</td><td align="center" valign="middle" rowspan="1" colspan="1">79.26 &#177; 5.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.63</td><td align="center" valign="middle" rowspan="1" colspan="1">78.22 &#177; 4.22</td><td align="center" valign="middle" rowspan="1" colspan="1">0.56</td><td align="center" valign="middle" rowspan="1" colspan="1">78.69 &#177; 5.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.56</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>TopoTempNet (Ours)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>81.06 &#177; 4.38</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.64</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>78.68 &#177; 4.18</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.56</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>80.18 &#177; 2.37</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.57</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05337-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05337-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation analysis of key modules in the TopoTempNet model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Graph-Theoretic Feature </th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GAM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bi-LSTM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy%</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MA<break/>WG<break/>UFFT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.06 &#177; 4.38<break/>72.35 &#177; 6.22<break/>73.08 &#177; 2.67</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MA<break/>WG<break/>UFFT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.91 &#177; 3.69<break/>77.62 &#177; 2.94<break/>81.43 &#177; 2.44</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MA<break/>WG<break/>UFFT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.86 &#177; 3.23<break/>64.43 &#177; 2.19<break/>69.43 &#177; 3.72</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MA<break/>WG<break/>UFFT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.02 &#177; 9.66<break/>58.34 &#177; 12.73<break/>61.74 &#177; 11.38</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MA<break/>WG<break/>UFFT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.04 &#177; 3.53<break/>78.33 &#177; 5.42<break/>81.66 &#177; 3.23</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05337-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05337-t004_Table 4</object-id><label>Table 4</label><caption><p>Summary of ablation results on global connection thresholds across different cognitive tasks, where AE denotes the Average Functional Connectivity Efficiency and Acc denotes the classification accuracy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Threshold</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MA</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">WG</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">UFFT</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" rowspan="1" colspan="1">99.03</td><td align="center" valign="middle" rowspan="1" colspan="1">91.58</td><td align="center" valign="middle" rowspan="1" colspan="1">91.58</td><td align="center" valign="middle" rowspan="1" colspan="1">81.23 &#177; 2.66</td><td align="center" valign="middle" rowspan="1" colspan="1">99.67</td><td align="center" valign="middle" rowspan="1" colspan="1">81.28 &#177; 2.33</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.2</td><td align="center" valign="middle" rowspan="1" colspan="1">98.03</td><td align="center" valign="middle" rowspan="1" colspan="1">83.51</td><td align="center" valign="middle" rowspan="1" colspan="1">83.51</td><td align="center" valign="middle" rowspan="1" colspan="1">81.57 &#177; 1.68</td><td align="center" valign="middle" rowspan="1" colspan="1">99.33</td><td align="center" valign="middle" rowspan="1" colspan="1">81.23 &#177; 2.66</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.3</td><td align="center" valign="middle" rowspan="1" colspan="1">96.98</td><td align="center" valign="middle" rowspan="1" colspan="1">76.10</td><td align="center" valign="middle" rowspan="1" colspan="1">76.10</td><td align="center" valign="middle" rowspan="1" colspan="1">81.66 &#177; 3.23</td><td align="center" valign="middle" rowspan="1" colspan="1">98.99</td><td align="center" valign="middle" rowspan="1" colspan="1">81.57 &#177; 1.68</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.4</td><td align="center" valign="middle" rowspan="1" colspan="1">92.99</td><td align="center" valign="middle" rowspan="1" colspan="1">69.35</td><td align="center" valign="middle" rowspan="1" colspan="1">69.35</td><td align="center" valign="middle" rowspan="1" colspan="1">78.59 &#177; 4.36</td><td align="center" valign="middle" rowspan="1" colspan="1">97.26</td><td align="center" valign="middle" rowspan="1" colspan="1">81.66 &#177; 3.23</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.61 &#177; 3.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.59 &#177; 4.36</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05337-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05337-t005_Table 5</object-id><label>Table 5</label><caption><p>Sensitivity analysis of window lengths: classification accuracy (Acc) under different epoch lengths (<inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and sliding window lengths (L).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
<inline-formula>
<mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MA</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">WG</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">UFFT</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L/s</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L/s</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L/s</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">86.21 &#177; 4.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">75.88 &#177; 5.19</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">80.12 &#177; 3.57</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">90.04 &#177; 3.53</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">78.33 &#177; 5.42</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">81.66 &#177; 3.23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">87.53 &#177; 3.89</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">74.62 &#177; 5.83</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">80.92 &#177; 2.98</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.36 &#177; 4.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.15 &#177; 6.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.35 &#177; 3.61</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05337-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05337-t006_Table 6</object-id><label>Table 6</label><caption><p>Ablation results of feature fusion strategies: comparison of classification accuracy under different scale processing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">WG</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UFFT</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Feature Concatenation</td><td align="center" valign="middle" rowspan="1" colspan="1">90.04 &#177; 3.53</td><td align="center" valign="middle" rowspan="1" colspan="1">78.33 &#177; 5.42</td><td align="center" valign="middle" rowspan="1" colspan="1">81.66 &#177; 3.23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Z-Score Normalized Concatenation</td><td align="center" valign="middle" rowspan="1" colspan="1">88.86 &#177; 4.25</td><td align="center" valign="middle" rowspan="1" colspan="1">78.21 &#177; 3.41</td><td align="center" valign="middle" rowspan="1" colspan="1">77.42 &#177; 3.29</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Min&#8211;Max Normalized Concatenation</td><td align="center" valign="middle" rowspan="1" colspan="1">88.13 &#177; 4.28</td><td align="center" valign="middle" rowspan="1" colspan="1">72.43 &#177; 5.37</td><td align="center" valign="middle" rowspan="1" colspan="1">76.48 &#177; 4.73</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed Weighted Concatenation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.49 &#177; 2.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.98 &#177; 6.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.16 &#177; 3.45</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>