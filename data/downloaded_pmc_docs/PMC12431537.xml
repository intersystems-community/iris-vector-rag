<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431537</article-id><article-id pub-id-type="pmcid-ver">PMC12431537.1</article-id><article-id pub-id-type="pmcaid">12431537</article-id><article-id pub-id-type="pmcaiid">12431537</article-id><article-id pub-id-type="doi">10.3390/s25175472</article-id><article-id pub-id-type="publisher-id">sensors-25-05472</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dynamic Vision-Based Non-Contact Rotating Machine Fault Diagnosis with EViT</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Jin</surname><given-names initials="Z">Zhenning</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05472" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Sun</surname><given-names initials="C">Cuiying</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af2-sensors-25-05472" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0569-2176</contrib-id><name name-style="western"><surname>Li</surname><given-names initials="X">Xiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-05472" ref-type="aff">1</xref><xref rid="c1-sensors-25-05472" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Xiang</surname><given-names initials="J">Jiawei</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05472"><label>1</label>Key Laboratory of Education Ministry for Modern Design and Rotor-Bearing System, Xi&#8217;an Jiaotong University, Xi&#8217;an 710049, China</aff><aff id="af2-sensors-25-05472"><label>2</label>State Key Laboratory of Engine and Powertrain System, Weichai Power Co., Ltd., Weifang 261061, China; <email>suncuiy@weichai.com</email></aff><author-notes><corresp id="c1-sensors-25-05472"><label>*</label>Correspondence: <email>lixiang@xjtu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>03</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5472</elocation-id><history><date date-type="received"><day>30</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>02</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>03</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05472.pdf"/><abstract><p>Event-based cameras, as a revolutionary class of dynamic vision sensors, offer transformative advantages for capturing transient mechanical phenomena through their asynchronous, per-pixel brightness change detection mechanism. These neuromorphic sensors excel in challenging industrial environments with their microsecond-level temporal resolution, ultra-low power requirements, and exceptional dynamic range that significantly outperform conventional imaging systems. In this way, the event-based camera provides a promising tool for machine vibration sensing and fault diagnosis. However, the dynamic vision data from the event-based cameras have a complex structure, which cannot be directly processed by the mainstream methods. This paper proposes a dynamic vision-based non-contact machine fault diagnosis method. The Eagle Vision Transformer (EViT) architecture is proposed, which incorporates biologically plausible computational mechanisms through its innovative Bi-Fovea Self-Attention and Bi-Fovea Feedforward Network designs. The proposed method introduces an original computational framework that effectively processes asynchronous event streams while preserving their inherent temporal precision and dynamic response characteristics. The proposed methodology demonstrates exceptional fault diagnosis performance across diverse operational scenarios through its unique combination of multi-scale spatiotemporal feature analysis, adaptive learning capabilities, and transparent decision pathways. The effectiveness of the proposed method is extensively validated by the practical condition monitoring data of rotating machines. By successfully bridging cutting-edge bio-inspired vision processing with practical industrial monitoring requirements, this work creates a new paradigm for dynamic vision-based non-contact machinery fault diagnosis that addresses critical limitations of conventional approaches. The proposed method provides new insights for predictive maintenance applications in smart manufacturing environments.</p></abstract><kwd-group><kwd>eagle vision transformer</kwd><kwd>deep learning</kwd><kwd>dynamic vision</kwd><kwd>event-based camera</kwd><kwd>fault diagnosis</kwd></kwd-group><funding-group><award-group><funding-source>Aviation Science Foundation</funding-source><award-id>2024Z071070001</award-id></award-group><funding-statement>The material in this paper is based on work supported by Aviation Science Foundation under Grant 2024Z071070001.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05472"><title>1. Introduction</title><p>Over the past few decades, intelligent machinery fault diagnosis methods have garnered considerable attention. Driven by the swift progress in artificial intelligence technologies, data-intensive fault diagnosis approaches have made remarkable achievements in machine maintenance [<xref rid="B1-sensors-25-05472" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05472" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05472" ref-type="bibr">3</xref>]. Current studies employ diverse signal modalities for machinery health assessment [<xref rid="B4-sensors-25-05472" ref-type="bibr">4</xref>], such as vibrational, electrical, acoustic, and emission-based measurements. Notably, vibration analysis has proven especially effective in characterizing equipment condition, given that mechanical faults typically induce measurable oscillatory responses during operation. Accelerometers [<xref rid="B5-sensors-25-05472" ref-type="bibr">5</xref>] have become a common choice for vibration signal acquisition, and the majority of studies on machine fault diagnosis rely on vibration data obtained from accelerometers. Nevertheless, contact sensors are not always the preferred option for machine condition monitoring in numerous industrial settings [<xref rid="B6-sensors-25-05472" ref-type="bibr">6</xref>]. In contrast, non-contact sensing eliminates the need for physical attachment, drastically reducing installation time, labor cost, and the risk of sensor-induced mechanical imbalance or resonance. It averts contamination and wear in corrosive, high-temperature, or high-speed environments, ensuring continuous operation without periodic sensor replacement or recalibration. Because there is no mechanical coupling, the technique measures the true surface motion of the target, free from mass-loading effects or frequency distortion. This preserves the fidelity of high-frequency and low-amplitude vibration signatures that are critical for early fault detection. Additionally, non-contact devices such as event-based or laser sensors can monitor multiple machines from a safe standoff distance, simplifying retrofitting on legacy equipment and enabling unobtrusive surveillance of sealed or rotating assemblies where cabling is impossible.</p><p>Convolutional Neural Networks (CNNs) [<xref rid="B7-sensors-25-05472" ref-type="bibr">7</xref>] have achieved remarkable success in the field of computer vision, driven by advancements in deep learning technologies and hardware computing capabilities. Their effectiveness can be largely attributed to the pyramidal structure of CNNs and their inherent inductive biases, such as translation invariance and local sensitivity. Despite their success in computer vision tasks, Convolutional Neural Networks face several limitations when applied to mechanical fault diagnosis [<xref rid="B8-sensors-25-05472" ref-type="bibr">8</xref>]. These limitations include inadequate modeling of global contextual information, computational complexity, sensitivity to data quality and noise, lack of sensitivity to local features and details, and data-intensive training requirements.</p><p>This article presents a novel signal modality, dynamic machine vision data [<xref rid="B9-sensors-25-05472" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05472" ref-type="bibr">10</xref>], and utilizes Eagle Vision Transformers (EViTs) [<xref rid="B11-sensors-25-05472" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05472" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05472" ref-type="bibr">13</xref>] model to process the event data, thereby achieving minimally contact vibration measurement and accurate fault diagnosis of machinery. Dynamic vision data are captured by event-based cameras [<xref rid="B12-sensors-25-05472" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05472" ref-type="bibr">13</xref>], as shown in <xref rid="sensors-25-05472-f001" ref-type="fig">Figure 1</xref>, which emulate the functionality of biological retinas and have been commercially available over the past decade.</p><p>These cameras are asynchronous sensors that record per-pixel brightness changes, referred to as events, instead of capturing the brightness values of all pixels within a frame. In contrast to traditional standard cameras, event-based cameras offer substantial advantages for industrial applications, such as high temporal resolution, high dynamic range, high-speed motion estimation, low latency, and low power consumption. Event-based cameras are particularly effective for addressing dynamic scene sensing challenges, including motion recognition, high-speed counting, and drone vision. Specifically, their microsecond-level temporal resolution (up to 10 kHz equivalent frame rate) enables the capture of impulsive vibrations or tool chatter that would be smeared out by conventional 30&#8211;120 fps imagers, while a dynamic range exceeding 120 dB allows for reliable operation under the extreme lighting contrasts found near welding arcs or bright conveyor belts. Because each pixel operates autonomously, data throughput&#8212;and thus latency&#8212;scales only with scene dynamics; in practice, end-to-end delays below 1 ms are routinely achieved on edge hardware, and idle pixels consume mere microwatts, yielding a 10&#8211;100&#215; power reduction versus streaming RGB sensors. These merits translate directly to industrial use cases: on high-speed spindles, event cameras track sub-micron run-out by observing microscopic laser-dot displacements without additional encoders; on bottling lines, they count up to 50,000 parts per second by detecting the edges of passing caps with zero motion blur; in steel mills, they monitor strip-flatness in real time by measuring the vibration of reflective edges under strong ambient glare; and on autonomous forklifts, they provide robust obstacle detection when sudden brightness changes (e.g., exiting a dark aisle into sunlight) would saturate conventional vision.</p><p>EViTs integrate the unique physiological and visual characteristics of eagle eyes with the architecture of vision transformers, thereby harnessing the potential advantages of both. Unlike CNNs, which rely on fixed local kernels and full-frame grids, EViT replaces convolution with global Bi-Fovea Self-Attention that operates directly on sparse, timestamped events. The Bi-Fovea Visual Interaction (BFVI) structure of EViTs is designed to combine the benefits of both cascaded and parallel architectures, including hierarchical organization and parallel information processing. Building on this foundation, EViTs employ a novel Bi-Fovea Self-Attention (BFSA) mechanism [<xref rid="B13-sensors-25-05472" ref-type="bibr">13</xref>] and a Bi-Fovea Feedforward Network (BFFN). These components mimic the hierarchical and parallel information processing scheme of the biological visual cortex. As improved variants of self-attention and feedforward networks, respectively, BFSA and BFFN enable the network to extract features in a coarse-to-fine manner, resulting in high computational efficiency and scalability. Its coarse-to-fine BFVI pipeline cuts computation while preserving microsecond timing, enabling low-power, minimally intrusive monitoring of industrial machinery.</p><p>Although event-based sensing holds substantial promise for machine fault detection, its practical implementation faces notable technical hurdles. The operational paradigm of event cameras differs fundamentally from traditional imaging systems, demanding specialized computational frameworks for reliable anomaly detection. To our knowledge, this work pioneers the systematic investigation of this research gap. The key innovations of this article are listed as follows.</p><list list-type="order"><list-item><p>A novel non-contact fault diagnosis method based on dynamic vision sensing is proposed. Experimental results demonstrate the viability of utilizing dynamic vision data acquired from event-based cameras for mechanical fault detection.</p></list-item><list-item><p>The EViT model is proposed for the first time to process vision data, addressing a critical research gap in mechanical fault diagnosis applications.</p></list-item><list-item><p>Experimental validation was conducted using real-world rotor machinery data to verify the performance of the EViT model for mechanical fault diagnosis.</p></list-item></list><p>The rest of this article is organized as follows. It starts with the related works in <xref rid="sec2-sensors-25-05472" ref-type="sec">Section 2</xref>. The proposed method is presented in <xref rid="sec3-sensors-25-05472" ref-type="sec">Section 3</xref>, and experimentally validated in <xref rid="sec4-sensors-25-05472" ref-type="sec">Section 4</xref>. Finally, <xref rid="sec5-sensors-25-05472" ref-type="sec">Section 5</xref> concludes this article.</p></sec><sec id="sec2-sensors-25-05472"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05472"><title>2.1. Intelligent Machinery Fault Diagnosis</title><p>Industrial equipment reliability critically depends on effective condition monitoring systems. Precise malfunction identification at early stages significantly improves operational security while optimizing upkeep expenditures [<xref rid="B14-sensors-25-05472" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05472" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05472" ref-type="bibr">16</xref>]. Recent advances in computational intelligence [<xref rid="B17-sensors-25-05472" ref-type="bibr">17</xref>] have led to substantial progress in automated equipment failure detection techniques. Among various sensing modalities, oscillatory motion measurements remain the predominant data source for equipment wellness assessment, with vibration analysis consistently demonstrating superior diagnostic performance. Innovative signal processing techniques, such as the enhanced empirical mode decomposition approach developed by Wang [<xref rid="B16-sensors-25-05472" ref-type="bibr">16</xref>] and colleagues, have shown remarkable efficacy in isolating critical failure signatures from mechanical vibration patterns. Complementary research by Martin-Diaz [<xref rid="B18-sensors-25-05472" ref-type="bibr">18</xref>] and collaborators established a pioneering technique for incipient defect identification in electric motors through sophisticated analysis of electromagnetic signatures across temporal and spectral domains, incorporating adaptive boosting with refined data acquisition strategies. Alternative monitoring approaches, including stress wave detection methodologies, have also proven valuable, as evidenced by successful applications in rail transportation bearing systems [<xref rid="B19-sensors-25-05472" ref-type="bibr">19</xref>].</p><p>The advent of sophisticated connectionist systems has revolutionized equipment failure analysis, with hierarchical learning architectures demonstrating exceptional proficiency in deciphering intricate data correlations [<xref rid="B20-sensors-25-05472" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05472" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05472" ref-type="bibr">22</xref>]. Modern diagnostic systems increasingly incorporate diverse neural network topologies, including spatial feature extractors, temporal sequence analyzers, and memory-enhanced architectures, to process mechanical oscillation data. Notable contributions include Shao&#8217;s hierarchical feature learning system for rotational equipment [<xref rid="B23-sensors-25-05472" ref-type="bibr">23</xref>], employing deep autoencoding structures for automated signature extraction and classification. Jia&#8217;s team [<xref rid="B24-sensors-25-05472" ref-type="bibr">24</xref>] advanced the field through their normalized spatial filtering network, specifically designed for handling uneven failure category distributions, with incorporated feature visualization for model interpretability. Yu&#8217;s innovative one-dimensional convolutional architecture demonstrated superior performance in vibration-based failure categorization [<xref rid="B5-sensors-25-05472" ref-type="bibr">5</xref>], incorporating bilateral weighting mechanisms for enhanced generalization to novel fault conditions. Parallel developments by Huang [<xref rid="B25-sensors-25-05472" ref-type="bibr">25</xref>] introduced sophisticated learning systems for fluid power apparatus diagnostics, enabling automated processing of heterogeneous temporal data without requiring specialized domain knowledge.</p><p>Contemporary research predominantly focuses on acceleration-based monitoring systems, with relatively limited exploration of alternative sensing modalities. Particularly scarce are investigations into non-contact optical measurement techniques for mechanical vibration analysis, representing a significant gap in current condition monitoring literature.</p></sec><sec id="sec2dot2-sensors-25-05472"><title>2.2. Event-Based Machine Vision</title><p>Bioinspired vision sensors, emerging over the past twenty years, have revolutionized dynamic scene capture through their unique ability to record asynchronous pixel-level luminance variations which are termed events [<xref rid="B26-sensors-25-05472" ref-type="bibr">26</xref>]. These neuromorphic imaging devices and their generated data streams possess distinct advantages including minimal energy requirements and exceptional temporal precision, making them particularly suitable for applications demanding ultra-fast motion analysis. Contemporary research has demonstrated the versatility of these sensors across diverse fields, ranging from 3D scene reconstruction and motion vector calculation to robotic navigation, image enhancement, and microscopic particle tracking [<xref rid="B27-sensors-25-05472" ref-type="bibr">27</xref>].</p><p>The automotive industry has particularly benefited from these innovative sensors, with numerous driver assistance systems now incorporating event-based visual processing [<xref rid="B15-sensors-25-05472" ref-type="bibr">15</xref>]. The sensors&#8217; wide dynamic ranges and near-instantaneous responses enable superior vehicular perception capabilities. Notable implementations include Zhou&#8217;s navigation system [<xref rid="B28-sensors-25-05472" ref-type="bibr">28</xref>] that maintains reliable operation under extreme lighting variations while requiring only standard computational resources. Jin&#8217;s team [<xref rid="B29-sensors-25-05472" ref-type="bibr">29</xref>] developed a sophisticated six-degree-of-freedom position estimation framework combining convolutional and recurrent neural architectures for processing event-based visual streams, achieving both computational efficiency and precision. Another breakthrough came from Lagorce&#8217;s motion tracking algorithm [<xref rid="B30-sensors-25-05472" ref-type="bibr">30</xref>], which exploits spatiotemporal event correlations to enhance system resilience while effectively solving object recognition challenges through innovative optical flow computation in velocity-direction coordinates.</p><p>Despite these advancements, the potential of dynamic vision in mechanical system monitoring remains largely unexplored. Traditional visual inspection techniques face inherent limitations in vibration analysis due to noise susceptibility and motion capture constraints. This investigation systematically evaluates the applicability of neuromorphic sensing for equipment diagnostics, presenting comprehensive experimental validation of its effectiveness for fault detection applications.</p></sec></sec><sec id="sec3-sensors-25-05472"><title>3. Event-Based Fault Diagnosis Method</title><sec id="sec3dot1-sensors-25-05472"><title>3.1. Event Vision Data and Representations</title><p>This research presents a novel framework for equipment fault detection utilizing dynamic vision technology. The event-based camera generates asynchronous data streams where individual events encode discrete brightness variations at specific pixels and timestamps. Each event comprises a four-dimensional vector e = [x, y, t, p], with (x,y) indicating spatial coordinates, t representing the precise timing, and p &#8712; {&#8722;1, +1} denoting brightness decrease or increase, respectively. These sensors operate by continuously monitoring pixel-level intensity changes and triggering events only when variations exceed predetermined thresholds.</p><p>For mechanical system monitoring applications, the event vision data in the time sequence are recorded as <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>th event, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of all the events in the concerned data collection time period <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The inherent asynchronicity and sparse nature of event data pose unique challenges for conventional deep learning approaches designed for regular 1D temporal or 2D spatial data. To address this, we introduce a novel two-channel image-like representation that preserves the spatiotemporal characteristics of event streams. As depicted in <xref rid="sensors-25-05472-f002" ref-type="fig">Figure 2</xref>, this representation separately accumulates positive and negative polarity events at each pixel location, creating complementary information channels that capture the dynamic evolution of machine vibrations.</p><p>The proposed methodology establishes a formal training framework <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#119967;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where each sample <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> consists of polarity-separated event count matrices with dimensions Nx &#215; Ny matching the sensor resolution. And <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> means the corresponding machine health condition label, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of the training samples, while <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> are the channels for the positive and negative cumulative event number, respectively. The corresponding health state labels <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> enable supervised learning of the diagnostic model. All training samples maintain consistent event counts, which are denoted as <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, to ensure dimensional uniformity. The core technical objective involves developing a deep neural network (EViTs) mapping function f that establishes the relationship h = f(r) between event-based representations and equipment health conditions.</p></sec><sec id="sec3dot2-sensors-25-05472"><title>3.2. Deep Neural Network Model</title><p>Our research concentrates on improving both event data interpretation and diagnostic reliability for mechanical fault detection. The central innovation involves implementing a specialized deep neural network, which is Eagle Vision Transformers (EViTs) [<xref rid="B13-sensors-25-05472" ref-type="bibr">13</xref>], whose architecture is visually documented in <xref rid="sensors-25-05472-f003" ref-type="fig">Figure 3</xref>.</p><p>The architectural framework of EViT incorporates three fundamental components: a convolutional stem, multiple 2 &#215; 2 convolutional operations, and Bionic Eagle Vision (BEV) modules. Critical implementation details include: the 2 &#215; 2 convolutional operators employ a stride setting of 2 layers to facilitate patch embedding; adhering to established hierarchical paradigms [<xref rid="B31-sensors-25-05472" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05472" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05472" ref-type="bibr">33</xref>], the network organizes its computation into four structurally homologous stages; and progressive feature resolution reduction (4&#215;, 8&#215;, 16&#215;, and 32&#215; scaling factors across stages 1&#8211;4, respectively) coupled with channel dimension expansion (D1 through D4).</p><p>Processing flow initiates with an H &#215; W &#215; 3 dimensional input undergoing initial feature extraction via the convolutional stem&#8212;a triple-stacked 3 &#215; 3 convolutional configuration where the initial layer&#8217;s stride-2 operation enhances early-stage training stability. Subsequent feature transformation occurs through an alternating sequence of 2 &#215; 2 convolutional layers and BEV modules, progressively building multi-scale target representations. For mechanical fault classification applications, the system terminates with a classification head comprising layer normalization, global average pooling, and a final fully connected projection layer to generate diagnostic predictions.</p><p>As the foundational structural elements of EViTs, BEV blocks synergistically combine the strengths of convolutional operations and visual transformer architectures. Each BEV block incorporates three core modules: a Convolutional Position Embedding (CPE) mechanism, a Bi-Fovea Self-Attention (BFSA) module, and a Bi-Fovea Feedforward Network (BFFN). Spatial relationships are fundamentally important for characterizing visual data representations. Conventional vision transformer implementations typically employ two approaches for position encoding: Absolute Position Embedding (APE) [<xref rid="B34-sensors-25-05472" ref-type="bibr">34</xref>] and Relative Position Embedding (RPE) [<xref rid="B35-sensors-25-05472" ref-type="bibr">35</xref>]. These embedding schemes generally utilize either sinusoidal functions with different frequency parameters or trainable parameter matrices.</p><p>APE implementations face notable limitations as they are resolution-dependent, requiring modification when feature token dimensions change, due to their lack of scale adaptability. Conversely, RPE mechanisms account for inter-token spatial relationships within input sequences, demonstrating translation-invariant characteristics. Nevertheless, RPE introduces computational overhead when determining pairwise feature token distances. More critically, computer vision applications often demand absolute positional data, which RPE cannot inherently supply.</p><p>To address these challenges, EViT first embeds every token with Convolutional Position Embedding (CPE). By replacing sinusoidal or learned absolute/relative encodings with a light-weight depth-wise convolution, CPE gains two unique properties: (i) zero-padded convolutions let the same layer adapt to arbitrary input resolutions without re-training, yielding true plug-and-play behaviour, and (ii) the inductive locality inherent in depth-wise kernels injects translation-equivariance into the otherwise bias-free Transformer, lifting the model&#8217;s effective capacity ceiling.</p><p>Within each Bionic Eagle Vision (BEV) block, CPE&#8217;s positional features are processed by the Bi-Fovea Self-Attention (BFSA) mechanism. Mimicking the eagle&#8217;s shallow and deep foveae, BFSA splits computation into a coarse global branch (Shallow-Fovea Attention) that downsamples keys/values to capture scene gist, and a fine-grained branch (Deep-Fovea Attention) that re-uses the global summary to refine every spatial location. Their outputs are additively fused, achieving simultaneous wide-field context and pinpoint detail without cascading stages or heavy dense maps.</p><p>Complementing BFSA, the Bi-Fovea Feed-Forward Network (BFFN) adopts a two-scale depth-wise design: an initial 3 &#215; 3 kernel enlarges receptive fields to harvest local textures, followed by a 1 &#215; 1 projection that mixes channels and re-weights features. GELU non-linearity and residual paths are retained to stabilize gradients. Together, CPE, BFSA and BFFN form a unified &#8220;coarse-to-fine yet parallel&#8221; pipeline that marries the efficiency of CNNs with the expressiveness of self-attention, all within a single, scalable block.</p></sec><sec id="sec3dot3-sensors-25-05472"><title>3.3. Loss Function Method</title><p>This article introduces an advanced loss function method to optimize the discriminative capability for mechanical fault pattern recognition. CrossEntropyLoss [<xref rid="B36-sensors-25-05472" ref-type="bibr">36</xref>], also known as categorical cross-entropy, is a widely used loss function in deep learning for classification tasks. It measures the dissimilarity between the predicted probability distribution and the true label distribution, guiding the model to adjust its parameters to minimize this discrepancy. Unlike regression losses such as Mean Squared Error (MSE), CrossEntropyLoss is specifically designed for probabilistic classification, making it more effective for tasks where outputs represent class probabilities.</p><p>Given a true label y (typically one-hot encoded) and a predicted probability p (obtained via Softmax activation), the CrossEntropyLoss for a single sample is defined as:<disp-formula id="FD1-sensors-25-05472"><label>(1)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05472"><label>(2)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of classes. Since <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is non-zero only for the correct class, the loss simplifies to the negative log-likelihood of the true class probability. This formulation penalizes incorrect predictions more severely as <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> approaches zero, ensuring rapid gradient updates during training. Before computing CrossEntropyLoss, raw model outputs(logits) [<xref rid="B37-sensors-25-05472" ref-type="bibr">37</xref>] are transformed into probabilities using the Softmax function. Softmax normalizes logits into a probability distribution, ensuring <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This step is crucial because CrossEntropyLoss operates on probabilities rather than unbounded logits.</p></sec><sec id="sec3dot4-sensors-25-05472"><title>3.4. General Implementation</title><p><xref rid="sensors-25-05472-f004" ref-type="fig">Figure 4</xref> presents the operational pipeline of our novel event-vision based approach for intelligent mechanical fault detection [<xref rid="B38-sensors-25-05472" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05472" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05472" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05472" ref-type="bibr">41</xref>]. The system workflow initiates with data acquisition, where event-based cameras capture dynamic visual streams from operational machinery across various health states to create annotated training datasets. These asynchronous event streams undergo specialized preprocessing to generate structured event representations suitable for computational analysis.</p><p>The training phase commences with initialization of the deep neural network model (EViTs), which subsequently undergoes iterative optimization using the annotated dataset [<xref rid="B42-sensors-25-05472" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-05472" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05472" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05472" ref-type="bibr">45</xref>]. During each epoch, the algorithm first selects a random minibatch from the original samples and generates complementary augmented instances.</p><p>The training procedure follows an iterative optimization cycle where samples are dynamically generated and utilized within each epoch. Upon completing parameter updates for a given epoch, the temporarily generated augmented instances are purged from memory before initiating the subsequent training cycle [<xref rid="B46-sensors-25-05472" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05472" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-05472" ref-type="bibr">48</xref>]. This process involves: reapplication of the data augmentation protocol to create fresh synthetic samples, forward-backward propagation through the network architecture, and continuous repetition of this sequence until convergence criteria are satisfied. The evaluation phase subsequently assesses model generalization capability by processing previously unseen unlabeled test data through the trained network to quantify diagnostic accuracy.</p></sec></sec><sec id="sec4-sensors-25-05472"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05472"><title>4.1. Event Vision Dataset for Fault Diagnosis</title><p>This research evaluates the proposed methodology using experimental data acquired from a dedicated rolling element bearing test platform. As shown in <xref rid="sensors-25-05472-f005" ref-type="fig">Figure 5</xref>, the experimental setup comprises an electric motor driving a shaft supported by ER-16K bearings, designed to simulate four distinct mechanical health states: healthy operation, inner race defect, rolling element defect, and outer race defect, collectively establishing a multi-category fault identification challenge. The schematic diagrams of the three types of bearings used in the experiment are shown in <xref rid="sensors-25-05472-f006" ref-type="fig">Figure 6</xref>. Artificially induced defects with approximate 1mm depth were carefully introduced at various bearing locations using precision machining tools to replicate realistic failure modes. Comprehensive testing was conducted across three operational speeds (1200, 1500, and 1800 RPM), generating vibration datasets encompassing all fault conditions under varying rotational velocities. Vibration monitoring was performed using a strategically positioned accelerometer mounted directly on the bearing housing, with all measurements captured at 12.8 kHz sampling frequency to ensure adequate temporal resolution for fault signature analysis.</p><p>A third-generation Prophesee neuromorphic vision sensor (Gen 3.1) was positioned adjacent to the test bearing to acquire asynchronous event streams. The device specifications include a 640 &#215; 480 pixel array, 200 &#956;s event latency, and peak event throughput of 50 million events per second. Synchronized acquisition of vibration signals and event-based visual data was performed across multiple rotational velocities and bearing conditions under controlled illumination. For targeted vibration analysis, a 64 &#215; 64 pixel region centered on the bearing housing was isolated from the raw event data for subsequent fault detection processing.</p></sec><sec id="sec4dot2-sensors-25-05472"><title>4.2. Fault Diagnosis Tasks and Comparisons</title><p>The dynamic vision data is evaluated for fault diagnosis in this study. For the event vision data, 2000 events are considered in each sample, which is prepared using the method described in <xref rid="sec3dot1-sensors-25-05472" ref-type="sec">Section 3.1</xref>. The experimental configuration employs a fixed sample length of 4096 data points, with each fault diagnosis task (combining specific health states and rotational speeds) containing 500 training instances and 250 testing instances. To comprehensively assess diagnostic performance of different deep neural network models, we conduct parallel evaluations across four distinct fault detection scenarios using both CNN and EViTs models. The comprehensive information of the related fault diagnosis tasks is presented in <xref rid="sensors-25-05472-t001" ref-type="table">Table 1</xref>.</p><p>This study conducted comparative analyses of fault diagnosis performance using diverse deep neural network architectures. For tasks A1 through A4, conventional CNN models were employed to process event-based vision data, while tasks B1 to B4 utilized the proposed EViT framework as detailed in <xref rid="sec3dot2-sensors-25-05472" ref-type="sec">Section 3.2</xref> for event data analysis. The experimental configurations and data acquisition procedures remain basically consistent with the proposed method. Classification accuracy serves as a well-established evaluation criterion in pattern recognition applications, particularly suitable for mechanical fault detection scenarios. This metric is consequently adopted for performance assessment throughout our experimental analysis. Formally, given a test set containing <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> samples, where <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the count of accurately classified instances matching the true fault labels, the diagnostic accuracy is computed as (<inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) &#215; 100%. The complete parameter configuration is documented in <xref rid="sensors-25-05472-t002" ref-type="table">Table 2</xref>, with parameter optimization playing a critical role in our methodology.</p><p>As delineated in <xref rid="sensors-25-05472-t002" ref-type="table">Table 2</xref>, the parameters are systematically categorized into: (1) trainable network parameters &#952;, and (2) tunable hyper-parameters. Regarding the model parameters, they are optimized within the deep neural network framework using training data and preset hyper-parameters. These parameters can be determined once the hyper-parameters and training data are provided, although minor variations may occur across different training runs due to inherent model stochasticity. For this study, both the CNN and EViT models were configured with identical fundamental parameters. Hyper-parameter optimization follows standard data-driven machine learning practices through dedicated validation procedures. In our experimental framework, this involves: (1) establishing a validation set under 1200 r/min operating conditions with 1000 training and 1000 test samples, which is completely independent from tasks A1&#8211;A4 datasets, and (2) adopting 300 training epochs as the benchmark based on observed convergence behavior and evidenced by stabilized loss metrics. For real-world deployment, the parameter configuration process maintains methodological consistency: operational data from target equipment forms the validation basis for hyper-parameter tuning. While field applications often face data scarcity challenges, two practical solutions are implemented: minimum viable validation using binary health states (normal/faulty), or temporal segmentation of extended unlabeled operational records into distinct condition periods. This approach ensures rigorous parameter optimization while accommodating industrial implementation constraints.</p></sec><sec id="sec4dot3-sensors-25-05472"><title>4.3. Experimental Results and Performance Evaluations</title><p><xref rid="sensors-25-05472-t003" ref-type="table">Table 3</xref> presents the comprehensive experimental results of different deep learning models across various fault diagnosis tasks. The proposed model consistently achieves testing accuracies above 95% under different operating conditions, demonstrating significant advantages for mechanical fault diagnosis problems and thereby validating the effectiveness of the event-vision-based diagnostic paradigm. All evaluated tasks exhibit similar patterns of result distribution.</p><p>Furthermore, this study conducts a comparative analysis of diagnostic performance between CNN and EViT models. Taking task B1 as an example, the EViT model achieves a testing accuracy of 99.0%, slightly outperforming the CNN model&#8217;s 96.1% accuracy, with similar performance trends observed across other tasks. These results demonstrate the superior competitiveness of EViT models over conventional CNN architectures for mechanical fault diagnosis applications. The experiments were implemented on a hardware platform consisting of a GeForce RTX 4060ti GPU and Intel i7 CPU, utilizing the PyTorch 1.12 programming framework. Computational efficiency tests reveal average training times of 672.5 s for tasks A1&#8211;A3 and 2136.7 s for task A4, indicating computationally acceptable overhead for an offline diagnostic approach.</p><p>The comparative performance evaluation of different diagnostic models is illustrated in <xref rid="sensors-25-05472-f007" ref-type="fig">Figure 7</xref>, which presents the confusion matrices for both CNN and EViT architectures when processing tasks A1 and A3. The experimental data reveals remarkable diagnostic capabilities across all fault categories, with classification accuracy consistently surpassing 95% for each fault mode while maintaining minimal false positive rates&#8212;findings that align perfectly with the outstanding test accuracy documented in <xref rid="sensors-25-05472-t003" ref-type="table">Table 3</xref>. Notably, the transformer-based EViT approach demonstrates significantly better fault identification performance than conventional CNN, particularly in detecting mechanical anomalies, thereby conclusively confirming the methodological advantages of the proposed framework.</p><p>In practical mechanical systems, operational conditions often exhibit significant variability. This study evaluates the performance of CNN and EViT models under such condition shifts, utilizing fault diagnosis datasets with wide operational ranges due to limited availability of specialized rotating machinery event datasets. As detailed in <xref rid="sensors-25-05472-t004" ref-type="table">Table 4</xref>, eight cross-condition tasks were designed: Tasks C1 and C2 incorporate both 1200 rpm (limited) and 1500 rpm (abundant) training data but test on 1200 rpm, whereas Tasks C3 and C4 exclude 1500 rpm data entirely. Comparative analysis of C1 versus C3 and C2 versus C4 reveals that models trained only on limited same-condition data (C3 and C4) achieve suboptimal accuracy, while introducing cross-condition data (C1 and C2) yields substantial improvements&#8212;particularly for EViT, which shows greater performance gains than CNN when leveraging heterogeneous operational data (C2 and D2 compared to C1 and D1).</p><p>These results demonstrate that cross-condition training data effectively compensates for single-condition data scarcity, enhancing model robustness to operational variations. EViT&#8217;s superior adaptability stems from its inherent capacity to extract condition-invariant features: its attention mechanism aligns naturally with the sparse spatiotemporal characteristics of mechanical event data, while its explicit clustering of feature distributions across conditions contrasts with CNN&#8217;s implicit learning approach that remains more sensitive to domain shifts. This architectural advantage allows EViT to more fully exploit complementary information embedded in multi-condition datasets, establishing it as a more versatile solution for real-world applications where operational parameters fluctuate.</p><p>Subsequently, visual validation was performed on the features extracted from event data using different methods. Specifically, the fully connected features from the final layer of the deep neural network were extracted and visualized through t-SNE dimensionality reduction. The results for tasks A1 and A3 are presented in <xref rid="sensors-25-05472-f008" ref-type="fig">Figure 8</xref>. The visualization demonstrates that compared to the CNN model, samples processed by the EViT model exhibit tighter intra-class clustering and more distinct inter-class boundaries across different health states. This clearly illustrates EViT&#8217;s superior effectiveness in event data-based fault pattern recognition.</p><p>To comprehensively validate the performance advantages of EViT over conventional CNNs, <xref rid="sensors-25-05472-t005" ref-type="table">Table 5</xref> presents a comparative analysis of diagnostic accuracy among different models. As shown in <xref rid="sensors-25-05472-t005" ref-type="table">Table 5</xref>, Task E1 employs the EViT model, while Tasks E2 and E3 utilize CNN architectures with varying network depths, and Task E4 adopts a CBAM-enhanced CNN model (CBAM-CNN), with all other experimental conditions maintained identical. The experimental results demonstrate that Task E1 achieves marginally higher accuracy compared to other tasks, thereby substantiating the superior diagnostic performance of the proposed EViT framework for mechanical fault detection.</p><p>In order to validate the effectiveness of individual modules in the EViT model for fault diagnosis, we conducted ablation studies on the BFSA, BFFN, and CPE modules. Experimental Group 1 employed the complete EViT model, Group 2 replaced the BFSA with standard MHSA, Group 3 substituted the BFFN with a standard FFN, and Group 4 removed the Convolutional Position Embedding (CPE). All groups were tested on the identical Task A1 dataset. As shown in <xref rid="sensors-25-05472-f009" ref-type="fig">Figure 9</xref>, the experimental results demonstrate significant performance impacts when removing these modules. Specifically, replacing BFSA with standard MHSA caused a 2.5% accuracy decrease, while substituting BFFN with FFN reduced accuracy by 1.8%. The complete removal of CPE led to the most substantial performance degradation. These findings clearly establish the critical importance of all three modules for optimal model performance.</p><p>Moreover, conventional vibration-based fault diagnosis techniques predominantly rely on single-point data acquisition. However, adopting a machine dynamic vision approach enables monitoring of extended spatial regions beyond the immediate target component. Notably, bearing defects can generate minute disturbances in shaft dynamics that often elude detection by conventional sensors. This research systematically examines how varying event frame regions of interest (ROI) affect model performance, evaluating both CNN and EViT architectures across multiple ROI configurations: the default 200 &#215; 300 pixel area plus three bearing-centered regions measuring 100 &#215; 200, 250 &#215; 350, and 300 &#215; 400 pixels.</p><p>As evidenced in <xref rid="sensors-25-05472-f010" ref-type="fig">Figure 10</xref>, experimental findings reveal a general trend where expanded ROIs correlate with enhanced classification accuracy for both architectures. This improvement stems from the incorporation of more comprehensive vibration signatures encompassing both bearing and shaft dynamics. Nevertheless, the performance gains diminish considerably when exceeding the 200 &#215; 300 pixel threshold. Given the substantial computational overhead associated with larger sample dimensions, the 200 &#215; 300 pixel ROI remains the optimal selection for practical implementation.</p><p>A particularly noteworthy observation concerns the differential impact of ROI scaling&#8212;the EViT architecture demonstrates significantly greater accuracy enhancement compared to its CNN counterpart as ROI dimensions increase. This phenomenon underscores EViT&#8217;s superior capacity for processing expanded input domains, attributed to its enhanced ability to capture extensive spatial dependencies and integrate global contextual features. Such characteristics substantiate the transformer-based model&#8217;s advantages in machine condition monitoring applications.</p><p>It should be noted that although the EViT model demonstrates superior performance over CNN in fault diagnosis tasks, both architectures face common challenges in processing event-based vision data. Firstly, vibration feature extraction through visual perception is inherently challenging, particularly in identifying subtle differences between various fault modes, which is constrained by the spatiotemporal resolution of event sensors and environmental noise interference. Secondly, current research has yet to establish standardized frameworks for feature extraction and fault recognition specifically for event vision data, leaving room for optimization in processing efficiency&#8212;both in CNN&#8217;s local convolutional characteristics and EViT&#8217;s global attention mechanism when handling event streams. As the first study to apply EViT to mechanical fault diagnosis, this work provides a benchmark investigation by systematically comparing the performance differences between these two architectures. The results demonstrate EViT&#8217;s advantages in both accuracy and cross-condition adaptability, promising for further investigation in this direction.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05472"><title>5. Conclusions</title><p>This article presents the first integration of event-based cameras with an Eagle Vision Transformer (EViT) to propose a novel non-contact fault diagnosis method for rotating machinery. As an innovative neural network architecture, the EViT model demonstrates the capability to distinguish features corresponding to different health states, thereby addressing a critical research gap in the application of EViT for mechanical fault diagnosis. Through a systematic comparison of EViT and conventional Convolutional Neural Networks (CNN) in processing event-based vision data, this work elucidates the performance differences between these two models in fault diagnosis tasks.</p><p>The research highlights three key advantages of EViT over CNN. First, the Bi-Fovea Self-Attention (BFSA) mechanism in EViT mimics the central-peripheral visual processing of eagle eyes, enabling multi-scale vibration feature extraction and overcoming the limitations of CNN&#8217;s localized receptive fields. Experimental results confirm that this mechanism more effectively identifies invariant features across varying rotational speeds, whereas CNN struggles to adapt due to the fixed kernel sizes of its convolutional layers. Second, EViT&#8217;s hierarchical feature fusion architecture, facilitated by the Bi-Fovea Feedforward Network (BFFN), achieves superior synergy between global and local features, outperforming CNN in diagnosing compound bearing faults. Third, EViT exhibits significantly enhanced spatiotemporal modeling capabilities for event data, particularly in large Region-of-Interest (ROI) scenarios like exceeding 200 &#215; 300 pixels, where its accuracy improvement surpasses that of CNN. This advantage stems from the self-attention mechanism&#8217;s ability to capture long-range spatial dependencies.</p><p>Notably, EViT&#8217;s superiority is most pronounced in cross-condition tasks. When trained on multi-speed operational data, EViT demonstrates a greater improvement in generalization performance compared to CNN, owing to its explicit feature distribution alignment strategy. In contrast, CNN relies on implicit regularization techniques such as data augmentation, leading to higher performance variability under unseen operating conditions. These findings provide a new solution for variable-condition diagnosis in industrial applications.</p><p>However, the study also identifies two limitations of EViT. First, the quadratic spatial complexity of the Bi-Fovea Self-Attention mechanism increases FLOPs by four to seven times and GPU memory usage by three to five times, rendering real-time deployment impractical on resource-constrained edge devices. Second, the Transformer backbone requires substantial labeled data to achieve optimal performance; in small-sample scenarios, EViT&#8217;s accuracy drops below that of lightweight CNNs, nullifying its theoretical advantages. These insights highlight critical directions for future research, which will focus on enhancing the model&#8217;s environmental generalization capability and optimizing its network architecture.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Z.J.; methodology, X.L.; formal analysis, C.S.; writing&#8212;original draft preparation, Z.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Dataset available on request from the author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Cuiying Sun was employed by the company Weichai Power Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05472"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Task-sequencing meta learning for intelligent few-shot fault diagnosis with limited data</article-title><source>IEEE Trans. Ind. Inform.</source><year>2021</year><volume>18</volume><fpage>3894</fpage><lpage>3904</lpage><pub-id pub-id-type="doi">10.1109/TII.2021.3112504</pub-id></element-citation></ref><ref id="B2-sensors-25-05472"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name></person-group><article-title>Open-set domain adaptation in machinery fault diagnostics using instance-level weighted adversarial learning</article-title><source>IEEE Trans. Ind. Inform.</source><year>2021</year><volume>17</volume><fpage>7445</fpage><lpage>7455</lpage><pub-id pub-id-type="doi">10.1109/TII.2021.3054651</pub-id></element-citation></ref><ref id="B3-sensors-25-05472"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>An Attention-Based Multidimensional Fault Information Sharing Framework for Bearing Fault Diagnosis</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>224</elocation-id><pub-id pub-id-type="doi">10.3390/s25010224</pub-id><pub-id pub-id-type="pmid">39797015</pub-id><pub-id pub-id-type="pmcid">PMC11723462</pub-id></element-citation></ref><ref id="B4-sensors-25-05472"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>C.</given-names></name></person-group><article-title>A deep-learning method for remaining useful life prediction of power machinery via dual-attention mechanism</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>497</elocation-id><pub-id pub-id-type="doi">10.3390/s25020497</pub-id><pub-id pub-id-type="pmid">39860867</pub-id><pub-id pub-id-type="pmcid">PMC11769517</pub-id></element-citation></ref><ref id="B5-sensors-25-05472"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name></person-group><article-title>Data-driven deep learning approach for thrust prediction of solid rocket motors</article-title><source>Measurement</source><year>2024</year><volume>225</volume><fpage>114051</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.114051</pub-id></element-citation></ref><ref id="B6-sensors-25-05472"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Deep-learning-based open set fault diagnosis by extreme value theory</article-title><source>IEEE Trans. Ind. Inform.</source><year>2021</year><volume>18</volume><fpage>185</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1109/TII.2021.3070324</pub-id></element-citation></ref><ref id="B7-sensors-25-05472"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Domain Adversarial Transfer Learning Bearing Fault Diagnosis Model Incorporating Structural Adjustment Modules</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>1851</elocation-id><pub-id pub-id-type="doi">10.3390/s25061851</pub-id><pub-id pub-id-type="pmid">40292990</pub-id><pub-id pub-id-type="pmcid">PMC11946592</pub-id></element-citation></ref><ref id="B8-sensors-25-05472"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name></person-group><article-title>A survey of convolutional neural networks: Analysis, applications, and prospects</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2021</year><volume>33</volume><fpage>6999</fpage><lpage>7019</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2021.3084827</pub-id><pub-id pub-id-type="pmid">34111009</pub-id></element-citation></ref><ref id="B9-sensors-25-05472"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Neuromorphic computing-enabled generalized machine fault diagnosis with dynamic vision</article-title><source>Adv. Eng. Inform.</source><year>2025</year><volume>65</volume><fpage>103300</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2025.103300</pub-id></element-citation></ref><ref id="B10-sensors-25-05472"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gallego</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lund</surname><given-names>J.E.A.</given-names></name><name name-style="western"><surname>Mueggler</surname><given-names>E.</given-names></name></person-group><article-title>Event-based, 6-DOF camera tracking from photometric depth maps</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>40</volume><fpage>2402</fpage><lpage>2412</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2769655</pub-id><pub-id pub-id-type="pmid">29990121</pub-id></element-citation></ref><ref id="B11-sensors-25-05472"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ceolini</surname><given-names>E.</given-names></name><name name-style="western"><surname>Frenkel</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shrestha</surname><given-names>S.B.</given-names></name></person-group><article-title>Hand-gesture recognition based on EMG and event-based camera sensor fusion: A benchmark in neuromorphic computing</article-title><source>Front. Neurosci.</source><year>2020</year><volume>14</volume><fpage>637</fpage><pub-id pub-id-type="doi">10.3389/fnins.2020.00637</pub-id><pub-id pub-id-type="pmid">32903824</pub-id><pub-id pub-id-type="pmcid">PMC7438887</pub-id></element-citation></ref><ref id="B12-sensors-25-05472"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Federated transfer learning for intelligent fault diagnostics using deep adversarial networks with data privacy</article-title><source>IEEE/ASME Trans. Mechatron.</source><year>2021</year><volume>27</volume><fpage>430</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2021.3065522</pub-id></element-citation></ref><ref id="B13-sensors-25-05472"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Evit: An eagle vision transformer with bi-fovea self-attention</article-title><source>IEEE Trans. Cybern.</source><year>2025</year><volume>55</volume><fpage>1288</fpage><lpage>1300</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2025.3532282</pub-id><pub-id pub-id-type="pmid">40031751</pub-id></element-citation></ref><ref id="B14-sensors-25-05472"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name></person-group><article-title>Enhancing 3-D LiDAR point clouds with event-based camera</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2021</year><volume>70</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TIM.2021.3097862</pub-id></element-citation></ref><ref id="B15-sensors-25-05472"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Conradt</surname><given-names>J.</given-names></name></person-group><article-title>Event-based neuromorphic vision for autonomous driving: A paradigm shift for bio-inspired visual sensing and perception</article-title><source>IEEE Signal Process. Mag.</source><year>2020</year><volume>37</volume><fpage>34</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1109/MSP.2020.2985815</pub-id></element-citation></ref><ref id="B16-sensors-25-05472"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Data privacy preserving federated transfer learning in machinery fault diagnostics using prior distributions</article-title><source>Struct. Health Monit.</source><year>2022</year><volume>21</volume><fpage>1329</fpage><lpage>1344</lpage><pub-id pub-id-type="doi">10.1177/14759217211029201</pub-id></element-citation></ref><ref id="B17-sensors-25-05472"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>S.</given-names></name></person-group><article-title>A deep learning based data-driven thruster fault diagnosis approach for satellite attitude control system</article-title><source>IEEE Trans. Ind. Electron.</source><year>2020</year><volume>68</volume><fpage>10162</fpage><lpage>10170</lpage><pub-id pub-id-type="doi">10.1109/TIE.2020.3026272</pub-id></element-citation></ref><ref id="B18-sensors-25-05472"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>Y.</given-names></name></person-group><article-title>A Cross-Machine Intelligent Fault Diagnosis Method with Small and Imbalanced Data Based on the ResFCN Deep Transfer Learning Model</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>1189</elocation-id><pub-id pub-id-type="doi">10.3390/s25041189</pub-id><pub-id pub-id-type="pmid">40006418</pub-id><pub-id pub-id-type="pmcid">PMC11859420</pub-id></element-citation></ref><ref id="B19-sensors-25-05472"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Du</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name></person-group><article-title>Fault diagnosis of rotating machines based on the EMD manifold</article-title><source>Mech. Syst. Signal Process.</source><year>2020</year><volume>135</volume><fpage>106443</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2019.106443</pub-id></element-citation></ref><ref id="B20-sensors-25-05472"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Deep learning-based prognostic approach for lithium-ion batteries with adaptive time-series prediction and on-line validation</article-title><source>Measurement</source><year>2020</year><volume>164</volume><fpage>108052</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2020.108052</pub-id></element-citation></ref><ref id="B21-sensors-25-05472"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Martin-Diaz</surname><given-names>I.</given-names></name><name name-style="western"><surname>Morinigo-Sotelo</surname><given-names>D.</given-names></name><name name-style="western"><surname>Duque-Perez</surname><given-names>O.</given-names></name></person-group><article-title>Early fault detection in induction motors using AdaBoost with imbalanced small data and optimized sampling</article-title><source>IEEE Trans. Ind. Appl.</source><year>2016</year><volume>53</volume><fpage>3066</fpage><lpage>3075</lpage><pub-id pub-id-type="doi">10.1109/TIA.2016.2618756</pub-id></element-citation></ref><ref id="B22-sensors-25-05472"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>High-speed train wheel set bearing fault diagnosis and prognostics: Fingerprint feature recognition method based on acoustic emission</article-title><source>Mech. Syst. Signal Process.</source><year>2022</year><volume>171</volume><fpage>108947</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2022.108947</pub-id></element-citation></ref><ref id="B23-sensors-25-05472"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name></person-group><article-title>Transfer learning using deep representation regularization in remaining useful life prediction across operating conditions</article-title><source>Reliab. Eng. Syst. Saf.</source><year>2021</year><volume>211</volume><fpage>107556</fpage><pub-id pub-id-type="doi">10.1016/j.ress.2021.107556</pub-id></element-citation></ref><ref id="B24-sensors-25-05472"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Blockchain-based decentralized federated transfer learning methodology for collaborative machinery fault diagnosis</article-title><source>Reliab. Eng. Syst. Saf.</source><year>2023</year><volume>229</volume><fpage>108885</fpage></element-citation></ref><ref id="B25-sensors-25-05472"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name></person-group><article-title>A novel method for intelligent fault diagnosis of rolling bearings using ensemble deep auto-encoders</article-title><source>Mech. Syst. Signal Process.</source><year>2018</year><volume>102</volume><fpage>278</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2017.09.026</pub-id></element-citation></ref><ref id="B26-sensors-25-05472"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>N.</given-names></name></person-group><article-title>Deep normalized convolutional neural network for imbalanced fault classification of machinery and its understanding via visualization</article-title><source>Mech. Syst. Signal Process.</source><year>2018</year><volume>110</volume><fpage>349</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2018.03.025</pub-id></element-citation></ref><ref id="B27-sensors-25-05472"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name></person-group><article-title>Fault diagnosis of hydraulic systems based on deep learning model with multirate data samples</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2021</year><volume>33</volume><fpage>6789</fpage><lpage>6801</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TNNLS.2021.3083401</pub-id><pub-id pub-id-type="pmid">34111001</pub-id></element-citation></ref><ref id="B28-sensors-25-05472"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mueggler</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rebecq</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gallego</surname><given-names>G.</given-names></name></person-group><article-title>The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM</article-title><source>Int. J. Robot. Res.</source><year>2017</year><volume>36</volume><fpage>142</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1177/0278364917691115</pub-id></element-citation></ref><ref id="B29-sensors-25-05472"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Deep-learning-based information fusion methodology for oil film coefficient identification of squeeze film dampers</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>20816</fpage><lpage>20827</lpage></element-citation></ref><ref id="B30-sensors-25-05472"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fischer</surname><given-names>T.</given-names></name><name name-style="western"><surname>Milford</surname><given-names>M.</given-names></name></person-group><article-title>Event-based visual place recognition with ensembles of temporal windows</article-title><source>IEEE Robot. Autom. Lett.</source><year>2020</year><volume>5</volume><fpage>6924</fpage><lpage>6931</lpage><pub-id pub-id-type="doi">10.1109/lra.2020.3025505</pub-id></element-citation></ref><ref id="B31-sensors-25-05472"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gallego</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>X.</given-names></name></person-group><article-title>Event-based motion segmentation with spatio-temporal graph cuts</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2021</year><volume>34</volume><fpage>4868</fpage><lpage>4880</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2021.3124580</pub-id><pub-id pub-id-type="pmid">34767515</pub-id></element-citation></ref><ref id="B32-sensors-25-05472"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name></person-group><article-title>A 6-DOFs event-based camera relocalization system by CNN-LSTM and image denoising</article-title><source>Expert Syst. Appl.</source><year>2021</year><volume>170</volume><fpage>114535</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2020.114535</pub-id></element-citation></ref><ref id="B33-sensors-25-05472"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lagorce</surname><given-names>X.</given-names></name><name name-style="western"><surname>Meyer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ieng</surname><given-names>S.H.</given-names></name></person-group><article-title>Asynchronous event-based multikernel algorithm for high-speed visual features tracking</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2014</year><volume>26</volume><fpage>1710</fpage><lpage>1720</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2014.2352401</pub-id><pub-id pub-id-type="pmid">25248193</pub-id></element-citation></ref><ref id="B34-sensors-25-05472"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>Y.</given-names></name></person-group><article-title>Dual vision transformer</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>10870</fpage><lpage>10882</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3268446</pub-id><pub-id pub-id-type="pmid">37074902</pub-id></element-citation></ref><ref id="B35-sensors-25-05472"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ke</surname><given-names>Z.</given-names></name></person-group><article-title>Biformer: Vision transformer with bi-level routing attention</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>10323</fpage><lpage>10333</lpage></element-citation></ref><ref id="B36-sensors-25-05472"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>568</fpage><lpage>578</lpage></element-citation></ref><ref id="B37-sensors-25-05472"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name></person-group><article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>10012</fpage><lpage>10022</lpage></element-citation></ref><ref id="B38-sensors-25-05472"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Convolutional embedding makes hierarchical vision transformer stronger</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><fpage>739</fpage><lpage>756</lpage></element-citation></ref><ref id="B39-sensors-25-05472"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mohri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>Y.</given-names></name></person-group><article-title>Cross-entropy loss functions: Theoretical analysis and applications</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>23&#8211;29 July 2023</conf-date><fpage>23803</fpage><lpage>23828</lpage></element-citation></ref><ref id="B40-sensors-25-05472"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sabuncu</surname><given-names>M.</given-names></name></person-group><article-title>Generalized cross entropy loss for training deep neural networks with noisy labels</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2018</year><volume>32</volume><fpage>8792</fpage><lpage>8802</lpage><pub-id pub-id-type="pmid">39839708</pub-id><pub-id pub-id-type="pmcid">PMC11747755</pub-id></element-citation></ref><ref id="B41-sensors-25-05472"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Connor</surname><given-names>R.</given-names></name><name name-style="western"><surname>Dearle</surname><given-names>A.</given-names></name><name name-style="western"><surname>Claydon</surname><given-names>B.</given-names></name></person-group><article-title>Correlations of cross-entropy loss in machine learning</article-title><source>Entropy</source><year>2024</year><volume>26</volume><elocation-id>491</elocation-id><pub-id pub-id-type="doi">10.3390/e26060491</pub-id><pub-id pub-id-type="pmid">38920500</pub-id><pub-id pub-id-type="pmcid">PMC11203011</pub-id></element-citation></ref><ref id="B42-sensors-25-05472"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name></person-group><article-title>Federated transfer learning for remaining useful life prediction in prognostics with data privacy</article-title><source>Meas. Sci. Technol.</source><year>2025</year><volume>36</volume><fpage>076107</fpage><pub-id pub-id-type="doi">10.1088/1361-6501/ade552</pub-id></element-citation></ref><ref id="B43-sensors-25-05472"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ling</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Industrial battery state-of-health estimation with incomplete limited data toward second-life applications</article-title><source>J. Dyn. Monit. Diagn.</source><year>2024</year><volume>3</volume><fpage>246</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.37965/jdmd.2024.562</pub-id></element-citation></ref><ref id="B44-sensors-25-05472"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Daman</surname><given-names>A.A.A.</given-names></name><name name-style="western"><surname>Smith</surname><given-names>W.</given-names></name></person-group><article-title>Wear Performance and Wear Monitoring of Nylon Gears Made Using Conventional and Additive Manufacturing Techniques</article-title><source>J. Dyn. Monit. Diagn.</source><year>2025</year><volume>4</volume><fpage>101</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.37965/jdmd.2025.758</pub-id></element-citation></ref><ref id="B45-sensors-25-05472"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thakuri</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ruan</surname><given-names>D.</given-names></name></person-group><article-title>The RUL Prediction of Li-Ion Batteries Based on Adaptive LSTM</article-title><source>J. Dyn. Monit. Diagn.</source><year>2025</year><volume>4</volume><fpage>53</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.37965/jdmd.2025.737</pub-id></element-citation></ref><ref id="B46-sensors-25-05472"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sepasiahooyi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Abdollahi</surname><given-names>F.</given-names></name></person-group><article-title>Fault detection of new and aged lithium-ion battery cells in electric vehicles</article-title><source>Green Energy Intell. Transp.</source><year>2024</year><volume>3</volume><fpage>100165</fpage><pub-id pub-id-type="doi">10.1016/j.geits.2024.100165</pub-id></element-citation></ref><ref id="B47-sensors-25-05472"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Du</surname><given-names>J.</given-names></name></person-group><article-title>Fault diagnosis for lithium-ion batteries in electric vehicles based on signal decomposition and two-dimensional feature clustering</article-title><source>Green Energy Intell. Transp.</source><year>2022</year><volume>1</volume><fpage>100009</fpage><pub-id pub-id-type="doi">10.1016/j.geits.2022.100009</pub-id></element-citation></ref><ref id="B48-sensors-25-05472"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Leng</surname><given-names>J.</given-names></name></person-group><article-title>Integrating large language model and digital twins in the context of industry 5.0: Framework, challenges and opportunities</article-title><source>Robot. Comput.-Integr. Manuf.</source><year>2025</year><volume>94</volume><fpage>102982</fpage><pub-id pub-id-type="doi">10.1016/j.rcim.2025.102982</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05472-f001" orientation="portrait"><label>Figure 1</label><caption><p>Schematic diagram of the dynamic vision data collected by the event-based camera.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g001.jpg"/></fig><fig position="float" id="sensors-25-05472-f002" orientation="portrait"><label>Figure 2</label><caption><p>Illustration of the generation of the event representations. The positive event representation is shown for instance.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g002.jpg"/></fig><fig position="float" id="sensors-25-05472-f003" orientation="portrait"><label>Figure 3</label><caption><p>Illustration of the EViT. The EViT architecture features a convolutional stem followed by a four-stage pyramid. Within each stage, a 2 &#215; 2 convolution operating at a stride of 2 precedes multiple Bionic Eagle Vision (BEV) blocks. Structurally, every BEV block integrates three core elements: a Convolutional Positional Embedding (CPE), a Bi-Fovea Self-Attention (BFSA) module, and a Bi-Fovea Feedforward Network (BFFN).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g003.jpg"/></fig><fig position="float" id="sensors-25-05472-f004" orientation="portrait"><label>Figure 4</label><caption><p>General implementation flowchart of the proposed event vision- based fault diagnosis method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g004.jpg"/></fig><fig position="float" id="sensors-25-05472-f005" orientation="portrait"><label>Figure 5</label><caption><p>Test rig of the rotating machine condition monitoring problem in this study.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g005.jpg"/></fig><fig position="float" id="sensors-25-05472-f006" orientation="portrait"><label>Figure 6</label><caption><p>Schematic diagrams of the three types of bearings.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g006.jpg"/></fig><fig position="float" id="sensors-25-05472-f007" orientation="portrait"><label>Figure 7</label><caption><p>Confusion matrices of two models in the fault diagnosis tasks A1 and A3.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g007.jpg"/></fig><fig position="float" id="sensors-25-05472-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visualization results of the learned features from the data by different models in the tasks A1 and A3.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g008.jpg"/></fig><fig position="float" id="sensors-25-05472-f009" orientation="portrait"><label>Figure 9</label><caption><p>Effects of each module in the EViT model on fault diagnosis.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g009.jpg"/></fig><fig position="float" id="sensors-25-05472-f010" orientation="portrait"><label>Figure 10</label><caption><p>Effects of the ROI of the event frame on the fault diagnosis model performance in different tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05472-g010.jpg"/></fig><table-wrap position="float" id="sensors-25-05472-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05472-t001_Table 1</object-id><label>Table 1</label><caption><p>Fault diagnosis tasks in this study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">A1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">A2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">A3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">A4</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">model</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">condition (r/min)</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" rowspan="1" colspan="1">1800</td><td align="center" valign="middle" rowspan="1" colspan="1">1200, 1500, 1800</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training samples</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" rowspan="1" colspan="1">6000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Testing samples</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Task</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">B1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">B2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">B3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">B4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">model</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">condition (r/min)</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" rowspan="1" colspan="1">1800</td><td align="center" valign="middle" rowspan="1" colspan="1">1200, 1500, 1800</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training samples</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" rowspan="1" colspan="1">6000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Testing samples</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3000</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05472-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05472-t002_Table 2</object-id><label>Table 2</label><caption><p>Parameters used in this study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#951;</td><td align="center" valign="middle" rowspan="1" colspan="1">1 &#215; 10<sup>&#8722;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Epochs</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">300</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation Function</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ReLU</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05472-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05472-t003_Table 3</object-id><label>Table 3</label><caption><p>Testing accuracy of different methods in different fault diagnosis tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CNN</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">EViT</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">A1</td><td align="center" valign="middle" rowspan="1" colspan="1">96.1 &#177; 0.2</td><td align="center" valign="middle" rowspan="1" colspan="1">B1</td><td align="center" valign="middle" rowspan="1" colspan="1">98.8 &#177; 0.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A2</td><td align="center" valign="middle" rowspan="1" colspan="1">95.8 &#177; 0.3</td><td align="center" valign="middle" rowspan="1" colspan="1">B2</td><td align="center" valign="middle" rowspan="1" colspan="1">98.6 &#177; 0.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A3</td><td align="center" valign="middle" rowspan="1" colspan="1">97.2 &#177; 0.3</td><td align="center" valign="middle" rowspan="1" colspan="1">B3</td><td align="center" valign="middle" rowspan="1" colspan="1">99.3 &#177; 0.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.9 &#177; 0.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">B4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.5 &#177; 0.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05472-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05472-t004_Table 4</object-id><label>Table 4</label><caption><p>Experimental results in the fault diagnosis tasks with variations in machine operating conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task<break/>Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model<break/>Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Sample No.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Condition (r/min)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Testing Sample No.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Testing Condition (r/min)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Testing Accuracy (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">C1</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">200<break/>2000</td><td align="center" valign="middle" rowspan="1" colspan="1">1200<break/>1500</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">89.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C2</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" rowspan="1" colspan="1">200<break/>2000</td><td align="center" valign="middle" rowspan="1" colspan="1">1200<break/>1500</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C3</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">83.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C4</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">84.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D1</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">200<break/>2000</td><td align="center" valign="middle" rowspan="1" colspan="1">1500<break/>1800</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D2</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" rowspan="1" colspan="1">200<break/>2000</td><td align="center" valign="middle" rowspan="1" colspan="1">1500<break/>1800</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" rowspan="1" colspan="1">92.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D3</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td><td align="center" valign="middle" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" rowspan="1" colspan="1">82.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05472-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05472-t005_Table 5</object-id><label>Table 5</label><caption><p>Experimental results in the fault diagnosis tasks with different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Condition (r/min)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Testing Accuracy (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">E1</td><td align="center" valign="middle" rowspan="1" colspan="1">EViT</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">99.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">E2</td><td align="center" valign="middle" rowspan="1" colspan="1">Model 1</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">96.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">E3</td><td align="center" valign="middle" rowspan="1" colspan="1">Model 2</td><td align="center" valign="middle" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" rowspan="1" colspan="1">97.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">E4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Model 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.6</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>