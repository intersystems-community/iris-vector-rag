<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">J Biomed Opt</journal-id><journal-id journal-id-type="iso-abbrev">J Biomed Opt</journal-id><journal-id journal-id-type="pmc-domain-id">953</journal-id><journal-id journal-id-type="pmc-domain">jbiomedopt</journal-id><journal-id journal-id-type="publisher-id">JBO</journal-id><journal-title-group><journal-title>Journal of Biomedical Optics</journal-title></journal-title-group><issn pub-type="ppub">1083-3668</issn><issn pub-type="epub">1560-2281</issn><publisher><publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431673</article-id><article-id pub-id-type="pmcid-ver">PMC12431673.1</article-id><article-id pub-id-type="pmcaid">12431673</article-id><article-id pub-id-type="pmcaiid">12431673</article-id><article-id pub-id-type="doi">10.1117/1.JBO.30.S3.S34109</article-id><article-id pub-id-type="publisher-id">250228SSR</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Special Issue Honoring Brian C. Wilson, Pioneer in Biomedical Optics</subject></subj-group><subj-group subj-group-type="SPIE-art-type"><subject>Paper</subject></subj-group></article-categories><title-group><article-title>Deep learning-enabled fluorescence imaging for oral cancer margin classification in preclinical models</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Kurosawa</surname><given-names initials="H">Hikaru</given-names></name><xref rid="aff1" ref-type="aff">a</xref><email>h.kurosawa@mail.utoronto.ca</email></contrib><contrib contrib-type="author"><name name-style="western"><surname>Won</surname><given-names initials="NJ">Natalie J.</given-names></name><xref rid="aff1" ref-type="aff">a</xref><email>natalie.won@uhn.ca</email></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wunder</surname><given-names initials="JB">Jack B.</given-names></name><xref rid="aff1" ref-type="aff">a</xref><email>JackBenjamin.Wunder@uhn.ca</email></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0002-8792-0143</contrib-id><name name-style="western"><surname>Patil</surname><given-names initials="S">Sujit</given-names></name><xref rid="aff1" ref-type="aff">a</xref><email>Sujit.Patil@uhn.ca</email></contrib><contrib contrib-type="author"><name name-style="western"><surname>Bartling</surname><given-names initials="M">Mandolin</given-names></name><xref rid="aff2" ref-type="aff">b</xref><email>mandolin.bartling@mail.utoronto.ca</email></contrib><contrib contrib-type="author"><name name-style="western"><surname>Najjar</surname><given-names initials="E">Esmat</given-names></name><xref rid="aff2" ref-type="aff">b</xref><email>Esmat.Najjar@uhn.ca</email></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7973-3212</contrib-id><name name-style="western"><surname>Tzelnick</surname><given-names initials="S">Sharon</given-names></name><xref rid="aff2" ref-type="aff">b</xref><email>sharon.tzelnick@uhn.ca</email></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5543-666X</contrib-id><name name-style="western"><surname>Wilson</surname><given-names initials="BC">Brian C.</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="aff3" ref-type="aff">c</xref><email>Brian.wilson@uhnresearch.ca</email></contrib><contrib contrib-type="author"><name name-style="western"><surname>Irish</surname><given-names initials="JC">Jonathan C.</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="aff2" ref-type="aff">b</xref><email>jonathan.irish@uhn.on.ca</email></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2217-4146</contrib-id><name name-style="western"><surname>Daly</surname><given-names initials="MJ">Michael J.</given-names></name><xref rid="aff1" ref-type="aff">a</xref><xref rid="cor1" ref-type="corresp">*</xref><email>michael.daly@uhn.ca</email></contrib><aff id="aff1"><label>a</label><institution>University Health Network</institution>, Princess Margaret Cancer Centre, Toronto, Ontario, <country>Canada</country></aff><aff id="aff2"><label>b</label><institution>University of Toronto</institution>, Department of Otolaryngology-Head and Neck Surgery, Toronto, Ontario, <country>Canada</country></aff><aff id="aff3"><label>c</label><institution>University of Toronto</institution>, Department of Medical Biophysics, Toronto, Ontario, <country>Canada</country></aff></contrib-group><author-notes><corresp id="cor1"><label>*</label>Address all correspondence to Michael J. Daly, <email>michael.daly@uhn.ca</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="ppub"><month>12</month><year>2025</year></pub-date><volume>30</volume><issue>Suppl 3</issue><issue-id pub-id-type="pmc-issue-id">489476</issue-id><elocation-id>S34109</elocation-id><history><date date-type="received"><day>24</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>26</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>12</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 The Authors</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>The Authors</copyright-holder><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="JBO-030-S34109.pdf"/><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:title="pdf" xlink:href="JBO_30_S3_S34109.pdf"/><abstract><title>Abstract.</title><sec><title>Significance</title><p>Oral cancer surgery demands precise margin delineation to ensure complete tumor resection (healthy tissue margin <inline-formula><mml:math id="math1" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&gt;</mml:mo><mml:mn>5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>) while preserving postoperative functionality. Inadequate margins most frequently occur at the deep surgical margins, where tumors are located beneath the tissue surface; however, current fluorescent optical imaging systems are limited by their inability to quantify subsurface structures. Combining structured light techniques with deep learning may enable intraoperative margin assessment of 3D surgical specimens.</p></sec><sec><title>Aim</title><p>A deep learning (DL)-enabled spatial frequency domain imaging (SFDI) system is investigated to provide subsurface depth quantification of fluorescent inclusions.</p></sec><sec><title>Approach</title><p>A diffusion theory-based numerical simulation of SFDI was used to generate synthetic images for DL training. ResNet and U-Net convolutional neural networks were developed to predict margin distance (subsurface depth) and fluorophore concentration from fluorescence images and optical property maps. Validation was conducted using <italic toggle="yes">in silico</italic> SFDI images of composite spherical harmonics, as well as simulated and phantom datasets of patient-derived tongue tumor shapes. Further testing was done in <italic toggle="yes">ex vivo</italic> animal tissue with fluorescent inclusions.</p></sec><sec><title>Results</title><p>For oral cancer optical properties, the U-Net DL model predicted the overall depth, concentration, and closest depth with errors of <inline-formula><mml:math id="math2" display="inline" overflow="scroll"><mml:mrow><mml:mn>1.43</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.84</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="math3" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.26</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.63</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="math4" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.33</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.31</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, using <italic toggle="yes">in silico</italic> patient-derived tongue shapes with closest depths below 10&#160;mm. In PpIX fluorescent phantoms of inclusion depths up to 8&#160;mm, the closest subsurface depth was predicted with an error of <inline-formula><mml:math id="math5" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.57</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.38</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>. For <italic toggle="yes">ex vivo</italic> tissue, the closest distance to the fluorescent inclusions with depths up to 6&#160;mm was predicted with an error of <inline-formula><mml:math id="math6" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.59</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.53</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec><sec><title>Conclusions</title><p>A DL-enabled SFDI system trained with <italic toggle="yes">in silico</italic> images demonstrates promise in providing margin assessment of oral cancer tumors.</p></sec></abstract><kwd-group><title>Keywords:</title><kwd>molecular guided surgery</kwd><kwd>depth-resolved fluorescence imaging</kwd><kwd>spatial frequency domain imaging</kwd><kwd>deep learning</kwd><kwd>oral cancer surgery</kwd></kwd-group><funding-group><award-group id="sp1"><funding-source>Raymond Ng &amp; Wendy Chui Foundation for Innovation in Otolaryngology&#8211;Head &amp; Neck Surgery</funding-source></award-group><award-group id="sp2"><funding-source>Terry Fox Research Institute (TFRI) New Frontiers Program Project</funding-source><award-id>1137</award-id></award-group><award-group id="sp3"><funding-source>Princess Margaret Cancer Foundation</funding-source></award-group><funding-statement>This work was supported by the Raymond Ng &amp; Wendy Chui Foundation for Innovation in Otolaryngology&#8211;Head &amp; Neck Surgery (Department of Otolaryngology&#8211;Head &amp; Neck Surgery, University of Toronto), Terry Fox Research Institute (TFRI) New Frontiers Program Project (Grant No. 1137), and the Princess Margaret Cancer Foundation (Allison AI for Cancer Scholarship, Lucky Bean Foundation GTx STEM Research Scholarship, Garron Family GTx Surgery&#8212;Engineering Fund).</funding-statement></funding-group><counts><fig-count count="9"/><table-count count="2"/><ref-count count="41"/><page-count count="14"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>running-head</meta-name><meta-value>Kurosawa et&#160;al.: Deep learning-enabled fluorescence imaging for oral cancer margin&#8230;</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="sec1"><label>1</label><title>Introduction</title><p>Oral cancer surgery exhibits high rates of inadequate margins relative to other common solid tumors.<xref rid="r1" ref-type="bibr"><sup>1</sup></xref> Recent clinical studies report inadequate margin rates of over 30%,<xref rid="r2" ref-type="bibr"><sup>2</sup></xref> which have a direct negative impact on patient outcomes.<xref rid="r3" ref-type="bibr"><sup>3</sup></xref><named-content content-type="online"><xref rid="r4" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#8211;</sup></named-content><xref rid="r5" ref-type="bibr"><sup>5</sup></xref> In the oral cavity, as per the National Comprehensive Cancer Network, the closest margin distance&#8212;distance from the tumor to the specimen surface&#8212;of <inline-formula><mml:math id="math7" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&lt;</mml:mo><mml:mn>1</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> is defined as positive, 1 to 5&#160;mm as close, and <inline-formula><mml:math id="math8" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&gt;</mml:mo><mml:mn>5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> as clear.<xref rid="r6" ref-type="bibr"><sup>6</sup></xref><sup>,</sup><xref rid="r7" ref-type="bibr"><sup>7</sup></xref> A key challenge in oral cancer surgery is determining the extent of tumor infiltration into surrounding tissue, as more than 87% of inadequate (positive and close) margins occur in these cases.<xref rid="r8" ref-type="bibr"><sup>8</sup></xref> Most oral cancers originate on a mucosal surface (e.g., tongue), which may provide visual cues to help delineate the lateral extension of disease prior to resection. When assessing tumor depth, however, surgeons must rely on preoperative imaging and intraoperative palpation, both of which can lack precision.<xref rid="r9" ref-type="bibr"><sup>9</sup></xref> The challenges of intraoperative surgical margin assessment, particularly along the deep resection surface, present the need for improved intraoperative techniques.</p><p>Recent clinical trials in fluorescence-guided oral cancer surgery provide promising results for resection guidance and margin assessment using tumor-specific contrast agents.<xref rid="r10" ref-type="bibr"><sup>10</sup></xref><named-content content-type="online"><xref rid="r11" ref-type="bibr"/><xref rid="r12" ref-type="bibr"/><xref rid="r13" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#8211;</sup></named-content><xref rid="r14" ref-type="bibr"><sup>14</sup></xref> These trials also provide insight into key limiting factors that motivate additional research. For margin assessment in resected specimens, de Wit et&#160;al.<xref rid="r13" ref-type="bibr"><sup>13</sup></xref> showed decreased accuracy across margin thickness, with area under the curve (AUC) values of 0.95 for tumor-positive (<inline-formula><mml:math id="math9" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&lt;</mml:mo><mml:mn>1</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>), 0.78 for close margins (1 to 3&#160;mm), and 0.65 for close margins (3 to 5&#160;mm). Although near-infrared (NIR) fluorophores (650 to 950&#160;nm) provide qualitative subsurface visualization due to low absorption within the biological imaging window,<xref rid="r15" ref-type="bibr"><sup>15</sup></xref> a quantitative, depth-resolved imaging system may offer improvement.</p><p>Several studies have investigated the use of fluorescence molecular imaging devices to resolve subsurface fluorescent structures. Rounds et&#160;al.<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> demonstrated a dual-aperture fluorescence ratio approach to classify surgical margins of oral squamous cell carcinoma resections in an initial patient group. Multiwavelength approaches have been explored to predict the depths and concentrations of subsurface objects,<xref rid="r17" ref-type="bibr"><sup>17</sup></xref><named-content content-type="online"><xref rid="r18" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#8211;</sup></named-content><xref rid="r19" ref-type="bibr"><sup>19</sup></xref> by exploiting penetration depth variations due to changes in optical properties across the excitation and/or emission band. Spatial frequency domain imaging (SFDI) systems, which provide depth sensitivity across spatial frequencies,<xref rid="r20" ref-type="bibr"><sup>20</sup></xref><sup>,</sup><xref rid="r21" ref-type="bibr"><sup>21</sup></xref> have also been investigated for subsurface depth quantification in preclinical models.<xref rid="r18" ref-type="bibr"><sup>18</sup></xref><sup>,</sup><xref rid="r22" ref-type="bibr"><sup>22</sup></xref><sup>,</sup><xref rid="r23" ref-type="bibr"><sup>23</sup></xref> In contrast to multiwavelength approaches, SFDI necessitates the use of a structured light source, making instrumentation more complex, but it avoids the dependence on optical property variations across fluorophore bands. Given these tradeoffs, multiwavelength and SFDI may be complementary (e.g., multiwavelength SFDI may offer potential improvements over the single-wavelength approach we present here). In prior SFDI work, Sibai et&#160;al.<xref rid="r22" ref-type="bibr"><sup>22</sup></xref> utilized the decay rate of the modulation amplitude of the fluorescence signal to calculate subsurface fluorophore depth, modeling the object as a point fluorescent source. The algorithm demonstrated depth recovery up to sub-surface depths of 9.5&#160;mm in brain-like optical properties. Smith et&#160;al.<xref rid="r23" ref-type="bibr"><sup>23</sup></xref> proposed a deep learning (DL) driven method that coupled the use of fluorescent lifetime imaging and SFDI-based optical property maps to predict the depth of fluorescent inclusions. In both studies, however, the algorithm validation was limited to simple inclusion shapes such as a cylindrical glass capillary, and the system capacity in more complex scenarios is not explored. Specifically, our group aims to develop an SFDI system that can predict subsurface depths of complex tumor shapes encountered in oral cancer cases.</p><p>Here, we build upon our previous work in fluorescence depth imaging for oral cancer surgery by applying a DL-enabled SFDI system (SFDI-DL) to preclinical models of margin assessment. Our earlier work by Won et&#160;al.<xref rid="r24" ref-type="bibr"><sup>24</sup></xref> demonstrated the utility of this technology to quantify the depth of the bottom surface of infiltrative tumors. This assumed an <italic toggle="yes">in vivo</italic> imaging scenario of the mucosal surface of oral cancer. In this study, we explore the application of SFDI-DL in a post-resection scenario, assessing surgical margins of the basal surface of the <italic toggle="yes">ex vivo</italic> surgical specimen. We focus on imaging the deep surface of the specimen as it is most often involved in the presence of positive and close surgical margins.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> In our previous work, we adopted the Siamese convolutional neural network (CNN) architecture, using spatial frequency fluorescence images and optical property maps as inputs and tumor thickness and concentration as outputs.<xref rid="r24" ref-type="bibr"><sup>24</sup></xref> The inputs were encoded using a ResNet structure, which is characterized by the use of residual blocks that include identity shortcut connections, facilitating information from previous layers to be carried forward.<xref rid="r25" ref-type="bibr"><sup>25</sup></xref> This relatively simple architecture performed well for <italic toggle="yes">in silico</italic> and phantom test cases of infiltrative scenarios. With tumors originating at the surface, sufficient signal was obtained at every spatial frequency. However, in a surgical margin assessment scenario, less fluorescence signals are present at higher spatial frequencies, which sample only the shallow regions. This necessitates the extraction of relevant features from fewer spatial frequency images (i.e., low spatial frequency), and our previous ResNet model with a limited number of learned parameters may not be suited for this task. Therefore, the use of a deeper model architecture, U-Net, is explored in this paper, as it is commonly used in biomedical dense prediction tasks.<xref rid="r26" ref-type="bibr"><sup>26</sup></xref> In general, computational power not only scales with the number of parameters and the depth of the architecture but also improves performance. The ResNet architecture retains the same lateral dimensions throughout the entire path, whereas the U-Net structure condenses the lateral dimensions of the feature maps in the intermediate encoder path. This allows a deeper model such as U-Net to be trained even with similar graphics processing unit (GPU) memory demands as in ResNet. Here, the performance of the ResNet and U-Net models was compared to evaluate the potential benefits of the deeper U-Net model in learning subsurface depth quantification. Protoporphyrin IX (PpIX) is used as a model fluorophore with red illumination (630&#160;nm); in principle, the algorithms are generalizable to other NIR fluorophores (e.g., IRdye800CW). This paper first reports on updates to the DL architecture and <italic toggle="yes">in silico</italic> training approach, followed by evaluations in simulations, phantoms, and <italic toggle="yes">ex vivo</italic> animal tissue models.</p></sec><sec id="sec2"><label>2</label><title>Methods</title><sec id="sec2.1"><label>2.1</label><title>Deep Learning Architectures</title><p>Our previous study used the Siamese ResNet CNN to predict the thickness and fluorophore concentration of infiltrative oral cancer tumor models originating at the surface.<xref rid="r24" ref-type="bibr"><sup>24</sup></xref> The architecture is displayed in <xref rid="f1" ref-type="fig">Fig.&#160;1(a)</xref> (2,000,642 parameters), which we refer to as ResNet. In this study, we extend upon this previous work and also adopt the attention U-Net architecture, as shown in <xref rid="f1" ref-type="fig">Fig.&#160;1(b)</xref> (33,145,143 parameters), which we refer to as U-Net. The two models primarily differed in their method of encoding in the intermediate layers, in which ResNet used a residual block while U-Net used max-pooling layers.<xref rid="r25" ref-type="bibr"><sup>25</sup></xref><sup>,</sup><xref rid="r26" ref-type="bibr"><sup>26</sup></xref></p><fig position="float" id="f1" orientation="portrait"><label>Fig. 1</label><caption><p>(a)&#160;ResNet and (b)&#160;Attention U-Net architecture diagram with fluorescence images across spatial frequency (<inline-formula><mml:math id="math10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and optical properties (<inline-formula><mml:math id="math11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>) as inputs to predict the margin distance (subsurface depth of fluorescent inclusion) and fluorophore concentration as 2D maps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g001.jpg"/></fig><p>In ResNet, the fluorescence images acquired at six different spatial frequencies and optical property maps (absorption and reduced scattering) were processed separately through convolutional layers. Fluorescence images were processed using 3D filters to extract spatial information laterally (<inline-formula><mml:math id="math13" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math14" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> axes) and in depth (across spatial frequencies), whereas optical property maps were processed using 2D filters. Residual blocks were used before and after concatenation of the two branches to encode relevant features. The extracted feature maps went through two separate 2D convolution branches to output the predicted fluorophore concentration and depth to the top surface of a buried fluorescent object.</p><p>In U-Net, similar to ResNet, the fluorescence images and optical property maps were processed in separate branches using 3D CNN and 2D CNN, respectively. The two branches were concatenated and underwent an encoder&#8211;decoder structure to extract image features at different image scales.<xref rid="r26" ref-type="bibr"><sup>26</sup></xref> Max pooling layers were used to reduce the dimensionality of the feature maps and preserve the most useful information for further processing.<xref rid="r27" ref-type="bibr"><sup>27</sup></xref> Upsampling layers were included to recover the resolution of the feature maps. In between the encoder and decoder paths, long skip connections from the encoder path were passed through the attention gates to disregard irrelevant information and support the decoding process.<xref rid="r28" ref-type="bibr"><sup>28</sup></xref> Similar to ResNet, the extracted feature maps were processed using two separate 2D convolution branches to predict the fluorophore concentration and subsurface fluorescent depth.</p><p>In both models, the subsurface depth of the fluorescent object measured along the imaging axis is defined as the margin distance. Each convolutional layer was followed by a ReLU activation function to introduce nonlinearity. A separate model, with a dropout layer after each convolutional layer before concatenation, was also trained specifically for testing on phantom and <italic toggle="yes">ex vivo</italic> datasets to provide robustness against overfitting.<xref rid="r29" ref-type="bibr"><sup>29</sup></xref></p></sec><sec id="sec2.2"><label>2.2</label><title>Deep Learning Training</title><p>The DL architectures were trained with <italic toggle="yes">in silico</italic> data using a synthetic tumor shape model based on spherical harmonics and an in-house numerical diffusion theory light propagation model, described in detail previously.<xref rid="r24" ref-type="bibr"><sup>24</sup></xref> Here, 10,000 synthetic tumor shapes were generated using composite spherical harmonics (CSH) in MATLAB. CSH were created by merging four spherical harmonics with the following randomized parameters: order and degree between 2 and 6, width between 5 and 40&#160;mm, height between 5 and 10&#160;mm, and closest top surface depth between 1 and 10&#160;mm. An example CSH shape is displayed in <xref rid="f2" ref-type="fig">Fig.&#160;2(a)</xref>. These shapes were inputted into the numerical light propagation model to produce synthetic reflectance and fluorescence images (<inline-formula><mml:math id="math15" display="inline" overflow="scroll"><mml:mrow><mml:mn>101</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>101</mml:mn><mml:mrow><mml:mtext>&#8201;&#8201;</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pixels</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="math16" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mtext>pixel</mml:mtext></mml:mrow></mml:math></inline-formula>) across six spatial frequencies (<inline-formula><mml:math id="math17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.15</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.25</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). Simulation time was &#8764;1&#160;h. Reflectance images were further processed to estimate absorption and scattering coefficient maps using SFDI lookup tables computed at two spatial frequencies (<inline-formula><mml:math id="math18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math19" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.2</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>).<xref rid="r20" ref-type="bibr"><sup>20</sup></xref> The training set was generated for an excitation wavelength of 630&#160;nm, with randomly assigned homogeneous optical properties in each tissue: absorption <inline-formula><mml:math id="math20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between 0.0015 and <inline-formula><mml:math id="math21" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.015</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, scattering <inline-formula><mml:math id="math22" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> between 0.75 and <inline-formula><mml:math id="math23" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and protoporphyrin IX (PpIX) fluorophore concentration between 1 and <inline-formula><mml:math id="math24" display="inline" overflow="scroll"><mml:mrow><mml:mn>10</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula> with quantum efficiency (<inline-formula><mml:math id="math25" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow></mml:math></inline-formula>) of 0.046 per tumor case.<xref rid="r30" ref-type="bibr"><sup>30</sup></xref> This absorption range corresponded to a total hemoglobin (THb) concentration between 0.5 and <inline-formula><mml:math id="math26" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:math></inline-formula> and oxygen saturation of 95%. In addition, a randomized amount of fluorescence was added to the healthy tissue background, ranging from 0.1% to 50% of the tumor fluorescence concentration. For testing on <italic toggle="yes">ex vivo</italic> animal tissue with lower scattering than typical human tissue, a second training set was generated with absorption <inline-formula><mml:math id="math27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between 0.01 and <inline-formula><mml:math id="math28" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.3</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and scattering <inline-formula><mml:math id="math29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> between 0.3 and <inline-formula><mml:math id="math30" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.7</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to ensure coverage of the observed optical properties within the training range. All data was inputted into the DL model on Amazon Web Services SageMaker with an ml.g5.2xlarge instance type (1 NVIDIA A10G GPU, 8 vCPU) for <inline-formula><mml:math id="math31" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&#8764;</mml:mo><mml:mn>2</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:math></inline-formula> of training per data set. A depth of 10&#160;mm was assigned to all background points to ensure the closest distance in the depth map corresponded to the tumor body rather than healthy tissue.</p><fig position="float" id="f2" orientation="portrait"><label>Fig. 2</label><caption><p>3D tumor shape models used for deep learning testing. (a)&#160;Composite spherical harmonics and (b)&#160;MRI-contoured patient tongue tumor shape. (c)&#160;Illustration of a tumor (green) submerged inside healthy tissue (red) with the deep surface oriented upwards.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g002.jpg"/></fig></sec><sec id="sec2.3"><label>2.3</label><title><italic toggle="yes">In Silico</italic> Testing</title><p>For <italic toggle="yes">in silico</italic> testing, 1000 CSH tumor shapes were newly generated with the same parameter ranges as the training set. The test cases had fixed optical properties (<inline-formula><mml:math id="math32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0045</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="math33" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>), fluorophore concentration (<inline-formula><mml:math id="math34" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula>), and randomized healthy tissue fluorescence between 0.1% and 50% of the tumor fluorophore concentration. These values fall within nominal ranges for both healthy oral tissue and cancer.<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> To evaluate the capacity of SFDI-DL to make predictions for tumor shapes with complex features representative of real tumors, oral cancer tumor meshes were extracted by contouring preoperative MRI images of tongue cancer patients as displayed in <xref rid="f2" ref-type="fig">Fig.&#160;2(b)</xref>. This was performed under institutional ethics board approval for retrospective patient data access (University Health Network REB #22-5471). For all 20 patient-derived data, the deep surface (at time of resection) was oriented upwards, and a layer of background tissue was created atop with a minimum margin distance of 1 to 10&#160;mm at 1&#160;mm intervals, for a total of 200 datasets. This orientation is illustrated in <xref rid="f2" ref-type="fig">Fig.&#160;2(c)</xref>.</p></sec><sec id="sec2.4"><label>2.4</label><title>SFDI Instrumentation</title><p>A prototype SFDI system has been developed for imaging. Further details of the system are detailed in previous work;<xref rid="r24" ref-type="bibr"><sup>24</sup></xref> briefly, the system combines a high-resolution projector (DLi6500 1080p Optics Bundle, DLi, Austin, Texas, United States), 14-bit monochrome charge-coupled device (CCD) camera (Pixelfly USB, PCO AG, Kelheim, Germany), light-emitting diode (LED) light engine (Spectra X, Lumencor, Beaverton, Oregon, United States), 675-nm-long pass filter (ET700/75m, Chroma Bellows Falls, VT, United States), and 6-position motorized filter wheel (88-171, Edmund Optics, Barrington, NJ, United States).</p><p>In phantom and <italic toggle="yes">ex vivo</italic> experiments, reflectance and fluorescence image sets were acquired at six spatial frequencies (<inline-formula><mml:math id="math35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.15</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.25</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) and three phase shifts ([0&#160;deg, 120&#160;deg, and 240&#160;deg]), totaling 36 images (18 reflectance and 18 fluorescence). Nominal acquisition times for each reflectance and fluorescence image were 24 and 4000&#160;ms, respectively. The camera lens aperture was set to an f/# of 2.8.</p></sec><sec id="sec2.5"><label>2.5</label><title>Phantom Testing</title><p>Further testing was conducted with real SFDI images obtained using optical phantoms. The phantom design process is shown in <xref rid="f3" ref-type="fig">Fig.&#160;3</xref>. Contoured meshes of the patient-derived tumor shapes were 3D printed (Original i3 MK3, Prusa, Prague, Czech Republic) to create a silicone (Ecoflex 00-30, Smooth-On, Easton, PA) negative mold for agar (AGA.501, BioShop, Burlington, Canada) phantoms. A solid fluorescent agar phantom (<inline-formula><mml:math id="math36" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mi>mm</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math37" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1.4</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) was created to represent the tumor region and a nonfluorescent liquid phantom without agar (<inline-formula><mml:math id="math38" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.011</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mi>mm</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math39" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>s</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mi>mm</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) for the healthy tissue region. The solid and liquid phantoms were made to approximately match the optical properties of tumor and healthy oral tissues, respectively. India Ink (Higgins PBk7, Chartpak Inc, Leeds, MA) was used as the absorbing agent and Intralipid (Fresenius Kabi, Toronto, Canada) as the scatterer. For the solid tumor phantom, PpIX (P8293, Sigma-Aldrich, Burlington, MA) was included as the fluorophore (<inline-formula><mml:math id="math40" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula>) and iohexol (Omnipaque, GE Healthcare, Chicago, IL) as the contrast agent for computed tomography (CT) scanning (<inline-formula><mml:math id="math41" display="inline" overflow="scroll"><mml:mrow><mml:mn>10</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula>). The tumor solution was poured into silicone negative molds, forming a solidified tumor phantom, as shown in <xref rid="f3" ref-type="fig">Fig.&#160;3(b)</xref>. This solid phantom was mounted onto circular pegs of a 3D-printed stage with the basal surface (i.e., most infiltrative surface) of the tumor oriented upwards. The stage was fixed onto the base of a customized container as shown in <xref rid="f3" ref-type="fig">Fig.&#160;3(c)</xref>. Both customized 3D-printed components were made from black polylactic acid (EconoFil Standard PLA Filament&#8211;1.75&#160;mm, InkSmith, Kitchener, Canada) (<inline-formula><mml:math id="math42" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mi>mm</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math43" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). The background liquid phantom was poured into the container until the tumor&#8217;s top surface was covered. Using the known container volume, liquid background was added to vary the closest subsurface depth to heights of 2, 4, 6, and 8&#160;mm. A fixed distance between the instrument and the liquid surface was maintained by lowering the adjustable stage by the same height as the added liquid. The prototype SFDI system used for data acquisition is shown in <xref rid="f3" ref-type="fig">Fig.&#160;3(d)</xref>.</p><fig position="float" id="f3" orientation="portrait"><label>Fig. 3</label><caption><p>(a)&#160;MRI-contoured patient-derived oral cancer tumor. (b)&#160;Fluorescent agar phantom (right) created using the contour and a stage with circular pegs (left). The solid phantom was mounted onto (c)&#160;a mounted optical phantom fixed in a 3D-printed container. (d)&#160;SFDI imaging liquid phantom.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g003.jpg"/></fig><p>A representative set of nine out of the 20 patient-derived shapes was used for the phantom experiment, as shown in <xref rid="f4" ref-type="fig">Fig.&#160;4</xref>. Those shapes were selected to cover a range of widths (20 to 40&#160;mm), thicknesses (5 to 15&#160;mm), and topographies. To obtain the true geometry of the tumor phantoms, cone-beam CT scans (Siemens Cios Spin) of these solid phantoms were acquired. The CT scans were contoured in 3D Slicer software.<xref rid="r32" ref-type="bibr"><sup>32</sup></xref> Four fiducial points located inside the container were selected to map the orientation of the 3D mesh in the CT space onto the coordinates of the acquired SFDI images, and the transformed CT data were used to obtain the ground truth depth map. The true concentration map was obtained by assigning the known fluorophore concentration of <inline-formula><mml:math id="math44" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula> to the tumor region.</p><fig position="float" id="f4" orientation="portrait"><label>Fig. 4</label><caption><p>3D models of nine patient-derived tumor shapes used for phantom testing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g004.jpg"/></fig></sec><sec id="sec2.6"><label>2.6</label><title><italic toggle="yes">Ex Vivo</italic> Testing</title><p>To validate with optically heterogeneous tissue, turkey breast was obtained from a local butcher and cut into rectangular blocks (<inline-formula><mml:math id="math45" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&#8764;</mml:mo><mml:mn>70</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> width and length, <inline-formula><mml:math id="math46" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&#8764;</mml:mo><mml:mn>15</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> height). Six tissue blocks were prepared in total, each with nominal optical properties (<inline-formula><mml:math id="math47" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.015</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math48" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.45</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) measured by the SFDI system. Semispherical molds were cut out at the center of these blocks to pour in agar solution for creating solid fluorescent tumor phantoms (<inline-formula><mml:math id="math49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math50" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1.4</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="math51" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>mL</mml:mi></mml:mrow></mml:math></inline-formula> PpIX). Variable thicknesses of tissue were preserved at the bottom surface of these molds to test different fluorescent inclusion heights. Once the phantom solidified, the tissue was flipped to mimic the margin assessment of infiltrative oral tumors in <italic toggle="yes">ex vivo</italic> specimens. The tumor shape contoured from the tissue phantom&#8217;s CT scan using 3D Slicer was manually aligned to match the agar phantom&#8217;s orientation during SFDI imaging. The same image processing steps done in 2.5 were applied and inputted to the prediction by the U-Net + dropout model. To extract ground truth margin thickness, 3D meshes for the turkey tissue and the agar phantom regions were extracted separately. Using MeshLab software,<xref rid="r33" ref-type="bibr"><sup>33</sup></xref> vertex-to-vertex distances between the tissue and tumor mesh were calculated to quantify the margin thickness at each point on the tissue mesh. Subsurface depths of the fluorescent inclusion in the tumor regions were further extracted from this map.</p></sec><sec id="sec2.7"><label>2.7</label><title>Analysis and Statistics</title><p>All analysis was conducted using Python 3.12.9.</p><p>For <italic toggle="yes">in silico</italic> test cases, the mean absolute error (MAE) and standard deviation (SD) were computed between the predicted and ground truth values of the depth map and fluorophore concentration maps over all pixels lying inside the tumor regions. MAE and SD at different depth ranges (0 to 5, 5 to 10, and 10 to 15&#160;mm) were also computed. The minimum values of the depth map were then used to compute the mean minimum margin distance (closest subsurface depth) error. Linear correlation between the predicted and true closest subsurface depth values was also determined using sklearn 1.6.1.</p><p>For phantom and <italic toggle="yes">ex vivo</italic> testing, only the closest subsurface depth was considered for analysis because margin assessment is primarily concerned with the closest margin distance.<xref rid="r6" ref-type="bibr"><sup>6</sup></xref> To evaluate the effects of both a deeper model and dropout on depth prediction, ResNet and U-Net architectures with and without dropouts were compared. The MAE, SD, and linear correlation of the minimum margin distance were computed with evaluate system performance. Significance (*<inline-formula><mml:math id="math52" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, **<inline-formula><mml:math id="math53" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, ***<inline-formula><mml:math id="math54" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>) was determined using an independent <inline-formula><mml:math id="math55" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> test (scipy 1.15.2) based on the MAE and SD of each model.</p><p>For <italic toggle="yes">ex vivo</italic> testing, the U-Net with dropout model was used for prediction. MAE, SD, and linear correlation of the closest margin distance were calculated using the values obtained from the true depth map and the DL prediction.</p></sec></sec><sec id="sec3"><label>3</label><title>Results</title><sec id="sec3.1"><label>3.1</label><title>Simulation Results</title><p>For <italic toggle="yes">in silico</italic> test cases, the DL predictions were generated using the U-Net model without dropout. For CSH, the subsurface depths and fluorophore concentration over all tumor regions were predicted with MAE of <inline-formula><mml:math id="math56" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.41</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.75</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math57" display="inline" overflow="scroll"><mml:mrow><mml:mn>1.41</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.24</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula>, respectively. In comparison, the patient-derived tumor shapes had higher subsurface and concentration errors of <inline-formula><mml:math id="math58" display="inline" overflow="scroll"><mml:mrow><mml:mn>1.43</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.84</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math59" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.26</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.63</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>ml</mml:mi></mml:mrow></mml:math></inline-formula>. The prediction and ground truth for the depth and concentration maps for a representative patient-derived tumor shape at shallow depth (closest distance = 2&#160;mm) and deep depth (closest distance = 6&#160;mm) are displayed in <xref rid="f5" ref-type="fig">Figs.&#160;5(a)</xref> and <xref rid="f5" ref-type="fig">5(b)</xref>, respectively. Patient-derived tumor shapes contained greater irregularity in topography with complex features such as a protrusion structure seen in <xref rid="f5" ref-type="fig">Fig.&#160;5(c)</xref>. The model predicted depth and concentration with minimal error for most points, but performance declined primarily in deeper regions (subsurface depth below <inline-formula><mml:math id="math60" display="inline" overflow="scroll"><mml:mrow><mml:mo form="prefix">&#8764;</mml:mo><mml:mn>10</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>) as seen from <xref rid="t001" ref-type="table">Table&#160;1</xref>.</p><fig position="float" id="f5" orientation="portrait"><label>Fig. 5</label><caption><p>Depth and concentration map results for representative patient-derived tumor shape cases with (a)&#160;a minimum depth (margin thickness) of 2&#160;mm and (b)&#160;a minimum depth 6&#160;mm (c)&#160;3D figure (right) of the tumor shape from (b)&#160;showing a protrusion (d)&#160;depth estimates of the minimum margin distance compared with the true depths for all composite spherical harmonics shapes (e)&#160;same for patient derived tumor shapes; the dashed line is the line of equality, and the best fit is indicated.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g005.jpg"/></fig><table-wrap position="float" id="t001" orientation="portrait"><label>Table 1</label><caption><p>MAE and SD at different depth ranges for CSH and patient-derived tumor shapes.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><th align="left" valign="top" colspan="1" rowspan="1">Tumor shape model</th><th align="center" valign="top" colspan="1" rowspan="1">Depths: 0 to 5&#160;mm</th><th align="center" valign="top" colspan="1" rowspan="1">Depths: 5 to 10&#160;mm</th><th align="center" valign="top" colspan="1" rowspan="1">Depths: 10 to 15&#160;mm</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">CSH</td><td align="center" colspan="1" rowspan="1">0.31 &#177; 0.86</td><td align="center" colspan="1" rowspan="1">0.42 &#177; 0.62</td><td align="center" colspan="1" rowspan="1">0.73 &#177; 0.75</td></tr><tr><td align="left" colspan="1" rowspan="1">Patient-derived</td><td align="center" colspan="1" rowspan="1">0.35 &#177; 0.74</td><td align="center" colspan="1" rowspan="1">0.69 &#177; 0.73</td><td align="center" colspan="1" rowspan="1">1.63 &#177; 1.25</td></tr></tbody></table></table-wrap><p>When error calculations are limited to the closest subsurface depth, which is most relevant for margin classification, the model predicted it with a mean minimum depth error of <inline-formula><mml:math id="math61" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.19</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.17</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> for CSH cases and <inline-formula><mml:math id="math62" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.33</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.31</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> for patient-derived tumor shapes. The minimum margin distance was predicted more accurately than the overall depth, with approximately one-half the error for CSH cases and one-quarter for patient-derived cases. As a result, both cases displayed strong agreement between the true and predicted closest margin depths, with <inline-formula><mml:math id="math63" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math></inline-formula> for CSH [<xref rid="f5" ref-type="fig">Fig.&#160;5(d)</xref>] and <inline-formula><mml:math id="math64" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.97</mml:mn></mml:mrow></mml:math></inline-formula> for patient shapes [<xref rid="f5" ref-type="fig">Fig.&#160;5(e)</xref>]. Given the primary application of the model for predicting the closest margin distance, subsequent analysis focuses only on the evaluation of minimum depth errors.</p></sec><sec id="sec3.2"><label>3.2</label><title>Phantom Results</title><p>To illustrate how SFDI enables depth sampling, <xref rid="f6" ref-type="fig">Fig.&#160;6</xref> displays fluorescence intensities for a single tumour shape across spatial frequencies of 0, 0.05, 0.1, 0.15, 0.2, and <inline-formula><mml:math id="math65" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.25</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> at closest subsurface depths of 2, 4, 6, and 8&#160;mm. As expected, as the inclusion got deeper, measured fluorescence intensity decreased, especially at the lowest spatial frequency. The average minimum depth error calculated using the four DL models, tested on nine tumor shapes at four depths each, is shown in <xref rid="f7" ref-type="fig">Fig.&#160;7</xref>. ResNet and U-Net models without dropout predicted the closest distance with errors of <inline-formula><mml:math id="math66" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.10</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.50</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="math67" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>0.33</mml:mn></mml:mrow></mml:math></inline-formula>) and <inline-formula><mml:math id="math68" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.95</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.36</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="math69" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1.1</mml:mn></mml:mrow></mml:math></inline-formula>), respectively. When dropout was applied, ResNet and U-Net models predicted with an average error of <inline-formula><mml:math id="math70" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.85</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.58</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="math71" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.79</mml:mn></mml:mrow></mml:math></inline-formula>) and <inline-formula><mml:math id="math72" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.55</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.38</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="math73" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.91</mml:mn></mml:mrow></mml:math></inline-formula>), respectively. The U-Net model with dropout attained significant improvement over the rest of the models (<inline-formula><mml:math id="math74" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>), and therefore, this model was used for subsequent testing.</p><fig position="float" id="f6" orientation="portrait"><label>Fig. 6</label><caption><p>Diffuse fluorescence images at six spatial frequencies from different depths in the liquid phantom. Color bars correspond to unitless diffuse fluorescence values.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g006.jpg"/></fig><fig position="float" id="f7" orientation="portrait"><label>Fig. 7</label><caption><p>Mean closest margin distance error with SD for performance comparison in ResNet and U-Net models with and without dropouts. Statistical significance indicated (*<inline-formula><mml:math id="math75" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, ***<inline-formula><mml:math id="math76" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g007.jpg"/></fig><p>Predicted depth and concentration maps from the U-Net + dropout model are compared with ground truth across all four depths for a representative tumor case, shown in <xref rid="f8" ref-type="fig">Fig.&#160;8(a)</xref>. The minimum margin distance predictions for all nine tumor shapes at the four different depths are displayed in <xref rid="f8" ref-type="fig">Fig.&#160;8(b)</xref>. Although the differences in minimum depth error across depths were insignificant, deeper depths (6 and 8&#160;mm) exhibited greater error and SD compared with shallower depths (2 and 4&#160;mm), with an error of <inline-formula><mml:math id="math77" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.60</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.44</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math78" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.51</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.31</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>, respectively.</p><fig position="float" id="f8" orientation="portrait"><label>Fig. 8</label><caption><p>(a)&#160;Predicted and true depth and concentration maps at different depths using the U-Net model with dropout. (b)&#160;Minimum margin distance estimate compared with known true depth (line of equality shown with dashed line and corresponding best fit is indicated).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g008.jpg"/></fig></sec><sec id="sec3.3"><label>3.3</label><title><italic toggle="yes">Ex Vivo</italic> Results</title><p>Fluorescence images of a representative tissue case with a fluorescent inclusion are displayed in <xref rid="f9" ref-type="fig">Fig.&#160;9(a)</xref>. A relatively high signal to background ratio at lower spatial frequencies (<inline-formula><mml:math id="math79" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="math80" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.05</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mi>mm</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) is observed compared with the higher spatial frequencies (<inline-formula><mml:math id="math81" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, 0.15, 0.2, <inline-formula><mml:math id="math82" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.25</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mi>mm</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). The 3D margin thickness graph for the same case, obtained from CT scan-derived meshes, is displayed in <xref rid="f9" ref-type="fig">Fig.&#160;9(b)</xref>. The fluorescent inclusion was located at a subsurface depth of 5.45&#160;mm. The true and predicted depth and concentration maps are displayed in <xref rid="f9" ref-type="fig">Fig.&#160;9(c)</xref>, predicting the closest margin distance of 5.38&#160;mm. Similarly, the closest subsurface depths of all cases are summarized in <xref rid="t002" ref-type="table">Table&#160;2</xref>. For the six cases, the SFDI-DL system predicted the closest distance with MAE and SD of <inline-formula><mml:math id="math83" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.59</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.53</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>.</p><fig position="float" id="f9" orientation="portrait"><label>Fig. 9</label><caption><p>(a)&#160;Diffuse fluorescence images obtained at six different spatial frequencies of turkey meat with fluorescent inclusions (closest depth = 5.45&#160;mm) beneath the tissue surface; regions outside the cropped area are padded with mean background intensity values. Color bars correspond to unitless diffuse fluorescence values. (b)&#160;3D margin thickness surface rendering obtained using a CT scan. (c)&#160;True and predicted depth and concentration maps of an <italic toggle="yes">ex vivo</italic> tissue case using the U-Net + dropout model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="JBO-030-S34109-g009.jpg"/></fig><table-wrap position="float" id="t002" orientation="portrait"><label>Table 2</label><caption><p>True and predicted closest subsurface depths for six different tissue cases with fluorescent inclusions.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><th align="left" valign="top" colspan="1" rowspan="1">&#160;</th><th align="center" valign="top" colspan="1" rowspan="1">1</th><th align="center" valign="top" colspan="1" rowspan="1">2</th><th align="center" valign="top" colspan="1" rowspan="1">3</th><th align="center" valign="top" colspan="1" rowspan="1">4</th><th align="center" valign="top" colspan="1" rowspan="1">5</th><th align="center" valign="top" colspan="1" rowspan="1">6</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">True closest depth (mm)</td><td align="center" colspan="1" rowspan="1">5.05</td><td align="center" colspan="1" rowspan="1">1.25</td><td align="center" colspan="1" rowspan="1">5.45</td><td align="center" colspan="1" rowspan="1">4.73</td><td align="center" colspan="1" rowspan="1">6.30</td><td align="center" colspan="1" rowspan="1">5.75</td></tr><tr><td align="left" colspan="1" rowspan="1">Predicted closest depth (mm)</td><td align="center" colspan="1" rowspan="1">4.98</td><td align="center" colspan="1" rowspan="1">0.16</td><td align="center" colspan="1" rowspan="1">5.38</td><td align="center" colspan="1" rowspan="1">5.34</td><td align="center" colspan="1" rowspan="1">4.84</td><td align="center" colspan="1" rowspan="1">5.53</td></tr></tbody></table></table-wrap></sec></sec><sec id="sec4"><label>4</label><title>Discussion and Conclusion</title><p>This study demonstrates the feasibility of an <italic toggle="yes">in silico</italic> trained DL-enabled SFDI system to quantify margin distance in preclinical oral cancer tumor models. It serves as an initial proof-of-concept that buried fluorescent objects with complex geometries, as observed in oral cancer resections, can be resolved for subsurface depth predictions. Within the optical property ranges observed in a nominal oral cancer environment,<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> our SFDI-DL system detected the closest subsurface depth of a PpIX fluorescent inclusion with a mean error less than 0.6&#160;mm for structures as deep as 10&#160;mm in <italic toggle="yes">in silico</italic> test cases, 8&#160;mm in phantoms, and 6&#160;mm in <italic toggle="yes">ex vivo</italic> cases. This detection depth range is highly relevant to oral cancer surgery, where achieving a clear surgical margin of at least 5&#160;mm is standard.<xref rid="r6" ref-type="bibr"><sup>6</sup></xref> Thus, the proposed SFDI system presents potential as an intraoperative margin assessment tool to provide more rapid surgical feedback without the need for specimen sectioning.</p><p>Margin distance prediction over all points was less accurate for more complex patient tumor shapes compared with synthetic CSH. This reduced performance can be attributed to the increased surface irregularities in patient tumor shapes (e.g., steeper surface, protrusions, asymmetry, concave geometries). It may also be due to the difference in population distribution between the training and testing data. As our DL model was trained using CSH, it learned the feature characteristics of CSH shapes but may not generalize well to the complexity of patient-derived tumors. However, this is not a major limitation in our application of margin assessment, where the clinical objective is to identify the closest distance from the tumor to the specimen surface. The system demonstrated better accuracy for detecting the shallowest point than for depth prediction across the entire tumor, which aligns well with the primary clinical need. For other surgical scenarios (e.g., glioma resection); however, the applicability of this technology may vary due to differences in clinical objectives and tissue environments. Different tissue environments present different optical properties, which directly influence the effective sampling depth.<xref rid="r17" ref-type="bibr"><sup>17</sup></xref><sup>,</sup><xref rid="r21" ref-type="bibr"><sup>21</sup></xref> Conducting a prior investigation of the effective penetration depth range using simulation tools such as Monte Carlo may be beneficial in determining the suitability of the system to the desired application and conditions.</p><p>In this study, two DL architectures were compared with assess the utility of a deeper model for depth predictions. Although no significant difference in performance between the models was observed for <italic toggle="yes">in silico</italic> test cases, significance was attained when testing on real SFDI images. It is also important to note that this improvement was only possible with the inclusion of dropout layers at the beginning branch to perform well on the phantom data as seen in <xref rid="f7" ref-type="fig">Fig.&#160;7</xref>. DL aims to optimize for a nonconvex problem with multiple local minima. Training a single model on one dataset may converge toward a local minimum, which may lead to a suboptimal solution. A common machine learning solution that often yields better performance is model combination, which involves training multiple models with the same architecture and averaging the outputs at test time. However, training multiple DL models is computationally expensive. One technique that addresses this problem is dropout, which can be used at training time to approximate model combination. Hence, dropout layers are known to serve as regularizers that introduce noise to the inputs and make the system more robust against overfitting, and it presents potential in adjusting for the discrepancy between simulated and real SFDI images<xref rid="r29" ref-type="bibr"><sup>29</sup></xref><sup>,</sup><xref rid="r34" ref-type="bibr"><sup>34</sup></xref>; however, dropouts in convolutional layers reduce the spatial correlations of the images.<xref rid="r35" ref-type="bibr"><sup>35</sup></xref> The use of a deeper model such as U-Net may have allowed the system to accommodate this limitation, yielding improved performance compared with Res-Net with a dropout model. This final model predicted the closest subsurface depths in <italic toggle="yes">ex vivo</italic> models with similar accuracy as seen in the phantom experiment. This highlights the robustness of the system even when tested in heterogeneous tissue that resembles real biological environments.</p><p>Limitations in our study lie in the assumptions we have incorporated in the training and testing process. First, although diffusion theory-based simulations offer a computationally efficient approach to generating synthetic images, they rely on the assumption of isotropy. This is only valid when the scattering coefficient dominates the absorption coefficient.<xref rid="r36" ref-type="bibr"><sup>36</sup></xref> Monte Carlo simulations to better model light propagation are under development to help address this although they are much more computationally demanding. Second, our simulation used a simple shape model, CSH, for generating images. This shape model does not fully capture the complex nature of oral cancer tumor shapes such as irregular topography, protrusions, or the presence of &#8220;satellite buds.&#8221;<xref rid="r37" ref-type="bibr"><sup>37</sup></xref> More advanced shape modeling using mathematical expressions for cancer development or 3D generative AI may be beneficial.<xref rid="r38" ref-type="bibr"><sup>38</sup></xref><sup>,</sup><xref rid="r39" ref-type="bibr"><sup>39</sup></xref> Third, the fluorophore was assumed to be confined to the tumor volume and homogeneously distributed throughout. <italic toggle="yes">In vivo</italic> fluorophores, however, preferentially accumulate at the tumor site but often exhibit heterogeneous distribution and can leak into the surrounding tissue environment. In addition, it is known that some fluorophores (e.g., nanoparticles) tend to accumulate at the tumor periphery due to tumor vasculature.<xref rid="r40" ref-type="bibr"><sup>40</sup></xref> Although an initial <italic toggle="yes">in silico</italic> investigation using a background fluorescent and peripheral fluorophore localization model did not lead to performance degradation, the extent of its impact in phantom and <italic toggle="yes">ex vivo</italic> cases remains uncertain. Fourth, as discussed in previous work,<xref rid="r24" ref-type="bibr"><sup>24</sup></xref>
<italic toggle="yes">in vivo</italic> changes in fluorophore quantum efficiency may result in discrepancies with a trained model assuming a nominal value, which requires further investigation. Fifth, our phantom experiment design used a relatively small container (<inline-formula><mml:math id="math84" display="inline" overflow="scroll"><mml:mrow><mml:mn>11</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>11</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>cm</mml:mi></mml:mrow></mml:math></inline-formula>). Although a highly absorbent, low-scattering material was used to mitigate reflections from the container walls, some light scattering likely persisted. Monte Carlo simulations (data not shown) predicted less than a 10% difference in fluorescence at lower spatial frequencies between cases with and without the container. Nevertheless, the container introduces artifacts that may affect image accuracy. Optimal imaging conditions during phantom experiments and intraoperative scenarios should be further explored. Finally, the depth map outputs in this study quantified the subsurface depth of the fluorescent inclusion measured along the imaging axis. However, this may not necessarily correspond to the margin distance at each location. Imaging the same specimen from multiple angles may serve to provide several depth maps that may align closer with the true margin distance when used in combination. With a streamlined software pipeline, the entire data acquisition&#8212;imaging, processing, and depth estimation&#8212;is expected to take less than 2&#160;min. Upgrading to a more sensitive camera (e.g., from the current CCD to a cooled sCMOS) would decrease the exposure times required for fluorescence imaging, which is currently the longest segment in the acquisition and processing pipeline.</p><p>In this study, our training set and test cases for <italic toggle="yes">in silico</italic> and phantom experiments assumed a homogenous optical property for the tumor and the background regions, along with a flat top surface. However, this represents a simplification of real patient cases with heterogeneity in optical properties and nonflat surface topography. Although the <italic toggle="yes">ex vivo</italic> model demonstrated system capacity in heterogeneous optical property environments with a nonflat top surface, further testing is required with more extensive optical and topography variations such as those found in patient specimens.<xref rid="r13" ref-type="bibr"><sup>13</sup></xref> For real resections, variation in the tissue optical properties through the volume can be expected with different amounts of blood perfusion and oxygen saturation, affecting the light penetration depth and signal quality.<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> With a lower signal-to-background ratio, SFDI-DL performance is expected to be reduced, and future work will investigate the extent of the effect in higher absorption scenarios. Moreover, patient resections would only contain a limited amount of tissue at the sides. This creates a finite boundary to the space where light can propagate, which may introduce fluorescent artifacts into the image. Future work requires implementation in patient cases to assess the system&#8217;s sensitivity to these factors, as well as to compare SFDI-DL margin classification performance with standard clinical techniques and emerging optical methods.</p></sec></body><back><ack><title>Acknowledgments</title><p>The work in this paper is an extension of an SPIE proceedings paper.<xref rid="r41" ref-type="bibr"><sup>41</sup></xref>
<named-content content-type="funding-statement">This work was supported by the Raymond Ng &amp; Wendy Chui Foundation for Innovation in Otolaryngology&#8211;Head &amp; Neck Surgery (Department of Otolaryngology&#8211;Head &amp; Neck Surgery, University of Toronto), Terry Fox Research Institute (TFRI) New Frontiers Program Project (Grant No. 1137), and the Princess Margaret Cancer Foundation (Allison AI for Cancer Scholarship, Lucky Bean Foundation GTx STEM Research Scholarship, Garron Family GTx Surgery&#8212;Engineering Fund).</named-content> The authors thank Dr. Jason Smith for helpful discussions on his open-source deep learning code (Rensselaer Polytechnic Institute, PI: Dr. Xavier Intes), as cited, which this work built upon. Thanks also to the undergraduate co-op students whose previous work helped advance this project.</p></ack><bio id="d1260e3232" content-type="general"><p>Biographies of the authors are not available.</p></bio><sec sec-type="conflict"><title>Disclosures</title><p>The authors declare no relevant financial or nonfinancial interests to disclose.</p></sec><sec sec-type="data-availability"><title>Code and Data Availability</title><p>Available upon reasonable request to the corresponding author.</p></sec><ref-list><title>References</title><ref id="r1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Orosco</surname><given-names>R. K.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Positive surgical margins in the 10 most common solid cancers</article-title>,&#8221; <source>Sci. Rep.</source><volume>8</volume>(<issue>1</issue>), <fpage>5686</fpage> (<year>2018</year>).<pub-id pub-id-type="coden">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type="doi">10.1038/s41598-018-23403-5</pub-id><pub-id pub-id-type="pmid">29632347</pub-id><pub-id pub-id-type="pmcid">PMC5890246</pub-id></mixed-citation></ref><ref id="r2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>R. K.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Positive margin rates in oral cavity and oropharyngeal malignancies: a national analysis</article-title>,&#8221; <source>Otolaryngol. Head Neck Surg.</source><volume>172</volume>(<issue>3</issue>), <fpage>922</fpage>&#8211;<lpage>930</lpage> (<year>2025</year>).<pub-id pub-id-type="doi">10.1002/ohn.1087</pub-id><pub-id pub-id-type="pmid">39709538</pub-id></mixed-citation></ref><ref id="r3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>T.-C.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>The impact of pathologic close margin on the survival of patients with early stage oral squamous cell carcinoma</article-title>,&#8221; <source>Oral Oncol.</source><volume>48</volume>(<issue>7</issue>), <fpage>623</fpage>&#8211;<lpage>628</lpage> (<year>2012</year>).<pub-id pub-id-type="coden">EJCCER</pub-id><issn>1368-8375</issn><pub-id pub-id-type="doi">10.1016/j.oraloncology.2012.01.015</pub-id><pub-id pub-id-type="pmid">22349276</pub-id></mixed-citation></ref><ref id="r4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Solomon</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>The impact of close surgical margins on recurrence in oral squamous cell carcinoma</article-title>,&#8221; <source>J. Otolaryngol. Head Neck Surg.</source><volume>50</volume>(<issue>1</issue>), <fpage>9</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1186/s40463-020-00483-w</pub-id><pub-id pub-id-type="pmid">33579388</pub-id><pub-id pub-id-type="pmcid">PMC7881652</pub-id></mixed-citation></ref><ref id="r5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luryi</surname><given-names>A. L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Positive surgical margins in early stage oral cavity cancer: an analysis of 20,602 cases</article-title>,&#8221; <source>Otolaryngol. Head Neck Surg.</source><volume>151</volume>(<issue>6</issue>), <fpage>984</fpage>&#8211;<lpage>990</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1177/0194599814551718</pub-id><pub-id pub-id-type="pmid">25257901</pub-id></mixed-citation></ref><ref id="r6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hinni</surname><given-names>M. L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Surgical margins in head and neck cancer: a contemporary review</article-title>,&#8221; <source>Head Neck</source><volume>35</volume>(<issue>9</issue>), <fpage>1362</fpage>&#8211;<lpage>1370</lpage> (<year>2013</year>).<pub-id pub-id-type="doi">10.1002/hed.23110</pub-id><pub-id pub-id-type="pmid">22941934</pub-id></mixed-citation></ref><ref id="r7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Woolgar</surname><given-names>J. A.</given-names></name><name name-style="western"><surname>Triantafyllou</surname><given-names>A.</given-names></name></person-group>, &#8220;<article-title>A histopathological appraisal of surgical margins in oral and oropharyngeal cancer resection specimens</article-title>,&#8221; <source>Oral Oncol.</source><volume>41</volume>(<issue>10</issue>), <fpage>1034</fpage>&#8211;<lpage>1043</lpage> (<year>2005</year>).<pub-id pub-id-type="coden">EJCCER</pub-id><issn>1368-8375</issn><pub-id pub-id-type="doi">10.1016/j.oraloncology.2005.06.008</pub-id><pub-id pub-id-type="pmid">16129652</pub-id></mixed-citation></ref><ref id="r8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brouwer de Koning</surname><given-names>S. G.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Assessment of the deep resection margin during oral cancer surgery: a systematic review</article-title>,&#8221; <source>Eur. J. Surg. Oncol.</source><volume>47</volume>(<issue>9</issue>), <fpage>2220</fpage>&#8211;<lpage>2232</lpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1016/j.ejso.2021.04.016</pub-id><pub-id pub-id-type="pmid">33895027</pub-id></mixed-citation></ref><ref id="r9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rosenthal</surname><given-names>E. L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>The status of contemporary image-guided modalities in oncologic surgery</article-title>,&#8221; <source>Ann. Surg.</source><volume>261</volume>(<issue>1</issue>), <fpage>46</fpage>&#8211;<lpage>55</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1097/SLA.0000000000000622</pub-id><pub-id pub-id-type="pmid">25599326</pub-id><pub-id pub-id-type="pmcid">PMC4299947</pub-id></mixed-citation></ref><ref id="r10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rosenthal</surname><given-names>E. L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Safety and tumor specificity of Cetuximab-IRDye800 for surgical navigation in head and neck cancer</article-title>,&#8221; <source>Clin. Cancer Res.</source><volume>21</volume>(<issue>16</issue>), <fpage>3658</fpage>&#8211;<lpage>3666</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1158/1078-0432.CCR-14-3284</pub-id><pub-id pub-id-type="pmid">25904751</pub-id><pub-id pub-id-type="pmcid">PMC4909371</pub-id></mixed-citation></ref><ref id="r11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>van Keulen</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>The sentinel margin: intraoperative ex vivo specimen mapping using relative fluorescence intensity</article-title>,&#8221; <source>Clin. Cancer Res.</source><volume>25</volume>(<issue>15</issue>), <fpage>4656</fpage>&#8211;<lpage>4662</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1158/1078-0432.CCR-19-0319</pub-id><pub-id pub-id-type="pmid">31142505</pub-id><pub-id pub-id-type="pmcid">PMC7021202</pub-id></mixed-citation></ref><ref id="r12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Voskuil</surname><given-names>F. J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Exploiting metabolic acidosis in solid cancers using a tumor-agnostic pH-activatable nanoprobe for fluorescence-guided surgery</article-title>,&#8221; <source>Nat. Commun.</source><volume>11</volume>(<issue>1</issue>), <fpage>3257</fpage> (<year>2020</year>).<pub-id pub-id-type="coden">NCAOBW</pub-id><issn>2041-1723</issn><pub-id pub-id-type="doi">10.1038/s41467-020-16814-4</pub-id><pub-id pub-id-type="pmid">32591522</pub-id><pub-id pub-id-type="pmcid">PMC7320194</pub-id></mixed-citation></ref><ref id="r13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Wit</surname><given-names>J. G.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>EGFR-targeted fluorescence molecular imaging for intraoperative margin assessment in oral cancer patients: a phase II trial</article-title>,&#8221; <source>Nat. Commun.</source><volume>14</volume>(<issue>1</issue>), <fpage>4952</fpage> (<year>2023</year>).<pub-id pub-id-type="coden">NCAOBW</pub-id><issn>2041-1723</issn><pub-id pub-id-type="doi">10.1038/s41467-023-40324-8</pub-id><pub-id pub-id-type="pmid">37587149</pub-id><pub-id pub-id-type="pmcid">PMC10432510</pub-id></mixed-citation></ref><ref id="r14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bou-Samra</surname><given-names>P.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Intraoperative molecular imaging: 3rd biennial clinical trials update</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>28</volume>(<issue>5</issue>), <fpage>050901</fpage> (<year>2023</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.28.5.050901</pub-id><pub-id pub-id-type="pmid">37193364</pub-id><pub-id pub-id-type="pmcid">PMC10182831</pub-id></mixed-citation></ref><ref id="r15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Keereweer</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Optical image-guided cancer surgery: challenges and limitations</article-title>,&#8221; <source>Clin. Cancer Res.</source><volume>19</volume>(<issue>14</issue>), <fpage>3745</fpage>&#8211;<lpage>3754</lpage> (<year>2013</year>).<pub-id pub-id-type="doi">10.1158/1078-0432.CCR-12-3598</pub-id><pub-id pub-id-type="pmid">23674494</pub-id></mixed-citation></ref><ref id="r16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rounds</surname><given-names>C. C.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Improved intraoperative identification of close margins in oral squamous cell carcinoma resections using a dual aperture fluorescence ratio approach: first in-human results</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>29</volume>(<issue>1</issue>), <fpage>016003</fpage> (<year>2024</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.29.1.016003</pub-id><pub-id pub-id-type="pmid">38235321</pub-id><pub-id pub-id-type="pmcid">PMC10793906</pub-id></mixed-citation></ref><ref id="r17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Topographic mapping of subsurface fluorescent structures in tissue using multiwavelength excitation</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>15</volume>(<issue>6</issue>), <fpage>066026</fpage> (<year>2010</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.3523369</pub-id><pub-id pub-id-type="pmid">21198200</pub-id><pub-id pub-id-type="pmcid">PMC3022586</pub-id></mixed-citation></ref><ref id="r18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Konecky</surname><given-names>S. D.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Spatial frequency domain tomography of protoporphyrin IX fluorescence in preclinical glioma models</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>17</volume>(<issue>5</issue>), <fpage>056008</fpage> (<year>2012</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.17.5.056008</pub-id><pub-id pub-id-type="pmid">22612131</pub-id><pub-id pub-id-type="pmcid">PMC3381025</pub-id></mixed-citation></ref><ref id="r19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Svensson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Andersson-Engels</surname><given-names>S.</given-names></name></person-group>, &#8220;<article-title>Modeling of spectral changes for depth localization of fluorescent inclusion</article-title>,&#8221; <source>Opt. Express</source><volume>13</volume>(<issue>11</issue>), <fpage>4263</fpage>&#8211;<lpage>4274</lpage> (<year>2005</year>).<pub-id pub-id-type="coden">OPEXFF</pub-id><issn>1094-4087</issn><pub-id pub-id-type="doi">10.1364/OPEX.13.004263</pub-id><pub-id pub-id-type="pmid">19495341</pub-id></mixed-citation></ref><ref id="r20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cuccia</surname><given-names>D. J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Quantitation and mapping of tissue optical properties using modulated imaging</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>14</volume>(<issue>2</issue>), <fpage>024012</fpage> (<year>2009</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.3088140</pub-id><pub-id pub-id-type="pmid">19405742</pub-id><pub-id pub-id-type="pmcid">PMC2868524</pub-id></mixed-citation></ref><ref id="r21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hayakawa</surname><given-names>C. K.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Optical sampling depth in the spatial frequency domain</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>24</volume>(<issue>7</issue>), <fpage>071603</fpage> (<year>2018</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.24.7.071603</pub-id><pub-id pub-id-type="pmcid">PMC6675966</pub-id><pub-id pub-id-type="pmid">30218504</pub-id></mixed-citation></ref><ref id="r22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sibai</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Quantitative subsurface spatial frequency-domain fluorescence imaging for enhanced glioma resection</article-title>,&#8221; <source>J. Biophotonics</source><volume>12</volume>(<issue>5</issue>), <fpage>e201800271</fpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1002/jbio.201800271</pub-id><pub-id pub-id-type="pmcid">PMC6470016</pub-id><pub-id pub-id-type="pmid">30358162</pub-id></mixed-citation></ref><ref id="r23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>J. T.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Macroscopic fluorescence lifetime topography enhanced via spatial frequency domain imaging</article-title>,&#8221; <source>Opt. Lett.</source><volume>45</volume>(<issue>15</issue>), <fpage>4232</fpage>&#8211;<lpage>4235</lpage> (<year>2020</year>).<pub-id pub-id-type="coden">OPLEDP</pub-id><issn>0146-9592</issn><pub-id pub-id-type="doi">10.1364/OL.397605</pub-id><pub-id pub-id-type="pmid">32735266</pub-id><pub-id pub-id-type="pmcid">PMC7935427</pub-id></mixed-citation></ref><ref id="r24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Won</surname><given-names>N. J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep learning&#8211;enabled fluorescence imaging for surgical guidance: in silico training for oral cancer depth quantification</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>30</volume>(<issue>S1</issue>), <fpage>S13706</fpage> (<year>2024</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.30.S1.S13706</pub-id><pub-id pub-id-type="pmid">39295734</pub-id><pub-id pub-id-type="pmcid">PMC11408754</pub-id></mixed-citation></ref><ref id="r25"><label>25.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep residual learning for image recognition</article-title>,&#8221; arXiv:1512.03385 (<year>2015</year>).</mixed-citation></ref><ref id="r26"><label>26.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group>, &#8220;<article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>,&#8221; arXiv:1505.04597 (<year>2015</year>).</mixed-citation></ref><ref id="r27"><label>27.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Gholamalinezhad</surname><given-names>H.</given-names></name><name name-style="western"><surname>Khosravi</surname><given-names>H.</given-names></name></person-group>, &#8220;<article-title>Pooling methods in deep neural networks, a review</article-title>,&#8221; arXiv:2009.07485 (<year>2020</year>).</mixed-citation></ref><ref id="r28"><label>28.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Oktay</surname><given-names>O.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Attention U-Net: learning where to look for the pancreas</article-title>,&#8221; arXiv:1804.03999 (<year>2018</year>).</mixed-citation></ref><ref id="r29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>,&#8221; <source>J. Mach. Learn. Res.</source><volume>15</volume>(<issue>1</issue>), <fpage>1929</fpage>&#8211;<lpage>1958</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.5555/2627435.2670313</pub-id></mixed-citation></ref><ref id="r30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Flynn</surname><given-names>B. P.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>White light-informed optical properties improve ultrasound-guided fluorescence tomography of photoactive protoporphyrin IX</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>18</volume>(<issue>4</issue>), <fpage>046008</fpage> (<year>2013</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.JBO.18.4.046008</pub-id><pub-id pub-id-type="pmid">23584445</pub-id><pub-id pub-id-type="pmcid">PMC3639786</pub-id></mixed-citation></ref><ref id="r31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>F.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Assessment of the sensitivity and specificity of tissue-specific-based and anatomical-based optical biomarkers for rapid detection of human head and neck squamous cell carcinoma</article-title>,&#8221; <source>Oral Oncol.</source><volume>50</volume>(<issue>9</issue>), <fpage>848</fpage>&#8211;<lpage>856</lpage> (<year>2014</year>).<pub-id pub-id-type="coden">EJCCER</pub-id><issn>1368-8375</issn><pub-id pub-id-type="doi">10.1016/j.oraloncology.2014.06.015</pub-id><pub-id pub-id-type="pmid">25037162</pub-id><pub-id pub-id-type="pmcid">PMC4229069</pub-id></mixed-citation></ref><ref id="r32"><label>32.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kikinis</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pieper</surname><given-names>S. D.</given-names></name><name name-style="western"><surname>Vosburgh</surname><given-names>K. G.</given-names></name></person-group>, &#8220;<article-title>3D Slicer: a platform for subject-specific image analysis, visualization, and clinical support</article-title>,&#8221; in <source>Intraoperative Imaging and Image-Guided Therapy</source>, <person-group person-group-type="editor"><name name-style="western"><surname>Jolesz</surname><given-names>F. A.</given-names></name></person-group>, Ed., pp.&#160;<fpage>277</fpage>&#8211;<lpage>289</lpage>, <publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc> (<year>2014</year>).</mixed-citation></ref><ref id="r33"><label>33.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cignoni</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>MeshLab: an open-source mesh processing tool</article-title>,&#8221; <conf-name>presented at EGIC</conf-name> (<year>2008</year>).</mixed-citation></ref><ref id="r34"><label>34.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Gal</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ghahramani</surname><given-names>Z.</given-names></name></person-group>, &#8220;<article-title>Bayesian convolutional neural networks with Bernoulli approximate variational inference</article-title>,&#8221; arXiv:1506.02158 (<year>2016</year>).</mixed-citation></ref><ref id="r35"><label>35.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tompson</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Efficient object localization using convolutional networks</article-title>,&#8221; in <conf-name>IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR)</conf-name>, <publisher-loc>Boston, MA, USA</publisher-loc>, <publisher-name>IEEE</publisher-name>, pp.&#160;<fpage>648</fpage>&#8211;<lpage>656</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2015.7298664</pub-id></mixed-citation></ref><ref id="r36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jacques</surname><given-names>S. L.</given-names></name><name name-style="western"><surname>Pogue</surname><given-names>B. W.</given-names></name></person-group>, &#8220;<article-title>Tutorial on diffuse light transport</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>13</volume>(<issue>4</issue>), <fpage>041302</fpage> (<year>2008</year>).<pub-id pub-id-type="coden">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type="doi">10.1117/1.2967535</pub-id><pub-id pub-id-type="pmid">19021310</pub-id></mixed-citation></ref><ref id="r37"><label>37.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ferrari</surname><given-names>M.</given-names></name><name name-style="western"><surname>Montalto</surname><given-names>N.</given-names></name><name name-style="western"><surname>Nicolai</surname><given-names>P.</given-names></name></person-group>, <source>Novel Approaches in Surgical Management: How to Assess Surgical Margins</source>, pp.&#160;<fpage>95</fpage>&#8211;<lpage>110</lpage>, <publisher-name>Springer International Publishing</publisher-name> (<year>2021</year>).</mixed-citation></ref><ref id="r38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>A review of mathematical models for tumor dynamics and treatment resistance evolution of solid tumors</article-title>,&#8221; <source>CPT Pharmacometr. Syst. Pharmacol.</source><volume>8</volume>(<issue>10</issue>), <fpage>720</fpage>&#8211;<lpage>737</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1002/psp4.12450</pub-id><pub-id pub-id-type="pmcid">PMC6813171</pub-id><pub-id pub-id-type="pmid">31250989</pub-id></mixed-citation></ref><ref id="r39"><label>39.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</article-title>,&#8221; in <conf-name>Adv. in Neural Inform. Process. Syst.</conf-name>, <publisher-name>Curran Associates, Inc.</publisher-name>, Vol.&#160;29 (<year>2016</year>).</mixed-citation></ref><ref id="r40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sahovaler</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Nanoparticle-mediated photodynamic therapy as a method to ablate oral cavity squamous cell carcinoma in preclinical models</article-title>,&#8221; <source>Cancer Res. Commun.</source><volume>4</volume>(<issue>3</issue>), <fpage>796</fpage>&#8211;<lpage>810</lpage> (<year>2024</year>).<pub-id pub-id-type="doi">10.1158/2767-9764.CRC-23-0269</pub-id><pub-id pub-id-type="pmid">38421899</pub-id><pub-id pub-id-type="pmcid">PMC10941731</pub-id></mixed-citation></ref><ref id="r41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kurosawa</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Spatial-frequency 3D fluorescence for surgical guidance: margin thickness quantification</article-title>,&#8221; <source>Proc. SPIE</source><volume>13301</volume>, <fpage>1330102</fpage> (<year>2025</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.3042275</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>