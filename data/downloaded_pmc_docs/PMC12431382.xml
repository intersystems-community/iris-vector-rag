<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431382</article-id><article-id pub-id-type="pmcid-ver">PMC12431382.1</article-id><article-id pub-id-type="pmcaid">12431382</article-id><article-id pub-id-type="pmcaiid">12431382</article-id><article-id pub-id-type="doi">10.3390/s25175310</article-id><article-id pub-id-type="publisher-id">sensors-25-05310</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Defect Detection in GIS X-Ray Images Based on Improved YOLOv10</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9923-4725</contrib-id><name name-style="western"><surname>Xu</surname><given-names initials="G">Guoliang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-05310" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-7698-0059</contrib-id><name name-style="western"><surname>Bai</surname><given-names initials="X">Xiaolong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-4565-939X</contrib-id><name name-style="western"><surname>Huang</surname><given-names initials="M">Menghao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Chen</surname><given-names initials="H">Honggang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05310">School of Communications and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing 400065, China; <email>s230101002@stu.cqupt.edu.cn</email> (X.B.); <email>s230132048@stu.cqupt.edu.cn</email> (M.H.)</aff><author-notes><corresp id="c1-sensors-25-05310"><label>*</label>Correspondence: <email>xugl@cqupt.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5310</elocation-id><history><date date-type="received"><day>20</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>24</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 13:25:28.783"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05310.pdf"/><abstract><p>Timely and accurate detection of internal defects in Gas-Insulated Switchgear (GIS) with X-ray imaging is critical for power system reliability. However, automated detection faces significant challenges from small, low-contrast defects and complex background structures. This paper proposes an enhanced object-detection model based on the lightweight YOLOv10n framework, specifically optimized for this task. Key improvements include adopting the Normalized Wasserstein Distance (NWD) loss function for small object localization, integrating Monte Carlo (MCAttn) and Parallelized Patch-Aware (PPA) attention to enhance feature extraction, and designing a GFPN-inspired neck for improved multi-scale feature fusion. The model was rigorously evaluated on a custom GIS X-ray dataset. The final model achieved a mean Average Precision (mAP) of 0.674 (IoU 0.5:0.95), representing a 5.0 percentage point improvement over the YOLOv10n baseline and surpassing other comparative models. Qualitative results also confirmed the model&#8217;s enhanced capability in detecting challenging small and low-contrast defects. This study presents an effective approach for automated GIS defect detection, with significant potential to enhance power grid maintenance efficiency and safety.</p></abstract><kwd-group><kwd>defect detection</kwd><kwd>Gas-Insulated Switchgear (GIS)</kwd><kwd>YOLOv10</kwd><kwd>MCAttn</kwd><kwd>PPA</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62171072</award-id></award-group><award-group><funding-source>State Energy Group Ningxia Electric Power Co., Ltd.</funding-source><award-id>SGNXNCNYCLJS2400261</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China (Grant No. 62171072) and State Energy Group Ningxia Electric Power Co., Ltd. (Grant No. SGNXNCNYCLJS2400261).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05310"><title>1. Introduction</title><p>Gas-Insulated Switchgear (GIS), characterized by its compact design, high integration level, stable performance, and low maintenance costs, has been widely adopted in modern power systems&#160;[<xref rid="B1-sensors-25-05310" ref-type="bibr">1</xref>]. As critical nodes in the power grid, the safe and stable operation of GIS directly impacts the reliability of the entire power system. However, during manufacturing, transportation, installation, and long-term operation, internal defects such as metallic particles, sharp protrusions on conductors, bubbles or cracks within insulating components, and foreign objects can arise in GIS&#160;[<xref rid="B1-sensors-25-05310" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05310" ref-type="bibr">2</xref>]. If not detected promptly, these potential defects can escalate into severe faults, jeopardizing grid security. Therefore, regular and effective internal condition monitoring of GIS, particularly the early identification and diagnosis of defects, is paramount for implementing preventive maintenance and ensuring power grid safety.</p><p>Multiple diagnostic methods are currently available for assessing the internal state of Gas-Insulated Switchgear (GIS), including Ultra-High Frequency (UHF) detection of partial discharges (PD) detection, ultrasonic detection, and <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> gas decomposition product detection&#160;[<xref rid="B2-sensors-25-05310" ref-type="bibr">2</xref>]. While these methods are effective in detecting discharge-related insulation defects, their capability is limited for physical defects that do not produce significant discharge signals, such as minor structural deformations, non-discharging foreign objects, or incipient cracks. In contrast, X-ray digital imaging modalities, such as Computed Radiography (CR) and Digital Radiography (DR), serve as non-contact, non-destructive inspection tools capable of penetrating the GIS metallic enclosure to directly visualize various physical defects, regardless of whether they generate discharges&#160;[<xref rid="B3-sensors-25-05310" ref-type="bibr">3</xref>]. This endows X-ray inspection with unique advantages and substantial application potential in the field of GIS defect diagnosis&#160;[<xref rid="B2-sensors-25-05310" ref-type="bibr">2</xref>]. Nevertheless, automated defect recognition in X-ray images poses three major challenges. First, the minute size and morphological diversity of critical defects (e.g., gas voids, micro-cracks, or metallic particles) may result in representations spanning only a few pixels&#160;[<xref rid="B4-sensors-25-05310" ref-type="bibr">4</xref>]. Second, the low contrast caused by minimal X-ray absorption differences between defects and surrounding materials leads to blurred feature boundaries. Finally, the intricate GIS internal architecture introduces overlapping projections and complex background noise, which may occlude or mimic defect signatures. The interpretation of X-ray images currently relies heavily on manual expertise, which is inefficient, costly, and subjective&#160;[<xref rid="B2-sensors-25-05310" ref-type="bibr">2</xref>], urgently necessitating the development of automated and intelligent detection technologies.</p><p>Traditional image processing-based defect-detection algorithms, such as threshold segmentation, morphological filtering, and edge detection&#160;[<xref rid="B5-sensors-25-05310" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05310" ref-type="bibr">6</xref>], while applicable in certain specific scenarios, are generally highly sensitive to image quality and parameter settings. Other recent approaches have also explored novel paradigms such as training-free learning for GIS image analysis&#160;[<xref rid="B7-sensors-25-05310" ref-type="bibr">7</xref>]. However, these methods can be inadequate for handling the full complexity and diversity of GIS X-ray images. In recent years, deep learning, particularly Convolutional Neural Networks (CNNs), has emerged as the mainstream approach in object detection&#160;[<xref rid="B4-sensors-25-05310" ref-type="bibr">4</xref>]. Single-stage detectors, notably the You Only Look Once (YOLO) series&#160;[<xref rid="B4-sensors-25-05310" ref-type="bibr">4</xref>], have garnered significant attention for their effective balance between detection speed and accuracy. From the initial YOLOv1&#160;[<xref rid="B8-sensors-25-05310" ref-type="bibr">8</xref>] and YOLOv3&#160;[<xref rid="B9-sensors-25-05310" ref-type="bibr">9</xref>], to YOLOv4&#160;[<xref rid="B10-sensors-25-05310" ref-type="bibr">10</xref>] and YOLOv5&#160;[<xref rid="B11-sensors-25-05310" ref-type="bibr">11</xref>], which incorporated CSPNet&#160;[<xref rid="B12-sensors-25-05310" ref-type="bibr">12</xref>] and PANet&#160;[<xref rid="B13-sensors-25-05310" ref-type="bibr">13</xref>], and more recently YOLOv8&#160;[<xref rid="B14-sensors-25-05310" ref-type="bibr">14</xref>], YOLOv9&#160;[<xref rid="B15-sensors-25-05310" ref-type="bibr">15</xref>], and YOLOv10&#160;[<xref rid="B16-sensors-25-05310" ref-type="bibr">16</xref>], and the very recent YOLOv11&#160;[<xref rid="B17-sensors-25-05310" ref-type="bibr">17</xref>], and even YOLOv12&#160;[<xref rid="B18-sensors-25-05310" ref-type="bibr">18</xref>], the YOLO framework has continuously evolved with steadily improving performance&#160;[<xref rid="B4-sensors-25-05310" ref-type="bibr">4</xref>]. Previous research has applied YOLOv5 to pattern recognition of GIS partial discharge PRPD patterns&#160;[<xref rid="B1-sensors-25-05310" ref-type="bibr">1</xref>], demonstrating the potential of the YOLO framework in this domain. However, directly applying general-purpose YOLO models to raw X-ray images for defect detection still encounters the aforementioned challenges: standard loss functions are insensitive to minuscule targets&#160;[<xref rid="B19-sensors-25-05310" ref-type="bibr">19</xref>]; fixed network architectures may not optimally extract low-contrast, detail-rich X-ray features; and standard multi-scale fusion mechanisms (e.g., FPN&#160;[<xref rid="B20-sensors-25-05310" ref-type="bibr">20</xref>], PANet&#160;[<xref rid="B13-sensors-25-05310" ref-type="bibr">13</xref>]) may not be optimal for GIS defects, which exhibit significant size variations&#160;[<xref rid="B21-sensors-25-05310" ref-type="bibr">21</xref>]. Therefore, in-depth optimization of YOLO models tailored to the characteristics of GIS X-ray images is essential.</p><p>To address the aforementioned issues and challenges, this paper adopts the lightweight and high-performance YOLOv10n as its baseline model. This choice was made based on the technological context at the time the experimental design and model development were finalized, when YOLOv10n provided an optimal trade-off between accuracy, speed, and computational efficiency for our specialized task. Although YOLOv11&#160;[<xref rid="B17-sensors-25-05310" ref-type="bibr">17</xref>] and YOLOv12&#160;[<xref rid="B18-sensors-25-05310" ref-type="bibr">18</xref>] have since been released, integrating them into this study would require a complete re-tuning of the training pipeline, hyperparameters, and improvement modules, which lies beyond the scope of this work. Our focus here is to demonstrate the effectiveness of the proposed improvement strategies on a stable and well-established baseline. Nevertheless, the extension and evaluation of these strategies on the latest YOLO architectures will be an important direction for future research. Building upon this baseline, we propose a series of synergistic improvement strategies aimed at constructing an enhanced model more suitable for defect detection in GIS X-ray images. The key enhancements comprise: adopting the Normalized Wasserstein Distance (NWD) loss function to strengthen the localization capability for small targets; introducing Monte Carlo Attention (MCAttn)&#160;[<xref rid="B22-sensors-25-05310" ref-type="bibr">22</xref>] and Parallelized Patch-Aware Attention (PPA)&#160;[<xref rid="B23-sensors-25-05310" ref-type="bibr">23</xref>] into the backbone network to enhance feature discriminability and robustness in complex backgrounds; and designing a novel neck network structure inspired by the concept of enhanced multi-scale interaction from the Generalized Feature Pyramid Network (GFPN)&#160;[<xref rid="B21-sensors-25-05310" ref-type="bibr">21</xref>]. Through the organic integration of these modules, we aim to achieve high-precision, high-efficiency detection of various internal defects in GIS, especially for challenging samples.</p><p>The main contributions of this paper can be summarized as follows:<list list-type="bullet"><list-item><p>A deeply improved object-detection model based on YOLOv10n is proposed, specifically tailored to the characteristics of GIS X-ray images.</p></list-item><list-item><p>The NWD loss function is applied to the task of GIS X-ray defect detection, and its effectiveness in enhancing the localization accuracy of minute defects is validated.</p></list-item><list-item><p>Two advanced attention mechanisms, MCAttn and PPA, are integrated into the backbone network of YOLOv10, exploring and demonstrating their potential in enhancing feature representation for X-ray images.</p></list-item><list-item><p>A neck network structure inspired by the GFPN concept is designed and implemented to improve the fusion of multi-scale defect features.</p></list-item><list-item><p>Comprehensive experimental validation is conducted on a GIS X-ray dataset collected from real-world industrial scenarios, with results indicating that the proposed model significantly outperforms the baseline and several other state-of-the-art models of similar types.</p></list-item></list></p><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-05310" ref-type="sec">Section 2</xref> details the proposed model architecture and its key improved components. <xref rid="sec3-sensors-25-05310" ref-type="sec">Section 3</xref> describes the dataset used for experiments, evaluation metrics, and specific implementation details. It also provides an in-depth analysis of the experimental results, including ablation studies and performance comparisons with existing advanced methods. <xref rid="sec4-sensors-25-05310" ref-type="sec">Section 4</xref> summarizes the entire work and outlines future research directions.</p></sec><sec id="sec2-sensors-25-05310"><title>2. Proposed Method</title><p>To address the challenges in detecting defects in GIS X-ray images, which are inherently characterized by small target sizes, low contrast, and complex backgrounds, this paper proposes an enhanced defect-detection method based on the YOLOv10n framework. To improve the model&#8217;s performance on this specific task, we have systematically enhanced the baseline model (YOLOv10n), focusing on three core aspects:<list list-type="simple"><list-item><label>1.</label><p>Loss Function: At the loss function level, the NWD loss is adopted to replace traditional bounding box losses based on Intersection over Union (IoU), to better address the localization issues of small targets.</p></list-item><list-item><label>2.</label><p>Backbone Network: At the backbone network level, the MCAttn mechanism is incorporated into the core C2f feature extraction modules, replacing the original C2f modules at the 2nd, 4th, and 6th layers, and PPA is used to replace the C2f module at the 8th layer. This aims to enhance the network&#8217;s ability to extract and perceive critical defect features.</p></list-item><list-item><label>3.</label><p>Neck Structure: At the neck network level, a new multi-scale feature fusion structure is designed, inspired by the principles of GFPN, to improve adaptability to defects of varying sizes.</p></list-item></list></p><p>The overall network architecture of the proposed method is illustrated in <xref rid="sensors-25-05310-f001" ref-type="fig">Figure 1</xref>. This architecture integrates all the aforementioned improvements. The subsequent subsections will elaborate on the specific design of the NWD loss function application (<xref rid="sec2dot1-sensors-25-05310" ref-type="sec">Section 2.1</xref>), the attention mechanism enhancements in the backbone network (<xref rid="sec2dot2-sensors-25-05310" ref-type="sec">Section 2.2</xref> and <xref rid="sec2dot3-sensors-25-05310" ref-type="sec">Section 2.3</xref>), and the GFPN-inspired neck network structure (<xref rid="sec2dot4-sensors-25-05310" ref-type="sec">Section 2.4</xref>).</p><sec id="sec2dot1-sensors-25-05310"><title>2.1. NWD Loss</title><p>Common bounding box regression loss functions in object detection, such as those based on IoU such as GIoU, DIoU, and CIoU, perform well in general object-detection tasks. However, they exhibit certain limitations when dealing with small defect targets commonly found in GIS X-ray images, as in this study. Specifically, these loss functions are highly sensitive to the positional offsets of small targets; even minor pixel deviations can cause drastic changes in the IoU value, potentially reducing it to zero. This instability complicates model training and hinders the precise regression of small target bounding boxes.</p><p>To overcome this challenge, this paper introduces NWD&#160;[<xref rid="B19-sensors-25-05310" ref-type="bibr">19</xref>] as both a metric and a loss function for bounding boxes. The core idea of NWD is to model a bounding box <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the center coordinates, and <italic toggle="yes">w</italic> and <italic toggle="yes">h</italic> are the width and height, respectively) and its internal pixel weight distribution as a two-dimensional Gaussian distribution <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>,</mml:mo><mml:mo>&#931;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The mean is <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the covariance matrix is <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-sans-serif">&#931;</mml:mi><mml:mo>=</mml:mo><mml:mi>diag</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>4</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Instead of directly calculating the overlapping area, NWD employs the Wasserstein distance (also known as Earth Mover&#8217;s distance) to measure the similarity between the Gaussian distributions <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> corresponding to two bounding boxes (e.g., a predicted box <italic toggle="yes">p</italic> and a ground truth box <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). The squared second-order Wasserstein distance between them, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, can be efficiently computed as:<disp-formula id="FD1-sensors-25-05310"><label>(1)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#956;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#956;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#956;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#956;</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the center coordinate vectors of the predicted box and the ground-truth box, respectively; <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are their corresponding width and height; and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the squared Euclidean distance. This distance, as shown in Equation&#160;(<xref rid="FD1-sensors-25-05310" ref-type="disp-formula">1</xref>), considers both the deviation in center points and the differences in the dimensions of the bounding box (width and height).</p><p>Compared to IoU-based losses, NWD offers significant advantages, particularly in small object detection. (1) Even if two bounding boxes do not overlap at all (where the IoU gradient would be zero), NWD can still provide smooth and informative gradients, thereby facilitating continuous model optimization. (2) NWD is insensitive to object scale and provides a more principled way to measure the similarity between small targets, exhibiting greater robustness to positional and size deviations of minute objects. These characteristics make NWD highly suitable for enhancing the localization accuracy of small defects in GIS X-ray images.</p><p>Based on the Wasserstein distance, the NWD similarity is defined as:<disp-formula id="FD2-sensors-25-05310"><label>(2)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt><mml:mi>C</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the Gaussian distributions corresponding to the predicted and ground-truth boxes, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the squared Wasserstein distance from Equation&#160;(<xref rid="FD1-sensors-25-05310" ref-type="disp-formula">1</xref>), and <italic toggle="yes">C</italic> is a constant used to scale the distance, typically set based on the statistics of the dataset (e.g., average target size). The similarity of the NWD ranges from <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where a value closer to 1 indicates a greater similarity between the two boxes. Finally, the NWD loss function <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:math></inline-formula> denotes loss) adopted in this paper is calculated as follows:<disp-formula id="FD3-sensors-25-05310"><label>(3)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the final NWD loss, and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the similarity metric defined in Equation&#160;(<xref rid="FD2-sensors-25-05310" ref-type="disp-formula">2</xref>). The range of this loss function <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; a higher NWD similarity results in a smaller loss value. In the model proposed in this paper, we replace the original bounding box regression loss branch of YOLOv10n with this <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>W</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> loss (see Equation&#160;(<xref rid="FD3-sensors-25-05310" ref-type="disp-formula">3</xref>)), aiming to achieve superior detection performance for minute defects.</p></sec><sec id="sec2dot2-sensors-25-05310"><title>2.2. C2f_MCAttn Module</title><p>To further enhance the model&#8217;s capability to extract subtle defect features from complex GIS X-ray backgrounds, this paper improves the core Convolutional-to-Feature (C2f) modules within the YOLOv10n backbone network. Although standard C2f modules achieve a balance between efficiency and performance, their feature extraction ability still has room for improvement when processing X-ray images with low signal-to-noise ratios and poor feature discriminability, often leading to the neglect of critical defect details or interference from background textures.</p><p>For this purpose, we chose to embed the MCAttn mechanism&#160;[<xref rid="B22-sensors-25-05310" ref-type="bibr">22</xref>] within the C2f modules used at the P2, P3, and P4 feature levels of the backbone network (corresponding to the 2nd, 4th, and 6th layers in the overall architecture shown in <xref rid="sensors-25-05310-f001" ref-type="fig">Figure 1</xref>). This forms the C2f_MCAttn unit (its structure is shown on the left side of <xref rid="sensors-25-05310-f002" ref-type="fig">Figure 2</xref>). Unlike the baseline C2f, the MCAttn module is deployed after the final 1 &#215; 1 convolutional layer of the C2f unit, performing dynamic, data-driven feature recalibration on the module&#8217;s output high-level features <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The core innovation of the MCAttn mechanism&#160;[<xref rid="B22-sensors-25-05310" ref-type="bibr">22</xref>] lies in its use of randomization strategies and multi-scale information aggregation to generate attention weights that are more robust and insensitive to scale variations. Its key operational principles can be broken down into the following steps, as illustrated in <xref rid="sensors-25-05310-f002" ref-type="fig">Figure 2</xref>:<list list-type="order"><list-item><p>Multi-Scale Contextual Feature Extraction: As depicted in the right panel of <xref rid="sensors-25-05310-f002" ref-type="fig">Figure 2</xref>, the process begins with multi-scale feature extraction. The input features <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> are passed through parallel adaptive average pooling operations with different output spatial resolutions (e.g., a preset set <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). This step, represented by the &#8220;Pooling&#8221; block in the diagram, captures feature statistical summaries <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>Pool</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> under different receptive fields.</p></list-item><list-item><p>Monte Carlo Attention Sampling: During the model training phase, one or a combination of feature representations <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is randomly sampled from the aforementioned set of multi-scale feature summaries. This stochastic sampling enhances the model&#8217;s robustness and prevents overfitting to specific patterns in the training data.</p></list-item><list-item><p>Channel Attention Generation and Application: The sampled features <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are then fed into a lightweight attention generation network. This network, represented by the subsequent series of operations in the diagram, is typically a Multi-Layer Perceptron (MLP) inspired by the Squeeze-and-Excitation (SE)&#160;[<xref rid="B24-sensors-25-05310" ref-type="bibr">24</xref>] structure. It comprises two linear layers and non-linear activations to learn the importance of each channel, ultimately outputting a channel attention vector <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>MLP</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This vector <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:math></inline-formula> is then applied to the original features <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> via element-wise multiplication (denoted by &#8855; in the final step of the diagram) to achieve adaptive feature channel recalibration:<disp-formula id="FD4-sensors-25-05310"><label>(4)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#8855;</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the final output feature map, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> is the input feature map from the C2f module, <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:math></inline-formula> is the computed channel attention vector, and &#8855; denotes element-wise multiplication.</p></list-item></list></p><p>Integrating MCAttn into C2f modules fundamentally aims to significantly improve the quality of the network&#8217;s feature representations. We anticipate that this enhancement will: (1) strengthen the model&#8217;s ability to discriminate features of defect targets in GIS X-ray images that vary widely in size, morphology, and signal-to-noise ratio; (2) improve the model&#8217;s generalization performance and robustness to interferences such as noise by introducing randomness during training; and (3) more effectively suppress task-irrelevant background features, enabling subsequent network layers to focus on information more pertinent to defect classification and localization.</p></sec><sec id="sec2dot3-sensors-25-05310"><title>2.3. PPA Module</title><p>At deeper backbone stages (e.g., the P5 stage, Layer 8 in <xref rid="sensors-25-05310-f001" ref-type="fig">Figure 1</xref>), repeated downsampling causes feature maps to progressively lose high-frequency spatial details essential for identifying minute defects, even as they gain semantic richness. Standard C2f modules often fail to balance the aggregation of this semantic context with the preservation of fine-grained features. To overcome this limitation, this paper replaces the C2f module at this stage with the Parallelized Patch-Aware Attention (PPA) module&#160;[<xref rid="B23-sensors-25-05310" ref-type="bibr">23</xref>]. Originally designed for infrared small target detection, PPA utilizes a parallel multi-branch architecture to capture features at multiple scales, which are then refined using subsequent attention mechanisms.</p><p>The overall structural design of the PPA module is illustrated in <xref rid="sensors-25-05310-f003" ref-type="fig">Figure 3</xref> and primarily consists of two core stages:</p><p>(1) Parallel Multi-Branch Feature Extraction: For an input feature <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:math></inline-formula>&#8242; (typically obtained after channel adjustment via an initial 1 &#215; 1 convolution), PPA employs three parallel branches to capture information at different levels and scales:<list list-type="bullet"><list-item><p>Local Perception Branch (Patch-Aware, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>): This branch divides the input feature map into <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> local image patches and independently learns and interacts with the embedding representation of each patch (e.g., through a Feed-Forward Network (FFN) and task-correlation-based feature selection mechanisms&#160;[<xref rid="B23-sensors-25-05310" ref-type="bibr">23</xref>,<xref rid="B25-sensors-25-05310" ref-type="bibr">25</xref>]). It ultimately reconstructs a feature map <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that focuses on local details.</p></list-item><list-item><p>Global Perception Branch (Patch-Aware, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>): Employing a similar mechanism but with larger <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> image patches, this branch aims to capture broader contextual dependencies, outputting a feature map <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. This patch-aware processing allows the model to perform adaptive feature aggregation across different spatial extents.</p></list-item><list-item><p>Serial Convolution Branch: This branch serves to capture classic local features with strong translation invariance, complementing the patch-based processing of the other two branches. It consists of a stack of several standard <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional layers, each typically followed by Batch Normalization (BN) and a ReLU activation function. This design ensures the extraction of robust, low-level feature patterns such as edges and textures, which are then fused with the multi-scale contextual information from the parallel branches to form a more comprehensive feature representation, yielding <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p>The selection of patch sizes <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is a deliberate design choice to establish a multi-scale perception mechanism. The smaller <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> patches enable the model to focus on fine-grained local features, which is critical for identifying the subtle textures and edges of small defects. Conversely, the larger <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> patches provide a broader receptive field, allowing the model to capture more extensive contextual information and better distinguish defects from the complex surrounding background structures. By processing these two scales in parallel, the PPA module can simultaneously perceive both detailed and contextual information, which is highly beneficial for the GIS defect-detection task. The outputs of these three branches are subsequently fused to obtain the aggregated feature <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">F</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>(2) Attention-based Feature Enhancement: As illustrated in the latter stage of <xref rid="sensors-25-05310-f003" ref-type="fig">Figure 3</xref>, the aggregated feature <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">F</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is fed into a cascaded attention module for further refinement. This two-stage process is designed to first determine &#8220;what&#8221; features are important, and then identify &#8220;where&#8221; they are located.</p><list list-type="bullet"><list-item><p>Channel Attention: First, the features pass through an Efficient Channel Attention (ECA-Net)&#160;[<xref rid="B26-sensors-25-05310" ref-type="bibr">26</xref>] module. ECA-Net adaptively recalibrates the importance of each channel by learning cross-channel interaction without dimensionality reduction, effectively highlighting which feature channels are most relevant to the defect-detection task. This yields a channel-refined feature map <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">F</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8855;</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">F</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the channel attention map.</p></list-item><list-item><p>Spatial Attention: Subsequently, the channel-refined feature map <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is processed by a spatial attention mechanism, similar to the one used in CBAM&#160;[<xref rid="B27-sensors-25-05310" ref-type="bibr">27</xref>]. This module generates a 2D spatial attention map that emphasizes the most informative regions within the feature map, guiding the model to focus on the precise locations of potential defects while suppressing background noise. The final output is produced by <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8855;</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the spatial attention map.</p></list-item></list><p>Here, &#8855; denotes element-wise multiplication. This cascaded channel-then-spatial attention structure ensures a comprehensive feature refinement, equipping the backbone network with stronger and more robust feature extraction capabilities for subsequent detection stages.</p><p>In the context of the GIS X-ray defect-detection task in this paper, replacing the C2f module in the deeper layers of the backbone with the PPA module offers several key advantages. (1) The multi-branch parallel processing can effectively capture both local detail features of defects (such as edges and textures) and their global background structural information simultaneously. (2) The Patch-Aware mechanism provides the model with the ability to learn and interact with features at different spatial granularities, which is beneficial for handling defects of varying sizes. (3) The subsequent dual channel and spatial attention mechanisms can further refine features, amplifying defect-related signals while suppressing irrelevant information common in X-ray images, such as noise or artifacts. Therefore, the introduction of PPA is expected to equip the backbone network with stronger and more robust feature extraction capabilities, thereby improving performance in subsequent detection stages.</p></sec><sec id="sec2dot4-sensors-25-05310"><title>2.4. Enhanced Feature Pyramid Neck</title><p>In object detection, effectively fusing features from different backbone levels is crucial for identifying targets across various scales. Over the years, several feature pyramid network architectures have been proposed to address this challenge, with their evolution illustrated in <xref rid="sensors-25-05310-f004" ref-type="fig">Figure 4</xref>. The classic Feature Pyramid Network (FPN)&#160;[<xref rid="B20-sensors-25-05310" ref-type="bibr">20</xref>] (<xref rid="sensors-25-05310-f004" ref-type="fig">Figure 4</xref>a) established a top-down pathway to merge high-level semantic features with low-level spatial details. Subsequently, the Path Aggregation Network (PANet)&#160;[<xref rid="B13-sensors-25-05310" ref-type="bibr">13</xref>] (<xref rid="sensors-25-05310-f004" ref-type="fig">Figure 4</xref>b) introduced an additional bottom-up pathway, creating a more effective bidirectional information flow. More advanced structures like BiFPN&#160;[<xref rid="B28-sensors-25-05310" ref-type="bibr">28</xref>] (<xref rid="sensors-25-05310-f004" ref-type="fig">Figure 4</xref>c) and GFPN&#160;[<xref rid="B21-sensors-25-05310" ref-type="bibr">21</xref>] (<xref rid="sensors-25-05310-f004" ref-type="fig">Figure 4</xref>d) further optimized the fusion process.</p><p>Our proposed neck network is designed by drawing upon the strengths of these established architectures. In terms of path topology, we adopt the proven bidirectional architecture of PANet (as shown in <xref rid="sensors-25-05310-f004" ref-type="fig">Figure 4</xref>b) to ensure robust two-way fusion of semantic and spatial information. However, our core design philosophy is inspired by the &#8220;heavy-neck&#8221; paradigm emphasized in GFPN (<xref rid="sensors-25-05310-f004" ref-type="fig">Figure 4</xref>d). The central idea of GFPN is that investing more computational resources and complexity in the neck network&#8217;s fusion nodes leads to more powerful and discriminative multi-scale feature representations.</p><p>In line with this philosophy, instead of using simple convolutional layers for fusion, our neck employs more powerful <monospace>CSPStage</monospace> modules within both the top-down and bottom-up pathways (as detailed in <xref rid="sensors-25-05310-f001" ref-type="fig">Figure 1</xref>). This use of complex feature processing units at each fusion stage is our key implementation of the &#8220;heavy-neck&#8221; concept. The specific pathways are as follows:<list list-type="bullet"><list-item><p>Top-down Pathway: Deep, high-level semantic features are progressively upsampled and fused with shallower features from the backbone. Each fused feature map is then processed by a <monospace>CSPStage</monospace> module to refine the representation.</p></list-item><list-item><p>Bottom-up Pathway: Subsequently, the refined, semantically-rich feature maps from the top-down path are progressively downsampled and fused with features from higher levels. These are also processed by <monospace>CSPStage</monospace> modules to enrich deeper feature maps with precise localization cues.</p></list-item></list></p><p>By combining a PANet-like bidirectional path structure with the GFPN-inspired &#8220;heavy-neck&#8221; design, our enhanced neck network achieves a more thorough and effective fusion of multi-scale features, significantly improving the model&#8217;s performance on detecting GIS defects.</p></sec></sec><sec id="sec3-sensors-25-05310"><title>3. Experiments and Result</title><sec id="sec3dot1-sensors-25-05310"><title>3.1. Dataset</title><p>All experiments in this paper were conducted on a custom-built X-ray image dataset for internal defects in Gas-Insulated Switchgear (GIS) equipment, provided by the State Grid Ningxia Electric Power Co., Ltd. (Yinchuan, Ningxia, China). This dataset, reflecting real-world industrial application scenarios, comprises a total of 718 X-ray images covering various equipment models and imaging conditions.</p><p>To support supervised learning, all images were annotated by professional engineers using bounding boxes. This study focuses on five representative types of internal GIS defects. A detailed description of their visual characteristics, along with the distribution of annotated instances for each category, is presented in <xref rid="sensors-25-05310-t001" ref-type="table">Table 1</xref>. The dataset exhibits a natural class imbalance, which is typical of real-world industrial data where some defect types occur more frequently than others.</p><p>The collected 718 annotated images were randomly partitioned into a Training Set, a Validation Set, and a Test Set according to an 8:1:1 ratio. Example images illustrating each of these defect types are shown in <xref rid="sensors-25-05310-f005" ref-type="fig">Figure 5</xref>.</p></sec><sec id="sec3dot2-sensors-25-05310"><title>3.2. Experimental Environment</title><p>All models in this paper were trained under the PyTorch 2.3.0 framework. Stochastic Gradient Descent (SGD) was employed as the optimizer, with an initial learning rate set to <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and a batch size of 32. All models were uniformly trained for 300 epochs. For other training hyperparameters, the default configuration of YOLOv10n was adopted. The specific hardware and software environment configurations for the experiments are detailed in <xref rid="sensors-25-05310-t002" ref-type="table">Table 2</xref>.</p><p>Recognizing that overfitting is a primary challenge when training on specialized datasets of limited size, we implemented a multi-layered strategy to ensure the model&#8217;s generalization ability. This strategy consisted of three key components:<list list-type="bullet"><list-item><p>Transfer Learning: All models were initialized with weights pre-trained on the large-scale COCO dataset. This provides the network with a robust foundation of general visual features, preventing it from having to learn these from scratch and significantly reducing the risk of fitting to noise in our specific dataset.</p></list-item><list-item><p>Extensive Data Augmentation: We utilized a rich set of online data augmentation techniques, including mosaic augmentation, random affine transformations (rotation, scaling, translation), and color space adjustments (HSV). This effectively creates a larger and more diverse &#8220;virtual&#8221; dataset, forcing the model to learn invariant and robust features rather than memorizing the training examples.</p></list-item><list-item><p>Regularization: A standard weight decay of <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0005</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> was applied during optimization. This technique penalizes large weights in the network, thereby constraining model complexity and discouraging it from learning overly complex patterns that are specific only to the training data.</p></list-item></list></p><p>The effectiveness of this comprehensive strategy is ultimately validated by the strong performance our model achieved on the unseen, held-out test set. A model that had significantly overfit would fail to generalize and perform well on this data; thus, the high accuracy reported in our results serves as compelling empirical evidence that overfitting was successfully mitigated.</p><p>The inference speed, measured in Frames Per Second (FPS), was evaluated on the same NVIDIA GeForce RTX 4090 GPU with a batch size of 1 to reflect real-world deployment performance.</p></sec><sec id="sec3dot3-sensors-25-05310"><title>3.3. Experimental Metrics</title><p>To comprehensively and quantitatively evaluate the performance of the proposed improved model and other comparative methods on the GIS X-ray defect-detection task, we adopted standard evaluation metrics recognized in the field of object detection. The calculation of these metrics is typically based on the following four fundamental statistics: True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN) (though TN is less directly used in object detection). In this task:<list list-type="bullet"><list-item><p>TP: Correctly detected defects (the IoU between the predicted bounding box and the ground truth bounding box is greater than a set threshold <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula>, and the predicted class matches the true class).</p></list-item><list-item><p>FP: Incorrect detection results (the IoU between the predicted box and all ground truth boxes is less than the threshold <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula>, or the predicted class is incorrect, or background is misidentified as a defect).</p></list-item><list-item><p>FN: Undetected true defects (a ground truth defect box exists, but no predicted box matches it with a sufficiently high IoU (<inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8805;</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and the correct class).</p></list-item></list></p><p>IoU is a key metric for measuring the degree of overlap between a predicted bounding box <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and a ground truth bounding box <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Its calculation formula is:<disp-formula id="FD5-sensors-25-05310"><label>(5)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>IoU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Area</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8745;</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>Area</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8746;</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The value range of IoU is <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where a higher value indicates better overlap between the predicted and ground truth boxes.</p><p>Based on the above fundamental definitions, we selected the following core metrics for model evaluation:<list list-type="bullet"><list-item><p><bold>Precision (P):</bold> The proportion of actual true defects among all samples predicted as defects by the model. It measures the accuracy of the model&#8217;s predictions.<disp-formula id="FD6-sensors-25-05310"><label>(6)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><bold>Recall (R):</bold> The proportion of true defect samples successfully detected by the model among all actual true defect samples. It measures the model&#8217;s ability to find all relevant targets (completeness).<disp-formula id="FD7-sensors-25-05310"><label>(7)</label><mml:math id="mm73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><bold>Mean Average Precision (mAP):</bold> This is the most important and commonly used comprehensive evaluation metric in object-detection tasks. For a single class <italic toggle="yes">i</italic>, its Average Precision (<inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) is typically defined as the average of precision values at different recall levels, which can be obtained by calculating the area under the Precision&#8211;Recall (PR) curve. The mAP is then the arithmetic mean of the AP values for all <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> classes:<disp-formula id="FD8-sensors-25-05310"><label>(8)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>mAP</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>AP</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since the calculation of AP depends on the choice of the IoU threshold <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula>, we follow the convention of international standard competitions and primarily report the following two mAP metrics:</p><list list-type="simple"><list-item><label>&#8211;</label><p><bold>mAP@0.5 (or mAP50):</bold> The mAP calculated at a single IoU threshold <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This is a widely adopted standard in the PASCAL VOC challenge [<xref rid="B29-sensors-25-05310" ref-type="bibr">29</xref>].</p></list-item><list-item><label>&#8211;</label><p><bold>mAP@0.5:0.95 (or mAP[0.5:0.95]):</bold> The mAP calculated by taking multiple IoU thresholds <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.95</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in steps of <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.05</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e., <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.55</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), calculating mAP for each threshold, and then averaging these mAP values. This is the primary evaluation metric used in the COCO dataset challenge [<xref rid="B30-sensors-25-05310" ref-type="bibr">30</xref>], and it imposes higher requirements on the localization accuracy of targets.</p></list-item></list></list-item></list></p><p>Through a comprehensive analysis of these metrics, we can thoroughly assess the model&#8217;s accuracy, recall capability, and localization precision in the GIS defect-detection task.</p></sec><sec id="sec3dot4-sensors-25-05310"><title>3.4. Ablation Experiments and Results</title><p>To systematically validate our design choices and quantify the contributions of the proposed components, we conducted two sets of ablation studies on the custom-built GIS-Xray dataset.</p><p>First, we performed a progressive ablation study starting from the lightweight YOLOv10n baseline. These studies involved incrementally introducing and combining our four key enhancements: (1) the NWD loss function; (2) the MCAttn mechanism; (3) the PPA module; and (4) the GFPN-inspired neck. All experiments were conducted under identical training configurations. The detailed results are summarized in <xref rid="sensors-25-05310-t003" ref-type="table">Table 3</xref>.</p><p>The experimental results in <xref rid="sensors-25-05310-t003" ref-type="table">Table 3</xref> clearly demonstrate the positive impact of each improved component on model performance. First, comparing Experiment 2 with Experiment 1 (BASE), the standalone introduction of the NWD loss function, without increasing any parameter count or computational load, improved mAP@0.5 by <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and mAP@0.5:0.95 by <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This fully validates the significant advantage of the NWD loss function in improving the localization of minute defect targets. Second, examining the backbone enhancements, the C2fMCAttn module (Experiment 3) and the PPA module (Experiment 4) both improved performance, with PPA showing slightly higher gains at the cost of more parameters. Next, the GFPN-inspired neck network (Experiment 5) increased mAP@0.5:0.95 by <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.7</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, proving the effectiveness of the improved fusion strategy. Finally, after progressively combining the components (Experiments 6, 7, and 8), the model&#8217;s performance continuously improved, with our final model achieving a <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> improvement in mAP@0.5:0.95 over the baseline.</p><p>Furthermore, to validate the superiority of our chosen modules over other mainstream alternatives, we conducted a second set of comparative experiments, with the results presented in <xref rid="sensors-25-05310-t004" ref-type="table">Table 4</xref>. The data show that our C2f_MCAttn module achieves a higher mAP@0.5:0.95 score (0.653) compared to both SE (0.631) and CBAM (0.635). Similarly, our GFPN (0.661) outperforms the popular BiFPN structure (0.654). These comparisons provide strong justification for our specific design choices.</p><p>In summary, the results from both sets of ablation experiments strongly demonstrate that the NWD loss function, the backbone network enhancement strategy based on MCAttn and PPA, and the GFPN-inspired neck network structure proposed in this paper each contribute significantly to the final performance. Not only are they effective when combined synergistically, but they also prove to be superior choices compared to other common alternatives for the GIS X-ray defect-detection task. Of course, the performance improvement is accompanied by a moderate increase in model complexity, reflecting a potential trade-off between accuracy and efficiency in practical applications.</p></sec><sec id="sec3dot5-sensors-25-05310"><title>3.5. Model Comparison and Visualization Analysis</title><p>To comprehensively evaluate the proposed model, we conducted both an in-depth per-category analysis against the baseline and a broad performance comparison against other mainstream lightweight models.</p><p>First, to delve deeper into the specific contributions of our improvements, we analyzed the per-category performance gains of our final model over the baseline, as detailed in <xref rid="sensors-25-05310-t005" ref-type="table">Table 5</xref>. The results show that our model achieves consistent improvements across all defect categories. Most notably, a remarkable gain of <bold>9.7 percentage points</bold> in AP is observed for the &#8220;Crack&#8221; class. This is a particularly important finding, as cracks are often characterized by their slender, elongated shapes and low contrast, making them one of the most challenging defect types to detect. This substantial improvement strongly suggests that our key enhancements&#8212;such as the NWD loss function tailored for small and slender objects, and the advanced attention mechanisms (MCAttn and PPA) for superior feature extraction&#8212;are highly effective in addressing the core difficulties of this detection task.</p><p>Next, to further validate the effectiveness and advancement of our complete model, we conducted a comprehensive performance comparison against several other mainstream lightweight real-time object-detection models on the GIS-Xray test set. The models included in the comparison were YOLOv3-tiny [<xref rid="B9-sensors-25-05310" ref-type="bibr">9</xref>], YOLOv5n [<xref rid="B11-sensors-25-05310" ref-type="bibr">11</xref>], YOLOv8n [<xref rid="B14-sensors-25-05310" ref-type="bibr">14</xref>], and YOLOv9s [<xref rid="B15-sensors-25-05310" ref-type="bibr">15</xref>]. The main quantitative evaluation results are summarized in <xref rid="sensors-25-05310-t006" ref-type="table">Table 6</xref>.</p><p>The comparison results in <xref rid="sensors-25-05310-t006" ref-type="table">Table 6</xref> clearly show that the model proposed in this paper significantly surpasses the baseline and other compared models in accuracy. Specifically, compared to the direct baseline YOLOv10n, our model achieved a <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4.6</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> improvement in mAP@0.5 and a <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> improvement in mAP@0.5:0.95. This significant accuracy gain was realized with a moderate increase in model complexity (from 8.2 to 10.2 GFLOPs) and a corresponding reduction in inference speed (from 103 to 85 FPS). This demonstrates an excellent trade-off between accuracy and real-time performance, as our model maintains a high inference speed well-suited for practical applications.</p><p>The advantages of our method are more pronounced when compared with other representative models. For instance, while YOLOv8n and YOLOv10n offer higher speeds, our model provides a substantial lead in accuracy (e.g., +7.3% mAP@0.5:0.95 over YOLOv8n), making it a more compelling choice for high-precision applications. Compared to bulkier models like YOLOv3-tiny and YOLOv9s, our model is not only significantly more accurate but also more than twice as fast, further highlighting the efficiency of our design. Overall, for the studied GIS X-ray defect-detection task, our model achieves an optimal balance between detection accuracy and inference speed.</p><p>In addition to the quantitative performance metrics evaluated above, to more intuitively compare the performance differences of various models in actual detection scenarios, we selected typical GIS X-ray image samples covering all five defect categories and conducted a visual analysis of their detection results. <xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref> shows a comparison of the specific detection outputs of our proposed model (Ours) against the baseline YOLOv10n and four other advanced lightweight detectors (YOLOv3-tiny, YOLOv5n, YOLOv8n, YOLOv9s) on these samples.</p><p>Through careful observation and comparison of the detection results from each model in <xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref>, the significant advantages of our proposed model in handling various complex situations can be clearly seen:</p><p>For the minute bubble defect in <xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref>a, its features are faint, making detection extremely difficult. Interestingly, YOLOv3-tiny, with its larger parameter count, can detect this defect relatively well, whereas newer models like YOLOv5n and YOLOv8n exhibit missed detections. This suggests that YOLOv3-tiny performs adequately in handling certain small defects. In contrast, our proposed model (Ours) not only stably detects this minute bubble but also does so with high prediction confidence (as indicated by the bounding box). This visually demonstrates that our model&#8217;s performance in handling such challenging minute, low-contrast defects is significantly superior to most other lightweight comparative models.</p><p>The model&#8217;s superiority in detecting challenging defects is particularly evident with the concealed, hairline crack in <xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref>b. Due to its extremely subtle features, most competing models failed to achieve a successful detection. It is noteworthy that, apart from our proposed model, only the significantly bulkier YOLOv3-tiny (10.3 M Params) and YOLOv9s (7.3 M Params) managed to identify this feature. This result highlights our architecture&#8217;s exceptional accuracy-efficiency trade-off, proving its ability to outperform larger models on such critical, fine-grained defects while maintaining a lightweight profile.</p><p>When detecting &#8220;foreign body (metal fitting)&#8221; type targets (<xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref>c), most models performed successfully. However, misdetections are a concern; for instance, YOLOv8n incorrectly identified background as a &#8220;bubble&#8221; in this sample. Similarly, in the detection of &#8220;foreign body (metal suspension)&#8221; (<xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref>d), other models also showed potential misdetections. It must be objectively acknowledged that while our model generally performs stably, it may occasionally produce a few False Positives in certain situations (as might be observed in <xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref>d), indicating that there is still room for further optimization in suppressing misdetections.</p><p>Finally, for the &#8220;foreign body (tool)&#8221; defect in <xref rid="sensors-25-05310-f006" ref-type="fig">Figure 6</xref>e, which is larger and has more distinct features, all compared models performed well, accurately detecting and localizing it, as expected.</p><p>Overall, these instances clearly demonstrate that through synergistic optimization of the loss function, backbone network attention mechanisms, and neck feature fusion structure, our model can more effectively cope with various complex situations in GIS X-ray images. It particularly exhibits significant advantages in detecting minute, low-contrast, and densely packed defects, thereby validating the effectiveness and advancement of the proposed method.</p></sec></sec><sec sec-type="conclusions" id="sec4-sensors-25-05310"><title>4. Conclusions</title><p>This research tackles the critical challenge of detecting micro-defects in GIS X-ray images, which are inherently characterized by submillimeter scales, low contrast, and cluttered backgrounds. The aim was to propose an automated detection method with higher precision and robustness.</p><p>To this end, we proposed a significantly improved model based on the YOLOv10n framework. This model enhances detection performance through multifaceted synergistic optimizations: (1) adopting the NWD loss function effectively improved the localization capability for small targets; (2) embedding MCAttn into the C2f modules of the backbone and introducing PPA to replace deeper modules significantly enhanced the extraction and discrimination of key features; and (3) leveraging the design principles of GFPN, a new neck network structure was built, optimizing the efficiency and effectiveness of multi-scale feature fusion.</p><p>A series of experiments on a real-world GIS X-ray image dataset, including detailed ablation studies and comparative analyses with several advanced lightweight detectors (such as YOLOv3-tiny, YOLOv5n, YOLOv8n, and YOLOv9s), validated the effectiveness of our method. The results indicated that our complete proposed model significantly outperformed the baseline YOLOv10n and other comparative models across various evaluation metrics. For instance, on the mAP@0.5:0.95 metric, it achieved a <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> improvement over YOLOv10n, reaching a state-of-the-art level of <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.674</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Qualitative analysis also intuitively demonstrated our model&#8217;s advantages in detecting challenging samples, especially minute and slender defects.</p><p>The proposed improvements and experimental results from this research demonstrate that by targeted optimization of the loss function, introduction of advanced attention mechanisms, and enhancement of the feature fusion structure, the performance of object-detection models in specific industrial X-ray imaging scenarios can be effectively boosted. This work holds significant theoretical reference value and potential application prospects for advancing intelligent GIS condition assessment technology, improving power grid operational and maintenance efficiency, and ensuring the safe and reliable operation of power systems.</p><p>Concurrently, we recognize that while our method brings accuracy improvements, it also leads to an increase in model parameter count and computational complexity. Furthermore, occasional misdetections in certain complex backgrounds suggest that there is still room for improvement in the model&#8217;s feature discrimination capabilities and background suppression strategies.</p><p>Future research will aim to build upon and broaden the findings of this study. Key directions include: (1) validating and further improving the model on larger and more diverse GIS X-ray datasets as they become available through our ongoing industrial collaboration, which will further enhance the model&#8217;s robustness and generalization ability within its primary application domain; (2) comparing and adapting our proposed enhancements to newer architectures like YOLOv11 and YOLOv12 to stay at the forefront of the field; (3) further exploring model lightweighting techniques and researching strategies to reduce the false alarm rate, thereby enhancing the model&#8217;s practical deployment value.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank the engineers from State Grid Ningxia Electric Power Co., Ltd. for their valuable assistance in the data acquisition and annotation process. We also express our gratitude to the anonymous reviewers for their insightful comments and suggestions, which have significantly improved this paper.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, G.X. and X.B.; methodology, X.B.; software, X.B.; validation, X.B., G.X., and M.H.; formal analysis, X.B.; investigation, X.B.; data curation, X.B. and M.H.; writing&#8212;original draft preparation, X.B.; writing&#8212;review and editing, G.X. and X.B.; visualization, X.B. and M.H.; supervision, G.X.; project administration, G.X.; funding acquisition, G.X. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw X-ray image dataset used in this study is not publicly available due to business confidentiality and proprietary restrictions from our industrial partner, State Grid Ningxia Electric Power Co., Ltd. However, the complete source code, model configurations, and the final trained model weights are publicly available on GitHub at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/imxiaolong0804/YoloV10Improve">https://github.com/imxiaolong0804/YoloV10Improve</uri> (accessed on 24 August 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05310"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name></person-group><article-title>A GIS partial discharge defect identification method based on YOLOv5</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>8360</elocation-id><pub-id pub-id-type="doi">10.3390/app12168360</pub-id></element-citation></ref><ref id="B2-sensors-25-05310"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name></person-group><article-title>Detection and diagnosis of defect in GIS based on X-ray digital imaging technology</article-title><source>Energies</source><year>2020</year><volume>13</volume><elocation-id>661</elocation-id><pub-id pub-id-type="doi">10.3390/en13030661</pub-id></element-citation></ref><ref id="B3-sensors-25-05310"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Q.</given-names></name></person-group><article-title>Research on defect pattern recognition of GIS equipment based on X-ray digital imaging technology</article-title><source>Proceedings of the 2018 IEEE International Conference on High Voltage Engineering and Application (ICHVE)</source><conf-loc>Athens, Greece</conf-loc><conf-date>10&#8211;13 September 2018</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B4-sensors-25-05310"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ali</surname><given-names>M.L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>The YOLO framework: A comprehensive review of evolution, applications, and benchmarks in object detection</article-title><source>Computers</source><year>2024</year><volume>13</volume><elocation-id>336</elocation-id><pub-id pub-id-type="doi">10.3390/computers13120336</pub-id></element-citation></ref><ref id="B5-sensors-25-05310"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jain</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Dubuisson</surname><given-names>M.P.</given-names></name></person-group><article-title>Segmentation of X-ray and C-scan images of fiber reinforced composite materials</article-title><source>Pattern Recognit.</source><year>1992</year><volume>25</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/0031-3203(92)90109-V</pub-id></element-citation></ref><ref id="B6-sensors-25-05310"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lashkia</surname><given-names>V.</given-names></name></person-group><article-title>Defect detection in X-ray images using fuzzy reasoning</article-title><source>Image Vis. Comput.</source><year>2001</year><volume>19</volume><fpage>261</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1016/S0262-8856(00)00075-5</pub-id></element-citation></ref><ref id="B7-sensors-25-05310"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kuang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kuang</surname><given-names>L.</given-names></name></person-group><article-title>Training-free Learning Applied in GIS X-DR Image Analysis</article-title><source>IEEE Trans. Power Deliv.</source><year>2025</year><volume>40</volume><fpage>1411</fpage><lpage>1420</lpage><pub-id pub-id-type="doi">10.1109/TPWRD.2025.3553848</pub-id></element-citation></ref><ref id="B8-sensors-25-05310"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified, real-time object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#8211;1 July 2016</conf-date><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="B9-sensors-25-05310"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>Yolov3: An incremental improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B10-sensors-25-05310"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.Y.M.</given-names></name></person-group><article-title>Yolov4: Optimal speed and accuracy of object detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id><pub-id pub-id-type="arxiv">2004.10934</pub-id></element-citation></ref><ref id="B11-sensors-25-05310"><label>11.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Ultralytics</collab></person-group><article-title>YOLOv5</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/yolov5" ext-link-type="uri">https://github.com/ultralytics/yolov5</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-03">(accessed on 3 August 2025)</date-in-citation></element-citation></ref><ref id="B12-sensors-25-05310"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.Y.M.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>P.Y.</given-names></name><name name-style="western"><surname>Hsieh</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>I.H.</given-names></name></person-group><article-title>CSPNet: A new backbone that can enhance learning capability of CNN</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>390</fpage><lpage>391</lpage></element-citation></ref><ref id="B13-sensors-25-05310"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Path aggregation network for instance segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>8759</fpage><lpage>8768</lpage></element-citation></ref><ref id="B14-sensors-25-05310"><label>14.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chaurasia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.</given-names></name></person-group><article-title>Ultralytics YOLO</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/ultralytics" ext-link-type="uri">https://github.com/ultralytics/ultralytics</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-03">(accessed on 3 August 2025)</date-in-citation></element-citation></ref><ref id="B15-sensors-25-05310"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>I.H.</given-names></name><name name-style="western"><surname>Mark Liao</surname><given-names>H.Y.</given-names></name></person-group><article-title>Yolov9: Learning what you want to learn using programmable gradient information</article-title><source>Computer Vision&#8212;ECCV 2024, 18th European Conference, Milan, Italy, 29 September&#8211;4 October 2024</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><fpage>1</fpage><lpage>21</lpage></element-citation></ref><ref id="B16-sensors-25-05310"><label>16.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name></person-group><article-title>Yolov10: Real-time end-to-end object detection</article-title><source>NIPS&#8217;24: 38th International Conference on Neural Information Processing Systems, Vancouver, BC, Canada, 10&#8211;15 December 2024</source><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2024</year><volume>Volume 37</volume><fpage>107984</fpage><lpage>108011</lpage></element-citation></ref><ref id="B17-sensors-25-05310"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khanam</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>M.</given-names></name></person-group><article-title>Yolov11: An overview of the key architectural enhancements</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2410.17725</pub-id><pub-id pub-id-type="arxiv">2410.17725</pub-id></element-citation></ref><ref id="B18-sensors-25-05310"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Doermann</surname><given-names>D.</given-names></name></person-group><article-title>Yolov12: Attention-centric real-time object detectors</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2502.12524</pub-id></element-citation></ref><ref id="B19-sensors-25-05310"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>L.</given-names></name></person-group><article-title>A normalized Gaussian Wasserstein distance for tiny object detection</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2110.13389</pub-id></element-citation></ref><ref id="B20-sensors-25-05310"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name></person-group><article-title>Feature pyramid networks for object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>2117</fpage><lpage>2125</lpage></element-citation></ref><ref id="B21-sensors-25-05310"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>GiraffeDet: A heavy-neck paradigm for object detection</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2202.04256</pub-id></element-citation></ref><ref id="B22-sensors-25-05310"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>Exploiting Scale-Variant Attention for Segmenting Small Medical Objects</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2407.07720</pub-id></element-citation></ref><ref id="B23-sensors-25-05310"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Teng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>A.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name></person-group><article-title>Hcf-net: Hierarchical context fusion network for infrared small object detection</article-title><source>Proceedings of the 2024 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Niagara Falls, ON, Canada</conf-loc><conf-date>15&#8211;19 July 2024</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B24-sensors-25-05310"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage></element-citation></ref><ref id="B25-sensors-25-05310"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Toast: Transfer learning via attention steering</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2305.15542</pub-id><pub-id pub-id-type="arxiv">2305.15542</pub-id></element-citation></ref><ref id="B26-sensors-25-05310"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zuo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name></person-group><article-title>ECA-Net: Efficient channel attention for deep convolutional neural networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11534</fpage><lpage>11542</lpage></element-citation></ref><ref id="B27-sensors-25-05310"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Woo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>Cbam: Convolutional block attention module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>3</fpage><lpage>19</lpage></element-citation></ref><ref id="B28-sensors-25-05310"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name></person-group><article-title>Efficientdet: Scalable and efficient object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>10781</fpage><lpage>10790</lpage></element-citation></ref><ref id="B29-sensors-25-05310"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Everingham</surname><given-names>M.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name><name name-style="western"><surname>Williams</surname><given-names>C.K.</given-names></name><name name-style="western"><surname>Winn</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>The pascal visual object classes (voc) challenge</article-title><source>Int. J. Comput. Vis.</source><year>2010</year><volume>88</volume><fpage>303</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id></element-citation></ref><ref id="B30-sensors-25-05310"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Maire</surname><given-names>M.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hays</surname><given-names>J.</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ramanan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zitnick</surname><given-names>C.L.</given-names></name></person-group><article-title>Microsoft coco: Common objects in context</article-title><source>Computer Vision&#8212;ECCV 2014: 13th European Conference, Zurich, Switzerland, 6&#8211;12 September 2014</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2014</year><comment>Part V</comment><fpage>740</fpage><lpage>755</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05310-f001" orientation="portrait"><label>Figure 1</label><caption><p>Improved YOLOv10 network model structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05310-g001.jpg"/></fig><fig position="float" id="sensors-25-05310-f002" orientation="portrait"><label>Figure 2</label><caption><p>Structure of C2f_MCAttn module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05310-g002.jpg"/></fig><fig position="float" id="sensors-25-05310-f003" orientation="portrait"><label>Figure 3</label><caption><p>Structure of PPA module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05310-g003.jpg"/></fig><fig position="float" id="sensors-25-05310-f004" orientation="portrait"><label>Figure 4</label><caption><p>Evolution of feature pyramid networks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05310-g004.jpg"/></fig><fig position="float" id="sensors-25-05310-f005" orientation="portrait"><label>Figure 5</label><caption><p>Example images from the custom GIS X-ray dataset, showcasing different defect types discussed in <xref rid="sensors-25-05310-t001" ref-type="table">Table 1</xref>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05310-g005.jpg"/></fig><fig position="float" id="sensors-25-05310-f006" orientation="portrait"><label>Figure 6</label><caption><p>Comparison of different models.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05310-g006.jpg"/></fig><table-wrap position="float" id="sensors-25-05310-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05310-t001_Table 1</object-id><label>Table 1</label><caption><p>Description and distribution of defect categories in the dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Defect Category</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Visual Characteristics in X-Ray Images</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Instances</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Bubble</td><td align="left" valign="middle" rowspan="1" colspan="1">Small, localized dark region (low density) with a defined shape, representing a gas void.</td><td align="center" valign="middle" rowspan="1" colspan="1">215</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Crack</td><td align="left" valign="middle" rowspan="1" colspan="1">Fine, irregular dark line (low density) with very low contrast, representing a fracture.</td><td align="center" valign="middle" rowspan="1" colspan="1">189</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Foreign body metal fitting</td><td align="left" valign="middle" rowspan="1" colspan="1">High-density (bright white) object with a regular geometric shape (e.g., screw).</td><td align="center" valign="middle" rowspan="1" colspan="1">452</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Foreign body metal suspension</td><td align="left" valign="middle" rowspan="1" colspan="1">Slender or rod-like dark object (low density), often appearing near complex structures like insulators.</td><td align="center" valign="middle" rowspan="1" colspan="1">378</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Foreign body tool</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large, high-density object with a recognizable tool-like shape.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">281</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1515</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05310-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05310-t002_Table 2</object-id><label>Table 2</label><caption><p>Hardware environment and software configuration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Environment</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Configuration</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hardware</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel<sup>&#174;</sup> Core&#8482; i7-13700KF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA<sup>&#174;</sup> GeForce<sup>&#174;</sup> RTX 4090 (24 GB GDDR6X)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64 GB DDR5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Software</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Operating System</td><td align="center" valign="middle" rowspan="1" colspan="1">Windows 11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CUDA</td><td align="center" valign="middle" rowspan="1" colspan="1">12.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">cuDNN</td><td align="center" valign="middle" rowspan="1" colspan="1">9.8.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pytorch</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Python</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.11</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05310-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05310-t003_Table 3</object-id><label>Table 3</label><caption><p>Results of ablation experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Experiment</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP@0.5:0.95</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GFLOPs</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv10n (BASE)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.832</td><td align="center" valign="middle" rowspan="1" colspan="1">0.837</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">0.624</td><td align="center" valign="middle" rowspan="1" colspan="1">2.70</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">BASE + NWD</td><td align="center" valign="middle" rowspan="1" colspan="1">0.883</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">0.939</td><td align="center" valign="middle" rowspan="1" colspan="1">0.656</td><td align="center" valign="middle" rowspan="1" colspan="1">2.70</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">BASE + C2fMCAttn</td><td align="center" valign="middle" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" rowspan="1" colspan="1">0.850</td><td align="center" valign="middle" rowspan="1" colspan="1">0.932</td><td align="center" valign="middle" rowspan="1" colspan="1">0.653</td><td align="center" valign="middle" rowspan="1" colspan="1">2.76</td><td align="center" valign="middle" rowspan="1" colspan="1">8.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">BASE + PPA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.911</td><td align="center" valign="middle" rowspan="1" colspan="1">0.903</td><td align="center" valign="middle" rowspan="1" colspan="1">0.948</td><td align="center" valign="middle" rowspan="1" colspan="1">0.658</td><td align="center" valign="middle" rowspan="1" colspan="1">4.41</td><td align="center" valign="middle" rowspan="1" colspan="1">9.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">BASE + GFPN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.910</td><td align="center" valign="middle" rowspan="1" colspan="1">0.921</td><td align="center" valign="middle" rowspan="1" colspan="1">0.937</td><td align="center" valign="middle" rowspan="1" colspan="1">0.661</td><td align="center" valign="middle" rowspan="1" colspan="1">3.32</td><td align="center" valign="middle" rowspan="1" colspan="1">8.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">BASE + NWD + C2fMCAttn</td><td align="center" valign="middle" rowspan="1" colspan="1">0.920</td><td align="center" valign="middle" rowspan="1" colspan="1">0.875</td><td align="center" valign="middle" rowspan="1" colspan="1">0.930</td><td align="center" valign="middle" rowspan="1" colspan="1">0.650</td><td align="center" valign="middle" rowspan="1" colspan="1">2.76</td><td align="center" valign="middle" rowspan="1" colspan="1">8.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">BASE + NWD + C2fMCAttn + PPA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.924</td><td align="center" valign="middle" rowspan="1" colspan="1">0.895</td><td align="center" valign="middle" rowspan="1" colspan="1">0.947</td><td align="center" valign="middle" rowspan="1" colspan="1">0.668</td><td align="center" valign="middle" rowspan="1" colspan="1">4.48</td><td align="center" valign="middle" rowspan="1" colspan="1">9.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.949</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.912</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.950</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.674</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.10</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>10.2</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05310-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05310-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of the proposed modules with other mainstream modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model Configuration</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP@0.5:0.95</th></tr></thead><tbody><tr><td colspan="5" align="left" valign="middle" rowspan="1">
<italic toggle="yes">
<bold>Attention Module Comparison on Backbone</bold>
</italic>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10n (Baseline) + SE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.875</td><td align="center" valign="middle" rowspan="1" colspan="1">0.832</td><td align="center" valign="middle" rowspan="1" colspan="1">0.915</td><td align="center" valign="middle" rowspan="1" colspan="1">0.631</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10n (Baseline) + CBAM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.889</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841</td><td align="center" valign="middle" rowspan="1" colspan="1">0.919</td><td align="center" valign="middle" rowspan="1" colspan="1">0.635</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv10n (Baseline) + C2f_MCAttn</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.902</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.850</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.932</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.653</bold>
</td></tr><tr><td colspan="5" align="left" valign="middle" rowspan="1">
<italic toggle="yes">
<bold>Neck Structure Comparison</bold>
</italic>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10n (Baseline) + BiFPN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.892</td><td align="center" valign="middle" rowspan="1" colspan="1">0.901</td><td align="center" valign="middle" rowspan="1" colspan="1">0.911</td><td align="center" valign="middle" rowspan="1" colspan="1">0.654</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv10n (Baseline) + GFPN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.910</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.921</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.937</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.661</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05310-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05310-t005_Table 5</object-id><label>Table 5</label><caption><p>Per-category Average Precision (AP@.5) comparison between the baseline model and our final proposed model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Defect Category</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">YOLOv10n (Baseline)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ours</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Improvement</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Bubble</td><td align="center" valign="middle" rowspan="1" colspan="1">0.898</td><td align="center" valign="middle" rowspan="1" colspan="1">0.935</td><td align="center" valign="middle" rowspan="1" colspan="1">+0.037</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Crack</td><td align="center" valign="middle" rowspan="1" colspan="1">0.780</td><td align="center" valign="middle" rowspan="1" colspan="1">0.877</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>+0.097</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Foreign body metal fitting</td><td align="center" valign="middle" rowspan="1" colspan="1">0.960</td><td align="center" valign="middle" rowspan="1" colspan="1">0.994</td><td align="center" valign="middle" rowspan="1" colspan="1">+0.034</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Foreign body metal suspension</td><td align="center" valign="middle" rowspan="1" colspan="1">0.941</td><td align="center" valign="middle" rowspan="1" colspan="1">0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">+0.037</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Foreign body tool</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.938</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.967</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+0.029</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP@0.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.904</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.950</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>+0.046</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05310-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05310-t006_Table 6</object-id><label>Table 6</label><caption><p>Model performance and speed comparison of YOLO series.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">P</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP@0.5:0.95</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GFLOPs</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS (Hz)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv3-tiny</td><td align="center" valign="middle" rowspan="1" colspan="1">0.889</td><td align="center" valign="middle" rowspan="1" colspan="1">0.843</td><td align="center" valign="middle" rowspan="1" colspan="1">0.854</td><td align="center" valign="middle" rowspan="1" colspan="1">0.611</td><td align="center" valign="middle" rowspan="1" colspan="1">10.3</td><td align="center" valign="middle" rowspan="1" colspan="1">31.5</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5n</td><td align="center" valign="middle" rowspan="1" colspan="1">0.801</td><td align="center" valign="middle" rowspan="1" colspan="1">0.777</td><td align="center" valign="middle" rowspan="1" colspan="1">0.811</td><td align="center" valign="middle" rowspan="1" colspan="1">0.597</td><td align="center" valign="middle" rowspan="1" colspan="1">2.6</td><td align="center" valign="middle" rowspan="1" colspan="1">18.9</td><td align="center" valign="middle" rowspan="1" colspan="1">60</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8n</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841</td><td align="center" valign="middle" rowspan="1" colspan="1">0.787</td><td align="center" valign="middle" rowspan="1" colspan="1">0.823</td><td align="center" valign="middle" rowspan="1" colspan="1">0.601</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1</td><td align="center" valign="middle" rowspan="1" colspan="1">8.1</td><td align="center" valign="middle" rowspan="1" colspan="1">105</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv9s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.876</td><td align="center" valign="middle" rowspan="1" colspan="1">0.812</td><td align="center" valign="middle" rowspan="1" colspan="1">0.883</td><td align="center" valign="middle" rowspan="1" colspan="1">0.632</td><td align="center" valign="middle" rowspan="1" colspan="1">7.3</td><td align="center" valign="middle" rowspan="1" colspan="1">27.6</td><td align="center" valign="middle" rowspan="1" colspan="1">45</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv10n</td><td align="center" valign="middle" rowspan="1" colspan="1">0.832</td><td align="center" valign="middle" rowspan="1" colspan="1">0.837</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td><td align="center" valign="middle" rowspan="1" colspan="1">0.624</td><td align="center" valign="middle" rowspan="1" colspan="1">2.70</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2</td><td align="center" valign="middle" rowspan="1" colspan="1">103</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.949</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.912</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.950</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.674</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.10</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>10.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>85</bold>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>