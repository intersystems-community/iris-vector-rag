<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="pmc-domain-id">440</journal-id><journal-id journal-id-type="pmc-domain">plosone</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>PLOS</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431670</article-id><article-id pub-id-type="pmcid-ver">PMC12431670.1</article-id><article-id pub-id-type="pmcaid">12431670</article-id><article-id pub-id-type="pmcaiid">12431670</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0331566</article-id><article-id pub-id-type="publisher-id">PONE-D-25-18536</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Fruits</subject><subj-group><subject>Apples</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Architecture</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Deep Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Age Groups</subject><subj-group><subject>Children</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Families</subject><subj-group><subject>Children</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Visual enumeration remains challenging for multimodal generative AI</article-title><alt-title alt-title-type="running-head">Visual enumeration remains challenging for multimodal generative AI</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7062-4861</contrib-id><name name-style="western"><surname>Testolin</surname><given-names initials="A">Alberto</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Hou</surname><given-names initials="K">Kuinan</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zorzi</surname><given-names initials="M">Marco</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref><xref rid="aff004" ref-type="aff">
<sup>4</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of General Psychology and Department of Mathematics, University of Padova, Padova, Italy</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Department of General Psychology, University of Padova, Padova, Italy</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Department of General Psychology and Padova Neuroscience Center University of Padova, Padova, Italy</addr-line></aff><aff id="aff004"><label>4</label>
<addr-line>IRCSS San Camillo Hospital, Venice-Lido, Italy</addr-line></aff><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Jan</surname><given-names initials="YK">Yih-Kuen</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Illinois Urbana-Champaign, UNITED STATES OF AMERICA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>alberto.testolin@unipd.it</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>9</issue><issue-id pub-id-type="pmc-issue-id">496058</issue-id><elocation-id>e0331566</elocation-id><history><date date-type="received"><day>7</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>18</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>12</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 Testolin et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Testolin et al</copyright-holder><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="pone.0331566.pdf"/><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pdf" xlink:href="pone.0331566.pdf"/><abstract><p>Many animal species can approximately judge the number of objects in a visual scene at a single glance, and humans can further determine the exact cardinality of a set by deploying systematic counting procedures. In contrast, it has been observed that even state-of-the-art AI systems have very limited enumeration skills. In this work, we propose two benchmark tasks inspired by cognitive science that allow to precisely evaluate the visual enumeration capabilities of multimodal foundation models, thereby providing an objective measure of their number sense and counting level. We consider popular visual question answering models (BLIP, LLaVA and ViLT) as well as advanced image-to-text (Gemini, GPT and Qwen) and text-to-image (DALL-E, FLUX and Stable Diffusion) AI systems. Our analyses show that even the most advanced models cannot reliably name the number of objects in simple visual stimuli or generate images containing a target number of items, as indexed by their low accuracy in both types of tasks. Especially for numbers outside the subitizing range, their responses are often far from the target numerosity, and, in stark contrast with human behavior, in many cases the distribution of errors depends on the object category. We also observe some striking mistakes with small numbers. Our findings demonstrate that developing an intuitive visual understanding of number remains challenging for AI models and that merely increasing model size might not be a viable strategy to promote the emergence of systematic counting skills. We release the full code of our benchmark to facilitate the evaluation of enumeration skills in future AI systems.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004543</institution-id><institution>China Scholarship Council</institution></institution-wrap></funding-source><award-id>202307820031</award-id><principal-award-recipient><name name-style="western"><surname>Hou</surname><given-names>Kuinan</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>Italian Ministry of University and Research</institution></funding-source><award-id>C53D23004110006</award-id><principal-award-recipient><name name-style="western"><surname>Zorzi</surname><given-names>Marco</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution>European Union Next-Generation EU</institution></funding-source><award-id>n.PE13 - BAC FAIR SP.10</award-id><principal-award-recipient><name name-style="western"><surname>Zorzi</surname><given-names>Marco</given-names></name></principal-award-recipient></award-group><funding-statement>We are grateful to OpenAI for granting research access to the GPT-4V and DALL-E APIs. This work was supported by the European Union Next-Generation EU grant (grant n.PE13 - BAC FAIR SP.10 to M.Z.) and by the Italian Ministry of University and Research (PRIN grant n. C53D23004110006 to M.Z.). K.H. acknowledges the support of the China Scholarship Council (ID: 202307820031). There was no additional external funding received for this study.</funding-statement></funding-group><counts><fig-count count="6"/><table-count count="3"/><page-count count="22"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>The data used in the current study and the complete code of our benchmark are freely available on GitHub at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/CCNL-UniPD/Numbersense-AI" ext-link-type="uri">https://github.com/CCNL-UniPD/Numbersense-AI</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>The data used in the current study and the complete code of our benchmark are freely available on GitHub at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/CCNL-UniPD/Numbersense-AI" ext-link-type="uri">https://github.com/CCNL-UniPD/Numbersense-AI</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>1 Introduction</title><p>Artificial Intelligence (AI) is progressing rapidly, with deep learning models approaching or even surpassing human performance in a variety of domains, including perceptual judgements [<xref rid="pone.0331566.ref001" ref-type="bibr">1</xref>] and natural language processing [<xref rid="pone.0331566.ref002" ref-type="bibr">2</xref>]. However, even the most advanced multimodal AI systems still struggle in judging the numerosity of visual sets, a core cognitive capability that humans share with many animal species [<xref rid="pone.0331566.ref003" ref-type="bibr">3</xref>]. Even infants are sensitive to numerosity [<xref rid="pone.0331566.ref004" ref-type="bibr">4</xref>] and toddlers can generate sets that contain a target number of items [<xref rid="pone.0331566.ref005" ref-type="bibr">5</xref>], suggesting that a pre-verbal understanding of numerical quantities develops well before language development and formal education. Small numerosities in the &#8220;subitizing range&#8221; (up to 4) are perceived in an exact manner (i.e., enumeration is error-free), while the numerosity of larger sets is only approximately estimated when counting is precluded [<xref rid="pone.0331566.ref006" ref-type="bibr">6</xref>]. In the latter case, responses follow Weber&#8217;s law, so that variability increases proportionally to the mean estimate [<xref rid="pone.0331566.ref003" ref-type="bibr">3</xref>]. Another key signature of our number sense is its abstract nature: similar response patterns are observed for items of all categories, despite variation in object features (such as color or shape). Indeed, numerosity is spontaneously extracted by our visual system [<xref rid="pone.0331566.ref007" ref-type="bibr">7</xref>] and it is encoded independently from object category, location, or presentation modality [<xref rid="pone.0331566.ref004" ref-type="bibr">4</xref>]. Moreover, during childhood, humans (but no other species) learn counting algorithms that allow them to establish the exact cardinality of any set of objects by performing a one-to-one mapping between visual or auditory items and the list of number words [<xref rid="pone.0331566.ref008" ref-type="bibr">8</xref>]. Importantly, there is a broad consensus that number sense and mastery of counting principles are foundational for the development of numeracy and the acquisition of higher-level mathematical competence [<xref rid="pone.0331566.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0331566.ref009" ref-type="bibr">9</xref>&#8211;<xref rid="pone.0331566.ref011" ref-type="bibr">11</xref>].</p><p>AI researchers have engineered a variety of specialized computer vision architectures to count objects in visual scenes, often tailored to specific categories such as animals [<xref rid="pone.0331566.ref012" ref-type="bibr">12</xref>], crowds [<xref rid="pone.0331566.ref013" ref-type="bibr">13</xref>] or common objects encountered in a specific domain of interest [<xref rid="pone.0331566.ref014" ref-type="bibr">14</xref>]. The most popular framework consists of running an object detector to first segment the target items in the image and then explicitly counting the resulting bounding boxes or object proposals [<xref rid="pone.0331566.ref015" ref-type="bibr">15</xref>,<xref rid="pone.0331566.ref016" ref-type="bibr">16</xref>], in some cases summing fractional counts estimated from different sections of the image [<xref rid="pone.0331566.ref017" ref-type="bibr">17</xref>]. However, in these approaches numerosity representations do not emerge within the model itself because the encoding of numbers is delegated to an external, hard-wired (and often category-specific) mechanism. A radically different perspective considers the possibility that numerosity representations might spontaneously emerge in neural systems as a high-order statistical feature of the sensory signal [<xref rid="pone.0331566.ref018" ref-type="bibr">18</xref>]. Indeed, a rudimentary visual number sense has been shown to emerge in small-scale generative models trained with the goal of reconstructing images with a varying number of items [<xref rid="pone.0331566.ref019" ref-type="bibr">19</xref>,<xref rid="pone.0331566.ref020" ref-type="bibr">20</xref>] and number-selective neurons have been observed in generic convolutional networks trained for object recognition [<xref rid="pone.0331566.ref021" ref-type="bibr">21</xref>,<xref rid="pone.0331566.ref022" ref-type="bibr">22</xref>]. Thus, one might wonder whether a similar capacity could emerge in modern multimodal foundation models [<xref rid="pone.0331566.ref023" ref-type="bibr">23</xref>], which are large-scale generative architectures trained on huge data sets that exhibit emergent abilities [<xref rid="pone.0331566.ref024" ref-type="bibr">24</xref>] and can readily solve a wide range of downstream tasks [<xref rid="pone.0331566.ref025" ref-type="bibr">25</xref>,<xref rid="pone.0331566.ref026" ref-type="bibr">26</xref>]. Unlike domain-specific architectures engineered for visual counting [<xref rid="pone.0331566.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0331566.ref028" ref-type="bibr">28</xref>], foundation models are domain-general systems that can be used out-of-the-box without the need of fine-tuning on numerical tasks. However, despite their flexibility and their remarkable performance in a variety of domains [<xref rid="pone.0331566.ref029" ref-type="bibr">29</xref>], even the most advanced foundation models fall short in tasks that require the manipulation of numerical information [<xref rid="pone.0331566.ref030" ref-type="bibr">30</xref>&#8211;<xref rid="pone.0331566.ref032" ref-type="bibr">32</xref>], calling for a systematic investigation of their basic visual enumeration skills.</p><p>In line with the proposal of using methods from cognitive science to test AI models [<xref rid="pone.0331566.ref033" ref-type="bibr">33</xref>], in this work we address this problem by introducing two benchmark tasks that are commonly used to evaluate enumeration skills in humans: numerosity naming [<xref rid="pone.0331566.ref034" ref-type="bibr">34</xref>], which requires establishing how many items are present in a given stimulus, and numerosity production [<xref rid="pone.0331566.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0331566.ref035" ref-type="bibr">35</xref>], which requires generating a set containing a target number of items. The former task can be used to probe <italic toggle="yes">image-to-text</italic> architectures, while the latter can be used to probe <italic toggle="yes">text-to-image</italic> architectures. Our benchmark allows to characterize the distribution of model responses using a variety of object categories, providing aggregate scores indicating the overall model performance and additional metrics that measure whether the distribution of responses follows a human-like pattern. Perfect accuracy across the entire numerical range would suggest the emergence of systematic counting skills, while error-free responses with only small numbers would either indicate subitizing capabilities, or that counting is only partially developed as in children who do not fully master the counting principles [<xref rid="pone.0331566.ref036" ref-type="bibr">36</xref>,<xref rid="pone.0331566.ref037" ref-type="bibr">37</xref>]. Error-prone responses centered on the target number would instead suggest that the AI model relies on numerosity estimation, which may follow Weber&#8217;s law (as in humans) or not.</p><p>We perform our evaluation across a wide range of models of different sizes and complexities, considering the most powerful multimodal AI systems and visual question answering models available at the time of the research. In the image-to-text domain, we consider AI models that can provide written answers to non-trivial questions about the content of an image or accurate descriptions of complex visual scenes. In particular, we test three popular architectures used in visual question answering: ViLT (vision-and-language transformer) [<xref rid="pone.0331566.ref038" ref-type="bibr">38</xref>], BLIP (bootstrapping language-image pre-training model) [<xref rid="pone.0331566.ref039" ref-type="bibr">39</xref>] and LLaVA (large language and vision assistant model) [<xref rid="pone.0331566.ref040" ref-type="bibr">40</xref>]. We further test Qwen2.5-VL [<xref rid="pone.0331566.ref041" ref-type="bibr">41</xref>], which is one of the latest open-source multimodal models available, as well as two proprietary models that are considered among the most advanced multimodal systems currently available: GPT-4V [<xref rid="pone.0331566.ref042" ref-type="bibr">42</xref>] and Gemini Pro Vision [<xref rid="pone.0331566.ref043" ref-type="bibr">43</xref>]. In the text-to-image domain, we instead consider foundation models that can produce high-quality visual content following detailed user prompts provided in natural language. We test two popular open-source generative architectures for images, Stable Diffusion [<xref rid="pone.0331566.ref044" ref-type="bibr">44</xref>,<xref rid="pone.0331566.ref045" ref-type="bibr">45</xref>] and FLUX [<xref rid="pone.0331566.ref046" ref-type="bibr">46</xref>], as well as DALL-E [<xref rid="pone.0331566.ref047" ref-type="bibr">47</xref>,<xref rid="pone.0331566.ref048" ref-type="bibr">48</xref>], which is regarded among the most powerful proprietary systems. For Qwen and DALL-E, we also compare different versions of the same architecture to investigate whether increasing model size supports more refined visual enumeration skills.</p><p>The research contributions of our work are multifaceted. From a methodological perspective, we introduce a unified experimental procedure to evaluate the numerical skills of both image-to-text and text-to-image AI models, which allows us to quantitatively characterize their numerical competence across different types of object categories. We make our benchmarking pipeline publicly available to allow systematic evaluation of future AI models [<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/CCNL-UniPD/Numbersense-AI" ext-link-type="uri">https://github.com/CCNL-UniPD/Numbersense-AI</ext-link>]. From a theoretical perspective, we demonstrate that although larger models generally possess a better number sense, merely scaling-up the model size might not be the best way to spur the emergence of systematic counting skills. In this regard, we show that the weak performance in visual enumeration might be partially due to properties of the training corpora commonly used to build foundation models: for example, in popular training datasets the frequency of numerosities rapidly falls off according to a power law, implying that larger numbers are underrepresented, and textual captions often contain numerical information that is not related to the numerosity of the visual scene, thus injecting noise into the alignment of different input modalities.</p><p>The article is structured as follows: in the Materials and Methods section, we describe the problem setting, introducing the benchmark tasks and the evaluation metrics used to quantify numerical competence. We also provide details about the models considered and the relevant baselines. In the Results section, we present the quantitative results comparing different models, along with a detailed analysis of response errors and the statistical properties of two representative corpora commonly used to train large-scale multimodal AI systems. In the Discussion, we review and interpret our findings, while in the Conclusions we highlight their implications for AI research and discuss possible future directions.</p></sec><sec id="sec002"><title>2 Material and methods</title><sec id="sec003"><title>2.1 Benchmark tasks</title><sec id="sec004"><title>2.1.1 Image-to-text: Numerosity naming.</title><p>In the numerosity naming task, the models are asked to establish how many objects are present in a set of simple images containing up to 10 objects. We created a new dataset of synthetic images, each including only items of the same category sampled from 5 possible object types: apples, people, butterflies, colored dots, and &#8220;fast cards&#8221; depicting regularly placed clip-arts similar to those used to test number sense in young children [<xref rid="pone.0331566.ref036" ref-type="bibr">36</xref>]. Examples of stimuli are shown in <xref rid="pone.0331566.g001" ref-type="fig">Fig 1</xref>. For each object class and target number <italic toggle="yes">n</italic>, we created 50 high-resolution images (<inline-formula id="pone.0331566.e001"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.e001g" position="anchor" orientation="portrait" xlink:href="pone.0331566.e001.jpg"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mn>1024</mml:mn><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#215;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mn>1024</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> pixels) where the <italic toggle="yes">n</italic> objects have variable size and are randomly placed on a uniform white background, with no overlap. Fast card stimuli are created using clip-arts of common objects (apples, bells, butterflies, candies, cars, fish, flowers, planes, stars) drawn in different colors (black, blue, green, orange, red).</p><fig position="float" id="pone.0331566.g001" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.g001</object-id><label>Fig 1</label><caption><title>Samples from the numerosiy naming task.</title><p>Each row contains samples from a different object category, while columns correspond to different numerosities: 1, 2, 3, 4 and 8.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="pone.0331566.g001.jpg"/></fig><p>For each model tested, we consider three different prompting methods and select the one leading to the best performance, measured as mean absolute distance from the target number. The first prompt requires to explicitly estimate the number of objects belonging to a specific category (i.e., <italic toggle="yes">How many apples / butterflies / people / dots / shapes are there in the picture?</italic>). The other two use the more general words &#8220;objects&#8221; or &#8220;things&#8221; to identify the items to count. To make sure that the models are prompted correctly, we also perform a control simulation related to a non-numerical task using the entire set of &#8220;apples&#8221; stimuli, probing the models with the following prompt: <italic toggle="yes">What does the image represent?</italic> and considering as correct the following answers: <italic toggle="yes">apple(s)</italic> and <italic toggle="yes">fruit</italic>. All models provided the correct answer for the entire set of stimuli in the control task, thus demonstrating a proper understanding of the image content and the prompt structure.</p><p>Model responses are automatically parsed: if present, number words are converted to numerical values using the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pypi.org/project/word2number/" ext-link-type="uri">word2number</ext-link> Python library, and responses are discarded if they contain multiple numbers or vague quantification terms (e.g., &#8220;a few&#8221;, &#8220;a bunch of&#8221;). We verified that at least 20 eligible trials were recorded for each number / object category combination.</p></sec><sec id="sec005"><title>2.1.2 Text-to-image: Numerosity production.</title><p>In the numerosity production task, each model is asked to generate 100 high-resolution images containing a target number of objects, in analogy with numerosity production tasks used in animal and human studies [<xref rid="pone.0331566.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0331566.ref035" ref-type="bibr">35</xref>]. Target objects belong to the same classes used for the naming task, except for the &#8220;fast cards&#8221; category, which could be underrepresented in the corpora used to train foundation models.</p><p>All models were initially prompted with the following text: <italic toggle="yes">An image with n apples / butterflies / people / dots</italic> (where <italic toggle="yes">n</italic> varied between 1 and 10). When <italic toggle="yes">n</italic> = 1 the prompt was adjusted to the singular form. However, for the dots category this prompting method resulted in poor generations: we obtained better results when the models were prompted with a more specific description of the image: <italic toggle="yes">n filled dots in white background</italic>. For the people category, instead, we obtained better results with the prompt <italic toggle="yes">An image with n persons</italic>.</p><p>The generated images were automatically parsed using a computer vision pipeline [<xref rid="pone.0331566.ref049" ref-type="bibr">49</xref>] optimized on a set of 4,000 images that were manually labeled by one of the authors (K.H.) and independently verified by another author (A.T.). The pipeline employs Grounding DINO [<xref rid="pone.0331566.ref050" ref-type="bibr">50</xref>], a state-of-the-art object detection model, to identify objects within the generated images (see <xref rid="pone.0331566.g002" ref-type="fig">Fig 2</xref>). The process begins with parsing the prompts used for the text-to-image models to identify the object category for each image. Grounding DINO then detects all objects belonging to the specified category by providing bounding boxes for each detected object, along with a confidence score for each bounding box. Bounding boxes with low confidence scores are filtered out. To align the detections made by Grounding DINO with human annotations, a grid search was performed to optimize its confidence score threshold based on the NAE metric. The grid search explored confidence scores from 0.01 to 0.99 in increments of 0.01. The confidence score was iteratively adjusted to minimize the NAE between Grounding DINO&#8217;s outputs and the human annotations. The optimal confidence score was determined to be 0.40, achieving a minimum NAE of 0.05. We provide the NAE as a function of different thresholds in <xref rid="pone.0331566.s004" ref-type="supplementary-material">S4 Fig</xref>. This calibration ensured that discrepancies between the object counts detected by Grounding DINO and the human annotations were minimized, thereby enhancing the accuracy and reliability of the automatic evaluation process. If the automatic pipeline did not find any countable object, the image was discarded and the model was prompted again.</p><fig position="float" id="pone.0331566.g002" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.g002</object-id><label>Fig 2</label><caption><title>Graphical representation of our evaluation pipeline.</title><p>Numerosity naming (image-to-text) is represented in the upper stream, while numerosity generation (text-to-image) is represented in the lower stream.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="pone.0331566.g002.jpg"/></fig></sec></sec><sec id="sec006"><title>2.2 Multimodal AI architectures</title><p>For the numerosity naming task, we consider several representative image-to-text foundation models. We first test three popular multimodal architectures used in visual question answering: ViLT [<xref rid="pone.0331566.ref038" ref-type="bibr">38</xref>], BLIP-2 [<xref rid="pone.0331566.ref039" ref-type="bibr">39</xref>], and LLaVA [<xref rid="pone.0331566.ref040" ref-type="bibr">40</xref>]. We further consider three large-scale multimodal language models representing the state-of-the-art in AI research: the open-source multimodal Qwen2.5-VL model recently developed by Alibaba [<xref rid="pone.0331566.ref041" ref-type="bibr">41</xref>], the multimodal GPT-4V model developed by OpenAI [<xref rid="pone.0331566.ref042" ref-type="bibr">42</xref>] and the multimodal Gemini Pro model developed by Google [<xref rid="pone.0331566.ref043" ref-type="bibr">43</xref>]. All these systems have remarkable visual reasoning abilities and can answer non-trivial questions related to image content (e.g., <italic toggle="yes">What does the image represent? What are the feelings of the people in the scene and why?</italic>). For ViLT, we test the vilt-b32-mlm version available through <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://huggingface.co/dandelin/vilt-b32-mlm" ext-link-type="uri">Hugging Face</ext-link>. This architecture incorporates text embeddings into a Vision Transformer, allowing it to have a minimal design for vision-and-language pre-training and thus speeding-up model training and inference phases. It has a total of 87.4 million parameters. For BLIP-2, we test the blip2-flan-t5-xl version developed by Salesforce, also available through <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://huggingface.co/Salesforce/blip2-flan-t5-xl" ext-link-type="uri">Hugging Face</ext-link>. This architecture is an improved version of BLIP [<xref rid="pone.0331566.ref051" ref-type="bibr">51</xref>] that approaches state-of-the-art performance on several challenging benchmarks [<xref rid="pone.0331566.ref052" ref-type="bibr">52</xref>]; we explored all versions of the backbone models except the t5-xxl model (due to GPU memory constraints) and found that the version that used Flan-T5 as a language model yielded the best accuracy. The chosen model version has a total of 4.1 billion parameters. For LLaVA, we test the 1.6 version available through <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://huggingface.co/liuhaotian/llava-v1.6-34b" ext-link-type="uri">Hugging Face</ext-link>. LLaVa is an open-source end-to-end trained large multimodal model based on the transformer architecture, which combines a vision encoder and the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B" ext-link-type="uri">Nous Hermes 2</ext-link> large language model for general-purpose visual and language understanding, achieving impressive chat capabilities [<xref rid="pone.0331566.ref040" ref-type="bibr">40</xref>]. The chosen model version has a total of 34 billion parameters. Qwen2.5-VL is the latest version of the Qwen model family [<xref rid="pone.0331566.ref041" ref-type="bibr">41</xref>]. It has been optimized for visual recognition, image reasoning, captioning, and answering general questions about an image, outperforming many open-source and proprietary models on common industry benchmarks. We focus on the most powerful model version, which has a total of 72 billion parameters, but we also consider the smaller architecture with 7 billion parameters to measure performance gains with respect to model size. GPT-4V and Gemini Pro are regarded among the most powerful generalist AI systems to date, thanks to their unprecedented ability to understand and process an arbitrary mix of input images and texts. Technical details regarding the underlying architecture and inner working of these models (including engineered modules that might be used to solve specific tasks) have not been revealed; it has been <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/" ext-link-type="uri">speculated</ext-link> that both these models might have more than one trillion parameters.</p><p>For the numerosity production task, we consider three different image generation architectures that have proven capable of generating high-quality images following a textual description, also taking into account stylistic instructions, fine-grained details, and relational features (e.g., <italic toggle="yes">A photo of an astronaut riding a horse in photorealistic style</italic>). One is represented by the open-source Stable Diffusion (SD) model family, developed by Stability AI and freely available through <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large" ext-link-type="uri">Hugging Face</ext-link>. It is a latent diffusion model that combines an autoencoder with a diffusion model trained on the latent space of the autoencoder. We test both version 2.1 [<xref rid="pone.0331566.ref044" ref-type="bibr">44</xref>], which has approximately 500 million parameters, and the newest version 3.5 Large, which has approximately 8 billion parameters. We then consider the open-source FLUX model [<xref rid="pone.0331566.ref046" ref-type="bibr">46</xref>], which is also a diffusion-based architecture with a hybrid design that combines multimodal and parallel diffusion transformer blocks, allowing for a more effective processing of visual and textual data. FLUX has approximately 12 billion parameters, providing enhanced capacity for generating high-resolution, hyper-realistic images and accurately rendering complex visual scenes and text. Finally, we test two proprietary systems from the DALL-E model family using the API interface provided by OpenAI. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://openai.com/index/dall-e/" ext-link-type="uri">DALL-E 2</ext-link> [<xref rid="pone.0331566.ref047" ref-type="bibr">47</xref>] is an improved version of the original text-to-image DALL-E model, featuring a total of 3.5 billion parameters. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://openai.com/index/dall-e-3/" ext-link-type="uri">DALL-E 3</ext-link> [<xref rid="pone.0331566.ref048" ref-type="bibr">48</xref>] is the latest and most powerful version, which was trained using highly descriptive synthetic captions for the training images. Its number of parameters is currently unknown.</p></sec><sec id="sec007"><title>2.3 Counting-specific baselines</title><p>As baselines for the numerosity naming task we also evaluate two state-of-the-art architectures specifically tailored for counting tasks: the Point, Segment, and Count (PseCo) model [<xref rid="pone.0331566.ref027" ref-type="bibr">27</xref>], which is a detection-based counting model that utilizes point-level supervision and segmentation cues to improve object localization and enumeration, and the Training-Free Object Counting model [<xref rid="pone.0331566.ref028" ref-type="bibr">28</xref>], which can perform category-agnostic object counting without additional training, leveraging pre-trained feature extractors.</p></sec><sec id="sec008"><title>2.4 Evaluation metrics</title><sec id="sec009"><title>2.4.1 Overall performance score.</title><p>For each benchmark task, in addition to accuracy, we also compute the Normalized Absolute Error (NAE) score, which is a commonly used metric for evaluating counting abilities that addresses limitations of the most commonly used Mean Absolute Error (MAE). Indeed, MAE treats all errors equally, while NAE normalizes the absolute error by dividing it by the target value, making it sensitive to proportional rather than absolute differences. This normalization ensures fairness across scales, as larger targets inherently permit greater absolute errors without compromising accuracy. NAE also aligns with perceptual principles like Weber&#8217;s law, reflecting the proportional nature of human numerosity estimation. NAE is defined as:</p><disp-formula id="pone.0331566.e002"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.e002g" position="anchor" orientation="portrait" xlink:href="pone.0331566.e002.jpg"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>NAE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula><p>where <italic toggle="yes">n</italic> is the total number of test samples, <italic toggle="yes">G</italic><sub><italic toggle="yes">i</italic></sub> is the generated numerosity for the <italic toggle="yes">i</italic>-th test sample and <italic toggle="yes">T</italic><sub><italic toggle="yes">i</italic></sub> is the target numerosity for the <italic toggle="yes">i</italic>-th test sample.</p><p>As a baseline, we report the NAE of a random model probed on the same number of test trials. This model generates responses randomly sampled from a uniform distribution in the range 1-20 (see related confusion matrix in Supplementary <xref rid="pone.0331566.s001" ref-type="supplementary-material">S1 Fig</xref>), obtaining a NAE of 2.38.</p></sec><sec id="sec010"><title>2.4.2 Counting level.</title><p>We also assess the counting level of each model by applying standard criteria used in the literature on the development of counting skills [<xref rid="pone.0331566.ref036" ref-type="bibr">36</xref>]. To be considered an &#8220;<italic toggle="yes">n</italic>-knower&#8221; (i.e., &#8220;1-knower&#8221;, &#8220;2-knower&#8221;, &#8220;3-knower&#8221;, &#8220;4-knower&#8221;) the model has to: 1) correctly return <italic toggle="yes">n</italic> at least 67% of the time when tested for <italic toggle="yes">n</italic>; and 2) return <italic toggle="yes">n</italic> less than 50% of the time when the target number is different from <italic toggle="yes">n</italic>. The counting level is assessed on the average responses collected across all object categories.</p></sec><sec id="sec011"><title>2.4.3 Estimation performance.</title><p>When explicit counting is precluded, in humans and other animal species numerosity estimation tasks yield a distribution of errors that varies systematically according to Weber&#8217;s law [<xref rid="pone.0331566.ref053" ref-type="bibr">53</xref>]. In particular, while responses are error-free in the 1-4 subitizing range, for larger numerosities the standard error of the estimates increases proportionally to the mean, indicating scalar variability [<xref rid="pone.0331566.ref054" ref-type="bibr">54</xref>,<xref rid="pone.0331566.ref055" ref-type="bibr">55</xref>]. To investigate whether the AI responses follow a human-like estimation pattern, we measure the Pearson correlation between the confusion matrices produced by the models and that obtained from an ideal human observer (see related confusion matrix in Supplementary <xref rid="pone.0331566.s001" ref-type="supplementary-material">S1 Fig</xref>) that estimates numerosity in accordance with Weber&#8217;s law, assuming a standard Weber fraction <italic toggle="yes">w</italic> of 0.15 [<xref rid="pone.0331566.ref056" ref-type="bibr">56</xref>] and error-free responses in the subitizing range.</p></sec><sec id="sec012"><title>2.4.4 Over- and under-estimation trends.</title><p>To investigate the presence of systematic biases in visual enumeration, we analyzed the distribution of prediction errors (i.e., response - target), separately for each model and category. We applied the Wilcoxon signed rank test to assess whether prediction errors significantly deviated from zero (with alpha level set to 0.05). This non-parametric test, chosen for its robustness to non-normal distributions, involves ranking the absolute values of the errors, then comparing the sum of ranks for positive errors against the sum of ranks for negative errors. We then computed the effect size (<italic toggle="yes">r</italic>) using Cohen&#8217;s formula:</p><disp-formula id="pone.0331566.e003"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.e003g" position="anchor" orientation="portrait" xlink:href="pone.0331566.e003.jpg"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula><p>where <italic toggle="yes">z</italic> is the z-score derived from the Wilcoxon test statistic and <italic toggle="yes">n</italic> is the sample size. Following standard practice, we interpret <inline-formula id="pone.0331566.e004"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.e004g" position="anchor" orientation="portrait" xlink:href="pone.0331566.e004.jpg"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> as evidence of a meaningful bias (medium-to-large effect size): a model is classified as overestimating if the sum of ranks for positive errors significantly exceeds that of negative errors and <inline-formula id="pone.0331566.e005"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.e005g" position="anchor" orientation="portrait" xlink:href="pone.0331566.e005.jpg"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, and as underestimating if the opposite occurs and <inline-formula id="pone.0331566.e006"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.e006g" position="anchor" orientation="portrait" xlink:href="pone.0331566.e006.jpg"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p></sec></sec></sec><sec sec-type="results" id="sec013"><title>3 Results</title><p><xref rid="pone.0331566.t001" ref-type="table">Table 1</xref> summarizes all results from our benchmark, ranking the models in terms of NAE. Despite the simplicity of the enumeration tasks, none of the models achieves perfect accuracy. Counting is particularly poor, with no model exceeding the level of 4 items. In humans, accurate performance up to 4 is also supported by subitizing [<xref rid="pone.0331566.ref006" ref-type="bibr">6</xref>], which in turn suggests that current multimodal foundation models cannot count at all. Some of the models mimic the pattern of human visual estimation, as indicated by the strong correlations with the confusion matrices produced by the ideal human observer.</p><table-wrap position="float" id="pone.0331566.t001" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.t001</object-id><label>Table 1</label><caption><title>Leader-board according to Normalized Absolute Error (NAE).</title><p>The Corr w/ Human column reports the correlation with the confusion matrix produced by an ideal human observer. The last column reports the estimated number of model parameters (in Billions).</p></caption><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.t001g" position="float" orientation="portrait" xlink:href="pone.0331566.t001.jpg"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Accuracy &#8593;</th><th align="left" rowspan="1" colspan="1">NAE &#8595;</th><th align="left" rowspan="1" colspan="1">Counting level</th><th align="left" rowspan="1" colspan="1">Corr w/ Human</th><th align="left" rowspan="1" colspan="1">Size (Billion)</th></tr></thead><tbody><tr><td align="left" colspan="6" rowspan="1">
<bold>
<italic toggle="yes">Image-to-text models</italic>
</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Qwen-72B</td><td align="left" rowspan="1" colspan="1">
<bold>0.89</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.01</bold>
</td><td align="left" rowspan="1" colspan="1">4-knower</td><td align="left" rowspan="1" colspan="1">0.89</td><td align="left" rowspan="1" colspan="1">72</td></tr><tr><td align="left" rowspan="1" colspan="1">Qwen-7B</td><td align="left" rowspan="1" colspan="1">0.82</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">4-knower</td><td align="left" rowspan="1" colspan="1">0.90</td><td align="left" rowspan="1" colspan="1">7</td></tr><tr><td align="left" rowspan="1" colspan="1">Gemini Pro</td><td align="left" rowspan="1" colspan="1">0.60</td><td align="left" rowspan="1" colspan="1">0.10</td><td align="left" rowspan="1" colspan="1">4-knower</td><td align="left" rowspan="1" colspan="1">0.93</td><td align="left" rowspan="1" colspan="1">600?</td></tr><tr><td align="left" rowspan="1" colspan="1">GPT-4V</td><td align="left" rowspan="1" colspan="1">0.74</td><td align="left" rowspan="1" colspan="1">0.13</td><td align="left" rowspan="1" colspan="1">4-knower</td><td align="left" rowspan="1" colspan="1">0.92</td><td align="left" rowspan="1" colspan="1">&gt;1000?</td></tr><tr><td align="left" rowspan="1" colspan="1">LLaVa</td><td align="left" rowspan="1" colspan="1">0.37</td><td align="left" rowspan="1" colspan="1">0.13</td><td align="left" rowspan="1" colspan="1">2-knower</td><td align="left" rowspan="1" colspan="1">0.85</td><td align="left" rowspan="1" colspan="1">34</td></tr><tr><td align="left" rowspan="1" colspan="1">VILT</td><td align="left" rowspan="1" colspan="1">0.28</td><td align="left" rowspan="1" colspan="1">0.27</td><td align="left" rowspan="1" colspan="1">1-knower</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.1</td></tr><tr><td align="left" rowspan="1" colspan="1">BLIP2</td><td align="left" rowspan="1" colspan="1">0.29</td><td align="left" rowspan="1" colspan="1">0.33</td><td align="left" rowspan="1" colspan="1">1-knower</td><td align="left" rowspan="1" colspan="1">0.52</td><td align="left" rowspan="1" colspan="1">4</td></tr><tr><td align="left" colspan="6" rowspan="1">
<bold>
<italic toggle="yes">Text-to-image models</italic>
</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">FLUX</td><td align="left" rowspan="1" colspan="1">
<bold>0.44</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.25</bold>
</td><td align="left" rowspan="1" colspan="1">2-knower</td><td align="left" rowspan="1" colspan="1">0.89</td><td align="left" rowspan="1" colspan="1">12</td></tr><tr><td align="left" rowspan="1" colspan="1">SD3.5</td><td align="left" rowspan="1" colspan="1">0.41</td><td align="left" rowspan="1" colspan="1">0.26</td><td align="left" rowspan="1" colspan="1">3-knower</td><td align="left" rowspan="1" colspan="1">0.91</td><td align="left" rowspan="1" colspan="1">8</td></tr><tr><td align="left" rowspan="1" colspan="1">DALLE-2</td><td align="left" rowspan="1" colspan="1">0.34</td><td align="left" rowspan="1" colspan="1">0.37</td><td align="left" rowspan="1" colspan="1">2-knower</td><td align="left" rowspan="1" colspan="1">0.83</td><td align="left" rowspan="1" colspan="1">3.5</td></tr><tr><td align="left" rowspan="1" colspan="1">SD2.1</td><td align="left" rowspan="1" colspan="1">0.28</td><td align="left" rowspan="1" colspan="1">0.39</td><td align="left" rowspan="1" colspan="1">1-knower</td><td align="left" rowspan="1" colspan="1">0.77</td><td align="left" rowspan="1" colspan="1">0.5</td></tr><tr><td align="left" rowspan="1" colspan="1">DALLE-3</td><td align="left" rowspan="1" colspan="1">0.32</td><td align="left" rowspan="1" colspan="1">0.47</td><td align="left" rowspan="1" colspan="1">1-knower</td><td align="left" rowspan="1" colspan="1">0.84</td><td align="left" rowspan="1" colspan="1">?</td></tr></tbody></table></alternatives></table-wrap><sec id="sec014"><title>3.1 Image-to-text models</title><p>All models achieved the minimum number of eligible trials required, without the need of further prompting (total number of responses discarded for ViLT: 0; BLIP-2: 6; LLaVA: 0; Qwen: 0; GPT-4V: 0; Gemini: 20). For all models, the best performance was achieved with the generic &#8220;things&#8221; prompt, while for BLIP-2 and LLaVa the best performance was achieved with the category-specific prompts.</p><p>For ViLT and BLIP-2 the response accuracy was lower than 30%. The corresponding confusion matrices (CMs) reported in <xref rid="pone.0331566.g003" ref-type="fig">Fig 3</xref> clearly show the presence of anchoring effects, leading these models to choose stereotyped responses (e.g., 4 or 6). The pattern of responses for these models also drastically varied between categories (minimum correlation between CMs for ViLT: 0.04, BLIP-2: 0.27), suggesting that they fail to abstract numerical information. Moreover, in sharp contrast with human adults, ViLT and BLIP-2 often returned wrong answers even for images with only one or two objects. According to standard criteria used in human developmental studies, these models can be considered &#8220;1-knowers&#8221;, that is, they can only reliably enumerate single objects, as typical of children younger than three years of age [<xref rid="pone.0331566.ref036" ref-type="bibr">36</xref>]. The responses of LLaVA were slightly more accurate (37%) and consistent across object categories (minimum correlation between CMs: 0.52). This model achieves a &#8220;2-knower&#8221; counting level and also shows a more robust correlation with the confusion matrix produced by the ideal human observer.</p><fig position="float" id="pone.0331566.g003" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.g003</object-id><label>Fig 3</label><caption><title>Confusion matrices for the numerosity naming task.</title><p>Each panel shows the distribution of models&#8217; responses across different object categories: apples, people, butterflies, dots and fast cards. The x-axis represents the target number, while the y-axis represents the corresponding model responses. Response frequency is encoded using a perceptually uniform colormap (blue = 0%, yellow = 100%). Qwen2.5-VL stands for Qwen2.5-VL 72B.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="pone.0331566.g003.jpg"/></fig><p>Among the large multimodal models, Qwen achieved the highest accuracy (89%) with a NAE of 0.01, suggesting that it has the strongest enumeration capabilities. This model performs well across different object categories (minimum correlation between CMs: 0.85) and shows reliable enumeration up to four items, which makes it a &#8220;4-knower&#8221;. Interestingly, the smaller version of Qwen with only 7B parameters still performs reasonably well, reaching an accuracy of 82% and demonstrating a higher correlation with the pattern of responses provided by the ideal human observer. The responses of proprietary foundation models, despite their larger size, are generally less accurate than Qwen (GPT-4V: 74%; Gemini: 60%), suggesting that these systems possess very rudimentary enumeration skills. Confusion matrices are consistent between categories (the minimum correlation is 0.87 for GPT-4V and 0.70 for Gemini). In some cases Gemini produces unexpected responses with images containing only one item, for example answering that <italic toggle="yes">There are two things in the image: an apple and a white background</italic> (these responses were discarded).</p><p>Considering the distribution of response errors, GPT-4V and Gemini, like Qwen, can be characterized as &#8220;4-knowers&#8221;, that is, they exhibit reliable enumeration only up to four items. As noted before, this level of performance is consistent with subitizing (i.e., parallel individuation of visual items), but it also highlights the failure in mastering counting skills.</p><p>As shown in <xref rid="pone.0331566.t002" ref-type="table">Table 2</xref>, image-to-text models have a general tendency to underestimate the numerosity across all object categories. Qwen-72b and GPT-4V are less biased compared to the other models, showing underestimation (Qwen) or overestimation (GPT) trends only for the Dots category.</p><table-wrap position="float" id="pone.0331566.t002" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.t002</object-id><label>Table 2</label><caption><title>Analysis of image-to-text models&#8217; estimation biases across object categories.</title><p>&#8220;-&#8221; indicates no systematic bias, &#8221;Over.&#8221; means the model&#8217;s responses are systematically higher than the ground truth, and &#8221;Under.&#8221; means they are systematically lower. Numbers in brackets represent the effect sizes (Cohen&#8217;s <italic toggle="yes">r</italic>).</p></caption><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.t002g" position="float" orientation="portrait" xlink:href="pone.0331566.t002.jpg"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Apples</th><th align="left" rowspan="1" colspan="1">People</th><th align="left" rowspan="1" colspan="1">Butterflies</th><th align="left" rowspan="1" colspan="1">Dots</th><th align="left" rowspan="1" colspan="1">Fastcards</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">ViLT</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Under. (0.44)</td><td align="left" rowspan="1" colspan="1">Under. (0.68)</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">BLIP2</td><td align="left" rowspan="1" colspan="1">Under. (0.64)</td><td align="left" rowspan="1" colspan="1">Under. (0.61)</td><td align="left" rowspan="1" colspan="1">Under. (0.62)</td><td align="left" rowspan="1" colspan="1">Under. (0.46)</td><td align="left" rowspan="1" colspan="1">Under. (0.54)</td></tr><tr><td align="left" rowspan="1" colspan="1">llava34b</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Over. (0.67)</td><td align="left" rowspan="1" colspan="1">Under. (0.68)</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">Qwen-72b</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Under. (0.51)</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">Qwen-7b</td><td align="left" rowspan="1" colspan="1">Under. (0.46)</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Under. (0.33)</td><td align="left" rowspan="1" colspan="1">Under. (0.40)</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">GPT-4V</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Over. (0.48)</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">Gemini Pro</td><td align="left" rowspan="1" colspan="1">Under. (0.61)</td><td align="left" rowspan="1" colspan="1">Under. (0.67)</td><td align="left" rowspan="1" colspan="1">Under. (0.50)</td><td align="left" rowspan="1" colspan="1">Under. (0.42)</td><td align="left" rowspan="1" colspan="1">Over. (0.33)</td></tr></tbody></table></alternatives></table-wrap><p>Notably, the counting-specific architectures considered as baseline were not able to successfully count the target objects across all categories, suggesting that models trained with real images might struggle with generalizing their counting skills on synthetic visual stimuli (see <xref rid="pone.0331566.s002" ref-type="supplementary-material">S2 Fig</xref> in Supplementary Information).</p></sec><sec id="sec015"><title>3.2 Text-to-image models</title><p>Examples of generated images are shown in <xref rid="pone.0331566.g004" ref-type="fig">Fig 4</xref>. In comparing the outputs of different text-to-image models, we observed noteworthy stylistic trends across both open-source and proprietary systems. The earlier version of Stable Diffusion (SD2.1) demonstrates a broader artistic scope, spanning paintings, clip-art, and realistic renderings. The latest open-source models (SD3.5 and Flux) appear to have narrowed their stylistic variations, resulting in more coherent and realistic images. Proprietary models from the DALL-E family instead produce visual scenes with a characteristic synthetic style. DALL-E 2 mostly generates images with white backgrounds and well-defined objects, while DALL-E 3 injects more details, but still retaining a cartoon-like style (in a few cases DALL-E 2 generated images containing reflections: we discarded these trials to avoid ambiguous identification of objects by the automatic evaluation pipeline).</p><fig position="float" id="pone.0331566.g004" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.g004</object-id><label>Fig 4</label><caption><title>Examples of images generated by text-to-image models in the numerosity production task, showcasing both correct and wrong generations (the target number is indicated at the bottom).</title><p>We report two images for each target category: apples, people, butterflies, and dots. For the dots category, in a few cases FLUX and DALL-E 2 generated images containing a wrong number of dots, which were nevertheless arranged according to the target digit shape (e.g., 8 in the figure). Surprisingly, SD3.5 was unable to generate a single correct response when the target numerosity was larger than 8.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="pone.0331566.g004.jpg"/></fig><p>The mean response accuracy was fairly low for all models (ranging between 0.28 and 0.44), suggesting that the numerosity production task is far more challenging than the numerosity naming task. Confusion matrices are shown in <xref rid="pone.0331566.g005" ref-type="fig">Fig 5</xref>. Similarly to the naming task, the response patterns were not homogeneous across categories (minimum correlation between CMs for SD2.1: 0.43, SD3.5: 0.37, FLUX: 0.69, DALL-E 2: 0.63, DALL-E 3: 0.56). In a few cases SD3.5, FLUX and DALL-E 2 exhibit error-free responses, but that mostly happens for the generation of a single object (and not across all categories). All other models make errors even in this condition. The NAE ranges from 0.25 (FLUX) to 0.47 (DALL-E 3). According to criteria used in human developmental studies, the highest counting level is achieved by SD3.5, which can be classified as a &#8220;3-knower&#8221;.</p><fig position="float" id="pone.0331566.g005" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.g005</object-id><label>Fig 5</label><caption><title>Confusion matrices for the numerosity production task.</title><p>The x-axis represents the target number, while the y-axis represents the corresponding model responses. Response frequency is encoded using a perceptually uniform colormap (blue = 0%, yellow = 100%).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="pone.0331566.g005.jpg"/></fig><p><xref rid="pone.0331566.t003" ref-type="table">Table 3</xref> shows the results of the analysis of over- vs. under-estimation trends for all text-to-image models; in this case we observe a tendency to overestimate.</p><table-wrap position="float" id="pone.0331566.t003" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.t003</object-id><label>Table 3</label><caption><title>Analysis of text-to-image models&#8217; estimation biases across object categories.</title><p>&#8220;-&#8221; indicates no systematic bias, &#8220;Over.&#8221; means the model&#8217;s outputs are systematically higher than ground truth, and &#8220;Under.&#8221; means they were systematically lower. Numbers in brackets represent the effect sizes (Cohen&#8217;s <italic toggle="yes">r</italic>).</p></caption><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="pone.0331566.t003g" position="float" orientation="portrait" xlink:href="pone.0331566.t003.jpg"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Apples</th><th align="left" rowspan="1" colspan="1">People</th><th align="left" rowspan="1" colspan="1">Butterflies</th><th align="left" rowspan="1" colspan="1">Dots</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">SD2.1</td><td align="left" rowspan="1" colspan="1">Over. (0.3)</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Over. (0.45)</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">SD3.5</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Under. (0.44)</td><td align="left" rowspan="1" colspan="1">Over. (0.57)</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">FLUX</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Over. (0.45)</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">DALLE-2</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Under. (0.34)</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td></tr><tr><td align="left" rowspan="1" colspan="1">DALLE-3</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">Over. (0.64)</td><td align="left" rowspan="1" colspan="1">Over. (0.76)</td><td align="left" rowspan="1" colspan="1">-</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec016"><title>3.3 Why is visual enumeration so challenging?</title><p>The poor performance of state-of-the-art AI models in a simple task like visual enumeration might seem surprising, given their impressive range of emergent abilities and considering that signatures of number sense have been observed in smaller-scale deep learning models (for discussion, see [<xref rid="pone.0331566.ref018" ref-type="bibr">18</xref>]).</p><p>One possible explanation lies in the statistical properties of the material used to train these foundation models. Indeed, it has been recently shown that the performance of multimodal models scales linearly as the concept frequency in pre-training data grows exponentially, which means that &#8220;zero-shot&#8221; performance in tasks involving underrepresented concepts will normally be poor [<xref rid="pone.0331566.ref057" ref-type="bibr">57</xref>]. To better characterize possible biases in the distribution of numerical information in popular training datasets, we conducted an in-depth analysis on the frequency of appearance of different numerosities in two datasets commonly used to train large-scale multimodal AI systems: Conceptual Captions 3 Million (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ai.google.com/research/ConceptualCaptions/download" ext-link-type="uri">CC3M</ext-link>) [<xref rid="pone.0331566.ref058" ref-type="bibr">58</xref>], which contains more than 3 million image-caption pairs, and <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://laion.ai/blog/laion-400-open-dataset/" ext-link-type="uri">LAION-400M</ext-link> [<xref rid="pone.0331566.ref059" ref-type="bibr">59</xref>], a large collection of 400 million English (image, text) pairs. Our investigation focused on examining the distribution of textual numerosities across all image captions present in the datasets, deploying natural language processing tools to identify numbers referring to countable objects in the image. In order to identify target numerosities, we first defined a valid numerosity as a textual segment containing a numerical token followed by either a noun or an optional adjective plus a noun (e.g., &#8220;1 apple&#8221; or &#8220;3 tall trees&#8221;). This criterion allowed to focus on explicit numerical references to countable objects while excluding textual references where numbers serve as model identifiers (e.g., &#8220;Porsche 911&#8221;) or cardinal descriptors in product names (e.g., &#8220;35th anniversary&#8221;). To implement this filtering process, we employed the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://spacy.io/" ext-link-type="uri">spaCy</ext-link> library to determine the part-of-speech (POS) tags for each word in the captions. After locating candidate structures, we systematically excluded measurements and units. The exclusion criteria encompassed a comprehensive range of metric prefixes, spanning from the microscopic to the massive: micro-, milli-, centi-, kilo-, mega-, giga-, nano-, tera-, and peta-. These prefixes were filtered out when combined with common measurement units. The excluded base units covered fundamental physical quantities: distance (meter/metre, mile, foot, inch, yard), volume (liter/litre), mass (gram, ounce, pound), power (watt), electrical potential (volt), current (amp), and energy (joule). Time-related measurements were similarly excluded, ranging from seconds to years, as were temperature scales (Fahrenheit, Celsius). By applying filtering criteria, we derived a more precise set of image-text pairs that genuinely reflected countable numerosities.</p><p>We analyzed the numerical information in image captions merging together heterogeneous object types, that is, all numerosities mentioned in a caption were summed into a single value, regardless of the object categories. For example, the caption &#8220;An image of 3 bananas and 2 apples&#8221; is counted as a total numerosity of 5, treating the quantities as a single aggregate and thus capturing the overall number of items described in the caption. Our analysis revealed that in both datasets numerosities are distributed according to a power law, which implies that larger numerosities are strongly underrepresented compared to small numerosities (see <xref rid="pone.0331566.g006" ref-type="fig">Fig 6</xref>). Interestingly, the frequency of appearance of decades (i.e., 10, 20, 30, 40, etc.) decreases less sharply, indicating a preferential bias for these regular numerosities in the training material.</p><fig position="float" id="pone.0331566.g006" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0331566.g006</object-id><label>Fig 6</label><caption><title>Power-law distribution of textual numerosities related to countable objects in the CC3M dataset (upper panel) and LAION-400M dataset (lower panel).</title><p>Decade numbers are highlighted in red. The y-axis represents the relative frequency of appearance. A broken y-axis is used to accommodate the large difference in peak frequencies between the two datasets. The layout is proportioned such that the lower-frequency regions of both datasets share the same vertical height, making them visually comparable despite the scale difference.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="pone.0331566.g006.jpg"/></fig><p>Such biased distribution of numerosities might contribute to the weak enumeration abilities we observed in all models, especially for images containing a large number of items. Nevertheless, it has been previously shown that signatures of number sense can emerge in smaller-scale models trained with synthetic (black and white) images containing a variable number of geometric shapes, even when numerosities are sampled according to the power-law Zipfian distribution observed in natural images [<xref rid="pone.0331566.ref060" ref-type="bibr">60</xref>]. This suggests that the poor number sense of large-scale AI systems might also (at least partially) stem from the more complex visual properties of real images, whose richness in finer-grained details could prevent the emergence of more explicit numerosity representations.</p><p>Finally, an additional issue could be the presence of noise in the linguistic captions, since the numerical information provided in the descriptive text sometimes might be completely unrelated to the numerosity depicted in the image. We manually checked a few image-text pairs containing explicit numerical information in their captions to assess whether the reported numerosities accurately matched the corresponding visual content, and indeed we found that sometimes the numerical text is misaligned with the image content (see examples in Supplementary <xref rid="pone.0331566.s003" ref-type="supplementary-material">S3 Fig</xref>). These noisy (image, text) pairs make it challenging to achieve an accurate semantic alignment between visual and textual representations, amplifying problems related to the modality gap observed in multimodal AI systems [<xref rid="pone.0331566.ref061" ref-type="bibr">61</xref>].</p></sec></sec><sec sec-type="conclusions" id="sec017"><title>4 Discussion</title><p>The present work demonstrates that modern AI systems cannot yet reliably enumerate the number of objects in a visual scene, both in image-to-text and text-to-image tasks. Such a striking deficit is often observed even for sets containing only a few items, suggesting a counting level that is at best comparable to that of preschool children who do not fully master the counting principles [<xref rid="pone.0331566.ref036" ref-type="bibr">36</xref>,<xref rid="pone.0331566.ref037" ref-type="bibr">37</xref>]. Exact enumeration up to <italic toggle="yes">n</italic>=4 is consistent with subitizing, which in humans is supported by fast parallel individuation related to object tracking [<xref rid="pone.0331566.ref062" ref-type="bibr">62</xref>] and is independent from counting skills. This observation fits well with the finding that the best performing models generate responses to larger numbers that broadly follow the pattern of human numerical estimation, with scalar variability of the response distribution. Overall, these findings demonstrate that multimodal foundation models do not master counting skills, though the best models exhibit sparks of human-like number sense.</p><p>The ability to represent and manipulate visual numerosity should be regarded as a foundational skill for multimodal AI systems because it would ground the subsequent learning of more complex numerical and arithmetic concepts. Numerosity in humans is a primary visual feature (just as orientation or color) [<xref rid="pone.0331566.ref063" ref-type="bibr">63</xref>] and it is encoded by neuronal populations in multiple cortical regions in the primate brain [<xref rid="pone.0331566.ref064" ref-type="bibr">64</xref>&#8211;<xref rid="pone.0331566.ref067" ref-type="bibr">67</xref>]. Computational modeling studies have shown that sensitivity to numerosity can indeed emerge in small-scale deep learning models trained to generate synthetic images of object sets [<xref rid="pone.0331566.ref019" ref-type="bibr">19</xref>,<xref rid="pone.0331566.ref020" ref-type="bibr">20</xref>,<xref rid="pone.0331566.ref068" ref-type="bibr">68</xref>]: diffusion models, such as Stable Diffusion, FLUX and DALL-E, are trained with a similar objective on huge and heterogeneous image datasets that most likely contain substantial variability in numerosity. However, our analyses have shown that the empirical distribution of numerosities in the (image, text) pairs commonly used to train these systems follows a power law, therefore it might be possible that oversampling of small numerosities in the training corpora of foundation models has detrimental effects on their emergent representational space [<xref rid="pone.0331566.ref057" ref-type="bibr">57</xref>]. Another issue might lie in the mapping between perceptual numerosity representations, encoded in image embeddings, and number symbols (number words or Arabic digits) encoded in text embeddings. In children, establishing such a bidirectional mapping is a sophisticated developmental process, which takes many years and requires explicit instruction [<xref rid="pone.0331566.ref069" ref-type="bibr">69</xref>]. The noisy nature of the (image, text) pairs used to train multimodal models might prevent the creation of a systematic mapping between different input modalities, with detrimental effects on the emergence of abstract numerosity representations.</p><p>It is also interesting to observe that even the most advanced proprietary models do not exhibit perfect accuracy on such simple tasks, suggesting that numerosity estimation has not been explicitly built-in in these systems. Nevertheless, one cannot exclude the possibility that some counting mechanisms were at least partially engineered as extra processing layers during prompt elaboration. Furthermore, it is well-known that the most recent AI systems can exploit the self-generation of code snippets to fulfill a user request, as in the case of mathematical problem solving [<xref rid="pone.0331566.ref070" ref-type="bibr">70</xref>], therefore models like GPT or Gemini could in principle also exploit external tools (e.g., based on object detection and a symbolic counting algorithm) to carry out these visual enumeration tasks. Quite surprisingly, however, in fact we noticed that compared to our own previous investigations [<xref rid="pone.0331566.ref032" ref-type="bibr">32</xref>] the performance of some proprietary models (DALL-E) has slightly degraded, suggesting that rather than pushing for improving number sense, the newest API provided by OpenAI might have actually reduced the compute budget available to the user. These issues highlight the dangers of using proprietary models in academic research [<xref rid="pone.0331566.ref071" ref-type="bibr">71</xref>], and calls for a broader adoption of open-source alternatives. In the case of our visual enumeration benchmark, it is encouraging to see that the best performance in both numerosity naming and numerosity production tasks was achieved by open-source models (Qwen and FLUX, respectively), demonstrating that future scientific investigations could be carried out by relying on transparent and well-documented neural architectures.</p></sec><sec sec-type="conclusions" id="sec018"><title>5 Conclusions</title><p>This work shows that systematic visual counting skills do not spontaneously emerge even in the most advanced foundation models, suggesting that significant progress in the design and training of multimodal architectures is still required to create systems that can reliably process visual quantities [<xref rid="pone.0331566.ref072" ref-type="bibr">72</xref>]. Efforts to improve the enumeration capabilities of multimodal models have been recently flourishing, highlighting the relevance of this type of benchmark for a comprehensive assessment of their emergent abilities [<xref rid="pone.0331566.ref032" ref-type="bibr">32</xref>,<xref rid="pone.0331566.ref073" ref-type="bibr">73</xref>]. For example, recent work suggests that fine-tuning the basic CLIP image-to-text model with a counting-contrastive loss can improve its ability to count the number of objects in images up to ten [<xref rid="pone.0331566.ref074" ref-type="bibr">74</xref>]. Another recently proposed approach is to enhance numerical reasoning by improving the numerical captions of the images used in training corpora [<xref rid="pone.0331566.ref075" ref-type="bibr">75</xref>]. For text-to-image models, others have proposed to implement counting guidance by using gradients of a counting network during the generative diffusion process, which however seems effective only for objects with a relatively simple shape [<xref rid="pone.0331566.ref076" ref-type="bibr">76</xref>]. We believe that our benchmark constitutes an important step forward to investigate these issues, possibly extending the upper limit of tested numerosities well beyond the 1-10 range as the performance of future AI models will improve.</p><p>It should also be noted that all models tested in the present work generate their output in a single step, thereby mimicking the kind of processing supported by an approximate estimation system operating in parallel [<xref rid="pone.0331566.ref077" ref-type="bibr">77</xref>]. However, in order to determine the exact number of items in a visual scene humans have learned to deploy iterative counting algorithms, which allow establishing a one-to-one mapping between visual items and number symbols [<xref rid="pone.0331566.ref010" ref-type="bibr">10</xref>]. This nevertheless requires generating outputs in a sequential manner, in analogy to &#8220;chain-of-thought&#8221; methods that have indeed proven useful to improve AI reasoning [<xref rid="pone.0331566.ref078" ref-type="bibr">78</xref>]. Whether a similar approach could be used to tackle the enumeration tasks presented in this work is still an open question , which might be addressed by considering modern methods that can incrementally guide image synthesis and editing using multiple modalities [<xref rid="pone.0331566.ref079" ref-type="bibr">79</xref>].</p><p>In conclusion, we believe that a better visual grounding of numeracy development in AI models could be the key to enabling these systems to acquire and more reliably master mathematical knowledge [<xref rid="pone.0331566.ref080" ref-type="bibr">80</xref>] or even geometrical principles [<xref rid="pone.0331566.ref081" ref-type="bibr">81</xref>] without resorting to highly specialized hybrid architectures [<xref rid="pone.0331566.ref082" ref-type="bibr">82</xref>], which could open new possibilities for the use of AI in symbolic reasoning and knowledge discovery [<xref rid="pone.0331566.ref083" ref-type="bibr">83</xref>].</p></sec><sec id="sec019" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0331566.s001" position="float" content-type="local-data" orientation="portrait"><label>S1 Fig</label><caption><title>Confusion matrices for baseline models.</title><p>On the left is the random choice model, while on the right is the ideal human observer, which has perfect accuracy in the subitizing range (1-4) and approximately estimates larger numbers according to Weber&#8217;s law.</p><p>(PDF)</p></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0331566.s001.pdf" position="float" orientation="portrait"/></supplementary-material><supplementary-material id="pone.0331566.s002" position="float" content-type="local-data" orientation="portrait"><label>S2 Fig</label><caption><title>Confusion matrices for counting-specific models for the numerosity naming task.</title><p>Each panel shows the distribution of models&#8217; responses across different object categories: apples, people, butterflies, dots and fast cards. The x-axis represents the target number, while the y-axis represents the corresponding model responses. Response frequency is encoded using a perceptually uniform colormap (blue = 0%, yellow = 100%).</p><p>(PDF)</p></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0331566.s002.pdf" position="float" orientation="portrait"/></supplementary-material><supplementary-material id="pone.0331566.s003" position="float" content-type="local-data" orientation="portrait"><label>S3 Fig</label><caption><title>Examples of numerical misalignment between images and textual captions in the LAION-400M training corpora.</title><p>The numerical information in the text is highlighted in red.</p><p>(PDF)</p></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0331566.s003.pdf" position="float" orientation="portrait"/></supplementary-material><supplementary-material id="pone.0331566.s004" position="float" content-type="local-data" orientation="portrait"><label>S4 Fig</label><caption><title>NAE of the image validation set as a function of Grounding DINO&#8217;s threshold.</title><p>The optimal threshold was found at 0.40.</p><p>(PDF)</p></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0331566.s004.pdf" position="float" orientation="portrait"/></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0331566.ref001"><label>1</label><mixed-citation publication-type="journal"><name name-style="western"><surname>McKinney</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Sieniek</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Godbole</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Godwin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Antropova</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ashrafian</surname><given-names>H</given-names></name>, <etal>et al</etal>. <article-title>International evaluation of an AI system for breast cancer screening</article-title>. <source>Nature</source>. <year>2020</year>;<volume>577</volume>(<issue>7788</issue>):<fpage>89</fpage>&#8211;<lpage>94</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-019-1799-6</pub-id><pub-id pub-id-type="pmid">31894144</pub-id></mixed-citation></ref><ref id="pone.0331566.ref002"><label>2</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Gilardi</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Alizadeh</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kubli</surname><given-names>M</given-names></name>. <article-title>ChatGPT outperforms crowd workers for text-annotation tasks</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2023</year>;<volume>120</volume>(<issue>30</issue>):e2305016120. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.2305016120</pub-id><pub-id pub-id-type="pmid">37463210</pub-id><pub-id pub-id-type="pmcid">PMC10372638</pub-id></mixed-citation></ref><ref id="pone.0331566.ref003"><label>3</label><mixed-citation publication-type="other">Dehaene S. The number sense: How the mind creates mathematics. OUP USA. 2011.</mixed-citation></ref><ref id="pone.0331566.ref004"><label>4</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Izard</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Sann</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Spelke</surname><given-names>ES</given-names></name>, <name name-style="western"><surname>Streri</surname><given-names>A</given-names></name>. <article-title>Newborn infants perceive abstract numbers</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2009</year>;<volume>106</volume>(<issue>25</issue>):<fpage>10382</fpage>&#8211;<lpage>5</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.0812142106</pub-id><pub-id pub-id-type="pmid">19520833</pub-id><pub-id pub-id-type="pmcid">PMC2700913</pub-id></mixed-citation></ref><ref id="pone.0331566.ref005"><label>5</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Sella</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Berteletti</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Lucangeli</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>. <article-title>Spontaneous non-verbal counting in toddlers</article-title>. <source>Dev Sci</source>. <year>2016</year>;<volume>19</volume>(<issue>2</issue>):<fpage>329</fpage>&#8211;<lpage>37</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/desc.12299</pub-id><pub-id pub-id-type="pmid">25754974</pub-id></mixed-citation></ref><ref id="pone.0331566.ref006"><label>6</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Feigenson</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dehaene</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Spelke</surname><given-names>E</given-names></name>. <article-title>Core systems of number</article-title>. <source>Trends Cogn Sci</source>. <year>2004</year>;<volume>8</volume>(<issue>7</issue>):<fpage>307</fpage>&#8211;<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.tics.2004.05.002</pub-id><pub-id pub-id-type="pmid">15242690</pub-id></mixed-citation></ref><ref id="pone.0331566.ref007"><label>7</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Cicchini</surname><given-names>GM</given-names></name>, <name name-style="western"><surname>Anobile</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Burr</surname><given-names>DC</given-names></name>. <article-title>Spontaneous perception of numerosity in humans</article-title>. <source>Nat Commun</source>. <year>2016</year>;<volume>7</volume>:<fpage>12536</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/ncomms12536</pub-id><pub-id pub-id-type="pmid">27555562</pub-id><pub-id pub-id-type="pmcid">PMC4999503</pub-id></mixed-citation></ref><ref id="pone.0331566.ref008"><label>8</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Gallistel</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Gelman</surname><given-names>R</given-names></name>. <article-title>Preverbal and verbal counting and computation</article-title>. <source>Cognition</source>. <year>1992</year>;<volume>44</volume>(1&#8211;2):<fpage>43</fpage>&#8211;<lpage>74</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0010-0277(92)90050-r</pub-id><pub-id pub-id-type="pmid">1511586</pub-id></mixed-citation></ref><ref id="pone.0331566.ref009"><label>9</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Halberda</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mazzocco</surname><given-names>MMM</given-names></name>, <name name-style="western"><surname>Feigenson</surname><given-names>L</given-names></name>. <article-title>Individual differences in non-verbal number acuity correlate with maths achievement</article-title>. <source>Nature</source>. <year>2008</year>;<volume>455</volume>(<issue>7213</issue>):<fpage>665</fpage>&#8211;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nature07246</pub-id><pub-id pub-id-type="pmid">18776888</pub-id></mixed-citation></ref><ref id="pone.0331566.ref010"><label>10</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Carey</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Barner</surname><given-names>D</given-names></name>. <article-title>Ontogenetic origins of human integer representations</article-title>. <source>Trends Cogn Sci</source>. <year>2019</year>;<volume>23</volume>(<issue>10</issue>):<fpage>823</fpage>&#8211;<lpage>35</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.tics.2019.07.004</pub-id><pub-id pub-id-type="pmid">31439418</pub-id></mixed-citation></ref><ref id="pone.0331566.ref011"><label>11</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Dolfi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Decarli</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lunardon</surname><given-names>M</given-names></name>, <name name-style="western"><surname>De Filippo De Grazia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gerola</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lanfranchi</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Weaker number sense accounts for impaired numerosity perception in dyscalculia: Behavioral and computational evidence</article-title>. <source>Dev Sci</source>. <year>2024</year>;<volume>27</volume>(<issue>6</issue>):e13538. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/desc.13538</pub-id><pub-id pub-id-type="pmid">38949566</pub-id></mixed-citation></ref><ref id="pone.0331566.ref012"><label>12</label><mixed-citation publication-type="other">Arteta C, Lempitsky V, Zisserman A. Counting in the wild. In: European conference on computer vision (ECCV), Amsterdam, The Netherlands; 2016. 483&#8211;98.</mixed-citation></ref><ref id="pone.0331566.ref013"><label>13</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Khan</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Menouar</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hamila</surname><given-names>R</given-names></name>. <article-title>Revisiting crowd counting: State-of-the-art, trends, and future perspectives</article-title>. <source>Image Vision Comput</source>. <year>2023</year>;<volume>129</volume>:<fpage>104597</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.imavis.2022.104597</pub-id></mixed-citation></ref><ref id="pone.0331566.ref014"><label>14</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Gao</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zhao</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>X</given-names></name>. <article-title>NWPU-MOC: A benchmark for fine-grained multicategory object counting in aerial images</article-title>. <source>IEEE Trans Geosci Remote Sensing</source>. <year>2024</year>;<volume>62</volume>:<fpage>1</fpage>&#8211;<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tgrs.2024.3356492</pub-id></mixed-citation></ref><ref id="pone.0331566.ref015"><label>15</label><mixed-citation publication-type="other">Trott A, Xiong C, Socher R. Interpretable counting for visual question answering. In: International conference on learning representations; 2018.</mixed-citation></ref><ref id="pone.0331566.ref016"><label>16</label><mixed-citation publication-type="other">Zhang Y, Hare J, Pr&#252;gel-Bennett A. Learning to count objects in natural images for visual question answering. In: International conference on learning representations; 2018.</mixed-citation></ref><ref id="pone.0331566.ref017"><label>17</label><mixed-citation publication-type="other">Chattopadhyay P, Vedantam R, Selvaraju RR, Batra D, Parikh D. Counting everyday objects in everyday scenes. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 1135&#8211;44.</mixed-citation></ref><ref id="pone.0331566.ref018"><label>18</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>. <article-title>An emergentist perspective on the origin of number sense</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2017</year>;<volume>373</volume>(<issue>1740</issue>):<fpage>20170043</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1098/rstb.2017.0043</pub-id><pub-id pub-id-type="pmid">29292348</pub-id><pub-id pub-id-type="pmcid">PMC5784047</pub-id></mixed-citation></ref><ref id="pone.0331566.ref019"><label>19</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Stoianov</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>. <article-title>Emergence of a &#8220;visual number sense&#8221; in hierarchical generative models</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>2</issue>):<fpage>194</fpage>&#8211;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nn.2996</pub-id><pub-id pub-id-type="pmid">22231428</pub-id></mixed-citation></ref><ref id="pone.0331566.ref020"><label>20</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Dolfi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Rochus</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>. <article-title>Visual sense of number vs. sense of magnitude in humans and machines</article-title>. <source>Sci Rep</source>. <year>2020</year>;<volume>10</volume>(<issue>1</issue>):<fpage>10045</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-020-66838-5</pub-id><pub-id pub-id-type="pmid">32572067</pub-id><pub-id pub-id-type="pmcid">PMC7308388</pub-id></mixed-citation></ref><ref id="pone.0331566.ref021"><label>21</label><mixed-citation publication-type="other">Nasr K, Viswanathan P, Nieder A. Number detectors spontaneously emerge in a deep neural network designed for visual object recognition. Science advances. 2019 ;5(5):eaav7903.<pub-id pub-id-type="doi" assigning-authority="pmc">10.1126/sciadv.aav7903</pub-id><pub-id pub-id-type="pmcid">PMC6506249</pub-id><pub-id pub-id-type="pmid">31086820</pub-id></mixed-citation></ref><ref id="pone.0331566.ref022"><label>22</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Mistry</surname><given-names>PK</given-names></name>, <name name-style="western"><surname>Strock</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Young</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Menon</surname><given-names>V</given-names></name>. <article-title>Learning-induced reorganization of number neurons and emergence of numerical representations in a biologically inspired neural network</article-title>. <source>Nat Commun</source>. <year>2023</year>;<volume>14</volume>(<issue>1</issue>):<fpage>3843</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-023-39548-5</pub-id><pub-id pub-id-type="pmid">37386013</pub-id><pub-id pub-id-type="pmcid">PMC10310708</pub-id></mixed-citation></ref><ref id="pone.0331566.ref023"><label>23</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Li</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gan</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Multimodal foundation models: From specialists to general-purpose assistants</article-title>. <source>FNT Comput Graph Vision</source>. <year>2024</year>;<volume>16</volume>(1&#8211;2):<fpage>1</fpage>&#8211;<lpage>214</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1561/0600000110</pub-id></mixed-citation></ref><ref id="pone.0331566.ref024"><label>24</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Wei</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tay</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bommasani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Raffel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Zoph</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Borgeaud</surname><given-names>S</given-names></name>. <article-title>Emergent abilities of large language models</article-title>. <source>Trans Mach Learn Res</source>. <year>2022</year>.</mixed-citation></ref><ref id="pone.0331566.ref025"><label>25</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Bommasani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Hudson</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Adeli</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Altman</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Arora</surname><given-names>S</given-names></name>, <name name-style="western"><surname>von Arx</surname><given-names>S</given-names></name>. On the opportunities and risks of foundation models. arXiv preprint; <year>2021</year>. <comment>doi: arXiv:210807258</comment></mixed-citation></ref><ref id="pone.0331566.ref026"><label>26</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Bubeck</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Chandrasekaran</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Eldan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gehrke</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Horvitz</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Kamar</surname><given-names>E</given-names></name>, <etal>et al</etal>. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint; <year>2023</year>. <comment>doi: arXiv:230312712</comment></mixed-citation></ref><ref id="pone.0331566.ref027"><label>27</label><mixed-citation publication-type="other">Huang Z, Dai M, Zhang Y, Zhang J, Shan H. Point segment and count: A generalized framework for object counting. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition; 2024. p. 17067&#8211;76.</mixed-citation></ref><ref id="pone.0331566.ref028"><label>28</label><mixed-citation publication-type="other">Shi Z, Sun Y, Zhang M. Training-free object counting with prompts. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision; 2024. p. 323&#8211;31.</mixed-citation></ref><ref id="pone.0331566.ref029"><label>29</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Romeo</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>. <article-title>Artificial intelligence can emulate human normative judgments on emotional visual scenes</article-title>. <source>R Soc Open Sci</source>. <year>2025</year>;<volume>12</volume>(<issue>7</issue>):<fpage>250128</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1098/rsos.250128</pub-id><pub-id pub-id-type="pmid">40740716</pub-id><pub-id pub-id-type="pmcid">PMC12308228</pub-id></mixed-citation></ref><ref id="pone.0331566.ref030"><label>30</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>. <article-title>Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models</article-title>. <source>Appl Sci</source>. <year>2024</year>;<volume>14</volume>(<issue>2</issue>):<fpage>744</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/app14020744</pub-id></mixed-citation></ref><ref id="pone.0331566.ref031"><label>31</label><mixed-citation publication-type="other">Rane S, Ku A, Baldridge J, Tenney I, Griffiths T, Kim B. Can generative multimodal models count to ten? In: Proceedings of the annual meeting of the cognitive science society; 2024.</mixed-citation></ref><ref id="pone.0331566.ref032"><label>32</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hou</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>. Visual enumeration is challenging for largescale generative AI; <year>2024</year>. <comment>doi: arXiv:240203328</comment><pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pone.0331566</pub-id><pub-id pub-id-type="pmid">40938963</pub-id></mixed-citation></ref><ref id="pone.0331566.ref033"><label>33</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Binz</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Schulz</surname><given-names>E</given-names></name>. <article-title>Using cognitive psychology to understand GPT-3</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2023</year>;<volume>120</volume>(<issue>6</issue>):e2218523120. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.2218523120</pub-id><pub-id pub-id-type="pmid">36730192</pub-id><pub-id pub-id-type="pmcid">PMC9963545</pub-id></mixed-citation></ref><ref id="pone.0331566.ref034"><label>34</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Revkin</surname><given-names>SK</given-names></name>, <name name-style="western"><surname>Piazza</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Izard</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Cohen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dehaene</surname><given-names>S</given-names></name>. <article-title>Does subitizing reflect numerical estimation?</article-title>. <source>Psychol Sci</source>. <year>2008</year>;<volume>19</volume>(<issue>6</issue>):<fpage>607</fpage>&#8211;<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02130.x</pub-id><pub-id pub-id-type="pmid">18578852</pub-id></mixed-citation></ref><ref id="pone.0331566.ref035"><label>35</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Whalen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gallistel</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Gelman</surname><given-names>R</given-names></name>. <article-title>Nonverbal counting in humans: The psychophysics of number representation</article-title>. <source>Psychol Sci</source>. <year>1999</year>;<volume>10</volume>(<issue>2</issue>):<fpage>130</fpage>&#8211;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/1467-9280.00120</pub-id></mixed-citation></ref><ref id="pone.0331566.ref036"><label>36</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Le Corre</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Carey</surname><given-names>S</given-names></name>. <article-title>One, two, three, four, nothing more: An investigation of the conceptual sources of the verbal counting principles</article-title>. <source>Cognition</source>. <year>2007</year>;<volume>105</volume>(<issue>2</issue>):<fpage>395</fpage>&#8211;<lpage>438</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cognition.2006.10.005</pub-id><pub-id pub-id-type="pmid">17208214</pub-id><pub-id pub-id-type="pmcid">PMC3880652</pub-id></mixed-citation></ref><ref id="pone.0331566.ref037"><label>37</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Lee</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Sarnecka</surname><given-names>BW</given-names></name>. <article-title>Number-knower levels in young children: Insights from Bayesian modeling</article-title>. <source>Cognition</source>. <year>2011</year>;<volume>120</volume>(<issue>3</issue>):<fpage>391</fpage>&#8211;<lpage>402</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cognition.2010.10.003</pub-id><pub-id pub-id-type="pmid">21109239</pub-id><pub-id pub-id-type="pmcid">PMC3116985</pub-id></mixed-citation></ref><ref id="pone.0331566.ref038"><label>38</label><mixed-citation publication-type="other">Kim W, Son B, Kim I. ViLT: Vision-and-language transformer without convolution or region supervision. In: International conference on machine learning; 2021. p. 5583&#8211;94.</mixed-citation></ref><ref id="pone.0331566.ref039"><label>39</label><mixed-citation publication-type="other">Li J, Li D, Savarese S, Hoi S. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In: International conference on machine learning; 2023.</mixed-citation></ref><ref id="pone.0331566.ref040"><label>40</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Liu</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>YJ</given-names></name>. <article-title>Visual instruction tuning</article-title>. <source>Adv Neural Inform Process Syst</source>. <year>2024</year>;<volume>36</volume>.<pub-id pub-id-type="pmcid">PMC11867732</pub-id><pub-id pub-id-type="pmid">40017809</pub-id></mixed-citation></ref><ref id="pone.0331566.ref041"><label>41</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Bai</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ge</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Song</surname><given-names>S</given-names></name>. Qwen2. 5-vl technical report. <year>2025</year>. <comment>doi: arXiv:250213923</comment></mixed-citation></ref><ref id="pone.0331566.ref042"><label>42</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Yang</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>CC</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>The dawn of lmms: Preliminary explorations with gpt-4v (ision)</article-title>. <source>arXiv preprint</source>. <year>2023</year>;<volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.48550/arXiv.230917421</pub-id></mixed-citation></ref><ref id="pone.0331566.ref043"><label>43</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Team</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Anil</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Borgeaud</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Alayrac</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>J</given-names></name>. Gemini: A family of highly capable multimodal models. arXiv preprint; <year>2023</year>. <comment>doi: arXiv:2312.11805</comment></mixed-citation></ref><ref id="pone.0331566.ref044"><label>44</label><mixed-citation publication-type="other">Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B. High-resolution image synthesis with latent diffusion models. In: IEEE/CVF conference on computer vision and pattern recognition; 2022.</mixed-citation></ref><ref id="pone.0331566.ref045"><label>45</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Podell</surname><given-names>D</given-names></name>, <name name-style="western"><surname>English</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Lacey</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Blattmann</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Dockhorn</surname><given-names>T</given-names></name>, <name name-style="western"><surname>M&#252;ller</surname><given-names>J</given-names></name>. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint; <year>2023</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.48550/arXiv.2307.01952</pub-id></mixed-citation></ref><ref id="pone.0331566.ref046"><label>46</label><mixed-citation publication-type="other">Labs BF. Flux. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/black-forest-labs/flux" ext-link-type="uri">https://github.com/black-forest-labs/flux</ext-link>. 2023.</mixed-citation></ref><ref id="pone.0331566.ref047"><label>47</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Ramesh</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Dhariwal</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Nichol</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Chu</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>M</given-names></name>. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint; <year>2022</year>. p. <fpage>3</fpage>. <comment>doi: arXiv:220406125</comment></mixed-citation></ref><ref id="pone.0331566.ref048"><label>48</label><mixed-citation publication-type="other">Betker J, Goh G, Jing L, Brooks T, Wang J, Li L. Improving image generation with better captions. OpenAI report; 2023.</mixed-citation></ref><ref id="pone.0331566.ref049"><label>49</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Hou</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>. <article-title>Estimating the distribution of numerosity and non-numerical visual magnitudes in natural scenes using computer vision</article-title>. <source>Psychol Res</source>. <year>2024</year>;<volume>89</volume>(<issue>1</issue>):<fpage>31</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00426-024-02064-2</pub-id><pub-id pub-id-type="pmid">39625570</pub-id></mixed-citation></ref><ref id="pone.0331566.ref050"><label>50</label><mixed-citation publication-type="other">Liu S, Zeng Z, Ren T, Li F, Zhang H, Yang J. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In: European conference on computer vision; 2025. p. 38&#8211;55.</mixed-citation></ref><ref id="pone.0331566.ref051"><label>51</label><mixed-citation publication-type="other">Li J, Li D, Xiong C, Hoi S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International conference on machine learning; 2022. p. 12888&#8211;900.</mixed-citation></ref><ref id="pone.0331566.ref052"><label>52</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Liang</surname><given-names>PP</given-names></name>, <name name-style="western"><surname>Goindani</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Chafekar</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Mathur</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname><given-names>R</given-names></name>. Hemm: Holistic evaluation of multimodal foundation models. arXiv preprint; <year>2024</year>. <comment>doi: arXiv:240703418</comment></mixed-citation></ref><ref id="pone.0331566.ref053"><label>53</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Shepard</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Kilpatric</surname><given-names>DW</given-names></name>, <name name-style="western"><surname>Cunningham</surname><given-names>JP</given-names></name>. <article-title>The internal representation of numbers</article-title>. <source>Cogn Psychol</source>. <year>1975</year>;<volume>7</volume>(<issue>1</issue>):<fpage>82</fpage>&#8211;<lpage>138</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0010-0285(75)90006-7</pub-id></mixed-citation></ref><ref id="pone.0331566.ref054"><label>54</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Dehaene</surname><given-names>S</given-names></name>. <article-title>The neural basis of the Weber-Fechner law: A logarithmic mental number line</article-title>. <source>Trends Cogn Sci</source>. <year>2003</year>;<volume>7</volume>(<issue>4</issue>):<fpage>145</fpage>&#8211;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/s1364-6613(03)00055-x</pub-id><pub-id pub-id-type="pmid">12691758</pub-id></mixed-citation></ref><ref id="pone.0331566.ref055"><label>55</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Gallistel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gelman I</surname><given-names>I</given-names></name>. <article-title>Non-verbal numerical cognition: From reals to integers</article-title>. <source>Trends Cogn Sci</source>. <year>2000</year>;<volume>4</volume>(<issue>2</issue>):<fpage>59</fpage>&#8211;<lpage>65</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/s1364-6613(99)01424-2</pub-id><pub-id pub-id-type="pmid">10652523</pub-id></mixed-citation></ref><ref id="pone.0331566.ref056"><label>56</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Piazza</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Facoetti</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Trussardi</surname><given-names>AN</given-names></name>, <name name-style="western"><surname>Berteletti</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Conte</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lucangeli</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Developmental trajectory of number acuity reveals a severe impairment in developmental dyscalculia</article-title>. <source>Cognition</source>. <year>2010</year>;<volume>116</volume>(<issue>1</issue>):<fpage>33</fpage>&#8211;<lpage>41</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cognition.2010.03.012</pub-id><pub-id pub-id-type="pmid">20381023</pub-id></mixed-citation></ref><ref id="pone.0331566.ref057"><label>57</label><mixed-citation publication-type="other">Udandarao V, Prabhu A, Ghosh A, Sharma Y, Torr P, Bibi A. No &#8220;zero-shot&#8221; without exponential data: Pretraining concept frequency determines multimodal model performance. In: The thirty-eighth annual conference on neural information processing systems; 2024.</mixed-citation></ref><ref id="pone.0331566.ref058"><label>58</label><mixed-citation publication-type="other">Sharma P, Ding N, Goodman S, Soricut R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: In: Proceedings of the 56th annual meeting of the association for computational linguistics (Volume 1: Long papers); 2018. p. 2556&#8211;65.</mixed-citation></ref><ref id="pone.0331566.ref059"><label>59</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Schuhmann</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Vencu</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Beaumont</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Kaczmarczyk</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Mullis</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Katta</surname><given-names>A</given-names></name>. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint; <year>2021</year>. <comment>doi: arXiv:211102114</comment></mixed-citation></ref><ref id="pone.0331566.ref060"><label>60</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zou</surname><given-names>WY</given-names></name>, <name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name>. <article-title>Numerosity discrimination in deep neural networks: Initial competence, developmental refinement and experience statistics</article-title>. <source>Dev Sci</source>. <year>2020</year>;<volume>23</volume>(<issue>5</issue>):e12940. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/desc.12940</pub-id><pub-id pub-id-type="pmid">31977137</pub-id></mixed-citation></ref><ref id="pone.0331566.ref061"><label>61</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Liang</surname><given-names>VW</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kwon</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Yeung</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Zou</surname><given-names>JY</given-names></name>. <article-title>Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning</article-title>. <source>Adv Neural Inform Process Syst</source>. <year>2022</year>;<volume>35</volume>:<fpage>17612</fpage>&#8211;<lpage>25</lpage>.</mixed-citation></ref><ref id="pone.0331566.ref062"><label>62</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Fu</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dolfi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Decarli</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Spironelli</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>. <article-title>Electrophysiological signatures of numerosity encoding in a delayed match-to-sample task</article-title>. <source>Front Hum Neurosci</source>. <year>2022</year>;<volume>15</volume>:<fpage>750582</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnhum.2021.750582</pub-id><pub-id pub-id-type="pmid">35058763</pub-id><pub-id pub-id-type="pmcid">PMC8764258</pub-id></mixed-citation></ref><ref id="pone.0331566.ref063"><label>63</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Burr</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Ross</surname><given-names>J</given-names></name>. <article-title>A visual sense of number</article-title>. <source>Curr Biol</source>. <year>2008</year>;<volume>18</volume>(<issue>6</issue>):<fpage>425</fpage>&#8211;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cub.2008.02.052</pub-id><pub-id pub-id-type="pmid">18342507</pub-id></mixed-citation></ref><ref id="pone.0331566.ref064"><label>64</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Harvey</surname><given-names>BM</given-names></name>, <name name-style="western"><surname>Klein</surname><given-names>BP</given-names></name>, <name name-style="western"><surname>Petridou</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Dumoulin</surname><given-names>SO</given-names></name>. <article-title>Topographic representation of numerosity in the human parietal cortex</article-title>. <source>Science</source>. <year>2013</year>;<volume>341</volume>(<issue>6150</issue>):<fpage>1123</fpage>&#8211;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.1239052</pub-id><pub-id pub-id-type="pmid">24009396</pub-id></mixed-citation></ref><ref id="pone.0331566.ref065"><label>65</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Castaldi</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Piazza</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dehaene</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vignaud</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Eger</surname><given-names>E</given-names></name>. <article-title>Attentional amplification of neural codes for number independent of other quantities along the dorsal visual stream</article-title>. <source>Elife</source>. <year>2019</year>;<volume>8</volume>:e45160. <comment>doi: </comment><pub-id pub-id-type="doi">10.7554/eLife.45160</pub-id><pub-id pub-id-type="pmid">31339490</pub-id><pub-id pub-id-type="pmcid">PMC6693892</pub-id></mixed-citation></ref><ref id="pone.0331566.ref066"><label>66</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Paul</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>van Ackooij</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ten Cate</surname><given-names>TC</given-names></name>, <name name-style="western"><surname>Harvey</surname><given-names>BM</given-names></name>. <article-title>Numerosity tuning in human association cortices and local image contrast representations in early visual cortex</article-title>. <source>Nat Commun</source>. <year>2022</year>;<volume>13</volume>(<issue>1</issue>):<fpage>1340</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-022-29030-z</pub-id><pub-id pub-id-type="pmid">35292648</pub-id><pub-id pub-id-type="pmcid">PMC8924234</pub-id></mixed-citation></ref><ref id="pone.0331566.ref067"><label>67</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Nieder</surname><given-names>A</given-names></name>. <article-title>The neuronal code for number</article-title>. <source>Nat Rev Neurosci</source>. <year>2016</year>;<volume>17</volume>(<issue>6</issue>):<fpage>366</fpage>&#8211;<lpage>82</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nrn.2016.40</pub-id><pub-id pub-id-type="pmid">27150407</pub-id></mixed-citation></ref><ref id="pone.0331566.ref068"><label>68</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Boccato</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zorzi</surname><given-names>M</given-names></name>. <article-title>Learning numerosity representations with transformers: Number generation tasks and out-of-distribution generalization</article-title>. <source>Entropy (Basel)</source>. <year>2021</year>;<volume>23</volume>(<issue>7</issue>):<fpage>857</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e23070857</pub-id><pub-id pub-id-type="pmid">34356398</pub-id><pub-id pub-id-type="pmcid">PMC8303966</pub-id></mixed-citation></ref><ref id="pone.0331566.ref069"><label>69</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Mundy</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Gilmore</surname><given-names>CK</given-names></name>. <article-title>Children&#8217;s mapping between symbolic and nonsymbolic representations of number</article-title>. <source>J Exp Child Psychol</source>. <year>2009</year>;<volume>103</volume>(<issue>4</issue>):<fpage>490</fpage>&#8211;<lpage>502</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jecp.2009.02.003</pub-id><pub-id pub-id-type="pmid">19327782</pub-id></mixed-citation></ref><ref id="pone.0331566.ref070"><label>70</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Drori</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Shuttleworth</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Tang</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ke</surname><given-names>E</given-names></name>, <etal>et al</etal>. <article-title>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2022</year>;<volume>119</volume>(<issue>32</issue>):e2123433119. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.2123433119</pub-id><pub-id pub-id-type="pmid">35917350</pub-id><pub-id pub-id-type="pmcid">PMC9371704</pub-id></mixed-citation></ref><ref id="pone.0331566.ref071"><label>71</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Palmer</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>NA</given-names></name>, <name name-style="western"><surname>Spirling</surname><given-names>A</given-names></name>. <article-title>Using proprietary language models in academic research requires explicit justification</article-title>. <source>Nat Comput Sci</source>. <year>2024</year>;<volume>4</volume>(<issue>1</issue>):<fpage>2</fpage>&#8211;<lpage>3</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s43588-023-00585-1</pub-id><pub-id pub-id-type="pmid">38177494</pub-id></mixed-citation></ref><ref id="pone.0331566.ref072"><label>72</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Testolin</surname><given-names>A</given-names></name>. <article-title>The challenge of modeling the acquisition of mathematical concepts</article-title>. <source>Front Hum Neurosci</source>. <year>2020</year>;<volume>14</volume>:<fpage>100</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnhum.2020.00100</pub-id><pub-id pub-id-type="pmid">32265678</pub-id><pub-id pub-id-type="pmcid">PMC7099599</pub-id></mixed-citation></ref><ref id="pone.0331566.ref073"><label>73</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Kajic</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Wiles</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Albuquerque</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Bauer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Pont-Tuset</surname><given-names>J</given-names></name>. Evaluating numerical reasoning in text-to-image models. arXiv preprint; <year>2024</year>. <comment>doi: arXiv:240614774</comment></mixed-citation></ref><ref id="pone.0331566.ref074"><label>74</label><mixed-citation publication-type="other">Paiss R, Ephrat A, Tov O, Zada S, Mosseri I, Irani M. Teaching clip to count to ten. In: Proceedings of the IEEE/CVF international conference on computer vision; 2023. p. 3170&#8211;80.</mixed-citation></ref><ref id="pone.0331566.ref075"><label>75</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Jeong</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Choi</surname><given-names>Y</given-names></name>. <article-title>NuCap: A numerically aware captioning framework for improved numerical reasoning</article-title>. <source>Appl Sci</source>. <year>2025</year>;<volume>15</volume>(<issue>10</issue>):<fpage>5608</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/app15105608</pub-id></mixed-citation></ref><ref id="pone.0331566.ref076"><label>76</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Kang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Galim</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Koo</surname><given-names>HI</given-names></name>. Counting guidance for high fidelity text-to-image synthesis. arXiv preprint arXiv:230617567; <year>2023</year>. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2306.17567" ext-link-type="uri">https://arxiv.org/abs/2306.17567</ext-link></mixed-citation></ref><ref id="pone.0331566.ref077"><label>77</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Nieder</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name>. <article-title>Analog numerical representations in rhesus monkeys: Evidence for parallel processing</article-title>. <source>J Cogn Neurosci</source>. <year>2004</year>;<volume>16</volume>(<issue>5</issue>):<fpage>889</fpage>&#8211;<lpage>901</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/089892904970807</pub-id><pub-id pub-id-type="pmid">15200715</pub-id></mixed-citation></ref><ref id="pone.0331566.ref078"><label>78</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Wei</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Schuurmans</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Bosma</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Xia</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Chi</surname><given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Chain-of-thought prompting elicits reasoning in large language models</article-title>. <source>Adv Neural Inform Process Syst</source>. <year>2022</year>;<volume>35</volume>:<fpage>24824</fpage>&#8211;<lpage>37</lpage>.</mixed-citation></ref><ref id="pone.0331566.ref079"><label>79</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Zhan</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Multimodal image synthesis and editing: The generative AI era</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2023</year>;<volume>45</volume>(<issue>12</issue>):<fpage>15098</fpage>&#8211;<lpage>119</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3305243</pub-id><pub-id pub-id-type="pmid">37624713</pub-id></mixed-citation></ref><ref id="pone.0331566.ref080"><label>80</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Mirzadeh</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Alizadeh</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Shahrokhi</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tuzel</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Farajtabar</surname><given-names>M</given-names></name>. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint; <year>2024</year>. <comment>doi: arXiv:241005229</comment></mixed-citation></ref><ref id="pone.0331566.ref081"><label>81</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Rudman</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Golovanevsky</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Palit</surname><given-names>V</given-names></name>, <name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Eickhoff</surname><given-names>C</given-names></name>. Forgotten polygons: Multimodal large language models are shape-blind. arXiv preprint; <year>2025</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.48550/arXiv.250215969</pub-id></mixed-citation></ref><ref id="pone.0331566.ref082"><label>82</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Romera-Paredes</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Barekatain</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Novikov</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Balog</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kumar</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Dupont</surname><given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Mathematical discoveries from program search with large language models</article-title>. <source>Nature</source>. <year>2024</year>;<volume>625</volume>(<issue>7995</issue>):<fpage>468</fpage>&#8211;<lpage>75</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-023-06924-6</pub-id><pub-id pub-id-type="pmid">38096900</pub-id><pub-id pub-id-type="pmcid">PMC10794145</pub-id></mixed-citation></ref><ref id="pone.0331566.ref083"><label>83</label><mixed-citation publication-type="journal"><name name-style="western"><surname>Wang</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Fu</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Du</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Gao</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>Scientific discovery in the age of artificial intelligence</article-title>. <source>Nature</source>. <year>2023</year>;<volume>620</volume>(<issue>7972</issue>):<fpage>47</fpage>&#8211;<lpage>60</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-023-06221-2</pub-id><pub-id pub-id-type="pmid">37532811</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>