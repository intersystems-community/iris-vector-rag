<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431529</article-id><article-id pub-id-type="pmcid-ver">PMC12431529.1</article-id><article-id pub-id-type="pmcaid">12431529</article-id><article-id pub-id-type="pmcaiid">12431529</article-id><article-id pub-id-type="doi">10.3390/s25175273</article-id><article-id pub-id-type="publisher-id">sensors-25-05273</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An Empirical Study on the Impact of Different Interaction Methods on User Emotional Experience in Cultural Digital Design</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5884-0473</contrib-id><name name-style="western"><surname>Zhao</surname><given-names initials="J">Jing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05273" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-3217-4360</contrib-id><name name-style="western"><surname>Ma</surname><given-names initials="Y">Yiming</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05273" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="X">Xinran</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05273" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Lin</surname><given-names initials="H">Hui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05273" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Lu</surname><given-names initials="Y">Yi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05273" ref-type="aff">1</xref><xref rid="c1-sensors-25-05273" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="R">Ruiyan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af2-sensors-25-05273" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="Z">Ziying</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05273" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zou</surname><given-names initials="F">Feng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05273" ref-type="aff">1</xref><xref rid="c1-sensors-25-05273" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Sato</surname><given-names initials="W">Wataru</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05273"><label>1</label>College of Art and Design, Beijing University of Technology, Beijing 100124, China; <email>zhaoj@bjut.edu.cn</email> (J.Z.); <email>mym1202@emails.bjut.edu.cn</email> (Y.M.); <email>zhangxinran02@emails.bjut.edu.cn</email> (X.Z.); <email>linhhui6828@emails.bjut.edu.cn</email> (H.L.); <email>ziyingzhang@emails.bjut.edu.cn</email> (Z.Z.)</aff><aff id="af2-sensors-25-05273"><label>2</label>College of Applied Arts and Science, Beijing Union University, Beijing 100101, China; <email>20232035110104@buu.edu.cn</email></aff><author-notes><corresp id="c1-sensors-25-05273"><label>*</label>Correspondence: <email>luyi@bjut.edu.cn</email> (Y.L.); <email>zoufeng@bjut.edu.cn</email> (F.Z.)</corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5273</elocation-id><history><date date-type="received"><day>26</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>15</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>25</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05273.pdf"/><abstract><p>Traditional culture plays a vital role in shaping national identity and emotional belonging, making it imperative to explore innovative strategies for its digital preservation and engagement. This study investigates how interaction design in cultural digital games influences users&#8217; emotional experiences and cultural understanding. Centering on the Chinese intangible cultural heritage puppet manipulation, we developed an interactive cultural game with three modes: gesture-based interaction via Leap Motion, keyboard control, and passive video viewing. A multimodal evaluation framework was employed, integrating subjective questionnaires with physiological indicators, including Functional Near-Infrared Spectroscopy (fNIRS), infrared thermography (IRT), and electrodermal activity (EDA), to assess users&#8217; emotional responses, immersion, and perception of cultural content. Results demonstrated that gesture-based interaction, which aligns closely with the embodied cultural behavior of puppet manipulation, significantly enhanced users&#8217; emotional engagement and cultural comprehension compared to the other two modes. Moreover, fNIRS data revealed broader activation in brain regions associated with emotion regulation and cognitive control during gesture interaction. These findings underscore the importance of culturally congruent interaction design in enhancing user experience and emotional resonance in digital cultural applications. This study provides empirical evidence supporting the integration of cultural context into interaction strategies, offering valuable insights for the development of emotionally immersive systems for intangible cultural heritage preservation.</p></abstract><kwd-group><kwd>cultural heritage</kwd><kwd>digital interaction design</kwd><kwd>user emotional experience</kwd><kwd>functional near-infrared spectroscopy</kwd><kwd>infrared thermography</kwd><kwd>electrodermal activity</kwd></kwd-group><funding-group><award-group><funding-source>Beijing Ideological and Political Work Research Center for Universities</funding-source><award-id>BJSZ2024YB47</award-id></award-group><funding-statement>This research was funded by the Beijing Ideological and Political Work Research Center for Universities, grant number BJSZ2024YB47.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05273"><title>1. Introduction</title><p>Traditional culture serves as a crucial vehicle for national identity, historical memory, and spiritual values, playing an irreplaceable role in cultural continuity. However, due to a lack of immersive experiences and weakened dissemination, traditional culture today often suffers from insufficient public cultural identity, leading to a disconnection in cultural inheritance. In particular, younger generations such as university students, despite being digitally native, often exhibit a disconnect from traditional cultural expressions. Thus, revitalizing traditional culture in a way that resonates with young people has become a pressing issue for contemporary cultural design.</p><p>In recent years, the development of digital interaction technologies has enabled traditional culture&#8212;especially intangible cultural heritage (ICH)&#8212;to be embedded into interactive systems in engaging and participatory ways, thereby integrating into everyday life [<xref rid="B1-sensors-25-05273" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05273" ref-type="bibr">2</xref>]. These practices collectively demonstrate that digital interaction has become an important avenue for cultural innovation. Beyond enabling performance, interaction, perception, experience, response, and reflection, such technologies profoundly influence users&#8217; understanding, internalization, and emotional identification with cultural information.</p><p>Currently, digital interaction design for cultural heritage is transitioning from &#8220;content presentation&#8221; to &#8220;experience construction&#8221;. The prevailing approaches mainly involve the digital preservation and presentation of cultural artifacts, such as cultural databases and digital archives, interactive exhibitions, multimedia guides, and immersive experiences, using virtual reality (VR), augmented reality (AR), and other interactive technologies [<xref rid="B3-sensors-25-05273" ref-type="bibr">3</xref>]. Designs aimed at cultural preservation focus on the visual representation and storage of cultural data, while those targeting cultural education emphasize user participation by integrating cultural elements into interactive scenarios or simulating cultural behaviors to enhance cultural perception and identification [<xref rid="B4-sensors-25-05273" ref-type="bibr">4</xref>].</p><p>However, the current digital interaction design of cultural heritage still faces limitations in interaction modalities. Most culture-based digital systems rely on generic interaction paradigms. They do not adequately consider cultural&#8211;contextual fit, which refers to the organic integration between interaction technology and cultural characteristics. As a result, the digital expression of culture often remains at the level of symbolic representation or functional behavior replication. Interaction modalities not only affect usability but also deeply shape users&#8217; cultural cognition and emotional experiences [<xref rid="B5-sensors-25-05273" ref-type="bibr">5</xref>]. Emotional experience involves the users&#8217; emotional engagement, sense of immersion triggered by cultural stimuli, and physiological arousal [<xref rid="B6-sensors-25-05273" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05273" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05273" ref-type="bibr">8</xref>]. The cultural appropriateness, embodied fidelity, and immersive potential of an interaction design directly influence the acceptance and internalization of cultural meaning. From the perspective of embodied cognition, interaction modalities that align with culturally familiar gestures or bodily movement not only support intuitive usability but also activate perceptual&#8211;motor systems that ground cultural symbols in bodily experience, thereby enhancing emotional experience [<xref rid="B9-sensors-25-05273" ref-type="bibr">9</xref>]. In addition, the field of affective computing emphasizes that sensor-based platforms (e.g., facial EMG and gesture kinematics) can provide real-time indicators of emotional engagement in cultural interactions [<xref rid="B10-sensors-25-05273" ref-type="bibr">10</xref>]. Thus, by bridging physical interaction, bodily simulation, and emotional arousal, culturally congruent interaction can more deeply shape cultural cognition and emotional experience.</p><p>Traditional user experience evaluation methods have mostly relied on subjective approaches like questionnaires and interviews. There has been limited use of physiological data for multimodal assessment. This makes it difficult to capture users&#8217; genuine emotional fluctuations and cognitive load [<xref rid="B11-sensors-25-05273" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05273" ref-type="bibr">12</xref>]. Recent studies in player experience and game user research have begun to adopt physiological measurements, such as fNIRS, eye tracking, and EDA, in conjunction with subjective assessments to create more comprehensive evaluation systems [<xref rid="B13-sensors-25-05273" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05273" ref-type="bibr">14</xref>]. In particular, research in immersive environments has shown that combining fNIRS with IRT and EDA can effectively track users&#8217; real-time stress levels, engagement, and cognitive load [<xref rid="B15-sensors-25-05273" ref-type="bibr">15</xref>]. These approaches provide methodological inspiration for building multimodal and developmentally sensitive evaluation systems in interactive cultural experiences. Nevertheless, there remains a lack of systematic research employing multimodal methods that focus specifically on cultural interaction experiences. Therefore, how to design more immersive interactive methods in the digitalization of culture and evaluate user experience with more scientific and comprehensive approaches requires in-depth exploration.</p><p>In light of these challenges, our research addresses the following core question: In cultural theme interaction designs, what types of interaction modalities are most effective in eliciting emotional responses and enhancing cultural experiences? We select the traditional puppet art of marionette puppetry, a form of intangible cultural heritage, as the thematic foundation for developing a cultural experience game. We then evaluate university students&#8217; emotional experiences across three interaction modes: Leap Motion-based gesture interaction, keyboard interaction, and non-interactive viewing.</p><p>Using a within-subjects experimental design, we integrated multiple measurement methods, including fNIRS, infrared thermography, EDA, and subjective questionnaires, to analyze users&#8217; emotional responses, immersion levels, and cultural perceptions under different interaction conditions. fNIRS was selected as a core tool for assessing emotional and cognitive responses due to its lower sensitivity to motion artifacts and its suitability for detecting brain function in naturalistic settings&#8212;advantages over positron emission tomography (PET), electroencephalography (EEG), and functional magnetic resonance imaging (fMRI) [<xref rid="B16-sensors-25-05273" ref-type="bibr">16</xref>]. Through this research, we aim to construct a causal link between the cultural congruence of interaction modalities and users&#8217; emotional experiences, thereby providing empirical evidence and design guidance for future cultural interaction design and cultural communication practices.</p><p>In the remainder of this paper, <xref rid="sec2-sensors-25-05273" ref-type="sec">Section 2</xref> reviews two relevant research domains: (1) interaction design that accounts for cultural characteristics and (2) evaluation methods using multimodal physiological data. <xref rid="sec3-sensors-25-05273" ref-type="sec">Section 3</xref> outlines our experimental setup, including materials, participants, measurement techniques, and procedures. <xref rid="sec4-sensors-25-05273" ref-type="sec">Section 4</xref> presents the results, followed by a discussion in <xref rid="sec5-sensors-25-05273" ref-type="sec">Section 5</xref> and conclusions in <xref rid="sec6-sensors-25-05273" ref-type="sec">Section 6</xref>.</p></sec><sec id="sec2-sensors-25-05273"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05273"><title>2.1. Research on Digital Interaction Design for Cultural Heritage with Cultural Characteristics as a Key Consideration</title><p>In recent years, digital interaction design related to cultural heritage has gradually become an important issue in the fields of intelligent interaction and cultural heritage revitalization. However, while existing research generally acknowledges the role of cultural elements in user experience, there are still limitations in the design of interaction modes.</p><p>On one hand, most studies primarily integrate cultural elements into visual styles, symbolic representations, and contextual construction, neglecting the cultural expression potential inherent in the interaction modes themselves. On the other hand, emerging technologies such as VR and artificial intelligence (AI) are increasingly applied in the digital expression of cultural heritage. However, most of these studies primarily emphasize the immersive experience and technical innovation of digital presentations. In contrast, relatively few explore how users perceive cultural meaning or how emotional resonance is triggered during interaction.</p><p>A large number of existing studies mainly focus on the reproduction of cultural content in practice, particularly in terms of visual and contextual design. For example, Muntean et al. [<xref rid="B17-sensors-25-05273" ref-type="bibr">17</xref>] embed the cultural values of the Masqun ethnic group into an interactive physical table as part of an exhibition setup, attempting to guide users in perceiving cultural narratives, but the interaction logic still relies on traditional click-and-browse operations. Sun et al. [<xref rid="B18-sensors-25-05273" ref-type="bibr">18</xref>], in their development of a VR system based on Dunhuang culture, stimulate user interest through storytelling and multimodal presentations. However, the core of the system is still a technology-driven experiential flow, rather than an interaction logic driven by culture. Li et al. [<xref rid="B19-sensors-25-05273" ref-type="bibr">19</xref>] explore how visual images and human&#8211;computer interaction can be combined in cultural creative product design. They propose several ideas related to cultural visual interaction. However, their approach remains centered on functional operations and lacks a clear mapping to cultural behavioral logic.</p><p>Further, some studies try to reflect cultural features in interaction design. However, they often focus on sensory stimulation and ease of use, without integrating cultural thinking patterns or embodied behaviors linked to heritage practices. For example, Long et al. [<xref rid="B20-sensors-25-05273" ref-type="bibr">20</xref>] introduce traditional Chinese cultural gestures on mobile devices, using tangible gestures to evoke users&#8217; sense of identity and cultural emotions. However, they also point out that there is still a lack of systematic construction and mapping mechanisms for cultural cognitive models, making it difficult to form stable cultural behavioral inertia. In culturally related digital games, the most commonly used interaction modes are key presses or clicks, but for the types of cultural heritage that rely on fine motor skills or physical participation, these modes cannot reproduce the rhythm of movement and bodily gestures, leaving the user experience in a passive reading or viewing stage. In other words, the existing interaction modes are largely &#8220;representational inputs&#8221; of cultural symbols, rather than &#8220;behavioral embedding,&#8221; failing to reach the deeper psychological channels of cultural identity.</p><p>Moreover, with the development of mixed reality (MR), motion-sensing technologies, and other innovations, an increasing number of projects are attempting to build immersive cultural heritage experience systems. However, these often fall into a &#8220;technology-centered&#8221; tendency in interaction design. In such studies, interaction modes typically serve technological display rather than cultural expression. For example, the mixed reality puppet performance system developed by Lin et al. [<xref rid="B21-sensors-25-05273" ref-type="bibr">21</xref>] and the interactive museum design at the Florence Cathedral by Rinaldi et al. [<xref rid="B22-sensors-25-05273" ref-type="bibr">22</xref>] both demonstrate the richness of cultural experiences, but the system interaction designs often lack a deeper construction of the cultural experience mechanisms. This emphasis on display over construction in the interaction path can turn users into &#8220;cultural spectators&#8221; rather than &#8220;cultural participants,&#8221; making it difficult to establish sustained cultural emotional connections.</p><p>In conclusion, while current research has begun to recognize the importance of cultural factors in interactive experiences, most studies still focus on superficial cultural content transmission and esthetic interface creation. Research that truly uses cultural characteristics as a guiding force to explore the mapping and coupling between interaction modes, cultural behaviors, and cultural traits remains scarce. Interaction modes are not only media for information transmission but also key channels for users&#8217; cultural perception, identity construction, and emotional resonance. Related research in the field of cross-cultural human&#8211;computer interaction emphasizes that interface gestures, timing, and affective cues must resonate with users&#8217; cultural schemas to elicit meaningful emotional and identity-oriented responses [<xref rid="B23-sensors-25-05273" ref-type="bibr">23</xref>]. Together, relevant research on embodied cognition [<xref rid="B24-sensors-25-05273" ref-type="bibr">24</xref>] and affective computing shows that by understanding visitors&#8217; personal cognitive needs and interests and their situational affective states [<xref rid="B25-sensors-25-05273" ref-type="bibr">25</xref>], as well as choosing interactive methods for embodied cultural heritage experiences, it is possible to enhance visitors&#8217; emotional experience and cultural identity and bridge the emotional gap [<xref rid="B26-sensors-25-05273" ref-type="bibr">26</xref>]. These studies show that interaction modes do not merely execute tasks but can elicit culturally patterned cognitive&#8211;emotional responses through bodily simulation, mirror-resonance, and sensorial grounded meaning. Accordingly, how to reflect the logical structure of cultural behaviors, value systems, and emotional expressions in interactive mechanisms remains an important area in the digital interaction design of cultural heritage that requires deeper exploration.</p></sec><sec id="sec2dot2-sensors-25-05273"><title>2.2. Research on User Emotional Experience Evaluation Driven by Multimodal Physiological and Psychological Data</title><p>As the role of emotional experience in human&#8211;computer interaction (HCI), immersive systems, and digital product design becomes increasingly prominent, more and more studies are attempting to explore how to assess users&#8217; emotional states in a more comprehensive and objective manner. Especially in complex interactive environments such as immersive experiences, VR, and gaming, single subjective assessments often fail to fully reflect users&#8217; true emotional states. The introduction of physiological signals has become a key approach to enhancing the accuracy and objectivity of assessments. However, despite the practical advancements in physiological measurements, most current research still limits its use to supplementary methods. The true integration of subjective psychological indicators with objective physiological data, and the construction of a systematic, multimodal emotional assessment framework, remains a core challenge in current research.</p><p>In existing evaluation methods, subjective tools still dominate. A range of tools&#8212;including traditional emotional self-report questionnaires (e.g., REQ) [<xref rid="B27-sensors-25-05273" ref-type="bibr">27</xref>], task-specific experience measures, and design-oriented methods like Mood Boards [<xref rid="B28-sensors-25-05273" ref-type="bibr">28</xref>]&#8212;highlight the user perspective and regard subjective feelings as a primary basis for understanding emotional experience. These methods are low-cost, easy to implement, and especially suitable for early prototype testing and effective design validation. However, these approaches are limited by factors such as users&#8217; self-expression abilities and subjective awareness, making it challenging to capture nuanced and dynamic emotional changes. Furthermore, users&#8217; subjective expressions are often influenced by cognitive biases, language abilities, and social expectations, making it difficult to provide continuous and objective data support.</p><p>In response to these methodological limitations, researchers have begun to introduce physiological signals as an important supplement to emotional assessment, such as heart rate, EDA, facial expressions, and EEG, to enhance the objectivity of evaluations. The core value of these methods lies in their ability to capture immediate, subconscious emotional responses. For example, Liapis et al. [<xref rid="B29-sensors-25-05273" ref-type="bibr">29</xref>] developed the PhysiOBS system, which combines physiological sensors with behavioral observation and self-reporting to achieve multidimensional assessments of users&#8217; emotional states, demonstrating the potential of multimodal collaborative analysis. Similarly, Barrow [<xref rid="B27-sensors-25-05273" ref-type="bibr">27</xref>] attempted to combine physiological arousal indicators with self-assessment in predicting emotional responses. The emergence of such tools indicates a shift in user experience evaluation from static measurement to dynamic perception. However, current physiological measurement methods still mainly focus on single-modal data collection, with limited integration of multimodal physiological data analysis.</p><p>Beyond traditional affective computing tools, sensor-based approaches have further extended emotional evaluation frameworks in interactive and developmentally sensitive environments. For example, real-time changes in skin conductance have been used to assess emotional arousal, decision-making pressure, and cognitive effort during gameplay and interactive learning [<xref rid="B21-sensors-25-05273" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05273" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05273" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05273" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05273" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05273" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05273" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05273" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05273" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05273" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05273" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05273" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05273" ref-type="bibr">33</xref>]. Facial thermography and muscle activity (EMG) also offer insights into subtle emotional shifts under stress or surprise [<xref rid="B34-sensors-25-05273" ref-type="bibr">34</xref>]. These physiological markers, when combined with subjective reporting, provide a more nuanced depiction of emotional intensity, engagement density, and behavioral readiness, especially in culturally rich or sensorimotor-driven contexts. The advancement of these sensor-based measurement techniques supports a more reliable and multidimensional understanding of user states in culturally immersive experiences.</p><p>In HCI and HMI research, as interaction methods become more diverse and complex, the demand for multimodal data fusion is also growing. However, existing practices related to physiological signals, such as heart rate and electrodermal activity, are mainly used to reflect immediate emotions, such as pleasure, not to effectively cover multidimensional psychological states, such as cognitive load and cultural contexts. For instance, Abriat A et al. [<xref rid="B35-sensors-25-05273" ref-type="bibr">35</xref>] integrate behavioral tools (such as pleasure scales) with physiological signals (EMG, RR) to assess the skincare product usage experience in menopausal women, but they still focus on a single emotional dimension, such as &#8220;pleasure.&#8221; Liu et al. [<xref rid="B36-sensors-25-05273" ref-type="bibr">36</xref>] use eye-tracking and emotional questionnaires to assess the public&#8217;s esthetic responses to streetlamp designs. Although they introduce multimodal tools, the analysis metrics remained limited to emotional responses at the surface level, without addressing the deeper cultural cognition and resonance mechanisms of users.</p><p>In summary, the integration of subjective and objective data in multimodal emotional experience assessments has become an important direction in interactive experience research. To further enhance the cultural adaptability and psychological recognition dimensions of these assessments, this study aims to systematically assess whether culturally adapted interactions can significantly enhance users&#8217; emotional experience quality and cultural identity, based on subjective evaluation and multimodal physiological data (such as fNIRS HbO concentration, facial temperature, EDA, etc.).</p></sec></sec><sec id="sec3-sensors-25-05273"><title>3. Materials and Methods</title><sec sec-type="subjects" id="sec3dot1-sensors-25-05273"><title>3.1. Participants</title><p>To investigate users&#8217; emotional experiences in a cultural game with different interaction modalities, the participants are recruited via the Wenjuanxing v2.2.6 program on WeChat v1.0.9 between April and August 2025. This study was approved by the Beijing University of Technology Ethics Committee. Written informed consent was obtained from all participants prior to data collection. Participants who successfully completed the experiment received a monetary reward of 20 RMB. A total of 66 volunteers participated in the study. Due to unexpected instrumental interruptions and accidental loss of certain channels in fNIRS recordings, complete and analyzable datasets were ultimately obtained from only 61 participants. We also administered a pre-experiment questionnaire to collect participants&#8217; background information. In addition to gender, age, and academic major, the questionnaire includes items assessing participants&#8217; familiarity with Chinese traditional culture and digital gaming experience to capture their cultural cognition and operational competence. Questionnaires are scored using a Likert scale (1 = not at all, 5 = very).</p><p>We conducted basic descriptive statistical analysis of the questionnaire results, see <xref rid="sensors-25-05273-t001" ref-type="table">Table 1</xref>. The results show that all participants are full-time college students, ranging in age from 18 to 28 years old, with undergraduate, master&#8217;s, and doctoral degrees. Their majors include electronic engineering, computer science, industrial design, mechanical engineering, etc. This diversity is intended to ensure a wide range of perspectives and evaluations during the cultural game experience. We also calculated the mean (M) and standard deviation (SD) of key variables. Participants report an average familiarity with Chinese traditional culture of M = 2.18, SD = 0.74, indicating a moderate cultural background. Their average weekly gameplay time is M = 11.37 h, SD = 10.46 h. About 75% report prior experience with PC or console games, but only 18.03% have used Leap Motion or similar gesture-based devices. These results show that participants generally possess sufficient gaming skills to complete the tasks. Meanwhile, cultural familiarity reduces the risk of misunderstanding the content of game. Limited exposure to gesture interaction also minimizes familiarity bias in the Leap Motion condition. Therefore, participants&#8217; background variables exert minimal influence on the internal validity of this study. As most participants have a comparable baseline in cultural understanding and operational ability, observed emotional and physiological differences are likely attributable to the interaction modality than to individual variation.</p><p>The experiment follows a single-blind design, in which participants are unaware of the research hypotheses and expected outcomes, thereby minimizing subjective bias. This participant setup provides a stable and representative data foundation for the subsequent multimodal data collection and comparison across interaction modalities.</p></sec><sec id="sec3dot2-sensors-25-05273"><title>3.2. Materials</title><p>In this study, the Quanzhou String Puppet is selected as the cultural carrier based on the digital interactive design of ICH. As an ancient form of traditional folk art, Quanzhou string puppetry is characterized by profound cultural attributes and distinctive artistic features [<xref rid="B37-sensors-25-05273" ref-type="bibr">37</xref>]. The structure of a typical string puppet consists of the puppet head, torso, limbs, control rods, and strings. Each puppet is equipped with more than a dozen to several dozen strings, meticulously connected to various joint parts, forming a complex and precise string control system. This system enables flexible joint movement and allows for highly refined motion control through string manipulation [<xref rid="B38-sensors-25-05273" ref-type="bibr">38</xref>]. The operational uniqueness of Quanzhou string puppetry lies in the intricate string system and the highly skilled string-handling techniques. During performance, puppeteers must precisely modulate the tension and rhythm of each string using delicate coordination of fingers and wrists, thereby enabling the puppet to exhibit a lifelike gait and expressive movements [<xref rid="B39-sensors-25-05273" ref-type="bibr">39</xref>].</p><p>Unlike most static or visually dominant forms of ICH, string puppetry emphasizes dynamic manipulation and embodied performance, relying on fine motor skills and long-term embodied memory. The mapping between the puppeteer&#8217;s hand movements and the puppet&#8217;s actions is fundamental to its cultural expression mechanism. Given the unique operational logic embedded in this traditional heritage, its digital reinterpretation necessitates an interaction modality that maintains both cultural fidelity and operational authenticity. The goal is to construct a digital interaction system capable of fostering physical engagement and re-enacting traditional manipulation techniques, thereby unlocking the embodied cultural experience embedded in this art form.</p><p>Based on these cultural performance characteristics, a Leap Motion gesture recognition device is introduced into the experiment as a culturally consistent gesture interaction paradigm. This setup enables users to control the digital puppet through natural hand and wrist movements, simulating traditional string manipulation. Through micro-operations of fingers such as opening and closing, rotating, pushing, and pulling, key movements such as puppet walking, jumping, and waving can be achieved. To allow for comparative analysis, two additional interaction conditions are implemented: keyboard-based interaction and non-interactive viewing. This comparative framework enables us to explore the relationship between interaction modality and user experience.</p><p>All interaction prototypes are developed using the Unity3D engine (version 2023.3.0f1c1). The tasks and visual content are kept consistent across all interaction conditions to minimize confounding effects from content variation. The core mechanic requires users to control puppet actions to perform designated cultural movements, preserving the symbolic cultural logic of traditional puppetry while enabling controlled manipulation of interaction variables.</p><p>As illustrated in <xref rid="sensors-25-05273-f001" ref-type="fig">Figure 1</xref>, the three interaction modalities vary in terms of their logical and cultural congruence:<list list-type="bullet"><list-item><p>Gesture Interaction: Gesture interaction allows players to engage with the game using intuitive, natural gestures that simulate real-world puppet manipulation. It offers high immersion and strong cultural alignment.</p></list-item><list-item><p>Keyboard Interaction: Keyboard interaction employs traditional key inputs to control the puppet&#8217;s arm movements. While moderately immersive, it lacks the physicality and cultural resonance of traditional manipulation.</p></list-item><list-item><p>No interaction: The non-interaction mode is to let participants simply watch a prerecorded gameplay video without any user input, representing a passive mode of cultural reception with low engagement and limited cultural transmission.</p></list-item></list></p><p>To enable smooth and natural gesture interaction, the Leap Motion controller is used for hand-tracking. This device employs high-precision infrared sensors to capture users&#8217; hand movements in real time without noticeable latency. Specifically, the height of the user&#8217;s index and ring fingers is mapped to the puppet&#8217;s right and left arm movements, as shown in the red circle in <xref rid="sensors-25-05273-f001" ref-type="fig">Figure 1</xref>(a). The position of hand determines the puppet&#8217;s spatial coordinates within the game environment. Compared to conventional motion tracking solutions, Leap Motion offers advantages such as portability, high responsiveness, and intuitive operation, making it particularly suitable for interactive scenarios that require detailed hand motion, like string puppetry. In contrast, the keyboard-based control uses the Shift key for left arm movement, the Space bar for right arm movement, and the &#8220;A&#8221; and &#8220;D&#8221; keys for directional navigation. The non-interactive condition involves a timed playback of a prerecorded gameplay session.</p><p>To ensure that the gesture design and narrative expression of the digital puppet system avoid cultural misrepresentation, a validation process is conducted through expert review and literature-based justification.</p><p>Semi-structured interviews are conducted in our research for the reason that they are flexible and also applicable. An outline of the interview could be drawn up in advance, but it is not necessary to follow it completely, and it can be adjusted flexibly according to the interview subjects, the process, and content [<xref rid="B40-sensors-25-05273" ref-type="bibr">40</xref>]. Therefore, this study adopts semi-structured interviews with highly representative stakeholders to obtain the most direct and honest perception of the cultural experts on the current status and shortcomings of the digital design of Puppetry. Three categories of stakeholders have been selected: The Puppetry expert performer, the Puppetry show organizer and manager, and also researcher focused on cultural heritage. The details of the respondents are shown in <xref rid="sensors-25-05273-t002" ref-type="table">Table 2</xref>.</p><p>The interviews are conducted through the Tencent Meeting online platform. Each one takes around 30 min. Firstly, we inquire about the background information. Then, after showing the digital game developed by our team, as well as the one used as the study material in our experiment, questions about the Puppetry gestures, overall narrative, and digital form are put forward to gain their insights on whether the gesture&#8211;action mappings and the narrative structure of the digital puppetry system align with the traditional string puppet. The specific questions are shown in <xref rid="sensors-25-05273-t003" ref-type="table">Table 3</xref>.</p><p>All three experts gave positive feedback on these questions. They confirm that the design retained the expressive qualities of traditional string puppetry and represented a culturally recognizable form of performance such as the gestures and ways of operating. Although the game is presented in a relatively simple way, it offers novel insights and a creative route for developing traditional Puppetry, especially regarding the needs of the audience of the young generation. In actual performances, the number of strings is usually large and the manipulation methods are also more complex. However, this digital game is sufficient for testing the form and is a valuable way to promote the communication of traditional Puppetry. Their attitude reflects a high degree of cultural consistency and endorsement of our digital approach. They also expressed approval of the narrative of the game. They are embracing and welcoming new repertoire stories that are rooted in our society. All three experts highly praise the new digital approach to redesigning Puppetry culture. They also provide further suggestions, such as diversifying the types of puppet shows by incorporating Glove Puppet shows and Rod Puppet shows, as string puppetry is typically popular in the southern regions of China, while rod puppetry is more commonly disseminated in the northern part. Designing distinct digital interaction methods based on different types of puppets will enhance cultural dissemination effectiveness and broaden audience reach.</p><p>Additionally, the design approach is supported by prior research on digital puppetry systems. Previous studies confirm that well-designed interactive and gesture-based systems are capable of preserving essential cultural elements [<xref rid="B41-sensors-25-05273" ref-type="bibr">41</xref>]. For instance, Zhang demonstrates that hand gestures such as opening and clenching can replicate traditional manipulation techniques like twisting and rubbing, thereby maintaining performative authenticity [<xref rid="B42-sensors-25-05273" ref-type="bibr">42</xref>]. Antonijoan et al. show that tangible puppets controlling virtual avatars help expand narratives without distorting cultural meaning [<xref rid="B43-sensors-25-05273" ref-type="bibr">43</xref>]. Wang further concludes that digital puppetry systems support embodied learning and contribute to the transmission of traditional skills and knowledge [<xref rid="B44-sensors-25-05273" ref-type="bibr">44</xref>]. These findings reinforce the cultural validity of the proposed system design.</p><p>To ensure that the gesture-based interaction does not introduce discomfort or excessive task demands that could confound the interpretation of physiological responses (e.g., mistaking discomfort for emotional arousal), a usability evaluation is conducted. The evaluation framework is informed by gestural interaction usability heuristics [<xref rid="B45-sensors-25-05273" ref-type="bibr">45</xref>] and adapts quantitative measures from established gesture interface evaluation studies [<xref rid="B46-sensors-25-05273" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05273" ref-type="bibr">47</xref>]. Three critical dimensions are examined: learnability, cognitive load, and fatigue. See <xref rid="sensors-25-05273-t004" ref-type="table">Table 4</xref> for the usability evaluation scale. A total of fourteen 5-point Likert scale items are used, with scores ranging from 1 (&#8220;strongly disagree&#8221;) to 5 (&#8220;strongly agree&#8221;). Positive items are reverse-coded to ensure that lower scores consistently indicate better usability. The median reference value is set to 3 for subsequent non-parametric testing.</p><p>Internal consistency of the scales is satisfactory to excellent, with Cronbach&#8217;s &#945; of 0.746 for learnability, 0.750 for cognitive load, and 0.814 for fatigue (overall &#945; = 0.832), as shown in <xref rid="sensors-25-05273-t004" ref-type="table">Table 4</xref>. Normality tests (Shapiro&#8211;Wilk) indicate that all item distributions significantly deviate from normality (<italic toggle="yes">p</italic> &#8804; 0.05); thus, Wilcoxon signed-rank tests with continuity correction (10,000 iterations) are applied to compare each item&#8217;s distribution against the median. <xref rid="sensors-25-05273-t005" ref-type="table">Table 5</xref> summarizes descriptive statistics and test results for each item.</p><p>Results indicate that all learnability items scored significantly below the median (all <italic toggle="yes">p</italic> &lt; 0.05 and all Z &lt; 0, the Z-score is a standardized test statistic used to measure the direction and strength of systematic deviations). For the reason that learnability is verse coding (1 for most learnable and 5 for not learnable at all) to stay consistent with the other two dimensions, it suggests that participants found the gesture interaction intuitive and easy to understand from the outset. Within the cognitive load dimension, most subscales are significantly below the median except mental demand (C1) and time demand (C3) (<italic toggle="yes">p</italic> &lt; 0.05 and Z &lt; 0), indicating that the gesture interface imposed a low cognitive load during use. All fatigue-related items are significantly below the median (all <italic toggle="yes">p</italic> &lt; 0.05 and all Z &lt; 0), with particularly low scores in energy depletion (F4) and motivation drop (F5), suggesting that the gesture interface does not cause notable physical or mental fatigue even during sustained use.</p><p>Taken together, the findings demonstrate that the gesture interface achieves high learnability, imposes minimal physical and mental strain, and maintains ergonomic comfort. This supports the conclusion that participants&#8217; physiological responses in the main experiment are unlikely to be confounded by discomfort or usability issues, reinforcing the ecological validity of the emotional engagement results.</p><p>Overall, the design aims to simulate real-world cultural practice processes while controlling for experimental consistency. This approach maximizes the comparability of emotional, cognitive, and physiological responses elicited by different interaction modalities. The findings serve as an empirical foundation for future studies on the role of cultural congruence and interactional immersion in shaping user emotional experiences in culturally themed digital interaction design.</p></sec><sec id="sec3dot3-sensors-25-05273"><title>3.3. Experimental Procedure</title><p>This study adopts a within-subject experimental design, in which each participant is asked to evaluate their personal gameplay experience with the Leap Motion system, keyboard interaction, and non-interactive viewing in order to determine individual preferences. To counterbalance potential carryover effects inherent in repeated-measures designs, participants are randomly assigned to different groups and exposed to the three systems in varied sequences, as illustrated in <xref rid="sensors-25-05273-f002" ref-type="fig">Figure 2</xref>. Specifically, a randomized sequence list of the three interaction modes was generated using an online random sequence generator prior to the experiment, and each participant was assigned a corresponding experience order accordingly. This approach effectively mitigates potential fatigue or learning effects associated with fixed sequences in within-subject designs, thereby improving the validity of the data and ensuring the fairness of cross-condition comparisons.</p><p>Prior to the experiment, administrators assist participants with the setup and calibration of physiological monitoring equipment, including an fNIRS device, EDA sensors, and an infrared thermal imaging camera. The entire experimental procedure is conducted under the supervision of two administrators and consists of the following six steps, as shown in <xref rid="sensors-25-05273-f003" ref-type="fig">Figure 3</xref>.</p><p>Step 1: Participants are asked to provide informed consent for the use of fNIRS, EDA sensors, and an infrared imaging camera to record their physiological responses throughout the experiment.</p><p>Step 2: Administrators introduce the task for each cultural experience game: &#8220;Use your hands or the keyboard to control the character&#8217;s movements. When the character reaches the designated position, the level is completed.&#8221; For the non-interactive condition, &#8220;Simply watch the video. No action is required.&#8221;</p><p>Step 3: Participants are engaged with the first assigned cultural heritage interaction game for 50 s, followed by a 50 s rest period. This process is repeated three times. During this phase, the fNIRS and EDA sensors continuously and automatically record participants&#8217; physiological activity, while the infrared imaging is manually operated and recorded by the experimenter using specialized software.</p><p>Step 4: After completing the first task, participants rest for 5 min before proceeding to the next interaction condition, repeating the procedures outlined in Step 3.</p><p>Step 5: After each interaction session, participants are given a 5 min interval to complete a subjective gameplay experience questionnaire, assessing their perceived experience during the game.</p><p>Step 6: Upon completing all three interaction conditions, participants are asked to indicate which cultural heritage experience game they prefer and explain why. The rationale for asking a comparative preference question rather than an absolute evaluation is to elicit more authentic insights into user preferences through relative judgments, which tend to produce more reliable results.</p><p>Throughout the experiment, two categories of data are collected: physiological signal data and subjective rating data. The physiological signals comprise three modalities: fluctuations in cerebral oxygenation obtained through fNIRS, variations in skin conductance recorded by EDA sensors, and thermal distribution images of the facial regions captured using an infrared thermal imaging camera. All physiological data are recorded as continuous time-series signals. The subjective data consist of questionnaire scale scores. <xref rid="sensors-25-05273-f004" ref-type="fig">Figure 4</xref> presents schematic representations and sample waveforms corresponding to the three physiological signal types.</p><p>The synchronized acquisition and analysis of both physiological and subjective data ensures multidimensional evidence for evaluating the effectiveness of different interaction modes, thereby providing a robust foundation for subsequent assessments.</p></sec><sec id="sec3dot4-sensors-25-05273"><title>3.4. Evaluation Criteria</title><p>As previously mentioned, we are investigating the impact of different interaction methods in cultural games on users&#8217; emotional experiences. The subjective dimension uses questionnaire items to measure participants&#8217; self-reported cultural perception and emotional involvement. <xref rid="sensors-25-05273-t006" ref-type="table">Table 6</xref> presents the questions used to assess user experience, which are categorized into three dimensions: The user experience dimension, which evaluates whether the game process is engaging under the given interaction method. The cultural understanding dimension, which measures whether the task enhances the participant&#8217;s cognition and emotional resonance with ICH. The emotional engagement dimension, which assesses whether the participant feels immersed during the task. Each question is answered using a 5-point Likert scale, with scores ranging from 1 to 5, where 1 corresponds to &#8220;very tired&#8221; and 5 corresponds to &#8220;very relaxed&#8221;.</p></sec><sec id="sec3dot5-sensors-25-05273"><title>3.5. Measurement</title><sec id="sec3dot5dot1-sensors-25-05273"><title>3.5.1. FNIRS</title><p>In this study, the brain oxygen &#946;-values are primarily measured to reflect users&#8217; cultural experience and their perception of ICH. For this experiment, the Photon Cap C20 system (Cortivision, Lublin, Poland) is utilized to collect real-time brain oxygen signals from participants&#8217; prefrontal cortex regions. The sampling frequency is 10 Hz, with wavelengths of 760 nm and 850 nm. Using the 10&#8211;20 coordinate system, 21 probes (11 light sources and 10 detectors) are placed on the participants&#8217; left and right prefrontal areas, covering three major functional regions: the orbitofrontal cortex (OFC), the ventrolateral prefrontal cortex (VLPFC), and the dorsolateral prefrontal cortex (DLPFC) [<xref rid="B48-sensors-25-05273" ref-type="bibr">48</xref>], as shown in <xref rid="sensors-25-05273-f005" ref-type="fig">Figure 5</xref>. The final measure of neural activation strength is quantified by the &#946;-value of changes in oxygenated hemoglobin concentration (HbO). Changes in HbO concentration, especially in the prefrontal cortex, are used to reflect affective responses during cultural interaction tasks, such as attention, engagement, and emotional regulation, which represent the emotional experience that culture brings to users.</p><p>The system&#8217;s accompanying software, Cortiview (version 1.11.1), is used to record the entire process, including both the interactive task segments and the corresponding resting segments. To minimize the motion artifacts, we conduct IMU (Inertial Measurement Unit) calibration through the Cortiview software. This calibration process captures real-time head motion parameters (accelerations and angular velocities across three axes) and adjusts the signal quality accordingly. The IMU module helps ensure that only stable head positions are accepted for the start of the task. During the recording, participants are also instructed to minimize unnecessary movement, especially during gesture-based tasks. Through this combination of IMU-based signal correction and behavioral control, the potential impact of motion artifacts is effectively reduced. For each interaction modality, three task repetitions are conducted, and the average value across the three trials is calculated. To control for individual baseline variability, we subtract the corresponding resting-state baseline from the averaged task-state value. This resting-state measurement serves as the emotional baseline of the participant, allowing for obtaining a baseline-normalized activation measure that more accurately reflects task-induced neural responses. This method ensures that the final values represent activation changes specifically attributable to the interaction task, rather than individual emotional or physiological differences at baseline.</p></sec><sec id="sec3dot5dot2-sensors-25-05273"><title>3.5.2. EDA</title><p>Galvanic skin response (GSR) sensors are employed in this study to monitor participants&#8217; emotional physiological reactions. GSR signals, due to their sensitivity to the sympathetic nervous system, are widely used in research on emotional arousal, stress perception, and interactive experiences. In the experiment, ErgoLAB Human Factors Experimentation Platform and the ErgoLAB EDA Wireless Galvanic Skin Sensor (Kingfar, Beijing, China) are used to collect participants&#8217; skin conductance signals. For the EDA indicator, the collection range is from 0 to 3 &#956;S, with an accuracy of 0.01 &#956;S, providing high temporal precision to ensure synchronization with events. The EDA sensors are attached to the index and middle fingers of the participant&#8217;s non-dominant hand, continuously recording the dynamic changes in skin conductance. The primary metrics record included the skin conductance (SC), tonic signal, and phasic signal, which are used to assess the intensity of emotional arousal and the activation level of the sympathetic nervous system during the gaming experience, reflecting the real-time intensity of emotional arousal during cultural interaction. After the experiment begins, EDA data are synchronously collected during both the resting and interactive phases of each task, aligned with the brain oxygen timeline. The final EDA index is obtained by subtracting the resting-state baseline from the task-state average value. This baseline-corrected EDA reflects sympathetic nervous system activation specifically induced by the interaction experience, minimizing the influence of individual emotional arousal levels at baseline.</p></sec><sec id="sec3dot5dot3-sensors-25-05273"><title>3.5.3. IRT</title><p>Infrared thermography (IRT) is used to track subtle temperature changes in facial areas, which are associated with emotional states such as stress, excitement, and engagement. The infrared camera used in our experiment is the KIR-2008z (Huajingkang Optoelectronics Technology Co., Ltd., Wuhan, China), which features a high-sensitivity, uncooled infrared focal plane detector, excellent imaging circuit components, and optical and display systems, providing superior infrared imaging performance. The camera&#8217;s optical resolution is 384 &#215; 288 pixels, with a measurement range from 30 &#176;C to 42 &#176;C and a temperature measurement accuracy of &#177;0.3 &#176;C. The field of view is 44.3&#176; &#215; 34.0&#176;. In the experiment, participants are instructed to face the thermal camera, which is positioned 0.5 m away from them.</p><p>The application used for data collection is the KIR-2008Z Infrared Thermal Imaging Health Management System (version 5.6.0), which is capable of receiving data from the thermal camera. Based on this software, an experimental procedure to acquire thermal images is carried out. During the resting period before each game session, a thermal image is captured every 10 s, resulting in 4 thermal images per game session&#8217;s resting phase, for a total of 12 images across three game sessions. The average temperature of the region of interest (ROI) in this state is taken as the participant&#8217;s baseline temperature. After the game begins, during each of the three 50 s gameplay periods, a thermal image is captured every 10 s, resulting in 4 thermal images per 50 s period and 12 thermal images per game session. The temperature of the ROI during these periods is recorded as the experimental temperature. The final IRT measure is derived by subtracting the resting-state baseline temperature from the average task-state temperature. This baseline correction isolates the thermal responses specifically induced by the interactive task and minimizes the impact of inter-individual variability.</p></sec></sec><sec id="sec3dot6-sensors-25-05273"><title>3.6. Data Preprocessing</title><p>In this study, to improve the quality of the fNIRS brain oxygen data and ensure its applicability for subsequent statistical analysis, the NIRS-KIT V3.0 Beta (a MATLAB toolbox) developed by the State Key Laboratory of Cognitive Neuroscience and Learning at Beijing Normal University is utilized for preprocessing task-related data [<xref rid="B49-sensors-25-05273" ref-type="bibr">49</xref>]. This toolbox supports graphical operation and various standardized data processing workflows, making it suitable for near-infrared brain imaging data in brain activation research. First, raw data exported from the Cortview system is imported into MATLAB R2024a and converted into a data structure compatible with NIRS-KIT V3.0 Beta. Then, the following preprocessing steps are performed on each participant&#8217;s data: (1) The raw oxygenated HbO concentration time series is trimmed to remove irrelevant time intervals. The data is segmented based on the recorded start and end timestamps of each trial. Only the time periods corresponding to the resting phase and the task phase are retained, resulting in a total of 24 min and 30 s of usable data per participant. All unrelated intervals, such as instruction time and system initialization, are excluded to avoid introducing noise into the analysis. (2) A first-order polynomial regression model is applied to estimate the underlying linear trend in the time series. The estimated trend is then subtracted from the original HbO concentration data to eliminate slow drifts and preserve task-evoked hemodynamic fluctuations. (3) To minimize motion-related artifacts, the Temporal Derivative Distribution Repair (TDDR) algorithm is applied. This method corrects sudden spikes in the data by modeling the distribution of temporal derivatives, effectively restoring signal continuity while preserving underlying neural signals. (4) Artifacts unrelated to the experimental data are eliminated by using the filtering module in NIRS-KIT, which preserves low-frequency brain activity related to the task and suppresses high-frequency physiological noise. A 3rd-order Butterworth Infinite Impulse Response (IIR) bandpass filter is applied with a frequency range set between 0.01 Hz and 0.1 Hz. This filtering process removes high-frequency physiological artifacts such as cardiac signals and motion noise, as well as low-frequency drift. It ensures the retention of meaningful hemodynamic signals that reflect task-related neural activation, consistent with standard frequency characteristics in task-based fNIRS research [<xref rid="B50-sensors-25-05273" ref-type="bibr">50</xref>]. After preprocessing, the data is saved in the NIRS-KIT standard format (.mat file), containing three types of hemoglobin concentration data (oxyData, dxyData, totalData), channel information, and task reference wave information. Following the preprocessing of the task-related fNIRS data, we perform individual-level statistical analysis based on the General Linear Model (GLM), extracting the &#946;-values corresponding to task activation for each channel, which serves as an indicator of neural activation strength. The model is formulated as in (1):<disp-formula id="FD1-sensors-25-05273"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#949;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this model, y represents the dependent variable, which is the fNIRS signal from a specific observation channel. x<sub>1</sub>, x<sub>2</sub>, &#8230;, x<sub>L</sub> denote the independent variables, which can be understood as the individual hemodynamic responses elicited by different task conditions. &#946;<sub>1</sub>, &#946;<sub>2</sub>, &#8230;, &#946;<sub>L</sub> represent the model coefficients for the independent variables, indicating the extent to which each variable contributes to the observed fNIRS signal. The portion of the fNIRS signal that cannot be explained by the explanatory variables is referred to as the residual &#949;. Considering all observation time points of the fNIRS signal y<sub>1</sub>, y<sub>2</sub>, &#8230;, y<sub>T</sub>, where T is the total number of observation points, the equation can be expressed in the following form, as in (2):<disp-formula id="FD2-sensors-25-05273"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">X</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">&#949;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this formula, Y is the observed data matrix, X is the design matrix, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the estimated parameter vector, and &#949; is the residual vector. Given the design matrix X and the observed data Y, the model parameters are estimated using the ordinary least squares method, as in (3):<disp-formula id="FD3-sensors-25-05273"><label>(3)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>X<sup>T</sup> represents the transpose of X, and the parameters <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> combine the independent variables to produce predicted values of &#374; that approximate the observed data, thereby minimizing the residual &#949;. The design matrix composed of all independent variables is the core of fNIRS data modeling and plays a decisive role in the quality of modeling and the accuracy of estimating individual hemodynamic response indicators [<xref rid="B50-sensors-25-05273" ref-type="bibr">50</xref>,<xref rid="B51-sensors-25-05273" ref-type="bibr">51</xref>]. In this study, the &#946;<sub>1</sub>, &#946;<sub>2</sub>, &#8230;, &#946;<sub>L</sub> derived from the model are used as the primary indicators of task-evoked neural activation. This standardized preprocessing workflow ensures the temporal consistency and comparability of the fNIRS data.</p><p>For the EDA data preprocessing, signal denoising, feature extraction, and baseline correction are carried out. Under synchronized experimental conditions, the ErgoLAB v3.17.16 records the time points for task onset, task offset, and rest states, ensuring that the physiological signals correspond accurately to the experimental conditions, which guarantees precise mapping between EDA data and task events. For the EDA data preprocessing, a standardized pipeline is implemented using the ErgoLAB Human Factors Experimentation Platform to ensure data quality and comparability. The preprocessing of raw EDA data is conducted as follows: (1) To remove high-frequency noise and enhance signal clarity, a Gaussian filter is applied for smoothing, with a window size of 5 samples. Gaussian filtering is a linear smoothing technique designed to reduce Gaussian noise in EDA signals. By modeling the signal data as an energy transformation process, where noise typically resides in the high-frequency domain, the Gaussian filter effectively suppresses noise while preserving the underlying physiological signal. Its core function is as in (4):<disp-formula id="FD4-sensors-25-05273"><label>(4)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi mathvariant="sans-serif">&#960;</mml:mi></mml:msqrt><mml:mi mathvariant="sans-serif">&#963;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi mathvariant="sans-serif">&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(2) The original SC signal comprises two components: a tonic component and a phasic component. The tonic component is represented by the Skin Conductance Level (SCL), which reflects the participant&#8217;s overall arousal level during a given task or resting period. For a given time interval, SCL is computed as the arithmetic mean of the SC samples in that interval, as in (5):<disp-formula id="FD5-sensors-25-05273"><label>(5)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(3) SCR features are extracted using the SCR analysis module in ErgoLAB, with peak detection sensitivity set to medium, a maximum rise time of 4 s, a half-recovery time of 4 s, and a minimum response amplitude threshold of 0.03 &#956;S. The SCR amplitude is computed as the peak minus baseline, as in (6):<disp-formula id="FD6-sensors-25-05273"><label>(6)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The SC<sub>peak</sub> is the maximum SC value within the post-stimulus window, and the SC<sub>baseline</sub> is the average SC value within the pre-stimulus baseline window. Event-related analysis windows are set to 1&#8211;4 s following the onset of each stimulus to ensure temporal relevance of SCR extraction. (4) To control for inter-individual variability, the SCL and SCR values for each task condition are baseline-corrected by subtracting the corresponding resting-state averages, resulting in &#916;SCL and &#916;SCR. All feature values are represented as the difference between each task round and the baseline phase to assess the relative changes induced by the interaction task. This enables cross-subject comparative analysis under different conditions, revealing how various interaction methods affect users&#8217; physiological arousal levels.</p><p>For preprocessing the infrared temperature data, we first define the ROI for analysis. According to previous studies, the nasal tip and bilateral cheeks in the facial region exhibit higher physiological sensitivity and stability in emotional regulation and autonomic nervous system responses [<xref rid="B52-sensors-25-05273" ref-type="bibr">52</xref>]. Among them, the nasal tip area, in particular, shows a significant response to sympathetic nervous system activation, with a notable decrease or increase in temperature under stress, pleasure, or alertness. Therefore, this study selects the nasal tip, left cheek, and right cheek as the primary temperature analysis areas, which have good emotional indicator validity and signal stability. A threshold of approximately 1.3 &#176;C change in facial temperature is considered metrologically significant and consistent with prior studies in emotion thermography [<xref rid="B53-sensors-25-05273" ref-type="bibr">53</xref>]. During the preprocessing of the temperature images, the optical detector first captures the infrared radiation signal from the target area and converts it into Analog-to-Digital (AD) data. The AD data are processed in the camera&#8217;s internal processing unit using non-uniformity correction, image filtering, sharpening, and related algorithms to transform the raw digital signal into calibrated temperature data in thermal images. In the infrared thermal images, color represents temperature, with red indicating higher temperatures and blue indicating lower temperatures. Subsequently, we use infrared thermal imaging analysis software to manually select the three ROI areas in each thermal image, with each region set as a 5 &#215; 5 pixel window to represent the target areas, as shown in <xref rid="sensors-25-05273-f006" ref-type="fig">Figure 6</xref>.</p><p>Subsequently, the average temperature value of the pixels within the window is extracted to quantify the thermal change in the region. The temperature change (&#8710;T<sub>ROI</sub>) is defined as a representative indicator of emotional activation level, as in (7).<disp-formula id="FD7-sensors-25-05273"><label>(7)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where T<sub>task</sub> represents the average temperature of the ROI during the interaction task, and T<sub>baseline</sub> is the average temperature during the resting state. The positive or negative change in &#8710;T<sub>ROI</sub> reflects the activation level of the autonomic nervous system (particularly the sympathetic nervous system) and is used to indirectly assess the emotional arousal and psychological stress levels of the participants under different interaction conditions.</p></sec></sec><sec sec-type="results" id="sec4-sensors-25-05273"><title>4. Results</title><sec id="sec4dot1-sensors-25-05273"><title>4.1. User Perception</title><p>To compare the effects of three interaction methods (gesture, keyboard, and no interaction) on users&#8217; subjective perceptions, we conduct a one-way ANOVA and post hoc tests on participants&#8217; subjective ratings across three dimensions, &#8220;user satisfaction and enjoyment,&#8221; &#8220;cultural identity and understanding,&#8221; and &#8220;emotional engagement and immersion.&#8221; Before conducting the one-way ANOVA, rigorous assumption checks were carried out to ensure the validity of the analytical results. Specifically, the Shapiro&#8211;Wilk test was used to assess whether the data for each interaction mode conformed to a normal distribution across the three subjective rating dimensions. The results indicated that the data from all groups met the normality assumption (<italic toggle="yes">p</italic> &gt; 0.05). Additionally, Levene&#8217;s test was employed to examine the homogeneity of variances, and the results confirmed that the variances across groups were homogeneous in all three dimensions (<italic toggle="yes">p</italic> &gt; 0.05), thereby satisfying the fundamental assumptions required for ANOVA. Upon meeting these prerequisites, a one-way ANOVA was conducted with the interaction mode (gesture, keyboard, and no interaction) as the independent variable and each of the three subjective rating dimensions as dependent variables to examine the main effect of the interaction mode. The results show that the interaction method has a significant main effect across all three dimensions (<italic toggle="yes">p</italic> &lt; 0.05, 95% CI), as shown in <xref rid="sensors-25-05273-t007" ref-type="table">Table 7</xref>. Further post hoc pairwise comparisons conducted using Tukey&#8217;s HSD test demonstrated that gesture interaction significantly outperforms both keyboard interaction and no interaction across all dimensions.</p><p>The results suggest that gesture interaction, with a higher cultural fit, effectively enhances users&#8217; emotional experience and cultural understanding, which is consistent with our core hypothesis that culturally congruent interaction methods provide greater user advantages.</p></sec><sec id="sec4dot2-sensors-25-05273"><title>4.2. Cortical Activation</title><sec id="sec4dot2dot1-sensors-25-05273"><title>4.2.1. Cortical Activation of Channels</title><p>In processing the HbO concentration data, we exclude records where high-quality fNIRS signals were difficult to obtain. Ultimately, 61 participants remained. To compare the cortical activation levels under different interaction conditions, we create heatmaps based on &#946;-values, illustrating the activation distribution across channels for the three interaction methods. The color range of the heatmaps goes from yellow (representing positive activation) to blue (representing negative activation), with a threshold set at &#177;1.3 &#215; 10<sup>&#8722;7</sup>, as shown in <xref rid="sensors-25-05273-f007" ref-type="fig">Figure 7</xref>.</p><p>From the heatmap, it is visually apparent that the cortical activation range is widest under the gesture interaction condition, with more yellow and green areas in the channel distribution, reflecting stronger activation levels. The keyboard interaction condition shows relatively weaker activation, with colors predominantly in blue and green, indicating lower brain activity involvement. In the no interaction condition, almost all channels present negative &#946;-values, with the blue areas most concentrated in the heatmap, suggesting a low activation state in the brain. Overall, the results indicate significant differences in cortical activation levels across the different interaction methods, with gesture interaction causing the most prominent activation.</p><p>We first conducted normality tests on the HbO concentration data collected from 29 channels, specifically using the Shapiro&#8211;Wilk test. The results showed that, except for a few channels (e.g., C4, C7, C9, and C10), the data from most channels significantly deviated from the normal distribution in both tests (<italic toggle="yes">p</italic> &lt; 0.01), indicating that the data did not meet the normality assumption. Given that parametric tests, such as analysis of variance (ANOVA), rely on strict normality assumptions, their results are prone to bias when the data fail to meet these conditions. Therefore, to ensure the validity and reliability of the statistical analysis, this study employed the Kruskal&#8211;Wallis H test, a non-parametric method. This test is applicable to scenarios involving multiple independent samples with non-normally distributed data (in this study, the interaction mode serves as the grouping variable, dividing the data into groups such as gesture interaction, keyboard interaction, and no interaction). Compared with parametric tests, the Kruskal&#8211;Wallis H test can more robustly examine the overall distribution differences among multiple independent samples when the data are non-normally distributed, avoiding result biases caused by violated distributional assumptions.</p><p>Subsequently, a non-parametric test was conducted with the three interaction modes as the grouping variable to analyze the HbO concentration data from 29 channels in the prefrontal cortex and the parietal motor cortex. The results indicated that five channels (C2, C10, C15, C18, and C24) exhibited significant differences in HbO concentration across the three interaction modes. Pairwise comparisons between groups were then performed using the Mann&#8211;Whitney U test, a non-parametric method suitable for analyzing differences between two independent samples, which was applied following a significant overall test to further identify specific between-group differences. Given that three pairwise comparisons were conducted (gesture vs. keyboard, gesture vs. no interaction, and keyboard vs. no interaction), the Bonferroni correction was applied to control the Type I error rate. Accordingly, the significance threshold was adjusted to &#945;&#8242; = 0.05/3 &#8776; 0.0167 to ensure the rigorous determination of statistical significance.</p><p>Gesture Interaction vs. Keyboard Interaction: In channel 2 (<italic toggle="yes">p</italic> = 0.015), the activation levels for gesture interaction are significantly higher than for keyboard interaction. This suggests that, compared to traditional keyboard input, gesture interaction may better engage users&#8217; cognitive resources and emotional experiences, especially in brain areas involved in motor control and contextual simulation.</p><p>Gesture Interaction vs. No Interaction: In channel 2 (<italic toggle="yes">p</italic> = 0.002), 10 (<italic toggle="yes">p</italic> = 0.004), 15 (<italic toggle="yes">p</italic> = 0.001), 18 (<italic toggle="yes">p</italic> = 0.006), and 24 (<italic toggle="yes">p</italic> = 0.001), the activation levels for gesture interaction are significantly higher than in the no interaction condition. This suggests that, compared to no interaction, gesture interaction triggers greater activation in the prefrontal cortex, reflecting that users may experience higher levels of cognitive load, emotional engagement, or decision-related neural activity under this interaction mode.</p><p>Further examination of the distribution of these significantly different channels in cortical space reveals that they are primarily concentrated in areas related to emotional processing, motivational evaluation, and cognitive control. This spatial distribution not only supports the advantage of gesture interaction in engaging the multidimensional neural system but also suggests that it may trigger deeper emotional and cognitive involvement in the context of cultural scenario construction. In contrast, although the heatmaps for keyboard interaction and no interaction conditions show certain visual differences, there are no significant differences in activation levels across all channels in terms of statistical significance. This indicates that the differences in cortical resource engagement between the two conditions are relatively limited and may not effectively stimulate higher-level integrative processing functions in the prefrontal cortex, which in turn affects the user&#8217;s immersion experience and active engagement.</p></sec><sec id="sec4dot2dot2-sensors-25-05273"><title>4.2.2. Cortical Activation of ROIs</title><p>To further explore the differences in HbO concentration activation across larger cortical areas for different interaction methods, we construct three ROIs based on data from 29 channels, namely OFC, VLPFC, and DLPFC, as shown in <xref rid="sensors-25-05273-f008" ref-type="fig">Figure 8</xref>. Each ROI consists of multiple mirror&#8211;image channels from the left and right hemispheres (OFC: Channels 14&#8211;19, VLPFC: Channels 1, 6, 7, 13, 22, 27, DLPFC: Channels 2 to 5, 8 to 12, 23, 26), and the cortical activation level for each ROI is calculated by averaging the &#946;-values across all channels within the respective ROI.</p><p>We first assessed the normality of the mean cortical activation values of three interaction modes (gesture, keyboard, and no interaction) within each region of interest (ROI) using the Shapiro&#8211;Wilk test. Since the normality assumption was not met (<italic toggle="yes">p</italic> &lt; 0.05), we conducted non-parametric tests using the Kruskal&#8211;Wallis H test to evaluate the overall differences between different conditions. Subsequently, the Mann&#8211;Whitney U test was employed for pairwise comparisons, and the Bonferroni correction was applied to control Type I errors, with the significance threshold adjusted to &#945;&#8242; &#8776; 0.0167. The results indicate significant differences in HbO activation levels between the different interaction modes across the three ROIs (see <xref rid="sensors-25-05273-t008" ref-type="table">Table 8</xref>), as detailed below.</p><p>Gesture Interaction vs. Keyboard Interaction: In the OFC (<italic toggle="yes">p</italic> = 0.010), the activation level for gesture interaction is significantly higher than that for keyboard interaction, whereas no significant difference is found in the VLPFC or DLPFC. This suggests that gesture interaction may more effectively engage brain areas associated with higher-order decision-making and attention control when handling culturally related tasks, while keyboard interaction shows a narrower activation range.</p><p>Gesture Interaction vs. No Interaction: In the OFC (<italic toggle="yes">p</italic> = 0.006), VLPFC (<italic toggle="yes">p</italic> = 0.010), and DLPFC (<italic toggle="yes">p</italic> = 0.005), the activation levels for gesture interaction are significantly higher than those for the no interaction condition. This result indicates that gesture interaction, compared to no interaction, triggers broader activation of the prefrontal cortex, involving neural processes related to decision-making, reward anticipation, action control, and emotional evaluation. This may reflect a higher level of user engagement and emotional involvement in this mode.</p><p>Keyboard Interaction vs. No Interaction: No significant differences in activation are observed between keyboard interaction and no interaction in any of the three ROIs. This suggests that traditional input methods may not significantly enhance the user&#8217;s neural engagement in the task, particularly in regions associated with emotional or cultural cognitive processing.</p><p>Overall, gesture interaction shows the highest brain oxygen activation levels across all three key prefrontal ROIs, further supporting its potential to provide more natural and immersive interactive support in contextualized, concrete cultural experience scenarios, thereby eliciting stronger emotional involvement and cognitive processing.</p></sec><sec id="sec4dot2dot3-sensors-25-05273"><title>4.2.3. Inter-Channel Activation Covariance Analysis</title><p>To investigate the coordination patterns of cortical activation levels across different brain regions under various interaction modes, a Pearson correlation analysis was conducted on the &#946;-values of 29 fNIRS channels for each of the three interaction modes, resulting in a total of 406 channel pairs (29 &#215; 28/2).</p><p><xref rid="sensors-25-05273-f009" ref-type="fig">Figure 9</xref> presents the correlation heatmaps under the three interaction modes. The heatmaps are symmetric along the diagonal from the top-left to the bottom-right. Each pixel in the 29 &#215; 29 matrix represents the Pearson correlation coefficient between a pair of channels, with blue indicating +1, red indicating &#8722;1, and white indicating 0, along with gradual color transitions representing intermediate values. The channels are ordered based on their corresponding regions of interest (ROIs), specifically the OFC, VLPFC, and DLPFC. The three ROIs are delineated by gaps forming a 3 &#215; 3 sub-matrix structure within the heatmap.</p><p>The comparison reveals that under the gesture interaction condition, the heatmap exhibited a relatively even distribution of color blocks, particularly within the DLPFC and between the DLPFC, OFC, and VLPFC regions. This pattern indicates widespread and flexible coordination patterns, reflecting more diverse and dynamically regulated cortical co-activation across multiple brain areas. In contrast, for the keyboard interaction condition, the darker color blocks were concentrated in specific regions, mainly within the OFC and parts of the DLPFC. This distribution represents a more stable but limited activation pattern, with moderate coordination strength, suggesting relatively restricted cognitive and emotional regulation demands. For the non-interactive condition, the darker color blocks were primarily confined within the OFC, VLPFC, and DLPFC regions, with markedly reduced cross-regional coordination. The activation pattern appeared more homogeneous and lacked dynamic variation.</p></sec></sec><sec id="sec4dot3-sensors-25-05273"><title>4.3. EDA Data Analysis</title><p>To assess the physiological arousal levels under different interaction modes, this study analyzes three key indicators in skin conductance: SC, the tonic component, and the phasic component. Data are preprocessed using the EDA signal processing tool, removing artifacts, and then extracting the mean values for the total SC, the mean value of the tonic baseline trend component, and the mean value of the phasic rapid fluctuation response component. To analyze electrodermal activity (EDA), the mean value for each interaction condition (gesture, keyboard, and no interaction) was first calculated across all participants. Since the EDA signals did not follow a normal distribution, thereby violating the assumption of normality required for parametric tests, the Kruskal&#8211;Wallis H test was employed. When a significant overall effect was observed, pairwise comparisons were conducted using the Mann&#8211;Whitney U test to identify specific differences between conditions. Additionally, the Bonferroni correction was applied to control for Type I error, adjusting the significance threshold to &#945;&#8242; = 0.05/3 &#8776; 0.0167 accordingly. The results, as shown in <xref rid="sensors-25-05273-t009" ref-type="table">Table 9</xref>, indicate significant differences across the three indicators (SC: <italic toggle="yes">p</italic> &lt; 0.001; tonic: <italic toggle="yes">p</italic> &lt; 0.001; phasic: <italic toggle="yes">p</italic> &lt; 0.001) between different interaction modes. Further post hoc testing reveals that gesture interaction significantly differs from no interaction across all indicators, while keyboard interaction also significantly differs from no interaction in SC (<italic toggle="yes">p</italic> &lt; 0.001), tonic (<italic toggle="yes">p</italic> &lt; 0.001), and phasic (<italic toggle="yes">p</italic> = 0.005). No significant differences are found between gesture interaction and keyboard interaction. <xref rid="sensors-25-05273-f010" ref-type="fig">Figure 10</xref> illustrates the distribution of mean values for the three skin conductance indicators under the three interaction modes, visually depicting the impact of different interaction modes on the skin conductance response.</p></sec><sec id="sec4dot4-sensors-25-05273"><title>4.4. Infrared Temperature Data</title><p>This experiment analyzes the effect of different interaction modes on facial temperature. Infrared thermography is used to investigate the physiological differences in users across various experiences. Temperature change is defined as the difference between the average temperature during rest and the average temperature during the three interaction modes. Three indicators are used as dependent variables: the temperature change at the nasal tip, the temperature change on the right cheek, and the temperature change on the left cheek. To ensure the integrity of facial temperature data, we exclude samples with missing data due to non-capture by the infrared camera and samples with anomalous temperature data from specific ROIs, leaving a total of 34 subjects. We observe that the temperatures of the three facial ROIs change when users perform tasks under the three different interaction modes. Prior to conducting the analysis of variance (ANOVA), the prerequisites for the analysis must be verified. First, the Shapiro&#8211;Wilk test was employed to assess the normality of data across the three regions of interest (ROIs) under each interaction condition. The results indicated that the data conformed to a normal distribution (<italic toggle="yes">p</italic> &gt; 0.05), thus satisfying the basis for parametric testing. Second, Levene&#8217;s test was employed to verify the homogeneity of variances, with &#8220;based on the mean&#8221; selected as the test type. The results confirmed that the variances across all ROI dimensions were homogeneous under the three interaction conditions (<italic toggle="yes">p</italic> &gt; 0.05), ensuring the validity of the subsequent one-way ANOVA results. A one-way ANOVA test was applied to examine group differences among the three interaction modes, with interaction mode (comprising three levels: gesture interaction, keyboard interaction, and no interaction) as the grouping independent variable, and temperature changes in the nasal tip, right cheek, and left cheek as the dependent variables, respectively. Separate models were constructed for each region of interest (ROI) for analysis. The results indicated that significant effects were observed in the nasal tip, right cheek, and left cheek, as shown in <xref rid="sensors-25-05273-t010" ref-type="table">Table 10</xref> (<italic toggle="yes">p</italic> &lt; 0.05, 95% CI). Subsequently, Tukey&#8217;s HSD test was employed for post hoc analyses to further compare the differences between groups. The nasal tip temperature shows significant differences between gesture interaction and keyboard interaction, as well as between gesture interaction and no interaction (<italic toggle="yes">p</italic> &lt; 0.01; <italic toggle="yes">p</italic> &lt; 0.05). The right cheek temperature also exhibits significant differences between gesture interaction and keyboard interaction and between gesture interaction and no interaction (<italic toggle="yes">p</italic> &lt; 0.01; <italic toggle="yes">p</italic> &lt; 0.05). The left cheek temperature shows a significant difference between gesture interaction and no interaction (<italic toggle="yes">p</italic> &lt; 0.05).</p><p>Temperature changes exhibit consistency across the three interaction modes, with the most significant temperature change occurring during gesture interaction, followed by keyboard interaction, and the smallest temperature change during no interaction. During finger-based interaction, the facial ROIs of users show a temperature clearly higher than the baseline temperature, while during keyboard interaction, the nasal tip and right cheek temperatures are lower than the baseline, and no significant changes are observed during no interaction, as shown in <xref rid="sensors-25-05273-t011" ref-type="table">Table 11</xref>.</p><p>From <xref rid="sensors-25-05273-f011" ref-type="fig">Figure 11</xref>, we observe that during finger interaction, the temperature at the nasal tip rises and then falls across three stages, with a gradual decrease in temperature throughout the interaction. Temperature changes in the cheeks are minimal. During keyboard interaction, the nasal tip temperature shows an overall decreasing trend, but with a smaller change compared to gesture interaction. When users watch a video, there are no significant temperature changes in the three regions of interest, with nasal tip, right cheek, and left cheek temperatures remaining at 33.8 &#176;C, 33.5 &#176;C, and 33.7 &#176;C, respectively.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05273"><title>5. Discussion</title><sec id="sec5dot1-sensors-25-05273"><title>5.1. Major Findings</title><sec id="sec5dot1dot1-sensors-25-05273"><title>5.1.1. User Perception</title><p>Gesture-based interaction demonstrates significantly higher performance in subjective evaluation metrics such as user satisfaction, cultural identity, and emotional immersion compared to keyboard-based interaction and non-interactive modes. This advantage stems from its inherent alignment with the traditional manipulation techniques of Chinese marionette puppetry. Unlike conventional input methods such as keystrokes or passive observation, gesture interaction more closely mirrors the expressive mode of &#8220;conveying emotion through hand movements&#8221; and &#8220;expressing meaning through form,&#8221; which are fundamental to this intangible cultural heritage. This alignment between physical actions and cultural context fosters stronger cultural resonance and immersion during user interaction, thereby enhancing the overall quality of the subjective experience.</p><p>Moreover, this study further validates the critical role of cultural consistency in interaction design. We observe that even when interaction modes serve similar functional purposes (e.g., keyboard vs. gesture), differences in cultural expression can significantly affect user evaluations. This finding suggests that cultural elements are not merely esthetic embellishments at the content level but serve as pivotal factors influencing interaction acceptance and user perception. This is particularly important in systems designed for specific cultural communities or heritage preservation purposes.</p></sec><sec id="sec5dot1dot2-sensors-25-05273"><title>5.1.2. Cortical Activation</title><list list-type="order"><list-item><p>Cortical Activation of Channels</p></list-item></list><p>In terms of channel-level activation, the gesture-based interaction mode elicits significantly higher activation across multiple prefrontal cortex (PFC) channels compared to keyboard interaction and the non-interactive condition. Notably, the differences are most prominent in channels 2, 10, 15, 18, and 24.</p><p>The prefrontal cortex, particularly the dorsolateral and ventrolateral regions, is widely associated with higher-order cognitive functions, goal-directed behavior, and interactive experiences within social contexts [<xref rid="B54-sensors-25-05273" ref-type="bibr">54</xref>]. The observed differences in activation across channels suggest that gesture-based interaction may drive users to engage with more cognitive resources, involving deeper motor simulation and contextual processing. This leads to heightened cortical responses at the neural level.</p><p>In the context of the marionette-themed digital game, gesture interaction enhances immersion and bodily engagement by simulating authentic performance gestures. Such embodied interaction appears to facilitate richer internal simulation and more effective allocation of attentional resources. In contrast, no significant activation differences are observed between the keyboard-based and non-interactive conditions, indicating that traditional interaction methods with merely formal input may be limited in eliciting engagement of emotional and cognitive systems in the brain.</p><list list-type="simple"><list-item><label>2.</label><p>Cortical Activation of ROIs</p></list-item></list><p>At the level of regions of interest (ROIs), gesture-based interaction demonstrates significant neural activation advantages. Compared with keyboard and non-interactive modes, gesture interaction elicited stronger activation in key prefrontal regions, including the OFC, DLPFC, and VLPFC. These ROI-level findings corroborate the channel-wise results discussed earlier, further indicating that gesture interaction more broadly recruits brain regions associated with emotional and cognitive processing.</p><p>Specifically, the significant activation of the OFC suggests that gesture interaction engages processes related to emotion regulation, value evaluation, and contextual perception. Given the OFC&#8217;s close association with positive emotional experiences, its heightened activity implies that users may achieve a higher level of emotional engagement during embodied and culturally grounded interactions [<xref rid="B55-sensors-25-05273" ref-type="bibr">55</xref>]. This neural pattern supports the subjective evaluation results showing that gesture interaction delivers a superior emotional experience.</p><p>The DLPFC, known as a central hub for cognitive control and task planning, also shows stronger activation under gesture-based interaction. This indicates that users engage in higher-order cognitive functions such as goal management, working memory retrieval, and action planning. These cognitive demands often co-occur with stronger immersion and emotional involvement, aligning with the cultural resonance and emotional efficacy perceived in the gesture interaction condition.</p><p>Regarding the VLPFC, the observed activation differences between gesture-based and non-interactive conditions further demonstrate that embodied interaction triggers neural responses associated with attention control and inhibitory processing. Although the difference between gesture and keyboard interaction in the VLPFC was not statistically significant, the trend still favors gesture interaction, suggesting potential advantages in contextual adaptation and motivational engagement.</p><p>In contrast, no significant differences are observed between the keyboard and non-interactive conditions across any ROI. This neural &#8220;silence&#8221; reinforces the limitations of traditional interaction methods in evoking cultural immersion and emotional arousal&#8212;lacking both action semantics and bodily embedding, such interactions fail to activate deeper neural processes.</p><p>Overall, the ROI results provide a clear neural basis for the observed subjective advantages of gesture interaction: the broader and deeper activation of prefrontal regions reveals a compound effect on emotional resonance, cognitive resource allocation, and attentional focus. This neural mechanism underpins the enhanced affective experience during culturally themed tasks and underscores the critical importance of culturally congruent interaction design.</p><list list-type="simple"><list-item><label>3.</label><p>Inter-Channel Activation Covariance Analysis</p></list-item></list><p>The inter-channel &#946;-value correlation analysis reveals distinct co-activation patterns within the prefrontal cortex under different interaction modalities, offering a novel perspective on how brain regions functionally integrate in response to task demands [<xref rid="B56-sensors-25-05273" ref-type="bibr">56</xref>]. Although this analysis does not capture time-resolved functional connectivity in the traditional sense, it reflects structural covariance across regions, shedding light on how cognitive resources are mobilized and integrated under various interaction conditions.</p><p>Among the three interaction modes, gesture-based interaction elicited the uniform inter-channel covariance, indicating a higher level of functional coordination across prefrontal regions. This suggests that, compared to keyboard input or passive observation, gesture interaction engages broader neural networks and facilitates cross-regional integration in the prefrontal cortex. Such widespread synchronization implies greater cognitive resource demands, likely due to the embodied, expressive, and contextually meaningful nature of gesture-based tasks.</p><p>In contrast, the keyboard interaction condition primarily exhibits intra-regional correlations centered within the OFC, potentially reflecting localized processing related to motivation, affective evaluation, or simple decision-making. The non-interactive condition, meanwhile, shows sparse inter-channel correlations with weaker cross-regional connectivity, indicating lower overall engagement and minimal functional integration.</p><p>These results underscore the significant influence of interaction modality on the functional architecture of the prefrontal cortex. The findings support a positive relationship between the complexity of interaction and the extent of cortical resource integration&#8212;suggesting that culturally meaningful and physically embodied interaction modes such as gestures not only enhance user experience but also promote more extensive and coherent neural processing patterns.</p></sec><sec id="sec5dot1dot3-sensors-25-05273"><title>5.1.3. EDA</title><p>In terms of overall trends, both skin conductance (SC) and phasic components are significantly higher under the gesture-based interaction condition compared to the non-interactive condition. This suggests that gesture interaction elicits a stronger physiological arousal response during task execution. Conversely, SC and tonic values remain lowest in the non-interactive condition, indicating that participants maintain a relatively calm and passive physiological state when not engaged in interaction.</p><p>Non-parametric tests of SC values reveal highly significant differences (<italic toggle="yes">p</italic> &lt; 0.001), with both gesture vs. non-interaction and keyboard vs. non-interaction conditions showing significant contrasts. However, no statistical difference is observed between gesture and keyboard interactions. This indicates that the presence of interaction itself is sufficient to elevate physiological arousal, while the specific modality (gesture vs. keyboard) plays a less critical role in modulating SC.</p><p>Regarding the tonic component, the keyboard interaction condition exhibits the highest mean tonic values, followed by gesture interaction, with the non-interactive condition being the lowest. This pattern may reflect the sustained autonomic nervous system activation required for keyboard-based tasks. Although the difference between gesture and keyboard interactions is not statistically significant (<italic toggle="yes">p</italic> = 0.718), both gesture vs. non-interaction and keyboard vs. non-interaction comparisons showed significant differences (<italic toggle="yes">p</italic> &lt; 0.001), reinforcing the notion that interaction engages baseline autonomic regulation more effectively than passive observation.</p><p>Phasic activity further supports these findings, with significantly higher values observed in the gesture interaction condition compared to the non-interactive condition (<italic toggle="yes">p</italic> &lt; 0.001). This suggests that gesture-based interaction is more likely to trigger rapid, short-term physiological responses. The difference between keyboard and non-interaction conditions also reached significance (<italic toggle="yes">p</italic> = 0.005), implying that phasic components are more sensitive to immersive and dynamic interaction forms such as gestures and keyboard.</p></sec><sec id="sec5dot1dot4-sensors-25-05273"><title>5.1.4. Thermal Imaging</title><p>Based on the results obtained from infrared data, it has been observed that each participant responds differently to the same game task. However, a similar trend of facial thermal responses is evident among the majority of participants. Significant differences in facial temperature changes have been found across three ROIs when comparing gesture-based interaction with non-interactive tasks. Furthermore, compared with keyboard interaction, gesture-based interaction shows significant temperature differences in the tip of the nose and the right cheek areas. Significant differences in facial temperature changes have been observed across three ROIs when comparing gesture-based interaction with non-interactive conditions. When comparing gesture-based interaction with keyboard-based interaction, significant thermal differences appear in the tip of the nose and the right cheek areas. During gesture interaction, temperature increases are evident in all three ROIs, with the most pronounced rise occurring in the nasal tip region [<xref rid="B57-sensors-25-05273" ref-type="bibr">57</xref>].</p><p>Thermal changes in the nasal area reflect the vasoconstriction and vasodilation controlled by central nervous system (CNS) activation mediated through the sympathetic nervous system (SNS). Compared to the cheeks, the nasal region has a thinner dermal layer, making it more sensitive to changes in blood flow. Therefore, when game tasks stimulate the autonomic nervous system (ANS), the acceleration of facial blood circulation leads to more notable temperature changes in the nasal region. The relatively larger thermal variations induced by gesture interaction, compared to keyboard interaction or passive video viewing, may be attributed to factors such as emotional responses and physical engagement.</p><p>Trend analyses of average temperature changes reveal variations in users&#8217; emotional valence and arousal levels under different interaction modes. The temperature of the nasal tip exhibits a rise-then-fall pattern across all phases of gesture-based and keyboard-based interaction, with a greater decline observed during gesture interaction. According to the Arousal Effect Theory, facial thermal responses correlate more strongly with arousal levels than emotional valence. The more arousing the stimulus, the more intense and rapid the thermal response [<xref rid="B58-sensors-25-05273" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-05273" ref-type="bibr">59</xref>]. Hence, gesture interaction appears to elicit higher arousal levels in users. Its complex and large-scale movements may quickly provoke emotional fluctuations and promote cognitive engagement, leading to increased SNS activity and a temporary rise in temperature.</p><p>By contrast, keyboard interaction, due to its simplicity and familiarity, results in lower levels of arousal. In the later stages of each game task, participants gradually become accustomed to the interaction method, leading to psychological relaxation and physical ease. As a result, SNS activity decreases, nasal temperature begins to drop, and physiological responses stabilize.</p><p>Under non-interactive conditions, compared to gesture interaction, users experience lower levels of emotional arousal and valence. Consequently, no substantial temperature changes are observed in the three ROIs.</p><p>When designing interaction systems based on cultural elements, the choice of interaction modality plays a critical role in shaping user experience. Gesture-based interaction, aligned with the cultural characteristics of Quanzhou Puppetry, can quickly activate emotional engagement and cognitive processing, thereby offering a highly immersive gaming experience. However, it is also necessary to manage the challenges posed by such specialized interaction forms, as high-load operations or repetitive tasks may lead to user fatigue or stress. A well-balanced design should thus ensure both immersion and usability.</p></sec></sec><sec id="sec5dot2-sensors-25-05273"><title>5.2. Design Implications</title><p>Our findings provide practical guidance for designing culturally resonant and inclusive interactive systems. Specifically, we address the following: (1) enhancing cultural relevance through congruent interaction modalities (e.g., tradition-inspired gestures); (2) indications for designers; (3) implications for inclusive design, particularly for users with sensory/motor differences; and (4) extending this approach to other cultural formats (music, crafts, storytelling). We detail these cross-cutting opportunities below.</p><sec id="sec5dot2dot1-sensors-25-05273"><title>5.2.1. For Interaction Design</title><p>It could be stated that the design of cultural digital interactions should be fundamentally driven by cultural characteristics, with the goal of constructing interaction models that align with users&#8217; cultural cognition. This approach enhances users&#8217; emotional engagement and cultural immersion. Designers are encouraged to embed culturally representative elements&#8212;such as ritualistic behaviors, linguistic conventions, visual symbols, and narrative structures&#8212;into the interaction logic, allowing users to develop a sense of cultural belonging and identity through active participation. Additionally, it is recommended to implement adaptive interaction feedback mechanisms that dynamically guide users toward emotional connections with cultural content, thereby enriching both the depth and warmth of the overall cultural experience. Furthermore, special attention should be given to users&#8217; diverse cultural backgrounds and cultural sensitivities. Designers should seek a balanced strategy between interaction personalization and cultural universality to support broader cultural understanding and emotional resonance across different user groups.</p></sec><sec id="sec5dot2dot2-sensors-25-05273"><title>5.2.2. For Designers</title><p>Designers of cultural interactive systems could also benefit from this study. Our study offers a novel perspective for interaction designers, particularly those working in cultural domains. Unlike conventional common concerns in interaction design that tend to be technology-driven or rely on sporadic creative insights, we point out a serious consideration that is focused on the intrinsic correlation between the interaction modalities and culturally inherent characteristics. This focus is empirically supported by experimental data, providing designers with actionable insights. Specifically, we demonstrate that employing culturally congruent input modalities&#8212;such as gesture interactions that align with traditional practices&#8212;can significantly enhance emotional engagement and cultural resonance. This enables designers to develop user experiences that are both intuitively accessible and rich in symbolic meaning.</p></sec><sec id="sec5dot2dot3-sensors-25-05273"><title>5.2.3. For Inclusive Design</title><p>Inclusive design in cultural interaction systems should address the diverse physical and sensory abilities of potential users while preserving the depth of cultural immersion. The key insight is that cultural authenticity can be preserved while lowering the participation threshold. For instance, in the cultural experience of gesture interaction, multisensory feedback&#8212;such as synchronized visual cues or subtle haptic signals&#8212;can compensate for reduced perceptual channels, enhancing situational awareness and emotional engagement. Additionally, gradual, adaptive guidance can further help users acclimate to culturally specific gestures without losing the symbolic resonance of the interaction. These strategies demonstrate how inclusive design can broaden access while retaining the immersive and emotionally rich qualities of culturally rooted interfaces.</p></sec><sec id="sec5dot2dot4-sensors-25-05273"><title>5.2.4. For Other Cultural Formats</title><p>Although this study focuses on a specific form of cultural heritage interaction, the methodological approach and design principles identified here are transferable to other cultural domains, such as traditional music, handicrafts, and oral storytelling. In music-based cultural systems, interaction modalities could mimic traditional instrument-handling postures, enabling users to engage not only with the auditory content but also with the embodied cultural practices. For crafts, multimodal sensing can capture users&#8217; fine-motor engagement, giving the user an immersive sense of operation. In oral storytelling contexts, combining speech-based interfaces with visual symbolism can replicate the rhythm, emphasis, and performative qualities of live narration. Across these varied formats, the use of culturally congruent interaction methods and symbolic alignment between interface and heritage elements can collectively foster emotional resonance, cultural respect, and experiential depth.</p></sec></sec><sec id="sec5dot3-sensors-25-05273"><title>5.3. Limitations</title><p>This study has several limitations. First, this study focuses on a clearly defined participant group, young university students because in this intelligent era, young people occupy a major proportion of the audience for digital heritage experiences rather than the traditional way. It is quite important to find a truly useful way for them to access culture. This group naturally possesses higher familiarity with digital interaction modalities compared to other groups. Therefore, the findings and interpretations presented here are most relevant to this demographic. While this targeted sampling ensures internal consistency, it also means that the results may not directly extend to other populations with different age profiles, cultural backgrounds, or levels of digital literacy. Another limitation lies in the relatively narrow experimental setting, which focused on a single interaction mode within a specific cultural context. This restricts the ability to compare how combinations of diverse cultural elements and interaction types affect user experience. Future research could explore the emotional and cultural impacts of digital heritage design across multicultural contexts and through wider interaction modalities, such as haptic, voice-based, or immersive visual interfaces.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05273"><title>6. Conclusions</title><p>This study explores the impact of culturally congruent interaction design on young university users&#8217; emotional and cultural experiences by integrating cultural characteristics with interaction design. The experimental results show that digital designs with more distinct cultural features and interaction methods that align with users&#8217; existing cultural cognition are more likely to evoke positive emotional responses and a sense of cultural identity. This finding provides empirical support for interaction design in a cultural activation context, indicating that interaction methods should not detach from their cultural context, but should fully consider users&#8217; cultural psychological foundations and emotional resonance mechanisms.</p><p>Furthermore, this study also finds that cultural factors in relevant designs do not merely remain on the content level but act as important variables influencing users&#8217; perceptions and behaviors. In culturally immersive interaction design, the degree to which cultural elements (such as language, action logic, and visual symbols) integrate with interaction mechanisms significantly affects users&#8217; acceptance of the system and their deeper cultural understanding. This conclusion provides theoretical references and practical insights for fields such as digital cultural heritage preservation, cultural heritage activation design, and immersive cultural tourism experience systems.</p><p>In future work, we aim to expand the diversity of user samples beyond the university student population, introducing a broader range of cultural backgrounds and age groups to verify the acceptance levels and differential responses to cultural interaction methods among different populations. We will also enrich the dimensions and complexity of interaction methods, combining multimodal interactions (such as motion capture and haptic feedback) and multisensory stimuli to explore richer cultural experience pathways.</p></sec></body><back><ack><title>Acknowledgments</title><p>We would like to express our sincere gratitude to Beijing Hengzhi Technology Co., Ltd. for their generous support in providing the Photon Cap C20 system used in this study. We also thank Wuhan Huajingkang Technology Co., Ltd. for providing the Kir-2008z infrared thermal imager, and Beijing Kingfa Co., Ltd. for their strong support and guidance. We also thank all individuals and organizations that contributed to the completion of this research, including Yi Liu from Beijing University of Technology and all participants from different majors.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.Z. and Y.L.; methodology, J.Z.; software, Y.M.; validation, J.Z., Y.M., R.W. and X.Z.; formal analysis, X.Z. and H.L.; investigation, R.W.; resources, F.Z.; data curation, Y.M. and H.L.; writing&#8212;original draft preparation, Y.M., H.L. and X.Z.; writing&#8212;review and editing, J.Z., Y.L., R.W. and Z.Z.; visualization, R.W. and Z.Z.; supervision, F.Z.; project administration, Y.L. and F.Z.; funding acquisition, J.Z. and F.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted following the Declaration of Helsinki and approved for human research by the Science and Technology Ethics Committee of Beijing University of Technology.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05273"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andreoli</surname><given-names>R.</given-names></name><name name-style="western"><surname>Corolla</surname><given-names>A.</given-names></name><name name-style="western"><surname>Faggiano</surname><given-names>A.</given-names></name><name name-style="western"><surname>Malandrino</surname><given-names>D.</given-names></name><name name-style="western"><surname>Pirozzi</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ranaldi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Santangelo</surname><given-names>G.</given-names></name><name name-style="western"><surname>Scarano</surname><given-names>V.</given-names></name></person-group><article-title>A Framework to Design, Develop, and Evaluate Immersive and Collaborative Serious Games in Cultural Heritage</article-title><source>J. Comput. Cult. Herit.</source><year>2017</year><volume>11</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1145/3064644</pub-id></element-citation></ref><ref id="B2-sensors-25-05273"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Theodoropoulos</surname><given-names>A.</given-names></name><name name-style="western"><surname>Antoniou</surname><given-names>A.</given-names></name></person-group><article-title>VR Games in Cultural Heritage: A Systematic Review of the Emerging Fields of Virtual Reality and Culture Games</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>8476</elocation-id><pub-id pub-id-type="doi">10.3390/app12178476</pub-id></element-citation></ref><ref id="B3-sensors-25-05273"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>J.</given-names></name></person-group><article-title>The Evolution of Digital Cultural Heritage Research: Identifying Key Trends, Hotspots, and Challenges through Bibliometric Analysis</article-title><source>Sustainability</source><year>2024</year><volume>16</volume><elocation-id>7125</elocation-id><pub-id pub-id-type="doi">10.3390/su16167125</pub-id></element-citation></ref><ref id="B4-sensors-25-05273"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>DaCosta</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kinsell</surname><given-names>C.</given-names></name></person-group><article-title>Serious Games in Cultural Heritage: A Review of Practices and Considerations in the Design of Location-Based Games</article-title><source>Educ. Sci.</source><year>2023</year><volume>13</volume><elocation-id>47</elocation-id><pub-id pub-id-type="doi">10.3390/educsci13010047</pub-id></element-citation></ref><ref id="B5-sensors-25-05273"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nacke</surname><given-names>L.E.</given-names></name></person-group><article-title>Games user research and physiological game evaluation</article-title><source>Human&#8211;Computer Interaction Series</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>63</fpage><lpage>86</lpage></element-citation></ref><ref id="B6-sensors-25-05273"><label>6.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Research on User Experience Evaluation and Quantitative Model of Virtual Exhibition Hall of Intangible Cultural Heritage</article-title><source>Master&#8217;s Thesis</source><publisher-name>Chongqing University</publisher-name><publisher-loc>Chongqing, China</publisher-loc><year>2019</year></element-citation></ref><ref id="B7-sensors-25-05273"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Research on User Experience Evaluation of Mobile Museum Based on APEC Model Framework</article-title><source>Shanxi Arch.</source><year>2020</year><volume>04</volume><fpage>132</fpage><lpage>146</lpage></element-citation></ref><ref id="B8-sensors-25-05273"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name></person-group><article-title>User Experience Evaluation Study of Red Culture Digital Museum</article-title><source>Master&#8217;s Thesis</source><publisher-name>Wuhan University of Science and Technology</publisher-name><publisher-loc>Wuhan, China</publisher-loc><year>2023</year><pub-id pub-id-type="doi">10.27380/d.cnki.gwkju.2023.000873</pub-id></element-citation></ref><ref id="B9-sensors-25-05273"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Memorable tourism experiences&#8217; formation mechanism in cultural creative tourism: From the perspective of embodied cognition</article-title><source>Sustainability</source><year>2023</year><volume>15</volume><elocation-id>4055</elocation-id><pub-id pub-id-type="doi">10.3390/su15054055</pub-id></element-citation></ref><ref id="B10-sensors-25-05273"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>K.Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Affective-Computing-Driven Personalized Display of Cultural Information for Commercial Heritage Architecture</article-title><source>Appl. Sci.</source><year>2025</year><volume>15</volume><fpage>2076</fpage><lpage>3417</lpage><pub-id pub-id-type="doi">10.3390/app15073459</pub-id></element-citation></ref><ref id="B11-sensors-25-05273"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siqueira</surname><given-names>E.S.</given-names></name><name name-style="western"><surname>Fleury</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Lamar</surname><given-names>M.V.</given-names></name><name name-style="western"><surname>Drachen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Castanho</surname><given-names>C.D.</given-names></name><name name-style="western"><surname>Jacobi</surname><given-names>R.P.</given-names></name></person-group><article-title>An automated approach to estimate player experience in game events from psychophysiological data</article-title><source>Multimed. Tools Appl.</source><year>2023</year><volume>82</volume><fpage>19189</fpage><lpage>19220</lpage><pub-id pub-id-type="doi">10.1007/s11042-022-13845-5</pub-id></element-citation></ref><ref id="B12-sensors-25-05273"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McGrath</surname><given-names>C.</given-names></name><name name-style="western"><surname>Palmgren</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>Liljedahl</surname><given-names>M.</given-names></name></person-group><article-title>Twelve tips for conducting qualitative research interviews</article-title><source>Med. Teach.</source><year>2018</year><volume>41</volume><fpage>1002</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1080/0142159X.2018.1497149</pub-id><pub-id pub-id-type="pmid">30261797</pub-id></element-citation></ref><ref id="B13-sensors-25-05273"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Karran</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Kreplin</surname><given-names>U.</given-names></name></person-group><article-title>The Drive to Explore: Physiological Computing in a Cultural Heritage Context</article-title><source>Advances in Physiological Computing</source><person-group person-group-type="editor"><name name-style="western"><surname>Fairclough</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gilleade</surname><given-names>K.</given-names></name></person-group><series>Human&#8211;Computer Interaction Series</series><publisher-name>Springer</publisher-name><publisher-loc>London, UK</publisher-loc><year>2014</year></element-citation></ref><ref id="B14-sensors-25-05273"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nacke</surname><given-names>L.</given-names></name></person-group><source>Introduction to Biometric Measures for Games User Research</source><publisher-name>Oxford University Press (OUP)</publisher-name><publisher-loc>Oxford, UK</publisher-loc><year>2018</year><pub-id pub-id-type="doi">10.1093/oso/9780198794844.003.0016</pub-id></element-citation></ref><ref id="B15-sensors-25-05273"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pergantis</surname><given-names>P.</given-names></name><name name-style="western"><surname>Bamicha</surname><given-names>V.</given-names></name><name name-style="western"><surname>Doulou</surname><given-names>A.</given-names></name><name name-style="western"><surname>Christou</surname><given-names>A.I.</given-names></name><name name-style="western"><surname>Bardis</surname><given-names>N.</given-names></name><name name-style="western"><surname>Skianis</surname><given-names>C.</given-names></name><name name-style="western"><surname>Drigas</surname><given-names>A.</given-names></name></person-group><article-title>Assistive and Emerging Technologies to Detect and Reduce Neurophysiological Stress and Anxiety in Children and Adolescents with Autism and Sensory Processing Disorders: A Systematic Review</article-title><source>Technologies</source><year>2025</year><volume>13</volume><elocation-id>144</elocation-id><pub-id pub-id-type="doi">10.3390/technologies13040144</pub-id></element-citation></ref><ref id="B16-sensors-25-05273"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>D.N.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.M.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name></person-group><article-title>The influence of neuroticism personality trait on user interaction with game-based hand rehabilitation training</article-title><source>Displays</source><year>2025</year><volume>87</volume><fpage>102944</fpage><pub-id pub-id-type="doi">10.1016/j.displa.2024.102944</pub-id></element-citation></ref><ref id="B17-sensors-25-05273"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Muntean</surname><given-names>R.</given-names></name><name name-style="western"><surname>Antle</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Matkin</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hennessy</surname><given-names>K.</given-names></name><name name-style="western"><surname>Rowley</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wilson</surname><given-names>J.</given-names></name></person-group><article-title>Designing cultural values into interaction</article-title><source>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</source><conf-loc>Denver, CO, USA</conf-loc><conf-date>6&#8211;11 May 2017</conf-date><fpage>6062</fpage><lpage>6074</lpage></element-citation></ref><ref id="B18-sensors-25-05273"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>T.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>X.</given-names></name></person-group><article-title>Restoring dunhuang murals: Crafting cultural heritage preservation knowledge into immersive virtual reality experience design</article-title><source>Int. J. Hum. Comput. Interact.</source><year>2024</year><volume>40</volume><fpage>2019</fpage><lpage>2040</lpage><pub-id pub-id-type="doi">10.1080/10447318.2023.2232976</pub-id></element-citation></ref><ref id="B19-sensors-25-05273"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Application of User Experience Interaction Design Method in R&amp;D Design of Cultural and Creative Products</article-title><source>Proceedings of the 2019 4th International Conference on Information Systems Engineering (ICISE)</source><conf-loc>Shanghai, China</conf-loc><conf-date>4&#8211;6 May 2019</conf-date><fpage>44</fpage><lpage>47</lpage></element-citation></ref><ref id="B20-sensors-25-05273"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>The impact of chinese traditional cultural on the gesture and user experience in mobile interaction design</article-title><source>Cross-Cultural Design, Proceedings of the 9th International Conference, CCD 2017, Held as Part of HCI International 2017, Vancouver, BC, Canada, 9&#8211;14 July 2017</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2017</year><fpage>49</fpage><lpage>58</lpage></element-citation></ref><ref id="B21-sensors-25-05273"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>She</surname><given-names>Y.Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.M.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.F.</given-names></name></person-group><article-title>Out of theater: Interactive Mixed-reality Performance for Intangible Culture Heritage Glove Puppetry</article-title><source>Proceedings of the Tenth International Symposium of Chinese CHI</source><conf-loc>Guangzhou, China</conf-loc><conf-date>22&#8211;23 October 2022</conf-date><fpage>181</fpage><lpage>189</lpage></element-citation></ref><ref id="B22-sensors-25-05273"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Rinaldi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Brischetto</surname><given-names>A.</given-names></name></person-group><article-title>Interaction Design for Experience and Inclusion in cultural heritage</article-title><source>Advances in Industrial Design, Proceedings of the AHFE 2021 Virtual Conferences on Design for Inclusion, Affective and Pleasurable Design, Interdisciplinary Practice in Industrial Design, Kansei Engineering, and Human Factors for Apparel and Textile Engineering, Orlando, FL, USA, 25&#8211;29 July 2021</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>397</fpage><lpage>406</lpage></element-citation></ref><ref id="B23-sensors-25-05273"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kirk</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bowen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chatting</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wright</surname><given-names>P.</given-names></name></person-group><article-title>Supporting the cross-cultural appreciation of traditional Chinese puppetry through a digital gesture library</article-title><source>J. Comput. Cult. Herit.</source><year>2019</year><volume>12</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1145/3341882</pub-id></element-citation></ref><ref id="B24-sensors-25-05273"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Manqi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hyun</surname><given-names>A.P.</given-names></name></person-group><article-title>A Study on Immersive Experience Characteristics Based on Embodied Cognition Focusing on Archaeological Site Museums in China in the Third Quarter of 2024</article-title><source>Des. Res.</source><year>2024</year><volume>9</volume><fpage>852</fpage><lpage>863</lpage></element-citation></ref><ref id="B25-sensors-25-05273"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cucchiara</surname><given-names>R.</given-names></name><name name-style="western"><surname>Del Bimbo</surname><given-names>A.</given-names></name></person-group><article-title>Visions for augmented cultural heritage experience</article-title><source>IEEE Multimed.</source><year>2014</year><volume>21</volume><fpage>74</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1109/MMUL.2014.19</pub-id></element-citation></ref><ref id="B26-sensors-25-05273"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>N.</given-names></name></person-group><article-title>The impact of emotional experience on tourists&#8217; cultural identity and behavior in the cultural heritage tourism context: An empirical study on Dunhuang Mogao Grottoes</article-title><source>Sustainability</source><year>2023</year><volume>15</volume><elocation-id>8823</elocation-id><pub-id pub-id-type="doi">10.3390/su15118823</pub-id></element-citation></ref><ref id="B27-sensors-25-05273"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Barrow</surname><given-names>A.P.</given-names></name></person-group><article-title>Assessing Emotional Evaluation: A Validation Study of the Reactions to Emotions Questionnaire</article-title><source>Master&#8217;s Thesis</source><publisher-name>The University of Texas at Austin</publisher-name><publisher-loc>Austin, TX, USA</publisher-loc><year>2007</year></element-citation></ref><ref id="B28-sensors-25-05273"><label>28.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>H.M.</given-names></name><name name-style="western"><surname>D&#237;az</surname><given-names>M.</given-names></name><name name-style="western"><surname>Catal&#224;</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Rauterberg</surname><given-names>M.</given-names></name></person-group><article-title>Mood boards as a universal tool for investigating emotional experience</article-title><source>Design, User Experience, and Usability. User Experience Design Practice, Proceedings of the Third International Conference, DUXU 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, 22&#8211;27 June 2014</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2014</year><fpage>220</fpage><lpage>231</lpage></element-citation></ref><ref id="B29-sensors-25-05273"><label>29.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Liapis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Karousos</surname><given-names>N.</given-names></name><name name-style="western"><surname>Katsanos</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xenos</surname><given-names>M.</given-names></name></person-group><article-title>Evaluating user&#8217;s emotional experience in HCI: The PhysiOBS approach</article-title><source>Human-Computer Interaction. Advanced Interaction Modalities and Techniques, Proceedings of the 16th International Conference, HCI International 2014, Heraklion, Crete, Greece, 22&#8211;27 June 2014</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2014</year><fpage>758</fpage><lpage>767</lpage></element-citation></ref><ref id="B30-sensors-25-05273"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kushki</surname><given-names>A.</given-names></name><name name-style="western"><surname>Drumm</surname><given-names>E.</given-names></name><name name-style="western"><surname>Mobarak</surname><given-names>M.P.</given-names></name><name name-style="western"><surname>Tanel</surname><given-names>N.</given-names></name><name name-style="western"><surname>Dupuis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chau</surname><given-names>T.</given-names></name><name name-style="western"><surname>Anagnostou</surname><given-names>E.</given-names></name></person-group><article-title>Investigating the Autonomic Nervous System Response to Anxiety in Children with Autism Spectrum Disorders</article-title><source>PLoS ONE</source><year>2013</year><volume>8</volume><elocation-id>e59730</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0059730</pub-id><pub-id pub-id-type="pmid">23577072</pub-id><pub-id pub-id-type="pmcid">PMC3618324</pub-id></element-citation></ref><ref id="B31-sensors-25-05273"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sparrow</surname><given-names>L.</given-names></name><name name-style="western"><surname>Six</surname><given-names>H.</given-names></name><name name-style="western"><surname>Varona</surname><given-names>L.</given-names></name><name name-style="western"><surname>Janin</surname><given-names>O.</given-names></name></person-group><article-title>Validation of affect-tag affective and cognitive indicators</article-title><source>Front. Neuroinform.</source><year>2021</year><volume>15</volume><elocation-id>535542</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2021.535542</pub-id><pub-id pub-id-type="pmid">34040510</pub-id><pub-id pub-id-type="pmcid">PMC8141551</pub-id></element-citation></ref><ref id="B32-sensors-25-05273"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lottridge</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chignell</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yasumura</surname><given-names>M.</given-names></name></person-group><article-title>Identifying Emotion through Implicit and Explicit Measures: Cultural Differences, Cognitive Load, and Immersion</article-title><source>IEEE Trans. Affect. Comput.</source><year>2011</year><volume>3</volume><fpage>199</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2011.36</pub-id></element-citation></ref><ref id="B33-sensors-25-05273"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Xavier</surname><given-names>R.A.C.</given-names></name><name name-style="western"><surname>de Almeida Neris</surname><given-names>V.P.</given-names></name></person-group><article-title>Measuring Users&#8217; Emotions with a Component-Based Approach</article-title><source>Enterprise Information Systems, Proceedings of the 14th International Conference, ICEIS 2012, Wroclaw, Poland, 28 June&#8211;1 July 2012</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2013</year><fpage>393</fpage><lpage>409</lpage></element-citation></ref><ref id="B34-sensors-25-05273"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Breaban</surname><given-names>A.</given-names></name><name name-style="western"><surname>Van de Kuilen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Noussair</surname><given-names>C.N.</given-names></name></person-group><article-title>Prudence, emotional state, personality, and cognitive ability</article-title><source>Front. Psychol.</source><year>2016</year><volume>7</volume><elocation-id>1688</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2016.01688</pub-id><pub-id pub-id-type="pmid">27840616</pub-id><pub-id pub-id-type="pmcid">PMC5083905</pub-id></element-citation></ref><ref id="B35-sensors-25-05273"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abriat</surname><given-names>A.</given-names></name><name name-style="western"><surname>Barkat</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bensafi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rouby</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fanchon</surname><given-names>C.</given-names></name></person-group><article-title>Psychological and physiological evaluation of emotional effects of a perfume in menopausal women</article-title><source>Int. J. Cosmet. Sci.</source><year>2007</year><volume>29</volume><fpage>399</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1111/j.1468-2494.2007.00398.x</pub-id><pub-id pub-id-type="pmid">18489374</pub-id></element-citation></ref><ref id="B36-sensors-25-05273"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name></person-group><article-title>Design Element Preferences in Public Facilities: An Eye Tracking Study</article-title><source>Land</source><year>2023</year><volume>12</volume><elocation-id>1411</elocation-id><pub-id pub-id-type="doi">10.3390/land12071411</pub-id></element-citation></ref><ref id="B37-sensors-25-05273"><label>37.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>C.L.</given-names></name></person-group><article-title>Design of a Virtual Quanzhou String Puppet Performance System Based on Gesture Interaction Technology</article-title><source>Master&#8217;s Thesis</source><publisher-name>Harbin Institute of Technology</publisher-name><publisher-loc>Harbin, China</publisher-loc><year>2014</year></element-citation></ref><ref id="B38-sensors-25-05273"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>F.F.</given-names></name></person-group><article-title>A Brief Discussion on the Artistic Features of Quanzhou String Puppetry</article-title><source>Yiyuan (Art Gard.)</source><year>2017</year><volume>3</volume><fpage>102</fpage><lpage>105</lpage></element-citation></ref><ref id="B39-sensors-25-05273"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shui</surname><given-names>W.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>M.</given-names></name><name name-style="western"><surname>Korkhov</surname><given-names>V.</given-names></name><name name-style="western"><surname>Gaspary</surname><given-names>L.P.</given-names></name></person-group><article-title>Research on Virtual Reality String Puppet Animation Based on Electromyographic Signals</article-title><source>J. Softw.</source><year>2019</year><volume>30</volume><fpage>2964</fpage><lpage>2985</lpage></element-citation></ref><ref id="B40-sensors-25-05273"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lim</surname><given-names>C.K.</given-names></name><name name-style="western"><surname>Halim</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>K.L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name></person-group><article-title>Digital Sustainability of Heritage: Exploring Indicators Affecting the Effectiveness of Digital Dissemination of Intangible Cultural Heritage Through Qualitative Interviews</article-title><source>Sustainability</source><year>2025</year><volume>17</volume><elocation-id>1593</elocation-id><pub-id pub-id-type="doi">10.3390/su17041593</pub-id></element-citation></ref><ref id="B41-sensors-25-05273"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Egusa</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sakai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Tamaki</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kusunoki</surname><given-names>F.</given-names></name><name name-style="western"><surname>Namatame</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mizoguchi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Inagaki</surname><given-names>S.</given-names></name></person-group><article-title>Designing a collaborative interaction experience for a puppet show system for hearing-impaired children</article-title><source>Proceedings of the International Conference on Computers Helping People with Special Needs</source><conf-loc>Linz, Austria</conf-loc><conf-date>13&#8211;15 July 2016</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2016</year><fpage>424</fpage><lpage>432</lpage></element-citation></ref><ref id="B42-sensors-25-05273"><label>42.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Z.</given-names></name></person-group><article-title>Research on Application of Gesture Recognition Technology in Traditional Puppet Show</article-title><source>International Conference of Design, User Experience, and Usability, Proceedings of the DUXU 2018, Las Vegas, NV, USA, 15&#8211;20 July 2018</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2018</year><fpage>498</fpage><lpage>512</lpage></element-citation></ref><ref id="B43-sensors-25-05273"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Antonijoan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Soler</surname><given-names>D.</given-names></name><name name-style="western"><surname>Miralles</surname><given-names>D.</given-names></name></person-group><article-title>Avatoys: Hibrid system with real and digital puppets</article-title><source>Proceedings of the 2014 9th Iberian Conference on Information Systems and Technologies (CISTI)</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>18&#8211;21 June 2014</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B44-sensors-25-05273"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.M.</given-names></name><name name-style="western"><surname>Tseng</surname><given-names>S.M.</given-names></name></person-group><article-title>Design and assessment of an interactive role-play system for learning and sustaining traditional glove puppetry by digital technology</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>5206</elocation-id><pub-id pub-id-type="doi">10.3390/app13085206</pub-id></element-citation></ref><ref id="B45-sensors-25-05273"><label>45.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chuan</surname><given-names>N.K.</given-names></name><name name-style="western"><surname>Sivaji</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>W.F.W.</given-names></name></person-group><article-title>Usability heuristics for heuristic evaluation of gestural interaction in HCI</article-title><source>Design, User Experience, and Usability: Design Discourse, Proceedings of the 4th International Conference, DUXU 2015, Held as Part of HCI International 2015, Los Angeles, CA, USA, 2&#8211;7 August 2015</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>138</fpage><lpage>148</lpage></element-citation></ref><ref id="B46-sensors-25-05273"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sellier</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Slu&#255;ters</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vanderdonckt</surname><given-names>J.</given-names></name><name name-style="western"><surname>Poncin</surname><given-names>I.</given-names></name></person-group><article-title>Evaluating gesture user interfaces: Quantitative measures, qualitative scales, and method</article-title><source>Int. J. Hum. Comput. Stud.</source><year>2024</year><volume>185</volume><fpage>103242</fpage><pub-id pub-id-type="doi">10.1016/j.ijhcs.2024.103242</pub-id></element-citation></ref><ref id="B47-sensors-25-05273"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simor</surname><given-names>F.W.</given-names></name><name name-style="western"><surname>Brum</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Schmidt</surname><given-names>J.D.E.</given-names></name><name name-style="western"><surname>Rieder</surname><given-names>R.</given-names></name><name name-style="western"><surname>De Marchi1</surname><given-names>A.C.B.</given-names></name></person-group><article-title>Usability evaluation methods for gesture-based games: A systematic review</article-title><source>JMIR Serious Games</source><year>2016</year><volume>4</volume><fpage>e5860</fpage><pub-id pub-id-type="doi">10.2196/games.5860</pub-id><pub-id pub-id-type="pmcid">PMC5069401</pub-id><pub-id pub-id-type="pmid">27702737</pub-id></element-citation></ref><ref id="B48-sensors-25-05273"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M.Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>F.M.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Z.S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Z.</given-names></name></person-group><article-title>Optical mapping of prefrontal brain connectivity and activation during emotion anticipation</article-title><source>Behav. Brain Res.</source><year>2018</year><volume>350</volume><fpage>122</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2018.04.051</pub-id><pub-id pub-id-type="pmid">29752969</pub-id></element-citation></ref><ref id="B49-sensors-25-05273"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>C.</given-names></name></person-group><article-title>NIRS-KIT: A MATLAB toolbox for both resting-state and task fNIRS data analysis</article-title><source>Neurophotonics</source><year>2021</year><volume>8</volume><fpage>010802</fpage><pub-id pub-id-type="doi">10.1117/1.NPh.8.1.010802</pub-id><pub-id pub-id-type="pmid">33506071</pub-id><pub-id pub-id-type="pmcid">PMC7829673</pub-id></element-citation></ref><ref id="B50-sensors-25-05273"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Okamoto</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jurcak</surname><given-names>V.</given-names></name><name name-style="western"><surname>Dan</surname><given-names>I.</given-names></name></person-group><article-title>Spatial registration of multichannel multi-subject fNIRS data to MNI space without MRI</article-title><source>Neuroimage</source><year>2005</year><volume>27</volume><fpage>842</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.05.019</pub-id><pub-id pub-id-type="pmid">15979346</pub-id></element-citation></ref><ref id="B51-sensors-25-05273"><label>51.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>C.</given-names></name></person-group><article-title>fNIRS Data Analysis</article-title><source>Functional Near-Infrared Spectroscopy</source><person-group person-group-type="editor"><name name-style="western"><surname>Zhu</surname><given-names>C.</given-names></name></person-group><publisher-name>Science Press</publisher-name><publisher-loc>Beijing, China</publisher-loc><year>2022</year><fpage>50</fpage><lpage>51</lpage></element-citation></ref><ref id="B52-sensors-25-05273"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ioannou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ebisch</surname><given-names>S.</given-names></name><name name-style="western"><surname>Aureli</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bafunno</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ioannides</surname><given-names>H.A.</given-names></name><name name-style="western"><surname>Cardone</surname><given-names>D.</given-names></name><name name-style="western"><surname>Manini</surname><given-names>B.</given-names></name><name name-style="western"><surname>Romani</surname><given-names>G.L.</given-names></name><name name-style="western"><surname>Gallese</surname><given-names>V.</given-names></name><name name-style="western"><surname>Merla</surname><given-names>A.</given-names></name><etal/></person-group><article-title>The Autonomic Signature of Guilt in Children: A Thermal Infrared Imaging Study</article-title><source>PLoS ONE</source><year>2013</year><volume>8</volume><elocation-id>e79440</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0079440</pub-id><pub-id pub-id-type="pmid">24260220</pub-id><pub-id pub-id-type="pmcid">PMC3834185</pub-id></element-citation></ref><ref id="B53-sensors-25-05273"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stani&#263;</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ger&#353;ak</surname><given-names>G.</given-names></name></person-group><article-title>Facial thermal imaging: A systematic review with guidelines and measurement uncertainty estimation</article-title><source>Measurement</source><year>2025</year><volume>242</volume><fpage>115879</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2024.115879</pub-id></element-citation></ref><ref id="B54-sensors-25-05273"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bicks</surname><given-names>L.K.</given-names></name><name name-style="western"><surname>Koike</surname><given-names>H.</given-names></name><name name-style="western"><surname>Akbarian</surname><given-names>S.</given-names></name><name name-style="western"><surname>Morishita</surname><given-names>H.</given-names></name></person-group><article-title>Prefrontal cortex and social cognition in mouse and man</article-title><source>Front. Psychol.</source><year>2015</year><volume>6</volume><elocation-id>1805</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01805</pub-id><pub-id pub-id-type="pmid">26635701</pub-id><pub-id pub-id-type="pmcid">PMC4659895</pub-id></element-citation></ref><ref id="B55-sensors-25-05273"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sakakibara</surname><given-names>E.</given-names></name><name name-style="western"><surname>Homae</surname><given-names>F.</given-names></name><name name-style="western"><surname>Kawasaki</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nishimura</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Takizawa</surname><given-names>R.</given-names></name><name name-style="western"><surname>Koike</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kinoshita</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sakurada</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yamagishi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nishimura</surname><given-names>F.</given-names></name><etal/></person-group><article-title>Detection of resting state functional connectivity using partial correlation analysis: A study using multi-distance and whole-head probe near-infrared spectroscopy</article-title><source>Neuroimage</source><year>2016</year><volume>142</volume><fpage>590</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.011</pub-id><pub-id pub-id-type="pmid">27521742</pub-id></element-citation></ref><ref id="B56-sensors-25-05273"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ge</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bu</surname><given-names>L.</given-names></name></person-group><article-title>The effects of two game interaction modes on cortical activation in subjects of different ages: A functional near-infrared spectroscopy study</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>11405</fpage><lpage>11415</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3050210</pub-id></element-citation></ref><ref id="B57-sensors-25-05273"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salazar-L&#243;pez</surname><given-names>E.</given-names></name><name name-style="western"><surname>Dom&#237;nguez</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>V.J.</given-names></name><name name-style="western"><surname>de la Fuente</surname><given-names>J.</given-names></name><name name-style="western"><surname>Meins</surname><given-names>A.</given-names></name><name name-style="western"><surname>Iborra</surname><given-names>O.</given-names></name><name name-style="western"><surname>G&#225;lvez</surname><given-names>G.</given-names></name><name name-style="western"><surname>Rodr&#237;guez-Artacho</surname><given-names>M.</given-names></name><name name-style="western"><surname>G&#243;mez-Mil&#225;n</surname><given-names>E.</given-names></name></person-group><article-title>The mental and subjective skin: Emotion, empathy, feelings and thermography</article-title><source>Conscious. Cogn.</source><year>2015</year><volume>34</volume><fpage>149</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2015.04.003</pub-id><pub-id pub-id-type="pmid">25955182</pub-id></element-citation></ref><ref id="B58-sensors-25-05273"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kosonogov</surname><given-names>V.</given-names></name><name name-style="western"><surname>De Zorzi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Honor&#233;</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mart&#237;nez-Vel&#225;zquez</surname><given-names>E.S.</given-names></name><name name-style="western"><surname>Nandrino</surname><given-names>J.-L.</given-names></name><name name-style="western"><surname>Martinez-Selva</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Sequeira</surname><given-names>H.</given-names></name><name name-style="western"><surname>Urgesi</surname><given-names>C.</given-names></name></person-group><article-title>Facial thermal variations: A new marker of emotional arousal</article-title><source>PLoS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0183592</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0183592</pub-id><pub-id pub-id-type="pmid">28922392</pub-id><pub-id pub-id-type="pmcid">PMC5603162</pub-id></element-citation></ref><ref id="B59-sensors-25-05273"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cruz-Albarran</surname><given-names>I.A.</given-names></name><name name-style="western"><surname>Benitez-Rangel</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Osornio-Rios</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Morales-Hernandez</surname><given-names>L.A.</given-names></name></person-group><article-title>Human emotions detection based on a smart-thermal system of thermographic images</article-title><source>Infrared Phys. Technol.</source><year>2017</year><volume>81</volume><fpage>250</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1016/j.infrared.2017.01.002</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05273-f001" orientation="portrait"><label>Figure 1</label><caption><p>Schematic diagram of interaction mode: (<bold>a</bold>) gesture interaction; (<bold>b</bold>) keyboard interaction; (<bold>c</bold>) no interaction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g001a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g001b.jpg"/></fig><fig position="float" id="sensors-25-05273-f002" orientation="portrait"><label>Figure 2</label><caption><p>Experimental scene and equipment used.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g002.jpg"/></fig><fig position="float" id="sensors-25-05273-f003" orientation="portrait"><label>Figure 3</label><caption><p>Experimental procedure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g003.jpg"/></fig><fig position="float" id="sensors-25-05273-f004" orientation="portrait"><label>Figure 4</label><caption><p>Schematic representations and sample waveforms of physiological signal types: (<bold>a</bold>) changes in cerebral oxygenated hemoglobin concentration; (<bold>b</bold>) temporal variation in skin temperature; (<bold>c</bold>) variations in heart rate and electrodermal activity signals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g004.jpg"/></fig><fig position="float" id="sensors-25-05273-f005" orientation="portrait"><label>Figure 5</label><caption><p>Schematic of the fNIRS cap layout design.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g005.jpg"/></fig><fig position="float" id="sensors-25-05273-f006" orientation="portrait"><label>Figure 6</label><caption><p>Facial ROIs for infrared thermal imaging.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g006.jpg"/></fig><fig position="float" id="sensors-25-05273-f007" orientation="portrait"><label>Figure 7</label><caption><p>(<bold>a</bold>) The brain activation state in gesture interaction; (<bold>b</bold>) the brain activation state in keyboard interaction; (<bold>c</bold>) the brain activation state in no interaction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g007.jpg"/></fig><fig position="float" id="sensors-25-05273-f008" orientation="portrait"><label>Figure 8</label><caption><p>ROIs.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g008.jpg"/></fig><fig position="float" id="sensors-25-05273-f009" orientation="portrait"><label>Figure 9</label><caption><p>(<bold>a</bold>) Correlation heatmap of &#946;-values between channels under the gesture-based interaction mode. (<bold>b</bold>) Correlation heatmap of &#946;-values between channels under the keyboard interaction mode. (<bold>c</bold>) Correlation heatmap of &#946;-values between channels under the non-interactive mode.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g009.jpg"/></fig><fig position="float" id="sensors-25-05273-f010" orientation="portrait"><label>Figure 10</label><caption><p>Mean values of EDA indicators under three interaction modes.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g010.jpg"/></fig><fig position="float" id="sensors-25-05273-f011" orientation="portrait"><label>Figure 11</label><caption><p>Temperature changes in facial ROIs during (<bold>a</bold>) gesture interaction; (<bold>b</bold>) keyboard interaction; (<bold>c</bold>) no interaction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05273-g011.jpg"/></fig><table-wrap position="float" id="sensors-25-05273-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t001_Table 1</object-id><label>Table 1</label><caption><p>Demographic profiles of participants.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Category</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Question</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Result</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Demographics</td><td align="center" valign="middle" rowspan="1" colspan="1">Gender</td><td align="center" valign="middle" rowspan="1" colspan="1">Male (25), Female (36)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Age</td><td align="center" valign="middle" rowspan="1" colspan="1">Under 18 (5), 18&#8211;25 (52), Over 25 (4)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Education Degree</td><td align="center" valign="middle" rowspan="1" colspan="1">Undergraduate (25), Master (33), Doctor (3)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Major</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Industrial Design (19); Art (7); Electronic Information Technology (5); Materials and Chemicals (5); Other Majors (25)</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Cultural Knowledge</td><td align="center" valign="middle" rowspan="1" colspan="1">Cultural Familiarity</td><td align="center" valign="middle" rowspan="1" colspan="1">M = 2.18; SD = 0.74</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cultural Interest</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M = 3.29; SD = 0.76</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Gaming Experience</td><td align="center" valign="middle" rowspan="1" colspan="1">Weekly Gaming Time</td><td align="center" valign="middle" rowspan="1" colspan="1">M = 11.37 h; SD = 10.46</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PC/Console Game Experience</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes (75.41%), No (24.59%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interactive Device Type Usage</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mouse (96.72%), Keyboard (96.72%), Joystick (68.85%), Motion Sensor (18.03%), VR (29.51%)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05273-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t002_Table 2</object-id><label>Table 2</label><caption><p>Overview of experts.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Expert</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Institute</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Role</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Years of Expert Experience</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Expert A</td><td align="center" valign="middle" rowspan="1" colspan="1">Quanzhou String Puppet Troupe</td><td align="center" valign="middle" rowspan="1" colspan="1">Former troupe leader and performer</td><td align="center" valign="middle" rowspan="1" colspan="1">45</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Expert B</td><td align="center" valign="middle" rowspan="1" colspan="1">Beijing Puppet Troupe</td><td align="center" valign="middle" rowspan="1" colspan="1">Veteran performer</td><td align="center" valign="middle" rowspan="1" colspan="1">18</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Expert C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">China Artists Association</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cultural Heritage Research Specialist</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05273-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t003_Table 3</object-id><label>Table 3</label><caption><p>The structure of the interviews.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Themes</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Questions</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" colspan="1">Background Information</td><td align="center" valign="middle" rowspan="1" colspan="1">What is your present role or status related to Puppetry culture?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">How many years have you been involved in puppet cultural heritage?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Puppetry Gestures</td><td align="center" valign="middle" rowspan="1" colspan="1">What do you think about the alignment of gestures with traditional cultural characteristics?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Overall Narrative</td><td align="center" valign="middle" rowspan="1" colspan="1">What do you think about the faithfulness of the narrative expression?</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Digital Form</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">What do you think about the appropriateness of redesigning puppetry<break/>for digital contexts in this form?</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05273-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t004_Table 4</object-id><label>Table 4</label><caption><p>Usability evaluation scale.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Stage</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dimension</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item/Scale</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Cronbach&#8217;s &#945;</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Pre-use</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Learnability</td><td align="left" valign="middle" rowspan="1" colspan="1">L1: The design of this interaction made it easy for me to understand. (positive)</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.746</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">L2: The operation of this interaction was simple, straightforward, and easy to grasp. (positive)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L3: My understanding of this interaction improved over time. (positive)</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:inset thin" colspan="1">During use</td><td rowspan="6" align="center" valign="middle" style="border-bottom:inset thin" colspan="1">Cognitive Load</td><td align="left" valign="middle" rowspan="1" colspan="1">C1: Mental demand&#8211;How much mental activity (e.g., thinking, deciding) was required to accomplish the task?</td><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.750</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">C2: Physical demand&#8211;How much physical effort was required to accomplish the task?</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">C3: Temporal demand&#8211;How much time pressure did you feel while accomplishing the task?</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">C4: Performance&#8211;How successful do you think you were in accomplishing the task? (positive)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">C5: Effort&#8211;How hard did you have to work to accomplish the task?</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C6: Frustration&#8211;How insecure, discouraged, irritated, or stressed did you feel during the task?</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Post-use</td><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Fatigue</td><td align="left" valign="middle" rowspan="1" colspan="1">F1: Overall fatigue&#8211;The overall level of fatigue you experienced.</td><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.814</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">F2: Physical fatigue&#8211;The level of physical tiredness you experienced.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">F3: Mental fatigue&#8211;The level of mental tiredness you experienced.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">F4: Energy level&#8211;The extent to which you felt a lack of energy.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F5: Motivation loss&#8211;The extent to which you felt a decrease in motivation for daily activities.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05273-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t005_Table 5</object-id><label>Table 5</label><caption><p>Descriptive statistics and test results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dimension</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">p</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Z</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Learnability</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>L1 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;4.992</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>L2 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;3.370</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>L3 *</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;5.649</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Cognitive Load</td><td align="center" valign="middle" rowspan="1" colspan="1">C1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.919</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.135</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>C2 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;2.557</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.297</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;1.032</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>C4 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;4.717</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>C5 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;3.364</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>C6 *</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;3.521</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Fatigue</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>F1 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;4.543</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>F2 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;4.613</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>F3 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;4.321</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>F4 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;6.511</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>F5 *</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;7.228</td></tr></tbody></table><table-wrap-foot><fn><p>* The significant difference that is also supported by the medium or large effect size is in boldface and marked with *.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05273-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t006_Table 6</object-id><label>Table 6</label><caption><p>User experience subjective questionnaire.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Evaluation Dimension</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Satisfaction Interest Closeness Indicator</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">User satisfaction and the fun of the experience</td><td align="center" valign="middle" rowspan="1" colspan="1">Satisfaction</td><td align="center" valign="middle" rowspan="1" colspan="1">I am satisfied with this interactive puppet digital game</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Interest</td><td align="center" valign="middle" rowspan="1" colspan="1">I think this interactive experience is very interesting</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Closeness</td><td align="center" valign="middle" rowspan="1" colspan="1">The interactive design makes it easy to understand and full of storytelling</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Novelty</td><td align="center" valign="middle" rowspan="1" colspan="1">This interactive method makes me feel novel and unique, as if I were in a real puppet scene</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Personalization</td><td align="center" valign="middle" rowspan="1" colspan="1">I can personalize this interaction to suit my personal preferences</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fulfillment</td><td align="center" valign="middle" rowspan="1" colspan="1">Through this interactive method, I learned new intangible cultural heritage knowledge and felt the rich cultural atmosphere</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Exploration</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This interactive method demonstrates the intangible cultural heritage skills through gameplay, which makes me want to explore</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Cultural identity and cultural understanding</td><td align="center" valign="middle" rowspan="1" colspan="1">Sense of scene</td><td align="center" valign="middle" rowspan="1" colspan="1">This interactive method allows me to experience the rich puppet scene</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Identification</td><td align="center" valign="middle" rowspan="1" colspan="1">I feel more connected to puppet culture when using this interaction</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sense of value</td><td align="center" valign="middle" rowspan="1" colspan="1">I think this interactive method makes me feel the value of puppet culture</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Intangible cultural heritage content</td><td align="center" valign="middle" rowspan="1" colspan="1">Through this interactive method, I learned about the rich and in-depth culture of puppets</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Intangible cultural heritage</td><td align="center" valign="middle" rowspan="1" colspan="1">This interactive method combines modern technology with traditional puppet culture</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Integration sense of involvement</td><td align="center" valign="middle" rowspan="1" colspan="1">This interactive method gives me the feeling of actually operating a puppet</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cultural fit</td><td align="center" valign="middle" rowspan="1" colspan="1">This interactive method is very consistent with the cultural characteristics of puppets</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cultural understanding</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This interactive method has improved my understanding of puppet culture</td></tr><tr><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Emotional involvement and immersion level</td><td align="center" valign="middle" rowspan="1" colspan="1">Emotional resonance</td><td align="center" valign="middle" rowspan="1" colspan="1">I was able to deeply feel the cultural emotions behind the puppets when experiencing this interactive method</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Memorability</td><td align="center" valign="middle" rowspan="1" colspan="1">After the interactive experience ends, I can still retain the memory of the experience</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Immersion</td><td align="center" valign="middle" rowspan="1" colspan="1">When experiencing this interactive method, I felt completely immersed in the world of the puppets</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Flow experience</td><td align="center" valign="middle" rowspan="1" colspan="1">The interactive method is very attractive, and the game level is well-designed, making people linger</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Storytelling</td><td align="center" valign="middle" rowspan="1" colspan="1">This interactive method allows me to immerse myself in the story scenes of the puppet show</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learnability</td><td align="center" valign="middle" rowspan="1" colspan="1">The interactive mode is simple, straightforward, and easy to understand</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Usefulness</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This interaction method can improve my cultural literacy</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05273-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of user&#8217;s subjective feelings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Evaluation Dimensions</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Significant Pairings</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Difference in the Average Values</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Significance</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">User satisfaction and experience enjoyment</td><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. Keyboard</td><td align="center" valign="middle" rowspan="1" colspan="1">4.723 *</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">9.390 *</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Keyboard vs. No Interaction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.667 *</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Cultural identity and cultural understanding</td><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. Keyboard</td><td align="center" valign="middle" rowspan="1" colspan="1">7.897 *</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">13.605 *</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Keyboard vs. No Interaction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.708 *</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Emotional engagement and immersion</td><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. Keyboard</td><td align="center" valign="middle" rowspan="1" colspan="1">3.675 *</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">8.698 *</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Keyboard vs. No Interaction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.023 *</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td></tr></tbody></table><table-wrap-foot><fn><p>* The significant difference that is also supported by the medium or large effect size is marked with *.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05273-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t008_Table 8</object-id><label>Table 8</label><caption><p>Comparison of changes in &#946;-values.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">ROI</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">OFC</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">VLPFC</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DLPFC</th></tr></thead><tbody><tr><td colspan="2" align="center" valign="middle" rowspan="1">Gesture Interaction (&#215;10<sup>&#8722;8</sup>)</td><td align="center" valign="middle" rowspan="1" colspan="1">3.391</td><td align="center" valign="middle" rowspan="1" colspan="1">3.819</td><td align="center" valign="middle" rowspan="1" colspan="1">4.736</td></tr><tr><td colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Keyboard Interaction (&#215;10<sup>&#8722;8</sup>)</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">&#8722;0.610</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">2.387</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">2.317</td></tr><tr><td colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">No Interaction (&#215;10<sup>&#8722;8</sup>)</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">2.912</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">0.633</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">0.804</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Gesture Interaction vs. Keyboard Interaction</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Difference (&#215;10<sup>&#8722;8</sup>)</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">4.001</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">1.433</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">2.419</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">z</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;2.594</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;1.887</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;2.056</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">p</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0</bold>
<bold>10 *</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.059</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.040</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gesture Interaction vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">Difference (&#215;10<sup>&#8722;8</sup>)</td><td align="center" valign="middle" rowspan="1" colspan="1">6.303</td><td align="center" valign="middle" rowspan="1" colspan="1">3.187</td><td align="center" valign="middle" rowspan="1" colspan="1">5.540</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Keyboard Interaction vs. No </td><td align="center" valign="middle" rowspan="1" colspan="1">z</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;2.755</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;2.576</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;2.786</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">p</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.00</bold>
<bold>6 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.010</bold>
<bold>*</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.005</bold>
<bold>*</bold>
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Keyboard Interaction vs. No Interaction</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Difference (&#215;10<sup>&#8722;8</sup>)</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">2.302</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">1.754</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">3.120</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">z</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.253</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.530</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.463</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">p</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.800</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.596</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.643</td></tr></tbody></table><table-wrap-foot><fn><p>* The significant difference that is also supported by the medium or large effect size is in boldface and marked with *.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05273-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t009_Table 9</object-id><label>Table 9</label><caption><p>Non-parametric test results of skin electrical indicators under three interaction modes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Indicators</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Significant Pairings</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Significance</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SC</td><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. Keyboard</td><td align="center" valign="middle" rowspan="1" colspan="1">0.329</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Keyboard vs. No Interaction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td rowspan="3" align="center" valign="middle" colspan="1">Tonic</td><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. Keyboard</td><td align="center" valign="middle" rowspan="1" colspan="1">0.718</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Keyboard vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Phasic</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Gesture vs. Keyboard</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">0.073</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gesture vs. No Interaction</td><td align="center" valign="middle" rowspan="1" colspan="1">&lt;0.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Keyboard vs. No Interaction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.005</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05273-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t010_Table 10</object-id><label>Table 10</label><caption><p>Post hoc test.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ROIs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Significance</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Nose</td><td align="center" valign="middle" rowspan="1" colspan="1">9.96</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.000 *</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Right cheek</td><td align="center" valign="middle" rowspan="1" colspan="1">3.43</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.036 *</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Left cheek</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.001 *</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>* The significant difference that is also supported by the medium or large effect size is in boldface and marked with *.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05273-t011" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05273-t011_Table 11</object-id><label>Table 11</label><caption><p>Mean value of ROI temperature differences.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ROIs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Gesture Interaction</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Keyboard Interaction</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">No Interaction</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mean value of nose temperature difference/&#176;C</td><td align="center" valign="middle" rowspan="1" colspan="1">0.42</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.23</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mean value of right cheek temperature difference/&#176;C</td><td align="center" valign="middle" rowspan="1" colspan="1">0.19</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;0.04</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean value of left cheek temperature difference/&#176;C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;0.01</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>