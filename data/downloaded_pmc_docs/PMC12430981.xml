<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430981</article-id><article-id pub-id-type="pmcid-ver">PMC12430981.1</article-id><article-id pub-id-type="pmcaid">12430981</article-id><article-id pub-id-type="pmcaiid">12430981</article-id><article-id pub-id-type="doi">10.3390/s25175582</article-id><article-id pub-id-type="publisher-id">sensors-25-05582</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MS-UNet: A Hybrid Network with a Multi-Scale Vision Transformer and Attention Learning Confusion Regions for Soybean Rust Fungus</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7171-5239</contrib-id><name name-style="western"><surname>Liu</surname><given-names initials="T">Tian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05582" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Sun</surname><given-names initials="L">Liangzheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05582" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="Q">Qiulong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05582" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zou</surname><given-names initials="Q">Qingquan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-05582" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0005-5445</contrib-id><name name-style="western"><surname>Su</surname><given-names initials="P">Peng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05582" ref-type="aff">1</xref><xref rid="c1-sensors-25-05582" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Xie</surname><given-names initials="P">Pengwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af3-sensors-25-05582" ref-type="aff">3</xref><xref rid="c1-sensors-25-05582" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Cornelis</surname><given-names initials="J">Jan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05582"><label>1</label>School of Electromechanical Engineering, Beijing Information Science and Technology University, Beijing 100192, China; <email>liutian@bistu.edu.cn</email> (T.L.); <email>20192260@bistu.edu.cn</email> (Q.W.); <email>2023020109@bistu.edu.cn</email> (Q.Z.)</aff><aff id="af2-sensors-25-05582"><label>2</label>School of Optoelectronics, Beijing Information Science and Technology University, Beijing 100192, China; <email>2023030031@bistu.edu.cn</email></aff><aff id="af3-sensors-25-05582"><label>3</label>School of Artificial Intelligence, Beijing Normal University, Beijing 100875, China</aff><author-notes><corresp id="c1-sensors-25-05582"><label>*</label>Correspondence: <email>supeng@bistu.edu.cn</email> (P.S.); <email>pwxie@mail.bnu.edu.cn</email> (P.X.)</corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5582</elocation-id><history><date date-type="received"><day>09</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>30</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>05</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>07</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05582.pdf"/><abstract><p>Soybean rust, caused by the fungus <italic toggle="yes">Phakopsora pachyrhizi</italic>, is recognized as the most devastating disease affecting soybean crops worldwide. In practical applications, performing accurate <italic toggle="yes">Phakopsora pachyrhizi</italic> segmentation (PPS) is essential for elucidating the morphodynamics of soybean rust, thereby facilitating effective prevention strategies and advancing research on related soybean diseases. Despite its importance, studies focusing on PPS-related datasets and the automatic segmentation of <italic toggle="yes">Phakopsora pachyrhizi</italic> remain limited. To address this gap, we propose an efficient semantic segmentation model named MS-UNet (Multi-Scale Confusion UNet Network). In the hierarchical Vision Transformer (ViT) module, the feature maps are down-sampled to reduce the lengths of the keys (K) and values (V), thereby minimizing the computational complexity. This design not only lowers the resource demands of the transformer but also enables the network to effectively capture multi-scale and high-resolution features. Additionally, depthwise separable convolutions are employed to compensate for positional information, which alleviates the difficulty the ViT faces in learning robust positional encodings, especially for small datasets. Furthermore, MS-UNet dynamically generates labels for both hard-to-segment and easy-to-segment regions, compelling the network to concentrate on more challenging locations and improving its overall segmentation capability. Compared to the existing state-of-the-art methods, our approach achieves a superior performance in PPS tasks.</p></abstract><kwd-group><kwd>deep learning</kwd><kwd>image processing</kwd><kwd><italic toggle="yes">Phakopsora pachyrhizi</italic></kwd><kwd>U-Net</kwd><kwd>feature extraction</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05582"><title>1. Introduction</title><p>Asian soybean rust (ASR) holds great significance in the prevention of and research into related soybean diseases [<xref rid="B1-sensors-25-05582" ref-type="bibr">1</xref>]. Since the 20th century, ASR has inflicted substantial losses on global soybean production, causing annual losses ranging from 10% to 30% and resulting in tens of billions of dollars of economic losses [<xref rid="B2-sensors-25-05582" ref-type="bibr">2</xref>]. In soybean disease prevention and research, PPS is an essential step in understanding the morphodynamics of ASR and providing a comprehensive analysis of soybean growth conditions [<xref rid="B3-sensors-25-05582" ref-type="bibr">3</xref>]. However, manual segmentation of <italic toggle="yes">Phakopsora pachyrhizi</italic> is laborious and inefficient and necessitates prior knowledge training for labelers in relevant fields. Given the scarcity of research on PPS-related datasets or automatic PPS, automated PPS holds significant practical application potential.</p><p>Numerous methods have been proposed for semantic segmentation of images, broadly categorized into traditional and deep-learning-based methods. Traditional methods typically employ classical image processing approaches that extract primary features using an object&#8217;s color, shape, or spatial position relationships. These methods generally offer faster inference speeds but are heavily dependent on manually designed features, exhibit a poor generalization performance for objects with varying characteristics, and are prone to pixel misclassification and sensitivity to variations in illumination in images. Recently, deep-learning-based methods have gained prominence as a promising solution to image semantic segmentation problems due to their robust nonlinear expression capabilities. For instance, the fully convolutional network (FCN) [<xref rid="B4-sensors-25-05582" ref-type="bibr">4</xref>]-based method remains a popular framework for its ease of implementation and exceptional feature extraction capabilities. Additionally, a ViT adapts pure transformers to image classification tasks, opening the door to self-attention applications in other computer vision tasks. Numerous follow-up models have been developed, yielding an improved performance on many natural image datasets, such as PASCAL VOC and Cityscapes.</p><p>Nonetheless, applying deep-learning-based methods to ASR is not straightforward and faces challenges, such as blurred target boundaries and a heavy network architecture. Although state-of-the-art methods can be used to complete ASR, images of <italic toggle="yes">Phakopsora pachyrhizi</italic> differ from natural images and are observed through high-throughput microscopy. Conventional semantic segmentation networks typically employ an encoder&#8211;decoder structure combined with max-pooling and upsampling operations, which inevitably results in the loss of spatial information and a suboptimal performance in dense prediction tasks. As images of <italic toggle="yes">Phakopsora pachyrhizi</italic> lack the sharp borders characteristic of natural images, pooling layers further contribute to edge information loss and inaccurate segmentation results. Researchers often address this issue by maintaining high-resolution feature maps [<xref rid="B5-sensors-25-05582" ref-type="bibr">5</xref>] or reducing the number of pooling layers during the encoding process [<xref rid="B6-sensors-25-05582" ref-type="bibr">6</xref>]. However, these methods significantly increase the computational and parameter requirements, hindering the practical deployment of the model. Furthermore, although ViTs [<xref rid="B7-sensors-25-05582" ref-type="bibr">7</xref>] have demonstrated a strong performance in computer vision, the self-attention mechanism&#8217;s <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time and space complexity for sequence length results in substantial training and inference overheads. Natural images are typically trained and inferred using ordinary-sized images (e.g., <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), which is computationally acceptable. In contrast, high-throughput microscopy images of <italic toggle="yes">Phakopsora pachyrhizi</italic> are generally small, sparse, and large. The <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time and space complexity further diminishes the advantages of deep-learning-based methods over image-based methods. A more suitable approach should involve an efficient model designed for <italic toggle="yes">Phakopsora pachyrhizi</italic>&#8217;s features, rather than the direct adoption of state-of-the-art ViT models.</p><p>To address these limitations, we propose a novel encoder&#8211;decoder-based model called the MS-UNet network, which takes into account the efficiency, accuracy, and robustness of PPS. The architecture of the proposed network is based on a hybrid transformer structure and is trained end to end, comprising the following designs: (1) We introduce a progressive hierarchical ViT encoder to make our network flexible for learning multi-scale and high-resolution features, reducing the resource consumption of the transformer and emulating the convolutions&#8217; inductive bias as the network deepens. Specifically, our network has a hierarchical structure like a CNN, and we decrease the sequence length of the keys and values when computing the patch interactions for self-attention, significantly reducing the overhead with minimal impact on performance. Additionally, since positional encodings trained on small datasets are often insufficiently robust, we employ depthwise separable convolution to incorporate position information after each self-attention block. (2) To mitigate the loss of spatial information caused by pooling layers, we propose a multi-branch decoder that uses the network output to identify low-confidence pixels and forces additional branches to learn hard-to-segment locations. The proposed network achieves a competitive performance on PPS tasks, and our contributions are summarized as follows:<list list-type="bullet"><list-item><p>We design a lightweight progressive hierarchical ViT encoder capable of learning multi-scale and high-resolution features;</p></list-item><list-item><p>We develop a multi-branch decoder that leverages low-confidence pixels in the output to compel the network to focus on challenging regions that are difficult to segment;</p></list-item><list-item><p>The proposed MS-UNet is tailored to the unique characteristics of soybean rust imagery and achieves satisfactory results on the PPS task.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05582"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05582"><title>2.1. Semantic Segmentation</title><p>Semantic segmentation extends image classification from the image level to the pixel level [<xref rid="B8-sensors-25-05582" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05582" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05582" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05582" ref-type="bibr">11</xref>]. In early deep learning applications, semantic segmentation was regarded as a pixel-wise classification task, unable to accomplish structured predictions [<xref rid="B12-sensors-25-05582" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05582" ref-type="bibr">13</xref>]. Subsequently, Long et al. [<xref rid="B4-sensors-25-05582" ref-type="bibr">4</xref>] proposed an FCN to address the structured prediction problem by integrating encoder&#8211;decoder architectures, becoming the predominant approach to semantic segmentation. Researchers then focused on enhancing the FCN from various perspectives [<xref rid="B5-sensors-25-05582" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05582" ref-type="bibr">6</xref>,<xref rid="B14-sensors-25-05582" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05582" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05582" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05582" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05582" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05582" ref-type="bibr">19</xref>]. To alleviate the spatial information loss caused by pooling layers, researchers typically maintained high-resolution feature maps [<xref rid="B5-sensors-25-05582" ref-type="bibr">5</xref>,<xref rid="B16-sensors-25-05582" ref-type="bibr">16</xref>] or reduced the number of pooling layers during the encoding process [<xref rid="B6-sensors-25-05582" ref-type="bibr">6</xref>,<xref rid="B18-sensors-25-05582" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05582" ref-type="bibr">19</xref>]. Chen et al. [<xref rid="B6-sensors-25-05582" ref-type="bibr">6</xref>] proposed a method employing atrous convolution (dilated convolution) to expand the receptive field while avoiding down-sampling operations. Deeplabv3+ [<xref rid="B19-sensors-25-05582" ref-type="bibr">19</xref>] introduced a simple and effective encoder&#8211;decoder FCN architecture by combining atrous spatial pyramid pooling. Sun et al. [<xref rid="B16-sensors-25-05582" ref-type="bibr">16</xref>] consistently maintained a high-resolution branch to prevent spatial information loss and obtained various receptive fields through dense connections between different scales. However, these methods considerably increased the computational load and the number of parameters, which is unfavorable for actual model deployment. In contrast, our approach adds only two additional decoder branches, providing a clear advantage in terms of the computational overhead.</p></sec><sec id="sec2dot2-sensors-25-05582"><title>2.2. Vision Transformers</title><p>Transformers were initially proposed by [<xref rid="B7-sensors-25-05582" ref-type="bibr">7</xref>] for translation tasks and quickly gained popularity across various NLP tasks. These models rely on self-attention mechanisms to capture long-range dependencies among tokens (words) in a sentence. The success of transformers in NLP has inspired several methods for computer vision tasks [<xref rid="B20-sensors-25-05582" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05582" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05582" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05582" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05582" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05582" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05582" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05582" ref-type="bibr">27</xref>]. Ref. [<xref rid="B20-sensors-25-05582" ref-type="bibr">20</xref>] demonstrated that self-attention is an instantiation of non-local means and used it to achieve gains in video classification and object detection. Ref. [<xref rid="B21-sensors-25-05582" ref-type="bibr">21</xref>] developed a simple local self-attention layer suitable for both small and large inputs, outperforming the convolutional baseline for both image classification and object detection. Ref. [<xref rid="B22-sensors-25-05582" ref-type="bibr">22</xref>] proposed a generic formulation for capturing long-range feature interdependencies via universal gathering and distribution functions. Ref. [<xref rid="B24-sensors-25-05582" ref-type="bibr">24</xref>] proposed FBoT-Net, a focal bottleneck transformer network that effectively detects small green apples in complex orchard environments. As proposed in [<xref rid="B25-sensors-25-05582" ref-type="bibr">25</xref>], MobileViTFace is a lightweight sheep face recognition model that combines convolutional and transformer structures, specifically using the cutting-edge ViT approach. It achieved a 97.13% recognition accuracy on 7434 sheep face images, outperforming lightweight convolutional models, while reducing the parameters and FLOPs compared to these values for ResNet-50 and providing real-time recognition results on edge devices like the Jetson Nano.</p><p>Although ViTs have achieved an exceptional performance in computer vision, the self-attention mechanism has an <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time and space complexity for sequence length, resulting in significant overheads during training and inference. To address this, we optimize the transformer for the characteristics of images of <italic toggle="yes">Phakopsora pachyrhizi</italic> and overcome numerous challenges when adapting self-attention to PPS tasks.</p></sec></sec><sec id="sec3-sensors-25-05582"><title>3. The Method</title><p>The U-Net network has demonstrated its ability to use a limited amount of data for model training in order to achieve pixel-level image prediction. Furthermore, the U-Net network has been shown to achieve a high level of segmentation performance in image processing [<xref rid="B28-sensors-25-05582" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05582" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05582" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05582" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05582" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05582" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05582" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05582" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05582" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05582" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05582" ref-type="bibr">38</xref>]. In this work, the U-Net network serves as the foundation for constructing a segmentation network tailored to PPS tasks.</p><p>We designed a hybrid network structure, as depicted in <xref rid="sensors-25-05582-f001" ref-type="fig">Figure 1</xref>, and named it MS-UNet to incorporate the self-attention mechanism into PPS tasks. The segmentation network employs a U-Net architecture, where a transformer is utilized for the encoder part and three convolution-based decoders are employed for the decoder part. <xref rid="sensors-25-05582-f001" ref-type="fig">Figure 1</xref> illustrates the network structure of a single decoder.</p><p>This study presents the development of a multi-branch segmentation network for high-precision segmentation of images of <italic toggle="yes">Phakopsora pachyrhizi</italic>. The objective of the network is to accurately segment regions in images that include both difficult and easy-to-segment areas. The network comprises a shared encoder and two distinct segmentation network branches. When an image is input into the multi-branch segmentation network, the encoder first extracts its features. These features correspond to both difficult and easy segmentation regions. The extracted features are then passed to three independent decoders, each tailored to these image characteristics. During the training of the complete multi-branch segmentation network, labeled <italic toggle="yes">Phakopsora pachyrhizi</italic> images are used as training data. Consequently, the most crucial aspect of the multi-branch segmentation network lies in devising suitable encoders and decoders for the network.</p><sec id="sec3dot1-sensors-25-05582"><title>3.1. The Progressive Hierarchical VIT Encoder</title><p>The Pyramid Vision Transformer (PVT) [<xref rid="B39-sensors-25-05582" ref-type="bibr">39</xref>] introduces a pyramid that gradually shrinks and an attention layer that reduces the spatial dimensions to obtain high-resolution and multi-scale feature maps while minimizing the consumption of memory and computing resources. Based on the PVT, we develop a new encoder architecture, which can be seen in <xref rid="sensors-25-05582-f002" ref-type="fig">Figure 2</xref>.</p><p><xref rid="sensors-25-05582-f002" ref-type="fig">Figure 2</xref> depicts the structure of the encoder. A patch embedding layer and transformer encoder layers make up the architecture shared by all tiers. In the PVT structure, we add depth-separable convolution instead of the original position encoding. This approach has been demonstrated in [<xref rid="B40-sensors-25-05582" ref-type="bibr">40</xref>].</p><p>Given an input image <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in size, we first partition it into <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> patches <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in size. The flattened patches are then passed through a linear projection to produce embedded patches with a size of <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Following this, a transformer encoder with <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> layers is employed to encode the embedded patches, and the output is reshaped into a feature map <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with the dimensions <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Similarly, by using the feature map from the previous step as the input, we obtain the subsequent feature maps <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which have strides of 8, 16, and 32 pixels relative to the input image, respectively. The feature pyramid of <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> enables us to learn multi-scale and high-resolution features.</p><p>The PVT reduces the memory overhead through a specialized reduction, which is based on the principle of decreasing the sequence length of K and V. We achieve this using a novel approach. The multi-head self-attention (MHSA) module [<xref rid="B7-sensors-25-05582" ref-type="bibr">7</xref>], on which the transformer is based, allows the model to jointly infer attention from multiple representation subspaces. The concatenated results from different heads are then transformed using a feed-forward network. In this study, we use eight heads, and for the sake of clarity in the subsequent formulation and illustration, the multi-head dimensions are omitted here for simplicity. Consider the input feature map <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">H</italic>, <italic toggle="yes">W</italic>, and <italic toggle="yes">C</italic> represent the height, width, and number of channels of the feature map, respectively.</p><p>To project <italic toggle="yes">X</italic> onto the query, key, and value embeddings, <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">V</italic>
<inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, we employ three <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions, where <italic toggle="yes">d</italic> is the dimension of each head&#8217;s embedding. After being flattened, <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">V</italic> are transposed into sequences of size <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Self-attention is computed through a scaled dot-product, as shown in Equation (<xref rid="FD1-sensors-25-05582" ref-type="disp-formula">1</xref>).<disp-formula id="FD1-sensors-25-05582"><label>(1)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow><mml:mo>&#65080;</mml:mo></mml:munder><mml:mi>U</mml:mi></mml:munder><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><italic toggle="yes">U</italic> can also be referred to as the similarity matrix or the context aggregating matrix.</p><p>More precisely, <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The context aggregating matrix for the <italic toggle="yes">i</italic>-th query computes the normalized pairwise dot-product between <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and each element in the keys. The weights used to extract context information from the data are then derived from the context aggregating matrix. In this way, self-attention is effective at capturing long-range dependencies and inherently possesses a global receptive field. Additionally, the context aggregating matrix is sensitive to input data for enhanced feature aggregation. However, the dot-product of <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> matrices results in <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> complexity. When the resolution of a feature map is high, <italic toggle="yes">n</italic> is often significantly larger than <italic toggle="yes">d</italic>; consequently, the sequence length dominates the self-attention computation, making self-attention infeasible for high-resolution feature maps. For instance, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> 50,176 for <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> feature maps.</p><p>We reduce the computational effort by shortening the length of the <italic toggle="yes">K</italic> and <italic toggle="yes">V</italic> sequences. The main idea is to project the key and value using two projections, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, into low-dimensional embeddings <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mi>w</mml:mi><mml:mo>&#8810;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">h</italic> and <italic toggle="yes">w</italic> are the scaled-down dimensions of the feature map after sub-sampling. Now, the self-attention is shown in Equation (<xref rid="FD2-sensors-25-05582" ref-type="disp-formula">2</xref>).<disp-formula id="FD2-sensors-25-05582"><label>(2)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msubsup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow><mml:mo>&#65080;</mml:mo></mml:munder><mml:mrow><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:munder><mml:munder accentunder="true"><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#65080;</mml:mo></mml:munder><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This reduction in computational complexity brings it down to <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. It is worth noting that any down-sampling operation, including average/max-pooling or stridden convolutions, can be utilized as the projection to low-dimensional embedding. In our method, the feature map is down-sampled using convolution and then reduced to a size of 4 using bilinear interpolation. The original expressions for <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the corresponding feature matrices. We modify the expressions of the <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> sequence length as follows: <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mi>Conv</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:msup><mml:mi>Conv</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Furthermore, we replace the original position encoding of the PVT with a depth-separable convolution. The purpose of positional encoding is to address the alignment invariance in the self-attention operation, and one approach is to break the invariance in subsequent self-attention operations by introducing positional information at the input [<xref rid="B7-sensors-25-05582" ref-type="bibr">7</xref>]. Another approach is to delve deep into the network and introduce some information at the self-attention operation, thereby breaking this invariance [<xref rid="B40-sensors-25-05582" ref-type="bibr">40</xref>]. The method employed here follows the second approach, which breaks the invariance by reshaping the tokens into feature maps and then aggregating tokens that are not adjacent to each other through depth-separable convolution. Consequently, when the tokens are rearranged, the convolutionally aggregated tokens will change, thus breaking the invariance.</p></sec><sec id="sec3dot2-sensors-25-05582"><title>3.2. The Multi-Branch Decoder</title><p>Inspired by [<xref rid="B41-sensors-25-05582" ref-type="bibr">41</xref>], which manually marks thick and thin vessels into two different labels and then trains the two different labels using two U-Net network decoders, the final output results are fused and used as the input for another U-Net network. This network is then trained with the original labels to obtain the final results. However, the downside of this approach is the need to manually segment the hard-to-segment regions, which is very time-consuming and labor-intensive. High-throughput images of <italic toggle="yes">Phakopsora pachyrhizi</italic> differ from general images, as they exhibit fuzzy boundaries and belong to the hard-to-segment category. Consequently, we design a network architecture that automatically distinguishes hard-to-segment regions through coarse segmentation to obtain low-confidence regions. Then, images with coarse segmentation, hard-to-segment regions, and easy-to-segment regions are trained by three decoders, ultimately serving as the primary source of information for the fusion network.</p><p>This multi-branch segmentation model comprises a universal encoder and several distinct network branches with different loss functions, which can handle different tasks effectively and provide an efficient method for dealing with <italic toggle="yes">Phakopsora pachyrhizi</italic> image segmentation. Based on the results outlined above, we propose a new image hybrid depth segmentation network in this paper to enhance the segmentation accuracy for <italic toggle="yes">Phakopsora pachyrhizi</italic> images. The overall structure of the multi-branch segmentation approach is depicted in <xref rid="sensors-25-05582-f003" ref-type="fig">Figure 3</xref>. The fundamental structure of the approach consists of two distinct parts: the multi-branch segmentation network and the fusion network. The multi-branch network employs an MS-UNet structure, with a hierarchical ViT model for the encoder part and a convolutional network for the decoder part. For the fusion network, U-Net is used.</p><p>The multi-branch segmentation network automatically distinguishes the hard- and easy-to-segment regions of images of <italic toggle="yes">Phakopsora pachyrhizi</italic> and feeds them into the multi-branch segmentation network for training. Segmentation images from the respective network branches are obtained and used as input for the fusion network, along with the coarse segmentation images. Due to the structural similarity between the hard and easy segmentation regions, there is an overlap between these two coarse segmentation images. Therefore, a fusion network is employed to merge these two images with coarse-grained segmentation. The coarsely segmented images generated by the multi-branch segmentation network are used to train the fusion network, which is then given the original ground-truth values from the <italic toggle="yes">Phakopsora pachyrhizi</italic> dataset. This process is iteratively performed. The image is binarized and subsequently processed to produce the final segmentation result of <italic toggle="yes">Phakopsora pachyrhizi</italic> using the fusion result. This technique has the potential to effectively remove the overlap between the segmented images of these two distinct regions, thereby enhancing the overall segmentation accuracy.</p><p>Our proposed decoder network aims to automatically acquire hard-to-segment regions and compel the network to learn. Therefore, we design a network architecture that automatically extracts hard-to-segment and easy-to-distinguish labels, as illustrated in <xref rid="sensors-25-05582-f004" ref-type="fig">Figure 4</xref>. First, the input information on the images is acquired through the designed hierarchical ViT encoder structure, and rough segmentation results are obtained. Next, the rough segmentation result is compared with the ground truth, and pixels with a confidence greater than 0.75 are considered well-segmented areas, while pixels with a confidence of less than 0.75 are considered poorly segmented areas. We set two masks both with 1 s, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and dimensions of <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The formula for generating the label is as follows:<disp-formula id="FD3-sensors-25-05582"><label>(3)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>&#948;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>&#948;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05582"><label>(4)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>&#948;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>&#948;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the confidence level of each pixel point in the rough segmentation results.<disp-formula id="FD5-sensors-25-05582"><label>(5)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>hard</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05582"><label>(6)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the ground truth.</p><p>These two masks, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo></mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, will be used to identify easy-to-segment and hard-to-segment regions, respectively. By applying these masks, the decoder network will focus on learning the segmentation patterns for each type of region separately, leading to an improved segmentation performance overall.</p><p>To obtain the labels for the problematic region, we set the values with a confidence of less than 0.75 in the mask to 1 and values greater than 0.75 to 0. We then multiply this mask with the ground truth. On the other hand, to obtain the labels for the simple region, we set the values with a confidence greater than 0.75 in the mask to 1 and values less than 0.75 to 0. We then multiply this mask with the ground truth. In essence, the hard-to-segment region is masked to retain the region with low confidence, and the easy-to-segment region is masked to retain the region with high confidence. We then multiply these masks with the ground truth for <italic toggle="yes">Phakopsora pachyrhizi</italic> to obtain two types of labels.</p></sec><sec id="sec3dot3-sensors-25-05582"><title>3.3. The Loss Function</title><p>The proposed training process consists of coarse segmentation, the separation of hard/easy regions based on a confidence threshold, and fusion of the segmentation results. For a binary <italic toggle="yes">Phakopsora pachyrhizi</italic> segmentation (PPS) task, we employ the Binary Cross-Entropy (BCE) loss:<disp-formula id="FD7-sensors-25-05582"><label>(7)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#8721;</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>log</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the ground-truth label probability, and <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted probability.</p><p>The total loss is a weighted sum of the BCE losses from five branches: coarse segmentation, hard regions, easy regions, hard/easy fusion, and the final fusion network:<disp-formula id="FD8-sensors-25-05582"><label>(8)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Pre</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Pre</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the output of branch <italic toggle="yes">i</italic>, and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is its weight coefficient.</p></sec></sec><sec id="sec4-sensors-25-05582"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05582"><title>4.1. Databases</title><p>We present a curated dataset comprising 4414 original microscopy images and their corresponding manually annotated labels, documenting the morphological development of fungal spore buds. The original images were obtained from the publicly accessible Cell Image Library (CIL, National Center for Microscopy and Imaging Research, La Jolla, CA, USA; available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://cellimagelibrary.org">https://cellimagelibrary.org</uri>, accessed on 4 September 2025) and systematically organized into a time-series collection that captured the morphological changes and growth directions of spore buds under DMSO treatment conditions. The dataset spans the entire developmental timeline, covering approximately 90 to 210 min post-germination, with images acquired at fixed 15 min intervals. This ensures that the dataset includes a complete morphological evolution sequence, from the early germination stages to the late developmental phases. For stricter temporal generalization, we grouped the dataset by 9 exclusive time intervals. We then applied cross-validation: in each round, 7 intervals were used for training, with 1 for validation and 1 for testing (an 8:1:1 ratio). This procedure was repeated across all combinations. The dataset supporting this study has been deposited in Zenodo and is publicly accessible at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doi.org/10.5281/zenodo.17004172">https://doi.org/10.5281/zenodo.17004172</uri>, accessed on 4 September 2025.</p></sec><sec id="sec4dot2-sensors-25-05582"><title>4.2. Evaluation Metrics</title><p>We evaluate segmentation performance using four standard metrics: precision (Pre), recall (Re), Mean Intersection over the Union (MIoU), and F1-score (F1). Let <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote true positives, false positives, true negatives, and false negatives, respectively. For binary segmentation (<inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), the metrics are defined as<disp-formula id="FD9-sensors-25-05582"><label>(9)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05582"><label>(10)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05582"><label>(11)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>FN</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi><mml:mo>+</mml:mo><mml:mi>TP</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05582"><label>(12)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These metrics jointly measure classification accuracy (Pre, Re), spatial overlap (MIoU), and the harmonic balance between precision and recall (<italic toggle="yes">F1</italic>).</p><p>We further adopt the Boundary Intersection over the Union (Boundary IoU) to evaluate the segmentation quality along object contours. Unlike the region-based IoU, which considers the overlap of entire regions, the Boundary IoU measures the consistency between the predicted and ground-truth boundary regions, making it more sensitive to boundary errors in images with fuzzy object contours such as fungal microscopy images. Formally, let <italic toggle="yes">P</italic> and <italic toggle="yes">G</italic> denote the predicted and ground-truth masks, respectively. Their boundary regions, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, are obtained by dilating the contours within a fixed distance. The metric is defined as:<disp-formula id="FD13-sensors-25-05582"><label>(13)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi><mml:mo>&#8745;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi><mml:mo>&#8746;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of pixels in a set, and &#8745; and &#8746; represent the intersection and union operations, respectively.</p><p>We also introduce the Shape Complexity Similarity (SCS) metric, which measures the similarity of the shape complexity between the predicted and ground-truth regions. Unlike traditional overlap-based metrics, SCS can quantify morphological similarity even when the boundaries are fuzzy, thereby providing a more robust assessment of structural preservation. In fungal image analysis, this metric can capture morphological variations across different strains or growth stages, complementing conventional metrics. The shape complexity of a region <italic toggle="yes">R</italic> is defined as<disp-formula id="FD14-sensors-25-05582"><label>(14)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Perimeter</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>Area</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, SCS is computed as<disp-formula id="FD15-sensors-25-05582"><label>(15)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo>{</mml:mo><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Perimeter</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Area</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the perimeter length and area of region <italic toggle="yes">R</italic>, respectively. A higher SCS indicates better agreement in morphological complexity between the prediction and the ground truth.</p></sec><sec id="sec4dot3-sensors-25-05582"><title>4.3. Implementation Details</title><p>The training consists of two stages: multi-branch segmentation network training and fusion network training. In the multi-branch network, three ground truths are used: original <italic toggle="yes">Phakopsora pachyrhizi</italic> labels, automatically extracted hard regions, and easily distinguishable regions. BCE loss and the RMSprop optimizer (initial learning rate <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) are employed, with 300 training epochs. The fusion network is trained using the segmentation results from the multi-branch network and the original labels, following the same loss and optimization settings. All experiments are conducted on an NVIDIA GeForce RTX 4090 (24 GB) GPU, NVIDIA, Santa Clara, CA, USA).</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-05582"><title>5. Results and Discussion</title><sec id="sec5dot1-sensors-25-05582"><title>5.1. Experiments with Automatic Label Generation</title><p>We first train a single-decoder MS-UNet to obtain coarse segmentation. The coarse results are then compared with the ground truth to identify hard-to-segment and easy-to-segment regions. This process enables the multi-branch segmentation network to focus on learning the hard-to-segment regions. As shown in <xref rid="sensors-25-05582-f005" ref-type="fig">Figure 5</xref>, difficult segmentation regions exist at the edge of the soybean rust fungus, which is why the multi-branch segmentation network is proposed to learn these challenging areas. The automatic extraction of labels and focusing on learning the boundary for soybean rust fungus can improve the segmentation accuracy and reduce spatial information loss.</p><p>Indeed, <xref rid="sensors-25-05582-f005" ref-type="fig">Figure 5</xref> highlights the presence of difficult segmentation regions at the edges of the soybean rust fungus. This observation is the main reason behind proposing a multi-branch segmentation network to learn these challenging areas. In high-throughput microscopy images of soybean rust fungus, the boundaries are not as clear as those in natural images, making segmentation a complex task.</p><p>By automatically extracting labels and focusing on learning the boundaries of soybean rust fungus, the segmentation accuracy can be improved, and the loss of spatial information can be reduced. This approach helps the multi-branch segmentation network to effectively identify and segment hard-to-segment regions, resulting in a better overall segmentation performance compared to that of other methods.</p></sec><sec id="sec5dot2-sensors-25-05582"><title>5.2. Comparison with Other Methods</title><p>To demonstrate the details of the segmentation results better, an image of soybean rust fungus was used as the experimental subject. Experiments were conducted with traditional network structures like DeepLab and an FCN and advanced network structures such as UTNet, HRFormer, MobileVIT, Swin-UNet, and SegFormer. The experimental results are shown in <xref rid="sensors-25-05582-f006" ref-type="fig">Figure 6</xref>. As illustrated in <xref rid="sensors-25-05582-f006" ref-type="fig">Figure 6</xref>, in the soybean rust fungus (<italic toggle="yes">Phakopsora pachyrhizi</italic>) segmentation task, we observed significant differences in the performance of different algorithms. <xref rid="sensors-25-05582-f006" ref-type="fig">Figure 6</xref> and <xref rid="sensors-25-05582-f007" ref-type="fig">Figure 7</xref> (where <xref rid="sensors-25-05582-f007" ref-type="fig">Figure 7</xref> shows the local magnification details) clearly demonstrate that the FCN, MobileVIT, and Swin-UNet exhibited overlapping phenomena when processing adjacent spores, leading to inaccurate segmentation results; while DeepLab, UTNet, HRFormer, and SegFormer avoided overlapping issues, they still had deficiencies in spore boundary localization and shape preservation, being prone to boundary blurring or shape distortion; our MS-UNet not only successfully avoided overlapping issues but also performed optimally in maintaining the complete spore morphology and boundary clarity.</p><p>To emphasize the model&#8217;s performance better, <xref rid="sensors-25-05582-t001" ref-type="table">Table 1</xref> presents the values of the evaluation metrics. The proposed MS-UNet achieves a superior performance across all evaluation metrics, including precision, recall, the mIoU, the F1-score, the Boundary IoU, and SCS, compared to those of the other methods. This demonstrates the effectiveness of the multi-branch segmentation network in learning and segmenting hard-to-segment regions in soybean rust fungal images.</p><p>The reason MS-UNet surpasses traditional networks such as the FCN, DeepLab, HRformer, and UTnet in segmenting soybean rust fungus is due to the addition of a hierarchical ViT structure to the encoder. This structure allows the network to learn multi-scale and high-resolution features. Furthermore, the model features a multi-task branching decoder, which forces the network to learn hard-to-segment regions, resulting in sharper edges in the segmented images.</p></sec><sec id="sec5dot3-sensors-25-05582"><title>5.3. An Ablation Study on Individual Components</title><p>In the ablation experiments (<xref rid="sensors-25-05582-t002" ref-type="table">Table 2</xref>), the validity of the modules proposed in this paper is investigated. The model framework is based on U-Net, which is used as the benchmark. <xref rid="sensors-25-05582-t002" ref-type="table">Table 2</xref> demonstrates that adding the attention mechanism to the U-Net algorithm improves its performance, as seen with Ours-Multibranch. Each module designed in this paper positively impacts the algorithm&#8217;s improvement. Compared to those of the U-Net network, Pre, Re, MIoU, the F1-score, the Boundary IoU, andSCS are improved by 7.0%, 5.6%, 6.1%, 7.0%, 9.2%, and 7.0%, respectively. The best performance comes from the combined effect of the two modules, showing the effectiveness of the proposed approach.</p><p>The computational cost of each component is summarized in <xref rid="sensors-25-05582-t003" ref-type="table">Table 3</xref>. To improve the overall performance of the model, each module introduces only a moderate increase in the computational overhead and inference latency, both of which remain within an acceptable range.</p></sec><sec id="sec5dot4-sensors-25-05582"><title>5.4. Ablation Experiments on the Threshold</title><p>The results of the threshold sensitivity analysis are presented in <xref rid="sensors-25-05582-t004" ref-type="table">Table 4</xref>. Specifically, we evaluated multiple thresholds at 0.15 intervals, starting from 0.15. The experimental results indicate that a threshold of 0.75 yields the best overall performance, achieving a balanced improvement across key metrics such as the MIoU and F1-score. This finding validates the effectiveness of using 0.75 as the decision threshold to distinguish between easy and hard regions.</p></sec><sec id="sec5dot5-sensors-25-05582"><title>5.5. Computational Analysis</title><p>We consider the MS-UNet model to be highly suitable for practical deployment scenarios, particularly in medical image segmentation tasks with stringent real-time requirements. As shown in <xref rid="sensors-25-05582-t005" ref-type="table">Table 5</xref>, MS-UNet contains only 19.21 M parameters, which is substantially fewer than those in classic models such as the FCN (134.3 M) and DeepLab (41.2 M). Its FLOPs are 31.2 G, also markedly lower than those of Swin-UNet (67.3 G) and HRFormer (89.2 G), enabling good portability in resource-constrained environments (e.g., on edge devices or mobile platforms). Moreover, the inference time on an RTX 4090 is only 27.6 ms for 512 &#215; 512 inputs, significantly faster than HRFormer (52.7 ms) and Swin-UNet (39.8 ms), thereby satisfying real-time processing requirements such as 30+ frame per second video streams. In addition, the deployment potential of MS-UNet could be enhanced further through model compression techniques such as quantization or pruning.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05582"><title>6. Conclusions</title><p>To address the task of segmenting high-throughput microscopic images of <italic toggle="yes">Phakopsora pachyrhizi</italic> with blurred edges, a network model with a hierarchical ViT structure for the encoder and a multi-branch network for the decoder is proposed. An image fusion network is also designed for fusing images. The main contributions of this study are as follows:<list list-type="bullet"><list-item><p>A segmentation dataset of <italic toggle="yes">Phakopsora pachyrhizi</italic> was produced manually for the related research.</p></list-item><list-item><p>A multi-branch segmentation network was proposed to improve the segmentation accuracy in PPS by accurately segmenting difficult- and easy-to-segment regions from images of Phakopsora pachyrhizi.</p></list-item><list-item><p>To ensure the segmentation performance of the multi-branch segmentation network, an upgraded U-Net network was proposed as the base segmentation network. Specifically, a combination of a transformer and convolution was used to create a progressive hierarchical ViT encoder capable of learning multi-scale and high-resolution features, resulting in a segmentation network suitable for the PPS problem.</p></list-item><list-item><p>A fusion network was employed to solve the issue of overlapping regions in images of <italic toggle="yes">Phakopsora pachyrhizi</italic> by fusing two distinct sections of a multi-branch segmentation network.</p></list-item></list></p><p>Using the <italic toggle="yes">Phakopsora pachyrhizi</italic> dataset, the proposed method demonstrates a good segmentation performance for PPS tasks compared to that of other recent segmentation methods and traditional segmentation techniques. Notably, it excels in edge contour extraction for spore images. Further improvements in the segmentation accuracy are needed. Theoretically, this method is applicable to other segmentation tasks, particularly when there are challenging parts of the image to be segmented.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, T.L., L.S. and P.S.; Methodology, L.S. and P.X.; Software, T.L. and L.S.; Validation, T.L., L.S. and P.X.; Formal analysis, Q.W.; Investigation, Q.Z.; Resources, L.S.; Data curation, L.S.; Writing&#8212;original draft, T.L., L.S. and P.X.; Writing&#8212;review &amp; editing, L.S. and P.X.; Supervision, P.S.; Project administration, P.S.; Funding acquisition, P.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset supporting this study has been deposited in Zenodo and is publicly accessible at [<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doi.org/10.5281/zenodo.17004172">https://doi.org/10.5281/zenodo.17004172</uri>, accessed on 4 September 2025].</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05582"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cui</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hartman</surname><given-names>G.L.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name></person-group><article-title>Image processing methods for quantitatively detecting soybean rust from multispectral images</article-title><source>Biosyst. Eng.</source><year>2010</year><volume>107</volume><fpage>186</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2010.06.004</pub-id></element-citation></ref><ref id="B2-sensors-25-05582"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Beyer</surname><given-names>S.F.</given-names></name><name name-style="western"><surname>Beesley</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rohmann</surname><given-names>P.F.W.</given-names></name><name name-style="western"><surname>Schultheiss</surname><given-names>H.</given-names></name><name name-style="western"><surname>Conrath</surname><given-names>U.</given-names></name><name name-style="western"><surname>Langenbach</surname><given-names>C.J.G.</given-names></name></person-group><article-title>The Arabidopsis non-host defence-associated coumarin scopoletin protects soybean from Asian soybean rust</article-title><source>Plant J.</source><year>2019</year><volume>99</volume><fpage>397</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1111/tpj.14426</pub-id><pub-id pub-id-type="pmid">31148306</pub-id><pub-id pub-id-type="pmcid">PMC6852345</pub-id></element-citation></ref><ref id="B3-sensors-25-05582"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cavanagh</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mosbach</surname><given-names>A.</given-names></name><name name-style="western"><surname>Scalliet</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lind</surname><given-names>R.</given-names></name><name name-style="western"><surname>Endres</surname><given-names>R.G.</given-names></name></person-group><article-title>Physics-informed deep learning characterizes morphodynamics of Asian soybean rust disease</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>6424</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-26577-1</pub-id><pub-id pub-id-type="pmid">34741028</pub-id><pub-id pub-id-type="pmcid">PMC8571353</pub-id></element-citation></ref><ref id="B4-sensors-25-05582"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shelhamer</surname><given-names>E.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2015</year><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B5-sensors-25-05582"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Hrformer: High-resolution vision transformer for dense predict</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>7281</fpage><lpage>7293</lpage></element-citation></ref><ref id="B6-sensors-25-05582"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kokkinos</surname><given-names>I.</given-names></name><name name-style="western"><surname>Murphy</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>A.L.</given-names></name></person-group><article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><pub-id pub-id-type="pmid">28463186</pub-id></element-citation></ref><ref id="B7-sensors-25-05582"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Proceedings of the Neural Information Processing Systems (NIPS)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date><volume>Volume 30</volume></element-citation></ref><ref id="B8-sensors-25-05582"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Georgiou</surname><given-names>T.</given-names></name><name name-style="western"><surname>Lew</surname><given-names>M.S.</given-names></name></person-group><article-title>A review of semantic segmentation using deep neural networks</article-title><source>Int. J. Multimed. Inf. Retr.</source><year>2018</year><volume>7</volume><fpage>87</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1007/s13735-017-0141-z</pub-id></element-citation></ref><ref id="B9-sensors-25-05582"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name></person-group><article-title>Multi-UNet: An effective Multi-U convolutional networks for semantic segmentation</article-title><source>Knowl. Based Syst.</source><year>2025</year><volume>309</volume><fpage>11</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2024.112854</pub-id></element-citation></ref><ref id="B10-sensors-25-05582"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>M.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Aslam</surname><given-names>M.</given-names></name><name name-style="western"><surname>Akpokodje</surname><given-names>E.</given-names></name><name name-style="western"><surname>Jilani</surname><given-names>S.F.</given-names></name></person-group><article-title>Enhanced U-Net for Improved Semantic Segmentation in Landslide Detection</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>2670</elocation-id><pub-id pub-id-type="doi">10.3390/s25092670</pub-id><pub-id pub-id-type="pmid">40363108</pub-id><pub-id pub-id-type="pmcid">PMC12074259</pub-id></element-citation></ref><ref id="B11-sensors-25-05582"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name></person-group><article-title>Methods and datasets on semantic segmentation: A review</article-title><source>Neurocomputing</source><year>2018</year><volume>304</volume><fpage>82</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2018.03.037</pub-id></element-citation></ref><ref id="B12-sensors-25-05582"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Farabet</surname><given-names>C.</given-names></name><name name-style="western"><surname>Couprie</surname><given-names>C.</given-names></name><name name-style="western"><surname>Najman</surname><given-names>L.</given-names></name><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><article-title>Learning hierarchical features for scene labeling</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2012</year><volume>35</volume><fpage>1915</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.231</pub-id><pub-id pub-id-type="pmid">23787344</pub-id></element-citation></ref><ref id="B13-sensors-25-05582"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ning</surname><given-names>F.</given-names></name><name name-style="western"><surname>Delhomme</surname><given-names>D.</given-names></name><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Piano</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bottou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Barbano</surname><given-names>P.E.</given-names></name></person-group><article-title>Toward automatic phenotyping of developing embryos from videos</article-title><source>IEEE Trans. Image Process.</source><year>2005</year><volume>14</volume><fpage>1360</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1109/TIP.2005.852470</pub-id><pub-id pub-id-type="pmid">16190471</pub-id></element-citation></ref><ref id="B14-sensors-25-05582"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Object-contextual representations for semantic segmentation</article-title><source>European Conference on Computer Vision</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>173</fpage><lpage>190</lpage></element-citation></ref><ref id="B15-sensors-25-05582"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Siam</surname><given-names>M.</given-names></name><name name-style="western"><surname>Elkerdawy</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jagersand</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yogamani</surname><given-names>S.</given-names></name></person-group><article-title>Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges</article-title><source>Proceedings of the 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>16&#8211;19 October 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2017</year><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B16-sensors-25-05582"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Deep high-resolution representation learning for human pose estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2019</year><fpage>5693</fpage><lpage>5703</lpage></element-citation></ref><ref id="B17-sensors-25-05582"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>An kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Alvarez</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name></person-group><article-title>SegFormer: Simple and efficient design for semantic segmentation with transformers</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>12077</fpage><lpage>12090</lpage></element-citation></ref><ref id="B18-sensors-25-05582"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yurtkulu</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>&#350;ahin</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Unal</surname><given-names>G.</given-names></name></person-group><article-title>Semantic segmentation with extended DeepLabv3 architecture</article-title><source>Proceedings of the 2019 27th Signal Processing and Communications Applications Conference (SIU)</source><conf-loc>Sivas, Turkey</conf-loc><conf-date>24&#8211;26 April 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2019</year><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B19-sensors-25-05582"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.-C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schroff</surname><given-names>F.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Encoder-decoder with atrous separable convolution for semantic image segmentation</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2018</year><fpage>801</fpage><lpage>818</lpage></element-citation></ref><ref id="B20-sensors-25-05582"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ramachandran</surname><given-names>P.</given-names></name><name name-style="western"><surname>Srinivas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hechtman</surname><given-names>B.</given-names></name><name name-style="western"><surname>Shlens</surname><given-names>J.</given-names></name></person-group><article-title>Scaling local self-attention for parameter efficient visual backbones</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2021</year><fpage>12894</fpage><lpage>12904</lpage></element-citation></ref><ref id="B21-sensors-25-05582"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ramachandran</surname><given-names>P.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bello</surname><given-names>I.</given-names></name><name name-style="western"><surname>Levskaya</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shlens</surname><given-names>J.</given-names></name></person-group><article-title>Stand-alone self-attention in vision models</article-title><source>Proceedings of the 33rd International Conference on Neural Information Processing Systems</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>8&#8211;14 December 2019</conf-date><volume>Volume 32</volume></element-citation></ref><ref id="B22-sensors-25-05582"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kalantidis</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name></person-group><article-title>A<sup>2</sup>-nets: Double attention networks</article-title><source>Proceedings of the 32nd International Conference on Neural Information Processing Systems</source><conf-loc>Montr&#233;al, DC, Canada</conf-loc><conf-date>3&#8211;8 December 2018</conf-date><volume>Volume 31</volume></element-citation></ref><ref id="B23-sensors-25-05582"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Carion</surname><given-names>N.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Synnaeve</surname><given-names>G.</given-names></name><name name-style="western"><surname>Usunier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name></person-group><article-title>End-to-end object detection with transformers</article-title><source>European Conference on Computer Vision</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B24-sensors-25-05582"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ruan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>W.</given-names></name></person-group><article-title>FBoT-Net: Focal bottleneck transformer network for small green apple detection</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>205</volume><fpage>107609</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.107609</pub-id></element-citation></ref><ref id="B25-sensors-25-05582"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name></person-group><article-title>Combining convolutional and vision transformer structures for sheep face recognition</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>205</volume><fpage>107651</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2023.107651</pub-id></element-citation></ref><ref id="B26-sensors-25-05582"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>CGViT: Cross-image GroupViT for zero-shot semantic segmentation</article-title><source>Pattern Recognit.</source><year>2025</year><volume>164</volume><fpage>111505</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2025.111505</pub-id></element-citation></ref><ref id="B27-sensors-25-05582"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Touazi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gaceb</surname><given-names>D.</given-names></name><name name-style="western"><surname>Boudissa</surname><given-names>N.</given-names></name><name name-style="western"><surname>Assas</surname><given-names>S.</given-names></name></person-group><article-title>Enhancing Breast Mass Cancer Detection Through Hybrid ViT-Based Image Segmentation Model</article-title><source>International Conference on Computing Systems and Applications</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><pub-id pub-id-type="doi">10.1007/978-3-031-71848-9_12</pub-id></element-citation></ref><ref id="B28-sensors-25-05582"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Heng</surname><given-names>P.A.</given-names></name></person-group><article-title>H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation From CT Volumes</article-title><source>IEEE Trans. Med. Imaging</source><year>2018</year><volume>37</volume><fpage>2663</fpage><lpage>2674</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2845918</pub-id><pub-id pub-id-type="pmid">29994201</pub-id></element-citation></ref><ref id="B29-sensors-25-05582"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bosma</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dushatskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Grewal</surname><given-names>M.</given-names></name><name name-style="western"><surname>Alderliesten</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bosman</surname><given-names>P.</given-names></name></person-group><article-title>Mixed-Block Neural Architecture Search for Medical Image Segmentation</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2202.11401</pub-id><pub-id pub-id-type="arxiv">2202.11401</pub-id></element-citation></ref><ref id="B30-sensors-25-05582"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>MacGillivray</surname><given-names>T.</given-names></name><name name-style="western"><surname>Macnaught</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Newby</surname><given-names>D.</given-names></name></person-group><article-title>A two-stage 3D Unet framework for multi-class segmentation on full resolution image</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1804.04341</pub-id></element-citation></ref><ref id="B31-sensors-25-05582"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Sikdar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chitnis</surname><given-names>P.V.</given-names></name></person-group><article-title>Fully dense UNet for 2-D sparse photoacoustic tomography artifact removal</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2019</year><volume>24</volume><fpage>568</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2019.2912935</pub-id><pub-id pub-id-type="pmid">31021809</pub-id></element-citation></ref><ref id="B32-sensors-25-05582"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lian</surname><given-names>S.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name></person-group><article-title>Weighted res-unet for high-quality retina vessel segmentation</article-title><source>Proceedings of the 2018 9th International Conference on Information Technology in Medicine and Education (ITME)</source><conf-loc>Hangzhou, China</conf-loc><conf-date>19&#8211;21 October 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2018</year><fpage>327</fpage><lpage>331</lpage></element-citation></ref><ref id="B33-sensors-25-05582"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>R.</given-names></name></person-group><article-title>Retinal vessel segmentation combined with generative adversarial networks and dense U-Net</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>194551</fpage><lpage>194560</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3033273</pub-id></element-citation></ref><ref id="B34-sensors-25-05582"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Melo</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Gon&#231;alves</surname><given-names>D.N.</given-names></name><name name-style="western"><surname>Gomes</surname><given-names>M.d.N.B.</given-names></name><name name-style="western"><surname>Faria</surname><given-names>G.</given-names></name><name name-style="western"><surname>de Andrade Silva</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>A.P.M.</given-names></name><name name-style="western"><surname>Osco</surname><given-names>L.P.</given-names></name><name name-style="western"><surname>Furuya</surname><given-names>M.T.G.</given-names></name><name name-style="western"><surname>Junior</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Gonxcxalves</surname><given-names>W.N.</given-names></name></person-group><article-title>Automatic segmentation of cattle rib-eye area in ultrasound images using the UNet++ deep neural network</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>195</volume><fpage>106818</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.106818</pub-id></element-citation></ref><ref id="B35-sensors-25-05582"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Du</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>T.</given-names></name></person-group><article-title>Weighted skip-connection feature fusion: A method for augmenting UAV oriented rice panicle image segmentation</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>207</volume><fpage>107754</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2023.107754</pub-id></element-citation></ref><ref id="B36-sensors-25-05582"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yogalakshmi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Rani</surname><given-names>B.S.</given-names></name></person-group><article-title>Enhanced BT segmentation with modified U-Net architecture: A hybrid optimization approach using CFO-SFO algorithm</article-title><source>Int. J. Syst. Assur. Eng. Manag.</source><year>2025</year><volume>16</volume><fpage>1451</fpage><lpage>1467</lpage><pub-id pub-id-type="doi">10.1007/s13198-025-02762-z</pub-id></element-citation></ref><ref id="B37-sensors-25-05582"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>L.</given-names></name></person-group><article-title>DPUSegDiff: A Dual-Path U-Net Segmentation Diffusion model for medical image segmentation</article-title><source>Electron. Res. Arch.</source><year>2025</year><volume>33</volume><fpage>2947</fpage><lpage>2971</lpage><pub-id pub-id-type="doi">10.3934/era.2025129</pub-id></element-citation></ref><ref id="B38-sensors-25-05582"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>L.</given-names></name></person-group><article-title>Shape-intensity-guided U-net for medical image segmentation</article-title><source>Neurocomputing</source><year>2024</year><volume>610</volume><fpage>128534</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2024.128534</pub-id></element-citation></ref><ref id="B39-sensors-25-05582"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>L.</given-names></name></person-group><article-title>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date></element-citation></ref><ref id="B40-sensors-25-05582"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name></person-group><article-title>Conditional Positional Encodings for Vision Transformers</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2102.10882</pub-id></element-citation></ref><ref id="B41-sensors-25-05582"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>G.</given-names></name></person-group><article-title>A Hybrid Deep Segmentation Network for Fundus Vessels via Deep-Learning Framework</article-title><source>Neurocomputing</source><year>2021</year><volume>448</volume><fpage>168</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.03.085</pub-id></element-citation></ref><ref id="B42-sensors-25-05582"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>M.</given-names></name><name name-style="western"><surname>Metaxas</surname><given-names>D.N.</given-names></name></person-group><article-title>UTNet: A hybrid transformer architecture for medical image segmentation</article-title><source>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Strasbourg, France</conf-loc><conf-date>27 September&#8211;1 October 2021</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>61</fpage><lpage>71</lpage></element-citation></ref><ref id="B43-sensors-25-05582"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>An kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>&#193;lvarez</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name></person-group><article-title>SegFormer: Simple and efficient design for semantic segmentation with transformers</article-title><source>Proceedings of the Neural Information Processing Systems</source><conf-loc>Online</conf-loc><conf-date>6&#8211;14 December 2021</conf-date><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2021</year><fpage>1</fpage><lpage>13</lpage></element-citation></ref><ref id="B44-sensors-25-05582"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name></person-group><article-title>Swin-Unet: Unet-like pure transformer for medical image segmentation</article-title><source>Proceedings of the ECCV Workshops</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>205</fpage><lpage>218</lpage></element-citation></ref><ref id="B45-sensors-25-05582"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mehta</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rastegari</surname><given-names>M.</given-names></name></person-group><article-title>MobileViT: Light-weight, general-purpose, and mobile-friendly vision transformer</article-title><source>Sensors</source><year>2021</year><volume>21</volume><fpage>9210</fpage></element-citation></ref><ref id="B46-sensors-25-05582"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><source>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Munich, Germany</conf-loc><conf-date>5&#8211;9 October 2015</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>234</fpage><lpage>241</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05582-f001" orientation="portrait"><label>Figure 1</label><caption><p>MS-UNetnetwork structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05582-g001.jpg"/></fig><fig position="float" id="sensors-25-05582-f002" orientation="portrait"><label>Figure 2</label><caption><p>In the encoder section, the entire model is divided into four stages, each consisting of a patch embedding layer and a Li-layer transformer encoder. The output resolution of these four stages progressively decreases from high (4-stride) to low (32-stride) in a pyramidal fashion. Each execution of the transformer encoder corresponds to one level of down-sampling.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05582-g002.jpg"/></fig><fig position="float" id="sensors-25-05582-f003" orientation="portrait"><label>Figure 3</label><caption><p>The proposed multi-branch decoder.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05582-g003.jpg"/></fig><fig position="float" id="sensors-25-05582-f004" orientation="portrait"><label>Figure 4</label><caption><p>The label extraction network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05582-g004.jpg"/></fig><fig position="float" id="sensors-25-05582-f005" orientation="portrait"><label>Figure 5</label><caption><p>The two generated labels.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05582-g005.jpg"/></fig><fig position="float" id="sensors-25-05582-f006" orientation="portrait"><label>Figure 6</label><caption><p>Segmentation results of different network structures on <italic toggle="yes">Phakopsora pachyrhizi</italic> dataset. Squares indicate the regions that are enlarged for detailed visualization.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05582-g006.jpg"/></fig><fig position="float" id="sensors-25-05582-f007" orientation="portrait"><label>Figure 7</label><caption><p>The details of a single <italic toggle="yes">Phakopsora pachyrhizi</italic> segmentation, where (<bold>a</bold>,<bold>b</bold>) denote the original image and the ground truth and (<bold>c</bold>&#8211;<bold>j</bold>) are the segmentation results of FCN, Deeplab, HRFormer, UTnet, Swin-UNet, MobileViT, SegFormer, and MS-UNet, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05582-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05582-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05582-t001_Table 1</object-id><label>Table 1</label><caption><p>Segmentation results of different methods on <italic toggle="yes">Phakopsora pachyrhizi</italic> dataset. <italic toggle="yes">Note</italic>: Bold numbers indicate the best performance in each column.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Re</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Boundary IoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SCS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCN&#160;[<xref rid="B4-sensors-25-05582" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.965 &#177; 0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.925 &#177; 0.018</td><td align="center" valign="middle" rowspan="1" colspan="1">0.945 &#177; 0.011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.944 &#177; 0.013</td><td align="center" valign="middle" rowspan="1" colspan="1">0.885 &#177; 0.015</td><td align="center" valign="middle" rowspan="1" colspan="1">0.832 &#177; 0.014</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepLab&#160;[<xref rid="B6-sensors-25-05582" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.901 &#177; 0.019</td><td align="center" valign="middle" rowspan="1" colspan="1">0.932 &#177; 0.016</td><td align="center" valign="middle" rowspan="1" colspan="1">0.895 &#177; 0.017</td><td align="center" valign="middle" rowspan="1" colspan="1">0.888 &#177; 0.018</td><td align="center" valign="middle" rowspan="1" colspan="1">0.856 &#177; 0.021</td><td align="center" valign="middle" rowspan="1" colspan="1">0.857 &#177; 0.019</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HRformer&#160;[<xref rid="B5-sensors-25-05582" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.785 &#177; 0.023</td><td align="center" valign="middle" rowspan="1" colspan="1">0.928 &#177; 0.020</td><td align="center" valign="middle" rowspan="1" colspan="1">0.862 &#177; 0.022</td><td align="center" valign="middle" rowspan="1" colspan="1">0.851 &#177; 0.021</td><td align="center" valign="middle" rowspan="1" colspan="1">0.822 &#177; 0.025</td><td align="center" valign="middle" rowspan="1" colspan="1">0.848 &#177; 0.023</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UTnet&#160;[<xref rid="B42-sensors-25-05582" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.923 &#177; 0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">0.985 &#177; 0.008</td><td align="center" valign="middle" rowspan="1" colspan="1">0.951 &#177; 0.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.953 &#177; 0.011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.899 &#177; 0.013</td><td align="center" valign="middle" rowspan="1" colspan="1">0.908 &#177; 0.012</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegFormer&#160;[<xref rid="B43-sensors-25-05582" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.922 &#177; 0.009</td><td align="center" valign="middle" rowspan="1" colspan="1">0.938 &#177; 0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.921 &#177; 0.008</td><td align="center" valign="middle" rowspan="1" colspan="1">0.935 &#177; 0.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.877 &#177; 0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">0.886 &#177; 0.011</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Swin-UNet&#160;[<xref rid="B44-sensors-25-05582" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.933 &#177; 0.011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.958 &#177; 0.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.945 &#177; 0.009</td><td align="center" valign="middle" rowspan="1" colspan="1">0.946 &#177; 0.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.878 &#177; 0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.894 &#177; 0.011</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MobileViT&#160;[<xref rid="B45-sensors-25-05582" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.906 &#177; 0.016</td><td align="center" valign="middle" rowspan="1" colspan="1">0.905 &#177; 0.015</td><td align="center" valign="middle" rowspan="1" colspan="1">0.913 &#177; 0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">0.906 &#177; 0.015</td><td align="center" valign="middle" rowspan="1" colspan="1">0.851 &#177; 0.017</td><td align="center" valign="middle" rowspan="1" colspan="1">0.872 &#177; 0.016</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MS-UNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.987 &#177; 0.012
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.959 &#177; 0.014
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.963 &#177; 0.012
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.967 &#177; 0.009
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.913 &#177; 0.011
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.935 &#177; 0.013
</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05582-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05582-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance comparison of different model components. <italic toggle="yes">Note</italic>: Bold numbers indicate the best performance in each column. The symbol &#8220;&#215;&#8221; indicates that the module is not included, while &#8220;&#10003;&#8221; indicates that the module is included.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Configuration</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Attention</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Multi-<break/>Branch</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">U-Net<break/>Fusion</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pre</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Re</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MIoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Boundary IoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SCS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Net [<xref rid="B46-sensors-25-05582" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.917</td><td align="center" valign="middle" rowspan="1" colspan="1">0.903</td><td align="center" valign="middle" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" rowspan="1" colspan="1">0.897</td><td align="center" valign="middle" rowspan="1" colspan="1">0.821</td><td align="center" valign="middle" rowspan="1" colspan="1">0.865</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.938</td><td align="center" valign="middle" rowspan="1" colspan="1">0.921</td><td align="center" valign="middle" rowspan="1" colspan="1">0.913</td><td align="center" valign="middle" rowspan="1" colspan="1">0.921</td><td align="center" valign="middle" rowspan="1" colspan="1">0.835</td><td align="center" valign="middle" rowspan="1" colspan="1">0.878</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours-Multibranch</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.951</td><td align="center" valign="middle" rowspan="1" colspan="1">0.925</td><td align="center" valign="middle" rowspan="1" colspan="1">0.936</td><td align="center" valign="middle" rowspan="1" colspan="1">0.941</td><td align="center" valign="middle" rowspan="1" colspan="1">0.851</td><td align="center" valign="middle" rowspan="1" colspan="1">0.892</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours-Attention-Multibranch</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.967</td><td align="center" valign="middle" rowspan="1" colspan="1">0.927</td><td align="center" valign="middle" rowspan="1" colspan="1">0.945</td><td align="center" valign="middle" rowspan="1" colspan="1">0.947</td><td align="center" valign="middle" rowspan="1" colspan="1">0.870</td><td align="center" valign="middle" rowspan="1" colspan="1">0.907</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.987
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.959
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.963
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.967
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.913
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.935
</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05582-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05582-t003_Table 3</object-id><label>Table 3</label><caption><p>Computational efficiency comparison of different model components. <italic toggle="yes">Note</italic>: The symbol &#8220;&#215;&#8221; indicates that the module is not included, while &#8220;&#10003;&#8221; indicates that the module is included.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Configuration</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Attention</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Multi-<break/>Branch</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">U-Net<break/>Fusion</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Maximum Memory<break/>Usage (MB)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Inference<break/>Time (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Net&#160;[<xref rid="B46-sensors-25-05582" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">7.76</td><td align="center" valign="middle" rowspan="1" colspan="1">6.2</td><td align="center" valign="middle" rowspan="1" colspan="1">642</td><td align="center" valign="middle" rowspan="1" colspan="1">12.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">10.23</td><td align="center" valign="middle" rowspan="1" colspan="1">14.8</td><td align="center" valign="middle" rowspan="1" colspan="1">798</td><td align="center" valign="middle" rowspan="1" colspan="1">16.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours-Multibranch</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">15.67</td><td align="center" valign="middle" rowspan="1" colspan="1">18.6</td><td align="center" valign="middle" rowspan="1" colspan="1">1245</td><td align="center" valign="middle" rowspan="1" colspan="1">20.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours-Attention-Multibranch</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">18.12</td><td align="center" valign="middle" rowspan="1" colspan="1">28.4</td><td align="center" valign="middle" rowspan="1" colspan="1">1487</td><td align="center" valign="middle" rowspan="1" colspan="1">25.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1576</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05582-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05582-t004_Table 4</object-id><label>Table 4</label><caption><p>The performance of the model across different threshold values. <italic toggle="yes">Note</italic>: Bold numbers indicate the best performance in each column.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Threshold</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Pre</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Re</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">MIoU</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">F1</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Boundary IoU</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">SCS</italic>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.891</td><td align="center" valign="middle" rowspan="1" colspan="1">0.973</td><td align="center" valign="middle" rowspan="1" colspan="1">0.896</td><td align="center" valign="middle" rowspan="1" colspan="1">0.935</td><td align="center" valign="middle" rowspan="1" colspan="1">0.831</td><td align="center" valign="middle" rowspan="1" colspan="1">0.862</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.30</td><td align="center" valign="middle" rowspan="1" colspan="1">0.915</td><td align="center" valign="middle" rowspan="1" colspan="1">0.965</td><td align="center" valign="middle" rowspan="1" colspan="1">0.912</td><td align="center" valign="middle" rowspan="1" colspan="1">0.943</td><td align="center" valign="middle" rowspan="1" colspan="1">0.849</td><td align="center" valign="middle" rowspan="1" colspan="1">0.878</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.938</td><td align="center" valign="middle" rowspan="1" colspan="1">0.961</td><td align="center" valign="middle" rowspan="1" colspan="1">0.927</td><td align="center" valign="middle" rowspan="1" colspan="1">0.951</td><td align="center" valign="middle" rowspan="1" colspan="1">0.867</td><td align="center" valign="middle" rowspan="1" colspan="1">0.894</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.60</td><td align="center" valign="middle" rowspan="1" colspan="1">0.956</td><td align="center" valign="middle" rowspan="1" colspan="1">0.954</td><td align="center" valign="middle" rowspan="1" colspan="1">0.939</td><td align="center" valign="middle" rowspan="1" colspan="1">0.955</td><td align="center" valign="middle" rowspan="1" colspan="1">0.882</td><td align="center" valign="middle" rowspan="1" colspan="1">0.909</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.75
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.987</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.959
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.963
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.967
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.913
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.935
</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.989</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.928</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.948</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.894</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.917</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05582-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05582-t005_Table 5</object-id><label>Table 5</label><caption><p>Computational efficiency comparison of different methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Maximum Memory<break/>Usage (MB)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Inference<break/>Time (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegFormer&#160;[<xref rid="B43-sensors-25-05582" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.7</td><td align="center" valign="middle" rowspan="1" colspan="1">8.4</td><td align="center" valign="middle" rowspan="1" colspan="1">648</td><td align="center" valign="middle" rowspan="1" colspan="1">11.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MobileViT&#160;[<xref rid="B45-sensors-25-05582" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">4.9</td><td align="center" valign="middle" rowspan="1" colspan="1">512</td><td align="center" valign="middle" rowspan="1" colspan="1">9.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UTnet&#160;[<xref rid="B42-sensors-25-05582" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">9.53</td><td align="center" valign="middle" rowspan="1" colspan="1">15.8</td><td align="center" valign="middle" rowspan="1" colspan="1">782</td><td align="center" valign="middle" rowspan="1" colspan="1">16.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Swin-UNet&#160;[<xref rid="B44-sensors-25-05582" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">27</td><td align="center" valign="middle" rowspan="1" colspan="1">67.3</td><td align="center" valign="middle" rowspan="1" colspan="1">2184</td><td align="center" valign="middle" rowspan="1" colspan="1">39.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HRformer&#160;[<xref rid="B5-sensors-25-05582" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">37.9</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3048</td><td align="center" valign="middle" rowspan="1" colspan="1">52.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepLab&#160;[<xref rid="B6-sensors-25-05582" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">41.2</td><td align="center" valign="middle" rowspan="1" colspan="1">52.7</td><td align="center" valign="middle" rowspan="1" colspan="1">1724</td><td align="center" valign="middle" rowspan="1" colspan="1">26.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCN&#160;[<xref rid="B4-sensors-25-05582" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">134.3</td><td align="center" valign="middle" rowspan="1" colspan="1">148.6</td><td align="center" valign="middle" rowspan="1" colspan="1">2856</td><td align="center" valign="middle" rowspan="1" colspan="1">18.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MS-UNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1576</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.6</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>