<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Plants (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Plants (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">2909</journal-id><journal-id journal-id-type="pmc-domain">plants</journal-id><journal-id journal-id-type="publisher-id">plants</journal-id><journal-title-group><journal-title>Plants</journal-title></journal-title-group><issn pub-type="epub">2223-7747</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430776</article-id><article-id pub-id-type="pmcid-ver">PMC12430776.1</article-id><article-id pub-id-type="pmcaid">12430776</article-id><article-id pub-id-type="pmcaiid">12430776</article-id><article-id pub-id-type="doi">10.3390/plants14172634</article-id><article-id pub-id-type="publisher-id">plants-14-02634</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Sparse-MoE-SAM: A Lightweight Framework Integrating MoE and SAM with a Sparse Attention Mechanism for Plant Disease Segmentation in Resource-Constrained Environments</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Zhao</surname><given-names initials="B">Benhan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Kang</surname><given-names initials="X">Xilin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af2-plants-14-02634" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhou</surname><given-names initials="H">Hao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shi</surname><given-names initials="Z">Ziyang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="L">Lin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref><xref rid="c1-plants-14-02634" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5142-4845</contrib-id><name name-style="western"><surname>Zhou</surname><given-names initials="G">Guoxiong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref><xref rid="c1-plants-14-02634" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wan</surname><given-names initials="F">Fangying</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhu</surname><given-names initials="J">Jiangzhang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yan</surname><given-names initials="Y">Yongming</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-plants-14-02634" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="L">Leheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af3-plants-14-02634" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="Y">Yulong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af4-plants-14-02634" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Rossi</surname><given-names initials="V">Vittorio</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-plants-14-02634"><label>1</label>School of Electronic Information and Physics, Central South University of Forestry and Technology, Changsha 410004, China; <email>20223337@csuft.edu.cn</email> (B.Z.); <email>20231100403@csuft.edu.cn</email> (H.Z.); <email>20231200582@csuft.edu.cn</email> (Z.S.); <email>t19940562@csuft.edu.cn</email> (F.W.); <email>t20070605@csuft.edu.cn</email> (J.Z.); <email>t20080581@csuft.edu.cn</email> (Y.Y.)</aff><aff id="af2-plants-14-02634"><label>2</label>School of Computer, Jiangsu University of Science and Technology, Zhenjiang 212100, China; <email>232241807217@stu.just.edu.cn</email></aff><aff id="af3-plants-14-02634"><label>3</label>School of Forestry, Central South University of Forestry and Technology, Changsha 410004, China; <email>20220147@csuft.edu.cn</email></aff><aff id="af4-plants-14-02634"><label>4</label>Bangor College, Central South University of Forestry and Technology, Changsha 410004, China; <email>t20216055@csuft.edu.cn</email></aff><author-notes><corresp id="c1-plants-14-02634"><label>*</label>Correspondence: <email>t20060540@csuft.edu.cn</email> (L.L.); <email>t20060599@csuft.edu.cn</email> (G.Z.)</corresp></author-notes><pub-date pub-type="epub"><day>24</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>14</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496803</issue-id><elocation-id>2634</elocation-id><history><date date-type="received"><day>17</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>17</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>18</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>24</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="plants-14-02634.pdf"/><abstract><p>Plant disease segmentation has achieved significant progress with the help of artificial intelligence. However, deploying high-accuracy segmentation models in resource-limited settings faces three key challenges, as follows: (A) Traditional dense attention mechanisms incur quadratic computational complexity growth (O(<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>)), rendering them ill-suited for low-power hardware. (B) Naturally sparse spatial distributions and large-scale variations in the lesions on leaves necessitate models that concurrently capture long-range dependencies and local details. (C) Complex backgrounds and variable lighting in field images often induce segmentation errors. To address these challenges, we propose Sparse-MoE-SAM, an efficient framework based on an enhanced Segment Anything Model (SAM). This deep learning framework integrates sparse attention mechanisms with a two-stage mixture of experts (MoE) decoder. The sparse attention dynamically activates key channels aligned with lesion sparsity patterns, reducing self-attention complexity while preserving long-range context. Stage 1 of the MoE decoder performs coarse-grained boundary localization; Stage 2 achieves fine-grained segmentation by leveraging specialized experts within the MoE, significantly enhancing edge discrimination accuracy. The expert repository&#8212;comprising standard convolutions, dilated convolutions, and depthwise separable convolutions&#8212;dynamically routes features through optimized processing paths based on input texture and lesion morphology. This enables robust segmentation across diverse leaf textures and plant developmental stages. Further, we design a sparse attention-enhanced Atrous Spatial Pyramid Pooling (ASPP) module to capture multi-scale contexts for both extensive lesions and small spots. Evaluations on three heterogeneous datasets (PlantVillage Extended, CVPPP, and our self-collected field images) show that Sparse-MoE-SAM achieves a mean Intersection-over-Union (mIoU) of 94.2%&#8212;surpassing standard SAM by 2.5 percentage points&#8212;while reducing computational costs by 23.7% compared to the original SAM baseline. The model also demonstrates balanced performance across disease classes and enhanced hardware compatibility. Our work validates that integrating sparse attention with MoE mechanisms sustains accuracy while drastically lowering computational demands, enabling the scalable deployment of plant disease segmentation models on mobile and edge devices.</p></abstract><kwd-group><kwd>plant disease segmentation</kwd><kwd>sparse attention</kwd><kwd>mixture of experts</kwd><kwd>SAM (Segment Anything Model)</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation in China</funding-source><award-id>61902436</award-id></award-group><award-group><funding-source>Education Department Key Program of Hunan Province</funding-source><award-id>21A0160</award-id></award-group><funding-statement>This work was supported by the National Natural Science Foundation in China (Grant No. 61902436) and the Education Department Key Program of Hunan Province (Grant No. 21A0160).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-plants-14-02634"><title>1. Introduction</title><p>Plant diseases present a persistent threat to global agricultural productivity, with profound implications for food security, economic stability, and environmental sustainability [<xref rid="B1-plants-14-02634" ref-type="bibr">1</xref>]. Conventional disease identification predominantly relies on manual inspection and expert diagnosis, which are labor-intensive, time-consuming, and difficult to scale, while automated computer vision methods for disease detection and segmentation have gained traction with increasing access to high-resolution imagery and computational resources [<xref rid="B2-plants-14-02634" ref-type="bibr">2</xref>,<xref rid="B3-plants-14-02634" ref-type="bibr">3</xref>], achieving precise and efficient diseased-region segmentation remains challenging. This is attributable to symptom heterogeneity, imaging condition variability, and the complex structural features of plant leaves.</p><p>Established deep learning models&#8212;including Convolutional Neural Networks (CNNs) and specialized architectures like U-Net and DeepLab&#8212;have demonstrated strong performance in generic image segmentation [<xref rid="B4-plants-14-02634" ref-type="bibr">4</xref>,<xref rid="B5-plants-14-02634" ref-type="bibr">5</xref>,<xref rid="B6-plants-14-02634" ref-type="bibr">6</xref>]. However, their utility in practical plant disease segmentation is limited by three factors [<xref rid="B7-plants-14-02634" ref-type="bibr">7</xref>]. First, computational bottlenecks arise from intensive feature processing pipelines, restricting real-time deployment on edge devices in agricultural environments [<xref rid="B8-plants-14-02634" ref-type="bibr">8</xref>]. Second, insufficient domain adaptation in model architectures impedes generalization across diverse plant species, disease types, and environmental conditions. Third, overfitting to training data distributions reduces robustness against natural variability in field-collected imagery.</p><p>Recent Transformer-based advances leverage global self-attention to enhance modeling capacity [<xref rid="B9-plants-14-02634" ref-type="bibr">9</xref>,<xref rid="B10-plants-14-02634" ref-type="bibr">10</xref>,<xref rid="B11-plants-14-02634" ref-type="bibr">11</xref>]. Foundational models like Meta&#8217;s Segment Anything Model (SAM) exhibit exceptional zero-shot generalization across segmentation tasks. Contemporary research has explored various efficiency improvements for plant disease segmentation. For instance, recent work by Upadhyay et al. (2025) reviewed advancements in deep learning techniques, emphasizing the use of convolutional neural networks (CNNs) and transformers for disease detection [<xref rid="B12-plants-14-02634" ref-type="bibr">12</xref>]. Zhang et al. (2025) proposed Reformer with re-parameterized kernels for grape disease segmentation, achieving competitive accuracy with reduced parameters [<xref rid="B13-plants-14-02634" ref-type="bibr">13</xref>]. Hu et al. (2024) introduced the LVF framework combining language and vision features for tomato disease segmentation, demonstrating multimodal fusion benefits [<xref rid="B11-plants-14-02634" ref-type="bibr">11</xref>]. However, these approaches either focus on specific crops or lack comprehensive efficiency optimization for edge deployment.</p><p>Nonetheless, agricultural deployment faces critical limitations [<xref rid="B14-plants-14-02634" ref-type="bibr">14</xref>]. SAM&#8217;s dense attention layers, although effective, incur quadratic computational complexity (O(n<sup>2</sup>)) relative to input size, making them unsuitable for deployment on resource-constrained agricultural hardware. Recent efficiency-oriented solutions include sparse attention patterns [<xref rid="B15-plants-14-02634" ref-type="bibr">15</xref>], pruning strategies [<xref rid="B16-plants-14-02634" ref-type="bibr">16</xref>], and knowledge distillation [<xref rid="B17-plants-14-02634" ref-type="bibr">17</xref>], yet none specifically address the unique challenges of agricultural deployment: extreme computational constraints, diverse environmental conditions, and the inherent sparsity of disease symptoms.</p><p>Comparison with Non-SAM Sparse Attention Architectures: Beyond the SAM series, several sparse attention mechanisms have been developed for computer vision tasks. Linformer reduces attention complexity to O(nk) through low-rank matrix approximation but lacks content-aware selection crucial for disease symptom localization. Performer employs random feature maps for linear attention complexity but struggles with precise boundary delineation required in medical/agricultural applications. Swin Transformer utilizes windowed attention for computational efficiency but its fixed spatial partitioning cannot adapt to the irregular, multi-scale nature of plant disease patterns. PVT (Pyramid Vision Transformer) incorporates spatial-reduction attention but maintains dense processing within reduced regions, limiting efficiency gains. Our Gumbel-TopK approach differs fundamentally by: (1) content-adaptive selection vs. fixed patterns, (2) differentiable routing enabling end-to-end training vs. discrete approximations, and (3) biologically motivated sparsity patterns specifically designed for agricultural pathology vs. general computer vision tasks. Comparative experiments show our method outperforms Linformer by 3.7% IoU, Performer by 2.9% IoU, and Swin-S by 1.8% IoU while achieving comparable or superior computational efficiency.</p><p>Plant diseases typically manifest as localized patterns (e.g., small lesions, chlorotic patches, irregular discolorations) exhibiting inherent spatial sparsity and multi-scale properties. This biological insight motivates architectures that prioritize computation for salient regions while suppressing irrelevant background features [<xref rid="B15-plants-14-02634" ref-type="bibr">15</xref>,<xref rid="B16-plants-14-02634" ref-type="bibr">16</xref>]. An adaptive mechanism enabling dynamic allocation of computational resources to discriminative feature domains based on spatial saliency is thus essential.</p><p>Efficient attention mechanisms and expert specialization represent critical design paradigms for addressing these challenges. Sparse attention architectures have demonstrated effectiveness in reducing computational complexity while preserving semantic relationships, particularly in scenarios with inherent spatial sparsity. Expert specialization through MoE frameworks enables adaptive processing tailored to distinct input characteristics. In plant pathology specifically, recent works have explored efficiency improvements: Guo et al. (2024) investigated dual U-shaped networks with coordinate attention for pest segmentation [<xref rid="B18-plants-14-02634" ref-type="bibr">18</xref>], while Yang et al. (2024) applied multi-scale attention for lesion segmentation [<xref rid="B7-plants-14-02634" ref-type="bibr">7</xref>]. However, these approaches primarily focus on conventional attention mechanisms without addressing the fundamental quadratic complexity issue. Sparse attention applications in agricultural computer vision remain largely unexplored, despite the natural sparsity of disease symptoms presenting ideal opportunities for computational optimization. Similarly, while MoE architectures have shown promise in natural language processing and general computer vision, their application to plant disease segmentation&#8212;where symptoms exhibit diverse morphological patterns requiring specialized processing&#8212;represents a significant research gap. The unique challenges of agricultural deployment, including extreme computational constraints, diverse environmental conditions, and the inherent sparsity of disease manifestations, necessitate novel architectural solutions that jointly optimize accuracy and efficiency.</p><p>Effective deployment in agricultural settings requires an optimal trade-off between segmentation accuracy, computational efficiency, and robustness to imaging variations. This work establishes a framework that advances state-of-the-art benchmark performance while ensuring minimal accuracy loss when transferred to field conditions [<xref rid="B19-plants-14-02634" ref-type="bibr">19</xref>].</p><p>This paper introduces Sparse-MoE-SAM, a novel architectural paradigm that represents the first comprehensive integration of bio-inspired sparse attention with hierarchical mixture-of-experts specifically designed for agricultural computer vision. Unlike existing approaches that address computational efficiency or segmentation accuracy in isolation, our framework simultaneously achieves both objectives through three synergistic innovations:<list list-type="simple"><list-item><label>(a)</label><p>To mitigate computational overhead, we introduce a sparse attention mechanism integrated with the MoE framework, further evaluated in the experimental section, we introduce a spatially constrained sparse attention module. This module employs dynamic top-k selection, retaining only the top k% of attention weights (where k &#8810; n), thereby reducing self-attention complexity to O(nk). This design reflects the natural sparsity patterns observed in plant lesions, such as localized necrotic tissue, achieving strong alignment with pathologist annotations (Pearson r &gt; 0.82). The mechanism preserves over 90% of critical feature connections while achieving a 23.7% reduction in computational requirements, enabling efficient processing of high-resolution field imagery.</p></list-item><list-item><label>(b)</label><p>To effectively process sparsely distributed, multi-scale lesions that require integration of both local and global contextual information, we develop a dual-stage MoE framework. Coarse-grained segmentation and fine-grained refinement are performed by specialized expert networks tailored to each processing stage. A gating network dynamically routes features based on input complexity, enabling this cascaded architecture to achieve superior boundary discrimination compared to uniform processing approaches, particularly under cluttered background conditions.</p></list-item><list-item><label>(c)</label><p>To mitigate segmentation errors arising from complex backgrounds and varying illumination conditions, we integrate sparse attention mechanisms into an enhanced ASPP module. This design captures multi-scale contextual information across diverse lesion sizes, from minute lesions to extensive macules, addressing fundamental limitations of both traditional SAM architectures and standard Transformer models.</p></list-item></list></p><p>Experimental validation demonstrates that sparse attention mechanisms, MoE design, and enhanced ASPP modules contribute both individually and synergistically to improved segmentation accuracy without computational overhead. Comprehensive evaluation through quantitative benchmarks, ablation studies, and cross-dataset validation confirms state-of-the-art performance with robust generalization across diverse agricultural scenarios.</p></sec><sec sec-type="results" id="sec2-plants-14-02634"><title>2. Results</title><sec id="sec2dot1-plants-14-02634"><title>2.1. Dataset</title><p>The primary dataset used for evaluating the proposed Sparse-MoE-SAM framework is the PlantVillage Extended dataset, which has been widely adopted in plant pathology research due to its broad taxonomic coverage and high-quality annotations. The dataset comprises 87,848 RGB images of individual leaves, representing 38 distinct disease categories across various plant species, including tomato, potato, grape, and corn. All images were standardized to <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixel resolution using bicubic interpolation to ensure consistent input dimensions across batches. Each image is paired with a pixel-level binary segmentation mask <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the presence of disease symptoms at pixel position (<italic toggle="yes">i,j</italic>). The dataset was partitioned into training (70%), validation (15%), and testing (15%) subsets using stratified sampling to maintain class distribution proportions. A quantitative summary of the disease class distribution entropy <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was computed using Shannon&#8217;s entropy formula, expressed as follows:<disp-formula id="FD1-plants-14-02634"><label>(1)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="1.em"/><mml:mi>where</mml:mi><mml:mspace width="1.em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the number of samples for class <italic toggle="yes">i</italic>, and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>38</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the total number of disease categories. The dataset demonstrates moderate class imbalance with an average per-class entropy of <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3.74</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which is addressed during training using class-weighted loss.</p><p>To assess cross-domain robustness and fine-grained segmentation accuracy under challenging visual conditions, we employ the CVPPP Leaf Segmentation dataset comprising 4477 RGB images of rosette plants captured under controlled greenhouse conditions. Unlike PlantVillage, this dataset focuses on leaf-level segmentation with densely clustered, frequently occluded leaves, making it ideal for evaluating boundary preservation and instance separation. Each image contains instance-level ground truth masks <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the number of leaves and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. To generate compatible binary disease masks for training, artificial lesions were synthesized using stochastic lesion functions <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, injected at randomized biologically plausible locations to simulate natural disease progression. Segmentation quality was quantified using instance-aware Intersection-over-Union (<inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>IoU</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and average Hausdorff distance (<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>HD</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) across test instances. The occlusion complexity <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mi>&#954;</mml:mi></mml:mrow></mml:math></inline-formula> was measured as follows:<disp-formula id="FD2-plants-14-02634"><label>(2)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#954;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Area</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#8745;</mml:mo><mml:msub><mml:mo>&#8899;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8800;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>Area</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This metric captures the degree of overlap among leaves, providing an analytical basis for benchmarking segmentation robustness under occlusion-heavy scenarios. To benchmark real-world adaptability, we compiled a custom agricultural field dataset containing 12,340 RGB images acquired from farmland across European and Southeast Asian climate zones. Images were captured using drone-mounted cameras and handheld smartphones under variable photometric conditions including direct sunlight, overcast skies, and nighttime illumination [<xref rid="B20-plants-14-02634" ref-type="bibr">20</xref>]. This dataset provides significant environmental diversity, with recorded photometric variation (<inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), background clutter index (<inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), and reflectance noise (<inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow></mml:math></inline-formula>) per image. Plant pathology experts manually annotated disease regions to generate binary masks <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Quality assurance via inter-annotator agreement analysis yielded a mean Cohen&#8217;s kappa score of <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#954;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.89</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Environmental variations were normalized using a domain-adaptive transformation:<disp-formula id="FD3-plants-14-02634"><label>(3)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
with <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> estimated from ambient light histograms.</p><p>This procedure ensures stable input distribution across varying capture settings, enabling more reliable feature learning under naturalistic agricultural conditions.</p><p><xref rid="plants-14-02634-t001" ref-type="table">Table 1</xref> provides a detailed comparison of the three datasets, highlighting their statistical characteristics and structural differences across key dimensions.</p><p>Collectively, these datasets provide a robust evaluation framework for segmentation accuracy and generalization capacity under heterogeneous conditions. Variations in annotation structure, visual complexity, and domain-specific characteristics establish a challenging evaluation benchmark for the proposed segmentation framework. By leveraging structured (PlantVillage), occlusion-intensive controlled (CVPPP), and real-world field-collected datasets, we enable comprehensive assessment of the model&#8217;s practical performance capabilities.</p><p>To maintain input consistency and training stability, we implemented rigorous image standardization. All input images were resized to 256 &#215; 256 pixels using bicubic interpolation, which preserves local gradients more effectively than bilinear or nearest-neighbor approaches. The resizing transformation is defined by the following function:<disp-formula id="FD4-plants-14-02634"><label>(4)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mn>256</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow><mml:mi>bicubic</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where I is the original image and I<inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:math></inline-formula> is the transformed image. Following resizing, pixel intensities were normalized using ImageNet statistics to align with the pretrained backbone feature distribution, applying the following:<disp-formula id="FD5-plants-14-02634"><label>(5)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the mean and standard deviation for channel <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. For the segmentation masks, we converted all annotated regions into binary format, where the pixel value is set to 1 if it falls within a disease-infected region, and 0 otherwise. Mathematically, the binary mask transformation is defined as follows:<disp-formula id="FD6-plants-14-02634"><label>(6)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the set of disease labels and M(x,y) is the original annotated label at pixel location (x,y).</p><p>The data cleaning and quality control pipeline consisted of multiple stages to ensure training and evaluation dataset integrity. First, a stratified sampling approach selected 10% of all data across disease classes for manual inspection. Trained annotators reviewed this subset using a standardized rubric assessing segmentation accuracy and annotation completeness. Annotation quality was validated through inter-annotator agreement analysis using Cohen&#8217;s Kappa coefficient (<inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mi>&#954;</mml:mi></mml:mrow></mml:math></inline-formula>) as the reliability metric. Let <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent observed agreement and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote chance-expected agreement as follows:<disp-formula id="FD7-plants-14-02634"><label>(7)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#954;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where values <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#954;</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> were considered acceptable, indicating substantial agreement among annotators. Images failing to meet this agreement threshold were flagged for reannotation or removal. Furthermore, corrupted images&#8212;defined as those with unreadable pixel arrays or compression artifacts&#8212;were filtered using a perceptual hash function and image entropy check. The entropy H of an image I was computed as:<disp-formula id="FD8-plants-14-02634"><label>(8)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>255</mml:mn></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the probability distribution of grayscale pixel intensity <italic toggle="yes">i</italic>; images with entropy <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> were considered low-information and discarded.</p><p>To further quantify dataset consistency, we introduced a statistical quality control matrix that assesses intra-class variance across normalized histograms, texture statistics, and structural similarity indices (SSIM). The class-wise histogram divergence was measured using Jensen&#8211;Shannon divergence <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between class centroids <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and image histograms <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD9-plants-14-02634"><label>(9)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8214;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For texture consistency analysis, Gray-Level Co-occurrence Matrix (GLCM) metrics including contrast, correlation, and entropy were computed. Outliers exceeding <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#177;</mml:mo><mml:mn>3</mml:mn><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> beyond class means were flagged. Structural Similarity Index Measure (SSIM) scores were computed pairwise within each disease class and averaged to quantify structural pattern coherence. All metrics were documented in a tabular quality report.</p><p>After preprocessing, the filtered dataset contained 85,763 validated samples with homogeneous input distributions across training (70%), validation (15%), and testing (15%) partitions. The final dataset was augmented with metadata descriptors including image entropy, histogram distance to class centroid, and expert confidence score. These were encoded in structured JSON format to support metadata-aware training and reproducibility auditing. Consistent preprocessing across PlantVillage, CVPPP, and field datasets minimized evaluation biases from procedural variations. This quantitatively validated pipeline ensures reliable model training and reproducible cross-domain evaluation in agricultural contexts.</p></sec><sec id="sec2dot2-plants-14-02634"><title>2.2. Evaluation&#160;Metrics</title><p>To rigorously assess the segmentation quality of plant disease regions, we employ a suite of performance metrics widely adopted in semantic segmentation benchmarks. The primary metric is Intersection over Union (IoU), which quantifies the pixel-wise overlap between the predicted segmentation mask and the ground truth. Given a set of predicted pixels P and ground truth pixels G, the IoU is defined as follows:<disp-formula id="FD10-plants-14-02634"><label>(10)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>IoU</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>&#8745;</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>&#8746;</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">TP</italic>, <italic toggle="yes">FP</italic>, and <italic toggle="yes">FN</italic> represent the number of true positives, false positives, and false negatives, respectively. The IoU is particularly sensitive to under-segmentation and over-segmentation, making it crucial for applications where accurate disease area delineation is essential for downstream treatment decisions.</p><p>Complementing IoU, the Dice coefficient offers an F1-like evaluation of segmentation performance, emphasizing the balance between precision and recall. Mathematically, the Dice coefficient is given by the following:<disp-formula id="FD11-plants-14-02634"><label>(11)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Dice</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>&#8745;</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Dice is especially robust to small object instances, which is advantageous in our task where early-stage disease symptoms may occupy minimal spatial regions. Moreover, unlike IoU, Dice score tends to be more lenient in overlapping predictions, which helps evaluate the network&#8217;s tolerance to slight boundary deviations.</p><p>Precision and recall are also analyzed to measure the model&#8217;s classification quality on a per-pixel basis. Precision (P) is defined as follows:<disp-formula id="FD12-plants-14-02634"><label>(12)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
and recall (R) as follows:<disp-formula id="FD13-plants-14-02634"><label>(13)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Recall</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>While precision quantifies the model&#8217;s ability to avoid false positives such as misclassifying healthy tissue as diseased, recall measures its capability to detect all pathological regions. Plant pathology applications frequently emphasize high recall to prevent overlooking potentially harmful symptoms, though this requires balancing against false positives that might prompt unnecessary treatments.</p><p>For boundary accuracy evaluation, the Hausdorff distance (HD) quantifies maximum contour deviation between predicted and ground-truth boundaries. Given two sets of points <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic> representing the predicted and ground truth boundaries, the directed Hausdorff Distance is defined as follows:<disp-formula id="FD14-plants-14-02634"><label>(14)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Hausdorff distance captures worst-case boundary error, making it essential for segmentation models in precision agriculture applications such as targeted pesticide application or robotic treatment systems. Lower HD values indicate superior boundary alignment, enabling accurate quantification of disease severity and spatial spread.</p><p>To complement accuracy metrics, computational efficiency was evaluated&#8212;a critical factor for real-time deployment in agricultural contexts. Floating Point Operations per Second (FLOPs) measure computational complexity through the number of multiply&#8211;add operations per forward pass. The theoretical FLOPs for a convolutional layer is calculated as follows:<disp-formula id="FD15-plants-14-02634"><label>(15)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>FLOPs</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the output height and width, <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is output channels, K is the kernel size, and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of input channels. In our model, sparse attention reduces attention-related FLOPs by replacing dense matrix operations with top-k selective computation, reducing theoretical complexity from O(<inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) to O(<inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) where <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8810;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Model size was quantified via total learnable parameters (millions) and peak GPU memory consumption (GB) during inference. These metrics determine deployability on edge devices including agricultural drones, smartphones, and robotic platforms. Our mobile variant demonstrates significant reductions in both memory footprint and parameter count without significant degradation in segmentation accuracy.</p><p>Real-time performance was evaluated using: inference time (in milliseconds per image), throughput (frames per second, FPS), and energy consumption (in millijoules per inference). These metrics are benchmarked under controlled environments using NVIDIA RTX 3090 GPUs and PyTorch&#8217;s built-in profiler. Energy consumption is estimated using GPU power profiles over multiple inference iterations, normalized by batch size and image resolution.</p><p>The inference latency T is computed as follows:<disp-formula id="FD16-plants-14-02634"><label>(16)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the inference time of the <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> sample, and throughput is given by the following:<disp-formula id="FD17-plants-14-02634"><label>(17)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Energy consumption per image is expressed as follows:<disp-formula id="FD18-plants-14-02634"><label>(18)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">P</italic> is average power draw (W) during inference. These metrics provide a comprehensive perspective on real-world feasibility, especially under energy-constrained field deployment conditions.</p></sec><sec id="sec2dot3-plants-14-02634"><title>2.3. Hardware and Software&#160;Environment</title><p>As illustrated in <xref rid="plants-14-02634-t002" ref-type="table">Table 2</xref>. The Sparse-MoE-SAM framework is deployed on a dedicated deep learning workstation optimized for multi-branch architectures and memory-intensive operations. Hardware specifications include the following: GPU: NVIDIA RTX 3090 (24GB GDDR6X VRAM) to support batched expert routing and sparse attention during backpropagation. CPU: Intel Core i9-11900K (8 cores/16 threads @ 3.5 GHz base) for efficient data loading/preprocessing. RAM: 64GB DDR4 @ 3200MHz to alleviate memory constraints during high-throughput augmentation.</p><p>The software stack is built on PyTorch (v1.13.0), selected for its flexibility in implementing custom attention masks and expert routing logic. CUDA 11.7 provides native GPU acceleration, leveraging Tensor Cores to significantly accelerate matrix operations in attention and expert modules. Python 3.9.7 serves as the core programming language for model definition, training loops, and evaluation scripts. We adopt the Apex library for mixed-precision training, Albumentations for advanced data augmentation, and TorchMetrics for standardized metric computation [<xref rid="B21-plants-14-02634" ref-type="bibr">21</xref>]. The entire development environment is containerized using Docker to ensure reproducibility and cross-system compatibility.</p><p>The implementation leverages PyTorch&#8217;s dynamic computation graph capabilities to integrate sparse attention and MoE without modifying the backpropagation pipeline. Sparse attention masks are constructed using top-k selection via PyTorch&#8217;s torch.topk() function, followed by zero-masking all non-selected attention scores. The MoE module is implemented using gated routing with token-level dispatch. Expert layers are constructed as parallel convolutional or transformer sub-networks, dynamically selected per token batch using scatter_add operations for efficient gradient accumulation. This dynamic routing strategy necessitates customized autograd functions for memory-efficient backward propagation.</p><p>Performance profiling uses PyTorch Profiler and NVIDIA Nsight to measure GPU memory footprint, FLOPs, and kernel execution time. These tools confirm that sparse attention reduces average memory usage by &#8764;34% compared to dense attention. Furthermore, the MoE module incurs negligible routing overhead (&lt;7% runtime increase versus standard feedforward layers) due to parallel expert computation. End-to-end training (including all ablation variants) is orchestrated via PyTorch Lightning for structured experiment tracking, reproducibility, and seamless TensorBoard integration.</p><p>The following is an overview of the hardware/software stack:</p><table-wrap position="anchor" id="plants-14-02634-t002" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t002_Table 2</object-id><label>Table 2</label><caption><p>Experimental hardware and software configuration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Environment<break/>Component</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specification</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Details</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA RTX 3090</td><td align="center" valign="middle" rowspan="1" colspan="1">24GB GDDR6X, CUDA cores optimized for tensor ops</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel Core i9-11900K</td><td align="center" valign="middle" rowspan="1" colspan="1">8-core, 16-thread, 3.5 GHz</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RAM</td><td align="center" valign="middle" rowspan="1" colspan="1">64 GB DDR4</td><td align="center" valign="middle" rowspan="1" colspan="1">3200 MHz, multi-channel memory</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OS</td><td align="center" valign="middle" rowspan="1" colspan="1">Ubuntu 20.04 LTS</td><td align="center" valign="middle" rowspan="1" colspan="1">Docker containerized execution</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Framework</td><td align="center" valign="middle" rowspan="1" colspan="1">PyTorch 1.13.0</td><td align="center" valign="middle" rowspan="1" colspan="1">With CUDA 11.7, cuDNN enabled</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mixed Precision</td><td align="center" valign="middle" rowspan="1" colspan="1">Apex AMP</td><td align="center" valign="middle" rowspan="1" colspan="1">Reduces memory usage, accelerates forward/backward pass</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training Management</td><td align="center" valign="middle" rowspan="1" colspan="1">PyTorch Lightning</td><td align="center" valign="middle" rowspan="1" colspan="1">Modular experiment design and logging</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Profiling</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PyTorch Profiler, Nsight Systems</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FLOPs, kernel time, memory bandwidth</td></tr></tbody></table></table-wrap></sec><sec id="sec2dot4-plants-14-02634"><title>2.4. Segmentation Performance&#160;Comparison</title><p>As illustrated in <xref rid="plants-14-02634-t003" ref-type="table">Table 3</xref>. Quantitative evaluation on the PlantVillage Extended dataset confirms the superior performance of our proposed approach across multiple metrics. The comprehensive comparison includes traditional segmentation architectures, modern transformer-based methods (SegFormer), and foundation models. The baseline U-Net achieves 86.3% IoU, 89.7% Dice, 88.4% precision, and 87.2% recall with 31.0 M parameters and 15.2 GFLOPs. Attention U-Net shows modest improvements at 88.1% IoU and 91.2% Dice with 34.9 M parameters and 18.7 GFLOPs. DeepLabV3+ achieves stronger performance (89.4% IoU, 92.1% Dice) using 41.3 M parameters and 22.8 GFLOPs. The transformer-based SegFormer reaches 90.2% IoU and 92.8% Dice but requires 64.1 M parameters and 31.5 GFLOPs.</p><p>Boundary accuracy assessment via Hausdorff Distance reveals significant contour precision improvements. Our full model achieves the lowest Hausdorff Distance (1.87 mm), representing 22.4% and 31.5% improvements over Standard SAM (2.41 mm) and U-Net (3.42 mm), respectively. This reduction demonstrates our sparse attention mechanism&#8217;s effectiveness in preserving sharp disease boundaries&#8212;critical for treatment planning and severity assessment. Progressive improvements from CNN-based to transformer-based methods highlight global context modeling importance, while our sparse refinement further enhances boundary delineation without computational overhead. The mobile variant maintains competitive accuracy (2.15 mm), confirming architectural optimizations preserve spatial precision.</p><p>As illustrated in <xref rid="plants-14-02634-t004" ref-type="table">Table 4</xref>. Cross-dataset performance analysis reveals robust generalization capabilities across diverse imaging conditions and data distributions. While all methods exhibit reduced performance when transitioning from controlled laboratory conditions (PlantVillage) to complex real-world scenarios (CVPPP and Custom Field), our approach maintains the smallest IoU performance gap. The 4.5-percentage-point decline from PlantVillage (94.2% IoU) to Custom Field (87.4% IoU) compares favorably to SAM&#8217;s 6.9-point decline and SegFormer&#8217;s 7.0-point reduction. This superior domain adaptation stems from the biologically motivated sparse attention patterns and adaptive specialization of expert modules, which intrinsically adapt to diverse symptom manifestations across imaging environments.</p><p>Complex Leaf Occlusion Performance Analysis: The CVPPP dataset provides an ideal testbed for evaluating performance under severe leaf occlusion conditions, with overlap ratios ranging from 15% to 85%. Our method achieved 89.7% IoU on CVPPP, demonstrating exceptional capability in handling overlapping structures. We conducted detailed analysis across three occlusion severity levels: mild (15&#8211;35% overlap), moderate (35&#8211;60% overlap), and severe (60&#8211;85% overlap). Under mild occlusion, our method maintains 92.3% IoU with only 1.9% degradation from isolated leaf scenarios. For moderate occlusion, IoU drops to 88.1% (6.1% degradation), while severe occlusion results in 84.2% IoU (10.0% degradation). Importantly, our dual-stage MoE decoder excels in these challenging scenarios through specialized expert routing: the first-stage experts focus on global context to identify partially visible leaf boundaries, while second-stage experts emphasize edge refinement and boundary disambiguation. The sparse attention mechanism particularly benefits occlusion handling by concentrating computational resources on visible leaf regions rather than occluded areas, leading to more accurate boundary delineation. Comparative analysis shows our method outperforms Standard SAM by 4.3% IoU under severe occlusion conditions, demonstrating the effectiveness of expert specialization for complex spatial reasoning tasks.</p><p>Low-Light Condition Performance: Agricultural imaging often occurs under suboptimal lighting conditions, including dawn/dusk field work, indoor greenhouse settings, and cloudy weather. We evaluated model robustness across synthetic low-light scenarios by systematically reducing image brightness and adding realistic noise patterns. Images were processed with brightness reduction factors of 0.7, 0.5, and 0.3 (representing moderate, low, and very low light conditions), combined with Gaussian noise (<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) to simulate sensor noise under high ISO settings. Under moderate low-light conditions (0.7&#215; brightness), our method maintains 91.8% IoU with only 2.4% degradation from normal lighting. Performance drops to 89.3% IoU (4.9% degradation) under low-light (0.5&#215; brightness) and 85.7% IoU (8.5% degradation) under very low-light conditions (0.3&#215; brightness). The sparse attention mechanism proves particularly valuable in these scenarios by concentrating computational resources on the most informative regions, effectively filtering out noise-dominated background areas. Our dual-stage MoE architecture adapts to varying lighting through expert specialization: certain experts become specialized for contrast enhancement while others focus on edge preservation under noise. Compared to Standard SAM, our method shows superior robustness with 3.1% better IoU under very low-light conditions, validating the framework&#8217;s applicability for practical agricultural deployment across diverse environmental conditions.</p><p>Analysis of Challenging Disease Types:While our method demonstrates consistent improvements across disease categories, certain pathological conditions present greater segmentation challenges. Mosaic Virus, despite showing the highest improvement margin, achieves the lowest absolute IoU (93.2%) among the evaluated diseases. This difficulty stems from viral symptoms manifesting as subtle color variations and irregular mottled patterns that lack clear boundaries, making precise segmentation inherently challenging. The sparse attention mechanism, while generally beneficial, occasionally struggles with such diffuse symptomatology where the entire leaf surface may be affected rather than discrete lesional areas. Similarly, Late Blight presents challenges due to its rapidly evolving nature, where symptoms progress from small water-soaked spots to large irregular patches with indistinct borders. Our analysis reveals that diseases with diffuse symptom patterns (Mosaic Virus: 93.2% IoU), rapidly changing morphology (Late Blight: 93.8% IoU), and subtle early-stage manifestations achieve lower performance compared to diseases with distinct, well-defined lesions like Rust (95.4% IoU) and Bacterial Spot (95.1% IoU). Future work should address these limitations through specialized expert training for handling gradual color transitions and incorporating temporal information for diseases with dynamic progression patterns.</p><p>Consistent improvement margins across disease types confirm the generalizability of our architectural innovations, indicating that learned specializations adapt effectively to varying pathological characteristics while maintaining specificity.</p><p>As illustrated in <xref rid="plants-14-02634-f001" ref-type="fig">Figure 1</xref>. Foundation model comparison indicates Standard SAM attains 91.7% IoU, 94.2% Dice, 93.1% precision, and 92.4% recall with substantial computational requirements (636.0 M parameters, 187.3 GFLOPs). FastSAM provides a more efficient alternative (90.8% IoU, 93.5% Dice) using 68.0 M parameters and 45.2 GFLOPs. Our proposed full-model achieves superior segmentation performance: 94.2% IoU, 96.1% Dice, 95.3% precision, and 94.8% recall with only 142.7 M parameters and 142.9 GFLOPs. This configuration yields a 23.7% computational cost reduction versus Standard SAM while improving IoU by 2.5 percentage points. The mobile variant maintains competitive performance (92.1% IoU, 94.7% Dice) with dramatic resource reduction (45.3 M parameters, 38.7 GFLOPs), representing 77.5% fewer parameters than the full-model without significant performance degradation.</p></sec><sec id="sec2dot5-plants-14-02634"><title>2.5. Computational Efficiency&#160;Analysis</title><p>The proposed method exhibits superior computational efficiency-segmentation performance trade-offs across evaluation dimensions. Comprehensive analysis confirms a 23.7% reduction in FLOPs with a simultaneous 2.5% IoU improvement versus baseline SAM. Memory efficiency analysis reveals 35% lower peak memory usage during inference, enabling deployment on resource-constrained agricultural devices with limited VRAM. Inference speed evaluation shows 2.3&#215; faster processing than standard SAM (102 ms vs. 234 ms per 256 &#215; 256 image), facilitating real-time field deployment for agricultural monitoring. Computational complexity analysis confirms sparse attention&#8217;s theoretical advantages: dense attention exhibits prohibitive quadratic scaling for high-resolution imagery, while our sparse approach maintains linear scaling.</p><p>As illustrated in <xref rid="plants-14-02634-f002" ref-type="fig">Figure 2</xref>. Memory usage comparison shows our method requires 6.7 GB peak memory versus SAM&#8217;s 15.8 GB, enabling deployment on consumer-grade agricultural research GPUs. <xref rid="plants-14-02634-t005" ref-type="table">Table 5</xref> details practical deployment characteristics across segmentation methods. Traditional CNN-based methods (e.g., U-Net) consume only 2.1 GB but achieve limited accuracy, whereas foundation models (e.g., SAM) require 15.8 GB. Our approach balances this trade-off, requiring 6.7 GB (full model) with state-of-the-art performance.</p><p>To evaluate real-world deployment feasibility, we conducted extensive performance testing on resource-constrained devices commonly used in agricultural settings. On a NVIDIA Jetson Xavier NX (6 GB RAM, 384 CUDA cores), our mobile variant achieves 92.1% IoU with 3.2 s inference time per 256 &#215; 256 image, compared to standard SAM which fails to run due to memory constraints. The mobile variant successfully processes images on a Jetson Nano (4 GB RAM) with 4.8 s inference time while maintaining 91.3% IoU. On smartphone-grade hardware (Snapdragon 855 with Adreno 640 GPU), our optimized model achieves 89.7% IoU with 6.1 s processing time using TensorFlow Lite optimization. These results demonstrate practical viability for edge deployment in precision agriculture applications where real-time processing is less critical than accuracy and device compatibility.</p><p>To reduce the model&#8217;s computational load, we apply pruning techniques that reduce the number of parameters and operations while maintaining accuracy. Specifically, sparse attention mechanisms are employed, which allow the model to focus only on the most relevant features, significantly lowering the model size. For mobile deployment, we convert the model into TensorFlow Lite format or PyTorch Mobile, both of which optimize the model for inference on edge devices. These tools provide optimized kernels for mobile GPUs, ensuring efficient resource use. Smartphones or drones equipped with the optimized model can be used by field scouts for rapid disease identification during routine inspections. The model processes the leaf images locally, offering instant disease detection without requiring internet connectivity.</p><p>Mobile Device Application Scenarios: Our framework enables diverse practical deployment scenarios across agricultural value chains. Scenario 1&#8212;Field Scout Applications: Agricultural scouts equipped with smartphones can capture leaf images during routine field inspections. The mobile variant processes images locally within 6.1 s, providing immediate disease identification without requiring internet connectivity. This enables rapid decision-making for treatment timing and resource allocation in remote farming areas. Scenario 2&#8212;Farmer Decision Support: Small-scale farmers using entry-level smartphones can photograph suspected disease symptoms for on-device analysis. The compressed model provides actionable recommendations about disease severity and treatment urgency, supporting precision application of pesticides and reducing unnecessary chemical usage. Scenario 3&#8212;Agricultural Extension Services: Extension agents can use tablet devices during farmer training sessions, demonstrating disease identification in real-time and building local diagnostic capacity. The mobile framework processes multiple leaf samples during field workshops, facilitating hands-on learning experiences. Scenario 4&#8212;Supply Chain Quality Control: Post-harvest facilities can deploy our framework on mobile devices for quality assessment of incoming produce, automatically flagging potentially diseased materials before storage or distribution. Processing times of 4&#8211;6 s per image enable integration into existing quality control workflows without significant throughput impacts.</p><p>The mobile variant achieves exceptional 3.1 GB memory efficiency (80.4% reduction vs. SAM), enabling edge deployment under 4&#8211;8 GB GPU constraints. Inference time analysis shows our mobile variant processes images in 58 ms (4.0&#215; faster than SAM&#8217;s 234 ms) on high-end hardware. Throughput reaches 17.2 frames/s, exceeding the 15 FPS threshold for smooth real-time processing while maintaining competitive accuracy. Energy consumption analysis reveals our mobile variant uses 189 mJ/inference versus SAM&#8217;s 1024 mJ, enabling extended operation in battery-powered field deployments. All variants exhibit speed-up improvements, with our full model achieving 2.3&#215; acceleration over SAM alongside superior accuracy, validating sparse attention and expert routing optimizations.</p></sec><sec id="sec2dot6-plants-14-02634"><title>2.6. Expert Utilization&#160;Analysis</title><p>Expert selection frequency analysis reveals distinct specialization patterns with balanced utilization across processing stages. During Stage 1 (coarse segmentation), standard convolution experts account for 28.4% of selections, demonstrating effectiveness in general pattern recognition. Dilated convolution experts achieve highest utilization (31.7%), reflecting superior capability for large disease regions and contextual relationships across extended spatial neighborhoods. Depthwise separable convolution experts maintain 25.9% utilization, while specialized context-aware experts contribute 14.0% for complex spatial reasoning.</p><p>Stage 2 refinement exhibits specialized adaptation: Standard convolution utilization increases to 35.2%, highlighting their role in boundary refinement. Dilated convolution decreases to 23.1% as large-scale contextual processing diminishes. Depthwise separable convolution achieves peak utilization (41.7%), confirming efficiency advantages for fine-grained boundary delineation. Balanced utilization prevents expert collapse, and stage-specific specialization emerges through training without explicit supervision, validating the gating mechanism&#8217;s effectiveness.</p><p>Load balancing analysis demonstrates the auxiliary balance loss effectiveness in maintaining expert diversity during training. Expert entropy increases from 1.89 (initial) to 1.99 (final), approaching the theoretical maximum (<inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>(4) = 2.0) for four experts, indicating near-optimal load distribution. The Gini coefficient decreases from 0.42 to 0.33, reflecting improved utilization equality [<xref rid="B18-plants-14-02634" ref-type="bibr">18</xref>]. Load variance reduces from 0.087 to 0.058, mitigating expert utilization skewness. Balance loss consistently declines from 0.034 to 0.019, confirming the gating network learns balanced routing without explicit supervision. Crucially, expert collapse remains at 0% throughout training, confirming our mechanism prevents the single-expert degradation characteristic of mixture-of-experts architectures in specialized domains.</p><p>As illustrated in <xref rid="plants-14-02634-t006" ref-type="table">Table 6</xref>. Attention diversity and sparsity analysis reveal specialized patterns across the eight heads of our sparse attention mechanism. Sparsity ratios consistently exceed 88.9% (average: 90.5%), confirming effective pruning of irrelevant connections while preserving critical spatial relationships. Head 5 achieves peak sparsity (92.1%) when processing texture variations, indicating selective attention for textural disease symptoms. Entropy scores range from 2.06 ti 2.19 (average: 2.11), demonstrating balanced attention distribution within retained connections.</p><p>Specialization analysis shows biologically aligned focus: Head 2 exhibits strongest pathologist attention correlation (0.87) for necrotic regions, validating detection of advanced symptoms. Head 1 effectively captures fine lesions (0.84 correlation), crucial for early detection. Head 7 shows 0.86 correlation for disease progression patterns, indicating temporal development identification. Head 8&#8217;s lower background correlation (0.78) reflects reduced alignment need. The average 0.83 correlation confirms our sparse attention learns biologically relevant representations, reconciling computational efficiency with plant pathology expertise.</p></sec><sec id="sec2dot7-plants-14-02634"><title>2.7. Ablation Study&#160;Results</title><p>As illustrated in <xref rid="plants-14-02634-t007" ref-type="table">Table 7</xref>. Comprehensive ablation study systematically evaluates each component&#8217;s contribution. Baseline SAM establishes foundation performance (91.7% IoU, 187.3 GFLOPs). Incorporating sparse attention improves IoU to 92.4% while reducing computation to 156.2 GFLOPs (0.7-pp IoU gain, 16.6% FLOPs reduction), confirming sparse attention maintains quality with strategic pruning.</p><p>Adding first-stage Mixture of Experts (MoE) elevates IoU to 93.1% at 148.7 GFLOPs (cumulative 1.4-pp IoU gain, 20.6% FLOPs reduction versus baseline). Enhanced ASPP module integration boosts IoU to 93.6% (145.3 GFLOPs), validating its multi-scale processing value. Final second-stage MoE decoder achieves 94.2% IoU and 142.9 GFLOPs (cumulative 2.5-pp IoU gain, 23.7% FLOPs reduction). Progressive improvements demonstrate meaningful contributions: sparse attention enables greatest computation savings, while each MoE stage adds &#8776;0.7-pp IoU through task-specialized feature processing.</p><p>Statistical Significance Analysis: To validate the reliability of our ablation results, we conducted statistical significance testing across five independent training runs with different random seeds. Using paired <italic toggle="yes">t</italic>-tests with Bonferroni correction for multiple comparisons, we found all IoU improvements to be statistically significant at <italic toggle="yes">p</italic> &lt; 0.01 level. Specifically: sparse attention addition (92.4% vs. 91.7%, <italic toggle="yes">p</italic> = 0.003), first-stage MoE integration (93.1% vs. 92.4%, <italic toggle="yes">p</italic> = 0.007), enhanced ASPP incorporation (93.6% vs. 93.1%, <italic toggle="yes">p</italic> = 0.009), and final second-stage MoE (94.2% vs. 93.6%, <italic toggle="yes">p</italic> = 0.004). The confidence intervals (95% CI) for each improvement are: sparse attention [0.4%, 1.0%], first-stage MoE [0.4%, 1.0%], enhanced ASPP [0.2%, 0.8%], and second-stage MoE [0.3%, 0.9%]. These results confirm that observed performance gains are not due to random variation and represent genuine architectural contributions. Furthermore, ANOVA analysis (F(4,20) = 127.3, <italic toggle="yes">p</italic> &lt; 0.001) confirms significant differences between configurations, with effect size (<inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#951;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> = 0.962) indicating that architectural choices explain 96.2% of performance variance.</p><p>Extended Parameter Sensitivity Analysis: To provide comprehensive understanding of design choices, we conducted extensive parameter sensitivity studies. Sparse Attention Top-K Analysis: We systematically varied the top-k retention ratio <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> from 0.05 to 0.3. Results show optimal performance at <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.1 (94.2% IoU), with degradation at both extremes: <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.05 achieves 92.1% IoU due to information loss, while <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.3 reaches 93.4% IoU with increased computational cost. The sweet spot at <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.1 balances accuracy retention with 23.7% FLOPs reduction.</p><p>MoE Expert Count Variation: We evaluated expert numbers from 2 to 8 for both stages. Stage 1 performs optimally with 4 experts (94.2% IoU); fewer experts (2&#8211;3) achieve 92.8&#8211;93.5% IoU due to insufficient specialization, while more experts (6&#8211;8) show marginal improvement (94.0&#8211;94.1% IoU) with significantly increased parameters. Stage 2 benefits from 3 experts (94.2% IoU) versus 2 experts (93.7% IoU) or 4+ experts (94.0&#8211;94.1% IoU with parameter overhead).</p><p>Gating Network Architecture: We tested gating depths from 1 to 4 layers. Single-layer gating achieves 93.1% IoU with rapid routing decisions but limited capacity for complex feature analysis. Two-layer gating (our choice) reaches 94.2% IoU, optimally balancing routing sophistication with computational efficiency. Deeper networks (3&#8211;4 layers) provide minimal gains (94.0&#8211;94.1% IoU) while increasing overhead.</p><p>Loss Function Weight Sensitivity: Balance loss weight <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> analysis shows optimal performance at 0.01; lower values (0.001&#8211;0.005) lead to expert collapse (91.8&#8211;92.6% IoU), while higher values (0.05&#8211;0.1) over-regularize expert diversity (92.4&#8211;93.1% IoU). Sparsity weight <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> optimal at 0.001 balances attention focus with flexibility.</p></sec><sec id="sec2dot8-plants-14-02634"><title>2.8. Case&#160;Study</title><p>As illustrated in <xref rid="plants-14-02634-f003" ref-type="fig">Figure 3</xref>, comprehensive visualization analysis reveals sophisticated internal mechanisms of our Sparse-MoE-SAM framework through attention pattern examination and expert activation mapping.</p><p>Attention Map Visualization: Sparse attention visualization shows focused activation patterns (peak weights: 0.85&#8211;0.94) precisely aligned with disease-affected regions, with minimal activation (&lt;0.08) in healthy tissues. <xref rid="plants-14-02634-f003" ref-type="fig">Figure 3</xref>A displays attention heatmaps overlaid on original images for representative disease cases: bacterial spot detection shows concentrated attention on circular lesions with clear boundaries, late blight attention maps highlight water-soaked margins characteristic of fungal infections, and mosaic virus attention patterns reveal the model&#8217;s ability to detect subtle color variations across leaf surfaces. Cross-validation with expert pathologist annotations confirms strong spatial alignment (Pearson correlation &gt;0.84 across disease types). Multi-head attention analysis demonstrates specialized preferences: Head 1 focuses on textural changes (entropy-based features), Head 2 emphasizes color transitions (HSV space variations), Heads 3&#8211;4 capture boundary characteristics (gradient-based features), and Heads 5&#8211;8 integrate multi-scale contextual information.</p><p>Expert Routing Visualization: <xref rid="plants-14-02634-f003" ref-type="fig">Figure 3</xref>B illustrates dynamic expert activation patterns through routing probability heatmaps. For bacterial spot cases, dilated convolution experts show 73% activation in central lesion areas requiring broad contextual analysis, while standard convolution experts dominate edge regions (68% activation) for precise boundary delineation. Depthwise separable experts preferentially activate in computationally constrained scenarios (mobile deployment), achieving 81% routing efficiency. Temporal stability analysis across video sequences shows consistent expert selection patterns (correlation coefficient &gt;0.79), indicating robust specializations that generalize across samples while adapting to local feature complexity. Stage-wise routing analysis reveals progressive specialization: Stage 1 achieves coarse disease localization with balanced expert utilization (25&#8211;35% per expert), while Stage 2 shows task-specific expert dominance based on lesion characteristics.</p><p>Expert activation heatmaps illustrate adaptive routing: dilated convolution experts preferentially activate in necrotic regions requiring broad context, while depthwise separable experts show increased utilization in efficiency-critical areas. Temporal consistency analysis reveals stable expert selection patterns (&gt;0.79 correlation coefficient), indicating robust specializations generalizing across samples while sensitive to local feature complexity.</p><p>Qualitative segmentation results demonstrate superior boundary accuracy and reduced false positives (3.2%) in challenging agricultural scenarios. Complex background discrimination achieves 93.8% boundary accuracy against environmental interference (soil, shadows, leaf artifacts). Dual-stage MoE refinement capability is evident in overlapping symptom scenarios: Stage 1 establishes 89.4% boundary accuracy, Stage 2 refines to 91.7% through specialized routing. Early-stage lesion detection shows 90.9% accuracy for sub-15-pixel lesions, outperforming human perception. Variable illumination analysis maintains &gt;89.6% boundary accuracy across diverse lighting, color temperature, and shadow conditions. These results validate that our architecture enhances computational efficiency, biological relevance, and clinical utility, establishing new standards for interpretable plant disease segmentation.</p></sec></sec><sec sec-type="discussion" id="sec3-plants-14-02634"><title>3. Discussion</title><p>This research addresses deployment challenges for high-precision plant disease segmentation in agricultural settings where computational resources are limited. Field conditions introduce complex backgrounds, variable lighting, and sparse disease symptoms that challenge traditional dense attention models.</p><p>Prior studies predominantly employ dense attention frameworks (e.g., ViT, SAM), effective for general segmentation but exhibiting quadratic computational complexity that hinders real-time edge deployment. We introduce Sparse-MoE-SAM, integrating sparse attention with a Mixture-of-Experts (MoE) decoding structure to address computational bottlenecks and biological feature sparsity [<xref rid="B22-plants-14-02634" ref-type="bibr">22</xref>,<xref rid="B23-plants-14-02634" ref-type="bibr">23</xref>].</p><p>The key novelty of our approach lies in three fundamental departures from existing methods. First, unlike traditional dense attention mechanisms that compute attention weights for all pairwise token interactions, our sparse attention mechanism selectively attends to only the top-k most relevant tokens per query, reducing computational complexity from O(n<sup>2</sup>) to O(nk). This bio-inspired design mirrors how plant pathologists focus attention on disease-relevant regions rather than processing entire leaf surfaces uniformly. Second, our dual-stage MoE decoder employs task-conditional expert routing where different experts specialize in processing distinct morphological patterns (e.g., necrotic spots, chlorotic regions, fungal structures). This contrasts with standard single-decoder architectures that use uniform processing across all spatial regions. Third, our framework integrates adaptive sparsity patterns that dynamically adjust based on disease symptom distribution, unlike fixed sparsity masks used in prior efficient attention methods.</p><p>Our domain-adaptive architecture leverages biologically inspired structural priors&#8212;particularly the spatial sparsity and multi-scale distribution of disease symptoms. By activating attention only in semantically relevant regions, our model improves mean Intersection over Union (mIoU) by 2.5% while reducing FLOPs by 23.7% compared to SAM. It achieves 2.3&#215; faster inference with 35% lower peak memory usage for 256 &#215; 256 images, enabling deployment on consumer-grade GPUs.</p><p>The framework centers on a two-stage MoE decoder that progressively refines boundaries under field conditions [<xref rid="B24-plants-14-02634" ref-type="bibr">24</xref>]. Gate-controlled expert routing allocates computation according to spatial complexity, capturing fine-grained patterns under occlusion or illumination noise. This hierarchical specialization advances pixel-level segmentation beyond dense architectures. Our approach demonstrates strong generalization across datasets (PlantVillage, field-collected data), highlighting practical alignment of architectural design with agricultural domain knowledge.</p><p>Several limitations of our study warrant discussion. First, our evaluation is primarily conducted on controlled datasets (PlantVillage) with high-quality annotations, which may not fully represent the complexity of real-world field conditions including soil occlusion, water droplets, and pest damage. Second, the sparse attention mechanism, while computationally efficient, may occasionally miss subtle disease symptoms that manifest as diffuse patterns across large leaf areas. Third, our MoE architecture requires careful hyperparameter tuning for optimal expert utilization, and the learned expert specializations may not transfer effectively across significantly different plant species or disease types. Fourth, the model&#8217;s performance on low-resolution imagery (below 128 &#215; 128 pixels) degrades substantially, limiting applicability in scenarios with poor camera quality or distant capture distances. Additionally, computational benefits of sparse attention are most pronounced on modern GPUs with efficient sparse matrix operations; older hardware may not fully realize these advantages. Future work should explore automated expert specialization via neural architecture search, extension to semi-supervised learning regimes to reduce annotation costs, and multimodal sensory data integration (thermal/hyperspectral imaging) to enhance environmental variability handling [<xref rid="B25-plants-14-02634" ref-type="bibr">25</xref>,<xref rid="B26-plants-14-02634" ref-type="bibr">26</xref>].</p></sec><sec id="sec4-plants-14-02634"><title>4. Materials and&#160;Methods</title><sec id="sec4dot1-plants-14-02634"><title>4.1. Overall Framework&#160;Architecture</title><p>The Sparse-MoE-SAM framework maintains SAM&#8217;s encoder generalization while incorporating two domain-specific adaptations: (1) biologically gated sparse attention, and (2) task-conditional MoE architecture [<xref rid="B27-plants-14-02634" ref-type="bibr">27</xref>,<xref rid="B28-plants-14-02634" ref-type="bibr">28</xref>]. These enable efficient, context-aware segmentation with enhanced discriminative capability for lesion regions. As <xref rid="plants-14-02634-f004" ref-type="fig">Figure 4</xref> illustrates, our end-to-end system comprises four integrated modules:A sparsity-constrained ViT encoder guided by lesion distribution priors, An adaptive MoE module embedded in the encoder&#8211;decoder pathway, A sparse-enhanced ASPP module extracting multi-scale lesion features and A dual-stage decoder with progressive boundary optimization via stage-wise expert routing [<xref rid="B29-plants-14-02634" ref-type="bibr">29</xref>,<xref rid="B30-plants-14-02634" ref-type="bibr">30</xref>]. These components are jointly optimized through a composite loss function that simultaneously reinforces segmentation fidelity, expert utilization balance, and attention sparsity. This alignment ensures internal representations satisfy both computational constraints (O(nk) complexity) and pathological feature semantics required for agricultural deployment.</p><p>The Sparse Vision Transformer Encoder modifies SAM&#8217;s ViT backbone by replacing dense multi-head self-attention with learnable sparsity patterns. Within each attention head, we dynamically select the <italic toggle="yes">k</italic> most relevant tokens per query position based on input-adaptive importance scores. Given an input token sequence <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the standard attention mechanism computes as follows:<disp-formula id="FD19-plants-14-02634"><label>(19)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the projected query, key, and value matrices. We redefine this as a top-<italic toggle="yes">k</italic> sparse attention mechanism, where for each query vector <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, only the top-<italic toggle="yes">k</italic> keys <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are selected based on the magnitude of their dot-product similarity. The sparsified attention formulation is presented in Equation (<xref rid="FD20-plants-14-02634" ref-type="disp-formula">20</xref>) as follows:<disp-formula id="FD20-plants-14-02634"><label>(20)</label><mml:math id="mm81" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>SparseAttention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The sparse attention reduces complexity from <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>&#961;</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>&#8810;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) and <italic toggle="yes">d</italic> is the feature dimension. Critically, the sparsity mask <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is dynamically regenerated per attention head and per input sample, enabling input-adaptive sparsity patterns aligned with lesion distributions. During training, sparsity is enforced as a hard constraint via Gumbel&#8211;Softmax-based differentiable top-<italic toggle="yes">k</italic> approximation.</p><p>To enhance feature processing specialization, we insert an MoE module between encoder and decoder stages. It consists of M expert networks <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, each with distinct inductive biases (e.g., receptive fields, texture sensitivity) optimized for specific spatial/morphological symptom scales. A gating network <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>&#8594;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> routes input features <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>w</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> by computing routing weights: <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The final output aggregates expert contributions as specified in Equation (<xref rid="FD21-plants-14-02634" ref-type="disp-formula">21</xref>) as follows:<disp-formula id="FD21-plants-14-02634"><label>(21)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>MoE</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> controls sparsity at the expert level, and only top-<italic toggle="yes">k</italic> experts are selected per token. To encourage balanced expert usage and prevent expert collapse, we introduce a load balancing loss term, The corresponding expression is provided in Equation (<xref rid="FD22-plants-14-02634" ref-type="disp-formula">22</xref>) as follows:<disp-formula id="FD22-plants-14-02634"><label>(22)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the fraction of tokens routed to expert <italic toggle="yes">i</italic>. The routing network trains jointly with the model, enabling dynamic adaptation to disease regions&#8217; visual complexity and severity.</p><p>Subsequent to the encoder, we integrate an Enhanced ASPP module with sparse attention to capture multi-scale context efficiently. This module employs dilated convolutions at rates <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mn>18</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to generate multi-resolution features <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which are concatenated and refined via sparse attention. The attention-enhanced fusion computes as follows:<disp-formula id="FD23-plants-14-02634"><label>(23)</label><mml:math id="mm97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>SparseAttention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This selective refinement operates over concatenated features while maintaining linear complexity. We apply projection layer <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mi>d</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to align fused outputs to the target embedding space, ensuring capture of both fine-grained and large-scale disease cues without excessive computation.</p><p>The Dual-Stage MoE Decoder progressively refines segmentation masks through cascaded stages. Stage 1 processes encoder-generated coarse semantic maps to predict initial disease regions. Stage 2 refines boundaries and recovers lesion structures. Denoting stage outputs as <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Each decoding stage utilizes its own MoE block, The expressions for <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are given in Equation (<xref rid="FD24-plants-14-02634" ref-type="disp-formula">24</xref>) as follows:<disp-formula id="FD24-plants-14-02634"><label>(24)</label><mml:math id="mm103" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msubsup><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msubsup><mml:mi>&#946;</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>D</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the <italic toggle="yes">i</italic>-th decoder expert at stage <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents top-<italic toggle="yes">k</italic> routing scores from the gating network. The stage-2 decoder concatenates encoder outputs with stage-1 decoder features, enabling hierarchical refinement.</p><p>The complete Sparse-MoE-SAM framework optimizes a composite loss function combining segmentation fidelity, attention sparsity, and expert diversity. The objective function is defined as follows:<disp-formula id="FD25-plants-14-02634"><label>(25)</label><mml:math id="mm107" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>sparse</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> includes binary cross-entropy and Dice coefficient terms, as shown in Equation (<xref rid="FD26-plants-14-02634" ref-type="disp-formula">26</xref>) as follows:<disp-formula id="FD26-plants-14-02634"><label>(26)</label><mml:math id="mm109" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>BCE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>&#8745;</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
and <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>sparse</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is an <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#8467;</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>-norm regularization applied to the attention maps, as defined in Equation (<xref rid="FD27-plants-14-02634" ref-type="disp-formula">27</xref>) as follows:<disp-formula id="FD27-plants-14-02634"><label>(27)</label><mml:math id="mm112" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>sparse</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes attention weights between query <italic toggle="yes">i</italic> and key <italic toggle="yes">j</italic>. This sparsity regularization term enforces localized attention patterns, aligning with plant diseases&#8217; hierarchical visual features. Experimental validation (<xref rid="sec2dot7-plants-14-02634" ref-type="sec">Section 2.7</xref>) confirms this formulation yields quantifiable improvements in segmentation performance and computational efficiency, enhancing suitability for agricultural field settings.</p></sec><sec id="sec4dot2-plants-14-02634"><title>4.2. Sparse Attention&#160;Mechanism</title><p>Traditional vision transformer self-attention mechanisms exhibit O(n2) computational complexity due to dense pairwise token interactions [<xref rid="B13-plants-14-02634" ref-type="bibr">13</xref>]. For an input token sequence <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> where <italic toggle="yes">n</italic> denotes spatial tokens (e.g., image patches) and <italic toggle="yes">d</italic> represents feature dimensionality, standard attention computes as follows:<disp-formula id="FD28-plants-14-02634"><label>(28)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As illustrated in <xref rid="plants-14-02634-f005" ref-type="fig">Figure 5</xref>, where <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are learned linear projections, the resulting <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> attention map incurs prohibitive computational cost for high-resolution images. To address this, we propose sparsity-constrained attention that limits each query <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to attend only to a top-<italic toggle="yes">k</italic> subset of keys <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the indices of the <italic toggle="yes">k</italic> most relevant positions for <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> based on dot product similarity as follows:<disp-formula id="FD29-plants-14-02634"><label>(29)</label><mml:math id="mm124" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>SparseAttention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As illustrated in <xref rid="plants-14-02634-f006" ref-type="fig">Figure 6</xref>, this formulation reduces the attention cost to O(nkd), with <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>&#961;</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>&#8810;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The sparsity mask <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is dynamically determined for each sample and each attention head. To make the selection differentiable, we employ a Gumbel&#8211;Softmax-based sampling strategy, approximating top-k through the following:<disp-formula id="FD30-plants-14-02634"><label>(30)</label><mml:math id="mm128" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>top</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Gumbel</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where Gumbel noise is injected to enable gradient propagation through the argmax-like selection operation.</p><p>Gumbel-TopK Strategy Justification: The choice of Gumbel-TopK selection over alternative sparse attention strategies is motivated by several critical advantages for plant disease segmentation. First, differentiability: Standard top-k selection involves discrete argmax operations that break gradient flow during backpropagation. Gumbel&#8211;Softmax provides a continuous relaxation that maintains gradient propagation while approximating discrete selection, essential for end-to-end training of our MoE architecture. Second, adaptive exploration: Gumbel noise introduces controlled stochasticity that prevents the attention mechanism from prematurely converging to suboptimal sparse patterns, particularly important given the diverse morphological patterns of plant diseases. Third, biological plausibility: The noise-injected selection mimics the variability in human visual attention when examining diseased leaves, where experts may focus on different but equally relevant symptom regions. Fourth, computational stability: Unlike learned sparse patterns that can collapse during training, Gumbel-TopK maintains consistent sparsity ratios throughout optimization. We empirically validated this choice by comparing against fixed sparse patterns (strided, random), learned sparse masks, and other differentiable selection methods (concrete relaxation, straight-through estimators), finding Gumbel-TopK achieves the best trade-off between accuracy retention (94.2% vs. 91.8% for nearest alternatives) and computational reduction (23.7% FLOPs savings).</p><p>Spatial and Semantic Correlation Evidence: Our design intuition for Gumbel-TopK selection is supported by extensive analysis of plant disease spatial patterns. Disease symptoms exhibit strong spatial autocorrelation (Moran&#8217;s I = 0.73 &#177; 0.12 across disease types), meaning neighboring pixels tend to have similar disease-related features. This spatial coherence justifies top-k selection based on attention similarity scores, as semantically relevant regions cluster spatially. Furthermore, semantic analysis reveals that disease-affected areas demonstrate consistent feature correlations: pixels within lesion boundaries show high cosine similarity (0.82 &#177; 0.08) in feature space, while healthy-diseased boundaries exhibit distinct feature gradients. The Gumbel noise component addresses the challenge of ambiguous boundary regions where multiple pixels may have similar attention scores, preventing deterministic selection that could miss subtle but critical symptom variations. Validation experiments confirm that our adaptive selection captures 94.3% of expert-annotated critical regions while examining only 10% of spatial locations, demonstrating the effectiveness of our sparsity-based approach for plant pathology applications.</p><p>Designing effective sparsity patterns is critical for preserving semantic relevance while achieving computational efficiency. Unlike fixed strategies (strided/blockwise masking), our adaptive sparsity dynamically selects the top-<italic toggle="yes">k</italic> keys per query based on content similarity (<xref rid="plants-14-02634-f007" ref-type="fig">Figure 7</xref>). Specifically, we compute attention logits <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> and apply Gumbel noise <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for differentiable top-<italic toggle="yes">k</italic> selection as follows:<disp-formula id="FD31-plants-14-02634"><label>(31)</label><mml:math id="mm131" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi>Gumbel</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
and the set <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of top-<italic toggle="yes">k</italic> indices is selected as follows:<disp-formula id="FD32-plants-14-02634"><label>(32)</label><mml:math id="mm133" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:mrow><mml:mi>top</mml:mi><mml:mo>-</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This approach yields highly flexible attention distributions that adapt to spatial variations in disease patterns. To prevent attention collapse, where all queries attend to a fixed set of dominant keys, we regularize the sparsity pattern diversity through an entropy-based loss term as follows:<disp-formula id="FD33-plants-14-02634"><label>(33)</label><mml:math id="mm134" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>diversity</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As shown in <xref rid="plants-14-02634-f008" ref-type="fig">Figure 8</xref>, where <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes normalized attention weights. Empirically, we observe that this regularization fosters diverse attention while maintaining accuracy. We analyze the theoretical and empirical complexity benefits brought by the sparse attention formulation. In dense attention, the computational cost is dominated by the attention matrix calculation <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, incurring <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> complexity. As shown in <xref rid="plants-14-02634-f009" ref-type="fig">Figure 9</xref>, by contrast, the sparse attention mechanism reduces the computational complexity to <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>&#961;</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The relative reduction factor is given in Equation (<xref rid="FD34-plants-14-02634" ref-type="disp-formula">34</xref>) as follows:<disp-formula id="FD34-plants-14-02634"><label>(34)</label><mml:math id="mm141" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Reduction</mml:mi><mml:mspace width="4.pt"/><mml:mi>Factor</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>&#961;</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For sparsity ratio <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the model achieves <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> theoretical acceleration in attention computation. Memory footprint reduces proportionally to <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> per attention head (vs. <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for dense attention), storing only <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> weights. As shown in <xref rid="sec2dot5-plants-14-02634" ref-type="sec">Section 2.5</xref>, the proposed model achieves a 23.7% FLOPs reduction compared to dense SAM while maintaining or exceeding segmentation accuracy.</p></sec><sec id="sec4dot3-plants-14-02634"><title>4.3. Mixture of Experts&#160;Framework</title><p>To address spatial and morphological heterogeneity in plant disease symptoms, our Mixture of Experts (MoE) framework integrates specialized branches optimized for distinct feature extraction modalities. This design enables adaptive processing across leaf textures, environmental variability, and disease progression stages. Standard Convolution Experts employ fixed-receptive-field blocks optimized for high-resolution morphological feature extraction. These excel at isolating early-stage symptoms like chlorosis and necrosis by capturing fine-grained patterns [<xref rid="B32-plants-14-02634" ref-type="bibr">32</xref>]. Dilated Convolution Experts utilize exponentially spaced dilation rates to capture multi-scale contextual dependencies. Their expanded receptive fields model co-occurring symptoms and inter-leaf disease propagation. Depthwise Separable Experts implement lightweight architectures that maximize computational efficiency. Optimized for low-contrast, high-noise regions (e.g., soil-dust interference), they minimize processing overhead while preserving feature discriminability.</p><p>Formally, each expert <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:mrow></mml:msup><mml:mo>&#8594;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as a differentiable transformation over a spatial input tensor <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For the standard convolution expert, its definition is given in Equation (<xref rid="FD35-plants-14-02634" ref-type="disp-formula">35</xref>) as follows:<disp-formula id="FD35-plants-14-02634"><label>(35)</label><mml:math id="mm150" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>std</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>BN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>&#8727;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution kernel, BN is batch normalization, and ReLU denotes the rectified linear unit. For the dilated expert, we have the following:<disp-formula id="FD36-plants-14-02634"><label>(36)</label><mml:math id="mm153" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>dil</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>BN</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>F</mml:mi><mml:msub><mml:mo>&#8727;</mml:mo><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>r</mml:mi><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
with r denoting dilation by rate r. The depthwise separable expert is decomposed as follows:<disp-formula id="FD37-plants-14-02634"><label>(37)</label><mml:math id="mm154" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>dwsep</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>BN</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>F</mml:mi><mml:msub><mml:mo>&#8727;</mml:mo><mml:mi>dw</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mo>&#8727;</mml:mo><mml:mi>pw</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, dw denotes depthwise convolution and pw pointwise convolution. These architectural variants implement modular blocks within the MoE framework, enabling efficient feature specialization across abstraction levels.</p><p>To enforce expert diversity, we apply orthogonal initialization to convolutional kernels and architecture-specific dropout paths. This strategy ensures non-redundant representation learning, facilitating complementary specialization across lesion types. Each expert processes identical input feature maps <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> through distinct parameter spaces, promoting feature disentanglement. Empirical results (<xref rid="sec2dot6-plants-14-02634" ref-type="sec">Section 2.6</xref>) confirm consistent expert utilization under dynamic routing, with significant improvements in lesion boundary recall (+3.1% vs. baseline) and regional consistency.</p><p>Expert selection is governed by a learnable gating network G operating over intermediate features. For spatial location p = (h,w), G computes probability distribution <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> over M experts. Parameterized as a lightweight multi-layer perceptron (MLP), the gating function is defined as follows:<disp-formula id="FD38-plants-14-02634"><label>(38)</label><mml:math id="mm157" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> is the GELU activation. This architecture balances expressive power and computational cost, enabling fine-grained control over expert routing at each spatial coordinate.</p><p>To mitigate excessive confidence in individual experts, we apply entropy regularization over the output distribution as follows:<disp-formula id="FD39-plants-14-02634"><label>(39)</label><mml:math id="mm161" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>entropy</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
which encourages smoother, more balanced expert selection. Additionally, we employ batch-wise normalization of the gating outputs to stabilize training dynamics and prevent gradient saturation. The gating network is trained jointly with the backbone and expert modules, allowing end-to-end optimization of the entire pipeline [<xref rid="B33-plants-14-02634" ref-type="bibr">33</xref>].</p><p>To support top-k selection, we further refine <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by retaining only the k highest values per location and setting the rest to zero, followed by re-normalization. This induces sparsity in expert utilization while preserving differentiability via Gumbel&#8211;Softmax approximation. Let <inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the top-k selection operator; then the routed output <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed as shown in Equation (<xref rid="FD40-plants-14-02634" ref-type="disp-formula">40</xref>).<disp-formula id="FD40-plants-14-02634"><label>(40)</label><mml:math id="mm165" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This gating mechanism enables conditional expert activation based on local feature complexity, achieving computational efficiency while preserving specialization. The expert selection strategy critically determines framework efficiency and robustness. We implement top-k routing (k = 2) to balance computational savings and segmentation accuracy, optimized through empirical validation [<xref rid="B34-plants-14-02634" ref-type="bibr">34</xref>]. For each spatial position p, the gating network computes probabilities <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then activates experts corresponding to the top-k scores. This conditional computation reduces active paths during forward propagation and minimizes redundant parameter updates.</p><p>The combined expert output at position p is computed as:<disp-formula id="FD41-plants-14-02634"><label>(41)</label><mml:math id="mm167" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>TopK</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the indices of the top-k expert weights. The dynamic routing pattern promotes sample-specific specialization, with different input regions being processed by distinct expert subsets. This flexibility is essential for modeling diverse disease appearances that may vary across leaf textures, shapes, and lighting conditions.</p><p>To stabilize training under sparse selection, we inject Gaussian noise <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> into the logits prior to softmax normalization during training as follows:<disp-formula id="FD42-plants-14-02634"><label>(42)</label><mml:math id="mm170" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Stochastic perturbation in routing encourages diverse expert combinations and mitigates suboptimal convergence. We further apply feature-level dropout before the gating MLP to prevent expert co-adaptation and maintain path diversity. <xref rid="sec2dot6-plants-14-02634" ref-type="sec">Section 2.6</xref> empirically confirms top-k routing improves boundary F1-score by 4.3% versus dense combinations.</p><p>Expert utilization during inference is tracked via selection statistics across spatial positions. For expert i, denote cumulative selection frequency as <inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over the validation set. Normalized utilization is defined as follows:<disp-formula id="FD43-plants-14-02634"><label>(43)</label><mml:math id="mm172" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:mo>&#8878;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mo>&#8878;</mml:mo></mml:mrow></mml:math></inline-formula> is the indicator function. These statistics guide subsequent pruning or architectural reconfiguration.</p><p>To prevent expert collapse&#8212;a phenomenon where only a few experts dominate routing decisions&#8212;we incorporate a load balancing loss that explicitly encourages uniform utilization across all experts. This auxiliary loss penalizes skewed routing distributions by maximizing entropy over the expert usage histogram. Formally, let <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the total fraction of input locations routed to expert i over a mini-batch. Then, the balance loss is defined as follows:<disp-formula id="FD44-plants-14-02634"><label>(44)</label><mml:math id="mm175" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>&#183;</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In practice, <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed using Equation (<xref rid="FD45-plants-14-02634" ref-type="disp-formula">45</xref>) as follows:<disp-formula id="FD45-plants-14-02634"><label>(45)</label><mml:math id="mm177" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>B</mml:mi><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where B is the batch size. This loss is weighted by a hyperparameter <inline-formula><mml:math id="mm178" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> in the total loss function, balancing its influence relative to segmentation and sparsity objectives.</p><p>To complement this loss, we periodically rescale expert weights to ensure consistent gradient flow. Additionally, we employ a scheduling mechanism where the strength of <inline-formula><mml:math id="mm179" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is gradually increased over the course of training, allowing initial specialization to emerge before enforcing diversity. This annealing strategy is essential for preserving convergence stability.</p><p>Layer-specific balance constraints were evaluated following [<xref rid="B35-plants-14-02634" ref-type="bibr">35</xref>]. Ablation studies (<xref rid="sec2dot7-plants-14-02634" ref-type="sec">Section 2.7</xref>) demonstrate that applying <inline-formula><mml:math id="mm180" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> solely at the decoder stage improves mean IoU by 1.2% over full-model constraints while maintaining expert diversity). This indicates spatial specialization is most critical during boundary refinement.</p></sec><sec id="sec4dot4-plants-14-02634"><title>4.4. Enhanced ASPP with Sparse&#160;Attention</title><p>In plant disease segmentation, the morphological diversity of symptoms&#8212;ranging from small localized lesions to large necrotic areas&#8212;necessitates robust multi-scale feature extraction. To address this, we enhance the Atrous Spatial Pyramid Pooling (ASPP) module by incorporating dilated convolutions with variable dilation rates of 1, 6, 12, 18, modulating receptive fields without increasing parameters. For an input feature map <inline-formula><mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the ASPP module employs parallel dilated convolutions <inline-formula><mml:math id="mm182" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Conv</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> with dilation rates <inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, generating intermediate representations <inline-formula><mml:math id="mm184" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm185" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This process is mathematically defined in Equation (<xref rid="FD46-plants-14-02634" ref-type="disp-formula">46</xref>) as follows:<disp-formula id="FD46-plants-14-02634"><label>(46)</label><mml:math id="mm186" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>&#937;</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm187" overflow="scroll"><mml:mrow><mml:mo>&#937;</mml:mo></mml:mrow></mml:math></inline-formula> denotes the convolutional kernel grid and <inline-formula><mml:math id="mm188" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is the kernel weight tensor at dilation rate <inline-formula><mml:math id="mm189" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This formulation allows the network to aggregate context at varying scales while preserving spatial resolution&#8212;an essential property for accurately segmenting lesions of different sizes.</p><p>In addition to dilated convolutional branches, we incorporate a global average pooling (GAP) branch to encode holistic contextual information, yielding a pooled representation <inline-formula><mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>GAP</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mn>1</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is then projected via a <inline-formula><mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution and broadcasted to the input spatial dimensions. The final concatenated feature tensor is mathematically expressed in Equation (<xref rid="FD47-plants-14-02634" ref-type="disp-formula">47</xref>) as follows:<disp-formula id="FD47-plants-14-02634"><label>(47)</label><mml:math id="mm192" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>concat</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>GAP</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
followed by batch normalization and ReLU activation. This design enables the ASPP module to capture both local structural and global semantic cues, enhancing segmentation granularity for fine-grained disease patterns.</p><p>However, traditional ASPP implementations suffer from redundant computations and indiscriminate feature aggregation, limiting deployment efficiency on edge devices. To mitigate this, we integrate a sparse attention mechanism within the ASPP module that dynamically prioritizes informative spatial positions during feature aggregation. Specifically, we introduce a binary masking function M(x,y) &#8712; 0, 1 based on top-k activation responses, applied to each dilated branch&#8217;s output. This yields sparsified features <inline-formula><mml:math id="mm193" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, preserving only the top-o activations per spatial window. The sparsity is formally defined as follows:<disp-formula id="FD48-plants-14-02634"><label>(48)</label><mml:math id="mm194" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm195" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm196" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#8467;</mml:mo><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>-norm counting non-zero entries. This substantially reduces computational redundancy while concentrating modeling capacity on salient regions, conforming to the localized nature of disease manifestations. Our sparse-enhanced multi-scale architecture thus achieves adaptively modulated receptive fields with computational efficiency [<xref rid="B36-plants-14-02634" ref-type="bibr">36</xref>].</p><p>Following multi-scale extraction, effective integration of heterogeneous contextual cues becomes critical for producing coherent and precise segmentation maps. To this end, we design a sparse attention-enhanced fusion mechanism that selectively attends to salient spatial positions during feature integration. Let <inline-formula><mml:math id="mm197" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>concat</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denote the concatenated multi-scale features. We first apply a linear projection <inline-formula><mml:math id="mm198" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>D</mml:mi></mml:msup><mml:mo>&#8594;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to reduce channel dimensionality for attention computation as follows:<disp-formula id="FD49-plants-14-02634"><label>(49)</label><mml:math id="mm199" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>concat</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm200" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a learned projection matrix. Then, query Q, key K, and value V matrices are computed from F&#8217; using independent learned linear layers. Sparse attention weights are derived via masked dot-product similarity, with the mask <inline-formula><mml:math id="mm201" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> encoding top-k spatial connections per query location as follows:<disp-formula id="FD50-plants-14-02634"><label>(50)</label><mml:math id="mm202" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>K</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>K</mml:mi><mml:msup><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm203" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the sparse neighborhood for query location i. This leads to the following refined features:<disp-formula id="FD51-plants-14-02634"><label>(51)</label><mml:math id="mm204" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This operation enforces localized feature aggregation while maintaining global structural coherence essential for segmenting complex lesion boundaries. Significantly, the sparsity mask is dynamically updated during training based on attention entropy to prevent convergence toward trivial attention patterns.</p><p>To consolidate attention-refined features, we incorporate residual connections from the original ASPP output and apply a squeeze-and-excitation (SE) block to recalibrate channel-wise dependencies. The SE module computes global descriptors for each channel, followed by two-layer MLP gating as follows:<disp-formula id="FD52-plants-14-02634"><label>(52)</label><mml:math id="mm205" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:mi>ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msubsup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mi>c</mml:mi><mml:mi>scaled</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm206" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm207" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are trainable matrices and <inline-formula><mml:math id="mm208" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> is the sigmoid function. This enhances feature selectivity across scales and channels, further suppressing irrelevant activations arising from noisy backgrounds or lighting variations common in agricultural field imagery.</p></sec><sec id="sec4dot5-plants-14-02634"><title>4.5. Dual-Stage MoE&#160;Decoder</title><p>To effectively segment leaf disease regions with varying granularity and texture complexity, we propose a dual-stage decoder architecture based on a Mixture of Experts (MoE) framework, as illustrated in <xref rid="plants-14-02634-f010" ref-type="fig">Figure 10</xref>.</p><p>Novelty of Dual-Stage MoE Architecture: Our dual-stage MoE decoder introduces several key innovations compared to existing multi-stage or hierarchical segmentation approaches. First, task-conditional expert specialization occurs: Unlike traditional hierarchical decoders that apply uniform processing across all spatial regions, our framework employs content-aware expert routing where different experts specialize in distinct morphological patterns (e.g., coarse-stage experts for large lesion detection vs. fine-stage experts for boundary refinement). Second, progressive complexity adaptation: The dual-stage design matches the natural progression of human pathological diagnosis&#8212;from coarse disease region identification to precise boundary delineation&#8212;with each stage employing appropriately specialized computational resources. Third, dynamic expert activation: Rather than fixed expert assignment used in prior MoE approaches, our gating mechanism dynamically selects experts based on input complexity, enabling adaptive computational allocation during inference. Fourth, cross-stage feature propagation: Our architecture incorporates learnable skip connections between stages that preserve both global context and fine-grained details, addressing the information bottleneck problem common in traditional U-Net-style architectures. This design significantly differs from existing methods: FPN/UNet use static decoder paths, transformer decoders apply uniform processing, and prior MoE implementations lack stage-specific specialization for segmentation tasks.</p><p>The motivation for this progressive design stems from the hierarchical nature of semantic segmentation, where coarse spatial priors require successive refinement into precise pixel-level predictions. In the initial stage, high-level semantic embeddings extracted by the encoder are processed through four coarse-level experts, each specialized for capturing distinct region-level characteristics including lesion location, approximate shape, and global boundaries. Each expert <inline-formula><mml:math id="mm209" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> operates on the shared input feature tensor <inline-formula><mml:math id="mm210" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The gated combination output is computed as specified in Equation (<xref rid="FD53-plants-14-02634" ref-type="disp-formula">53</xref>) as follows:<disp-formula id="FD53-plants-14-02634"><label>(53)</label><mml:math id="mm211" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4.pt"/><mml:mi>with</mml:mi><mml:mspace width="4.pt"/><mml:msubsup><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>TopK</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm212" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the gating weight at spatial location p for expert i, and <inline-formula><mml:math id="mm213" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the top-2 selected experts. The coarse segmentation result <inline-formula><mml:math id="mm214" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> captures disease-affected zones and generates an initial confidence mask. Crucially, this stage operates at reduced spatial resolution (stride = 4) to prioritize contextual aggregation, enhancing global structural coherence.</p><p>The second decoder stage targets fine-grained boundary delineation and structural detail refinement. Building upon the coarse output <inline-formula><mml:math id="mm215" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, this stage integrates the encoder&#8217;s low-level spatial features <inline-formula><mml:math id="mm216" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mi>low</mml:mi></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mn>2</mml:mn><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msup><mml:mi>D</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. These features are concatenated and processed through a dedicated MoE block comprising three refinement experts. The combination mechanism follows the first-stage paradigm but operates at <inline-formula><mml:math id="mm217" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> spatial resolution, formally defined by the following:<disp-formula id="FD54-plants-14-02634"><label>(54)</label><mml:math id="mm218" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>p</mml:mi><mml:mi>low</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msubsup><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>TopK</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#945;</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm219" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the fine-level experts, and <inline-formula><mml:math id="mm220" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#945;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represent the corresponding spatial gating weights. This dual-stage refinement enables conditional allocation of computational resources to regions exhibiting prediction uncertainty or complex textural patterns, producing sharper boundaries and improved region consistency.</p><p>To ensure stable training and expert diversity across both stages, we inject Gaussian noise into the gating logits and employ entropy regularization and balance loss. The overall segmentation prediction is computed as <inline-formula><mml:math id="mm221" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>Upsample</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, projected back to the original image resolution. Empirical results indicate that this two-phase strategy achieves a 1.2% gain in boundary F1-score and consistently reduces false positives near ambiguous lesion edges. Notably, it maintains computational tractability due to the sparse top-k expert routing at both levels.</p><p>The effectiveness of the dual-stage MoE decoder heavily relies on a carefully designed feature integration mechanism that bridges hierarchical semantics. Between the encoder and the first-stage decoder, we incorporate lateral connections from intermediate encoder layers to preserve spatial fidelity while maintaining semantic richness. Let <inline-formula><mml:math id="mm222" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>enc</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the feature maps from the -th encoder layer. We apply 1 &#215; 1 convolutions followed by bilinear upsampling to align these features to the decoder resolution as follows:<disp-formula id="FD55-plants-14-02634"><label>(55)</label><mml:math id="mm223" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>aligned</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Upsample</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>enc</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>scale</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These aligned features are aggregated with the decoder input F via additive fusion and normalization, improving signal propagation across the network depth. The integration strategy at the refinement stage is more complex, involving both additive fusion and concatenation with attention gates. Specifically, for each pixel location p, we compute an integration score as follows:<disp-formula id="FD56-plants-14-02634"><label>(56)</label><mml:math id="mm224" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm225" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> is the sigmoid activation, and <inline-formula><mml:math id="mm226" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are learnable matrices. This score modulates the fusion between coarse and fine features, yielding the following refined representation:<disp-formula id="FD57-plants-14-02634"><label>(57)</label><mml:math id="mm227" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>p</mml:mi><mml:mi>ref</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>p</mml:mi><mml:mi>low</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This attention-gated fusion enables the model to selectively prioritize encoder or decoder information based on contextual reliability. Channel-wise recalibration via squeeze-and-excitation blocks further suppresses redundant activations while enhancing discriminative channels after fusion.</p><p>We introduce a refinement residual branch employing deformable convolutions to capture geometric distortions and curved lesion contours that conventional convolutions cannot adequately represent. This auxiliary stream outputs a residual mask <inline-formula><mml:math id="mm228" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, added to the stage-2 prediction as follows:<disp-formula id="FD58-plants-14-02634"><label>(58)</label><mml:math id="mm229" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Sigmoid</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Overall, the integration strategy ensures multi-resolution alignment, semantic-spatial coherence, and dynamic information routing, all critical for fine-grained agricultural segmentation tasks.</p><p>To maximize the MoE decoder&#8217;s utility, each expert is designed with distinct architectural specialization and learning objectives. In the first decoder stage, four coarse-level experts A, B, C, D are implemented with specialized configurations [<xref rid="B37-plants-14-02634" ref-type="bibr">37</xref>] as follows:</p><p>Expert A: Standard 3 &#215; 3 convolutions optimized for general lesion patterns and dominant foreground structures; Expert B: 3 &#215; 3 dilated convolutions (r = 6) modeling long-range dependencies in diffuse disease regions; Expert C: Depthwise separable convolutions preserving parameter efficiency and activation sparsity for subtle lesions; Expert D: Structural replica of A with Xavier initialization and GroupNorm promoting stochastic feature specialization.</p><p>This diversity ensures non-redundant feature learning and robust ensemble aggregation. All experts incorporate dropout regularization and kernel orthogonalization to maintain diverse gradient paths and prevent mode collapse. During training, we monitor expert selection distributions using frequency metric <inline-formula><mml:math id="mm230" overflow="scroll"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (the ratio of samples routed to expert <italic toggle="yes">i</italic> per batch), formally defined in Equation (<xref rid="FD59-plants-14-02634" ref-type="disp-formula">59</xref>) as follows:<disp-formula id="FD59-plants-14-02634"><label>(59)</label><mml:math id="mm231" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>B</mml:mi><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:mo>&#8878;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mi mathvariant="script">J</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where B is the batch size and <inline-formula><mml:math id="mm232" overflow="scroll"><mml:mrow><mml:mo>&#8878;</mml:mo></mml:mrow></mml:math></inline-formula> denotes the indicator function. These statistics guide adaptive loss scaling and architecture pruning if imbalance is detected.</p><p>In the second-stage decoder, three fine-level experts are employed focusing on detail enhancement and boundary sharpening. Expert A emphasizes edge sensitivity by using small 1 &#215; 1 and 3 &#215; 3 convolutions combined with filters similar to Sobel, while Expert B incorporates residual blocks with skip connections for gradient stability and preservation of fine-scale structures. Expert C incorporates a lightweight spatial attention module that selectively amplifies activations aligned with contours. Each fine-level expert also receives boundary distance maps as auxiliary inputs to guide localization. These maps, computed via signed distance transforms, are concatenated with the input feature maps and embedded using 1 &#215; 1 convolutions. Let <inline-formula><mml:math id="mm233" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the signed distance at pixel p. The input to each expert then becomes <inline-formula><mml:math id="mm234" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The benefit of this formulation results in improved alignment between predicted and ground truth lesion edges, particularly under variable lighting or partial occlusion conditions. Altogether, the ensemble of decoder experts exhibits complementary strengths, and the sparse top-k routing mechanism enables the network to dynamically utilize these specializations based on input content. This architectural heterogeneity, combined with optimized gating strategies and specific regularization, forms the core of the model&#8217;s superior performance in fine-scale, high-variance plant disease segmentation tasks.</p></sec><sec id="sec4dot6-plants-14-02634"><title>4.6. Hyperparameter&#160;Configuration</title><p>The proposed Sparse-MoE-SAM framework requires careful hyperparameter tuning to achieve optimal performance while maintaining computational efficiency. We provide comprehensive details of all critical hyperparameters used in our experiments.</p><p>Training Hyperparameters: The model was trained using AdamW optimizer with a learning rate of <inline-formula><mml:math id="mm235" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the encoder and <inline-formula><mml:math id="mm236" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the decoder components. We employed cosine annealing learning rate schedule with warm-up period of 10 epochs and minimum learning rate of <inline-formula><mml:math id="mm237" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The total training spans 100 epochs with batch size of 16 on our RTX 3090 setup. Weight decay was set to <inline-formula><mml:math id="mm238" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for regularization, and gradient clipping was applied with maximum norm of 1.0 to prevent gradient explosion.</p><p>Sparse Attention Parameters: The sparsity ratio <inline-formula><mml:math id="mm239" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> was set to 0.1, retaining only the top 10% attention weights per query. This value was determined through ablation studies showing optimal balance between computational reduction and performance retention. The Gumbel noise temperature <inline-formula><mml:math id="mm240" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula> was initialized at 1.0 and annealed to 0.1 over training epochs using <inline-formula><mml:math id="mm241" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#964;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#964;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>&#964;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> where <italic toggle="yes">t</italic> is current epoch and <italic toggle="yes">T</italic> is total epochs.</p><p>MoE Architecture Parameters: The first-stage decoder employs 4 experts with gating temperature <inline-formula><mml:math id="mm242" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, while the second-stage uses 3 experts with <inline-formula><mml:math id="mm243" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Expert diversity is maintained through load balancing with coefficient <inline-formula><mml:math id="mm244" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The routing gate networks use 2-layer MLPs with hidden dimension 512 and ReLU activation. Dropout probability for expert networks was set to 0.1 during training.</p><p>Loss Function Weights: The multi-component loss function weights were: segmentation loss weight <inline-formula><mml:math id="mm245" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, balance loss weight <inline-formula><mml:math id="mm246" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and sparsity regularization weight <inline-formula><mml:math id="mm247" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. These weights were determined through grid search optimization on validation set performance.</p><p>Data Augmentation: Training images underwent random horizontal/vertical flipping (probability 0.5), random rotation (<inline-formula><mml:math id="mm248" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#177;</mml:mo><mml:msup><mml:mn>15</mml:mn><mml:mo>&#8728;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), color jittering (brightness 0.2, contrast 0.2, saturation 0.1), and random scaling (0.8&#8211;1.2). Gaussian noise (<inline-formula><mml:math id="mm249" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) was added with probability 0.3 to improve robustness.</p></sec><sec id="sec4dot7-plants-14-02634"><title>4.7. Training Strategy and Loss&#160;Functions</title><p>To achieve both segmentation accuracy and architectural efficiency, we design a multi-component loss function that jointly optimizes prediction fidelity, expert diversity, and attention sparsity. <xref rid="plants-14-02634-t008" ref-type="table">Table 8</xref> provides a comprehensive overview of all loss components used in our framework.</p><p>The total loss is composed of three terms: the segmentation loss <inline-formula><mml:math id="mm250" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the balance loss <inline-formula><mml:math id="mm251" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the sparsity regularization <inline-formula><mml:math id="mm252" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>sparsity</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The complete formulation is given in Equation (<xref rid="FD60-plants-14-02634" ref-type="disp-formula">60</xref>) as follows:<disp-formula id="FD60-plants-14-02634"><label>(60)</label><mml:math id="mm253" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>sparsity</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The segmentation loss consists of a weighted combination of Binary Cross Entropy (BCE) and soft Dice loss, which jointly address pixel-wise prediction accuracy and global mask overlap. Given the predicted mask <inline-formula><mml:math id="mm254" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the ground truth mask <inline-formula><mml:math id="mm255" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, we define it as shown in Equation (<xref rid="FD61-plants-14-02634" ref-type="disp-formula">61</xref>), expressed as follows:<disp-formula id="FD61-plants-14-02634"><label>(61)</label><mml:math id="mm256" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>BCE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm257" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> is a smoothing term ensuring numerical stability. This hybrid loss enhances boundary adherence and prevents collapse induced by class imbalance, particularly crucial in lesion detection where diseased regions exhibit high fragmentation. To promote balanced expert utilization, a balance loss penalizes significant deviations in expert selection frequencies. Let <inline-formula><mml:math id="mm258" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8878;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denote the selection frequency of expert <italic toggle="yes">i</italic>, where <inline-formula><mml:math id="mm259" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the top-k expert set for spatial location <italic toggle="yes">j</italic>. The variance across experts is then computed as Equation (<xref rid="FD62-plants-14-02634" ref-type="disp-formula">62</xref>), as follows:<disp-formula id="FD62-plants-14-02634"><label>(62)</label><mml:math id="mm260" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>E</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="4.pt"/><mml:mi>where</mml:mi><mml:mspace width="4.pt"/><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>E</mml:mi></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This formulation maintains differentiability, enabling gradient backpropagation through the Softmax-based gating weights to discourage dominance of specific experts. Furthermore, temperature-controlled noise injection during training regularizes expert activation diversity.</p><p>The sparsity regularization term enforces L1 constraints on attention weight matrices to reduce unnecessary activations. For attention matrix <inline-formula><mml:math id="mm261" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>H</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the regularization term is defined as follows:<disp-formula id="FD63-plants-14-02634"><label>(63)</label><mml:math id="mm262" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>sparsity</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This encourages most attention weights to converge toward zero, preserving only the top-<inline-formula><mml:math id="mm263" overflow="scroll"><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:math></inline-formula> interactions per token. We empirically tune <inline-formula><mml:math id="mm264" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm265" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> via grid search, typically setting <inline-formula><mml:math id="mm266" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm267" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for optimal trade-offs.</p><p>MoE Training Challenges and Solutions: Training the MoE architecture presents several challenges that required specialized solutions. First, expert collapse occurred during early training, where certain experts received disproportionately high routing probabilities, leading to underutilization of other experts. We addressed this through dynamic load balancing with exponential moving average tracking of expert utilization rates and adaptive penalty scaling. Second, routing instability emerged from noisy gating decisions, causing training oscillations. We implemented Gumbel&#8211;Softmax smoothing with temperature annealing from 1.0 to 0.1 to gradually sharpen routing decisions. Third, gradient interference between experts caused optimization difficulties. We resolved this using orthogonal initialization for expert parameters and separate learning rates for routing networks (<inline-formula><mml:math id="mm268" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) versus expert networks (<inline-formula><mml:math id="mm269" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). Fourth, memory overhead from storing multiple expert parameters was mitigated through selective gradient accumulation and expert-specific batch processing.</p><p>The training process divides into three phases to ensure stable convergence, promote expert specialization, and enhance system robustness. During initialization, we pretrain the backbone by loading SAM encoder weights and freezing the encoder <inline-formula><mml:math id="mm270" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula>. The decoder modules <inline-formula><mml:math id="mm271" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> (including both MoE decoder stages) are trained independently from encoder gradients using only <inline-formula><mml:math id="mm272" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This enables the decoder to learn coarse-to-fine mappings without interference from large encoder gradients. Formally, for frozen encoder <inline-formula><mml:math id="mm273" overflow="scroll"><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:math></inline-formula> and decoder <inline-formula><mml:math id="mm274" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula>, the objective is as follows:<disp-formula id="FD64-plants-14-02634"><label>(64)</label><mml:math id="mm275" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mi>&#952;</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#981;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The second phase unfreezes all components and initiates full end-to-end fine-tuning. Here, the complete <inline-formula><mml:math id="mm276" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>total</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is used, and gradient updates are propagated across both encoder and decoder. This stage employs mixed-precision training and gradient accumulation (if needed) for memory efficiency. Learning rate scheduling uses cosine annealing <inline-formula><mml:math id="mm277" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#951;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>&#951;</mml:mi><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#951;</mml:mi><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#951;</mml:mi><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#960;</mml:mi><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where T is total epochs and t is current step.</p><p>In the final phase, we encourage expert specialization by increasing <inline-formula><mml:math id="mm278" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to amplify balance loss and introducing dropout-based stochastic gating during training. This prevents expert collapse and reinforces role differentiation. Expert statistics (mean activation, selection entropy, spatial overlap) are monitored in each epoch, and early stopping is applied if diversity metrics converge prematurely. This curriculum-based regime results in smoother optimization landscapes and improved generalization under noisy agricultural conditions.</p><p>For optimization, we adopt the AdamW optimizer due to its decoupled weight decay mechanism, which improves generalization in deep networks with attention or gating structures. The initial learning rate <inline-formula><mml:math id="mm279" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#951;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is modulated using cosine annealing to progressively decay towards <inline-formula><mml:math id="mm280" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, following the schedule: where <inline-formula><mml:math id="mm281" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#951;</mml:mi><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm282" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>total</mml:mi><mml:mspace width="4.pt"/><mml:mi>number</mml:mi><mml:mspace width="4.pt"/><mml:mi>of</mml:mi><mml:mspace width="4.pt"/><mml:mi>iterations</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We apply gradient clipping with max norm 1.0 to stabilize updates in sparse attention branches, which are prone to large gradient spikes due to discontinuous activation maps. Weight decay is fixed at <inline-formula><mml:math id="mm283" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and batch normalization statistics are frozen after the first training phase to prevent internal covariate shifts.</p><p>To enhance model generalization and robustness to field conditions, we employ a diverse augmentation pipeline. Each input image undergoes stochastic transformations including random rotation (<inline-formula><mml:math id="mm284" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#177;</mml:mo><mml:msup><mml:mn>30</mml:mn><mml:mo>&#8728;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), spatial scaling <inline-formula><mml:math id="mm285" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>1.2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, independent horizontal/vertical flipping (<inline-formula><mml:math id="mm286" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), and color jittering (<inline-formula><mml:math id="mm287" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>brightness</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>contrast</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Let <inline-formula><mml:math id="mm288" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote this composite augmentation. The training objective is then expressed as follows:<disp-formula id="FD65-plants-14-02634"><label>(65)</label><mml:math id="mm289" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mi>&#952;</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>total</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This regularization enforces invariance to orientation, illumination, and elastic deformations characteristic of agricultural imagery. Empirical results indicate that augmentation improves IoU by up to 1.1% and reduces variance across test folds, highlighting its necessity for real-world deployment scenarios.</p></sec></sec><sec sec-type="conclusions" id="sec5-plants-14-02634"><title>5. Conclusions</title><p>This research introduces Sparse-MoE-SAM, a pioneering architectural framework that addresses the critical trade-off between segmentation accuracy and computational efficiency in agricultural computer vision. By integrating bio-inspired sparse attention mechanisms with hierarchical mixture-of-experts architectures, our approach represents a significant advance in practical plant disease segmentation for resource-constrained environments.</p><p>Technical Contributions and Innovations: Our work presents three fundamental innovations: (1) A biologically motivated sparse attention mechanism that reduces computational complexity from <inline-formula><mml:math id="mm290" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm291" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through Gumbel-TopK selection, achieving 23.7% FLOPs reduction while maintaining 94.2% IoU performance. This design mirrors how plant pathologists focus attention on disease-relevant regions rather than processing entire leaf surfaces uniformly. (2) A novel dual-stage MoE decoder architecture featuring task-conditional expert specialization, where coarse-stage experts handle global disease region identification while fine-stage experts refine boundary delineation. This progressive specialization approach differs fundamentally from existing uniform processing architectures. (3) An enhanced ASPP module with sparse attention integration that captures multi-scale lesion patterns while suppressing irrelevant background features through dynamic feature routing.</p><p>Experimental Validation and Performance: Comprehensive evaluations across three diverse datasets (PlantVillage, CVPPP, Custom Field) demonstrate superior performance with consistent improvements over state-of-the-art methods. Our framework achieves 94.2% IoU on PlantVillage, outperforming Standard SAM by 2.5 percentage points while requiring 55% fewer parameters (142.7 M vs. 636.0 M). Cross-dataset validation reveals robust generalization with minimal performance degradation (4.5% IoU drop vs. 6.9% for SAM) when transitioning from controlled to real-world conditions. Disease-specific analysis shows consistent improvements across pathological categories, with particular efficacy for rust detection (95.4% IoU) and challenges identified for diffuse viral symptoms (93.2% IoU).</p><p>Practical Impact and Deployment: The mobile variant (45.3 M parameters, 38.7 GFLOPs) enables unprecedented deployment opportunities on resource-constrained devices. Performance testing on agricultural hardware platforms&#8212;from Jetson Nano (4.8s inference, 91.3% IoU) to smartphone-grade processors (6.1s inference, 89.7% IoU)&#8212;validates practical viability for field deployment. Detailed analysis of challenging scenarios including complex leaf occlusions (84.2% IoU under severe occlusion) and low-light conditions (85.7% IoU at 0.3&#215; brightness) demonstrates robustness under realistic agricultural constraints.</p><p>Future Directions and Limitations: While our framework advances state-of-the-art in plant disease segmentation, several areas warrant future investigation.</p><p>Hardware-Level Sparse Optimization: We plan to implement custom CUDA kernels for sparse attention operations, targeting 40&#8211;60% additional speedup on modern GPUs. Specific steps include the following: (1) developing block-sparse matrix multiplication routines optimized for top-k attention patterns, (2) implementing fused attention-routing kernels to reduce memory bandwidth requirements, (3) designing quantization-aware training for 8-bit inference on mobile devices while maintaining accuracy within 1% of full-precision models, and (4) exploring tensor core utilization for mixed-precision sparse computations on Ampere/Hopper architectures.</p><p>Meta-Learning Expert Discovery: We propose gradient-based meta-learning for automated expert architecture search. Technical approach involves the following: (1) implementing MAML (Model-Agnostic Meta-Learning) for rapid adaptation to new plant species with 10&#8211;50 training samples, (2) developing differentiable architecture search (DARTS) for expert network topology optimization within computational budgets, (3) designing few-shot learning protocols for emerging disease detection using prototypical networks with expert embeddings, and (4) creating continual learning mechanisms to incrementally add new expert specializations without catastrophic forgetting.</p><p>Multimodal Imaging Integration: Extension to multi-spectral sensing will involve the following: (1) developing cross-modal attention mechanisms for RGB-thermal-hyperspectral fusion, (2) implementing wavelength-specific expert routing for optimal spectral band utilization, (3) designing domain adaptation techniques for sensor-agnostic disease detection across different imaging modalities, and (4) creating multi-temporal analysis frameworks for disease progression monitoring using sequential imaging data.</p><p>Enhanced handling of diffuse symptom patterns through specialized expert training could address current limitations with viral diseases. Incorporation of temporal information for diseases with dynamic progression patterns presents opportunities for improved early detection.</p><p>This work establishes a new paradigm for efficient agricultural computer vision, demonstrating that careful architectural design can simultaneously achieve superior accuracy and practical deployability. The framework&#8217;s integration of biological insights with computational efficiency principles provides a foundation for next-generation precision agriculture systems.</p></sec><sec id="sec6-plants-14-02634"><title>6. Data and Code&#160;Availability</title><p>To ensure reproducibility and facilitate community adoption, we commit to making our research artifacts publicly available upon paper acceptance. The complete implementation will be released on GitHub (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/BenhanZhao/Sparse-MoE-SAM">https://github.com/BenhanZhao/Sparse-MoE-SAM</uri>) including the following: (1) Full PyTorch implementation of the Sparse-MoE-SAM framework with detailed documentation, (2) Pre-trained models for both full and mobile variants on PlantVillage dataset, (3) Training and evaluation scripts with hyperparameter configurations, (4) Data preprocessing pipelines and augmentation procedures, (5) Comprehensive setup instructions for local and cloud deployment, (6) Jupyter notebooks demonstrating inference on sample images, and (7) Edge device deployment guides for Jetson and mobile platforms. Additionally, we will provide model weights trained on our Custom Field dataset (subject to data sharing agreements) and comprehensive benchmarking scripts for comparison with baseline methods. All code will be released under an open-source license to maximize community benefit and enable further research in agricultural computer vision.</p></sec></body><back><ack><title>Acknowledgments</title><p>We express our gratitude to the editor and reviewers for providing valuable suggestions aimed at enhancing the caliber of this paper.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, B.Z., H.Z. and F.W.; methodology, B.Z. and H.Z.; software, B.Z. and H.Z.; writing&#8212;original draft preparation, B.Z., X.K. and Z.S.; writing&#8212;review and editing, F.W., Y.W., L.L. (Lin Li) and G.Z.; validation, L.L. (Lin Li), Y.Y. and G.Z.; visualization, B.Z.; supervision, L.L. (Lin Li) and G.Z.; formal analysis, L.L. (Leheng Li), H.Z. and J.Z.; investigation, H.Z., X.K. and L.L. (Leheng Li); resources, L.L. (Lin Li), G.Z. and F.W.; data curation, B.Z. and Y.W.; project administration, B.Z.; funding acquisition, F.W., L.L. (Lin Li), G.Z. and J.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Requests to access the datasets should be sent via email to <email>zhaobh0908@163.com</email>.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-plants-14-02634"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Islam</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Mahdee</surname><given-names>G.M.S.</given-names></name><name name-style="western"><surname>Kabir</surname><given-names>W.U.</given-names></name></person-group><article-title>Improved segmentation approach for plant disease detection</article-title><source>Proceedings of the 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT)</source><conf-loc>Dhaka, Bangladesh</conf-loc><conf-date>3&#8211;5 May 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2019</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B2-plants-14-02634"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bhatti</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Zeeshan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Syam</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Bhatti</surname><given-names>U.A.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ghadi</surname><given-names>Y.Y.</given-names></name><name name-style="western"><surname>Alsenan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Asif</surname><given-names>M.</given-names></name><name name-style="western"><surname>Afzal</surname><given-names>T.</given-names></name></person-group><article-title>Advanced plant disease segmentation in precision agriculture using optimal dimensionality reduction with fuzzy c-means clustering and deep learning</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2024</year><volume>17</volume><fpage>18264</fpage><lpage>18277</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2024.3437469</pub-id></element-citation></ref><ref id="B3-plants-14-02634"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>W.</given-names></name></person-group><article-title>GS-DeepLabV3+: A mountain tea disease segmentation network based on improved shuffle attention and gated multidimensional feature extraction</article-title><source>Crop Prot.</source><year>2024</year><volume>183</volume><fpage>106762</fpage><pub-id pub-id-type="doi">10.1016/j.cropro.2024.106762</pub-id></element-citation></ref><ref id="B4-plants-14-02634"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name></person-group><article-title>A Precise Framework for Rice Leaf Disease Image&#8211;Text Retrieval Using FHTW-Net</article-title><source>Plant Phenomics</source><year>2024</year><volume>7</volume><fpage>0168</fpage><pub-id pub-id-type="doi">10.34133/plantphenomics.0168</pub-id><pub-id pub-id-type="pmcid">PMC11045261</pub-id><pub-id pub-id-type="pmid">38666226</pub-id></element-citation></ref><ref id="B5-plants-14-02634"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jafar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bibi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Naqvi</surname><given-names>R.A.</given-names></name><name name-style="western"><surname>Sadeghi-Niaraki</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>D.</given-names></name></person-group><article-title>Revolutionizing agriculture with artificial intelligence: Plant disease detection methods, applications, and their limitations</article-title><source>Front. Plant Sci.</source><year>2024</year><volume>15</volume><elocation-id>1356260</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2024.1356260</pub-id><pub-id pub-id-type="pmid">38545388</pub-id><pub-id pub-id-type="pmcid">PMC10965613</pub-id></element-citation></ref><ref id="B6-plants-14-02634"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaushik</surname><given-names>I.</given-names></name><name name-style="western"><surname>Prakash</surname><given-names>N.</given-names></name><name name-style="western"><surname>Jain</surname><given-names>A.</given-names></name></person-group><article-title>Plant disease detection using a depth-wise separable-based adaptive deep neural network</article-title><source>Multimed. Tools Appl.</source><year>2025</year><volume>84</volume><fpage>887</fpage><lpage>915</lpage><pub-id pub-id-type="doi">10.1007/s11042-024-19047-5</pub-id></element-citation></ref><ref id="B7-plants-14-02634"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lian</surname><given-names>J.</given-names></name></person-group><article-title>Plant Diseased Lesion Image Segmentation and Recognition Based on Improved Multi-Scale Attention Net</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>1716</elocation-id><pub-id pub-id-type="doi">10.3390/app14051716</pub-id></element-citation></ref><ref id="B8-plants-14-02634"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ananthi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Balaji</surname><given-names>V.</given-names></name><name name-style="western"><surname>Mohana</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gnanapriya</surname><given-names>S.</given-names></name></person-group><article-title>Smart plant disease net: Adaptive Dense Hybrid Convolution network with attention mechanism for IoT-based plant disease detection by improved optimization approach</article-title><source>Netw. Comput. Neural Syst.</source><year>2025</year><volume>36</volume><fpage>368</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1080/0954898X.2024.2316080</pub-id><pub-id pub-id-type="pmid">38400837</pub-id></element-citation></ref><ref id="B9-plants-14-02634"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>B.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><etal/></person-group><article-title>LUNETR: Language-Infused UNETR for precise pancreatic tumor segmentation in 3D medical image</article-title><source>Neural Netw.</source><year>2025</year><volume>187</volume><fpage>107414</fpage><pub-id pub-id-type="doi">10.1016/j.neunet.2025.107414</pub-id><pub-id pub-id-type="pmid">40117980</pub-id></element-citation></ref><ref id="B10-plants-14-02634"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name></person-group><article-title>AISOA-SSformer: An Effective Image Segmentation Method for Rice Leaf Disease Based on the Transformer Architecture</article-title><source>Plant Phenomics</source><year>2024</year><volume>6</volume><fpage>0218</fpage><pub-id pub-id-type="doi">10.34133/plantphenomics.0218</pub-id><pub-id pub-id-type="pmid">39105185</pub-id><pub-id pub-id-type="pmcid">PMC11298559</pub-id></element-citation></ref><ref id="B11-plants-14-02634"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>G.</given-names></name><name name-style="western"><surname>He</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name></person-group><article-title>LVF: A language and vision fusion framework for tomato diseases segmentation</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>227</volume><fpage>109484</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109484</pub-id></element-citation></ref><ref id="B12-plants-14-02634"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Upadhyay</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chandel</surname><given-names>N.S.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>K.P.</given-names></name><name name-style="western"><surname>Chakraborty</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Nandede</surname><given-names>B.M.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Subeesh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Upendar</surname><given-names>K.</given-names></name><name name-style="western"><surname>Salem</surname><given-names>A.</given-names></name><name name-style="western"><surname>Elbeltagi</surname><given-names>A.</given-names></name></person-group><article-title>Deep learning and computer vision in plant disease detection: A comprehensive review of techniques, models, and trends in precision agriculture</article-title><source>Artif. Intell. Rev.</source><year>2025</year><volume>58</volume><fpage>92</fpage><pub-id pub-id-type="doi">10.1007/s10462-024-11100-x</pub-id></element-citation></ref><ref id="B13-plants-14-02634"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Mu</surname><given-names>W.</given-names></name></person-group><article-title>Reformer: Re-parameterized kernel lightweight transformer for grape disease segmentation</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>265</volume><fpage>125757</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.125757</pub-id></element-citation></ref><ref id="B14-plants-14-02634"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Su</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>W.</given-names></name></person-group><article-title>Advances in Deep Learning Applications for Plant Disease and Pest Detection: A Review</article-title><source>Remote Sens.</source><year>2025</year><volume>17</volume><elocation-id>698</elocation-id><pub-id pub-id-type="doi">10.3390/rs17040698</pub-id></element-citation></ref><ref id="B15-plants-14-02634"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lei</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.</given-names></name></person-group><article-title>Deep learning implementation of image segmentation in agricultural applications: A comprehensive review</article-title><source>Artif. Intell. Rev.</source><year>2024</year><volume>57</volume><fpage>149</fpage><pub-id pub-id-type="doi">10.1007/s10462-024-10775-6</pub-id></element-citation></ref><ref id="B16-plants-14-02634"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alam</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sagar</surname><given-names>A.S.M.S.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>L.M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Park</surname><given-names>H.Y.</given-names></name><name name-style="western"><surname>Hyeonjoon</surname><given-names>M.</given-names></name></person-group><article-title>Deep learning based radish and leaf segmentation for phenotype trait measurement</article-title><source>Signal Image Video Process.</source><year>2025</year><volume>19</volume><fpage>178</fpage><pub-id pub-id-type="doi">10.1007/s11760-024-03691-3</pub-id></element-citation></ref><ref id="B17-plants-14-02634"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Prashanth</surname><given-names>K.</given-names></name><name name-style="western"><surname>Harsha</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Srilekh</surname><given-names>A.</given-names></name></person-group><article-title>Towards accurate disease segmentation in plant images: A comprehensive dataset creation and network evaluation</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2024</conf-date><fpage>7086</fpage><lpage>7094</lpage></element-citation></ref><ref id="B18-plants-14-02634"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>T.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>CTDUNet: A Multimodal CNN&#8211;Transformer Dual U-Shaped Network with Coordinate Space Attention for Camellia oleifera Pests and Diseases Segmentation in Complex Environments</article-title><source>Plants</source><year>2024</year><volume>13</volume><elocation-id>2274</elocation-id><pub-id pub-id-type="doi">10.3390/plants13162274</pub-id><pub-id pub-id-type="pmid">39204710</pub-id><pub-id pub-id-type="pmcid">PMC11359422</pub-id></element-citation></ref><ref id="B19-plants-14-02634"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qadri</surname><given-names>S.A.A.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>N.F.</given-names></name><name name-style="western"><surname>Wani</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Bhat</surname><given-names>S.A.</given-names></name></person-group><article-title>Advances and challenges in computer vision for image-based plant disease detection: A comprehensive survey of machine and deep learning approaches</article-title><source>IEEE Trans. Autom. Sci. Eng.</source><year>2024</year><volume>22</volume><fpage>2639</fpage><lpage>2670</lpage><pub-id pub-id-type="doi">10.1109/TASE.2024.3382731</pub-id></element-citation></ref><ref id="B20-plants-14-02634"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Talaat</surname><given-names>F.M.</given-names></name><name name-style="western"><surname>Shams</surname><given-names>M.Y.</given-names></name><name name-style="western"><surname>Gamel</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>ZainEldin</surname><given-names>H.</given-names></name></person-group><article-title>DeepLeaf: An optimized deep learning approach for automated recognition of grapevine leaf diseases</article-title><source>Neural Comput. Appl.</source><year>2025</year><fpage>8799</fpage><lpage>8823</lpage><pub-id pub-id-type="doi">10.1007/s00521-025-11038-3</pub-id></element-citation></ref><ref id="B21-plants-14-02634"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shinde</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ambhaikar</surname><given-names>A.</given-names></name></person-group><article-title>An efficient plant disease prediction model based on machine learning and deep learning classifiers</article-title><source>Evol. Intell.</source><year>2025</year><volume>18</volume><fpage>14</fpage><pub-id pub-id-type="doi">10.1007/s12065-024-01000-y</pub-id></element-citation></ref><ref id="B22-plants-14-02634"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Du</surname><given-names>N.</given-names></name><name name-style="western"><surname>Longpre</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chung</surname><given-names>H.W.</given-names></name><name name-style="western"><surname>Zoph</surname><given-names>B.</given-names></name><name name-style="western"><surname>Fedus</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><etal/></person-group><article-title>Mixture-of-experts meets instruction tuning: A winning combination for large language models</article-title><source>arXiv</source><year>2023</year><volume>2305</volume><fpage>14705</fpage></element-citation></ref><ref id="B23-plants-14-02634"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>B.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ning</surname><given-names>M.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>L.</given-names></name></person-group><article-title>Moe-llava: Mixture of experts for large vision-language models</article-title><source>arXiv</source><year>2024</year><volume>2401</volume><fpage>15947</fpage></element-citation></ref><ref id="B24-plants-14-02634"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ke</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xun</surname><given-names>T.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>Y.</given-names></name></person-group><article-title>Learning Heterogeneous Tissues with Mixture of Experts for Gigapixel Whole Slide Images</article-title><source>Proceedings of the Computer Vision and Pattern Recognition Conference</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>11&#8211;15 June 2025</conf-date><fpage>5144</fpage><lpage>5153</lpage></element-citation></ref><ref id="B25-plants-14-02634"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sahu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Niranjan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Joshi</surname><given-names>P.</given-names></name></person-group><article-title>HACDRAN: A Novel Hybrid Atrous Convolution Model With Densenet and Residual Attenuation Network for Multi-Plant Leaf Disease Classification</article-title><source>Trans. Emerg. Telecommun. Technol.</source><year>2025</year><volume>36</volume><fpage>e70046</fpage><pub-id pub-id-type="doi">10.1002/ett.70046</pub-id></element-citation></ref><ref id="B26-plants-14-02634"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kotwal</surname><given-names>J.G.</given-names></name><name name-style="western"><surname>Kashyap</surname><given-names>R.</given-names></name><name name-style="western"><surname>Shafi</surname><given-names>P.M.</given-names></name></person-group><article-title>Artificial driving based EfficientNet for automatic plant leaf disease classification</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>38209</fpage><lpage>38240</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-16882-w</pub-id></element-citation></ref><ref id="B27-plants-14-02634"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kouvelas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Makridis</surname><given-names>M.A.</given-names></name></person-group><article-title>CASAformer: Congestion-aware sparse attention transformer for traffic speed prediction</article-title><source>Commun. Transp. Res.</source><year>2025</year><volume>5</volume><fpage>100174</fpage><pub-id pub-id-type="doi">10.1016/j.commtr.2025.100174</pub-id></element-citation></ref><ref id="B28-plants-14-02634"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aghdam</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name></person-group><article-title>Da-moe: Towards dynamic expert allocation for mixture-of-experts models</article-title><source>arXiv</source><year>2024</year><volume>2409</volume><fpage>06669</fpage></element-citation></ref><ref id="B29-plants-14-02634"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shoaib</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sadeghi-Niaraki</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>I.</given-names></name><name name-style="western"><surname>Khalid</surname><given-names>S.</given-names></name></person-group><article-title>Leveraging deep learning for plant disease and pest detection: A comprehensive review and future directions</article-title><source>Front. Plant Sci.</source><year>2025</year><volume>16</volume><elocation-id>1538163</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2025.1538163</pub-id><pub-id pub-id-type="pmid">40061031</pub-id><pub-id pub-id-type="pmcid">PMC11885274</pub-id></element-citation></ref><ref id="B30-plants-14-02634"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pacal</surname><given-names>I.</given-names></name><name name-style="western"><surname>Kunduracioglu</surname><given-names>I.</given-names></name><name name-style="western"><surname>Alma</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Deveci</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kadry</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nedoma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Slany</surname><given-names>V.</given-names></name><name name-style="western"><surname>Martinek</surname><given-names>R.</given-names></name></person-group><article-title>A systematic review of deep learning techniques for plant diseases</article-title><source>Artif. Intell. Rev.</source><year>2024</year><volume>57</volume><fpage>304</fpage><pub-id pub-id-type="doi">10.1007/s10462-024-10944-7</pub-id></element-citation></ref><ref id="B31-plants-14-02634"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Child</surname><given-names>R.</given-names></name><name name-style="western"><surname>Gray</surname><given-names>S.</given-names></name><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sutskever</surname><given-names>I.</given-names></name></person-group><article-title>Generating long sequences with sparse transformers</article-title><source>arXiv</source><year>2019</year><volume>1904</volume><fpage>10509</fpage></element-citation></ref><ref id="B32-plants-14-02634"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bouacida</surname><given-names>I.</given-names></name><name name-style="western"><surname>Farou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Djakhdjakha</surname><given-names>L.</given-names></name><name name-style="western"><surname>Seridi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kurulay</surname><given-names>M.</given-names></name></person-group><article-title>Innovative deep learning approach for cross-crop plant disease detection: A generalized method for identifying unhealthy leaves</article-title><source>Inf. Process. Agric.</source><year>2025</year><volume>12</volume><fpage>54</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.inpa.2024.03.002</pub-id></element-citation></ref><ref id="B33-plants-14-02634"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Elinisa</surname><given-names>C.A.</given-names></name><name name-style="western"><surname>Wa Maina</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vodacek</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mduma</surname><given-names>N.</given-names></name></person-group><article-title>Image Segmentation Deep Learning Model for Early Detection of Banana Diseases</article-title><source>Appl. Artif. Intell.</source><year>2025</year><volume>39</volume><fpage>2440837</fpage><pub-id pub-id-type="doi">10.1080/08839514.2024.2440837</pub-id></element-citation></ref><ref id="B34-plants-14-02634"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shafik</surname><given-names>W.</given-names></name><name name-style="western"><surname>Tufail</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liyanage De Silva</surname><given-names>C.</given-names></name><name name-style="western"><surname>Apong</surname><given-names>R.A.A.H.</given-names></name></person-group><article-title>A novel hybrid inception-xception convolutional neural network for efficient plant disease classification and detection</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><elocation-id>3936</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-82857-y</pub-id><pub-id pub-id-type="pmid">39890849</pub-id><pub-id pub-id-type="pmcid">PMC11785716</pub-id></element-citation></ref><ref id="B35-plants-14-02634"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alhwaiti</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Asim</surname><given-names>M.</given-names></name><name name-style="western"><surname>Siddiqi</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Ishaq</surname><given-names>M.</given-names></name><name name-style="western"><surname>Alruwaili</surname><given-names>M.</given-names></name></person-group><article-title>Leveraging YOLO deep learning models to enhance plant disease identification</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><elocation-id>7969</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-025-92143-0</pub-id><pub-id pub-id-type="pmid">40055410</pub-id><pub-id pub-id-type="pmcid">PMC11889226</pub-id></element-citation></ref><ref id="B36-plants-14-02634"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mukherjee</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ghosh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chakraborty</surname><given-names>C.</given-names></name><name name-style="western"><surname>Narayan De</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mishra</surname><given-names>D.P.</given-names></name></person-group><article-title>Rice leaf disease identification and classification using machine learning techniques: A comprehensive review</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>139</volume><fpage>109639</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.109639</pub-id></element-citation></ref><ref id="B37-plants-14-02634"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mehzabeen</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Gayathri</surname><given-names>R.</given-names></name></person-group><article-title>Heuristically improvised rice disease classification framework based on adaptive segmentation with the fusion of LSTM layer into Multi-Scale Residual Attention Network</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>99</volume><elocation-id>106875</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2024.106875</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="plants-14-02634-f001" orientation="portrait"><label>Figure 1</label><caption><p>(<bold>A</bold>) compares IoU performance across models, with our mobile variant achieving the highest accuracy (92.1%). (<bold>B</bold>) visualizes the performance-efficiency trade-off via scatter plot (IoU vs. FLOPs), where circle area encodes parameter count; our method occupies the Pareto-optimal front with 41% fewer FLOPs than SAM. (<bold>C</bold>) presents ablation study results, indicating performance variations across configurations. (<bold>D</bold>) illustrates cross-dataset generalization, where Ours (Full) maintains superior performance consistently.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g001.jpg"/></fig><fig position="float" id="plants-14-02634-f002" orientation="portrait"><label>Figure 2</label><caption><p>(<bold>A</bold>) compares IoU performance versus computational cost (FLOPs), while (<bold>B</bold>) evaluates model size (parameters), collectively demonstrating that the proposed method achieves the highest accuracy while significantly reducing computational and memory costs. (<bold>C</bold>) validates the sparse attention&#8217;s computational efficiency, showing sub-linear complexity growth with increasing resolution that enables deployment on high-resolution agricultural imagery. (<bold>D</bold>) compares memory usage and inference speed, with the proposed method outperforming the standard SAM in both memory efficiency and processing speed.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g002.jpg"/></fig><fig position="float" id="plants-14-02634-f003" orientation="portrait"><label>Figure 3</label><caption><p>Visualization of sparse attention patterns and expert activation in the Sparse-MoE-SAM framework.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g003.jpg"/></fig><fig position="float" id="plants-14-02634-f004" orientation="portrait"><label>Figure 4</label><caption><p>Sparse-MoE-SAM framework architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g004.jpg"/></fig><fig position="float" id="plants-14-02634-f005" orientation="portrait"><label>Figure 5</label><caption><p>Standard scaled dot-product attention mechanism [<xref rid="B31-plants-14-02634" ref-type="bibr">31</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g005.jpg"/></fig><fig position="float" id="plants-14-02634-f006" orientation="portrait"><label>Figure 6</label><caption><p>Illustration of the proposed sparse attention mechanism with Gumbel-Top-k sampling [<xref rid="B31-plants-14-02634" ref-type="bibr">31</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g006.jpg"/></fig><fig position="float" id="plants-14-02634-f007" orientation="portrait"><label>Figure 7</label><caption><p>Adaptive sparsity pattern design with gumbel perturbation for query-key selection [<xref rid="B31-plants-14-02634" ref-type="bibr">31</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g007.jpg"/></fig><fig position="float" id="plants-14-02634-f008" orientation="portrait"><label>Figure 8</label><caption><p>Attention diversity regularization and computational reduction effect [<xref rid="B31-plants-14-02634" ref-type="bibr">31</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g008.jpg"/></fig><fig position="float" id="plants-14-02634-f009" orientation="portrait"><label>Figure 9</label><caption><p>Computational complexity comparison between dense and sparse attention [<xref rid="B31-plants-14-02634" ref-type="bibr">31</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g009.jpg"/></fig><fig position="float" id="plants-14-02634-f010" orientation="portrait"><label>Figure 10</label><caption><p>Architecture of the dual-stage mixture of experts decoder.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="plants-14-02634-g010.jpg"/></fig><table-wrap position="float" id="plants-14-02634-t001" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t001_Table 1</object-id><label>Table 1</label><caption><p>Detailed description of plant disease segmentation datasets used in this study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Images</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Classes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Resolution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Annotation Type</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Occlusion<break/> Level</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Lighting<break/> Diversity</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Use Case</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PlantVillage<break/>Extended</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87,848</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256 &#215; 256</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Binary disease masks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Core<break/>training/testing</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CVPPP Leaf<break/>Segmentation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4477</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Varied</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Instance leaf masks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Robustness<break/>validation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Custom<break/>Agricultural Field</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12,340</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16+</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">512 &#215; 512</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Expert-verified masks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Real-world<break/>deployment</td></tr></tbody></table></table-wrap><table-wrap position="float" id="plants-14-02634-t003" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t003_Table 3</object-id><label>Table 3</label><caption><p>Quantitative results on the PlantVillage extended dataset. The best case scenarios for each indicator are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">IoU (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dice (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Hausdorff Dist. (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">88.4</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.42</td><td align="center" valign="middle" rowspan="1" colspan="1">31.0</td><td align="center" valign="middle" rowspan="1" colspan="1">15.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Attention U-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">88.1</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td><td align="center" valign="middle" rowspan="1" colspan="1">3.18</td><td align="center" valign="middle" rowspan="1" colspan="1">34.9</td><td align="center" valign="middle" rowspan="1" colspan="1">18.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepLabV3+</td><td align="center" valign="middle" rowspan="1" colspan="1">89.4</td><td align="center" valign="middle" rowspan="1" colspan="1">92.1</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">2.95</td><td align="center" valign="middle" rowspan="1" colspan="1">41.3</td><td align="center" valign="middle" rowspan="1" colspan="1">22.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">90.2</td><td align="center" valign="middle" rowspan="1" colspan="1">92.8</td><td align="center" valign="middle" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" rowspan="1" colspan="1">90.4</td><td align="center" valign="middle" rowspan="1" colspan="1">2.73</td><td align="center" valign="middle" rowspan="1" colspan="1">64.1</td><td align="center" valign="middle" rowspan="1" colspan="1">31.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Standard SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7</td><td align="center" valign="middle" rowspan="1" colspan="1">94.2</td><td align="center" valign="middle" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4</td><td align="center" valign="middle" rowspan="1" colspan="1">2.41</td><td align="center" valign="middle" rowspan="1" colspan="1">636.0</td><td align="center" valign="middle" rowspan="1" colspan="1">187.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FastSAM</td><td align="center" valign="middle" rowspan="1" colspan="1">90.8</td><td align="center" valign="middle" rowspan="1" colspan="1">93.5</td><td align="center" valign="middle" rowspan="1" colspan="1">92.7</td><td align="center" valign="middle" rowspan="1" colspan="1">91.9</td><td align="center" valign="middle" rowspan="1" colspan="1">2.58</td><td align="center" valign="middle" rowspan="1" colspan="1">68.0</td><td align="center" valign="middle" rowspan="1" colspan="1">45.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours (Full)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>94.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>96.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>95.3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>94.8</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>1.87</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">142.7</td><td align="center" valign="middle" rowspan="1" colspan="1">142.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours (Mobile)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="plants-14-02634-t004" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance breakdown by dataset. The best case scenarios for each indicator are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PlantVillage IoU (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CVPPP IoU (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Custom Field IoU (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average IoU (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">U-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td><td align="center" valign="middle" rowspan="1" colspan="1">82.1</td><td align="center" valign="middle" rowspan="1" colspan="1">79.8</td><td align="center" valign="middle" rowspan="1" colspan="1">82.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Attention U-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">88.1</td><td align="center" valign="middle" rowspan="1" colspan="1">84.3</td><td align="center" valign="middle" rowspan="1" colspan="1">81.5</td><td align="center" valign="middle" rowspan="1" colspan="1">84.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DeepLabV3+</td><td align="center" valign="middle" rowspan="1" colspan="1">89.4</td><td align="center" valign="middle" rowspan="1" colspan="1">85.7</td><td align="center" valign="middle" rowspan="1" colspan="1">82.9</td><td align="center" valign="middle" rowspan="1" colspan="1">86.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SegFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">90.2</td><td align="center" valign="middle" rowspan="1" colspan="1">86.1</td><td align="center" valign="middle" rowspan="1" colspan="1">83.2</td><td align="center" valign="middle" rowspan="1" colspan="1">86.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Standard SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7</td><td align="center" valign="middle" rowspan="1" colspan="1">87.3</td><td align="center" valign="middle" rowspan="1" colspan="1">84.8</td><td align="center" valign="middle" rowspan="1" colspan="1">87.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FastSAM</td><td align="center" valign="middle" rowspan="1" colspan="1">90.8</td><td align="center" valign="middle" rowspan="1" colspan="1">86.8</td><td align="center" valign="middle" rowspan="1" colspan="1">84.1</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ours (Full)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>94.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>89.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>87.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>90.4</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours (Mobile)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="plants-14-02634-t005" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t005_Table 5</object-id><label>Table 5</label><caption><p>Comprehensive efficiency analysis. The best case scenarios for each indicator are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Memory (GB)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Inference Time (ms)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Throughput (FPS)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Energy (mJ)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Speed-Up</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Memory Reduction</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>2.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>45</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>22.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>127</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">68.7%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Attention U-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">2.8</td><td align="center" valign="middle" rowspan="1" colspan="1">52</td><td align="center" valign="middle" rowspan="1" colspan="1">19.2</td><td align="center" valign="middle" rowspan="1" colspan="1">156</td><td align="center" valign="middle" rowspan="1" colspan="1">2.0&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">58.2%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepLabV3+</td><td align="center" valign="middle" rowspan="1" colspan="1">3.4</td><td align="center" valign="middle" rowspan="1" colspan="1">67</td><td align="center" valign="middle" rowspan="1" colspan="1">14.9</td><td align="center" valign="middle" rowspan="1" colspan="1">198</td><td align="center" valign="middle" rowspan="1" colspan="1">1.5&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">49.3%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">5.2</td><td align="center" valign="middle" rowspan="1" colspan="1">89</td><td align="center" valign="middle" rowspan="1" colspan="1">11.2</td><td align="center" valign="middle" rowspan="1" colspan="1">267</td><td align="center" valign="middle" rowspan="1" colspan="1">1.1&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">23.1%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Standard SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">15.8</td><td align="center" valign="middle" rowspan="1" colspan="1">234</td><td align="center" valign="middle" rowspan="1" colspan="1">4.3</td><td align="center" valign="middle" rowspan="1" colspan="1">1024</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FastSAM</td><td align="center" valign="middle" rowspan="1" colspan="1">4.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78</td><td align="center" valign="middle" rowspan="1" colspan="1">12.8</td><td align="center" valign="middle" rowspan="1" colspan="1">312</td><td align="center" valign="middle" rowspan="1" colspan="1">3.0&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">70.9%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours (Full)</td><td align="center" valign="middle" rowspan="1" colspan="1">6.7</td><td align="center" valign="middle" rowspan="1" colspan="1">102</td><td align="center" valign="middle" rowspan="1" colspan="1">9.8</td><td align="center" valign="middle" rowspan="1" colspan="1">421</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">57.6%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours (Mobile)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">189</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.0&#215;</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>80.4%</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="plants-14-02634-t006" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t006_Table 6</object-id><label>Table 6</label><caption><p>Attention diversity and sparsity analysis.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Attention Head</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sparsity Ratio (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Entropy Score</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Focus Regions</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pathologist<break/>Correlation</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 1</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">2.14</td><td align="center" valign="middle" rowspan="1" colspan="1">Fine lesions</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 2</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">2.08</td><td align="center" valign="middle" rowspan="1" colspan="1">Large necrotic areas</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 3</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td><td align="center" valign="middle" rowspan="1" colspan="1">2.19</td><td align="center" valign="middle" rowspan="1" colspan="1">Leaf boundaries</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 4</td><td align="center" valign="middle" rowspan="1" colspan="1">90.3</td><td align="center" valign="middle" rowspan="1" colspan="1">2.11</td><td align="center" valign="middle" rowspan="1" colspan="1">Color transitions</td><td align="center" valign="middle" rowspan="1" colspan="1">0.85</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 5</td><td align="center" valign="middle" rowspan="1" colspan="1">92.1</td><td align="center" valign="middle" rowspan="1" colspan="1">2.06</td><td align="center" valign="middle" rowspan="1" colspan="1">Texture variations</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 6</td><td align="center" valign="middle" rowspan="1" colspan="1">89.4</td><td align="center" valign="middle" rowspan="1" colspan="1">2.16</td><td align="center" valign="middle" rowspan="1" colspan="1">Vein patterns</td><td align="center" valign="middle" rowspan="1" colspan="1">0.81</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 7</td><td align="center" valign="middle" rowspan="1" colspan="1">90.8</td><td align="center" valign="middle" rowspan="1" colspan="1">2.09</td><td align="center" valign="middle" rowspan="1" colspan="1">Disease progression</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Head 8</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.07</td><td align="center" valign="middle" rowspan="1" colspan="1">Background regions</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.83</td></tr></tbody></table></table-wrap><table-wrap position="float" id="plants-14-02634-t007" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of the Sparse-MoE-SAM ablation experiments by module.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Configuration</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">IoU (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Improvement</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Baseline SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">91.7</td><td align="center" valign="middle" rowspan="1" colspan="1">187.3</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">+Sparse Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4</td><td align="center" valign="middle" rowspan="1" colspan="1">156.2</td><td align="center" valign="middle" rowspan="1" colspan="1">+0.7% IoU, &#8722;16.6% FLOPs</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">+MoE (Stage 1)</td><td align="center" valign="middle" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" rowspan="1" colspan="1">148.7</td><td align="center" valign="middle" rowspan="1" colspan="1">+1.4% IoU, &#8722;20.6% FLOPs</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+Enhanced ASPP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">145.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+1.9% IoU, &#8722;22.4% FLOPs</td></tr></tbody></table></table-wrap><table-wrap position="float" id="plants-14-02634-t008" orientation="portrait"><object-id pub-id-type="pii">plants-14-02634-t008_Table 8</object-id><label>Table 8</label><caption><p>Summary of loss function components.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Loss Component</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Formula</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Weight</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Purpose</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm292" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>balance</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>sparsity</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Joint optimization</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Segmentation<break/>Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm293" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>BCE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm294" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>seg</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pixel-wise accuracy<break/>+ global overlap</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Balance<break/>Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm295" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:mfrac></mml:mstyle><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>E</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm296" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Expert diversity</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sparsity<break/>Regularization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm297" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm298" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Attention sparsity</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>