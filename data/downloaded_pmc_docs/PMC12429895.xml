<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Materials (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Materials (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">3169</journal-id><journal-id journal-id-type="pmc-domain">materials</journal-id><journal-id journal-id-type="publisher-id">materials</journal-id><journal-title-group><journal-title>Materials</journal-title></journal-title-group><issn pub-type="epub">1996-1944</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12429895</article-id><article-id pub-id-type="pmcid-ver">PMC12429895.1</article-id><article-id pub-id-type="pmcaid">12429895</article-id><article-id pub-id-type="pmcaiid">12429895</article-id><article-id pub-id-type="doi">10.3390/ma18174074</article-id><article-id pub-id-type="publisher-id">materials-18-04074</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-Algorithm Ensemble Learning Framework for Predicting the Solder Joint Reliability of Wafer-Level Packaging</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Su</surname><given-names initials="Q">Qinghua</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0984-0164</contrib-id><name name-style="western"><surname>Chiang</surname><given-names initials="KN">Kuo-Ning</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="c1-materials-18-04074" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Strek</surname><given-names initials="T">Tomasz</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-materials-18-04074">Department of Power Mechanical Engineering, National Tsing Hua University, Hsinchu City 300044, Taiwan; <email>0967356474shq@gmail.com</email></aff><author-notes><corresp id="c1-materials-18-04074"><label>*</label>Correspondence: <email>knchiang@pme.nthu.edu.tw</email>; Tel.: +886-03-574-2925</corresp></author-notes><pub-date pub-type="epub"><day>30</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>18</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496788</issue-id><elocation-id>4074</elocation-id><history><date date-type="received"><day>31</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>25</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>28</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>30</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="materials-18-04074.pdf"/><abstract><p>To enhance design efficiency, this study employs an effective prediction approach that utilizes validated finite element analysis (FEA) to generate simulation data and subsequently applies machine learning (ML) techniques to predict packaging reliability. Validated FEA models are used to replace the costly design-on-experiment approach. However, the training time for some ML algorithms is costly; therefore, reducing the size of the training dataset to lower computational cost is a critical issue for ML. Nevertheless, this approach simultaneously introduces new challenges in maintaining prediction accuracy due to the inherent limitations of small data machine learning. To address these challenges, this work adopts Wafer-Level Packaging (WLP) as a case study. It proposes an ensemble learning framework that integrates multiple machine learning algorithms to enhance predictive robustness. By leveraging the complementary strengths of different algorithms and frameworks, the ensemble approach effectively improves generalization, enabling accurate predictions even with constrained training data.</p></abstract><kwd-group><kwd>Wafer-Level Packaging (WLP)</kwd><kwd>finite element analysis (FEA)</kwd><kwd>machine learning</kwd><kwd>ensemble learning</kwd></kwd-group><funding-group><award-group><funding-source>National Tsing Hua University, Taiwan</funding-source></award-group><funding-statement>This research is partially supported by the National Tsing Hua University, Taiwan.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-materials-18-04074"><title>1. Introduction</title><p>Long-term reliability remains a central concern in electronic packaging, particularly as device miniaturization and functional integration continue to advance. Among advanced packaging technologies, WLP has gained significant traction due to its compact form factor and superior electrical performance, as demonstrated by the study of Chen et al. [<xref rid="B1-materials-18-04074" ref-type="bibr">1</xref>], who highlighted the challenges and prospects for advanced packaging. However, the inherent mismatch in the coefficients of thermal expansion (CTE) between dissimilar materials frequently induces thermomechanical stress, leading to failure in solder joints. Ismail et al. [<xref rid="B2-materials-18-04074" ref-type="bibr">2</xref>] further reviewed the effects of extreme conditions on solder joint reliability and confirmed that CTE mismatch is a primary driver of failure mechanisms under thermal loading. Notably, the solder ball located at the farthest Distance to Neutral Point (DNP) is often the most susceptible to damage.</p><p>To evaluate the thermal reliability of electronic packaging, the accelerated thermal cycling test (ATCT) is widely employed by the electronic packaging industry, as highlighted in the work of Bender et al. [<xref rid="B3-materials-18-04074" ref-type="bibr">3</xref>], who discussed modern trends in microelectronics packaging reliability testing. Although experimental validation methods are robust and widely accepted, they are typically associated with high costs and extended time requirements, making them inefficient for iterative design optimization. As a result, FEA has emerged as a valuable tool during the early design stages, offering a computational means to assess reliability performance. For instance, Liu et al. [<xref rid="B4-materials-18-04074" ref-type="bibr">4</xref>] developed a three-dimensional FEA model of a Wafer-Level Chip Scale Package (WLCSP) to estimate the incremental equivalent plastic strain, showing good agreement with experimental data. Likewise, Wu et al. [<xref rid="B5-materials-18-04074" ref-type="bibr">5</xref>] proposed a two-dimensional symmetric half-diagonal model to reduce simulation time while maintaining prediction accuracy.</p><p>Despite its advantages, FEA-based simulation requires considerable domain expertise and is prone to variability due to modeling assumptions, mesh sensitivity, and the researcher&#8217;s experience. To address these limitations, a data-driven strategy [<xref rid="B6-materials-18-04074" ref-type="bibr">6</xref>] is proposed: generating datasets from validated FEA simulations to train machine learning models for rapid and consistent reliability predictions. This approach minimizes reliance on manual FEA modeling and enables real-time estimation once the AI model is trained. A detailed discussion of how to validate the simulation method will be provided in a later section.</p><p>However, generating high-quality datasets still incurs computational costs. Therefore, it is essential to develop machine learning strategies that can effectively operate under the constraint of small data. In this study, ensemble learning, which integrates multiple predictive algorithms and frameworks, offers a promising solution to improve generalization. Su et al. [<xref rid="B6-materials-18-04074" ref-type="bibr">6</xref>] demonstrated the feasibility of using ensemble neural networks trained on small datasets to predict WLP reliability with high accuracy. Building upon this foundation, the present study further investigates ensemble learning architectures that combine diverse machine learning algorithms to enhance prediction performance. In addition, a novel ensemble framework is proposed in this study, where the 1000-cycle threshold serves as a decision boundary to guide the weighting of base models and improve prediction accuracy.</p><p>For algorithm selection, beyond the widely adopted Artificial Neural Network (ANN), prior work by Sunil et al. [<xref rid="B7-materials-18-04074" ref-type="bibr">7</xref>] identified additional models with strong reliability prediction capabilities, such as Gaussian Process Regression (GPR), which provides probabilistic predictions with uncertainty quantification, and Recurrent Neural Network (RNN), which captures temporal dependencies in sequential data. Further discussion on the algorithmic mechanisms and the reliability empirical model will be presented in the subsequent section.</p><p>As an overview, <xref rid="materials-18-04074-f001" ref-type="fig">Figure 1</xref> outlines the complete workflow developed for small datasets, which serves as the foundation for the discussions in the following sections.</p></sec><sec id="sec2-materials-18-04074"><title>2. Fundamental Theory</title><sec id="sec2dot1-materials-18-04074"><title>2.1. Coffin&#8211;Manson Model</title><p>In reliability life prediction of packaging structures, the methodologies are generally categorized into two types: strain-based and energy-based approaches. The present study adopts a strain-based approach, specifically utilizing the Coffin&#8211;Manson model. Following FEA, the increment of equivalent plastic strain is extracted, which serves as a critical parameter for estimating the fatigue life of solder joints. The Coffin&#8211;Manson relationship is formulated as shown in Equation (1) [<xref rid="B8-materials-18-04074" ref-type="bibr">8</xref>].<disp-formula id="FD1-materials-18-04074"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted fatigue life (in cycles), and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the increment of equivalent plastic strain obtained by simulation. In this study, the coefficients <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, determined through regression analysis, are empirical constants with values of 0.235 and &#8722;1.75 [<xref rid="B6-materials-18-04074" ref-type="bibr">6</xref>], respectively.</p></sec><sec id="sec2dot2-materials-18-04074"><title>2.2. Artificial Neural Network (ANN)</title><p>As a classical machine learning model for handling nonlinear regression problems, the ANN is a computational model inspired by the architecture and functioning of biological neural systems. Over the decades, ANN has evolved into modern forms such as multi-layer perceptrons (MLP), which are widely employed as supervised learning models to approximate a target function <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>&#183;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. As illustrated in <xref rid="materials-18-04074-f002" ref-type="fig">Figure 2</xref>, each neuron, which serves as the fundamental unit of the network, receives multiple numerical inputs, each associated with an individual weight. These inputs undergo a weighted summation, and the result is processed through an activation function, which introduces nonlinearity into the model and enables it to capture complex relationships between inputs and outputs. In this study, the Rectified Linear Unit (ReLU) function [<xref rid="B9-materials-18-04074" ref-type="bibr">9</xref>], as defined in Equation (2), is adopted as the fixed activation function, considering both training efficiency and generalization performance, which is particularly beneficial in shallow networks with limited training data.<disp-formula id="FD2-materials-18-04074"><label>(2)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mi>x</mml:mi><mml:mo>&#8804;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The main components of an ANN include the input layer, hidden layer(s), and output layer. Each layer is composed of neurons, and connections are established between neurons in adjacent layers. <xref rid="materials-18-04074-f003" ref-type="fig">Figure 3</xref> illustrates the structure of an ANN with a single hidden layer. This type of structure, where information flows in a single direction (as indicated by the arrows) without any loops or feedback paths, is referred to as a feedforward neural network. The inclusion of bias terms enhances the model&#8217;s generalization ability, helping to prevent both overfitting and underfitting.</p><p>To train such a network effectively, a learning algorithm is required to adjust the connection weights based on the observed error. One of the most widely adopted algorithms for this purpose is the Backpropagation (BP) algorithm [<xref rid="B10-materials-18-04074" ref-type="bibr">10</xref>], which enables supervised learning by iteratively minimizing the difference between predicted and actual outputs. This process is typically carried out using an optimization method, commonly referred to as a solver, which updates the weights by following the gradient of a defined loss function. For regression problems, the loss is typically measured using the mean square error (MSE), as shown in Equation (3).<disp-formula id="FD3-materials-18-04074"><label>(3)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of training data, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> presents the target value, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> presents the predicted value.</p><p>The choice of solver, such as Adaptive Moment Estimation (Adam) or Limited-memory Broyden&#8211;Fletcher&#8211;Goldfarb&#8211;Shanno (L-BFGS), influences the convergence behavior and the final performance of the network.</p><p>Adam is a first-order optimization algorithm that adaptively adjusts learning rates using estimates of the first and second moments of gradients. Its computational efficiency and robustness make it well-suited for training deep networks and handling large-scale datasets.</p><p>In contrast, L-BFGS is a quasi-Newton method that approximates second-order curvature information to accelerate convergence. It is particularly effective in small-data scenarios and shallow network architectures.</p><p>The task addressed in this study aligns well with the characteristics of small-data and shallow-network scenarios. Consistent with this, experimental results indicate that the L-BFGS solver outperforms Adam in terms of training performance [<xref rid="B11-materials-18-04074" ref-type="bibr">11</xref>]. However, to promote model diversity within the ensemble framework, both solvers are employed in this work.</p></sec><sec id="sec2dot3-materials-18-04074"><title>2.3. Recurrent Neural Network (RNN)</title><p>RNN constitutes a distinct class of neural architecture developed to process sequential data by capturing temporal dependencies and dynamic patterns. RNN incorporates a hidden state that is recursively updated at each time step, allowing the network to preserve and integrate information across different positions in the input sequence. This internal memory structure enables the capture of both long-term and short-term dependencies in data where temporal order or sequential structure is relevant. Although the target task in this study is a conventional, non-sequential regression problem, RNN has still demonstrated strong predictive capability. More importantly, their unique architecture allows them to learn feature representations that differ from those of feedforward models, thereby contributing complementary insights within the ensemble framework and enhancing overall prediction robustness.</p><p><xref rid="materials-18-04074-f004" ref-type="fig">Figure 4</xref> presents a simplified illustration of the RNN structure. The left side of the figure highlights the recurrent connection using a self-loop, while the right side depicts the network unrolled over three consecutive time steps: <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Equation (4) defines the general expression of the output <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> without bias:<disp-formula id="FD4-materials-18-04074"><label>(4)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>U</mml:mi><mml:mo>&#8901;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mo>&#8901;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where both <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8901;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8901;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> are used as activation functions. <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the weight matrices that remain shared across all time steps in this RNN structure.</p><p>This structure is also referred to as the vanilla (simple) RNN unit. It computes the current state value solely based on the input at the current time step and the state value from the previous time step, without incorporating any additional processing mechanisms. While the simple RNN unit provides a foundational framework for processing sequential data by maintaining a hidden state across time steps, it often encounters difficulties in capturing long-term dependencies due to the vanishing and exploding gradient problems during training. To address these limitations, Long Short-Term Memory (LSTM) networks [<xref rid="B12-materials-18-04074" ref-type="bibr">12</xref>] were introduced as an improved recurrent architecture, as shown in <xref rid="materials-18-04074-f005" ref-type="fig">Figure 5</xref>. LSTM extends the memory capability of RNNs by introducing a memory cell along with three gating mechanisms: the input gate, the forget gate, and the output gate. These components work together to regulate the flow of information over time. This structure enables LSTM networks to selectively preserve or discard information, which makes them particularly effective in capturing long-term dependencies in sequential data.</p><p>The update rule is defined in Equations (5)&#8211;(10).<disp-formula id="FD5-materials-18-04074"><label>(5)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-materials-18-04074"><label>(6)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-materials-18-04074"><label>(7)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">tanh</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-materials-18-04074"><label>(8)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-materials-18-04074"><label>(9)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-materials-18-04074"><label>(10)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the input, hidden state, and cell state at time step <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. The symbols <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> correspond to the forget gate, input gate, and output gate. The term <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the candidate cell state. <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the weight matrices and bias vectors associated with each gate. The function <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes sigmoid activation, while <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the hyperbolic tangent function. The operator <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8857;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates Hadamard multiplication.</p><p>As a type of neural network architecture, RNN also adopts a training mechanism based on the backpropagation algorithm [<xref rid="B13-materials-18-04074" ref-type="bibr">13</xref>]. The selection of several key hyperparameters remains consistent with that used in the feedforward ANN model based on a trade-off between prediction accuracy and training efficiency, as supported by empirical observations in this study. Specifically, the ReLU function is employed as the activation function, and the Adam optimizer is adopted. It should be noted that the use of ReLU here refers to the activation applied at the output of each RNN unit, which is distinct from the sigmoid and tanh functions internally used within LSTM units for gate operations.</p><p>It is worth emphasizing that, according to the study by Sunil et al. [<xref rid="B7-materials-18-04074" ref-type="bibr">7</xref>], the use of mean absolute percentage error (MAPE) as the loss function yields slightly better performance than MSE under shallow network architectures with a limited number of neurons. Therefore, MAPE is adopted in this study as shown in Equation (11), where all <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> values are positive.<disp-formula id="FD11-materials-18-04074"><label>(11)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Additionally, a single-directional RNN is used to reduce training costs, as bidirectional models require higher computation due to their dual-directional processing. Like the ANN, both simple RNN units and LSTM units are employed in this study to enhance the diversity of the base models.</p></sec><sec id="sec2dot4-materials-18-04074"><title>2.4. Gaussian Process Regression (GPR)</title><p>As a non-neural network algorithm for training base models, GPR has demonstrated strong performance under a small amount of training data, which is theoretically supported by its Bayesian formulation and the use of kernel functions to reflect prior knowledge about the target function. This allows the model to generalize effectively even with limited training data.</p><p>Unlike parametric models, GPR formulates the regression problem in a fully probabilistic manner by directly modeling the latent function, as illustrated in <xref rid="materials-18-04074-f006" ref-type="fig">Figure 6</xref>. A major advantage of this approach is its inherent ability to quantify predictive uncertainty through the posterior distribution [<xref rid="B14-materials-18-04074" ref-type="bibr">14</xref>]. GPR assumes that the target function f(x) follows a Gaussian Process prior, while observational noise is modeled as an independent Gaussian distribution, as expressed in Equation (12).<disp-formula id="FD12-materials-18-04074"><label>(12)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#949;</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mtext>&#160;</mml:mtext><mml:mo>~</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>G</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>k</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mtext>&#160;</mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mtext>&#160;</mml:mtext><mml:mi>&#949;</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>~</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the function, which is typically assumed to be zero, and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the kernel function that defines the covariance between input points <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Let <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the training inputs and their corresponding observations. <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the test inputs and predicted outputs derived from the latent function <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The joint distribution of the above parameters is given in Equation (13).<disp-formula id="FD14-materials-18-04074"><label>(13)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mtext>&#160;</mml:mtext><mml:mo>~</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>N</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi></mml:mtd><mml:mtd><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> means a noise term, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the covariance matrix, and 0 denotes the simplified expression of <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The predictive mean and covariance for the test outputs can be derived from the conditional properties of multivariate Gaussian distributions, as shown in Equations (14) and (15).<disp-formula id="FD15-materials-18-04074"><label>(14)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-materials-18-04074"><label>(15)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This framework can adapt to the intrinsic structure of the data while simultaneously optimizing its hyperparameters, which are determined by the choice of the kernel function, via maximum likelihood estimation, enabling flexible and precise modeling of complex functions. It is essential to emphasize that the selection of the kernel function has a significant impact on the effectiveness of GPR. In this study, the Mat&#233;rn kernel and the Radial Basis Function (RBF) kernel are employed, as defined in Equations (16) and (17), respectively.<disp-formula id="FD17-materials-18-04074"><label>(16)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#915;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>v</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>v</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-materials-18-04074"><label>(17)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#915;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>&#183;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the gamma function, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the smoothness parameter governing the differentiability of the function, <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mo>&#183;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the modified Bessel function of the second kind, <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the length scale, and <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the Euclidean distance between input points.</p><p>Once the kernel function is defined, the corresponding set of kernel hyperparameters <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to be optimized is also specified in the GPR framework. Although both the GPR and ANN models in this study employ variants of the L-BFGS algorithm for training, their respective optimization objectives differ fundamentally due to the underlying differences in model formulation. For ANN, the training objective is to minimize the MSE between predictions and targets. In contrast, GPR minimizes an objective function as well, where the training process aims to identify the optimal hyperparameters <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> by minimizing the negative log marginal likelihood (NLML) [<xref rid="B15-materials-18-04074" ref-type="bibr">15</xref>] of the observed data. The general form of the objective function is given as Equation (18):<disp-formula id="FD19-materials-18-04074"><label>(18)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">det</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the kernel matrix parameterized by <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector of observed targets.</p><p>To address this unconstrained optimization task, the &#8220;fmin_l_bfgs_b&#8221; algorithm from the SciPy library is utilized. This algorithm is a quasi-Newton method that approximates the inverse Hessian and allows for bound constraints, making it well-suited for maintaining kernel parameters within physically meaningful limits.</p><p>In practice, a noise term <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is also added to the kernel matrix to account for observation noise in the training data, resulting in the modified covariance expression <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This adjustment not only improves numerical stability but also more accurately captures the uncertainty inherent in real-world measurements. In implementation, this is achieved by specifying the parameter &#8220;alpha&#8221;, which can be interpreted as the variance of additional Gaussian noise associated with the training targets.</p></sec><sec id="sec2dot5-materials-18-04074"><title>2.5. Ensemble Learning</title><p>To mitigate the time cost associated with data generation, training on small datasets is often necessary. However, this constraint can result in unstable predictive performance of individual AI models due to overfitting. Ensemble learning provides an effective approach to improve prediction accuracy and robustness. This study adopts three ensemble strategies, namely Bagging, Boosting, and Stacking [<xref rid="B16-materials-18-04074" ref-type="bibr">16</xref>], as illustrated in <xref rid="materials-18-04074-f007" ref-type="fig">Figure 7</xref>, <xref rid="materials-18-04074-f008" ref-type="fig">Figure 8</xref> and <xref rid="materials-18-04074-f009" ref-type="fig">Figure 9</xref>.</p><p>In brief, Bagging improves prediction by averaging the outputs of multiple base learners, with optional weighting in regression tasks. Boosting assigns higher importance to previously mispredicted data points, which are defined as samples with large prediction errors, during successive training rounds and aggregates predictions through a weighted average. Stacking combines the outputs of several base learners as inputs to a meta-model, which is then trained to generate the final prediction.</p><p>In this study, ensemble learning is employed as an essential approach to enhance predictive reliability in small-data scenarios. By combining multiple machine learning models, including ANN, RNN, and GPR, using Bagging, boosting, and stacking strategies, the proposed framework improves accuracy, enhances stability, and reduces the risk of overfitting. These improvements contribute to a more robust and dependable solution for data-driven reliability prediction.</p></sec></sec><sec id="sec3-materials-18-04074"><title>3. FEA Validation of WLCSP</title><p>To reduce computational cost, a simplified two-dimensional finite element model is employed in this study, with the following fundamental assumptions: all materials are homogeneous and isotropic; the temperature within the structure is uniform; residual stresses are neglected; and perfect bonding is assumed at all material interfaces.</p><p>Let <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent the CTE mismatch between the substrate and the wafer and L denote the DNP of the solder joint. After material parameters are determined, &#8710;&#945; is treated as a fixed value. The mismatch in thermal deformation between the substrate/PCB and the die at the solder joint is expressed as <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#916;</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>&#916;</mml:mi><mml:mi>&#945;</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>&#916;</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Since this deformation difference increases with distance from the chip center, the outermost corner solder ball typically becomes the most critical location for failure initiation. A top view of the package layout is shown in <xref rid="materials-18-04074-f010" ref-type="fig">Figure 10</xref>. The red line indicates the half-diagonal direction, and the circles represent solder balls. <xref rid="materials-18-04074-f011" ref-type="fig">Figure 11</xref> presents the boundary condition configuration of the FEA model.</p><p>The FEA models are constructed according to the geometric specifications of the test vehicles (TVs) [<xref rid="B17-materials-18-04074" ref-type="bibr">17</xref>,<xref rid="B18-materials-18-04074" ref-type="bibr">18</xref>]. The model comprises the silicon chip, low-k layer, stress buffer layer (SBL), under-bump metallurgy (UBM), redistribution layer (RDL), printed circuit board (PCB), copper pad, solder mask, and solder ball, as shown in <xref rid="materials-18-04074-f012" ref-type="fig">Figure 12</xref>.</p><p>Following the approach proposed by Tsou [<xref rid="B2-materials-18-04074" ref-type="bibr">2</xref>], fixed mesh sizes are applied to critical regions to improve the accuracy and reliability of thermal simulations in the FEA model. As shown in <xref rid="materials-18-04074-f013" ref-type="fig">Figure 13</xref>, the mesh is refined to 7.5 <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#956;</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in height and 12.5 <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#956;</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in width.</p><p>It should be emphasized that Surface Evolver V2.70 [<xref rid="B19-materials-18-04074" ref-type="bibr">19</xref>] has been employed to estimate the geometric profile of the solder balls after reflow, and the coordinate data of key nodes have been extracted for FEA input. <xref rid="materials-18-04074-f014" ref-type="fig">Figure 14</xref> presents the reference geometry.</p><p>Apart from specifying the structural geometry and mesh size in the FEA model, material properties represent another key modeling requirement. Except for the solder ball, all materials in the model are assumed to be linear elastic, with their properties listed in <xref rid="materials-18-04074-t001" ref-type="table">Table 1</xref>, where T is expressed in degrees Celsius. The solder material used in this study is SAC305, whose Young&#8217;s modulus varies with temperature and exhibits pronounced nonlinear mechanical behavior. To characterize this nonlinearity, the Chaboche kinematic hardening model is applied.</p><p>As shown in <xref rid="materials-18-04074-f015" ref-type="fig">Figure 15</xref>, uniaxial tensile tests were conducted on SAC305 to obtain its stress&#8211;strain responses under different temperature conditions [<xref rid="B20-materials-18-04074" ref-type="bibr">20</xref>]. These experimental results serve as the basis for curve fitting to extract the material parameters required by the Chaboche model.</p><p>The corresponding formulation is given in Equation (19) [<xref rid="B21-materials-18-04074" ref-type="bibr">21</xref>]:<disp-formula id="FD20-materials-18-04074"><label>(19)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>&#947;</mml:mi><mml:msub><mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the initial yield stress, with <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> representing empirical coefficients determined through curve fitting. The corresponding values are listed in <xref rid="materials-18-04074-t002" ref-type="table">Table 2</xref>.</p><p>For the loading setup, thermal cycling is applied to the WLCSP model based on JEDEC standard Condition G, with the temperature varying between &#8722;40 &#176;C and 125 &#176;C. Each cycle consists of 10 min of dwell time at the temperature extremes and a heating/cooling rate of 16.5 &#176;C per minute, resulting in a total cycle duration of 40 min, as shown in <xref rid="materials-18-04074-f016" ref-type="fig">Figure 16</xref>.</p><p>Upon completing the necessary model setup and preprocessing, the Coffin&#8211;Manson equation is then applied to evaluate the simulated reliability of the package using the incremental equivalent plastic strain from the simulation result. <xref rid="materials-18-04074-t003" ref-type="table">Table 3</xref> presents a comparison of the mean time to failure (MTTF) between simulation results and experimental data for five TVs [<xref rid="B13-materials-18-04074" ref-type="bibr">13</xref>,<xref rid="B14-materials-18-04074" ref-type="bibr">14</xref>].</p><p>The discrepancies between the two sets of results remain within an acceptable margin, with deviations of less than 10%. This confirms the predictive accuracy of the FEA-based fatigue life estimation.</p><p>With the FEA models validated through comparison with experimental results, datasets have been subsequently generated to support machine learning. The database construction is based on fixed material properties, boundary conditions, and thermal loading, while systematically varying the geometric dimensions of the WLCSP structure. This database serves as the foundation for training the AI model.</p></sec><sec id="sec4-materials-18-04074"><title>4. Data Sampling and Model Training</title><sec id="sec4dot1-materials-18-04074"><title>4.1. Data Sampling</title><p>Before data generation, two critical aspects must be addressed to ensure the quality of the training dataset: feature selection and sample distribution. Feature selection relies on domain knowledge or numerical analysis to identify variables that are highly relevant to the prediction target, thereby enhancing both model accuracy and interpretability [<xref rid="B22-materials-18-04074" ref-type="bibr">22</xref>]. Meanwhile, in small-data scenarios, the distribution of samples plays a pivotal role. A well-structured dataset that adequately covers the input space is essential for improving generalization and ensuring reliable predictive performance.</p><p>The key factors influencing solder ball reliability in WLP are identified based on expert knowledge [<xref rid="B23-materials-18-04074" ref-type="bibr">23</xref>,<xref rid="B24-materials-18-04074" ref-type="bibr">24</xref>,<xref rid="B25-materials-18-04074" ref-type="bibr">25</xref>] and are listed in <xref rid="materials-18-04074-t004" ref-type="table">Table 4</xref>, along with their corresponding structural locations, as shown in <xref rid="materials-18-04074-f017" ref-type="fig">Figure 17</xref>.</p><p>This study focuses on constructing machine learning datasets based on the four influential parameters: upper pad diameter, lower pad diameter, SBL thickness, and chip thickness. All remaining structural variables are held constant, following the configuration of TV2. <xref rid="materials-18-04074-t005" ref-type="table">Table 5</xref> presents the defined value ranges of the four input features used for generating the dataset.</p><p>For data distribution, the training datasets are constructed using a combination of space-filling and adaptive sampling methods [<xref rid="B26-materials-18-04074" ref-type="bibr">26</xref>]. The base models are trained following the standard ensemble neural network procedure outlined by Su et al. [<xref rid="B27-materials-18-04074" ref-type="bibr">27</xref>]. To promote model diversity, each base model is trained using different data volumes and hyperparameter configurations.</p><p>Inspired by the concept of boosting, a three-step training process is designed, as summarized in <xref rid="materials-18-04074-t006" ref-type="table">Table 6</xref>.</p><p>Three datasets, each containing 144 samples, are prepared. In Step 1, Dataset 1 is used for training, and Dataset 2 serves as the test set. In Step 2, mispredicted samples from the previous test set are added to the training set, while Dataset 3 is used for testing. In Step 3, all available data are combined for training without a separate test set. The sampling details of the three datasets were established based on the adaptive sampling procedure proposed by Su et al. [<xref rid="B27-materials-18-04074" ref-type="bibr">27</xref>].</p><p>The grid search method [<xref rid="B28-materials-18-04074" ref-type="bibr">28</xref>] is employed at each step, resulting in multiple candidate models. Only the best-performing model is selected from each step. The selection criteria are based on the minimum average test difference in Steps 1 and 2. Due to the absence of testing data, the cross-validation (CV) score is used as the optimal criterion in Step 3. Finally, the generalization capability of the trained models is evaluated using a dataset consisting of 9601 samples [<xref rid="B27-materials-18-04074" ref-type="bibr">27</xref>].</p></sec><sec id="sec4dot2-materials-18-04074"><title>4.2. Prediction Results</title><p>To ensure consistency among the sub-models, all AI models employ the Robust Scaler, as shown in Equation (20), as the fixed preprocessing method [<xref rid="B29-materials-18-04074" ref-type="bibr">29</xref>]. This preprocessing technique reduces the impact of outliers, improves model robustness, and ensures balanced feature contributions by addressing scale inconsistencies.<disp-formula id="FD21-materials-18-04074"><label>(20)</label><mml:math id="mm82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the raw feature value, <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the corresponding scaled value, and the quartiles (<inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) are computed from the feature distribution.</p><p>The three algorithms, namely ANN, RNN, and GPR, are preliminarily combined based on the ensemble structure presented in <xref rid="materials-18-04074-f018" ref-type="fig">Figure 18</xref>.</p><p>The term &#8220;ULCS&#8221; denotes a set of four geometric structural parameters used as input features in the machine learning model. The optimal models from Steps 1, 2, and 3 are assigned as base models 1, 2, and 3&#8211;4, respectively. To enhance model diversity, base model 5 is constructed as a stacking model, which uses the outputs from base models 1 to 3 as its inputs.</p><p><xref rid="materials-18-04074-t007" ref-type="table">Table 7</xref>, <xref rid="materials-18-04074-t008" ref-type="table">Table 8</xref> and <xref rid="materials-18-04074-t009" ref-type="table">Table 9</xref> present the key hyperparameter settings used for training the predictive models with the three algorithms. As previously mentioned, to enhance diversity among base models, two solvers (Adam and L-BFGS) are employed for the ANN, two types of recurrent units (LSTM and Simple RNN) are selected for the RNN, and two kernels (Mat&#233;rn and RBF) are adopted for the GPR.</p><p>Additionally, &#8220;N_restarts&#8221; in <xref rid="materials-18-04074-t009" ref-type="table">Table 9</xref> refers to the number of optimizer restarts, which helps improve model performance by reducing the risk of convergence to suboptimal local minima.</p><p>Based on the optimal model selection criteria defined in <xref rid="materials-18-04074-t006" ref-type="table">Table 6</xref>, <xref rid="materials-18-04074-t010" ref-type="table">Table 10</xref>, <xref rid="materials-18-04074-t011" ref-type="table">Table 11</xref> and <xref rid="materials-18-04074-t012" ref-type="table">Table 12</xref> present the key hyperparameter settings and corresponding model performance for the five base models trained using the three algorithms.</p><p>The item &#8220;Neuron number&#8221; indicates the number of neurons in the three hidden layers. In &#8220;Maximum difference&#8221;, the listed values represent, in order, the absolute difference, target/prediction, and percentage error.</p><p>To improve computational efficiency, the RNN structure assigns the same number of neurons to each hidden layer, thereby significantly reducing the total time required for grid search.</p><p>At this stage, all 15 base models used for ensemble learning have been finalized. Their generalization performance is evaluated using a dataset comprising 9601 data points, and the results are summarized in <xref rid="materials-18-04074-t013" ref-type="table">Table 13</xref>. The values in parentheses represent, in order, the target value, the predicted value, and the percentage error.</p><p>The comparison shows that when the training dataset reaches its maximum size of 432, the average test difference of individual sub-models across all three algorithms remains around 10 cycles.</p><p>Overall, the GPR base models underperform compared to those based on ANN and RNN, which also negatively impacts the final ensemble results. The base models from the three algorithms are combined following the structure shown in <xref rid="materials-18-04074-f018" ref-type="fig">Figure 18</xref>, where the assigned weights are proportional to their respective training set sizes. In this framework, model weights are determined in proportion to the size of their respective training datasets, such that learners trained on larger datasets contribute more strongly to the final prediction. The ensemble models from each algorithm are then equally weighted and further aggregated into a second-level ensemble. The outcomes are presented in <xref rid="materials-18-04074-t014" ref-type="table">Table 14</xref>.</p><p>The first-level ensembles of ANN and RNN demonstrate notable improvements in prediction performance. In contrast, the ensemble results of GPR are suboptimal, mainly due to the limited accuracy of its individual base models. The second-level ensemble, combining ANN and RNN models, further enhances predictive accuracy, whereas the inclusion of GPR models offers no additional benefit. These results highlight that both model diversity and base-model accuracy are essential to ensemble effectiveness. Ensemble learning becomes truly beneficial only when sufficient predictive accuracy is achieved alongside adequate model diversity. Simply increasing the number of base models does not consistently lead to improved predictive performance.</p><p>Thus far, the performance of all ensemble models has been fully demonstrated. The optimal ensemble model achieves an average test error of only seven cycles, along with a notable reduction in the maximum test difference. These results demonstrate the advantage of ensemble learning in improving both the accuracy and stability of predictions. The ensemble not only reduces the average difference but also effectively suppresses extreme deviations, resulting in more reliable and consistent model performance.</p><p>The ensemble framework, as illustrated in <xref rid="materials-18-04074-f018" ref-type="fig">Figure 18</xref>, was designed to facilitate parallel data generation and model training. It adopts a staged training strategy, in which models are iteratively refined based on data generated at each step. Once the training dataset is finalized, the focus naturally shifts from data generation to exploring informative structures within the data that can guide the training of AI models.</p><p>In the case of WLP reliability studied here, the 1000-cycle mark [<xref rid="B30-materials-18-04074" ref-type="bibr">30</xref>] is widely recognized as a representative threshold in evaluating WLP reliability. Based on this understanding, a revised ensemble structure can be developed to enhance predictive performance further. The newly proposed ensemble framework is illustrated in <xref rid="materials-18-04074-f019" ref-type="fig">Figure 19</xref>.</p><p>All four AI models in the proposed framework are flexible in algorithm selection. This study only presents results using the ANN. The outputs <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>0,1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> correspond to three models: Model 0 is trained on samples with lifetimes below 1000 cycles (label 0), Model 1 on samples with lifetimes equal to or above 1000 cycles (label 1), and Model 2 on the entire dataset without threshold-based data partitioning. All AI models are optimized using Grid Search with Cross-Validation to determine the best hyperparameter configurations. The weights, as shown in <xref rid="materials-18-04074-f019" ref-type="fig">Figure 19</xref>, are defined by Equations (21) and (22).<disp-formula id="FD22-materials-18-04074"><label>(21)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD23-materials-18-04074"><label>(22)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>&#8901;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the output <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of &#8220;reg_model3&#8221;, which maps input features <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to a probability value in the range [0, 1], indicating the likelihood that the sample belongs to the high-reliability category (&#8805;1000 cycles). <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a normalized weight derived from the output <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the full-data (432) &#8220;reg_model2&#8221;, scaled linearly between predefined bounds (<inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and clipped to the range [0, 1] to ensure numerical stability. <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the lower and upper bounds of the mapping interval as 900 and 1100, respectively. The final ensemble weight <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is computed as the average of <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The final prediction performance of the two ensemble frameworks is compared in <xref rid="materials-18-04074-t015" ref-type="table">Table 15</xref>. In the evaluation using 9601 test samples, the new framework performs slightly better than the previous one. The second-level ensemble of two ensemble frameworks with the same weight further improved prediction performance, reducing the average test difference to 6.5 cycles.</p><p>On the other hand, the highest prediction difference consistently occurs at the same data point across two single-algorithm ensemble models, highlighting a key limitation of this approach. Without the introduction of new training data or an improved algorithm, it becomes difficult for models to enhance prediction accuracy on such high-difference samples.</p><p>In contrast, the high-error samples tend to differ across algorithms, and integrating multiple algorithms helps offset individual model weaknesses. As a result, multi-algorithm ensembles offer greater potential for improving prediction performance on outlier cases. Nevertheless, the effectiveness of this strategy still relies on the base models achieving sufficient predictive accuracy.</p><p>Finally, the ensemble model demonstrating the best overall performance was selected. Using the WLP with design parameters of upper pad diameter (240 &#181;m), lower pad diameter (220 &#181;m), chip thickness (330 &#181;m), and stress buffer layer thickness (7.5 &#181;m) as an example, the differences between the simulation and AI model are compared in <xref rid="materials-18-04074-t016" ref-type="table">Table 16</xref>. Once the AI model is trained and finalized, it enables the rapid evaluation of design parameter variations while ensuring consistency and accuracy in prediction results.</p></sec></sec><sec sec-type="conclusions" id="sec5-materials-18-04074"><title>5. Conclusions</title><p>This study examines ensemble learning with multiple algorithms to enhance the reliability prediction of WLP under small-data conditions. The results confirm that ensemble methods can improve predictive performance, particularly when combining models with complementary characteristics. For instance, the ensemble of ANN and RNN achieved good results, demonstrating the value of algorithmic complementarity in addressing data limitations.</p><p>However, the findings also reveal that integrating multiple algorithms does not necessarily lead to better outcomes. Although GPR is theoretically well-suited for small-data scenarios, its inclusion consistently reduced ensemble accuracy due to its relatively poor performance on the datasets used in this study. This highlights a key insight: the effectiveness of ensemble learning depends not only on the diversity of models but also on the predictive quality of each base model. Simply increasing the number of sub-models or adding more algorithms can lead to diminishing returns and may introduce issues such as overfitting and increased computational cost.</p><p>In addition to algorithm selection, this study highlights the importance of ensemble architecture design. The comparison between two frameworks with distinct structural logic shows that both achieved comparable predictive performance. The second-level ensemble, comprising two ensemble frameworks, further improved prediction performance. These findings suggest that the effectiveness of ensemble learning depends not only on the choice of algorithms but also on how the models are organized and combined.</p><p>In summary, ensemble learning remains a promising approach for reducing prediction errors and enhancing generalization, particularly in cases involving small datasets. Future work should focus on improving the accuracy of base models, refining model selection criteria, and developing adaptive ensemble strategies that leverage both algorithmic diversity and architectural flexibility.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, methodology, software, formal analysis, investigation, data curation, writing&#8212;original draft preparation, Q.S.; problem identification, project administration, concept initialization, writing&#8212;review and editing, discussion during research, supervision, K.-N.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author due to project confidentiality concerns.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-materials-18-04074"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>C.-P.</given-names></name></person-group><article-title>Challenges and prospects for advanced packaging</article-title><source>Fundam. Res.</source><year>2024</year><volume>4</volume><fpage>1455</fpage><lpage>1458</lpage><pub-id pub-id-type="doi">10.1016/j.fmre.2023.04.014</pub-id><pub-id pub-id-type="pmid">39734548</pub-id><pub-id pub-id-type="pmcid">PMC11670716</pub-id></element-citation></ref><ref id="B2-materials-18-04074"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ismail</surname><given-names>N.</given-names></name><name name-style="western"><surname>Yusoff</surname><given-names>W.Y.W.</given-names></name><name name-style="western"><surname>Amat</surname><given-names>A.</given-names></name><name name-style="western"><surname>Manaf</surname><given-names>N.A.A.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>N.</given-names></name></person-group><article-title>A review of extreme condition effects on solder joint reliability: Understanding failure mechanisms</article-title><source>Def. Technol.</source><year>2024</year><volume>41</volume><fpage>134</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1016/j.dt.2024.05.013</pub-id></element-citation></ref><ref id="B3-materials-18-04074"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bender</surname><given-names>E.</given-names></name><name name-style="western"><surname>Bernstein</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Boning</surname><given-names>D.S.</given-names></name></person-group><article-title>Modern trends in microelectronics packaging reliability testing</article-title><source>Micromachines</source><year>2024</year><volume>15</volume><elocation-id>398</elocation-id><pub-id pub-id-type="doi">10.3390/mi15030398</pub-id><pub-id pub-id-type="pmid">38542645</pub-id><pub-id pub-id-type="pmcid">PMC10972392</pub-id></element-citation></ref><ref id="B4-materials-18-04074"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.-M.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>C.-C.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>K.-N.</given-names></name></person-group><article-title>Enhancing the reliability of wafer level packaging by using solder joints layout design</article-title><source>IEEE Trans. Compon. Packag. Technol.</source><year>2006</year><volume>29</volume><fpage>877</fpage><lpage>885</lpage><pub-id pub-id-type="doi">10.1109/TCAPT.2006.886846</pub-id></element-citation></ref><ref id="B5-materials-18-04074"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>P.L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.H.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>K.N.</given-names></name></person-group><article-title>Empirical Solutions and Reliability Assessment of Thermal Induced Creep Failure for Wafer Level Packaging</article-title><source>IEEE Trans. Device Mater. Reliab.</source><year>2019</year><volume>19</volume><fpage>126</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1109/TDMR.2018.2887163</pub-id></element-citation></ref><ref id="B6-materials-18-04074"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>K.-N.</given-names></name></person-group><article-title>A small database with an adaptive data selection method for solder joint fatigue life prediction in advanced packaging</article-title><source>Materials</source><year>2024</year><volume>17</volume><elocation-id>4091</elocation-id><pub-id pub-id-type="doi">10.3390/ma17164091</pub-id><pub-id pub-id-type="pmid">39203269</pub-id><pub-id pub-id-type="pmcid">PMC11356193</pub-id></element-citation></ref><ref id="B7-materials-18-04074"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Panigrahy</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Tseng</surname><given-names>Y.-C.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>B.-R.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>K.-N.</given-names></name></person-group><article-title>An overview of AI-Assisted design-on-Simulation technology for reliability life prediction of advanced packaging</article-title><source>Materials</source><year>2021</year><volume>14</volume><elocation-id>5342</elocation-id><pub-id pub-id-type="doi">10.3390/ma14185342</pub-id><pub-id pub-id-type="pmid">34576571</pub-id><pub-id pub-id-type="pmcid">PMC8472661</pub-id></element-citation></ref><ref id="B8-materials-18-04074"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hamasha</surname><given-names>S.D.</given-names></name><name name-style="western"><surname>Alahmer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Belhadi</surname><given-names>M.E.A.</given-names></name></person-group><article-title>Assessing the SAC305 solder joint fatigue in ball grid array assembly using strain-controlled and stress-controlled approaches</article-title><source>J. Electron. Packag.</source><year>2023</year><volume>145</volume><fpage>031005</fpage><pub-id pub-id-type="doi">10.1115/1.4056559</pub-id></element-citation></ref><ref id="B9-materials-18-04074"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dubey</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Chaudhuri</surname><given-names>B.B.</given-names></name></person-group><article-title>Activation functions in deep learning: A comprehensive survey and benchmark</article-title><source>Neurocomputing</source><year>2022</year><volume>503</volume><fpage>92</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2022.06.111</pub-id></element-citation></ref><ref id="B10-materials-18-04074"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>J.C.</given-names></name></person-group><article-title>Artificial Neural Networks and Backpropagation</article-title><source>Geometry of Deep Learning</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2021</year><volume>Volume 37</volume><fpage>91</fpage><lpage>112</lpage></element-citation></ref><ref id="B11-materials-18-04074"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mannel</surname><given-names>F.</given-names></name><name name-style="western"><surname>Aggrawal</surname><given-names>H.O.</given-names></name><name name-style="western"><surname>Modersitzki</surname><given-names>J.</given-names></name></person-group><article-title>A structured L-BFGS method and its application to inverse problems</article-title><source>Inverse Probl.</source><year>2024</year><volume>40</volume><fpage>045022</fpage><pub-id pub-id-type="doi">10.1088/1361-6420/ad2c31</pub-id></element-citation></ref><ref id="B12-materials-18-04074"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zargar</surname><given-names>S.</given-names></name></person-group><source>Introduction to Sequence Learning Models: RNN, LSTM, GRU</source><publisher-name>Department of Mechanical and Aerospace Engineering, North Carolina State University</publisher-name><publisher-loc>Raleigh, NC, USA</publisher-loc><year>2021</year></element-citation></ref><ref id="B13-materials-18-04074"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.-Q.</given-names></name></person-group><article-title>Towards memory-and time-efficient backpropagation for training spiking neural networks</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>6166</fpage><lpage>6176</lpage></element-citation></ref><ref id="B14-materials-18-04074"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>An intuitive tutorial to Gaussian process regression</article-title><source>Comput. Sci. Eng.</source><year>2023</year><volume>25</volume><fpage>4</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2023.3342149</pub-id></element-citation></ref><ref id="B15-materials-18-04074"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lotfi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Izmailov</surname><given-names>P.</given-names></name><name name-style="western"><surname>Benton</surname><given-names>G.</given-names></name><name name-style="western"><surname>Goldblum</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wilson</surname><given-names>A.G.</given-names></name></person-group><article-title>Bayesian model selection, the marginal likelihood, and generalization</article-title><source>Proceedings of the 39th International Conference on Machine Learning</source><conf-loc>Baltimore, MD, USA</conf-loc><conf-date>17&#8211;23 July 2022</conf-date><fpage>14223</fpage><lpage>14247</lpage><comment>PMLR: 162:14223-14247</comment></element-citation></ref><ref id="B16-materials-18-04074"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Q.</given-names></name></person-group><article-title>A survey on ensemble learning</article-title><source>Front. Comput. Sci.</source><year>2020</year><volume>14</volume><fpage>241</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1007/s11704-019-8208-z</pub-id></element-citation></ref><ref id="B17-materials-18-04074"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hsieh</surname><given-names>M.-C.</given-names></name><name name-style="western"><surname>Tzeng</surname><given-names>S.-L.</given-names></name></person-group><article-title>Solder joint fatigue life prediction in large size and low cost wafer-level chip scale packages</article-title><source>Proceedings of the 2014 15th International Conference on Electronic Packaging Technology</source><conf-loc>Chengdu, China</conf-loc><conf-date>12&#8211;15 August 2014</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2014</year><fpage>496</fpage><lpage>501</lpage></element-citation></ref><ref id="B18-materials-18-04074"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hsieh</surname><given-names>M.-C.</given-names></name></person-group><article-title>Modeling correlation for solder joint fatigue life estimation in wafer-level chip scale packages</article-title><source>Proceedings of the 2015 10th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT)</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>21&#8211;23 October 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2015</year><fpage>65</fpage><lpage>68</lpage></element-citation></ref><ref id="B19-materials-18-04074"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Januddi</surname><given-names>M.A.F.M.S.</given-names></name><name name-style="western"><surname>Harun</surname><given-names>M.N.</given-names></name></person-group><article-title>A study of micro-scale solder bump geometric shapes using minimizing energy approach for different solder materials</article-title><source>Ain Shams Eng. J.</source><year>2022</year><volume>13</volume><fpage>101769</fpage><pub-id pub-id-type="doi">10.1016/j.asej.2022.101769</pub-id></element-citation></ref><ref id="B20-materials-18-04074"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Motalab</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Suhling</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Lall</surname><given-names>P.</given-names></name></person-group><article-title>Determination of Anand constants for SAC solders using stress-strain or creep data</article-title><source>Proceedings of the 13th InterSociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>30 May&#8211;1 June 2012</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2012</year><fpage>910</fpage><lpage>922</lpage></element-citation></ref><ref id="B21-materials-18-04074"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hai</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ban</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Du</surname><given-names>X.</given-names></name></person-group><article-title>A simplified prediction method on Chaboche isotropic/kinematic hardening model parameters of structural steels</article-title><source>J. Build. Eng.</source><year>2023</year><volume>68</volume><fpage>106151</fpage><pub-id pub-id-type="doi">10.1016/j.jobe.2023.106151</pub-id></element-citation></ref><ref id="B22-materials-18-04074"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dhal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Azad</surname><given-names>C.</given-names></name></person-group><article-title>A comprehensive survey on feature selection in the various fields of machine learning</article-title><source>Appl. Intell.</source><year>2022</year><volume>52</volume><fpage>4543</fpage><lpage>4581</lpage><pub-id pub-id-type="doi">10.1007/s10489-021-02550-9</pub-id></element-citation></ref><ref id="B23-materials-18-04074"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Varia</surname><given-names>B.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Q.</given-names></name></person-group><article-title>Design and optimization of thermo-mechanical reliability in wafer level packaging</article-title><source>Microelectron. Reliab.</source><year>2010</year><volume>50</volume><fpage>536</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.microrel.2009.11.010</pub-id></element-citation></ref><ref id="B24-materials-18-04074"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ladani</surname><given-names>L.J.</given-names></name></person-group><article-title>Numerical analysis of thermo-mechanical reliability of through silicon vias (TSVs) and solder interconnects in 3-dimensional integrated circuits</article-title><source>Microelectron. Eng.</source><year>2010</year><volume>87</volume><fpage>208</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.mee.2009.07.022</pub-id></element-citation></ref><ref id="B25-materials-18-04074"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Praful</surname><given-names>P.</given-names></name><name name-style="western"><surname>Bailey</surname><given-names>C.</given-names></name></person-group><article-title>Warpage in wafer-level packaging: A review of causes, modelling, and mitigation strategies</article-title><source>Front. Electron.</source><year>2025</year><volume>5</volume><elocation-id>1515860</elocation-id><pub-id pub-id-type="doi">10.3389/felec.2024.1515860</pub-id></element-citation></ref><ref id="B26-materials-18-04074"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fuhg</surname><given-names>J.N.</given-names></name><name name-style="western"><surname>Fau</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nackenhorst</surname><given-names>U.</given-names></name></person-group><article-title>State-of-the-art and comparative review of adaptive sampling methods for kriging</article-title><source>Arch. Comput. Methods Eng.</source><year>2021</year><volume>28</volume><fpage>2689</fpage><lpage>2747</lpage><pub-id pub-id-type="doi">10.1007/s11831-020-09474-6</pub-id></element-citation></ref><ref id="B27-materials-18-04074"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>K.</given-names></name></person-group><article-title>Utilizing Ensemble Learning on Small Database for Predicting the Reliability Life of Wafer-Level Packaging</article-title><source>Proceedings of the 2024 IEEE 26th Electronics Packaging Technology Conference (EPTC)</source><conf-loc>Singapore</conf-loc><conf-date>3&#8211;6 December 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2025</year><fpage>1248</fpage><lpage>1252</lpage></element-citation></ref><ref id="B28-materials-18-04074"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ogunsanya</surname><given-names>M.</given-names></name><name name-style="western"><surname>Isichei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Desai</surname><given-names>S.</given-names></name></person-group><article-title>Grid search hyperparameter tuning in additive manufacturing processes</article-title><source>Manuf. Lett.</source><year>2023</year><volume>35</volume><fpage>1031</fpage><lpage>1042</lpage><pub-id pub-id-type="doi">10.1016/j.mfglet.2023.08.056</pub-id></element-citation></ref><ref id="B29-materials-18-04074"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>V.</given-names></name></person-group><article-title>A study on data scaling methods for machine learning</article-title><source>Int. J. Glob. Acad. Sci. Res.</source><year>2022</year><volume>1</volume><fpage>31</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.55938/ijgasr.v1i1.4</pub-id></element-citation></ref><ref id="B30-materials-18-04074"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jayaram</surname><given-names>V.</given-names></name><name name-style="western"><surname>Gupte</surname><given-names>O.</given-names></name><name name-style="western"><surname>Smet</surname><given-names>V.</given-names></name></person-group><article-title>Modeling and design for system-level reliability and warpage mitigation of large 2.5 D glass BGA packages</article-title><source>Proceedings of the 2022 IEEE 72nd Electronic Components and Technology Conference (ECTC)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>31 May&#8211;3 June 2022</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2022</year><fpage>1060</fpage><lpage>1067</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="materials-18-04074-f001" orientation="portrait"><label>Figure 1</label><caption><p>The workflow of AI-assisted design of simulation with a small dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g001.jpg"/></fig><fig position="float" id="materials-18-04074-f002" orientation="portrait"><label>Figure 2</label><caption><p>Schematic of a single neuron.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g002.jpg"/></fig><fig position="float" id="materials-18-04074-f003" orientation="portrait"><label>Figure 3</label><caption><p>Structure of one hidden layer ANN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g003.jpg"/></fig><fig position="float" id="materials-18-04074-f004" orientation="portrait"><label>Figure 4</label><caption><p>Schematic of RNN structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g004.jpg"/></fig><fig position="float" id="materials-18-04074-f005" orientation="portrait"><label>Figure 5</label><caption><p>Schematic of an LSTM unit.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g005.jpg"/></fig><fig position="float" id="materials-18-04074-f006" orientation="portrait"><label>Figure 6</label><caption><p>Schematic of GPR.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g006.jpg"/></fig><fig position="float" id="materials-18-04074-f007" orientation="portrait"><label>Figure 7</label><caption><p>Schematic of Bagging.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g007.jpg"/></fig><fig position="float" id="materials-18-04074-f008" orientation="portrait"><label>Figure 8</label><caption><p>Schematic of boosting.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g008.jpg"/></fig><fig position="float" id="materials-18-04074-f009" orientation="portrait"><label>Figure 9</label><caption><p>Schematic of stacking.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g009.jpg"/></fig><fig position="float" id="materials-18-04074-f010" orientation="portrait"><label>Figure 10</label><caption><p>The top view of the WLCSP structure. The red line represents the half-diagonal, and the circles represent solder balls.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g010.jpg"/></fig><fig position="float" id="materials-18-04074-f011" orientation="portrait"><label>Figure 11</label><caption><p>Two-dimensional FEA model for WLCSP.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g011.jpg"/></fig><fig position="float" id="materials-18-04074-f012" orientation="portrait"><label>Figure 12</label><caption><p>Local view of the FEA model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g012.jpg"/></fig><fig position="float" id="materials-18-04074-f013" orientation="portrait"><label>Figure 13</label><caption><p>Mesh size in the critical region.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g013.jpg"/></fig><fig position="float" id="materials-18-04074-f014" orientation="portrait"><label>Figure 14</label><caption><p>Solder ball shape generated by Surface Evolver.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g014.jpg"/></fig><fig position="float" id="materials-18-04074-f015" orientation="portrait"><label>Figure 15</label><caption><p>Stress&#8211;strain curves for SAC305 at different temperatures.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g015.jpg"/></fig><fig position="float" id="materials-18-04074-f016" orientation="portrait"><label>Figure 16</label><caption><p>Temperature profile for thermal cycling.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g016.jpg"/></fig><fig position="float" id="materials-18-04074-f017" orientation="portrait"><label>Figure 17</label><caption><p>Geometric design factors of the WLP structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g017.jpg"/></fig><fig position="float" id="materials-18-04074-f018" orientation="portrait"><label>Figure 18</label><caption><p>Ensemble structure for a single algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g018.jpg"/></fig><fig position="float" id="materials-18-04074-f019" orientation="portrait"><label>Figure 19</label><caption><p>Threshold-based ensemble framework.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="materials-18-04074-g019.jpg"/></fig><table-wrap position="float" id="materials-18-04074-t001" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t001_Table 1</object-id><label>Table 1</label><caption><p>Linear elastic properties used in WLCSP modeling.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Material</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Young&#8217;s Modulus (GPa)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Poisson&#8217;s Ratio</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CTE (ppm/&#176;C)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Silicon chip</td><td align="center" valign="middle" rowspan="1" colspan="1">150</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28</td><td align="center" valign="middle" rowspan="1" colspan="1">2.62</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Low-k</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SBL</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">55</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cu</td><td align="center" valign="middle" rowspan="1" colspan="1">68.9</td><td align="center" valign="middle" rowspan="1" colspan="1">0.34</td><td align="center" valign="middle" rowspan="1" colspan="1">16.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solder ball</td><td align="center" valign="middle" rowspan="1" colspan="1">38.7&#8722;0.176T</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solder mask</td><td align="center" valign="middle" rowspan="1" colspan="1">6.87</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">19</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PCB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t002" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t002_Table 2</object-id><label>Table 2</label><caption><p>Coefficients in Chaboche model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">T(K)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> (GPa)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">&#947;</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">233</td><td align="center" valign="middle" rowspan="1" colspan="1">47.64</td><td align="center" valign="middle" rowspan="1" colspan="1">8894.8</td><td align="center" valign="middle" rowspan="1" colspan="1">639.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">253</td><td align="center" valign="middle" rowspan="1" colspan="1">38.87</td><td align="center" valign="middle" rowspan="1" colspan="1">8573.3</td><td align="center" valign="middle" rowspan="1" colspan="1">660.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">313</td><td align="center" valign="middle" rowspan="1" colspan="1">24.06</td><td align="center" valign="middle" rowspan="1" colspan="1">6011.4</td><td align="center" valign="middle" rowspan="1" colspan="1">625.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">353</td><td align="center" valign="middle" rowspan="1" colspan="1">18.12</td><td align="center" valign="middle" rowspan="1" colspan="1">5804.2</td><td align="center" valign="middle" rowspan="1" colspan="1">697.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">395</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4804.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">699.9</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t003" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of reliability results for five TVs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TV</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MTTF<break/>(Cycle)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Simulation<break/>(Cycle)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Difference<break/>(Cycle)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Difference<break/>(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">318</td><td align="center" valign="middle" rowspan="1" colspan="1">313</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">1.6%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">1013</td><td align="center" valign="middle" rowspan="1" colspan="1">982</td><td align="center" valign="middle" rowspan="1" colspan="1">31</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">587</td><td align="center" valign="middle" rowspan="1" colspan="1">587</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">876</td><td align="center" valign="middle" rowspan="1" colspan="1">804</td><td align="center" valign="middle" rowspan="1" colspan="1">72</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">904</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">885</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.1%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t004" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t004_Table 4</object-id><label>Table 4</label><caption><p>Ten structural design factors for WLP reliability.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Influence Factors</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Chip Thickness</td><td align="center" valign="middle" rowspan="1" colspan="1">Chip Size</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Upper Pad Thickness</td><td align="center" valign="middle" rowspan="1" colspan="1">Upper Pad Diameter</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lower Pad Thickness</td><td align="center" valign="middle" rowspan="1" colspan="1">Lower Pad Diameter</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SBL Thickness</td><td align="center" valign="middle" rowspan="1" colspan="1">PCB Thickness</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Solder Diameter</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pitch</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t005" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t005_Table 5</object-id><label>Table 5</label><caption><p>Value range of four features.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Features</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Feature Values</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Upper Pad Dia. (Unit: mm)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18~0.24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lower Pad Dia. (Unit: mm)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18~0.24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Chip Thickness (Unit: mm)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.15~0.45</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SBL Thickness (Unit: &#956;m)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5~32.5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t006" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t006_Table 6</object-id><label>Table 6</label><caption><p>Dataset configuration and model selection criteria.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Step 1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Step 2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Step 3</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training data</td><td align="center" valign="middle" rowspan="1" colspan="1">144</td><td align="center" valign="middle" rowspan="1" colspan="1">144 + 14</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Testing data</td><td align="center" valign="middle" rowspan="1" colspan="1">144</td><td align="center" valign="middle" rowspan="1" colspan="1">144 + 144</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;&#8212;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimal criteria</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Test difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Test difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CV score</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t007" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t007_Table 7</object-id><label>Table 7</label><caption><p>ANN hyperparameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Setting</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Activation function</td><td align="center" valign="middle" rowspan="1" colspan="1">ReLU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solver</td><td align="center" valign="middle" rowspan="1" colspan="1">L-BFGS/Adam</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learning rate </td><td align="center" valign="middle" rowspan="1" colspan="1">Adaptive</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Initial learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hidden layers</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Neuron number</td><td align="center" valign="middle" rowspan="1" colspan="1">Grid search</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max_iter</td><td align="center" valign="middle" rowspan="1" colspan="1">5000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Loss function</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t008" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t008_Table 8</object-id><label>Table 8</label><caption><p>RNN hyperparameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Setting</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Activation function</td><td align="center" valign="middle" rowspan="1" colspan="1">ReLU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Directions</td><td align="center" valign="middle" rowspan="1" colspan="1">Single</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Unit</td><td align="center" valign="middle" rowspan="1" colspan="1">LSTM/Simple RNN</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learning rate </td><td align="center" valign="middle" rowspan="1" colspan="1">Adaptive</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Initial learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hidden layers</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Neuron number</td><td align="center" valign="middle" rowspan="1" colspan="1">100/200</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Epochs</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Loss function</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MAPE</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t009" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t009_Table 9</object-id><label>Table 9</label><caption><p>GPR hyperparameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Setting</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Kernel function</td><td align="center" valign="middle" rowspan="1" colspan="1">Mat&#233;rn/RBF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alpha</td><td align="center" valign="middle" rowspan="1" colspan="1">Grid search (1 &#215; 10<sup>&#8722;10</sup>~1)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">fmin_l_bfgs_b</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N_restarts</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grid search (0~20)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t010" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t010_Table 10</object-id><label>Table 10</label><caption><p>Training results of five ANN-based models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ANN-1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ANN-2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ANN-3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ANN-4</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ANN-5</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training data</td><td align="center" valign="middle" rowspan="1" colspan="1">144</td><td align="center" valign="middle" rowspan="1" colspan="1">158</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Feature</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">Reliability (3)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solver</td><td align="center" valign="middle" rowspan="1" colspan="1">L-BFGS</td><td align="center" valign="middle" rowspan="1" colspan="1">L-BFGS</td><td align="center" valign="middle" rowspan="1" colspan="1">Adam</td><td align="center" valign="middle" rowspan="1" colspan="1">L-BFGS</td><td align="center" valign="middle" rowspan="1" colspan="1">L-BFGS</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neuron number</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88-112-96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">104-72-76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">112-112-52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88-64-60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56-96-44</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum training difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0<break/>(0)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0<break/>(0)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61<break/>660/721<break/>(9.2%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6<break/>1327/1333<break/>(0.5%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31<break/>903/872<break/>(3.4%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average training difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0<break/>(0)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0<break/>(0)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.3<break/>(0.8%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7<break/>(0.1%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.8<break/>(0.5%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum testing difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71<break/>1396/1467<break/>(5.1%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66<break/>1311/1377<break/>(5.0%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average testing difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.7<break/>(1.0%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.0<break/>(0.9%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t011" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t011_Table 11</object-id><label>Table 11</label><caption><p>Training results of five RNN-based models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RNN-1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RNN-2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RNN-3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RNN-4</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RNN-5</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training data</td><td align="center" valign="middle" rowspan="1" colspan="1">144</td><td align="center" valign="middle" rowspan="1" colspan="1">158</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Feature</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">Reliability (3)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Unit</td><td align="center" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">Simple RNN</td><td align="center" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">LSTM</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neuron number</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum training difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53<break/>1319/1266<break/>(4.0%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29<break/>1145/1174<break/>(2.5%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47<break/>1690/1643<break/>(2.8%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16<break/>1470/1486<break/>(1.1%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24<break/>1230/1254<break/>(2.0%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average training difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.1<break/>(0.6%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.7<break/>(0.6%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.2<break/>(0.6%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9<break/>(0.4%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.0<break/>(0.5%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum testing difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85<break/>828/743<break/>(10.3%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64<break/>927/991<break/>(6.9%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average testing difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.7<break/>(1.7%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.3<break/>(1.4%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t012" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t012_Table 12</object-id><label>Table 12</label><caption><p>Training results of five GPR-based models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GPR-1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GPR-2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GPR-3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GPR-4</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GPR-5</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Training data</td><td align="center" valign="middle" rowspan="1" colspan="1">144</td><td align="center" valign="middle" rowspan="1" colspan="1">158</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td><td align="center" valign="middle" rowspan="1" colspan="1">432</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Feature</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">ULCS</td><td align="center" valign="middle" rowspan="1" colspan="1">Reliability (3)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Kernel</td><td align="center" valign="middle" rowspan="1" colspan="1">Mat&#233;rn</td><td align="center" valign="middle" rowspan="1" colspan="1">Mat&#233;rn</td><td align="center" valign="middle" rowspan="1" colspan="1">RBF</td><td align="center" valign="middle" rowspan="1" colspan="1">Mat&#233;rn</td><td align="center" valign="middle" rowspan="1" colspan="1">Mat&#233;rn</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Alpha</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N_restarts</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum training difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23<break/>1690/1667<break/>(1.4%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20<break/>1319/1299<break/>(1.5%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50<break/>1690/1640<break/>(3.0%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21<break/>1225/1204<break/>(1.7%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53<break/>1321/1268<break/>(4.0%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average training difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2<break/>(0.4%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2<break/>(0.4%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.7<break/>(0.8%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.0<break/>(0.3%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.4<break/>(0.7%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum testing difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63<break/>609/672<break/>(10.3%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67<break/>1321/1254<break/>(5.1%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average testing difference</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.0<break/>(1.4%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.7<break/>(1.0%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8212;&#8212;</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t013" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t013_Table 13</object-id><label>Table 13</label><caption><p>The performance of the base models on 9601 datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Maximum Testing Difference</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Testing Difference</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN-1</td><td align="center" valign="middle" rowspan="1" colspan="1">102 (1141/1039/8.9%)</td><td align="center" valign="middle" rowspan="1" colspan="1">13.9 (1.4%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN-2</td><td align="center" valign="middle" rowspan="1" colspan="1">85 (1595/1680/5.3%)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.6 (1.1%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN-3</td><td align="center" valign="middle" rowspan="1" colspan="1">104 (1595/1699/6.5%)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.0 (0.9%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN-4</td><td align="center" valign="middle" rowspan="1" colspan="1">86 (689/775/12.5%)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.9 (1.2%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN-5</td><td align="center" valign="middle" rowspan="1" colspan="1">78 (1595/1673/4.9%)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.0 (0.9%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RNN-1</td><td align="center" valign="middle" rowspan="1" colspan="1">118 (1237/1355/9.5%)</td><td align="center" valign="middle" rowspan="1" colspan="1">13.8 (1.4%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RNN-2</td><td align="center" valign="middle" rowspan="1" colspan="1">140 (672/812/20.8%)</td><td align="center" valign="middle" rowspan="1" colspan="1">14.6 (1.5%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RNN-3</td><td align="center" valign="middle" rowspan="1" colspan="1">94 (864/770/10.9%)</td><td align="center" valign="middle" rowspan="1" colspan="1">10.7 (1.0%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RNN-4</td><td align="center" valign="middle" rowspan="1" colspan="1">114 (921/807/12.4%)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.3 (0.8%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RNN-5</td><td align="center" valign="middle" rowspan="1" colspan="1">114 (664/778/17.2%)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.8 (1.0%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPR-1</td><td align="center" valign="middle" rowspan="1" colspan="1">124 (1340/1216/9.3%)</td><td align="center" valign="middle" rowspan="1" colspan="1">15.2 (1.5%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPR-2</td><td align="center" valign="middle" rowspan="1" colspan="1">86 (1432/1346/6.0%)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.8 (1.3%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPR-3</td><td align="center" valign="middle" rowspan="1" colspan="1">83 (1340/1257/6.2%)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.7 (1.2%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPR-4</td><td align="center" valign="middle" rowspan="1" colspan="1">80 (1340/1260/6.0%)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.8 (0.9%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPR-5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82 (1585/1503/5.2%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.6 (1.1%)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t014" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t014_Table 14</object-id><label>Table 14</label><caption><p>The performance of ensemble models on 9601 datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ensemble</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Maximum Testing Difference</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Testing Difference</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN</td><td align="center" valign="middle" rowspan="1" colspan="1">66 (1595/1661/4.1%)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.1 (0.8%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RNN</td><td align="center" valign="middle" rowspan="1" colspan="1">82 (921/839/8.9%)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.0 (0.8%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPR</td><td align="center" valign="middle" rowspan="1" colspan="1">69 (1097/1028/6.3%)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.3 (1.0%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN + RNN</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>63</bold> (921/858/6.8%)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>7.0 (0.7%)</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ANN + GPR</td><td align="center" valign="middle" rowspan="1" colspan="1">70 (1097/1027/6.4%)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.7 (0.9%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RNN + GPR</td><td align="center" valign="middle" rowspan="1" colspan="1">77 (1340/1263/5.7%)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.8 (0.9%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ALL</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67 (1142/1075/5.9%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.9 (0.8%)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t015" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t015_Table 15</object-id><label>Table 15</label><caption><p>Performance comparison of two ensemble frameworks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ensemble<break/>Framework</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Maximum Testing Difference</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Testing Difference</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1 (ANN)</td><td align="center" valign="middle" rowspan="1" colspan="1">66 (1595/1661/4.1%)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.1 (0.8%)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2 (ANN)</td><td align="center" valign="middle" rowspan="1" colspan="1">62 (1595/1657/3.9%)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.5 (0.8%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 + 2 (ANN)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64 (1595/1659/4.0%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>6.5</bold> (0.6%)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="materials-18-04074-t016" orientation="portrait"><object-id pub-id-type="pii">materials-18-04074-t016_Table 16</object-id><label>Table 16</label><caption><p>Performance comparison of the simulation and AI models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Prediction<break/>(Cycles)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Difference from Experiment<break/>(Cycles)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Experiment</td><td align="center" valign="middle" rowspan="1" colspan="1">1013</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Simulation</td><td align="center" valign="middle" rowspan="1" colspan="1">982</td><td align="center" valign="middle" rowspan="1" colspan="1">31 (3.1%)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Machine Learning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1029</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 (1.6%)</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>