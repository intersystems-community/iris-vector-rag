<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431288</article-id><article-id pub-id-type="pmcid-ver">PMC12431288.1</article-id><article-id pub-id-type="pmcaid">12431288</article-id><article-id pub-id-type="pmcaiid">12431288</article-id><article-id pub-id-type="doi">10.3390/s25175389</article-id><article-id pub-id-type="publisher-id">sensors-25-05389</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MCH-YOLOv12: Research on Surface Defect Detection Algorithm for Aluminum Profiles Based on Improved YOLOv12</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Sun</surname><given-names initials="Y">Yuyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-05389" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-1414-3858</contrib-id><name name-style="western"><surname>Yan</surname><given-names initials="H">Heqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shang</surname><given-names initials="Z">Zongkai</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yang</surname><given-names initials="M">Mingxiao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Gan</surname><given-names initials="TH">Tat-Hean</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05389">College of Computer Science and Technology, Changchun University, Changchun 130022, China; <email>231502546@mails.ccu.edu.cn</email> (H.Y.); <email>13386378939@163.com</email> (Z.S.); <email>19806085316@163.com</email> (M.Y.)</aff><author-notes><corresp id="c1-sensors-25-05389"><label>*</label>Correspondence: <email>sunyy@ccu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>01</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5389</elocation-id><history><date date-type="received"><day>21</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>29</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>29</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>01</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 12:25:22.357"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05389.pdf"/><abstract><p>Surface defect detection in aluminum profiles is critical for maintaining product quality and ensuring efficient industrial production. However, existing detection algorithms often struggle to address the challenges of imbalanced defect categories, low detection accuracy for small-scale defects, and irregular flaw geometries. These limitations compromise both detection accuracy and algorithmic robustness. Accordingly, we proposed MCH-YOLOv12&#8212;an improved YOLOv12-based algorithm for precise defect detection. Firstly, we enhanced the original Ghost convolution by incorporating multi-scale feature extraction and named the improved version MultiScaleGhost, which replaced the standard convolutions in the Backbone of YOLOv12. This improvement mitigated the limitations of single-scale convolution, enhancing feature representation and the detection of irregularly shaped defects. Secondly, we addressed the directional and edge-specific nature of defects by enhancing the traditional Channel-wise Gated Linear Unit (CGLU). We proposed the Spatial-Channel Collaborative Gated Linear Unit (SCCGLU), which was embedded after the C3k2 module in the Neck of YOLOv12 to better capture fine-grained features. Finally, we designed a Hybrid Head combining anchor-based and anchor-free detection to improve adaptability to defects of various sizes and shapes. Experimental results on an aluminum profile defect dataset demonstrated improved accuracy, reduced category imbalance, and lower parameters and Floating Point Operations (FLOPs), making the algorithm suitable for real-time industrial inspection.</p></abstract><kwd-group><kwd>YOLOv12</kwd><kwd>deep learning</kwd><kwd>aluminum profiles</kwd><kwd>surface defect detection</kwd></kwd-group><funding-group><award-group><funding-source>Technology Development Project of Jilin Provincial Department of Science and Technology</funding-source><award-id>2024220008000851</award-id></award-group><funding-statement>This study was funded by the Technology Development Project of Jilin Provincial Department of Science and Technology (No. 2024220008000851).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05389"><title>1. Introduction</title><p>Aluminum profiles, as an important basic material in the industrial field, have surface defects that directly affect product quality and safety. However, the surface defects of aluminum profiles are characterized by a multi-scale distribution [<xref rid="B1-sensors-25-05389" ref-type="bibr">1</xref>], irregular morphology, and strong background interference under complex lighting, which makes it difficult for traditional manual feature-based detection methods to effectively capture the defect features. Furthermore, existing deep learning models still lack proficiency in multi-scale feature fusion, complex background suppression, and adaptability of the anchor frame mechanism to irregular defects [<xref rid="B2-sensors-25-05389" ref-type="bibr">2</xref>]. How to improve the algorithm&#8217;s multi-scale perception ability and detection robustness for aluminum profile surface defects is a key challenge in the field of industrial vision inspection.</p><p>Target detection is one of the core tasks in the field of computer vision [<xref rid="B3-sensors-25-05389" ref-type="bibr">3</xref>], aiming to recognize the class of a target in an image or video and locate its position. The technology is widely used in security monitoring, automatic driving, industrial quality inspection, medical imaging, and other fields. With the development of deep learning, target detection algorithms [<xref rid="B4-sensors-25-05389" ref-type="bibr">4</xref>] have undergone a revolutionary change from traditional methods to those that are deep neural network-based, the core of which can be divided into two categories: traditional target detection and deep learning-based target detection. The traditional target detection algorithm is the core method in the early computer vision field [<xref rid="B5-sensors-25-05389" ref-type="bibr">5</xref>], which mainly describes the texture, shape, and other information of the target by manually designing the features; combines with the sliding window to generate the candidate region; and then utilizes the classifiers, such as Support Vector Machine, Adaboost, etc., to complete discrimination between the target and the background. The process is divided into three stages: candidate region generation, feature extraction, and classification decision. Representative algorithms include the Viola&#8211;Jones and Deformable Part-based Model. These algorithms rely on domain knowledge to design features, are computationally lightweight but weak in generalization, and are difficult to cope with multi-scale targets and background interference in complex scenarios. They have gradually been replaced by end-to-end detection algorithms based on deep learning. The deep learning-based target detection algorithm [<xref rid="B6-sensors-25-05389" ref-type="bibr">6</xref>] automatically learns image features through a neural network without manually designing features, which significantly improves detection accuracy and efficiency. It is mainly divided into two categories: two-stage algorithms first generate candidate frames through the area suggestion network [<xref rid="B7-sensors-25-05389" ref-type="bibr">7</xref>] and then classify and regress them, which is highly accurate but slow; single-stage algorithms predict the target location and category directly on the feature map, which is fast and balanced in terms of accuracy.</p><p>In recent years, more and more research work has emerged in the field of surface defect detection of objects. Wang et al. [<xref rid="B8-sensors-25-05389" ref-type="bibr">8</xref>] introduced a lightweight de-weighted Bidirectional Feature Pyramid Network (BiFPN) feature fusion structure and an Efficient Channel Attention (ECA) mechanism based on improvements to YOLOv7, and they replaced the bounding box loss function with Scylla-IoU (SIoU) Loss. This approach significantly improved detection accuracy and speed, and the method exhibited superior performance on both the GC10-DET and NEU-DET datasets. However, its robustness under conditions involving very small defects and complex backgrounds still needs improvement. Huang et al. [<xref rid="B9-sensors-25-05389" ref-type="bibr">9</xref>] improved the generalization ability of the model for complex defects and rare classes by introducing GAN-based data augmentation and a dynamic anchor frame assignment mechanism into the YOLOv11 framework. This approach achieved high detection accuracy and recall in the PCB defect detection task; however, some misclassification phenomena remained in samples with blurred defect boundaries and small inter-class differences. Chen et al. [<xref rid="B10-sensors-25-05389" ref-type="bibr">10</xref>] embedded a Gabor filter layer in Faster R-CNN to suppress fabric background texture interference and used a genetic algorithm to optimize the filtering parameters. This strategy significantly enhanced the model&#8217;s ability to locate defects in high-texture interference scenes and achieved a high Mean Average Precision (mAP) in the fabric defect detection task. However, the computational cost was high, which made it difficult to meet the demands of real-time detection. Chen et al. [<xref rid="B11-sensors-25-05389" ref-type="bibr">11</xref>] proposed the YOLOv3-dense model based on improvements to YOLOv3. They replaced the backbone network with DenseNet-121 to enhance the detection capability for targets with small defects and complex backgrounds and combined pseudo-defect generation with the Taguchi method for hyperparameter optimization. This approach effectively improved the model&#8217;s detection accuracy and generalization capability. The method achieved excellent detection performance across various SMD LED defect categories. However, the complex structure of DenseNet and the high overall computational overhead limited the model&#8217;s deployment efficiency in real-time detection tasks. Wang et al. [<xref rid="B12-sensors-25-05389" ref-type="bibr">12</xref>] proposed the BL-YOLOv8 model based on improvements to YOLOv8. They enhanced the feature fusion capability and target perception ability by introducing BiFPN and Large Selective Kernel (LSK)-attention. Meanwhile, they replaced the original Spatial Pyramid Pooling-Fast (SPPF) module with Simple Spatial Pyramid Pooling-Fast (SimSPPF) to reduce computational complexity. Experiments on the RDD2022 road defect dataset verified the improvements in the model in terms of accuracy and efficiency. However, issues of missed detections and false detections in cases involving partially blurred cracks and complex backgrounds remained, and the real-time performance of the model slightly decreased. Xu et al. [<xref rid="B13-sensors-25-05389" ref-type="bibr">13</xref>] proposed an improved model for tunnel surface defect detection based on enhancements to Mask R-CNN. They enhanced the information propagation capability of low-level features by introducing the Path Aggregation Feature Pyramid Network (PAFPN) and added an edge detection branch to improve the recognition accuracy of defect edges, which significantly boosted the accuracy of both detection and segmentation. This method achieved excellent performance on images of metro tunnel leakage and spalling defects (where leakage refers to water seepage through tunnel walls, and spalling denotes the flaking or breaking off of concrete surfaces), with a 9.18% increase in mAP and an error rate reduced to 0.51%. However, due to the introduction of multiple sub-modules in the model, the overall computational load increased, making it difficult to meet the real-time requirements of low-computing-power devices.</p><p>To address the above problems, we proposed an algorithm, MCH-YOLOv12, based on YOLOv12 for detecting surface defects in aluminum profiles. It should be noted that &#8220;MCH&#8221; stands for &#8220;MultiScaleGhost convolution, Channel-wise Gated Linear Unit and Hybrid Head&#8221;, which reflects the three core improvements introduced in the proposed model. While preserving the model&#8217;s lightweight characteristics, this approach effectively mitigates the issue of defect category imbalance and enhances both the accuracy and robustness in detecting small-scale and irregularly shaped defects. The primary contributions of this research are outlined as follows:<list list-type="simple"><list-item><label>(1)</label><p>We designed MultiScaleGhost convolution, which enhances the ability to extract multi-scale defect features. This not only improves the detection performance for small targets but also reduces the number of parameters and computational load.</p></list-item><list-item><label>(2)</label><p>We proposed the SCCGLU. By combining direction-aware channel modeling with edge-guided spatial enhancement, it effectively improved the perception capability for irregular defects. Additionally, we integrated it after the C3K2 module of YOLOv12 using a post-enhancement fusion strategy.</p></list-item><list-item><label>(3)</label><p>We constructed the Hybrid Head detection head, which integrates the advantages of both anchor-based and anchor-free approaches. This integration thereby enhances the detection accuracy and robustness for irregular defects and defects with class imbalance.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05389"><title>2. YOLOv12 Detection Algorithm</title><p>YOLOv12 [<xref rid="B14-sensors-25-05389" ref-type="bibr">14</xref>], proposed by Yunjie Tian et al., is a real-time object detection algorithm with the attention mechanism at its core. The network architecture of YOLOv12 inherits the classic framework of the YOLO series, comprising three key components: Backbone, Neck, and Head. Specifically, in YOLOv12, the Backbone is tasked with extracting features from the input image [<xref rid="B15-sensors-25-05389" ref-type="bibr">15</xref>], thereby providing foundational information for subsequent object detection processes. The Neck serves to fuse and refine the features extracted by the Backbone to enhance feature representability. Finally, the Head performs the ultimate object detection task based on the processed features and outputs both the category and location information of the detected objects.</p><p>The Backbone comprises Conv, C3k2, and A2C2f modules. Among these, C3k2 is a module introduced in YOLO11 to assist in feature extraction. The A2C2f, a structure first proposed in YOLO12, is designed for further processing and refinement of features. Its primary function is to extract features from the input image [<xref rid="B16-sensors-25-05389" ref-type="bibr">16</xref>], which are then utilized by subsequent network layers for prediction. Notably, its architecture exerts a significant influence on the quality of feature maps generated in the object detection model [<xref rid="B17-sensors-25-05389" ref-type="bibr">17</xref>].</p><p>The Neck incorporates Concat, Upsample, A2C2f, and C3k2 modules. It fuses and refines the features extracted by the Backbone, integrating multi-level feature information through upsampling and concatenation operations. This process enhances feature expressiveness, renders multi-scale feature fusion more targeted, and provides richer feature information for subsequent detection tasks.</p><p>The Head of YOLOv12 shares an identical structure with that of YOLO11 [<xref rid="B18-sensors-25-05389" ref-type="bibr">18</xref>], comprising multiple Detect modules. It undertakes the final object detection task and outputs both the category and location information of detected objects based on the features preprocessed by the Backbone and Neck. The network structure of YOLOv12 is shown in <xref rid="sensors-25-05389-f001" ref-type="fig">Figure 1</xref>.</p></sec><sec sec-type="methods" id="sec3-sensors-25-05389"><title>3. Methods</title><p>YOLOv12 is a state-of-the-art object detection model [<xref rid="B19-sensors-25-05389" ref-type="bibr">19</xref>] featuring excellent feature extraction capabilities and detection performance. The authors employed a multi-scale detection architecture to meet the detection requirements of objects with varying sizes. However, in the task of aluminum profile surface defect detection [<xref rid="B20-sensors-25-05389" ref-type="bibr">20</xref>], defects often exhibited characteristics such as small target sizes, irregular shapes, and subtle texture variations. Additionally, the images contained significant industrial noise and complex textured backgrounds, which imposed certain limitations on the detection performance of YOLOv12&#8217;s original structure in such scenarios. To address these issues, we adopted YOLOv12 as the base model and optimized its structure from three aspects&#8212;convolutional modules, attention mechanisms, and detection heads&#8212;tailored to the characteristics of aluminum profile surface defect detection [<xref rid="B21-sensors-25-05389" ref-type="bibr">21</xref>].</p><sec id="sec3dot1-sensors-25-05389"><title>3.1. MultiScaleGhost: Lightweight GhostConv Module with Multi-Scale Feature Enhancement</title><p>Traditional convolution generates feature maps through extensive computations, but studies have shown that many feature maps contain redundancy. GhostConv [<xref rid="B22-sensors-25-05389" ref-type="bibr">22</xref>] is a lightweight convolutional architecture that splits feature generation into two parts: one part obtains high-quality features via standard convolution, while the other generates &#8220;Ghost features&#8221; through inexpensive operations. The structure diagram of GhostConv is shown in <xref rid="sensors-25-05389-f002" ref-type="fig">Figure 2</xref>. However, the original GhostConv employed a fixed linear transformation when generating redundant feature maps, which led to insufficient feature representation in the presence of complex background interference or minor defects, thereby affecting the final detection performance. To address this, we proposed an improved GhostConv module, named MultiScaleGhost.</p><p>In <xref rid="sensors-25-05389-f002" ref-type="fig">Figure 2</xref>, the term &#8220;cheap operation&#8221; denotes lightweight computational operations designed to balance feature extraction effectiveness and computational efficiency. Specifically, within this architecture, it is instantiated as a sequence of DWConv with a 3 &#215; 3 kernel, followed by BN and the ReLU activation function. </p><p>MultiScaleGhost is a multi-scale enhanced version of GhostConv, with its core objective to capture features of targets of different sizes within a lightweight framework. Traditional GhostConv generated derived features via single-scale DWConv, whereas MultiScaleGhost employed multiple convolutional kernels of varying sizes in parallel. This enabled the model to simultaneously focus on local details and global contextual information.</p><p>Specifically, let the input feature map be <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Firstly, the main convolutional module reduced the dimensionality of the input feature map <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> via a 1 &#215; 1 convolutional layer coupled with BN and the ReLU activation function, generating the preliminary feature representation <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow><mml:mi>s</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>. The specific calculation was <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Re</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8901;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, the multi-scale derivative module employed a parallel branch structure, where each branch utilized DWConv of varying sizes combined with BN and ReLU; the convolution kernel size of each branch was set to <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> (this formula generates a sequence of odd-sized kernels (3 &#215; 3, 5 &#215; 5, 7 &#215; 7, &#8230;) to ensure symmetric receptive fields. Starting with <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (the smallest odd kernel for fine-grained features), each subsequent kernel increases by 2 (i.e., <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Substituting <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the recursive relation simplifies to <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, enabling the achievement of local receptive fields of 3 &#215; 3, 5 &#215; 5, and 7 &#215; 7 across different scales to extract more comprehensive multi-scale feature representations. Each branch generates derived features in the form of <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Re</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, the feature fusion module concatenated the primary feature <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> with the derived features <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> generated by each branch along the channel dimension to form the final output feature map <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e., <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>). Here, n denotes the number of branches, and the number of channels could be adjusted via an optional 1 &#215; 1 convolution to achieve effective fusion of multi-scale features. The structural diagram of MultiScaleGhost is shown in <xref rid="sensors-25-05389-f003" ref-type="fig">Figure 3</xref>. For ease of reading, we have provided the terminology symbols involved in this section in <xref rid="sensors-25-05389-t001" ref-type="table">Table 1</xref>.</p><p>Compared with the original GhostConv, MultiScaleGhost introduced multi-scale convolution branches while maintaining its lightweight characteristics. Instead of relying solely on simple linear transformations to generate redundant feature maps, it enhanced the ability to perceive defects of different sizes through parallel multi-scale receptive fields. We embedded this module into the Backbone of YOLOv12 to replace the original traditional convolution module. This not only enhances the network&#8217;s capability to extract features of surface defects on aluminum profiles but also further improves the model&#8217;s detection accuracy and robustness for small-target defects.</p></sec><sec id="sec3dot2-sensors-25-05389"><title>3.2. Spatial-Channel Collaborative Gated Linear Unit</title><p>In object detection networks, an efficient feature representation structure is of great significance for accurately capturing fine-grained information such as small objects and irregularly shaped defects. To enhance the capability of information interaction among feature map channels and suppress redundant features, the Gated Mechanism is widely introduced into deep models. Among these, the Channel-wise Gated Linear Unit is a lightweight and efficient channel-level gating module. It effectively enhances the network&#8217;s responsiveness to key channels by segmenting and selectively activating input features along the channel dimension.</p><sec id="sec3dot2dot1-sensors-25-05389"><title>3.2.1. Channel-Wise Gated Linear Unit</title><p>The Channel-wise Gated Linear Unit structure was initially extended from the Gated Linear Unit (GLU) [<xref rid="B26-sensors-25-05389" ref-type="bibr">26</xref>]. Its core idea is to apply an independent gating mechanism to each channel of the input features, selectively activating or suppressing specific channel information through inter-channel interactions. Unlike the standard GLU, it focused more on feature selection in the channel dimension and was commonly used to enhance the model&#8217;s ability to perceive channel semantics. The calculation formula is presented in Equation (1), and the flowchart is illustrated in <xref rid="sensors-25-05389-f004" ref-type="fig">Figure 4</xref>.<disp-formula id="FD1-sensors-25-05389"><label>(1)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>G</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced><mml:mo>&#8857;</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> denotes the input feature map; <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent 1 &#215; 1 convolution operations, which generate the value branch and the gating branch, respectively; <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mo>&#8857;</mml:mo></mml:mrow></mml:math></inline-formula> indicates element-wise multiplication, enabling channel-wise selective activation; and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> stands for the Sigmoid activation function.</p><p>The original Channel-wise Gated Linear Unit, while capable of assigning different retention weights to each channel via the gating mechanism, thereby enabling the model to focus more on semantically significant channels, had certain structural limitations. The gated features were typically generated solely by simple linear transformations, which made it difficult to model local contextual information. Meanwhile, the independent processing after channel splitting lacked cross-channel information fusion and global dependency modeling. Moreover, this mechanism operated only on the channel dimension, neglecting the importance of spatial structure and local details, which limited its performance in complex visual tasks such as object detection.</p></sec><sec id="sec3dot2dot2-sensors-25-05389"><title>3.2.2. Improved Channel-Wise Gated Linear Unit</title><p>To address the aforementioned issues, we proposed the Spatial-Channel Collaborative Gated Linear Unit, which enhanced the targeted capture of defect features through dual-channel collaborative modeling, as illustrated in <xref rid="sensors-25-05389-f005" ref-type="fig">Figure 5</xref>. The core idea of the Spatial-Channel Collaborative Gated Linear Unit was to generate joint gating signals via parallel channel attention branches and spatial-edge enhancement branches, achieving collaborative optimization of channel importance screening and spatial location enhancement. The specific process was as follows: we fed the input feature map <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> into two branches simultaneously. The channel attention branch focused on global channel semantic correlations to generate channel-level weights, while the spatial-edge enhancement branch strengthened local spatial features and edge structures to generate spatial-level weights. We fused the outputs of the two branches to obtain a joint gating signal, which was multiplied element-wise with the input features to output the enhanced feature Y.</p><p>The channel attention branch adopted the Direction-Aware Channel Attention mechanism to capture the global dependencies among channels while considering the directional characteristics of aluminum profile defects. First, we employed multi-directional edge feature extraction, designing four sets of fixed-direction convolution kernels for the horizontal, vertical, 45&#176;, and 135&#176; directions, respectively. We used these kernels to perform convolution operations on the input features, extracting edge responses in different directions, as shown in Equation (2).<disp-formula id="FD2-sensors-25-05389"><label>(2)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8727;</mml:mo><mml:mi>X</mml:mi><mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the edge response feature map in the <italic toggle="yes">i</italic>-th direction, which extracts edge information related to that direction from the input features via a convolution kernel of a specific direction. <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the edge detection convolution kernel in the <italic toggle="yes">i</italic>-th direction. These convolution kernels have fixed parameters and do not participate in training; they directly extract edge information in specific directions through predefined gradient operators. <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mo>&#8727;</mml:mo></mml:mrow></mml:math></inline-formula> denotes the two-dimensional convolution operation, and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> represents the input feature map.</p><p>Then, we concatenated the response features from the four directions [D0; D1; D2; D3] and performed dimensionality reduction and mapping via two layers of 1 &#215; 1 convolutions. We generated the channel weights C using the Sigmoid activation function, and the calculation formula is presented in Equation (3).<disp-formula id="FD3-sensors-25-05389"><label>(3)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8855;</mml:mo><mml:mi>Re</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8855;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> denotes the output channel attention weight; <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> represents the Sigmoid activation function; <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> signifies the weight parameters of the second 1 &#215; 1 convolutional layer, which maps the intermediate features back to the original number of channels; <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mo>&#8855;</mml:mo></mml:mrow></mml:math></inline-formula> denotes the 1 &#215; 1 convolution operation; <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the weight parameters of the first 1 &#215; 1 convolutional layer, which reduces the dimensionality of the concatenated multi-directional edge features; and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the concatenation of response features from edge detection in four directions along the channel dimension.</p><p>To highlight the spatial location information and edge structure of defects, the spatial branch introduced a fusion mechanism of edge detection and spatial attention, and we designed a spatial-edge enhancement branch. First, we extracted the edge response of the input features using the Laplacian Kernel Wedge and obtained the Edge Probability Map E through Sigmoid activation. Its calculation formula is presented in Equation (4).<disp-formula id="FD4-sensors-25-05389"><label>(4)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">edge</mml:mi></mml:mrow></mml:msub><mml:mo>&#8727;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>8</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the equation, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> denotes the probability that each spatial position in the input feature <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> belongs to an edge; <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> represents the Sigmoid activation function; <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> signifies the Laplacian operator convolution kernel, which extracts edge information from the input features; <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mo>&#8727;</mml:mo></mml:mrow></mml:math></inline-formula> indicates two-dimensional convolution; and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> stands for the input features.</p><p>Subsequently, we employed a 3 &#215; 3 convolution to perform local spatial modeling on the input features, generating the base spatial weight S. We then fused this weight with the edge probability map to obtain the enhanced spatial attention map S&#8217;, and its calculation formula is presented in Equation (5).<disp-formula id="FD5-sensors-25-05389"><label>(5)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>&#8857;</mml:mo><mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mo>&#8857;</mml:mo></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication, and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the response weight used to amplify edge regions.</p><p>We expanded the channel weight C to the same dimension as the input features, concatenated it with the spatial enhancement weight S&#8217;, and then compressed the concatenated result through a 1 &#215; 1 convolution to generate the final gating signal G, thereby achieving adaptive feature selection. Its calculation formula is presented in Equation (6).<disp-formula id="FD6-sensors-25-05389"><label>(6)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#8855;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mi>C</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the Sigmoid activation function; <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the weight parameters of the 1 &#215; 1 convolutional layer, which maps the concatenated features back to the original number of channels C; <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mo>&#8855;</mml:mo></mml:mrow></mml:math></inline-formula> denotes the 1 &#215; 1 convolution operation; and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mi>C</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> signifies the concatenation of the channel attention weight C and the spatial-edge enhancement weight <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> along the channel dimension.</p><p>Ultimately, the module output was the element-wise product of the input features and the gating signal, as presented in Equation (7).<disp-formula id="FD7-sensors-25-05389"><label>(7)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo>&#8857;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> denotes the output feature after gated enhancement; <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> represents the input feature of the Spatial-Channel Collaborative Gated Linear Unit; <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mo>&#8857;</mml:mo></mml:mrow></mml:math></inline-formula> indicates element-wise multiplication; and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> signifies the joint gating signal.</p><p>To justify the employment of the proposed SCCGLU attention module, we performed structural and functional comparisons with several representative attention mechanisms, including the Squeeze-and-Excitation Network (SE-Net) [<xref rid="B27-sensors-25-05389" ref-type="bibr">27</xref>], Convolutional Block Attention Module (CBAM) [<xref rid="B28-sensors-25-05389" ref-type="bibr">28</xref>], Self-Attention (SA) [<xref rid="B29-sensors-25-05389" ref-type="bibr">29</xref>], and Global Attention Module (GAM) [<xref rid="B30-sensors-25-05389" ref-type="bibr">30</xref>]. <xref rid="sensors-25-05389-t002" ref-type="table">Table 2</xref> summarizes their key differences in terms of attention type, computational complexity, feature enhancement strategies, and application scope.</p><p>Compared to existing modules, SE-Net focuses solely on global channel recalibration, while CBAM sequentially applies channel and spatial attention without explicitly modeling edge cues. Although SA and GAM enable stronger global modeling, they introduce substantial computational overhead. In contrast, SCCGLU was specifically designed for industrial surface defect detection, where irregular shapes and blurred edges are prevalent. It incorporates direction-aware channel enhancement and edge-guided spatial refinement, thus achieving a superior balance between effectiveness and efficiency for fine-grained defect localization.</p></sec><sec id="sec3dot2dot3-sensors-25-05389"><title>3.2.3. SCCGLU-C3k2 Fusion Structure Design</title><p>To balance structural stability and improvement effectiveness, we adopted a post-enhanced fusion strategy, integrating the proposed SCCGLU module after the C3k2 module in the Neck part of YOLOv12. We utilized the C3k2 module as the original feature fusion structure, preserving its capabilities for multi-path convolutional modeling and residual connection-based semantic extraction. Based on the output of the C3k2 module, the SCCGLU module further enhanced the feature map through joint modeling and weighting of both spatial and channel dimensions, thereby improving the model&#8217;s responsiveness to fine-grained defects.</p><p>The specific implementation was as follows: first, we processed the input feature map using the C3k2 module to extract intermediate features. We then fed these intermediate features into the SCCGLU module, which generated channel weights and spatial weights separately before fusing them. Finally, we computed the joint gating signal and used it to perform weighted modulation on the output features of the C3k2 module. The enhanced feature map was denoted as <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo>&#8857;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which could be directly connected to the downstream object detection head. By keeping the original YOLOv12 Backbone unchanged, this structure enhanced the feature perception capability of the Neck layer for defect regions and effectively improved detection performance. The structure of SCCGLU-C3k2 is illustrated in <xref rid="sensors-25-05389-f006" ref-type="fig">Figure 6</xref>.</p><p>Compared with the original unimproved method, our proposed SCCGLU module achieved fundamental improvements in both structure and function. The original module had certain limitations in capturing the directional and edge features of defects, making it difficult to fully identify defect regions with complex morphologies and blurred boundaries. In contrast, by introducing direction-aware channel modeling, SCCGLU was able to specifically enhance the feature variations of defects in different directions. Meanwhile, combined with an edge-guided spatial enhancement mechanism, it further improved the model&#8217;s ability to perceive defects with irregular shapes and blurred edges. Furthermore, we adopted a post-enhancement fusion strategy to integrate it after the C3K2 module of YOLOv12, enabling the enhanced information to fuse more effectively with the backbone features. This significantly improved the detection accuracy and robustness while maintaining computational efficiency.</p></sec></sec><sec id="sec3dot3-sensors-25-05389"><title>3.3. Hybrid Head: Integrating Anchor-Based and Anchor-Free Approaches</title><p>The detection head of YOLOv12 employs a typical anchor-based [<xref rid="B31-sensors-25-05389" ref-type="bibr">31</xref>] design and structurally features a Decoupled Head [<xref rid="B32-sensors-25-05389" ref-type="bibr">32</xref>]. Anchor-based object detection represents a prevalent detection paradigm, whose core mechanism involves presetting a set of anchor boxes with varied scales and aspect ratios across the feature map of an image. These anchors are then matched with ground-truth objects to regress both positional offsets and class probabilities. In terms of the loss function, YOLOv12 employed Complete-IoU (CIoU) Loss [<xref rid="B33-sensors-25-05389" ref-type="bibr">33</xref>] for bounding box regression and Binary Cross-Entropy (BCE) Loss [<xref rid="B34-sensors-25-05389" ref-type="bibr">34</xref>] for object confidence and multi-label classification; the overall loss was a weighted combination of these three components. The structure diagram of the YOLOv12 detection head is illustrated in <xref rid="sensors-25-05389-f007" ref-type="fig">Figure 7</xref>. This detection head structure boasted advantages such as fast inference speed and good convergence. However, it faces challenges in anchor box matching, a process involving the assignment of predefined anchor boxes to ground-truth objects based on overlap criteria, and exhibits inadequate accuracy when detecting small targets, densely packed defects, or surface flaws with complex morphologies.</p><p>To further enhance the accuracy and robustness of YOLOv12 in the task of surface defect detection for aluminum profiles, we made targeted improvements to its detection head structure and designed a Hybrid Detection Head based on the fusion of anchor-based and anchor-free approaches. This detection head comprised two parallel branches: the anchor-based branch retained the original structure of YOLOv12, which was used for the detection of conventional targets; the anchor-free branch adopted a dense prediction approach and incorporated a centerness [<xref rid="B35-sensors-25-05389" ref-type="bibr">35</xref>] module to enhance the recognition capability for small objects and edge-blurred defects. The two branches, respectively, outputted bounding boxes, confidence scores, and category prediction results. During the training phase, we independently optimized each branch and conducted joint training through a weighted loss combination. During the inference phase, we employed Weighted-NMS [<xref rid="B36-sensors-25-05389" ref-type="bibr">36</xref>] to fuse the outputs of the dual branches, thereby enhancing the confidence and localization accuracy of the final prediction boxes. In terms of the loss function, the anchor-based branch adopted CIoU loss and BCE classification loss, while the anchor-free branch combined GIoU regression loss [<xref rid="B37-sensors-25-05389" ref-type="bibr">37</xref>], Focal classification loss [<xref rid="B38-sensors-25-05389" ref-type="bibr">38</xref>], and BCE centerness loss, forming an overall more discriminative optimization objective function.</p><sec id="sec3dot3dot1-sensors-25-05389"><title>3.3.1. Anchor-Free Branch</title><p>Anchor-free detection methods eliminate the need for predefined anchor boxes by directly predicting object locations, thereby simplifying model design and reducing computational complexity.</p><p>This branch adopted a dense prediction [<xref rid="B39-sensors-25-05389" ref-type="bibr">39</xref>] mechanism, directly using each pixel position on the feature map as the center of candidate boxes and regressing the bounding box positions of corresponding targets. In comparison to the anchor box mechanism, which relies on predefined anchor boxes of varying sizes and aspect ratios for object detection, this method eliminates the IoU-based matching process associated with preset anchors, thereby providing enhanced scale adaptability and regression flexibility. It was particularly suitable for locating irregular small target defects.</p><p>In the structural design, the anchor-free branch consisted of three parallel sub-modules: BBox Head, Class Head, and Centerness Head. The BBox Head outputted the distance offsets from each pixel to the target boundaries, the Class Head outputted the existence probability for each category, and the Centerness Head learned the geometric distance scores between pixels and the target center, which were used to suppress redundant predictions near the target edges. Ultimately, the total output shape of this branch was <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mfenced><mml:mrow><mml:mn>4</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, where 4 corresponds to the four regression parameters of the bounding box (representing the distance offsets from the pixel to the left, right, top, and bottom boundaries of the target, respectively), 1 represented the centerness score, and C represented the number of categories.</p><p>In terms of the loss function, we employ GIoU Loss for bounding box regression to enhance the constraint on the geometric alignment of predicted boxes. To address the class imbalance problem, where certain object categories have significantly more training instances than others and potentially bias the model toward dominant classes, we adopted Focal Loss for classification. Additionally, BCE Loss is used as the supervision signal for centerness prediction. We jointly optimized this branch with the anchor-based branch, which effectively enhanced the detection capability for tiny, dense, and irregularly shaped defects. The specific process is illustrated in <xref rid="sensors-25-05389-f008" ref-type="fig">Figure 8</xref>.</p><p>Although the introduction of an additional anchor-free branch may appear to increase computational load, it actually simplifies several stages of the detection pipeline. Specifically, anchor-free detection eliminates the need for generating and matching a large number of predefined anchors, as well as the computationally expensive IoU-based anchor-matching process during inference. The dense prediction strategy allows each feature map location to directly contribute to the prediction without relying on predefined priors, leading to a more streamlined and efficient detection process. Consequently, the overall inference efficiency remains high, ensuring that the model maintains real-time performance while achieving improved robustness and accuracy.</p></sec><sec id="sec3dot3dot2-sensors-25-05389"><title>3.3.2. Balancing and Optimization of Multiple Loss Functions in Hybrid Head</title><p>During the training process, multiple loss functions were employed to optimize the performance of the Hybrid Head. To balance the contributions of these loss functions, a rigorous weight assignment and tuning strategy was devised. The total loss <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was defined as a weighted sum of individual loss functions: <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> Here, <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> serve for bounding box regression, aiming to enhance the accuracy of object localization. <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> focus on classification tasks, ensuring that the model can accurately identify object categories. <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is utilized to control prediction quality and reduce the occurrence of low-quality predictions. The coefficients <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> determine the relative importance of each loss function.</p><p>To determine these weight coefficients, a grid search combined with validation set feedback was adopted. Initially, uniform weights <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> were set as a baseline. Subsequently, each <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was systematically varied within a reasonable range (e.g., <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, with a step size of 0.5), and key metrics on the validation set, such as mAP for detection accuracy and inference speed for efficiency, were monitored. For example, increasing the weight of <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e., increasing <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) was observed to suppress low-quality predictions but risked over-penalizing edge cases. Thus, it was balanced against <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (weight for <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) to maintain the stability of bounding box regression. After multiple rounds of iterative adjustments, the final weight combination was determined as <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This combination achieves a favorable balance among precise object localization, robust classification performance, and high-quality prediction filtering.</p><p>Moreover, the weight tuning strategy was based on a deep understanding of the objectives of each loss function. As regression losses, <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were assigned moderate weights to ensure the accuracy of bounding box localization, which is crucial for object detection tasks. <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">Focal</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as the classification loss for the anchor-free branch, was given a relatively high weight <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to address the foreground&#8211;background imbalance issue. In contrast, <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was assigned a relatively low weight (<inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) to avoid over-suppressing valid but off-center predictions. This allows it to assist regression losses, thereby improving overall performance without overly constraining the model.</p><p>This balancing and tuning strategy for loss functions adheres to the principles of multi-task learning optimization. That is, loss weights are assigned based on the relative difficulty and importance of subtasks (localization, classification, and quality control), thereby enabling the Hybrid Head architecture to operate efficiently in complex object detection tasks.</p></sec><sec id="sec3dot3dot3-sensors-25-05389"><title>3.3.3. Weighted Non-Maximum Suppression</title><p>During the inference phase, we employed the Weighted Non-Maximum Suppression (Weighted-NMS) method to integrate predictions from the anchor-based and anchor-free approaches, thereby further enhancing the confidence and localization accuracy of the final prediction boxes. First, we preliminarily filtered the sets of candidate bounding boxes output by the two branches separately. Then, we merged the two sets of candidate bounding boxes by category and performed weighted fusion on candidate bounding boxes with higher overlap. The position of the fused prediction box was obtained by weighted averaging the bounding box coordinates of multiple candidate boxes according to their confidence scores, and the confidence score was the weighted sum of the confidence scores of the participating candidate boxes. The calculation formulas are presented in Equations (8) and (9).<disp-formula id="FD8-sensors-25-05389"><label>(8)</label><mml:math id="mm85" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mi>B</mml:mi><mml:mo>&#8743;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05389"><label>(9)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="normal">s</mml:mi><mml:mo>&#8743;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the equations, <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the position of the <italic toggle="yes">i</italic>-th candidate box; <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents its corresponding confidence score; and <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mi>B</mml:mi><mml:mo>&#8743;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="normal">s</mml:mi><mml:mo>&#8743;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> indicate the fused box position and confidence score, respectively.</p><p>This strategy effectively integrated the high recall rate of the anchor-based branch with the precision advantage of the anchor-free branch in small object detection, removed redundant boxes, and retained high-quality prediction results. The flowchart of the Hybrid Head detector with anchor-based and anchor-free approaches is shown in <xref rid="sensors-25-05389-f009" ref-type="fig">Figure 9</xref>.</p><p>Compared with the detection head before improvement, our proposed Hybrid Head achieved a fundamental breakthrough in both structural design and detection mechanism. Traditional anchor-based detection heads relied on priors for target localization. Although they exhibited high localization accuracy, they were prone to limitations imposed by prior box designs when confronted with targets of irregular shapes or class imbalance. In contrast, anchor-free methods performed regression via key points or center points, exhibiting stronger generalization ability and higher adaptability. However, they might suffer from localization deviations in the case of small targets or complex backgrounds.</p><p>Our proposed Hybrid Head detection head integrates the two methods into a unified framework. It retained the advantages of anchor-based methods in precise regression while incorporating the robustness of anchor-free methods in adapting to complex morphologies and category distributions. Through the complementary synergy of this structure, the model exhibited higher accuracy and stability in detecting irregular defects and class-imbalanced data, significantly outperforming traditional detection heads that solely utilized a single detection mechanism.</p><p><xref rid="sensors-25-05389-t003" ref-type="table">Table 3</xref> summarizes the loss functions used in this subsection, including their target modules, purposes, and distinguishing characteristics, which clarify their roles in the training process.</p><p>We proposed three improvements based on YOLOv12: first, we designed a lightweight convolutional module with multi-scale feature enhancement, named MultiScaleGhost, to replace the original Conv layer in the Backbone, thereby enhancing feature extraction capability and efficiency. Second, we proposed a Spatial-Channel Collaborative Gated Linear Unit, which achieved significant enhancement of fine-grained defect regions through the collaborative fusion of direction-aware channel attention and edge-guided spatial modeling. Using a post-enhancement strategy, we integrated this unit after the C3k2 module in the Neck section of YOLOv12. Third, we constructed a Hybrid Detection Head that integrates anchor-based and anchor-free detection methods, combining the advantages of both approaches to enhance localization accuracy and sample adaptability. The overall improved MCH-YOLOv12 network structure is illustrated in <xref rid="sensors-25-05389-f010" ref-type="fig">Figure 10</xref>.</p><p>To provide a clearer overview of the architectural modifications, we present a comparative table (<xref rid="sensors-25-05389-t004" ref-type="table">Table 4</xref>) summarizing the structural differences between the original YOLOv12 and MCH-YOLOv12. This table highlights the specific components that were modified, added, or replaced, along with the rationale behind each modification.</p></sec></sec></sec><sec id="sec4-sensors-25-05389"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05389"><title>4.1. Dataset</title><p>The images in this dataset were sourced from the preliminary open dataset of the &#8220;Aluminum Profile Surface Defect Recognition&#8221; competition, which was part of the 2018 Guangdong Industrial Intelligent Manufacturing Big Data Innovation Contest&#8212;Intelligent Algorithm Challenge [<xref rid="B40-sensors-25-05389" ref-type="bibr">40</xref>]. The preliminary round of the competition was a classification contest, and its corresponding dataset contained only classification labels. We annotated defects in the original classification dataset, reconstructing it into a defect detection dataset. To construct the detection dataset, we re-annotated the original classification images using the LabelImg tool (version 1.8.4), drawing bounding boxes around visible defect regions. To ensure labeling quality, each annotated image was reviewed by at least two annotators. Discrepancies were resolved through cross-checking and consensus discussions, with the aim of maintaining consistency and accuracy across the dataset. We named this annotated detection dataset APDDD. The images in the APDDD dataset had a resolution of 2560 &#215; 1920 and contained ten types of defects: aoxian (Dent), budaodian (Non-conductive Area), cahua (Scratch), jupi (Orange Peel), loudi (Exposed Substrate), pengshang (Impact Damage), qikeng (Pit), tufen (Powder Bulge), tucengkailie (Coating Crack), and zangdian (Dirt Spot). The dataset consisted of a total of 1885 images, which we divided into training, validation, and test sets in a 7:2:1 ratio. Specifically, the training set contained 1320 images, the validation set included 377 images, and the test set comprised 188 images. Examples of the ten types of aluminum profile surface defects are illustrated in <xref rid="sensors-25-05389-f011" ref-type="fig">Figure 11</xref>.</p></sec><sec id="sec4dot2-sensors-25-05389"><title>4.2. Experimental Environment and Parameter Settings</title><p>This study was implemented on the Ubuntu system using the PyTorch deep learning framework and Python programming environment, with specific experimental configurations detailed in <xref rid="sensors-25-05389-t005" ref-type="table">Table 5</xref>. During the training process, we set the parameters shown in <xref rid="sensors-25-05389-t006" ref-type="table">Table 6</xref>.</p></sec><sec id="sec4dot3-sensors-25-05389"><title>4.3. Evaluation Metrics</title><p>We employed evaluation metrics, including precision, recall, mAP@0.5, mAP@0.5~0.95, parameters, and Floating Point Operations (FLOPs). Among these, precision primarily measured the proportion of instances predicted as positive by the model that were actually positive. Recall reflected the comprehensiveness and coverage of the model in object detection; mAP could comprehensively reflect the model&#8217;s detection performance across different categories. Parameters served as a crucial metric for assessing model complexity and computational requirements. FLOPs represented the number of floating-point operations required during the model&#8217;s forward propagation, which objectively reflected the model&#8217;s computational complexity. The specific calculation formulas are presented in Equations (10)&#8211;(12).<disp-formula id="FD10-sensors-25-05389"><label>(10)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05389"><label>(11)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05389"><label>(12)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formulas, TP represents the number of positive samples correctly identified as positive; FP represents the number of negative samples incorrectly classified as positive; FN represents the number of positive samples incorrectly classified as negative; n denotes the category index; N indicates the total number of detection categories.</p></sec><sec id="sec4dot4-sensors-25-05389"><title>4.4. Comparison with YOLOv12</title><p>To verify the improved detection performance of MCH-YOLOv12, we conducted comparative experiments between MCH-YOLOv12 and YOLOv12n. <xref rid="sensors-25-05389-t007" ref-type="table">Table 7</xref> presents the precision of MCH-YOLOv12 and YOLOv12n for each defect type, as well as the mAP@0.5 for all defects. The experimental results indicated that MCH-YOLOv12 achieved a 3.5% improvement in mAP@0.5, with varying degrees of enhancement in precision for each defect type. Specifically, the precision for qikeng, zangdian, and loudi increased by 8.2%, 6.7%, and 9.8%, respectively. These results demonstrate that the improved model can effectively address the class imbalance issue in defect detection and enhance detection accuracy.</p><p><xref rid="sensors-25-05389-f012" ref-type="fig">Figure 12</xref> illustrates the variation curves of several key evaluation metrics for MCH-YOLOv12 and YOLOv12n during the training process. As observed in the figure, after 10 epochs of training, the values of mAP@0.5, precision, and recall were all significantly better than those of the baseline. After 300 epochs of training, the model tended to stabilize. Compared to the baseline, our proposed improvement method demonstrated better training effectiveness and superior detection performance.</p><p>To more comprehensively evaluate the model&#8217;s ability to identify various types of defects, we introduced a confusion matrix for visual analysis, comparing the classification performance of the model before and after improvements, as shown in <xref rid="sensors-25-05389-f013" ref-type="fig">Figure 13</xref>. In the graph, the horizontal axis represents the true class, the vertical axis represents the model&#8217;s predicted class, and the darker the color, the higher the prediction accuracy for that class.</p><p>As observed in the figure, MCH-YOLOv12 demonstrated higher recognition accuracy across most defect categories. Particularly in categories such as jupi and tucengkailie, the prediction results were almost flawless, with an accuracy rate of 1.00 on the diagonal. This indicated that the model was more robust in extracting and identifying features of these defect types. Furthermore, categories such as qikeng, cahua, and pengshang, which previously had higher misdetection rates, also showed significant improvements in accuracy. This indicated that the enhanced model could more effectively distinguish these easily confused categories.</p></sec><sec id="sec4dot5-sensors-25-05389"><title>4.5. Ablation Experiment</title><p>To validate the independent contributions and combined effects of each improvement module on detection performance, we designed a series of ablation experiments. We progressively introduced each module into the original YOLOv12 and analyzed their effectiveness through performance comparisons. The experiments used the original YOLOv12 as the baseline (A0), sequentially adding MultiScaleGhost (A1), SCCGLU-C3K2 (A2), Hybrid Head (A3), and various module combinations (A4~A7). We maintained consistent training strategies and datasets, introduced only structural modifications, and ensured the fairness of the experiments. The experimental results are presented in <xref rid="sensors-25-05389-t008" ref-type="table">Table 8</xref>.</p><p>As observed in the table, all three improvements positively impacted the model&#8217;s detection accuracy, validating their respective effectiveness. Among them, A3 delivered the most significant performance improvement, demonstrating its strong expressive capability in locating and identifying multi-scale defect targets. A1 achieved improved accuracy while reducing the number of parameters, which demonstrated that its lightweight design maintained strong feature extraction capabilities without compromising efficiency. The improvement of A2 on mAP@0.5~0.95 was particularly significant, indicating that the proposed Spatial-Channel Collaborative Gating Mechanism helped enhance the model&#8217;s perception ability of object boundaries under different IoU thresholds, thereby improving overall robustness. The final integrated A7 model, which incorporated all improvements, achieved well-balanced performance in detection accuracy, parameter control, and computational load. This fully demonstrated the synergistic benefits among its modules. The visualization results of models A0 to A7 are presented in <xref rid="sensors-25-05389-f014" ref-type="fig">Figure 14</xref>.</p><p>The experimental results demonstrated that the A1 model had enhanced capability in capturing multi-scale features, and its mAP@0.5 increased to 0.929. Notably, the PR curve for small target defects shifted toward the upper-left corner of the coordinate system, indicating that multi-scale feature fusion had effectively improved the issues of missed detection and false detection of small defects. Leveraging the spatial-channel collaborative gating mechanism, the A2 model enhanced the model&#8217;s ability to distinguish similar defects, achieving an mAP@0.5 of 0.936. The dispersion of PR curves across various categories was reduced, validating the optimization effect of collaborative gating on fine-grained feature differentiation. The precision retention capability of the A3 model at different recall rates was further enhanced, with an mAP@0.5 of 0.932. Its robustness in detecting irregular and polymorphic defects improved, and the overall coverage area of the PR curve expanded. The progressive changes in the aforementioned PR curves intuitively demonstrated the innovative value of MultiScaleGhost in enhancing feature extraction, SCCGLU-C3k2 in optimizing feature interaction, and Hybrid Head in balancing the detection needs for different defects. The collaborative efforts of various modules enhanced the accuracy and robustness of the model in the task of surface defect detection for aluminum profiles, providing a superior solution for defect detection in industrial scenarios.</p></sec><sec id="sec4dot6-sensors-25-05389"><title>4.6. Comparative Experiment</title><p>To comprehensively validate the overall performance of the proposed MCH-YOLOv12 model in the task of surface defect detection for aluminum profiles, we designed a large-scale comparative experiment. We selected mainstream versions of the YOLO series (YOLOv5, YOLOv7, YOLOv8, YOLOv9, YOLOv10, and YOLOv11), as well as Faster-RCNN [<xref rid="B41-sensors-25-05389" ref-type="bibr">41</xref>] and Single Shot MultiBox Detector (SSD) [<xref rid="B42-sensors-25-05389" ref-type="bibr">42</xref>] as reference objects, covering the typical development stages of object detectors in recent years. These models exhibited distinct characteristics in detection accuracy, speed, and model architecture, serving as backbone frameworks widely adopted in practical application scenarios such as industrial inspection and unmanned inspection. By conducting comparisons under a unified experimental setup, we could more intuitively evaluate the applicability and advantages of the proposed method in practical tasks.</p><p>The experimental results are presented in <xref rid="sensors-25-05389-t009" ref-type="table">Table 9</xref>. As a representative of lightweight architectures, YOLOv5 had certain advantages in inference speed but lagged relatively in detection accuracy. YOLOv7 and YOLOv8 performed well on the mAP metric; particularly, YOLOv8 achieved superior overall performance after introducing the dynamic Head design. YOLOv9 to YOLOv11 exhibited an increasing trend in accuracy, but this was accompanied by a continuous rise in the number of parameters and computational requirements, which led to higher deployment costs. Faster-RCNN achieved excellent detection performance, particularly in mAP@0.5~0.95, where it ranked the highest among all detection models, surpassing MCH-YOLOv12 by 2.4%. However, its FLOPs and parameters were approximately 15 times and 6 times those of MCH-YOLOv12, respectively. Despite maintaining large FLOPs and high parameter demands, SSD yielded suboptimal detection results. The comparative results between MCH-YOLOv12, Faster-RCNN, and SSD are presented in <xref rid="sensors-25-05389-f015" ref-type="fig">Figure 15</xref> and <xref rid="sensors-25-05389-f016" ref-type="fig">Figure 16</xref>. In comparison, the MCH-YOLOv12 model proposed in this paper achieved the highest mAP@0.5 and mAP@0.5~0.95 while maintaining a low parameter count and computational cost. This demonstrated its strong detection capabilities and engineering deployability.</p><p><xref rid="sensors-25-05389-f016" ref-type="fig">Figure 16</xref>a&#8211;c, respectively, present the precision&#8211;recall curves of MCH-YOLOv12, Faster R-CNN, and SSD models on the task of aluminum profile surface defect detection. A comprehensive comparison showed that MCH-YOLOv12 exhibited the most superior detection performance across all defect categories, achieving an overall mAP@0.5 of 95.0%, which was significantly higher than those of Faster-RCNN (94.1%) and SSD (85.5%). This demonstrated its stronger precision and recall capabilities. Based on the trends of the PR curves, the curves of MCH-YOLOv12 for most categories were smooth and close to the upper-left corner, indicating its strong discriminative ability for different types of defects. Faster-RCNN performed second best; although it exhibited outstanding performance in some categories, its overall stability was slightly inferior. The PR curves of SSD generally declined early, reflecting significant shortcomings in handling small targets and complex backgrounds. Thus, MCH-YOLOv12 demonstrated the optimal detection performance while maintaining a high inference speed, making it particularly suitable for industrial vision scenarios where high precision and robustness are required.</p><p>As observed in the table, MCH-YOLOv12 achieved a 7.8% improvement in mAP@0.5 compared to YOLOv5 and a 7.0% improvement compared to the current mainstream version, YOLOv11. On the more stringent mAP@0.5~0.95 metric, the improvement was also significant, with a 6.8% increase over YOLOv11. It is noteworthy that although YOLOv11 featured a larger parameter scale and higher computational complexity, its improvement in detection accuracy tended to plateau. In contrast, the method proposed in this paper achieved a favorable balance between lightweight design and accuracy through module optimization and structural reconstruction. This makes it particularly suitable for defect detection tasks in industrial scenarios where both real-time performance and accuracy are highly needed. The visualization of the comparative experimental results is presented in <xref rid="sensors-25-05389-f017" ref-type="fig">Figure 17</xref>.</p><p>The detection results of ten types of surface defects on aluminum profiles using YOLOv12 and MCH-YOLOv12 are presented in <xref rid="sensors-25-05389-f018" ref-type="fig">Figure 18</xref>.</p><p>As observed in the figure, MCH-YOLOv12 exhibits superior detection performance compared to the original YOLOv12 across the ten types of defects on aluminum profiles.</p></sec><sec id="sec4dot7-sensors-25-05389"><title>4.7. Generalization Evaluation on the NEU-DET Dataset</title><p>Although the proposed MCH-YOLOv12 model demonstrated excellent performance on the aluminum profile defect dataset, its generalizability to other types of surface defects remained to be validated. Therefore, to further demonstrate the robustness and applicability of our method, we conducted additional experiments on the publicly available NEU-DET dataset.</p><p>To verify the generalization capability of our method, we further evaluated the model on the widely used public NEU-DET dataset. This dataset, constructed by the research team led by Kechen Song at Northeastern University, contained 1800 grayscale images of 200 &#215; 200 pixels, evenly distributed across six common steel surface defect categories: crazing, inclusion, patches, pitted surface, rolled-in scale, and scratches, with 300 labeled samples per class. This balanced dataset provided a solid benchmark for comprehensively evaluating defect detection models under diverse defect types and imaging conditions. The six types of defects in the NEU-DET dataset are shown in <xref rid="sensors-25-05389-f019" ref-type="fig">Figure 19</xref>.</p><p>Based on this, we conducted comparative experiments on the NEU-DET dataset using several mainstream detectors, including YOLOv5 through YOLOv12, with the results shown in <xref rid="sensors-25-05389-t010" ref-type="table">Table 10</xref>. Additionally, model performance was compared across mAP@0.5, precision, and recall metrics, as illustrated in <xref rid="sensors-25-05389-f020" ref-type="fig">Figure 20</xref>. MCH-YOLOv12 achieved superior performance in all metrics, attaining the highest mAP@0.5 while maintaining high precision and recall, demonstrating consistent detection advantages. These results indicated that the proposed model possessed strong robustness and cross-domain generalization capability, effectively adapting to defect types beyond those in the original training domain.</p><p>As can be seen in the comparative experimental results in the table, our proposed model exhibited significant advantages across multiple key performance metrics. Specifically, our method outperformed all comparative models across four precision metrics: precision, recall, mAP@0.5, and mAP@0.5~0.95. In particular, it achieved a 6.0% improvement in mAP@0.5~0.95 compared to the state-of-the-art YOLOv12, demonstrating stronger comprehensive detection capability and generalization performance.</p><p>Furthermore, it exhibited excellent performance in maintaining low model complexity. Our method contained only 6.8 M parameters and 16.9 G FLOPs. In contrast, although YOLOv12 achieved relatively high accuracy, its parameter count and computational load were 10.7 M and 19.3 G, respectively. While YOLOv8 exhibited relatively close accuracy, its FLOPs were as high as 25.7 G. These results indicated that our method, while maintaining high accuracy, significantly reduced the model&#8217;s computational overhead and deployment costs, demonstrating superior lightweight advantages and higher practical application value.</p><p>The detection results of six types of surface defects in the NEU-DET dataset, using YOLOv12 and MCH-YOLOv12, respectively, are shown in <xref rid="sensors-25-05389-f021" ref-type="fig">Figure 21</xref>.</p><p>The evaluation results on the NEU-DET dataset confirmed the strong generalization and cross-domain robustness of the proposed MCH-YOLOv12 model. It maintained superior detection accuracy across various defect types and imaging conditions, thereby validating its applicability to broader industrial surface inspection tasks.</p></sec><sec id="sec4dot8-sensors-25-05389"><title>4.8. Analysis of Failure Cases</title><p>To conduct a more comprehensive evaluation of the proposed model, we analyzed representative failure cases from the test set (as illustrated in <xref rid="sensors-25-05389-f022" ref-type="fig">Figure 22</xref>). These cases cover misdetections and missed detections of surface defects on aluminum profiles, enabling us to identify the model&#8217;s performance bottlenecks in complex scenarios.</p><p>The causes of detection failures can be summarized as follows: concave defects and non-conductive defects exhibit high similarity in grayscale distribution and edge blurriness, making it difficult for the model to accurately distinguish their morphological features and thus leading to misclassification; convex powder defects, due to their tiny size and low contrast against the background, have weak feature signals that fail to be effectively captured by the model, resulting in missed detections; scratches and dirty spots share certain commonalities in local texture performance, and the model&#8217;s insufficient extraction of fine-grained differential features between them leads to category confusion and misclassification. These failure cases reflect that the model still has room for improvement in terms of fine-grained modeling of morphological features and the ability to enhance differential features when dealing with low-contrast, small-sized, and visually similar defects.</p><p>To address the issues exposed by the aforementioned detection failures, future model optimization will proceed in three directions: first, designing a morphology-aware attention mechanism that incorporates geometric prior features such as curvature and circularity to strengthen the modeling of shape characteristics for defects like aoxian and pengshang, thereby reducing category confusion caused by visual similarity; second, constructing a multi-scale weak feature enhancement module to improve the capture capability of weak signals for small-sized, low-contrast defects (e.g., tufen) through cross-layer feature aggregation and adaptive threshold adjustment, aiming to lower the missed detection rate; third, expanding the dataset with edge cases under extreme lighting and complex backgrounds, combined with generative data augmentation techniques, to enhance the model&#8217;s generalization ability in complex industrial scenarios. Ultimately, this will enable more accurate and robust detection of various surface defects on aluminum profiles, providing more reliable technical support for industrial quality inspection.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05389"><title>5. Conclusions</title><p>We proposed the MCH-YOLOv12 algorithm to address the challenges of surface defect detection in aluminum profiles. By introducing multiple innovative technologies and optimization strategies, we significantly improved the model&#8217;s detection accuracy and robustness. Experimental results on the aluminum profile surface defect dataset showed that the MCH-YOLOv12 algorithm achieved significant performance metrics in precision, recall, mAP@0.5, and mAP@0.5~0.95. Compared to the original YOLOv12 algorithm, it achieved improvements of 7.9%, 3.2%, 3.5%, and 2.5% in these metrics, respectively, while parameters and FLOPs were reduced by 7.0 M and 2.5 G, respectively. The improved algorithm showed significant enhancements across all performance metrics, demonstrating the effectiveness of the algorithm&#8217;s refinements. Additionally, the algorithm demonstrated high practicality and scalability, making it widely applicable in various industrial sectors, such as metal processing, automotive manufacturing, and aerospace.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to acknowledge Jilin Provincial Department of Science and Technology for their support and encouragement toward this study.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Y.S., H.Y., Z.S. and M.Y.; methodology, Y.S. and H.Y.; software, H.Y. and Z.S.; validation, H.Y., Z.S. and M.Y.; formal analysis, Y.S. and H.Y.; investigation, Y.S. and H.Y.; resources, Y.S., H.Y., Z.S. and M.Y.; data curation, H.Y., Z.S. and M.Y.; writing&#8212;original draft preparation, Y.S., H.Y., Z.S. and M.Y.; writing&#8212;review and editing, Y.S. and H.Y.; visualization, H.Y.; supervision, H.Y., Z.S. and M.Y.; project administration, H.Y.; funding acquisition, Y.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets presented in this article are not readily available because the data are part of an ongoing study. Requests to access the datasets should be directed to 231502546@mails.ccu.edu.cn.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05389"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Song</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Improved Method Based on Faster R-CNN Network Optimization for Small Target Surface Defects Detection of Aluminum Profile</article-title><source>Proceedings of the 2021 IEEE 15th International Conference on Electronic Measurement &amp; Instruments (ICEMI)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>22&#8211;24 October 2021</conf-date></element-citation></ref><ref id="B2-sensors-25-05389"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>DMPNet: A Lightweight Remote Sensing Forest Wildfire Detection Network Based on Multi-Scale Heterogeneous Attention Mechanism and Dynamic Scaling Fusion Strategy</article-title><source>Digit. Signal Process.</source><year>2025</year><volume>164</volume><fpage>105252</fpage><pub-id pub-id-type="doi">10.1016/j.dsp.2025.105252</pub-id></element-citation></ref><ref id="B3-sensors-25-05389"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>Y.</given-names></name></person-group><article-title>A Novel Detection Method Based on DETR for Drone Aerial Images</article-title><source>Proceedings of the 2023 IEEE 6th International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)</source><conf-loc>Shenyang, China</conf-loc><conf-date>15&#8211;17 December 2023</conf-date></element-citation></ref><ref id="B4-sensors-25-05389"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name></person-group><article-title>Research on E-Commerce Special Commodity Recommendation System Based on Attention Mechanism and Dense Net Model</article-title><source>Syst. Soft Comput.</source><year>2025</year><volume>7</volume><fpage>200216</fpage><pub-id pub-id-type="doi">10.1016/j.sasc.2025.200216</pub-id></element-citation></ref><ref id="B5-sensors-25-05389"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>G.</given-names></name></person-group><article-title>Research of Integrating Prior Knowledge into Abnormal Behavior Recognition Model of EV Charging Station</article-title><source>Proceedings of the 2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)</source><conf-loc>Changchun, China</conf-loc><conf-date>24&#8211;26 February 2023</conf-date></element-citation></ref><ref id="B6-sensors-25-05389"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guan</surname><given-names>X.</given-names></name></person-group><article-title>SNc Neuron Detection Method Based on Deep Learning for Efficacy Evaluation of Anti-PD Drugs</article-title><source>Proceedings of the 2018 Annual American Control Conference (ACC)</source><conf-loc>Milwaukee, WI, USA</conf-loc><conf-date>27&#8211;29 June 2018</conf-date></element-citation></ref><ref id="B7-sensors-25-05389"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>A Lightweight Object Detection Algorithm for Remote Sensing Images Based on Attention Mechanism and YOLOv5s</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>2429</elocation-id><pub-id pub-id-type="doi">10.3390/rs15092429</pub-id></element-citation></ref><ref id="B8-sensors-25-05389"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xin</surname><given-names>Z.</given-names></name></person-group><article-title>Efficient Detection Model of Steel Strip Surface Defects Based on YOLO-V7</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>133936</fpage><lpage>133944</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3230894</pub-id></element-citation></ref><ref id="B9-sensors-25-05389"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>Defect Detection Network in PCB Circuit Devices Based on GAN Enhanced YOLOv11</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2501.06879</pub-id><pub-id pub-id-type="doi">10.54254/2755-2721/2025.20610</pub-id></element-citation></ref><ref id="B10-sensors-25-05389"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ke</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Improved Faster R-CNN for Fabric Defect Detection Based on Gabor Filter with Genetic Algorithm Optimization</article-title><source>Comput. Ind.</source><year>2022</year><volume>134</volume><fpage>103551</fpage><pub-id pub-id-type="doi">10.1016/j.compind.2021.103551</pub-id></element-citation></ref><ref id="B11-sensors-25-05389"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>S.-H.</given-names></name><name name-style="western"><surname>Tsai</surname><given-names>C.-C.</given-names></name></person-group><article-title>SMD LED Chips Defect Detection Using a YOLOv3-Dense Model</article-title><source>Adv. Eng. Inform.</source><year>2021</year><volume>47</volume><fpage>101255</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2021.101255</pub-id></element-citation></ref><ref id="B12-sensors-25-05389"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>BL-YOLOv8: An Improved Road Defect Detection Model Based on YOLOv8</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>8361</elocation-id><pub-id pub-id-type="doi">10.3390/s23208361</pub-id><pub-id pub-id-type="pmid">37896455</pub-id><pub-id pub-id-type="pmcid">PMC10610575</pub-id></element-citation></ref><ref id="B13-sensors-25-05389"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Automatic Defect Detection and Segmentation of Tunnel Surface Using Modified Mask R-CNN</article-title><source>Measurement</source><year>2021</year><volume>178</volume><fpage>109316</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2021.109316</pub-id></element-citation></ref><ref id="B14-sensors-25-05389"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Doermann</surname><given-names>D.</given-names></name></person-group><article-title>Yolov12: Attention-Centric Real-Time Object Detectors</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2502.12524</pub-id></element-citation></ref><ref id="B15-sensors-25-05389"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>Lightweight YOLOv8 Networks for Driver Profile Face Drowsiness Detection</article-title><source>Int. J. Automot. Technol.</source><year>2024</year><volume>25</volume><fpage>1331</fpage><lpage>1343</lpage><pub-id pub-id-type="doi">10.1007/s12239-024-00103-w</pub-id></element-citation></ref><ref id="B16-sensors-25-05389"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>X.</given-names></name><name name-style="western"><surname>He</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.</given-names></name></person-group><article-title>Research on Soybean Leaf Disease Recognition in Natural Environment Based on Improved YOLOv8</article-title><source>Front. Plant Sci.</source><year>2025</year><volume>16</volume><elocation-id>1523633</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2025.1523633</pub-id><pub-id pub-id-type="pmid">40260440</pub-id><pub-id pub-id-type="pmcid">PMC12009899</pub-id></element-citation></ref><ref id="B17-sensors-25-05389"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>S.</given-names></name></person-group><article-title>Propagation of Stress Wave and Fragmentation Characteristics of Gangue-Containing Coal Subjected to Water Jets</article-title><source>J. Nat. Gas Sci. Eng.</source><year>2021</year><volume>95</volume><fpage>104137</fpage><pub-id pub-id-type="doi">10.1016/j.jngse.2021.104137</pub-id></element-citation></ref><ref id="B18-sensors-25-05389"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khanam</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>M.</given-names></name></person-group><article-title>Yolov11: An Overview of the Key Architectural Enhancements</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2410.17725</pub-id><pub-id pub-id-type="arxiv">2410.17725</pub-id></element-citation></ref><ref id="B19-sensors-25-05389"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>&#214;zkan</surname><given-names>C.</given-names></name></person-group><article-title>Computer Vision in Wind Turbine Blade Inspections: An Analysis of Resolution Impact on Detection and Classification of Leading-Edge Erosion</article-title><source>Master&#8217;s Thesis</source><publisher-name>University of Stavanger</publisher-name><publisher-loc>Stavanger, Norway</publisher-loc><year>2023</year></element-citation></ref><ref id="B20-sensors-25-05389"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Y.-A.</given-names></name><name name-style="western"><surname>Song</surname><given-names>W.-W.</given-names></name></person-group><article-title>Surface Defect Detection for Aerospace Aluminum Profiles with Attention Mechanism and Multi-Scale Features</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>2861</elocation-id><pub-id pub-id-type="doi">10.3390/electronics13142861</pub-id></element-citation></ref><ref id="B21-sensors-25-05389"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>J.</given-names></name></person-group><article-title>Novel Variant Transformer-Based Method for Aluminum Profile Surface Defect Detection</article-title><source>Meas. Sci. Technol.</source><year>2024</year><volume>36</volume><fpage>025602</fpage><pub-id pub-id-type="doi">10.1088/1361-6501/ada0d2</pub-id></element-citation></ref><ref id="B22-sensors-25-05389"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>GhostNet: More Features from Cheap Operations</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date></element-citation></ref><ref id="B23-sensors-25-05389"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><article-title>Xception: Deep Learning with Depthwise Separable Convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date></element-citation></ref><ref id="B24-sensors-25-05389"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ioffe</surname><given-names>S.</given-names></name><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><article-title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</article-title><source>Proceedings of the 32nd International Conference on Machine Learning (ICML)</source><conf-loc>Lille, France</conf-loc><conf-date>6&#8211;11 July 2015</conf-date></element-citation></ref><ref id="B25-sensors-25-05389"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Glorot</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bordes</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Deep Sparse Rectifier Neural Networks</article-title><source>Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</source><conf-loc>Fort Lauderdale, FL, USA</conf-loc><conf-date>11&#8211;13 April 2011</conf-date></element-citation></ref><ref id="B26-sensors-25-05389"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dauphin</surname><given-names>Y.N.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Auli</surname><given-names>M.</given-names></name><name name-style="western"><surname>Grangier</surname><given-names>D.</given-names></name></person-group><article-title>Language Modeling with Gated Convolutional Networks</article-title><source>Proceedings of the 34th International Conference on Machine Learning (ICML)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>6&#8211;11 August 2017</conf-date></element-citation></ref><ref id="B27-sensors-25-05389"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-Excitation Networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date></element-citation></ref><ref id="B28-sensors-25-05389"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Woo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.-Y.</given-names></name><name name-style="western"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>CBAM: Convolutional Block Attention Module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date></element-citation></ref><ref id="B29-sensors-25-05389"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention Is All You Need</article-title><source>Proceedings of the 30th International Conference on Neural Information Processing Systems (NeurIPS)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date></element-citation></ref><ref id="B30-sensors-25-05389"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hoffmann</surname><given-names>N.</given-names></name></person-group><article-title>Global Attention Mechanism: Retain Information to Enhance Channel-Spatial Interactions</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2112.05561</pub-id><pub-id pub-id-type="arxiv">2112.05561</pub-id></element-citation></ref><ref id="B31-sensors-25-05389"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.Z.</given-names></name></person-group><article-title>Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date></element-citation></ref><ref id="B32-sensors-25-05389"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>YOLOX: Exceeding YOLO Series in 2021</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2107.08430</pub-id><pub-id pub-id-type="arxiv">2107.08430</pub-id></element-citation></ref><ref id="B33-sensors-25-05389"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>D.</given-names></name></person-group><article-title>Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New York, NY, USA</conf-loc><conf-date>7&#8211;12 February 2020</conf-date></element-citation></ref><ref id="B34-sensors-25-05389"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>J.</given-names></name></person-group><article-title>Rediscovering BCE Loss for Uniform Classification</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2403.07289</pub-id><pub-id pub-id-type="arxiv">2403.07289</pub-id></element-citation></ref><ref id="B35-sensors-25-05389"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>T.</given-names></name></person-group><article-title>FCOS: Fully Convolutional One-Stage Object Detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date></element-citation></ref><ref id="B36-sensors-25-05389"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ning</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name></person-group><article-title>Inception Single Shot Multibox Detector for Object Detection</article-title><source>Proceedings of the 2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</source><conf-loc>Hong Kong, China</conf-loc><conf-date>10&#8211;14 July 2017</conf-date></element-citation></ref><ref id="B37-sensors-25-05389"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rezatofighi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tsoi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Gwak</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sadeghian</surname><given-names>A.</given-names></name><name name-style="western"><surname>Reid</surname><given-names>I.</given-names></name><name name-style="western"><surname>Savarese</surname><given-names>S.</given-names></name></person-group><article-title>Generalized Intersection over Union: A Metric and a Loss for Bounding Box Regression</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date></element-citation></ref><ref id="B38-sensors-25-05389"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.-Y.</given-names></name><name name-style="western"><surname>Goyal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name></person-group><article-title>Focal Loss for Dense Object Detection</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date></element-citation></ref><ref id="B39-sensors-25-05389"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ranftl</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Koltun</surname><given-names>V.</given-names></name></person-group><article-title>Vision Transformers for Dense Prediction</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date></element-citation></ref><ref id="B40-sensors-25-05389"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Miao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>S.</given-names></name></person-group><article-title>Surface Defect Detection of Hot Rolled Steel Based on Multi-Scale Feature Fusion and Attention Mechanism Residual Block</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>7671</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-57990-3</pub-id><pub-id pub-id-type="pmid">38561416</pub-id><pub-id pub-id-type="pmcid">PMC10984981</pub-id></element-citation></ref><ref id="B41-sensors-25-05389"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</article-title><source>Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NeurIPS)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>7&#8211;12 December 2015</conf-date></element-citation></ref><ref id="B42-sensors-25-05389"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name></person-group><article-title>SSD: Single Shot Multibox Detector</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>8&#8211;16 October 2016</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05389-f001" orientation="portrait"><label>Figure 1</label><caption><p>YOLOv12 network structure diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g001.jpg"/></fig><fig position="float" id="sensors-25-05389-f002" orientation="portrait"><label>Figure 2</label><caption><p>GhostConv structure diagram. In this figure, &#934;1 to &#934;m denote a series of lightweight transformation operations that generate ghost features from the intrinsic feature maps. Each &#934; operation is composed of a 3 &#215; 3 Depthwise Separable Convolution (DWConv) [<xref rid="B23-sensors-25-05389" ref-type="bibr">23</xref>], followed sequentially by Batch Normalization (BN) [<xref rid="B24-sensors-25-05389" ref-type="bibr">24</xref>] and a Rectified Linear Unit (ReLU) [<xref rid="B25-sensors-25-05389" ref-type="bibr">25</xref>] activation. By implementing multiple &#934; operations in parallel, the module can effectively enrich feature diversity while incurring low computational costs.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g002.jpg"/></fig><fig position="float" id="sensors-25-05389-f003" orientation="portrait"><label>Figure 3</label><caption><p>MultiScaleGhost structure diagram. Here, &#934;1, &#934;2, and &#934;3 denote lightweight DWConv operations with distinct kernel sizes (3 &#215; 3, 5 &#215; 5, and 7 &#215; 7, respectively), each followed by BN and ReLU activation. These operations are specifically engineered to generate multi-scale ghost features in an efficient manner.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g003.jpg"/></fig><fig position="float" id="sensors-25-05389-f004" orientation="portrait"><label>Figure 4</label><caption><p>Flowchart of Channel-wise Gated Linear Unit.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g004.jpg"/></fig><fig position="float" id="sensors-25-05389-f005" orientation="portrait"><label>Figure 5</label><caption><p>Spatial-Channel Collaborative Gated Linear Unit structure diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g005.jpg"/></fig><fig position="float" id="sensors-25-05389-f006" orientation="portrait"><label>Figure 6</label><caption><p>SCCGLU-C3k2 structural diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g006.jpg"/></fig><fig position="float" id="sensors-25-05389-f007" orientation="portrait"><label>Figure 7</label><caption><p>YOLOv12 Detection Head structure diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g007.jpg"/></fig><fig position="float" id="sensors-25-05389-f008" orientation="portrait"><label>Figure 8</label><caption><p>Flowchart of the anchor-free branch.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g008.jpg"/></fig><fig position="float" id="sensors-25-05389-f009" orientation="portrait"><label>Figure 9</label><caption><p>The flowchart of the Hybrid Head detector with anchor-based and anchor-free.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g009.jpg"/></fig><fig position="float" id="sensors-25-05389-f010" orientation="portrait"><label>Figure 10</label><caption><p>MCH-YOLOv12 network architecture diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g010.jpg"/></fig><fig position="float" id="sensors-25-05389-f011" orientation="portrait"><label>Figure 11</label><caption><p>Ten types of defects on the surface of aluminum profiles. (<bold>a</bold>) An image containing the defect known as jupi. (<bold>b</bold>) An image containing the defect known as budaodian. (<bold>c</bold>) An image containing the defect known as tufen. (<bold>d</bold>) An image containing the defect known as cahua. (<bold>e</bold>) An image containing the defect known as aoxian. (<bold>f</bold>) An image containing the defect known as qikeng. (<bold>g</bold>) An image containing the defect known as zangdian. (<bold>h</bold>) An image containing the defect known as tucengkailie. (<bold>i</bold>) An image containing the defect known as loudi. (<bold>j</bold>) An image containing the defect known as pengshang.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g011.jpg"/></fig><fig position="float" id="sensors-25-05389-f012" orientation="portrait"><label>Figure 12</label><caption><p>Variation curves of evaluation metrics. (<bold>a</bold>) Comparison of mAP@0.5 between YOLOv12 and MCH-YOLOv12; (<bold>b</bold>) comparison of precision between YOLOv12 and MCH-YOLOv12; (<bold>c</bold>) comparison of recall between YOLOv12 and MCH-YOLOv12.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g012.jpg"/></fig><fig position="float" id="sensors-25-05389-f013" orientation="portrait"><label>Figure 13</label><caption><p>Comparison of Confusion Matrices between YOLOv12 and MCH-YOLOv12. (<bold>a</bold>) The Confusion Matrix of YOLOv12; (<bold>b</bold>) the Confusion Matrix of MCH-YOLOv12.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g013.jpg"/></fig><fig position="float" id="sensors-25-05389-f014" orientation="portrait"><label>Figure 14</label><caption><p>Visualization results of the ablation experiments. (<bold>a</bold>) The precision&#8211;recall curve of A0; (<bold>b</bold>) the precision&#8211;recall curve of A1; (<bold>c</bold>) the precision&#8211;recall curve of A2; (<bold>d</bold>) the precision&#8211;recall curve of A3; (<bold>e</bold>) the mAP@0.5 comparison chart of A4~A7.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g014.jpg"/></fig><fig position="float" id="sensors-25-05389-f015" orientation="portrait"><label>Figure 15</label><caption><p>Comparison chart of mAP@0.5. (<bold>a</bold>) The comparison results of mAP@0.5 between MCH-YOLOv12 and Faster-RCNN. (<bold>b</bold>) The comparison results of mAP@0.5 between MCH-YOLOv12 and SSD.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g015.jpg"/></fig><fig position="float" id="sensors-25-05389-f016" orientation="portrait"><label>Figure 16</label><caption><p>Visualization result diagram. (<bold>a</bold>) The precision&#8211;recall curve of MCH-YOLOv12. (<bold>b</bold>) The precision&#8211;recall curve of Faster-RCNN. (<bold>c</bold>) The precision&#8211;recall curve of SSD.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g016.jpg"/></fig><fig position="float" id="sensors-25-05389-f017" orientation="portrait"><label>Figure 17</label><caption><p>Visualization results of the comparative experiments. (<bold>a</bold>) The precision comparison chart of each model; (<bold>b</bold>) the recall comparison chart of each model; (<bold>c</bold>) the mAP@0.5 comparison chart of each model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g017.jpg"/></fig><fig position="float" id="sensors-25-05389-f018" orientation="portrait"><label>Figure 18</label><caption><p>Performance of 10 types of surface defects on aluminum profiles in YOLOv12 and MCH-YOLOv12. (<bold>a</bold>) Detection results of the 10 defects in YOLOv12. (<bold>b</bold>) Detection results of the 10 defects in MCH-YOLOv12.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g018.jpg"/></fig><fig position="float" id="sensors-25-05389-f019" orientation="portrait"><label>Figure 19</label><caption><p>The six types of defects in the NEU-DET dataset. (<bold>a</bold>) An image containing the defect known as crazing. (<bold>b</bold>) An image containing the defect known as inclusion. (<bold>c</bold>) An image containing the defect known as patches. (<bold>d</bold>) An image containing the defect known as pitted surface. (<bold>e</bold>) An image containing the defect known as rolled-in scale. (<bold>f</bold>) An image containing the defect known as scratches.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g019.jpg"/></fig><fig position="float" id="sensors-25-05389-f020" orientation="portrait"><label>Figure 20</label><caption><p>Visualization results of the comparative experiments. (<bold>a</bold>) The precision comparison chart of each model; (<bold>b</bold>) the recall comparison chart of each model; (<bold>c</bold>) the mAP@0.5 comparison chart of each model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g020.jpg"/></fig><fig position="float" id="sensors-25-05389-f021" orientation="portrait"><label>Figure 21</label><caption><p>Performance of YOLOv12 and MCH-YOLOv12 on 6 types of surface defects in the NEU-DET dataset. (<bold>a</bold>) Detection results of the 6 defects in YOLOv12. (<bold>b</bold>) Detection results of the 6 defects in MCH-YOLOv12.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g021.jpg"/></fig><fig position="float" id="sensors-25-05389-f022" orientation="portrait"><label>Figure 22</label><caption><p>Failure cases. (<bold>a</bold>) Aoxian is misclassified as a budaodian. (<bold>b</bold>) Tufen is missed (no detection output). (<bold>c</bold>) Pengshang is misclassified as a zangdian.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05389-g022.jpg"/></fig><table-wrap position="float" id="sensors-25-05389-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t001_Table 1</object-id><label>Table 1</label><caption><p>Symbol explanation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Symbol</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Definition</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">input feature map</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X<sub>p</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the base feature map output by the main convolutional module</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X<sub>gi</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the multi-scale feature map generated by the <italic toggle="yes">i</italic>-th branch</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Y</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the final fused feature map</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the number of channels in the input feature map</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C&#8217;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the target number of channels for the final feature map</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the number of output channels of the main convolutional module</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H,W</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the height and width of the feature map</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">W<sub>p</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">parameters of the 1 &#215; 1 convolutional kernel in the main convolutional module</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">W<sub>gi</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the parameters of the depthwise separable convolution kernel for the <italic toggle="yes">i</italic>-th branch</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">k<sub>i</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the kernel size of the <italic toggle="yes">i</italic>-th branch</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the reduction ratio of the main convolution module</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">n</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the number of branches in the multi-scale derivation module</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Batch Normalization</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ReLU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rectified Linear Unit</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concat</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">channel concatenation operation</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of SCCGLU with representative attention mechanisms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Attention Module</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Attention Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Enhanced Dimensions</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Key Mechanism</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Complexity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Suitability for Defect Detection</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SE-Net</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Channel-wise</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Channel</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Squeeze-and-Excitation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Good for global context; lacks spatial focus</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CBAM</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Channel and Spatial</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Channel and Spatial</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Channel and spatial attention in sequence</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Better local focus; still limited on edges</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SA</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Self-Attention</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Full Feature Map</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long-range dependencies with high computational cost</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Powerful but expensive for real-time use</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAM</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global Attention</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global Channel + Spatial</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global context fusion with separate attention branches</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Effective but overcomplex for light models</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SCCGLU (Ours)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Direction-aware and Edge</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Directional Channel + Edge Spatial</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Combines directional channel modeling and edge-aware enhancement via Gated Linear Unit</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Designed for fine-grained, edge-blurred, irregular defects</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of loss functions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Loss Function</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Applied Module</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Purpose</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Key Feature</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CIoU Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor-based regression</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimizes bounding box localization by considering overlap, center distance, and aspect ratio</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">More accurate and stable than IoU and GIoU, especially in tight bounding box regression</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BCE Classification Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor-based classification</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Binary classification to predict object presence per anchor</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Simple cross-entropy loss; does not address class imbalance</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GIoU Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor-free regression</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimizes bounding box regression by penalizing non-overlapping predictions</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improves over IoU when predicted boxes do not overlap with ground truth</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Focal Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor-free classification</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Enhances classification by addressing foreground&#8211;background imbalance</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Down-weights easy examples, focuses on hard examples, mitigates class imbalance</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BCE Centerness Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Centerness prediction</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Supervises the centerness score, indicating how close a point is to object center</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Helps suppress low-quality predictions far from the center of objects</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t004_Table 4</object-id><label>Table 4</label><caption><p>A comparison of structural components between YOLOv12 and MCH-YOLOv12.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Module</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">YOLOv12</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MCH-YOLOv12</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Purpose of Modification</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Benefit</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Backbone</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard<break/>convolution</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MultiScaleGhost Conv replace Standard Conv</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">To overcome the limitation<break/>of single-scale feature extraction</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Enhance feature<break/>representation and detection of irregular defects</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neck</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C3k2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SCCGLU-C3k2: Spatial-Channel Collaborative Gated Linear Unit added after C3k2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">To better capture direction-aware and edge-sensitive features</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improve perception of fine-grained or edge-localized defect features</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Head</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anchor-based</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hybrid Head: combines<break/>anchor-based and anchor-<break/>free branches</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">To adapt to varied object shapes/sizes and handle<break/>class imbalance</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improve accuracy, robustness, and localization under diverse scenarios</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t005_Table 5</object-id><label>Table 5</label><caption><p>Experimental environment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Experimental Configuration</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Programming language</td><td align="center" valign="middle" rowspan="1" colspan="1">Python 3.12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep learning framework</td><td align="center" valign="middle" rowspan="1" colspan="1">PyTorch 2.3.0 + CUDA 12.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel (R) Xeon (R) Platinum 8352 V (Intel Corporation, Santa Clara, CA, USA.)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Memory</td><td align="center" valign="middle" rowspan="1" colspan="1">48 GB</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">RTX 3080x2 (20 GB) (NVIDIA Corporation, Santa Clara, CA, USA.)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Development environment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">JupyterLab (version 3.6.3)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t006_Table 6</object-id><label>Table 6</label><caption><p>Parameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Input image size</td><td align="center" valign="middle" rowspan="1" colspan="1">640 &#215; 640</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of CPU threads</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Initial learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Final learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">SGD</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of training rounds</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">400</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of detection results between MCH-YOLOv12 and YOLOv12.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Models</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">jupi</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">budaodian</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">tufen</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">cahua</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">aoxian</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">qikeng</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">zangdian</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">tucengkailie</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">loudi</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">pengshang</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5/%</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv12n</td><td align="center" valign="middle" rowspan="1" colspan="1">94.0</td><td align="center" valign="middle" rowspan="1" colspan="1">86.9</td><td align="center" valign="middle" rowspan="1" colspan="1">88.5</td><td align="center" valign="middle" rowspan="1" colspan="1">88.6</td><td align="center" valign="middle" rowspan="1" colspan="1">86.2</td><td align="center" valign="middle" rowspan="1" colspan="1">65.4</td><td align="center" valign="middle" rowspan="1" colspan="1">76.0</td><td align="center" valign="middle" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" rowspan="1" colspan="1">76.9</td><td align="center" valign="middle" rowspan="1" colspan="1">93.8</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t008_Table 8</object-id><label>Table 8</label><caption><p>Ablation experiment results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Number</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MultiScaleGhost</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SCCGLU-C3K2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hybrid Head</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5~0.95/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter/10<sup>6</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs/G</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">A0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">87.9</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" rowspan="1" colspan="1">69.4</td><td align="center" valign="middle" rowspan="1" colspan="1">11.1</td><td align="center" valign="middle" rowspan="1" colspan="1">19.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7</td><td align="center" valign="middle" rowspan="1" colspan="1">92.9</td><td align="center" valign="middle" rowspan="1" colspan="1">71.6</td><td align="center" valign="middle" rowspan="1" colspan="1">8.3</td><td align="center" valign="middle" rowspan="1" colspan="1">18.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A2</td><td align="center" valign="top" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">86.1</td><td align="center" valign="middle" rowspan="1" colspan="1">91.1</td><td align="center" valign="middle" rowspan="1" colspan="1">93.6</td><td align="center" valign="middle" rowspan="1" colspan="1">74.1</td><td align="center" valign="middle" rowspan="1" colspan="1">7.5</td><td align="center" valign="middle" rowspan="1" colspan="1">19.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A3</td><td align="center" valign="top" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">90.3</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">92.3</td><td align="center" valign="middle" rowspan="1" colspan="1">71.6</td><td align="center" valign="middle" rowspan="1" colspan="1">7.1</td><td align="center" valign="middle" rowspan="1" colspan="1">18.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">87.0</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">93.3</td><td align="center" valign="middle" rowspan="1" colspan="1">71.7</td><td align="center" valign="middle" rowspan="1" colspan="1">9.3</td><td align="center" valign="middle" rowspan="1" colspan="1">18.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A5</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">88.6</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td><td align="center" valign="middle" rowspan="1" colspan="1">94.1</td><td align="center" valign="middle" rowspan="1" colspan="1">74.3</td><td align="center" valign="middle" rowspan="1" colspan="1">9.4</td><td align="center" valign="middle" rowspan="1" colspan="1">18.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A6</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" rowspan="1" colspan="1">94.3</td><td align="center" valign="middle" rowspan="1" colspan="1">75.6</td><td align="center" valign="middle" rowspan="1" colspan="1">8.0</td><td align="center" valign="middle" rowspan="1" colspan="1">19.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t009_Table 9</object-id><label>Table 9</label><caption><p>Evaluation of Experimental Outcomes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5~0.95/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter/10<sup>6</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs/G</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5</td><td align="center" valign="middle" rowspan="1" colspan="1">81.3</td><td align="center" valign="middle" rowspan="1" colspan="1">83.4</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td><td align="center" valign="middle" rowspan="1" colspan="1">62.0</td><td align="center" valign="middle" rowspan="1" colspan="1">9.6</td><td align="center" valign="middle" rowspan="1" colspan="1">16.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv7</td><td align="center" valign="middle" rowspan="1" colspan="1">83.9</td><td align="center" valign="middle" rowspan="1" colspan="1">86.0</td><td align="center" valign="middle" rowspan="1" colspan="1">90.1</td><td align="center" valign="middle" rowspan="1" colspan="1">66.7</td><td align="center" valign="middle" rowspan="1" colspan="1">36.9</td><td align="center" valign="middle" rowspan="1" colspan="1">104.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">86.6</td><td align="center" valign="middle" rowspan="1" colspan="1">84.5</td><td align="center" valign="middle" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" rowspan="1" colspan="1">69.1</td><td align="center" valign="middle" rowspan="1" colspan="1">11.2</td><td align="center" valign="middle" rowspan="1" colspan="1">28.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv9</td><td align="center" valign="middle" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" rowspan="1" colspan="1">67.3</td><td align="center" valign="middle" rowspan="1" colspan="1">72.7</td><td align="center" valign="middle" rowspan="1" colspan="1">46.0</td><td align="center" valign="middle" rowspan="1" colspan="1">9.1</td><td align="center" valign="middle" rowspan="1" colspan="1">26.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv10</td><td align="center" valign="middle" rowspan="1" colspan="1">84.1</td><td align="center" valign="middle" rowspan="1" colspan="1">81.4</td><td align="center" valign="middle" rowspan="1" colspan="1">87.8</td><td align="center" valign="middle" rowspan="1" colspan="1">64.5</td><td align="center" valign="middle" rowspan="1" colspan="1">9.3</td><td align="center" valign="middle" rowspan="1" colspan="1">21.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv11</td><td align="center" valign="middle" rowspan="1" colspan="1">83.9</td><td align="center" valign="middle" rowspan="1" colspan="1">82.2</td><td align="center" valign="middle" rowspan="1" colspan="1">88.0</td><td align="center" valign="middle" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" rowspan="1" colspan="1">9.4</td><td align="center" valign="middle" rowspan="1" colspan="1">21.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv12</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">87.9</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" rowspan="1" colspan="1">69.4</td><td align="center" valign="middle" rowspan="1" colspan="1">11.1</td><td align="center" valign="middle" rowspan="1" colspan="1">19.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster-RCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.6</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td><td align="center" valign="middle" rowspan="1" colspan="1">94.1</td><td align="center" valign="middle" rowspan="1" colspan="1">74.3</td><td align="center" valign="middle" rowspan="1" colspan="1">41.3</td><td align="center" valign="middle" rowspan="1" colspan="1">251.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SSD</td><td align="center" valign="middle" rowspan="1" colspan="1">84.6</td><td align="center" valign="middle" rowspan="1" colspan="1">77.4</td><td align="center" valign="middle" rowspan="1" colspan="1">85.5</td><td align="center" valign="middle" rowspan="1" colspan="1">60.2</td><td align="center" valign="middle" rowspan="1" colspan="1">24.7</td><td align="center" valign="middle" rowspan="1" colspan="1">98.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05389-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05389-t010_Table 10</object-id><label>Table 10</label><caption><p>Comparison of experimental results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5~0.95/%</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter/10<sup>6</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs/G</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5</td><td align="center" valign="middle" rowspan="1" colspan="1">76.7</td><td align="center" valign="middle" rowspan="1" colspan="1">72.8</td><td align="center" valign="middle" rowspan="1" colspan="1">80.3</td><td align="center" valign="middle" rowspan="1" colspan="1">54.1</td><td align="center" valign="middle" rowspan="1" colspan="1">8.7</td><td align="center" valign="middle" rowspan="1" colspan="1">15.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv7</td><td align="center" valign="middle" rowspan="1" colspan="1">77.9</td><td align="center" valign="middle" rowspan="1" colspan="1">72.8</td><td align="center" valign="middle" rowspan="1" colspan="1">80.4</td><td align="center" valign="middle" rowspan="1" colspan="1">55.4</td><td align="center" valign="middle" rowspan="1" colspan="1">33.4</td><td align="center" valign="middle" rowspan="1" colspan="1">98.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">81.3</td><td align="center" valign="middle" rowspan="1" colspan="1">80.4</td><td align="center" valign="middle" rowspan="1" colspan="1">85.2</td><td align="center" valign="middle" rowspan="1" colspan="1">61.4</td><td align="center" valign="middle" rowspan="1" colspan="1">10.8</td><td align="center" valign="middle" rowspan="1" colspan="1">25.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv9</td><td align="center" valign="middle" rowspan="1" colspan="1">78.5</td><td align="center" valign="middle" rowspan="1" colspan="1">76.5</td><td align="center" valign="middle" rowspan="1" colspan="1">82.3</td><td align="center" valign="middle" rowspan="1" colspan="1">58.0</td><td align="center" valign="middle" rowspan="1" colspan="1">8.3</td><td align="center" valign="middle" rowspan="1" colspan="1">24.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv10</td><td align="center" valign="middle" rowspan="1" colspan="1">78.0</td><td align="center" valign="middle" rowspan="1" colspan="1">71.6</td><td align="center" valign="middle" rowspan="1" colspan="1">81.1</td><td align="center" valign="middle" rowspan="1" colspan="1">54.0</td><td align="center" valign="middle" rowspan="1" colspan="1">8.9</td><td align="center" valign="middle" rowspan="1" colspan="1">20.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv11</td><td align="center" valign="middle" rowspan="1" colspan="1">78.3</td><td align="center" valign="middle" rowspan="1" colspan="1">78.8</td><td align="center" valign="middle" rowspan="1" colspan="1">83.1</td><td align="center" valign="middle" rowspan="1" colspan="1">58.3</td><td align="center" valign="middle" rowspan="1" colspan="1">8.6</td><td align="center" valign="middle" rowspan="1" colspan="1">20.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv12</td><td align="center" valign="middle" rowspan="1" colspan="1">85.0</td><td align="center" valign="middle" rowspan="1" colspan="1">80.3</td><td align="center" valign="middle" rowspan="1" colspan="1">87.9</td><td align="center" valign="middle" rowspan="1" colspan="1">61.4</td><td align="center" valign="middle" rowspan="1" colspan="1">10.7</td><td align="center" valign="middle" rowspan="1" colspan="1">19.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.9</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>