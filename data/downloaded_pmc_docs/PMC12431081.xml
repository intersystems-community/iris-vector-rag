<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431081</article-id><article-id pub-id-type="pmcid-ver">PMC12431081.1</article-id><article-id pub-id-type="pmcaid">12431081</article-id><article-id pub-id-type="pmcaiid">12431081</article-id><article-id pub-id-type="doi">10.3390/s25175439</article-id><article-id pub-id-type="publisher-id">sensors-25-05439</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Progressive Multi-Scale Perception Network for Non-Uniformly Blurred Underwater Image Restoration</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1830-1752</contrib-id><name name-style="western"><surname>Kong</surname><given-names initials="D">Dechuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05439" ref-type="aff">1</xref><xref rid="af2-sensors-25-05439" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="Y">Yandi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af3-sensors-25-05439" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhao</surname><given-names initials="X">Xiaohu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-05439" ref-type="aff">2</xref><xref rid="c1-sensors-25-05439" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="Y">Yanyan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-05439" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="Y">Yanqiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05439" ref-type="aff">1</xref><xref rid="c1-sensors-25-05439" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Chen</surname><given-names initials="H">Honggang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05439"><label>1</label>School of Artificial Intelligence, Henan Institute of Science and Technology, Xinxiang 453003, China; <email>kdc@hist.edu.cn</email> (D.K.); <email>wang_yy@hist.edu.cn</email> (Y.W.)</aff><aff id="af2-sensors-25-05439"><label>2</label>National and Local Joint Engineering Laboratory of Internet Application Technology on Mine, China University of Mining and Technology, Xuzhou 221116, China</aff><aff id="af3-sensors-25-05439"><label>3</label>School of Information Science and Engineering, Shenyang University of Technology, Shenyang 110870, China; <email>zhangyd331@163.com</email></aff><author-notes><corresp id="c1-sensors-25-05439"><label>*</label>Correspondence: <email>zhaoxiaohu@cumt.edu.cn</email> (X.Z.); <email>wangyanqiang@hist.edu.cn</email> (Y.W.)</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5439</elocation-id><history><date date-type="received"><day>13</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>28</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>29</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05439.pdf"/><abstract><p>Underwater imaging is affected by spatially varying blur caused by water flow turbulence, light scattering, and camera motion, resulting in severe visual quality loss and diminished performance in downstream vision tasks. Although numerous underwater image enhancement methods have been proposed, the issue of addressing non-uniform blur under realistic underwater conditions remains largely underexplored. To bridge this gap, we propose PMSPNet, a Progressive Multi-Scale Perception Network, designed to handle underwater non-uniform blur. The network integrates a Hybrid Interaction Attention Module to enable precise modeling of feature ambiguity directions and regional disparities. In addition, a Progressive Motion-Aware Perception Branch is employed to capture spatial orientation variations in blurred regions, progressively refining the localization of blur-related features. A Progressive Feature Feedback Block is incorporated to enhance reconstruction quality by leveraging iterative feature feedback across scales. To facilitate robust evaluation, we construct the Non-uniform Underwater Blur Benchmark, which comprises diverse real-world blur patterns. Extensive experiments on multiple real-world underwater datasets demonstrate that PMSPNet consistently surpasses state-of-the-art methods, achieving on average 25.51 dB PSNR and an inference speed of 0.01 s, which provides high-quality visual perception and downstream application input from underwater sensors for underwater robots, marine ecological monitoring, and inspection tasks.</p></abstract><kwd-group><kwd>underwater image enhancement</kwd><kwd>underwater non-uniform blur</kwd><kwd>multi-scale perception</kwd><kwd>hybrid interaction attention</kwd></kwd-group><funding-group><award-group><funding-source>Science and Technology Project of Henan Province</funding-source><award-id>242102211025</award-id><award-id>252102211059</award-id><award-id>252102210114</award-id></award-group><funding-statement>This research was funded by the Science and Technology Project of Henan Province, grant numbers 242102211025, 252102211059, and 252102210114.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05439"><title>1. Introduction</title><p>With the advancement of marine science and technology, high-definition underwater imaging has become increasingly vital for underwater applications such as underwater robotic navigation, seabed topographic mapping, and aquatic life monitoring [<xref rid="B1-sensors-25-05439" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05439" ref-type="bibr">2</xref>]. However, underwater imaging is affected by a variety of factors, including light absorption, scattering effects, and relative motion between the camera and the dynamic scene, resulting in image degradation, especially the non-uniform spatial blur distribution. We perform Fourier analysis on underwater images, extract the direction and intensity of localized frequency domain energy, and map them into blur direction and intensity heatmaps to visualize underwater blur patterns. As illustrated in <xref rid="sensors-25-05439-f001" ref-type="fig">Figure 1</xref>, real-world underwater images commonly exhibit non-uniform blur that varies in spatial extent and directional orientation, violating the common assumption of uniform blur kernels. Such degradation obscures critical visual cues, impairing the performance of downstream computer vision tasks. In autonomous underwater systems, the loss of structural information and motion cues due to non-uniform blur can lead to perceptual errors and decision-making biases, thereby posing challenges to the robustness and reliability of underwater operations.</p><p>To mitigate underwater image degradation, numerous hardware-based solutions have been developed, including specialized underwater cameras, structured lighting systems, and active imaging techniques such as laser illumination and time-gated imaging. These approaches aim to reduce visibility loss and scattering effects during image acquisition, thereby alleviating certain forms of blur. However, despite their potential to enhance raw image quality, such systems are commonly challenged for deployment in real-world environments due to their high cost, large physical footprint, and sensitivity to the environment. More critically, hardware-based methods do not explicitly address the underwater non-uniform blur arising from camera motion, moving objects, or dynamic water flow. This limitation has spurred increasing interest in algorithmic deblurring techniques, which can restore image sharpness directly from captured data without the need for auxiliary hardware.</p><p>Numerous studies have investigated traditional image enhancement techniques to address underwater image degradation. These methods typically leverage handcrafted priors and physical modeling to compensate for light attenuation and scattering. Common strategies include histogram equalization, white balance, gray-world assumptions, and Retinex-based methods for illumination decomposition. Some approaches further adapt atmospheric dehazing techniques, such as the Dark Channel Prior (DCP), to estimate the transmission map of underwater scenes. While these methods are relatively easy to deploy and interpretable, they generally assume static scenes and well-defined image structures, rendering them ineffective in the presence of the spatially non-uniform blur prevalent in real-world underwater environments. As a result, they commonly fail to recover fine image details and maintain structural consistency, limiting their utility in supporting high-level visual tasks.</p><p>With the growing application of deep learning (DL) in image restoration, learning-based methods for underwater image enhancement (UIE) have attracted increasing attention. Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) are employed to correct color distortions, enhance contrast, and improve the visibility of underwater images, owing to their powerful feature extraction and representation learning capabilities. More recently, Transformer-based architectures have been introduced into underwater image enhancement and restoration tasks, demonstrating promising performance due to their global receptive field and long-range dependency modeling. However, most of these methods remain focused primarily on enhancing the visual quality of the image, lacking explicit mechanisms to model underwater image blur. Furthermore, the enhancement performance of the above networks often scales with their architectural complexity, which increases significantly in Transformer-based designs. This imposes substantial computational burdens, making real-time deployment in underwater applications impractical. Consequently, the performance of existing enhancement algorithms is frequently compromised under non-uniform blur conditions, underscoring the urgent need for dedicated deblurring frameworks tailored to the unique challenges of underwater environments.</p><p>Focusing on the challenge of non-uniform blur in underwater images, we propose PMSPNet, a Progressive Multi-Scale Perception Network. PMSPNet perceives and models blur features from multiple perspectives, encompassing both local and global receptive fields, contextual semantic information, as well as the direction and intensity of the blur. By integrating these diverse perceptual cues, the network effectively maximizes blur removal while enhancing overall image quality.</p><p>The main work of this article is summarized as follows:<list list-type="bullet"><list-item><p>We propose a Progressive Multi-Scale Perception Network to effectively eliminate non-uniform blur in underwater images, enabling real-time underwater image enhancement.</p></list-item><list-item><p>We introduce a Hybrid Interaction Attention Module that extracts and integrates local and global blur features to capture multi-view information and accurately perceive the direction and intensity of underwater blur.</p></list-item><list-item><p>We design a Progressive Motion-Aware Perception Branch and a Progressive Feature Feedback Block to enable progressive fine-tuning of features, precise localization of blur, and efficient recovery of reconstruction details.</p></list-item><list-item><p>We construct a Non-uniform Underwater Blur Dataset to provide a benchmark for evaluating underwater image deblurring algorithms. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art approaches, validating its robustness and effectiveness.</p></list-item></list></p><p>The remainder of this article is organized as follows. <xref rid="sec2-sensors-25-05439" ref-type="sec">Section 2</xref> introduces related work. <xref rid="sec3-sensors-25-05439" ref-type="sec">Section 3</xref> describes the PMSPNet network. <xref rid="sec4-sensors-25-05439" ref-type="sec">Section 4</xref> presents the analysis and discussion of the experimental results, and <xref rid="sec5-sensors-25-05439" ref-type="sec">Section 5</xref> of this article concludes with a summary of the article and discusses potential future research areas.</p></sec><sec id="sec2-sensors-25-05439"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05439"><title>2.1. Hardware-Based Approach</title><p>To address the challenges of underwater image degradation, numerous hardware-based methods have been proposed. These approaches employ specialized imaging systems to capture higher-quality data at the point of acquisition, thereby reducing dependence on post-processing algorithms [<xref rid="B3-sensors-25-05439" ref-type="bibr">3</xref>]. Some systems utilize auxiliary light sources or polarization filters to suppress scattering and backscatter effects [<xref rid="B4-sensors-25-05439" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05439" ref-type="bibr">5</xref>]. In contrast, others adopt structured light or multi-camera setups to reconstruct clearer underwater scenes [<xref rid="B6-sensors-25-05439" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05439" ref-type="bibr">7</xref>]. Notably, high-speed cameras and inertial measurement units (IMUs) have been leveraged to estimate and compensate for camera motion, thereby mitigating motion blur during image capture [<xref rid="B8-sensors-25-05439" ref-type="bibr">8</xref>]. In parallel, the emergence of IoT-enabled sensor networks has driven research on adaptive routing, data reliability, and energy efficiency, which are critical for real-time underwater monitoring and communication systems [<xref rid="B9-sensors-25-05439" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05439" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05439" ref-type="bibr">11</xref>]. These studies explore strategies such as cross-layer optimization, secure data aggregation, and energy-aware routing to improve the dependability and scalability of sensor-based infrastructures [<xref rid="B12-sensors-25-05439" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05439" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05439" ref-type="bibr">14</xref>], providing complementary insights into the broader landscape of underwater sensing and communication. Integrating underwater image restoration techniques with efficient IoT-based sensing systems represents a promising direction, enabling both high-quality perception and reliable data delivery in challenging underwater environments. Although such hardware-enhanced systems can yield superior image quality under controlled conditions, they are commonly constrained by practical limitations, including high cost, limited deployment flexibility, and susceptibility to failure in dynamic or harsh underwater environments. Consequently, there is a pressing need for software-based solutions that can adaptively enhance underwater images without relying on specialized equipment, offering greater practicality, scalability, and robustness in real-world applications.</p></sec><sec id="sec2dot2-sensors-25-05439"><title>2.2. Traditional Approach</title><p>Traditional approaches to underwater image enhancement typically rely on physical priors or hand-crafted models to address degradation caused by absorption, scattering, and turbidity [<xref rid="B15-sensors-25-05439" ref-type="bibr">15</xref>]. Zhou et al. employed a modified underwater image formation model incorporating depth estimation and color correction to effectively mitigate the effects of light absorption and scattering [<xref rid="B16-sensors-25-05439" ref-type="bibr">16</xref>]. Ma et al. proposed an improved Retinex-based variational model that integrates information entropy smoothing and non-uniform illumination priors, enabling effective handling of uneven lighting in underwater images [<xref rid="B17-sensors-25-05439" ref-type="bibr">17</xref>]. Liu et al. introduced an illumination-constrained, structure-preserving Retinex model with adaptive channel compensation and joint estimation of illumination and reflection, demonstrating competitive performance on turbid underwater images in both subjective and objective evaluations [<xref rid="B18-sensors-25-05439" ref-type="bibr">18</xref>]. Zhou et al. combine pixel distribution remapping with a Retinex variational model and noise-texture priors, achieving notable improvements in color correction and contrast enhancement [<xref rid="B19-sensors-25-05439" ref-type="bibr">19</xref>].</p><p>Beyond physics-based models, some methods focus on enhancing images by improving contrast, correcting color, or reducing haze based on prior knowledge or statistical assumptions without explicitly modeling the underwater imaging process. Zhang et al. proposed a principal component fusion method (PCFB) that enhances underwater images by fusing contrast-enhanced foregrounds and dehazed backgrounds using principal component analysis [<xref rid="B20-sensors-25-05439" ref-type="bibr">20</xref>]. Zhang et al. introduced a multi-channel adaptive fusion approach that addresses color distortion and contrast loss through adaptive channel correction and dual-branch enhancement [<xref rid="B21-sensors-25-05439" ref-type="bibr">21</xref>]. To accommodate images across different color gamuts, Zhang et al. proposed the RAG-IMF method, which integrates global&#8211;local color correction and multi-channel fusion in both RGB and LAB color spaces, thereby extending color gamut and improving quality metrics [<xref rid="B22-sensors-25-05439" ref-type="bibr">22</xref>]. Similarly, Jha et al. developed the CBLA method, which performs RGB-based color correction and LAB-based contrast and naturalness restoration for effective underwater enhancement [<xref rid="B23-sensors-25-05439" ref-type="bibr">23</xref>]. To preserve fine details, recent works have also explored multi-scale representations and frequency-domain techniques, leading to more robust enhancement across a variety of underwater scenes [<xref rid="B24-sensors-25-05439" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>].</p><p>While the above approaches offer strong interpretability, their performance tends to degrade in highly dynamic underwater environments, where the underlying model assumptions are frequently violated. As a result, they are generally less effective in handling complex, non-uniform degradations such as motion blur and spatially varying illumination.</p></sec><sec id="sec2dot3-sensors-25-05439"><title>2.3. Data-Driven Approach</title><p>With the rapid advancement of deep learning, data-driven approaches have emerged as a powerful alternative for UIE [<xref rid="B26-sensors-25-05439" ref-type="bibr">26</xref>]. Unlike traditional methods, data-driven techniques can implicitly model complex nonlinear degradations and adapt to diverse underwater conditions through training on large-scale datasets. Xue et al. addressed the limitations of conventional color spaces by introducing a learnable Underwater Scenes Orient (USO) color space and a Scene-Adapted Semantic-Aggregated Degradation-Decoupling (S2D2) framework [<xref rid="B27-sensors-25-05439" ref-type="bibr">27</xref>]. Park et al. proposed a lightweight enhancement framework based on an adaptive standardization and normalization network, which effectively corrects distorted feature distributions and improves image contrast and brightness, all while maintaining low computational complexity [<xref rid="B28-sensors-25-05439" ref-type="bibr">28</xref>].</p><p>In parallel, Generative Adversarial Networks (GANs) [<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05439" ref-type="bibr">30</xref>], particularly diffusion models [<xref rid="B31-sensors-25-05439" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05439" ref-type="bibr">32</xref>], have further improved the capacity of neural networks to learn complex mappings between degraded underwater images and their high-quality counterparts. These models have demonstrated significant improvements in addressing color distortion, motion blur, and low contrast [<xref rid="B33-sensors-25-05439" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05439" ref-type="bibr">34</xref>]. The recent success of Transformers in computer vision has further accelerated progress in underwater image enhancement [<xref rid="B35-sensors-25-05439" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05439" ref-type="bibr">36</xref>]. Their powerful global modeling capabilities and context-aware mechanisms significantly improve detail restoration and color correction, resulting in more natural and visually appealing enhancement outcomes. Yang et al. introduced a progressive aggregation framework that utilizes a feature-prompted Transformer, combining global-local attention with multi-scale feature aggregation, to enhance detail preservation, color fidelity, and blur removal [<xref rid="B37-sensors-25-05439" ref-type="bibr">37</xref>]. Huang et al. proposed an underwater enhancement network incorporating a cross-wise Transformer module and a feature supplementation strategy to capture inter-stage dependencies and compensate for feature loss [<xref rid="B38-sensors-25-05439" ref-type="bibr">38</xref>]. Moreover, techniques involving multi-domain feature extraction, physics-guided priors, and unsupervised learning have been actively explored to improve the perceptual and generalization abilities of enhancement networks [<xref rid="B39-sensors-25-05439" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05439" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05439" ref-type="bibr">41</xref>,<xref rid="B42-sensors-25-05439" ref-type="bibr">42</xref>].</p><p>Despite their promising results, data-driven methods also face notable limitations. Convolutional Neural Networks (CNNs) struggle to capture long-range dependencies due to their localized and static receptive fields. Although Transformers effectively address this issue, their high computational cost limits their practicality, especially for deployment on resource-constrained edge devices. GAN-based models, while exhibiting strong feature generation capabilities, often suffer from limited generalization across diverse underwater scenes. Furthermore, the aforementioned methods require large volumes of training data, yet datasets specifically addressing underwater blur remain scarce. Therefore, the models are prone to domain bias, making them less effective in handling complex and variable non-uniform underwater blurred images.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05439"><title>3. Methods</title><p>To address the challenges, we propose PMSPNet, a Progressive Multi-Scale Perception Network, as illustrated in <xref rid="sensors-25-05439-f002" ref-type="fig">Figure 2</xref>. The network incorporates a Hybrid Interaction Attention Module (HIAM) to capture local details and global contextual dependencies, enabling an initial coarse perception of image blur features. Building upon this, we introduce the Progressive Motion-Aware Perception Branch (PMAB) and the Progressive Feature Feedback Block (PFFB), which can incrementally guide and refine the network&#8217;s perceptual capability, thereby achieving accurate localization and representation of non-uniform blur regions. Furthermore, to facilitate more realistic and comprehensive training and evaluation, we construct a dedicated Non-uniform Underwater Blur Dataset (N2UD), which encompasses a wide range of blur patterns encountered in real-world underwater environments.</p><sec id="sec3dot1-sensors-25-05439"><title>3.1. N2UD</title><p>To address the scarcity of underwater blur datasets, we constructed a specialized dataset by filtering an existing publicly available underwater image dataset. Specifically, we combined quantitative blur metrics with manual visual inspection to identify and select images exhibiting non-uniform blur. The resulting collection forms a new benchmark, termed the Non-uniform Underwater Blur Dataset (N2UD), which serves as a representative testbed for evaluating underwater deblurring algorithms under complex and realistic degradation conditions. As illustrated in <xref rid="sensors-25-05439-f003" ref-type="fig">Figure 3</xref>, N2UD includes underwater images affected by non-uniform blur across various resolutions and scenes, ensuring diversity and practical relevance.</p><p>The quantitative blur metrics of which can be expressed as<disp-formula id="FD1-sensors-25-05439"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>isBlurred</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the underwater image sharpness measure, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the global contrast level, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the mean image gradient, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> corresponds to the high-frequency energy. When <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mi>IsBlurred</mml:mi></mml:mrow></mml:math></inline-formula> equals 1, it signifies that the image exhibits non-uniform blurring. The underwater image sharpness measure <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined as<disp-formula id="FD2-sensors-25-05439"><label>(2)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:munderover><mml:mn>20</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>log</mml:mi><mml:mfenced open="(" close=")"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where the image is converted to the YUV color space and then partitioned into <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> non-overlapping blocks; <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the maximum and minimum luminance values, respectively, for the <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mi>th</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> block; <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> is a small constant (<inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) to avoid division by zero; and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the corresponding threshold, set to 2.0. The <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is expressed as<disp-formula id="FD3-sensors-25-05439"><label>(3)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mi>LAB</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>255</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mi>LAB</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the LAB color space of the input image <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>RGB</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the standard deviation, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the corresponding threshold, set to 0.15. The <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is expressed as<disp-formula id="FD4-sensors-25-05439"><label>(4)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the total number of pixels in the image; <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are the width and height of the image, respectively; and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the corresponding threshold, set to 15. The <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the gradient magnitude calculation for each pixel point, which is expressed as<disp-formula id="FD5-sensors-25-05439"><label>(5)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> respectively perform Sobel filtering on the horizontal and vertical gradients of the grayscale image <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>gray</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the original image <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>RGB</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is expressed as<disp-formula id="FD6-sensors-25-05439"><label>(6)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#948;</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mi>HF</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the corresponding threshold, set to 0.2. <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mi>HF</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is expressed as<disp-formula id="FD7-sensors-25-05439"><label>(7)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mi>HF</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>Mask</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Mask</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are expressed as<disp-formula id="FD8-sensors-25-05439"><label>(8)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>FS</mml:mi><mml:mo>(</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>gray</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">u</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msup><mml:mi mathvariant="normal">e</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mi>&#223;</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub><mml:mi mathvariant="normal">H</mml:mi></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mi mathvariant="normal">W</mml:mi></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05439"><label>(9)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Mask</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>&#8804;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>FS</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a frequency shift that centers the zero-frequency (DC) component, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Mask</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a circular low-pass suppression mask centered at <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with radius <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>min</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">H</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Considering that existing blur metrics may not fully capture the spatial variations of blur, we conducted manual visual inspections of each candidate image to refine the dataset. Based on evaluations by multiple experts specializing in visual perception, we retained only those images exhibiting noticeable non-uniform blur. This process ensures that the dataset accurately reflects the complex and spatially varying blur characteristics commonly observed in real-world underwater environments.</p></sec><sec id="sec3dot2-sensors-25-05439"><title>3.2. Hybrid Interaction Attention Module</title><p>To enhance the network&#8217;s capacity for deep feature extraction and precise localization of blurred regions in underwater scenes, we propose a Hybrid Interaction Attention Module (HIAM), as shown in <xref rid="sensors-25-05439-f004" ref-type="fig">Figure 4</xref>. Unlike traditional hybrid methods [<xref rid="B43-sensors-25-05439" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05439" ref-type="bibr">44</xref>], HIAM, a cross-scale dual-channel design, utilizes an interactive cross-attention fusion dual-attention strategy for adaptive weighting control. This hybrid design enables collaborative modeling of localized blur features and global semantic structures, achieving coarse estimation of blurred feature extraction and localization.</p><p>Specifically, given an input feature <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">I</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the process is represented as<disp-formula id="FD10-sensors-25-05439"><label>(10)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicates <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution operation. The calculation process for other characters is as follows.</p><p>Firstly, we obtain <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD11-sensors-25-05439"><label>(11)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:mi>DSC</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> indicates <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution operation, and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>DSC</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates depthwise separable convolution. Subsequently, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">V</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are obtained by<disp-formula id="FD12-sensors-25-05439"><label>(12)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05439"><label>(13)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD14-sensors-25-05439"><label>(14)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">V</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are adaptive global max pooling and average pooling, and <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are trainable matrices. <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be given by the following:<disp-formula id="FD15-sensors-25-05439"><label>(15)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>sig</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>soft</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold">V</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>sig</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>soft</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are Sigmoid and Softmax activation functions, and <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">d</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the dimension of the key vector. Thus, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed as<disp-formula id="FD16-sensors-25-05439"><label>(16)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mi mathvariant="bold">d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#8857; is a pointwise multiplication operation. <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed as<disp-formula id="FD17-sensors-25-05439"><label>(17)</label><mml:math id="mm73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">S</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot3-sensors-25-05439"><title>3.3. Progressive Motion-Aware Perception Branch</title><p>To further enhance the modeling of aware features in underwater non-uniform blur scenarios, we introduce a Progressive Motion-Aware Perception Branch (PMPB), as depicted in the lower part of <xref rid="sensors-25-05439-f004" ref-type="fig">Figure 4</xref>. This module progressively refines the localization of non-uniform blur features in a coarse-to-fine manner, thereby strengthening the network&#8217;s multi-level perception of structural details.</p><p>Specifically, a two-dimensional Butterworth filter is applied to the input image to perform frequency adjustment. This operation amplifies the response of blurred regions across varying frequency components, improving the network&#8217;s sensitivity to related features [<xref rid="B45-sensors-25-05439" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05439" ref-type="bibr">46</xref>]. The filtering process is defined as<disp-formula id="FD18-sensors-25-05439"><label>(18)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">H</mml:mi><mml:mi>hf</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">L</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">R</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> converts the image from LAB to RGB color space; <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Concat</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a channel splicing operation; and <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mi>LAB</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mi>LAB</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mi>LAB</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> are the luminance and color channels in the LAB space, respectively. <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">H</mml:mi><mml:mi>hf</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents high-frequency extraction operations, expressed as<disp-formula id="FD19-sensors-25-05439"><label>(19)</label><mml:math id="mm81" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">H</mml:mi><mml:mi>hf</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">L</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#8476;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>spatial</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8476;</mml:mo><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the real part of a complex number. <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>spatial</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be expressed as<disp-formula id="FD20-sensors-25-05439"><label>(20)</label><mml:math id="mm84" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>spatial</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the inverse fast Fourier transform (IFFT). <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be expressed as<disp-formula id="FD21-sensors-25-05439"><label>(21)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>IS</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD22-sensors-25-05439"><label>(22)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">&#732;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#183;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>bw</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>IS</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates that the centralized spectrum diagram will be restored to its original layout. <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">C</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed as<disp-formula id="FD23-sensors-25-05439"><label>(23)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>FS</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">L</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the fast Fourier transform (FFT).</p><p>The Butterworth filter <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>bw</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is constructed as<disp-formula id="FD24-sensors-25-05439"><label>(24)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>bw</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mi>w</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:msqrt></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the cutoff frequency, <italic toggle="yes">n</italic> is the filter order, and <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> is the anti-decimation constant, set to <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>Subsequently, the frequency domain enhanced image <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is subjected to initial feature extraction, which is denoted as<disp-formula id="FD25-sensors-25-05439"><label>(25)</label><mml:math id="mm99" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>re</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">b</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>re</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">b</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>re</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the ReLU activation function.</p><p>Finally, we introduce Deformable Convolution [<xref rid="B47-sensors-25-05439" ref-type="bibr">47</xref>] to adaptively adjust the sampling position for more accurate perception and feature extraction of the non-uniform blur region, which is expressed as<disp-formula id="FD26-sensors-25-05439"><label>(26)</label><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:mi mathvariant="bold">o</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#9651;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mi mathvariant="normal">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#9651;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow><mml:mi mathvariant="normal">q</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:mi mathvariant="bold">o</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula> is the value of the output feature map at position <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the weight of the <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> position of the convolution kernel; <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow></mml:math></inline-formula> is a bilinear interpolating neighborhood; <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the interpolated weight, satisfying <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>q</mml:mi></mml:msub><mml:mi mathvariant="normal">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> standard sampling offset in the convolution kernel; <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#9651;</mml:mo><mml:msub><mml:mi mathvariant="bold">p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the learnable offset of the <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> position; and <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow><mml:mi mathvariant="normal">q</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the value of the input feature <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> at position <italic toggle="yes">q</italic>.</p><p>The fine features are fed into the backbone to provide it with feature guidance, which is expressed as<disp-formula id="FD27-sensors-25-05439"><label>(27)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="bold">a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">o</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="bold">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi mathvariant="bold">o</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot4-sensors-25-05439"><title>3.4. Progressive Feature Feedback Block</title><p>To achieve lossless extraction of hierarchical semantic features during reconstruction, we propose a Progressive Feature Feedback Block (PFFB), as illustrated in <xref rid="sensors-25-05439-f004" ref-type="fig">Figure 4</xref>. This module facilitates inter-layer information interaction by constructing a hierarchical feedback pathway, allowing recovered features from subsequent stages to guide and refine the representation learning in previous stages. The structure of the PFFB is represented as<disp-formula id="FD28-sensors-25-05439"><label>(28)</label><mml:math id="mm116" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>re</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>att</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>att</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be expressed as<disp-formula id="FD29-sensors-25-05439"><label>(29)</label><mml:math id="mm118" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>att</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>soft</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>MLP</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>up</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>cu</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>DSC</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>up</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>cu</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>MLP</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is a multilayer perceptron, and <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>up</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi>cu</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the previous and current level recovery features in the decoder, respectively.</p></sec><sec id="sec3dot5-sensors-25-05439"><title>3.5. Loss Function</title><p>To increase visual quality and perceptual fidelity of the restored image, we design a composite loss function that jointly enforces constraints on color accuracy, structural consistency, frequency response, and perceptual realism. This multi-objective formulation ensures that the restored images align closely with ground truth data and exhibit enhanced perceptual quality. The overall loss is defined as<disp-formula id="FD30-sensors-25-05439"><label>(30)</label><mml:math id="mm123" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>charb</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>fft</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>fft</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>lab</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lab</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>lch</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lch</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>vgg</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>color</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>charb</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>fft</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lab</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lch</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>vgg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>color</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are charbonnier loss, fast fourier transform loss, LAB color loss, LCH color loss, perceptual loss, and color constancy loss, respectively. <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>fft</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>lab</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>lch</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the corresponding weighting coefficients, set to <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>charb</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined as<disp-formula id="FD31-sensors-25-05439"><label>(31)</label><mml:math id="mm137" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>charb</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>&#1013;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> are the predicted image and ground truth, and <italic toggle="yes">N</italic> is the total number of pixels in the image. <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>fft</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined as<disp-formula id="FD32-sensors-25-05439"><label>(32)</label><mml:math id="mm139" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>fft</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:mi mathvariant="script">F</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mfenced><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is L1 loss. <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lab</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined as<disp-formula id="FD33-sensors-25-05439"><label>(33)</label><mml:math id="mm142" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lab</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>ab</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is the weighting coefficient, set to 1. <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>ab</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are the brightness and color channel loss in the LAB channel, defined as<disp-formula id="FD34-sensors-25-05439"><label>(34)</label><mml:math id="mm146" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD35-sensors-25-05439"><label>(35)</label><mml:math id="mm147" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>ab</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#183;</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#8776;</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#183;</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are the softmax weights for the <italic toggle="yes">k</italic>th bin at the <italic toggle="yes">i</italic>th pixel position of images <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>. <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lch</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined as<disp-formula id="FD36-sensors-25-05439"><label>(36)</label><mml:math id="mm151" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>ll</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lc</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lh</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>ll</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lc</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent brightness and chromaticity L1 loss (reference Equation (<xref rid="FD34-sensors-25-05439" ref-type="disp-formula">34</xref>)), and <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>lh</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents hue distribution loss (reference Equation (<xref rid="FD35-sensors-25-05439" ref-type="disp-formula">35</xref>)) in LCH color space.</p><p>In addition, <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>vgg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> utilizes the multi-scale features of the VGG19 network to measure the differences between the enhanced image and the reference image in the high-level semantic space. <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>color</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> achieves color balance consistency by minimizing the pairwise Euclidean distances between the <italic toggle="yes">R</italic>, <italic toggle="yes">G</italic>, and <italic toggle="yes">B</italic> channels.</p></sec></sec><sec id="sec4-sensors-25-05439"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05439"><title>4.1. Datasets</title><p>We train and evaluate the proposed network using the constructed N2UD dataset and compare its performance against state-of-the-art methods on N2UD and three publicly available underwater image enhancement benchmarks: EUVP [<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>], LSUI [<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>], and UIEB [<xref rid="B50-sensors-25-05439" ref-type="bibr">50</xref>]. Additionally, to assess the practical benefits of the enhanced images in real-world scenarios, we conduct downstream task evaluations on the DUO dataset [<xref rid="B51-sensors-25-05439" ref-type="bibr">51</xref>], demonstrating the model&#8217;s effectiveness in supporting higher-level underwater vision tasks.</p><sec id="sec4dot1dot1-sensors-25-05439"><title>4.1.1. N2UD</title><p>The N2UD dataset comprises 3201 real underwater image pairs, each consisting of a non-uniformly blurred image and its corresponding reference. Among these, 2246 pairs were used for training, 624 for testing, and 322 for validation. The dataset consists of several publicly available underwater image enhancement datasets. Using a combination of quantitative blur metrics with manual visual inspection, we screened high-quality non-uniform blur image pairs. Specifically, the dataset includes 555 images from the EUVP dataset, 2190 from the LSUI dataset, and 465 from the UIEB dataset. The selected images span a range of resolutions, from <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1280</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and diverse underwater environments and imaging conditions. The dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/UI2025/N2UD">https://github.com/UI2025/N2UD</uri>, accessed on 28 August 2025.</p></sec><sec id="sec4dot1dot2-sensors-25-05439"><title>4.1.2. EUVP</title><p>The EUVP dataset is designed to support the enhancement of color and structural details in real-world underwater images. It employs multiple underwater sensors (GoPros, Aqua AUV&#8217;s uEye cameras, low-light USB cameras, and Trident ROV&#8217;s HD camera) to capture underwater images and conduct marine exploration and human-robot cooperative experiments in different locations under various visibility conditions. It contains both paired and unpaired collections of approximately 20,000 images, covering a broad spectrum of underwater environments, lighting conditions, and viewpoints. The dataset includes both real and synthetically degraded underwater images at various resolutions, making it suitable for training and evaluating deep learning-based underwater image enhancement models. The dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://irvlab.cs.umn.edu/resources/euvp-dataset">https://irvlab.cs.umn.edu/resources/euvp-dataset</uri>, accessed on 17 May 2025.</p></sec><sec id="sec4dot1dot3-sensors-25-05439"><title>4.1.3. LSUI</title><p>The LSUI dataset, filtered and screened based on various real underwater image data, is a large-scale collection of 4279 real-world underwater images captured across multiple scenes, including shallow waters, deep sea, coral reefs, and shipwrecks. It encompasses a wide variety of conditions in terms of lighting, water quality, and color degradation. LSUI emphasizes real data distributions and high scene complexity, making it well-suited for studying naturally occurring underwater degradations. The dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://lintaopeng.github.io/_pages/UIE%20Project%20Page.html">https://lintaopeng.github.io/_pages/UIE%20Project%20Page.html</uri>, accessed on 17 May 2025.</p></sec><sec id="sec4dot1dot4-sensors-25-05439"><title>4.1.4. UIEB</title><p>The UIEB dataset collects and processes underwater images on different platforms and includes private underwater shooting videos. It is one of the most widely used benchmarks in underwater image enhancement. It includes 890 real-world underwater images of varying resolutions, most of which have been enhanced by domain experts to form a high-quality paired dataset. It is commonly used for both subjective and objective evaluation of enhancement algorithms. In addition, UIEB includes a challenging subset, Challenging-60, comprising 60 challenging unpaired samples used to test algorithmic robustness under extreme conditions. The dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://li-chongyi.github.io/proj_benchmark.html">https://li-chongyi.github.io/proj_benchmark.html</uri>, accessed on 17 May 2025.</p></sec><sec id="sec4dot1dot5-sensors-25-05439"><title>4.1.5. DUO</title><p>The DUO dataset is a high-quality benchmark designed for underwater object detection (UOD) tasks. These data were obtained by collecting underwater images through underwater photography and labeling them, and they were used to study the perception decisions and sensor sensitivity issues of underwater robots. It contains 7782 images captured from diverse underwater environments, including shallow and deep water, turbid regions, and varied lighting conditions. The dataset features a wide array of target types, including starfish, sea urchins, and fish, along with substantial variation in scene structure, imaging quality, and target scale. All images are provided at a uniform resolution of <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1920</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1080</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, with 6671 images allocated for training and 1111 for testing. In this work, we utilize the trained enhancement model to augment the DUO dataset and perform downstream detection tasks, thereby validating the positive influence of our method on higher-level vision applications. The dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/chongweiliu/DUO">https://github.com/chongweiliu/DUO</uri>, accessed on 18 May 2025.</p></sec></sec><sec id="sec4dot2-sensors-25-05439"><title>4.2. Experimental Configuration</title><sec id="sec4dot2dot1-sensors-25-05439"><title>4.2.1. Implementation Details</title><p>The proposed network was implemented using the PyTorch 2.0.1 framework and trained on a Linux-based system equipped with a GeForce RTX 3090 GPU (24 GB), 250 GB of memory, and 80 Intel(R) Xeon(R) Gold 5218R processors (2.10 GHz). The training was performed using the Adam optimizer with an initial learning rate of <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. A cosine learning rate scheduling strategy with a warm-up phase was employed, in which the learning rate was gradually increased during the first 10 epochs, after which it followed a cosine decay schedule. The network was trained for 200 epochs with a batch size of 8. To improve the model&#8217;s robustness and generalization capability, standard data augmentation techniques such as random horizontal and vertical flipping were applied during training. To prevent overfitting and ensure stable convergence, an early stopping mechanism was employed, in which training was terminated if the validation loss failed to improve for 10 consecutive epochs. Furthermore, random seeds were fixed across all experiments to guarantee reproducibility of the reported results.</p></sec><sec id="sec4dot2dot2-sensors-25-05439"><title>4.2.2. Evaluation Metrics</title><p>To comprehensively assess the performance of the proposed method in restoring underwater images affected by non-uniform blur, we employed a diverse set of evaluation metrics covering full-reference, no-reference, perceptual, and sharpness-based aspects. For full-reference evaluation, we used the Peak Signal-to-Noise Ratio (PSNR), the Structural Similarity Index (SSIM), and the Feature Similarity Index (FSIM) to quantify pixel-level fidelity and structural consistency between the restored images and their ground-truth counterparts. To evaluate perceptual quality, we adopted LPIPS [<xref rid="B52-sensors-25-05439" ref-type="bibr">52</xref>], which measures perceptual similarity using deep feature representations and aligns better with human visual perception.</p><p>For no-reference evaluation, we employed underwater-specific quality metrics, including UIQM [<xref rid="B53-sensors-25-05439" ref-type="bibr">53</xref>] and UCIQE [<xref rid="B54-sensors-25-05439" ref-type="bibr">54</xref>], to evaluate key visual attributes such as color fidelity, contrast, and clarity. Additionally, we introduced NIQE [<xref rid="B55-sensors-25-05439" ref-type="bibr">55</xref>], a natural image quality evaluator, and URanker [<xref rid="B56-sensors-25-05439" ref-type="bibr">56</xref>], a human preference-based perceptual scoring model, to further assess image naturalness and subjective quality. To evaluate the preservation of fine details and edge sharpness, we incorporated high-frequency sharpness metrics, including Laplacian variance, Tenengrad, and Brenner gradient, each capturing edge clarity and detail recovery from different computational perspectives.</p><p>The integration of these complementary metrics enables a comprehensive and objective evaluation of image enhancement performance, ensuring the model&#8217;s robustness and practical applicability in real-world underwater scenarios and downstream tasks.</p></sec></sec><sec id="sec4dot3-sensors-25-05439"><title>4.3. Performance Comparison</title><sec id="sec4dot3dot1-sensors-25-05439"><title>4.3.1. N2UD</title><p>To comprehensively evaluate the effectiveness of the proposed method, we conducted extensive experiments on the constructed N2UD dataset using both full-reference and no-reference image quality assessment metrics. The results are presented in <xref rid="sensors-25-05439-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05439-t002" ref-type="table">Table 2</xref>.</p><p>As shown in <xref rid="sensors-25-05439-t001" ref-type="table">Table 1</xref>, traditional image processing methods exhibit generally poor performance across full-reference metrics. Specifically, their PSNR values typically fall below 18 dB, and LPIPS scores often exceed 0.3, indicating significant limitations in perceptual quality and visual fidelity. These methods struggle to adaptively model complex blur degradation, which severely limits their scalability in real-world applications. Furthermore, traditional methods are often computationally intensive and inefficient. Some require up to 1.24 s to process a single image, rendering them unsuitable for time-sensitive tasks. In contrast, deep learning-based UIE methods demonstrate more robust performance. GAN-based approaches offer modest improvements in some metrics. CycleGAN achieves relatively high SSIM (<inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.83</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.11</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) and LPIPS (<inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.24</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) compared to traditional methods. However, it exhibits considerable PSNR variance (<inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#177;</mml:mo><mml:mn>4.72</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), highlighting a lack of consistency and reliability in generative adversarial models. U-Shape, leveraging a multi-scale architecture, performs well across all full-reference metrics but still falls short of the performance achieved by PMSPNet. Among recently proposed Transformer-based methods, HistFormer suffers from mode collapse issues, leading to degraded performance across multiple metrics. Additionally, it incurs a high computational cost (44.42 GFLOPs) and slower inference speeds. PhaseFormer, in contrast, shows a more balanced performance. However, its relatively low SSIM indicates limitations in preserving structural integrity. PUIE-Net achieves favorable results across most metrics, yet its complexity is a concern, with computational overhead reaching 150.69 GFLOPs and inference time extending to 0.13 s per image. SGUIE performs well in FSIM and LPIPS, reflecting strengths in perceptual quality. Nonetheless, its overall effectiveness remains inferior when compared to PMSPNet. PMSPNet achieves superior performance in both objective metrics and computational efficiency, confirming its robustness and practical applicability for real-world underwater deblurring tasks.</p><p>As shown in the no-reference image quality evaluation results in <xref rid="sensors-25-05439-t002" ref-type="table">Table 2</xref>, although some methods perform reasonably well on individual metrics, a noticeable gap remains in overall quality. Notably, PMSPNet consistently leads across multiple perceptual quality metrics, such as UIQM and UCIQE, indicating that its enhanced images exhibit more natural color, clarity, and contrast, aligning well with human visual preferences. Interestingly, some traditional methods achieve higher scores on sharpness-based metrics, such as Tenengrad and Brenner. This contrasts with their overall inferior performance on full-reference and other no-reference metrics. The root cause of this &#8220;contradiction&#8221; is that many current reference-free metrics capture only specific local attributes, failing to reflect comprehensive image quality or perceptual realism.</p><p>The vision comparisons in <xref rid="sensors-25-05439-f005" ref-type="fig">Figure 5</xref> further support this conclusion. While traditional methods can enhance local edge sharpness, they often suffer from over-sharpening, resulting in structural distortions, unnatural textures, or even pattern collapse, leading to a visually unrealistic &#8220;pseudo-enhancement&#8221; effect. In contrast, deep learning-based methods leverage strong feature representation and adaptive learning to restore degraded details more comprehensively. By capturing complex, non-uniform blur patterns across multiple scales and dimensions, PMSPNet achieves accurate regional restoration while preserving structural clarity and avoiding artifacts. This results in improved naturalness, semantic consistency, and stronger generalization in real-world conditions.</p><p>To further validate model robustness, <xref rid="sensors-25-05439-f006" ref-type="fig">Figure 6</xref> presents the statistical distribution of three representative full-reference metrics. Traditional methods exhibit low and tightly clustered scores, indicating limited and inconsistent enhancement. Deep learning-based methods perform better overall. PMSPNet shows the highest median and maximum values, especially exceeding 35 dB in PSNR, and maintains stable, high distributions in SSIM and FSIM. The low dispersion of PMSPNet scores reflects its adaptability across various scenes, attributed to its hybrid interaction attention and progressive motion-aware modules, which jointly enhance spatially non-uniform blur modeling and semantic detail recovery.</p></sec><sec id="sec4dot3dot2-sensors-25-05439"><title>4.3.2. EUVP</title><p>To further assess the generalization capability of the proposed network, we evaluated pre-trained models on the EUVP dataset. As shown in <xref rid="sensors-25-05439-t003" ref-type="table">Table 3</xref>, PMSPNet achieves the best performance across nearly all full-reference metrics, including PSNR (25.81), SSIM (0.85), and FSIM (0.94), and the lowest LPIPS (0.21), demonstrating superior fidelity and perceptual quality. In no-reference perceptual assessments, PMSPNet remains competitive, achieving a UIQM of 3.09, a UCIQE of 0.36, and a URanker of 1.16, consistently outperforming some existing methods.</p><p>As illustrated in <xref rid="sensors-25-05439-f007" ref-type="fig">Figure 7</xref>, visual comparisons further substantiate PMSPNet&#8217;s advantages. Compared to prior methods, PMSPNet produces images with sharper edges, more natural color reproduction, and enhanced contrast. Traditional methods such as HLRP, ACDC, and MMLE suffer from color shifts, amplified noise, or loss of structural detail. Although deep learning methods like CycleGAN and U-Shape show improved enhancement, they still exhibit color deviations or insufficient deblurring in complex scenes. In contrast, PMSPNet effectively restores clarity and color fidelity even under severe degradation, delivering results that are perceptually closest to the ground truth.</p></sec><sec id="sec4dot3dot3-sensors-25-05439"><title>4.3.3. LSUI</title><p><xref rid="sensors-25-05439-t004" ref-type="table">Table 4</xref> presents the evaluation results on the LSUI dataset. PMSPNet achieves the highest scores across key full-reference metrics, along with the lowest LPIPS, indicating superior fidelity and perceptual quality in the restored images. In the no-reference evaluation, PMSPNet also ranks first in UIQM and second in UCIQE, further demonstrating its effectiveness in enhancing visual quality. In contrast, traditional methods and other deep learning-based approaches show reduced performance across most metrics due to their limited capacity to manage complex color degradation and structural blurring prevalent in underwater environments.</p><p>The visual results in <xref rid="sensors-25-05439-f008" ref-type="fig">Figure 8</xref> further highlight the advantages of PMSPNet. Traditional algorithms often leave residual blur or introduce significant color distortions. While learning-based models such as U-Shape and CCL-Net show improved results, they still suffer from over-smoothing and color shifts, particularly in fine details. In comparison, PMSPNet effectively recovers key textures, preserves natural color tones, and enhances contrast, producing visually more realistic and aesthetically pleasing results.</p></sec><sec id="sec4dot3dot4-sensors-25-05439"><title>4.3.4. UIEB</title><p>The UIEB dataset presents a broad spectrum of underwater imaging challenges, including severe color distortion, low contrast, and complex lighting conditions. As shown in <xref rid="sensors-25-05439-t005" ref-type="table">Table 5</xref>, PMSPNet outperforms most compared methods across full-reference and no-reference metrics. Specifically, it achieves a PSNR of 22.43 dB, SSIM of 0.86, and a low LPIPS of 0.21, reflecting strong reconstruction accuracy and perceptual similarity. While methods like CCL-Net and SGUIE yield competitive results, their higher variance in metrics such as URanker and UIQM suggests inconsistent performance. In contrast, PMSPNet delivers high-quality results, demonstrating robustness across varying underwater degradation types.</p><p>As illustrated in <xref rid="sensors-25-05439-f009" ref-type="fig">Figure 9</xref>, PMSPNet excels at restoring fine structures and maintaining natural color tones, even in severely degraded scenarios. Compared to methods like UIR-PolyKernel and HistFormer, which often produce over-smoothed textures or color shifts, PMSPNet effectively reconstructs edges and preserves the texture of marine organisms. It also avoids common artifacts, such as excessive blue saturation or overly enhanced contrast, frequently observed in other approaches. This balance across metrics highlights the visual fidelity and realism of PMSPNet, making it both quantitatively superior and perceptually compelling.</p><p>In summary, the above experiments demonstrate that PMSPNet can effectively remove underwater non-uniform blur, generating clearer structures, more natural colors, and fewer artifacts and exhibiting strong generalization across diverse underwater scenes. Compared to both traditional and deep learning-based baselines, PMSPNet achieves an optimal trade-off between detail preservation and deblurring, especially under challenging conditions, validating the effectiveness and robustness of the proposed approach.</p></sec></sec><sec id="sec4dot4-sensors-25-05439"><title>4.4. Ablation Study</title><p>To validate the effectiveness of the proposed components in PMSPNet, we conduct comprehensive ablation experiments by removing key modules and evaluating the impact of different loss function combinations. Furthermore, we assess the scalability of the proposed algorithm in downstream tasks.</p><sec id="sec4dot4dot1-sensors-25-05439"><title>4.4.1. Butterworth Filter</title><p>We introduce a Butterworth filter in the input stage of the PMPB to improve the frequency correspondence and mitigate the aliasing phenomenon. As shown in <xref rid="sensors-25-05439-t006" ref-type="table">Table 6</xref>, although removing the module results in a slight increase in PSNR by 0.47, all other evaluation metrics exhibit a decline. In particular, SSIM drops to 0.88, UIQM decreases to 3.08, and UCIQE falls to 0.39. These results underscore the module&#8217;s effectiveness in enhancing structural clarity and perceptual quality as perceived by the human visual system. As illustrated in <xref rid="sensors-25-05439-f010" ref-type="fig">Figure 10</xref>b, the absence of this module leads to noticeable blurring and color artifacts, particularly around object edges and in background regions. This further confirms the module&#8217;s critical role in improving the overall structural integrity of the image and enhancing the network&#8217;s sensitivity to spatially non-uniform blur.</p></sec><sec id="sec4dot4dot2-sensors-25-05439"><title>4.4.2. Deformable Convolution</title><p>The high dynamics of deformable convolution enable precise localization and directional modeling of non-uniform blur, allowing the network to effectively capture regionally continuous blur patterns and apply accurate weighting to key features. As shown in <xref rid="sensors-25-05439-t006" ref-type="table">Table 6</xref>, the removal of deformable convolution results in a substantial decline across all evaluation metrics, with PSNR dropping sharply to 17.18, indicating a significant reduction in reconstruction quality. Furthermore, the network&#8217;s overall stability is noticeably compromised. As illustrated in <xref rid="sensors-25-05439-f010" ref-type="fig">Figure 10</xref>c, the network without this module struggles to preserve object boundaries and spatial consistency, and the residual blur remains largely unaddressed. These results demonstrate that deformable convolution significantly enhances the network&#8217;s capacity to identify critical image features and improves the modeling of local structures, which is essential for effective UIE under non-uniform degradation conditions.</p></sec><sec id="sec4dot4dot3-sensors-25-05439"><title>4.4.3. Progressive Motion-Aware Perception Branch</title><p>The PMPB integrates a Butterworth filter and deformable convolution to progressively capture the spatial distribution of blur and guide feature extraction in the backbone, enabling a coarse-to-fine refinement of blur localization. As shown in <xref rid="sensors-25-05439-t006" ref-type="table">Table 6</xref>, while the removal of PMPB results in a slight increase in PSNR, it reduces the overall stability of the network. This observation also validates the complementary roles of HIAM and PMPB: HIAM facilitates multi-directional feature perception and coarse localization of blur, whereas PMPB further refines this localization, enhancing the network&#8217;s generalization and robustness. Additionally, the observed decline in metrics such as SSIM confirms PMPB&#8217;s contribution to structural clarity and perceptual quality. As illustrated in <xref rid="sensors-25-05439-f010" ref-type="fig">Figure 10</xref>d, the absence of PMPB increases color distortion and reduces image sharpness, particularly in fine structures. These results underscore the importance of PMPB in preserving texture details and scale-aware representations, achieving a critical balance between global semantic understanding and local detail restoration.</p></sec><sec id="sec4dot4dot4-sensors-25-05439"><title>4.4.4. Progressive Feature Feedback Block</title><p>The PFFB hierarchically integrates multi-level features by facilitating cross-layer feedback, enabling the fusion of spatial details with deep semantic representations. As shown in <xref rid="sensors-25-05439-t006" ref-type="table">Table 6</xref>, the removal of this module leads to increased information loss during image reconstruction, which diminishes the network&#8217;s adaptive reconstruction capability, resulting in a consistent, albeit modest, degradation across all evaluation metrics. <xref rid="sensors-25-05439-f010" ref-type="fig">Figure 10</xref>e reveals noticeable artifacts and diminished smoothness, particularly along background contours, indicating the loss of critical structural details. These results demonstrate that PFFB plays a vital role in regulating cross-layer information flow, guiding adaptive feature reweighting, and reducing reconstruction-induced information loss. Consequently, it contributes significantly to improving structural fidelity and perceptual consistency in enhanced underwater images.</p></sec><sec id="sec4dot4dot5-sensors-25-05439"><title>4.4.5. Loss Function</title><p>We investigated the impact of different loss components on the overall performance of the proposed network. As illustrated in <xref rid="sensors-25-05439-t007" ref-type="table">Table 7</xref>, removing most auxiliary losses leads to a notable decline across multiple evaluation metrics. Among these, the FFT, LAB, and LCH loss functions contribute most significantly to performance improvement, enhancing objective image quality and perceptual fidelity. In particular, the inclusion of the LAB loss enhances network stability, underscoring the importance of multi-domain color perception. While the VGG and Color losses did not yield substantial improvements in numerical metrics, they contributed positively to visual quality by enhancing hue perception and overall appearance. <xref rid="sensors-25-05439-f010" ref-type="fig">Figure 10</xref>f&#8211;k demonstrates that models trained without specific loss terms suffer from excessive smoothing or unnatural color shifts. In contrast, the complete loss configuration consistently generates underwater images with sharper edges, more balanced color tones, and enhanced visual appeal, validating the effectiveness of each component.</p></sec><sec id="sec4dot4dot6-sensors-25-05439"><title>4.4.6. Blur Perceptual Localization</title><p>To further investigate the contribution of components to blur perception, we visualize the heatmaps generated under various ablation settings in <xref rid="sensors-25-05439-f011" ref-type="fig">Figure 11</xref>, using the second column of <xref rid="sensors-25-05439-f001" ref-type="fig">Figure 1</xref> as a representative example. When the Butterworth filter is removed, the heatmap exhibits higher noise levels, indicating a weakened ability to extract key features and accurately capture blur-related information. The removal of deformable convolution and the PFFB markedly impairs the network&#8217;s capacity for accurately identifying and localizing blurred regions. These components are essential for enabling coarse-to-fine blur localization and iterative attention modulation. Eliminating the PMPB module causes the network&#8217;s attention to drift while demonstrating the capability of HIAM in achieving coarse localization of blur features.</p><p>From the perspective of loss functions, removing the FFT loss impairs the network&#8217;s ability to capture frequency-domain information, thereby diminishing its capacity to identify and localize blur across multiple spatial domains. VGG, LAB, and LCH losses contribute to guiding the network&#8217;s attention toward perceptually and semantically important regions, facilitating more accurate blur localization. While Color loss has a relatively smaller effect on network performance, it plays a crucial role in suppressing over-sharpening, thereby enhancing visual naturalness.</p><p>In contrast, the proposed model exhibits the most concentrated and semantically accurate activation responses, highlighting blurred regions. These results validate the effectiveness of our architectural design and multi-component loss in improving the network&#8217;s capability for blur perception and structural fidelity restoration in underwater environments.</p></sec><sec id="sec4dot4dot7-sensors-25-05439"><title>4.4.7. Downstream Task Evaluation</title><p>To further validate the practical benefits of the proposed method, we conducted evaluations on downstream tasks, including classic feature extraction and underwater object detection (UOD).</p><p>As illustrated in <xref rid="sensors-25-05439-f012" ref-type="fig">Figure 12</xref>, we applied SIFT keypoint detection and Canny edge detection to assess the influence of enhancement on low-level visual features. The first row shows the SIFT results, while the second row displays the Canny edges. Traditional methods often fail to preserve sufficient structural detail, introducing noise and blurring in non-salient regions, which results in some key edges being obscured or missing. In contrast, images enhanced by PMSPNet exhibit clear contours and rich edge information, closely matching the ground truth, demonstrating that PMSPNet effectively enhances geometric clarity, facilitating better feature localization.</p><p>To assess high-level perception, we applied YOLOv12 [<xref rid="B71-sensors-25-05439" ref-type="bibr">71</xref>] for underwater object detection. A YOLOv12 model pre-trained on the original DUO dataset was used to detect objects in images enhanced by different methods. As shown in <xref rid="sensors-25-05439-f013" ref-type="fig">Figure 13</xref>, traditional enhancement approaches often introduce significant color distortions, which impair detection performance. Deep learning-based baselines exhibit limited performance in small object detection, primarily due to underperformance in detail preservation. In contrast, PMSPNet-enhanced images have more accurate and stable detection outcomes, achieving higher confidence scores and better alignment with the original detection results. Notably, because YOLOv12 was trained on the original images, detection performance is best in raw images.</p><p>The above results demonstrate that PMSPNet not only improves perceptual image quality but also preserves semantic fidelity, making it well-suited for integration into real-world underwater robotic systems and visual monitoring pipelines.</p></sec><sec id="sec4dot4dot8-sensors-25-05439"><title>4.4.8. Limitation Analysis</title><p>Although PMSPNet demonstrates superior performance in non-uniform underwater image deblurring, several limitations remain. Due to the incorporation of multi-scale perception and progressive feedback mechanisms, the model introduces additional computational overhead, which may restrict its real-time deployment on extremely resource-constrained underwater robotic platforms. While the proposed N2UD dataset covers diverse non-uniform blur patterns, it still cannot fully represent the wide spectrum of degradations in real-world underwater environments, such as extreme turbidity, lighting fluctuations, or dynamic background interference. While we validated the universality of PMSPNet on downstream tasks such as detection, edge detection, and keypoint localization, comprehensive evaluations on segmentation tasks remain limited. In addition, this study primarily focuses on algorithm-level comparisons. Integrating advanced hardware-oriented solutions, such as novel imaging sensors or optical acquisition systems, represents another promising research direction.</p><p>Future work will focus on addressing these issues by optimizing the network components to improve their flexibility, enabling lightweight and real-time deployment. Moreover, we will expand the dataset with more challenging real-world scenarios and extend the validation to segmentation tasks.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05439"><title>5. Conclusions</title><p>This article proposes PMSPNet, a Progressive Multi-Scale Perception Network designed for the challenging task of non-uniform underwater image deblurring. PMSPNet incorporates a Hybrid Interaction Attention Module (HIAM) to effectively capture fine-grained visual textures and long-range contextual dependencies, enabling robust modeling of feature ambiguity and spatial disparity. Furthermore, the Progressive Motion-Aware Perception Branch (PMPB) is introduced to explicitly represent spatial orientation variations and progressively refine the localization of blur-affected regions. In addition, the Progressive Feature Feedback Block (PFFB) enhances feature reconstruction by leveraging multi-level information in a feedback manner, improving feature restoration quality. To enable reliable evaluation, we construct the N2UD dataset, which contains diverse non-uniform blur patterns representative of real-world underwater environments. Extensive experiments on real-world datasets validate the superiority of our method in terms of both quantitative metrics and visual quality. While achieving an inference speed of 0.01 s, it reached the highest PSNR of 25.51 dB and SSIM of 0.92 on the N2UD dataset. PMSPNet also demonstrates clear advantages in downstream tasks such as edge detection and object recognition, which enhances the visual reliability of underwater perception systems, facilitating downstream tasks such as object detection, recognition, and navigation in robotic platforms. Future efforts will focus on extending PMSPNet for real-time deployment in underwater robotic systems. Moreover, extending PMSPNet to handle real-time underwater video restoration and multimodal fusion with sonar or depth data remains a promising direction. In addition, diffusion-based generative models, despite their current limitations in computational efficiency, offer great potential for modeling complex blur patterns and realistic underwater degradations. Integrating the strengths of diffusion models into underwater deblurring frameworks will be an important avenue for our future research.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, D.K.; methodology, D.K. and Y.Z.; software, D.K. and Y.Z.; validation, X.Z. and Y.W. (Yanyan Wang); formal analysis, D.K. and Y.Z.; investigation, D.K. and Y.W. (Yanyan Wang); resources, D.K. and X.Z.; data curation, Y.Z. and Y.W. (Yanqiang Wang); writing&#8212;original draft preparation, D.K. and Y.Z.; writing&#8212;review and editing, D.K. and Y.Z.; visualization, X.Z. and Y.W. (Yanqiang Wang); supervision, X.Z.; project administration, D.K. and Y.Z.; funding acquisition, D.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Publicly available datasets were analyzed in this study. These data can be found at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/UI2025/N2UD">https://github.com/UI2025/N2UD</uri> (N2UD), accessed on 28 August 2025; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://irvlab.cs.umn.edu/resources/euvp-dataset">https://irvlab.cs.umn.edu/resources/euvp-dataset</uri> (EUVP), accessed on 17 May 2025; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://lintaopeng.github.io/_pages/UIE%20Project%20Page.html">https://lintaopeng.github.io/_pages/UIE%20Project%20Page.html</uri> (LSUI), accessed on 17 May 2025; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://li-chongyi.github.io/proj_benchmark.html">https://li-chongyi.github.io/proj_benchmark.html</uri> (UIEB), accessed on 17 May 2025; and <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/chongweiliu/DUO">https://github.com/chongweiliu/DUO</uri> (DUO), accessed on 18 May 2025.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05439"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Long</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Muhammad</surname><given-names>G.</given-names></name></person-group><article-title>Degradation-Decoupling Vision Enhancement for Intelligent Underwater Robot Vision Perception System</article-title><source>IEEE Internet Things J.</source><year>2025</year><volume>12</volume><fpage>17880</fpage><lpage>17895</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2025.3540033</pub-id></element-citation></ref><ref id="B2-sensors-25-05439"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hui</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name></person-group><article-title>Underwater Robots and Key Technologies for Operation Control</article-title><source>Cyborg Bionic Syst.</source><year>2024</year><volume>5</volume><fpage>0089</fpage><pub-id pub-id-type="doi">10.34133/cbsystems.0089</pub-id><pub-id pub-id-type="pmid">38550252</pub-id><pub-id pub-id-type="pmcid">PMC10976592</pub-id></element-citation></ref><ref id="B3-sensors-25-05439"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gonz&#225;lez-Sabbagh</surname><given-names>S.P.</given-names></name><name name-style="western"><surname>Robles-Kelly</surname><given-names>A.</given-names></name></person-group><article-title>A Survey on Underwater Computer Vision</article-title><source>ACM Comput. Surv.</source><year>2023</year><volume>55</volume><fpage>1</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1145/3578516</pub-id></element-citation></ref><ref id="B4-sensors-25-05439"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Reda</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>S.G.</given-names></name></person-group><article-title>Polarization-Driven Solution for Mitigating Scattering and Uneven Illumination in Underwater Imagery</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3358828</pub-id></element-citation></ref><ref id="B5-sensors-25-05439"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>X.</given-names></name></person-group><article-title>An Underwater Image Restoration Method With Polarization Imaging Optimization Model for Poor Visible Conditions</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2025</year><volume>35</volume><fpage>3924</fpage><lpage>3939</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2024.3512600</pub-id></element-citation></ref><ref id="B6-sensors-25-05439"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name></person-group><article-title>Multi-modality object detection with sonar and underwater camera via object-shadow feature generation and saliency information</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>287</volume><fpage>128021</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2025.128021</pub-id></element-citation></ref><ref id="B7-sensors-25-05439"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>L.</given-names></name></person-group><article-title>A laser field synchronous scanning imaging system for underwater long-range detection</article-title><source>Opt. Laser Technol.</source><year>2024</year><volume>175</volume><fpage>110849</fpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2024.110849</pub-id></element-citation></ref><ref id="B8-sensors-25-05439"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>AQUA-SLAM: Tightly Coupled Underwater Acoustic-Visual-Inertial SLAM With Sensor Calibration</article-title><source>IEEE Trans. Robot.</source><year>2025</year><volume>41</volume><fpage>2785</fpage><lpage>2803</lpage><pub-id pub-id-type="doi">10.1109/TRO.2025.3554396</pub-id></element-citation></ref><ref id="B9-sensors-25-05439"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Han</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.</given-names></name></person-group><article-title>Environment-Tolerant Trust Opportunity Routing Based on Reinforcement Learning for Internet of Underwater Things</article-title><source>IEEE Trans. Mob. Comput.</source><year>2025</year><volume>24</volume><fpage>6348</fpage><lpage>6360</lpage><pub-id pub-id-type="doi">10.1109/TMC.2025.3540774</pub-id></element-citation></ref><ref id="B10-sensors-25-05439"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Song</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>J.H.</given-names></name></person-group><article-title>Joint Power Control and Multipath Routing for Internet of Underwater Things in Varying Environments</article-title><source>IEEE Internet Things J.</source><year>2025</year><volume>12</volume><fpage>15197</fpage><lpage>15210</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2025.3527692</pub-id></element-citation></ref><ref id="B11-sensors-25-05439"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Song</surname><given-names>H.</given-names></name></person-group><article-title>Security and Reliability of InternSecurity and Reliability of Internet of Underwater Things: Architecture, Challenges, and Opportunities</article-title><source>ACM Comput. Surv.</source><year>2024</year><volume>57</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1145/3700640</pub-id></element-citation></ref><ref id="B12-sensors-25-05439"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Draz</surname><given-names>U.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yasin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chaudary</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Yasin</surname><given-names>I.</given-names></name><name name-style="western"><surname>Ayaz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Aggoune</surname><given-names>E.H.M.</given-names></name></person-group><article-title>Hybrid Underwater Localization Communication Framework for Blockchain-Enabled IoT Underwater Acoustic Sensor Network</article-title><source>IEEE Internet Things J.</source><year>2025</year><volume>12</volume><fpage>16858</fpage><lpage>16885</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2025.3535375</pub-id></element-citation></ref><ref id="B13-sensors-25-05439"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rupa</surname><given-names>C.</given-names></name><name name-style="western"><surname>Varshitha</surname><given-names>G.S.</given-names></name><name name-style="western"><surname>Divya</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gadekallu</surname><given-names>T.R.</given-names></name><name name-style="western"><surname>Srivastava</surname><given-names>G.</given-names></name></person-group><article-title>A Novel and Robust Authentication Protocol for Secure Underwater Communication Systems</article-title><source>IEEE Internet Things J.</source><year>2025</year><pub-id pub-id-type="doi">10.1109/JIOT.2025.3601984</pub-id></element-citation></ref><ref id="B14-sensors-25-05439"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Boukerche</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name></person-group><article-title>An Efficient Secure and Adaptive Routing Protocol Based on GMM-HMM-LSTM for Internet of Underwater Things</article-title><source>IEEE Internet Things J.</source><year>2024</year><volume>11</volume><fpage>16491</fpage><lpage>16504</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2024.3354820</pub-id></element-citation></ref><ref id="B15-sensors-25-05439"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chandrasekar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sreenivas</surname><given-names>M.</given-names></name><name name-style="western"><surname>Biswas</surname><given-names>S.</given-names></name></person-group><article-title>PhISH-Net: Physics Inspired System for High Resolution Underwater Image Enhancement</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2024</conf-date><fpage>1506</fpage><lpage>1516</lpage></element-citation></ref><ref id="B16-sensors-25-05439"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><article-title>Underwater image restoration via depth map and illumination estimation based on a single image</article-title><source>Opt. Express</source><year>2021</year><volume>29</volume><fpage>29864</fpage><lpage>29886</lpage><pub-id pub-id-type="doi">10.1364/OE.427839</pub-id><pub-id pub-id-type="pmid">34614723</pub-id></element-citation></ref><ref id="B17-sensors-25-05439"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>Retinex-inspired underwater image enhancement with information entropy smoothing and non-uniform illumination priors</article-title><source>Pattern Recognit.</source><year>2025</year><volume>162</volume><fpage>111411</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2025.111411</pub-id></element-citation></ref><ref id="B18-sensors-25-05439"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name><name name-style="western"><surname>An</surname><given-names>D.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Turbid Underwater Image Enhancement with Illumination-Constrained and Structure-Preserved Retinex Model</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2025</year><pub-id pub-id-type="doi">10.1109/TCSVT.2025.3575846</pub-id></element-citation></ref><ref id="B19-sensors-25-05439"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Sohel</surname><given-names>F.</given-names></name></person-group><article-title>A Pixel Distribution Remapping and Multi-Prior Retinex Variational Model for Underwater Image Enhancement</article-title><source>IEEE Trans. Multimed.</source><year>2024</year><volume>26</volume><fpage>7838</fpage><lpage>7849</lpage><pub-id pub-id-type="doi">10.1109/TMM.2024.3372400</pub-id></element-citation></ref><ref id="B20-sensors-25-05439"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>P.</given-names></name></person-group><article-title>Underwater Image Enhancement via Principal Component Fusion of Foreground and Background</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>34</volume><fpage>10930</fpage><lpage>10943</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2024.3412748</pub-id></element-citation></ref><ref id="B21-sensors-25-05439"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Dian</surname><given-names>S.</given-names></name></person-group><article-title>Underwater Image Enhancement Based on Multichannel Adaptive Compensation</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2024</year><volume>73</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1109/TIM.2024.3378290</pub-id></element-citation></ref><ref id="B22-sensors-25-05439"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name></person-group><article-title>Underwater Image Enhancement Based on Red Channel Correction and Improved Multiscale Fusion</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3388157</pub-id></element-citation></ref><ref id="B23-sensors-25-05439"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jha</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bhandari</surname><given-names>A.K.</given-names></name></person-group><article-title>CBLA: Color-Balanced Locally Adjustable Underwater Image Enhancement</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2024</year><volume>73</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1109/TIM.2024.3396850</pub-id></element-citation></ref><ref id="B24-sensors-25-05439"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.H.</given-names></name></person-group><article-title>A multi-level wavelet-based underwater image enhancement network with color compensation prior</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>242</volume><fpage>122710</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.122710</pub-id></element-citation></ref><ref id="B25-sensors-25-05439"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>J.</given-names></name></person-group><article-title>Underwater Image Enhancement via Wavelet Decomposition Fusion of Advantage Contrast</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2025</year><volume>35</volume><fpage>7807</fpage><lpage>7820</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2025.3545595</pub-id></element-citation></ref><ref id="B26-sensors-25-05439"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kong</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>L.</given-names></name></person-group><article-title>MUFFNet: Lightweight dynamic underwater image enhancement network based on multi-scale frequency</article-title><source>Front. Mar. Sci.</source><year>2025</year><volume>12</volume><fpage>1541265</fpage><pub-id pub-id-type="doi">10.3389/fmars.2025.1541265</pub-id></element-citation></ref><ref id="B27-sensors-25-05439"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xue</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Degradation-Decoupled and semantic-aggregated cross-space fusion for underwater image enhancement</article-title><source>Inf. Fusion</source><year>2025</year><volume>118</volume><fpage>102927</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2024.102927</pub-id></element-citation></ref><ref id="B28-sensors-25-05439"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Eom</surname><given-names>I.K.</given-names></name></person-group><article-title>Underwater image enhancement using adaptive standardization and normalization networks</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>127</volume><fpage>107445</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.107445</pub-id></element-citation></ref><ref id="B29-sensors-25-05439"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Park</surname><given-names>T.</given-names></name><name name-style="western"><surname>Isola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Efros</surname><given-names>A.A.</given-names></name></person-group><article-title>Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date></element-citation></ref><ref id="B30-sensors-25-05439"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kong</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>Dual-Domain Adaptive Synergy GAN for Enhancing Low-Light Underwater Images</article-title><source>J. Mar. Sci. Eng.</source><year>2025</year><volume>13</volume><elocation-id>1092</elocation-id><pub-id pub-id-type="doi">10.3390/jmse13061092</pub-id></element-citation></ref><ref id="B31-sensors-25-05439"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jain</surname><given-names>A.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name></person-group><article-title>Denoising Diffusion Probabilistic Models</article-title><source>Proceedings of the Advances in Neural Information Processing Systems</source><person-group person-group-type="editor"><name name-style="western"><surname>Larochelle</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ranzato</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hadsell</surname><given-names>R.</given-names></name><name name-style="western"><surname>Balcan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>H.</given-names></name></person-group><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2020</year><volume>Volume 33</volume><fpage>6840</fpage><lpage>6851</lpage></element-citation></ref><ref id="B32-sensors-25-05439"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rombach</surname><given-names>R.</given-names></name><name name-style="western"><surname>Blattmann</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lorenz</surname><given-names>D.</given-names></name><name name-style="western"><surname>Esser</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ommer</surname><given-names>B.</given-names></name></person-group><article-title>High-Resolution Image Synthesis with Latent Diffusion Models</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>10684</fpage><lpage>10695</lpage></element-citation></ref><ref id="B33-sensors-25-05439"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gowing</surname><given-names>G.</given-names></name></person-group><article-title>INGC-GAN: An Implicit Neural-Guided Cycle Generative Approach for Perceptual-Friendly Underwater Image Enhancement</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2025</year><volume>36</volume><fpage>10084</fpage><lpage>10098</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2025.3539841</pub-id><pub-id pub-id-type="pmid">40085466</pub-id></element-citation></ref><ref id="B34-sensors-25-05439"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qing</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>DiffUIE: Learning Latent Global Priors in Diffusion Models for Underwater Image Enhancement</article-title><source>IEEE Trans. Multimed.</source><year>2025</year><volume>27</volume><fpage>2516</fpage><lpage>2529</lpage><pub-id pub-id-type="doi">10.1109/TMM.2024.3521710</pub-id></element-citation></ref><ref id="B35-sensors-25-05439"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.</given-names></name></person-group><article-title>Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>10012</fpage><lpage>10022</lpage></element-citation></ref><ref id="B36-sensors-25-05439"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style="western"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2010.11929</pub-id><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B37-sensors-25-05439"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>A.</given-names></name></person-group><article-title>PAFPT: Progressive aggregator with feature prompted transformer for underwater image enhancement</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>262</volume><fpage>125539</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.125539</pub-id></element-citation></ref><ref id="B38-sensors-25-05439"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>L.</given-names></name></person-group><article-title>Underwater variable zoom: Depth-guided perception network for underwater image enhancement</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>259</volume><fpage>125350</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.125350</pub-id></element-citation></ref><ref id="B39-sensors-25-05439"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y.</given-names></name></person-group><article-title>Frequency domain-based latent diffusion model for underwater image enhancement</article-title><source>Pattern Recognit.</source><year>2025</year><volume>160</volume><fpage>111198</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2024.111198</pub-id></element-citation></ref><ref id="B40-sensors-25-05439"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name></person-group><article-title>Unsupervised Underwater Image Enhancement Based on Disentangled Representations via Double-Order Contrastive Loss</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3353371</pub-id></element-citation></ref><ref id="B41-sensors-25-05439"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>MDA-Net: A Multidistribution Aware Network for Underwater Image Enhancement</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2025</year><volume>63</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3524758</pub-id></element-citation></ref><ref id="B42-sensors-25-05439"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name></person-group><article-title>NPT-UL: An Underwater Image Enhancement Framework Based on Nonphysical Transformation and Unsupervised Learning</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3363037</pub-id></element-citation></ref><ref id="B43-sensors-25-05439"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pun</surname><given-names>C.M.</given-names></name></person-group><article-title>Underwater Image Restoration Through a Prior Guided Hybrid Sense Approach and Extensive Benchmark Analysis</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2025</year><volume>35</volume><fpage>4784</fpage><lpage>4800</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2025.3525593</pub-id></element-citation></ref><ref id="B44-sensors-25-05439"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ji</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hao</surname><given-names>L.Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>FBDPN: CNN-Transformer hybrid feature boosting and differential pyramid network for underwater object detection</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>256</volume><fpage>124978</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.124978</pub-id></element-citation></ref><ref id="B45-sensors-25-05439"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shalini</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ashok Kumar</surname><given-names>L.</given-names></name></person-group><article-title>An explainable artificial intelligence driven fall system for sensor data analysis enhanced by butterworth filtering</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>158</volume><fpage>111364</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2025.111364</pub-id></element-citation></ref><ref id="B46-sensors-25-05439"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kurinjimalar</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pradeep</surname><given-names>J.</given-names></name><name name-style="western"><surname>Harikrishnan</surname><given-names>M.</given-names></name></person-group><article-title>Underwater Image Enhancement Using Gaussian Pyramid, Laplacian Pyramid and Contrast Limited Adaptive Histogram Equalization</article-title><source>Proceedings of the 2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC)</source><conf-loc>Gwalior, India</conf-loc><conf-date>27&#8211;28 July 2024</conf-date><fpage>729</fpage><lpage>734</lpage><pub-id pub-id-type="doi">10.1109/AIC61668.2024.10730935</pub-id></element-citation></ref><ref id="B47-sensors-25-05439"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name></person-group><article-title>Deformable Convolutional Networks</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date></element-citation></ref><ref id="B48-sensors-25-05439"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sattar</surname><given-names>J.</given-names></name></person-group><article-title>Fast Underwater Image Enhancement for Improved Visual Perception</article-title><source>IEEE Robot. Autom. Lett.</source><year>2020</year><volume>5</volume><fpage>3227</fpage><lpage>3234</lpage><pub-id pub-id-type="doi">10.1109/LRA.2020.2974710</pub-id></element-citation></ref><ref id="B49-sensors-25-05439"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>L.</given-names></name></person-group><article-title>U-Shape Transformer for Underwater Image Enhancement</article-title><source>IEEE Trans. Image Process.</source><year>2023</year><volume>32</volume><fpage>3066</fpage><lpage>3079</lpage><pub-id pub-id-type="doi">10.1109/TIP.2023.3276332</pub-id><pub-id pub-id-type="pmid">37200123</pub-id></element-citation></ref><ref id="B50-sensors-25-05439"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cong</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kwong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name></person-group><article-title>An Underwater Image Enhancement Benchmark Dataset and Beyond</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>4376</fpage><lpage>4389</lpage><pub-id pub-id-type="doi">10.1109/TIP.2019.2955241</pub-id><pub-id pub-id-type="pmid">31796402</pub-id></element-citation></ref><ref id="B51-sensors-25-05439"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>A Dataset and Benchmark of Underwater Object Detection for Robot Picking</article-title><source>Proceedings of the 2021 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</source><conf-loc>Shenzhen, China</conf-loc><conf-date>5&#8211;9 July 2021</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICMEW53276.2021.9455997</pub-id></element-citation></ref><ref id="B52-sensors-25-05439"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Isola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Efros</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Shechtman</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>O.</given-names></name></person-group><article-title>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date></element-citation></ref><ref id="B53-sensors-25-05439"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Panetta</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Agaian</surname><given-names>S.</given-names></name></person-group><article-title>Human-Visual-System-Inspired Underwater Image Quality Measures</article-title><source>IEEE J. Ocean. Eng.</source><year>2016</year><volume>41</volume><fpage>541</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1109/JOE.2015.2469915</pub-id></element-citation></ref><ref id="B54-sensors-25-05439"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sowmya</surname><given-names>A.</given-names></name></person-group><article-title>An Underwater Color Image Quality Evaluation Metric</article-title><source>IEEE Trans. Image Process.</source><year>2015</year><volume>24</volume><fpage>6062</fpage><lpage>6071</lpage><pub-id pub-id-type="doi">10.1109/TIP.2015.2491020</pub-id><pub-id pub-id-type="pmid">26513783</pub-id></element-citation></ref><ref id="B55-sensors-25-05439"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mittal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Soundararajan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bovik</surname><given-names>A.C.</given-names></name></person-group><article-title>Making a &#8220;Completely Blind&#8221; Image Quality Analyzer</article-title><source>IEEE Signal Process. Lett.</source><year>2013</year><volume>20</volume><fpage>209</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1109/LSP.2012.2227726</pub-id></element-citation></ref><ref id="B56-sensors-25-05439"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Underwater Ranker: Learn Which Is Better and How to Be Better</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>702</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i1.25147</pub-id></element-citation></ref><ref id="B57-sensors-25-05439"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Underwater Image Enhancement via Weighted Wavelet Visual Perception Fusion</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>34</volume><fpage>2469</fpage><lpage>2483</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2023.3299314</pub-id></element-citation></ref><ref id="B58-sensors-25-05439"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>An</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>HFM: A hybrid fusion method for underwater image enhancement</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>127</volume><fpage>107219</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.107219</pub-id></element-citation></ref><ref id="B59-sensors-25-05439"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhuang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Porikli</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Underwater Image Enhancement with Hyper-Laplacian Reflectance Priors</article-title><source>IEEE Trans. Image Process.</source><year>2022</year><volume>31</volume><fpage>5442</fpage><lpage>5455</lpage><pub-id pub-id-type="doi">10.1109/TIP.2022.3196546</pub-id><pub-id pub-id-type="pmid">35947571</pub-id></element-citation></ref><ref id="B60-sensors-25-05439"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Underwater Image Enhancement by Attenuated Color Channel Correction and Detail Preserved Contrast Enhancement</article-title><source>IEEE J. Ocean. Eng.</source><year>2022</year><volume>47</volume><fpage>718</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1109/JOE.2022.3140563</pub-id></element-citation></ref><ref id="B61-sensors-25-05439"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>H.H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kwong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement</article-title><source>IEEE Trans. Image Process.</source><year>2022</year><volume>31</volume><fpage>3997</fpage><lpage>4010</lpage><pub-id pub-id-type="doi">10.1109/TIP.2022.3177129</pub-id><pub-id pub-id-type="pmid">35657839</pub-id></element-citation></ref><ref id="B62-sensors-25-05439"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Underwater Image Enhancement via Piecewise Color Correction and Dual Prior Optimized Contrast Enhancement</article-title><source>IEEE Signal Process. Lett.</source><year>2023</year><volume>30</volume><fpage>229</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1109/LSP.2023.3255005</pub-id></element-citation></ref><ref id="B63-sensors-25-05439"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>W.</given-names></name></person-group><article-title>TEBCF: Real-World Underwater Image Texture Enhancement Model Based on Blurriness and Color Fusion</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3110575</pub-id></element-citation></ref><ref id="B64-sensors-25-05439"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>Y.T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.R.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>C.J.</given-names></name></person-group><article-title>Histoformer: Histogram-Based Transformer for Efficient Underwater Image Enhancement</article-title><source>IEEE J. Ocean. Eng.</source><year>2025</year><volume>50</volume><fpage>164</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1109/JOE.2024.3474919</pub-id></element-citation></ref><ref id="B65-sensors-25-05439"><label>65.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Negi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kulkarni</surname><given-names>A.</given-names></name><name name-style="western"><surname>Phutke</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Vipparthi</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Murala</surname><given-names>S.</given-names></name></person-group><article-title>Phaseformer: Phase-Based Attention Mechanism for Underwater Image Restoration and Beyond</article-title><source>Proceedings of the 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Tucson, AZ, USA</conf-loc><conf-date>28 February&#8211;4 March 2025</conf-date><fpage>9618</fpage><lpage>9629</lpage><pub-id pub-id-type="doi">10.1109/WACV61041.2025.00931</pub-id></element-citation></ref><ref id="B66-sensors-25-05439"><label>66.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>F.</given-names></name><name name-style="western"><surname>Pun</surname><given-names>C.M.</given-names></name></person-group><article-title>Underwater Image Restoration via Polymorphic Large Kernel CNNs</article-title><source>Proceedings of the ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Hyderabad, India</conf-loc><conf-date>6&#8211;11 April 2025</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/ICASSP49660.2025.10890803</pub-id></element-citation></ref><ref id="B67-sensors-25-05439"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name></person-group><article-title>Underwater Image Enhancement with Cascaded Contrastive Learning</article-title><source>IEEE Trans. Multimed.</source><year>2025</year><volume>27</volume><fpage>1512</fpage><lpage>1525</lpage><pub-id pub-id-type="doi">10.1109/TMM.2024.3521739</pub-id></element-citation></ref><ref id="B68-sensors-25-05439"><label>68.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>K.K.</given-names></name></person-group><article-title>Uncertainty Inspired Underwater Image Enhancement</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2022: 17th European Conference, Tel Aviv, Israel, 23&#8211;27 October 2022</source><comment>Proceedings, Part XVIII</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2022</year><fpage>465</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-19797-0_27</pub-id></element-citation></ref><ref id="B69-sensors-25-05439"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name></person-group><article-title>Unsupervised Underwater Image Restoration: From a Homology Perspective</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2022</year><volume>36</volume><fpage>643</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1609/aaai.v36i1.19944</pub-id></element-citation></ref><ref id="B70-sensors-25-05439"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>K.</given-names></name></person-group><article-title>SGUIE-Net: Semantic Attention Guided Underwater Image Enhancement with Multi-Scale Perception</article-title><source>IEEE Trans. Image Process.</source><year>2022</year><volume>31</volume><fpage>6816</fpage><lpage>6830</lpage><pub-id pub-id-type="doi">10.1109/TIP.2022.3216208</pub-id><pub-id pub-id-type="pmid">36288230</pub-id></element-citation></ref><ref id="B71-sensors-25-05439"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Doermann</surname><given-names>D.</given-names></name></person-group><article-title>YOLOv12: Attention-Centric Real-Time Object Detectors</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2502.12524</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05439-f001" orientation="portrait"><label>Figure 1</label><caption><p>Illustration of non-uniform blur in underwater images. (<bold>i</bold>) The raw underwater images. (<bold>ii</bold>) The estimated blur trajectories, where the arrow directions and positions represent the motion blur vectors. (<bold>iii</bold>) The corresponding blur intensity heatmaps, where warmer colors indicate stronger blur magnitude.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g001.jpg"/></fig><fig position="float" id="sensors-25-05439-f002" orientation="portrait"><label>Figure 2</label><caption><p>PMSPNet network architecture. Core components include the Hybrid Interaction Attention Module (HIAM), Progressive Motion-Aware Perception Branch (PMPB), and Progressive Feature Feedback Block (PFFB). The data come from the Non-uniform Underwater Blur Dataset (N2UD).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g002.jpg"/></fig><fig position="float" id="sensors-25-05439-f003" orientation="portrait"><label>Figure 3</label><caption><p>The N2UD dataset, which contains various underwater environments and different degrees of non-uniform blur.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g003.jpg"/></fig><fig position="float" id="sensors-25-05439-f004" orientation="portrait"><label>Figure 4</label><caption><p>PMSPNet network components, including HIAM, PMPB, and PFFB.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g004.jpg"/></fig><fig position="float" id="sensors-25-05439-f005" orientation="portrait"><label>Figure 5</label><caption><p>Visual comparison on the N2UD dataset, with the PSNR value of the image shown in the upper-right corner of the image. (<bold>a</bold>) Raw images. (<bold>b</bold>) WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]. (<bold>c</bold>) WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]. (<bold>d</bold>) HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]. (<bold>e</bold>) HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]. (<bold>f</bold>) ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]. (<bold>g</bold>) MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]. (<bold>h</bold>) PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]. (<bold>i</bold>) TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]. (<bold>j</bold>) CycleGAN [<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]. (<bold>k</bold>) U-Shape [<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]. (<bold>l</bold>) FUnIE-GAN [<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]. (<bold>m</bold>) Histoformer [<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]. (<bold>n</bold>) Phaseformer [<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]. (<bold>o</bold>) UIR-PolyKernel [<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]. (<bold>p</bold>) CCL-Net [<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]. (<bold>q</bold>) PUIE-Net [<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]. (<bold>r</bold>) USUIR [<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]. (<bold>s</bold>) SGUIE [<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]. (<bold>t</bold>) PMSPNet. (<bold>u</bold>) Ground Truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g005.jpg"/></fig><fig position="float" id="sensors-25-05439-f006" orientation="portrait"><label>Figure 6</label><caption><p>Box plot comparison of PSNR, SSIM, and FSIM on the N2UD dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g006.jpg"/></fig><fig position="float" id="sensors-25-05439-f007" orientation="portrait"><label>Figure 7</label><caption><p>Visual comparison on the EUVP dataset, with the PSNR value of the image shown in the upper-right corner of the image. (<bold>a</bold>) Raw image. (<bold>b</bold>) WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]. (<bold>c</bold>) WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]. (<bold>d</bold>) HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]. (<bold>e</bold>) HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]. (<bold>f</bold>) ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]. (<bold>g</bold>) MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]. (<bold>h</bold>) PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]. (<bold>i</bold>) TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]. (<bold>j</bold>) CycleGAN [<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]. (<bold>k</bold>) U-Shape [<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]. (<bold>l</bold>) FUnIE-GAN [<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]. (<bold>m</bold>) Histoformer [<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]. (<bold>n</bold>) Phaseformer [<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]. (<bold>o</bold>) UIR-PolyKernel [<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]. (<bold>p</bold>) CCL-Net [<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]. (<bold>q</bold>) PUIE-Net [<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]. (<bold>r</bold>) USUIR [<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]. (<bold>s</bold>) SGUIE [<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]. (<bold>t</bold>) PMSPNet. (<bold>u</bold>) Ground Truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g007.jpg"/></fig><fig position="float" id="sensors-25-05439-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visual comparison on the LSUI dataset, with the PSNR value of the image shown in the upper-right corner of the image. (<bold>a</bold>) Raw image. (<bold>b</bold>) WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]. (<bold>c</bold>) WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]. (<bold>d</bold>) HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]. (<bold>e</bold>) HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]. (<bold>f</bold>) ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]. (<bold>g</bold>) MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]. (<bold>h</bold>) PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]. (<bold>i</bold>) TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]. (<bold>j</bold>) CycleGAN [<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]. (<bold>k</bold>) U-Shape [<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]. (<bold>l</bold>) FUnIE-GAN [<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]. (<bold>m</bold>) Histoformer [<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]. (<bold>n</bold>) Phaseformer [<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]. (<bold>o</bold>) UIR-PolyKernel [<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]. (<bold>p</bold>) CCL-Net [<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]. (<bold>q</bold>) PUIE-Net [<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]. (<bold>r</bold>) USUIR [<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]. (<bold>s</bold>) SGUIE [<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]. (<bold>t</bold>) PMSPNet. (<bold>u</bold>) Ground Truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g008.jpg"/></fig><fig position="float" id="sensors-25-05439-f009" orientation="portrait"><label>Figure 9</label><caption><p>Visual comparison on the UIEB dataset, with the PSNR value of the image shown in the upper-right corner of the image. (<bold>a</bold>) Raw image. (<bold>b</bold>) WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]. (<bold>c</bold>) WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]. (<bold>d</bold>) HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]. (<bold>e</bold>) HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]. (<bold>f</bold>) ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]. (<bold>g</bold>) MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]. (<bold>h</bold>) PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]. (<bold>i</bold>) TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]. (<bold>j</bold>) CycleGAN [<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]. (<bold>k</bold>) U-Shape [<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]. (<bold>l</bold>) FUnIE-GAN [<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]. (<bold>m</bold>) Histoformer [<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]. (<bold>n</bold>) Phaseformer [<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]. (<bold>o</bold>) UIR-PolyKernel [<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]. (<bold>p</bold>) CCL-Net [<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]. (<bold>q</bold>) PUIE-Net [<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]. (<bold>r</bold>) USUIR [<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]. (<bold>s</bold>) SGUIE [<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]. (<bold>t</bold>) PMSPNet. (<bold>u</bold>) Ground Truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g009.jpg"/></fig><fig position="float" id="sensors-25-05439-f010" orientation="portrait"><label>Figure 10</label><caption><p>Visual comparison of networks using different components and loss functions. PSNR is shown in the top-right, and the bottom-left presents a zoomed-in view of the red-boxed region. (<bold>a</bold>) Raw image. (<bold>b</bold>) w/o Butterworth. (<bold>c</bold>) w/o Deformable Convolution. (<bold>d</bold>) w/o PFFB. (<bold>e</bold>) w/o PMPB. (<bold>f</bold>) w/o FFT loss. (<bold>g</bold>) w/o FFT and LAB loss. (<bold>h</bold>) w/o FFT, LAB and LCH loss. (<bold>i</bold>) w/o FFT, LAB, LCH, and VGG loss. (<bold>j</bold>) w/o FFT, LAB, LCH, VGG, and Color loss. (<bold>k</bold>) PMSPNet. (<bold>l</bold>) Ground Truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g010.jpg"/></fig><fig position="float" id="sensors-25-05439-f011" orientation="portrait"><label>Figure 11</label><caption><p>Visualization of blur localization under different components and loss functions. Each group of three images corresponds to the enhanced outputs at three different scales. (<bold>a</bold>) w/o Butterworth. (<bold>b</bold>) w/o Deformable Convolution. (<bold>c</bold>) w/o PFFB. (<bold>d</bold>) w/o PMPB. (<bold>e</bold>) w/o FFT loss. (<bold>f</bold>) w/o FFT and LAB loss. (<bold>g</bold>) w/o FFT, LAB, and LCH loss. (<bold>h</bold>) w/o FFT, LAB, LCH, and VGG loss. (<bold>i</bold>) w/o FFT, LAB, LCH, VGG, and Color loss. (<bold>j</bold>) PMSPNet.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g011.jpg"/></fig><fig position="float" id="sensors-25-05439-f012" orientation="portrait"><label>Figure 12</label><caption><p>Visual comparison of SIFT keypoint and Canny edge detection results. The first row presents the visualization of SIFT keypoints, while the second row displays the corresponding Canny edge detection results. (<bold>a</bold>) Raw image. (<bold>b</bold>) WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]. (<bold>c</bold>) WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]. (<bold>d</bold>) HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]. (<bold>e</bold>) HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]. (<bold>f</bold>) ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]. (<bold>g</bold>) MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]. (<bold>h</bold>) PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]. (<bold>i</bold>) TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]. (<bold>j</bold>) CycleGAN [<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]. (<bold>k</bold>) U-Shape [<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]. (<bold>l</bold>) FUnIE-GAN [<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]. (<bold>m</bold>) Histoformer [<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]. (<bold>n</bold>) Phaseformer [<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]. (<bold>o</bold>) UIR-PolyKernel [<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]. (<bold>p</bold>) CCL-Net [<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]. (<bold>q</bold>) PUIE-Net [<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]. (<bold>r</bold>) USUIR [<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]. (<bold>s</bold>) SGUIE [<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]. (<bold>t</bold>) PMSPNet. (<bold>u</bold>) Ground Truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g012.jpg"/></fig><fig position="float" id="sensors-25-05439-f013" orientation="portrait"><label>Figure 13</label><caption><p>Comparison of YOLOv12 detection effects on enhanced underwater images from the DUO dataset. (<bold>a</bold>) Raw image. (<bold>b</bold>) WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]. (<bold>c</bold>) WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]. (<bold>d</bold>) HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]. (<bold>e</bold>) HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]. (<bold>f</bold>) ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]. (<bold>g</bold>) MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]. (<bold>h</bold>) PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]. (<bold>i</bold>) TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]. (<bold>j</bold>) CycleGAN [<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]. (<bold>k</bold>) U-Shape [<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]. (<bold>l</bold>) FUnIE-GAN [<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]. (<bold>m</bold>) Histoformer [<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]. (<bold>n</bold>) Phaseformer [<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]. (<bold>o</bold>) UIR-PolyKernel [<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]. (<bold>p</bold>) CCL-Net [<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]. (<bold>q</bold>) PUIE-Net [<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]. (<bold>r</bold>) USUIR [<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]. (<bold>s</bold>) SGUIE [<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]. (<bold>t</bold>) PMSPNet. (<bold>u</bold>) Detection effect of raw image.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05439-g013.jpg"/></fig><table-wrap position="float" id="sensors-25-05439-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05439-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of performance on the N2UD dataset, where evaluation includes full-reference image quality metrics and resource consumption metrics. All results are reported in the format of mean &#177; standard deviation. &#8593; indicates that a higher value is better, while &#8595; indicates that a lower value is better.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PSNR &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LPIPS &#8595; </th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M) &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G) &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Time (s) &#8595;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.73 &#177; 3.21</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.77 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.48 &#177; 3.59</td><td align="center" valign="middle" rowspan="1" colspan="1">&#177;0.73 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.31 &#177; 3.18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.88 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.53</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">12.80 &#177; 1.97</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.51 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">16.63 &#177; 2.90</td><td align="center" valign="middle" rowspan="1" colspan="1">0.70 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.34 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.80 &#177; 3.73</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.40 &#177; 2.96</td><td align="center" valign="middle" rowspan="1" colspan="1">0.62 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.90 &#177; 2.30</td><td align="center" valign="middle" rowspan="1" colspan="1">0.69 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">1.24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CycleGAN&#160;[<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">23.91 &#177; 4.72</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">22.76</td><td align="center" valign="middle" rowspan="1" colspan="1">99.364</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Shape&#160;[<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.32 &#177; 3.87</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">31.59</td><td align="center" valign="middle" rowspan="1" colspan="1">26.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FUnIE-GAN&#160;[<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">21.42 &#177; 3.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">3.59</td><td align="center" valign="middle" rowspan="1" colspan="1">26.72</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Histoformer&#160;[<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">13.96 &#177; 2.25</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">25.71</td><td align="center" valign="middle" rowspan="1" colspan="1">44.42</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Phaseformer&#160;[<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.13 &#177; 3.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.21 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">1.78</td><td align="center" valign="middle" rowspan="1" colspan="1">14.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UIR-PolyKernel&#160;[<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">22.19 &#177; 4.62</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">1.89</td><td align="center" valign="middle" rowspan="1" colspan="1">13.68</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CCL-Net&#160;[<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">23.81 &#177; 5.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83 &#177; 0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23 &#177; 0.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.55</td><td align="center" valign="middle" rowspan="1" colspan="1">37.36</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PUIE-Net&#160;[<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.40 &#177; 3.78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.96 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83</td><td align="center" valign="middle" rowspan="1" colspan="1">150.69</td><td align="center" valign="middle" rowspan="1" colspan="1">0.13</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">USUIR&#160;[<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">18.79 &#177; 2.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23</td><td align="center" valign="middle" rowspan="1" colspan="1">14.88</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SGUIE&#160;[<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.13 &#177; 4.50</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">18.63</td><td align="center" valign="middle" rowspan="1" colspan="1">20.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PMSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.51 &#177; 3.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92 &#177; 0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95 &#177; 0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.19 &#177; 0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.01</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05439-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05439-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of performance on the N2UD dataset, where evaluation is based on no-reference image quality metrics. All results are reported in the format of mean &#177; standard deviation. &#8593; indicates that a higher value is better, while &#8595; indicates that a lower value is better.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UIQM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UCIQE &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NIQE &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">URANKER &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Laplacian &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Tenengrad &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Brenner &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.16 &#177; 0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.42 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">6.02 &#177; 3.46</td><td align="center" valign="middle" rowspan="1" colspan="1">2.49 &#177; 0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11 &#177; 0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.53 &#177; 0.23</td><td align="center" valign="middle" rowspan="1" colspan="1">2044.20 &#177; 1559.13</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.85 &#177; 0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">5.43 &#177; 2.20</td><td align="center" valign="middle" rowspan="1" colspan="1">2.50 &#177; 0.741</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44 &#177; 0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">1291.93 &#177; 911.55</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.91 &#177; 0.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">5.49 &#177; 2.02</td><td align="center" valign="middle" rowspan="1" colspan="1">2.35 &#177; 0.81</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">662.62 &#177; 639.15</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.62 &#177; 0.67</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">5.78 &#177; 2.25</td><td align="center" valign="middle" rowspan="1" colspan="1">1.69 &#177; 0.96</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">1016.97 &#177; 489.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.34 &#177; 0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">5.24 &#177; 1.78</td><td align="center" valign="middle" rowspan="1" colspan="1">2.68 &#177; 0.76</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.42 &#177; 0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">990.85 &#177; 790.32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.77 &#177; 0.41</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">5.56 &#177; 2.42</td><td align="center" valign="middle" rowspan="1" colspan="1">2.53 &#177; 0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46 &#177; 0.19</td><td align="center" valign="middle" rowspan="1" colspan="1">1504.57 &#177; 1072.266</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.66 &#177; 0.68</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">5.50 &#177; 1.98</td><td align="center" valign="middle" rowspan="1" colspan="1">2.59 &#177; 0.74</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.50 &#177; 0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">1671.21 &#177; 995.43</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.00 &#177; 0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">5.60 &#177; 1.75</td><td align="center" valign="middle" rowspan="1" colspan="1">2.42 &#177; 0.69</td><td align="center" valign="middle" rowspan="1" colspan="1">0.049 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">1274.67 &#177; 699.73</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CycleGAN&#160;[<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.19 &#177; 0.42</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">4.62 &#177; 1.30</td><td align="center" valign="middle" rowspan="1" colspan="1">1.50 &#177; 0.81</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">448.48 &#177; 291.55</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Shape&#160;[<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.10 &#177; 0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">5.05 &#177; 1.37</td><td align="center" valign="middle" rowspan="1" colspan="1">1.35 &#177; 0.74</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">313.03 &#177; 202.13</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FUnIE-GAN&#160;[<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.10 &#177; 0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">4.17 &#177; 0.90</td><td align="center" valign="middle" rowspan="1" colspan="1">1.80 &#177; 0.78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">607.95 &#177; 504.67</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Histoformer&#160;[<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.09 &#177; 0.27</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">12.00 &#177; 3.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.74 &#177; 0.54</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.12 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">99.17 &#177; 76.09</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Phaseformer&#160;[<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.77 &#177; 0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">7.75 &#177; 6.61</td><td align="center" valign="middle" rowspan="1" colspan="1">1.26 &#177; 0.79</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">399.73 &#177; 353.68</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UIR-PolyKernel&#160;[<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.91 &#177; 0.62</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">5.09 &#177; 1.38</td><td align="center" valign="middle" rowspan="1" colspan="1">1.18 &#177; 0.99</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">435.62 &#177; 447.07</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CCL-Net&#160;[<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.03 &#177; 0.48</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">5.56 &#177; 2.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1.46 &#177; 0.76</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">488.43 &#177; 408.52</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PUIE-Net&#160;[<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.00 &#177; 0.51</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">5.55 &#177; 1.85</td><td align="center" valign="middle" rowspan="1" colspan="1">1.42 &#177; 0.85</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">430.14 &#177; 375.36</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">USUIR&#160;[<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.96 &#177; 0.30</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">4.80 &#177; 1.15</td><td align="center" valign="middle" rowspan="1" colspan="1">1.51 &#177; 0.82</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01 &#177; 0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">528.35 &#177; 372.24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SGUIE&#160;[<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.96 &#177; 0.56</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">5.46 &#177; 1.52</td><td align="center" valign="middle" rowspan="1" colspan="1">1.31 &#177; 0.90</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">350.12 &#177; 326.33</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PMSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.46 &#177; 0.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46 &#177; 0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.30 &#177; 1.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.73 &#177; 0.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.01 &#177; 0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.22 &#177; 0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">319.80 &#177; 254.67</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05439-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05439-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of performance on the EUVP dataset, where evaluation includes full-reference and no-reference image quality metrics. All results are reported in the format of mean &#177; standard deviation. &#8593; indicates that a higher value is better, while &#8595; indicates that a lower value is better.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PSNR &#8593; </th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LPIPS &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UIQM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UCIQE &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">URANKER &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">13.24 &#177; 2.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.54 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">2.81 &#177; 0.26</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">2.97 &#177; 0.90</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">14.62 &#177; 2.83</td><td align="center" valign="middle" rowspan="1" colspan="1">0.60 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">2.68 &#177; 0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.16 &#177; 0.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.13 &#177; 2.74</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.93 &#177; 0.25</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.44 &#177; 0.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">11.41 &#177; 1.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.60 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">2.65 &#177; 0.59</td><td align="center" valign="middle" rowspan="1" colspan="1">0.50 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">2.09 &#177; 0.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">14.42 &#177; 2.61</td><td align="center" valign="middle" rowspan="1" colspan="1">0.60 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">3.34 &#177; 0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.98 &#177; 0.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">14.12 &#177; 2.69</td><td align="center" valign="middle" rowspan="1" colspan="1">0.59 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">2.56 &#177; 0.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.78 &#177; 0.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">13.55 &#177; 2.48</td><td align="center" valign="middle" rowspan="1" colspan="1">0.52 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.37 &#177; 0.56</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">2.97 &#177; 0.80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.07 &#177; 2.55</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">2.82 &#177; 0.36</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.59 &#177; 0.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CycleGAN&#160;[<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">22.68 &#177; 3.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">3.11 &#177; 0.50</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.30 &#177; 0.85</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Shape&#160;[<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.92 &#177; 3.78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">2.97 &#177; 0.62</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1.18 &#177; 0.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FUnIE-GAN&#160;[<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.06 &#177; 2.60</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.27 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.88 &#177; 0.57</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1.36 &#177; 0.82</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Histoformer&#160;[<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">14.82 &#177; 2.88</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.65 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.71 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">3.12 &#177; 0.23</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.50</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Phaseformer&#160;[<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">23.58 &#177; 2.64</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.27 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">2.63 &#177; 0.51</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98 &#177; 0.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UIR-PolyKernel&#160;[<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.92 &#177; 3.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.85 &#177; 0.74</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.37 &#177; 0.90</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CCL-Net&#160;[<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.55 &#177; 3.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.97 &#177; 0.60</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.34 &#177; 0.80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PUIE-Net&#160;[<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.71 &#177; 2.70</td><td align="center" valign="middle" rowspan="1" colspan="1">0.85 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.97 &#177; 0.61</td><td align="center" valign="middle" rowspan="1" colspan="1">0.37 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.12 &#177; 0.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">USUIR&#160;[<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.53 &#177; 2.47</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">2.82 &#177; 0.22</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1.78 &#177; 0.89</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SGUIE&#160;[<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">25.48 &#177; 3.23</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">2.83 &#177; 0.71</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.12 &#177; 0.87</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PMSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.81 &#177; 3.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85 &#177; 0.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.94 &#177; 0.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.21 &#177; 0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.09 &#177; 0.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.36 &#177; 0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.16 &#177; 0.78</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05439-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05439-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of performance on the LSUI dataset, where evaluation includes full-reference and no-reference image quality metrics. All results are reported in the format of mean &#177; standard deviation. &#8593; indicates that a higher value is better, while &#8595; indicates that a lower value is better.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PSNR &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LPIPS &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UIQM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UCIQE &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">URANKER &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.35 &#177; 3.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.37 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">2.76 &#177; 0.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">2.55 &#177; 0.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.51 &#177; 3.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.70 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.81 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">2.76 &#177; 0.41</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">2.59 &#177; 0.65</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.63 &#177; 2.93</td><td align="center" valign="middle" rowspan="1" colspan="1">0.74 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.80 &#177; 0.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.41 &#177; 0.73</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">13.04 &#177; 1.87</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.55 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">2.80 &#177; 0.58</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">1.65 &#177; 0.88</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">16.96 &#177; 2.63</td><td align="center" valign="middle" rowspan="1" colspan="1">0.71 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">3.34 &#177; 0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.63 &#177; 0.72</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.59 &#177; 3.15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.69 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">2.55 &#177; 0.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.59 &#177; 0.80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.25 &#177; 2.30</td><td align="center" valign="middle" rowspan="1" colspan="1">0.59 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">2.32 &#177; 0.57</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.75 &#177; 0.69</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.95 &#177; 2.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">2.93 &#177; 0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.58 &#177; 0.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CycleGAN&#160;[<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.93 &#177; 4.32</td><td align="center" valign="middle" rowspan="1" colspan="1">0.85 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">3.21 &#177; 0.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.42 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.63 &#177; 0.71</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Shape&#160;[<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.94 &#177; 3.54</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">3.10 &#177; 0.41</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1.38 &#177; 0.61</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FUnIE-GAN&#160;[<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">21.47 &#177; 3.32</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.28 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">3.09 &#177; 0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1.84 &#177; 0.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Histoformer&#160;[<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">14.05 &#177; 2.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.65 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">3.07 &#177; 0.28</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.70 &#177; 0.56</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Phaseformer&#160;[<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.64 &#177; 3.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">2.79 &#177; 0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.29 &#177; 0.70</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UIR-PolyKernel&#160;[<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">22.22 &#177; 4.20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">2.91 &#177; 0.54</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1.16 &#177; 0.90</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CCL-Net&#160;[<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">25.15 &#177; 4.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 &#177; 0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.19 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">3.02 &#177; 0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.52 &#177; 0.67</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PUIE-Net&#160;[<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">26.21 &#177; 3.63</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">3.06 &#177; 0.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.39 &#177; 0.65</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">USUIR&#160;[<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">18.86 &#177; 2.74</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">2.96 &#177; 0.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1.44 &#177; 0.75</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SGUIE&#160;[<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.59 &#177; 4.40</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.19 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">2.96 &#177; 0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1.37 &#177; 0.78</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PMSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.46 &#177; 3.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92 &#177; 0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.96 &#177; 0.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.13 &#177; 0.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.31 &#177; 0.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.45 &#177; 0.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.51 &#177; 0.70</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05439-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05439-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of performance on the UIEB dataset, where evaluation includes full-reference and no-reference image quality metrics. All results are reported in the format of mean &#177; standard deviation. &#8593; indicates that a higher value is better, while &#8595; indicates that a lower value is better.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PSNR &#8593; </th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LPIPS &#8595;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UIQM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UCIQE &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">URANKER &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">WFAC [<xref rid="B25-sensors-25-05439" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.39 &#177; 2.25</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.74 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.79 &#177; 0.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">2.39 &#177; 0.79</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WWPF [<xref rid="B57-sensors-25-05439" ref-type="bibr">57</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.52 &#177; 2.71</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.75 &#177; 0.54</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.54 &#177; 0.89</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HFM [<xref rid="B58-sensors-25-05439" ref-type="bibr">58</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.76 &#177; 3.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26 &#177; 0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">2.93 &#177; 0.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.33 &#177; 0.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HLRP [<xref rid="B59-sensors-25-05439" ref-type="bibr">59</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">13.33 &#177; 1.58</td><td align="center" valign="middle" rowspan="1" colspan="1">0.19 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.55 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">3.10 &#177; 0.67</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">1.46 &#177; 0.88</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ACDC [<xref rid="B60-sensors-25-05439" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.70 &#177; 3.20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.27 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">3.39 &#177; 0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">2.57 &#177; 0.85</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MMLE [<xref rid="B61-sensors-25-05439" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.35 &#177; 2.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.46 &#177; 0.57</td><td align="center" valign="middle" rowspan="1" colspan="1">0.45 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.56 &#177; 0.96</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PCDE [<xref rid="B62-sensors-25-05439" ref-type="bibr">62</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.20 &#177; 3.67</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61 &#177; 0.19</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75 &#177; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">2.27 &#177; 0.94</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44 &#177; 0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">2.69 &#177; 0.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TEBCF [<xref rid="B63-sensors-25-05439" ref-type="bibr">63</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.68 &#177; 2.51</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">2.84 &#177; 0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">2.60 &#177; 0.87</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CycleGAN&#160;[<xref rid="B29-sensors-25-05439" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">19.44 &#177; 4.28</td><td align="center" valign="middle" rowspan="1" colspan="1">0.77 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.88 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.27 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">3.19 &#177; 0.50</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.15 &#177; 1.02</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Shape&#160;[<xref rid="B49-sensors-25-05439" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">20.72 &#177; 3.59</td><td align="center" valign="middle" rowspan="1" colspan="1">0.81 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.21 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">3.25 &#177; 0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.37 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1.39 &#177; 1.09</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FUnIE-GAN&#160;[<xref rid="B48-sensors-25-05439" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">18.02 &#177; 2.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.88 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29 &#177; 0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">3.42 &#177; 0.21</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">2.09 &#177; 1.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Histoformer&#160;[<xref rid="B64-sensors-25-05439" ref-type="bibr">64</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">12.50 &#177; 1.62</td><td align="center" valign="middle" rowspan="1" colspan="1">0.23 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.59 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">3.15 &#177; 0.22</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.47</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Phaseformer&#160;[<xref rid="B65-sensors-25-05439" ref-type="bibr">65</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">22.41 &#177; 3.28</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68 &#177; 0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.16 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">2.82 &#177; 0.47</td><td align="center" valign="middle" rowspan="1" colspan="1">0.43 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1.48 &#177; 1.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UIR-PolyKernel&#160;[<xref rid="B66-sensors-25-05439" ref-type="bibr">66</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">17.72 &#177; 4.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.96 &#177; 0.78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1.07 &#177; 1.44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CCL-Net&#160;[<xref rid="B67-sensors-25-05439" ref-type="bibr">67</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">16.93 &#177; 5.99</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64 &#177; 0.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.79 &#177; 0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38 &#177; 0.27</td><td align="center" valign="middle" rowspan="1" colspan="1">3.16 &#177; 0.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1.32 &#177; 0.97</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PUIE-Net&#160;[<xref rid="B68-sensors-25-05439" ref-type="bibr">68</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">22.43 &#177; 3.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.13 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">3.09 &#177; 0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1.45 &#177; 1.27</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">USUIR&#160;[<xref rid="B69-sensors-25-05439" ref-type="bibr">69</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">19.95 &#177; 3.41</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.24 &#177; 0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">3.18 &#177; 0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46 &#177; 0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">1.56 &#177; 0.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SGUIE&#160;[<xref rid="B70-sensors-25-05439" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">20.42 &#177; 4.47</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.19 &#177; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">3.10 &#177; 0.63</td><td align="center" valign="middle" rowspan="1" colspan="1">0.37 &#177; 0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1.25 &#177; 1.35</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PMSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.43 &#177; 3.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86 &#177; 0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92 &#177; 0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.21 &#177; 0.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.27 &#177; 0.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.39 &#177; 0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.53 &#177; 1.08</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05439-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05439-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of performance using different components, where evaluation includes full-reference and no-reference image quality metrics. All results are reported in the format of mean &#177; standard deviation. &#8593; indicates that a higher value is better. &#10003; indicates the model is evaluated w/the corresponding module, while &#10007; denotes the model is evaluated w/o the corresponding module.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ButterWorth</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Deformable</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PMPB</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PFFB</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PSNR &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FSIM &#8593; </th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UIQM &#8593; </th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UCIQE &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">25.98 &#177; 4.20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.88 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">3.08 &#177; 0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">17.18 &#177; 5.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73 &#177; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 &#177; 0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">2.31 &#177; 0.84</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">25.99 &#177; 4.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">3.09 &#177; 0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40 &#177; 0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">24.78 &#177; 4.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">3.08 &#177; 0.41</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.06</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.51 &#177; 3.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92 &#177; 0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95 &#177; 0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.46 &#177; 0.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46 &#177; 0.06</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05439-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05439-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of performance using different loss functions, where evaluation includes full-reference and no-reference image quality metrics. All results are reported in the format of mean &#177; standard deviation. &#8593; indicates that a higher value is better. &#10003; indicates the model is evaluated w/the corresponding module, while &#10007; denotes the model is evaluated w/o the corresponding module.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Charbonnier</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FFT</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LAB</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LCH</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">VGG</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Color</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PSNR &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FSIM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UIQM &#8593;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UCIQE &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">24.00 &#177; 3.98</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.90 &#177; 0.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.41 &#177; 0.07</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">21.27 &#177; 4.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">2.95 &#177; 0.41</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35 &#177; 0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">21.43 &#177; 4.21</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">3.08 &#177; 0.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35 &#177; 0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">22.53 &#177; 4.36</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">3.08 &#177; 0.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.37 &#177; 0.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">24.06 &#177; 3.84</td><td align="center" valign="middle" rowspan="1" colspan="1">0.85 &#177; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 &#177; 0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">3.08 &#177; 0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39 &#177; 0.06</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.51 &#177; 3.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92 &#177; 0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95 &#177; 0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.46 &#177; 0.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46 &#177; 0.06</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>